[
    {
        "id": "UgGANSuJ2pm",
        "original": null,
        "number": 1,
        "cdate": 1665995913803,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665995913803,
        "tmdate": 1666019168781,
        "tddate": null,
        "forum": "-0tPmzgXS5",
        "replyto": "-0tPmzgXS5",
        "invitation": "ICLR.cc/2023/Conference/Paper211/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an augmentation technique for video action recognition named GM (ghost motion). In specific, GM generates the fused clips via the shifted channel and temporal misalignment, encouraging the model to alleviate over-fitting. Besides, the authors apply logits smoothing on the temporal-sensitive benchmark (SSv1/v2) for emphasizing temporal dependency. The experiments demonstrate that GM improves frame-wise accuracy and boosts multiple baseline architectures significantly.",
            "strength_and_weaknesses": "Strength:\n+ The way of combining channel disorder and temporal misalignment is novel and interesting. This approach enlarges the input space and transfers the semantic frames to adjacent frames. \n+ The method is computationally friendly and introduces minimal overhead.\n+ The experimental results are comprehensive on various benchmarks including SSv1, SSv2, UCF, HMDB, ActivityNet and Mini-Kinetics.\n+ The paper is generally easy to follow.\n\nWeakness:\n- Miss the ablation study on the disorder channel, which is one of the key components in the paper. Namely, what the performance will be if the mixing video is TSM-like, $(C^R_{i}, C^G_{i}, C^B_{i+1})\\times H \\times W$ or $(C^R_{i-1}, C^G_{i}, C^B_{i})\\times H \\times W$. \n- All the baseline performance is far lower than SOTA. I understand for a fair comparison, the paper unifies the training and test recipes and reproduces the 'bad' results of many previous methods. But the community would be more interested in how much the augmentation technique pushes prior arts and the absolute improvement on top of the baseline methods. \n- The logits smoothing is orthogonal to the ghost motion augmentation and more looks like add-on to improve the performance on Something-Something dataset. What is the result only equipped with ghost motion augmentation? Is it still effective?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The citing format is not precise. Many citations in the paper are supposed to be ~\\citep{} instead of ~\\cite{}. For example, in the first paragraph of the introduction: TSM Lin et al. (2019) ---> TSM (Lin et al. 2019). Abundant wrong citation formats distract the clarity of the paper. \n\nReproducibility: The implementation details are specific and easy to follow for the practitioners. \n\nOriginality: The method is novel and original, though the shifting trick is introduced in the video action recognition field before [1].\n\n[1] TSM: Temporal Shift Module for Efficient Video Understanding. ICCV 2019",
            "summary_of_the_review": "Overall, it is a good paper. The paper introduces a simple yet effective augmentation method to resist over-fitting for video action recognition. The experiments verify the augmentation boosts the performance considerably across the board. Thus, my initial rating is borderline accept. I would like to see the responses from the authors during the discussion period.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper211/Reviewer_Xyj5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper211/Reviewer_Xyj5"
        ]
    },
    {
        "id": "eveHfO7L3IV",
        "original": null,
        "number": 2,
        "cdate": 1666649632568,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649632568,
        "tmdate": 1669110135900,
        "tddate": null,
        "forum": "-0tPmzgXS5",
        "replyto": "-0tPmzgXS5",
        "invitation": "ICLR.cc/2023/Conference/Paper211/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper makes two contributions to reduce overfitting of video recognition models: Ghost Motion (GM) data augmentation which randomly shifts one RGB channel forward or backward in time, and logit smoothing (temperature scaling) to reduce overconfidence on background frames. The authors hypothesize that GM works by exchanging information between neighboring frames, creating motion artifacts that guide the model to focus on temporal cues. Experiments on multiple video action datasets show the proposed training scheme reduces overfitting and improves test accuracy of classifiers.",
            "strength_and_weaknesses": "## Pros\n\n- This paper studies an important problem of video data augmentation, where there has been limited prior work in the literature.  \n- The proposed method is simple and appears effective across datasets and models.  \n- Ablation experiments on the augmentation parameters are provided.  \n\n## Cons\n\n- Methods  \n\t- I don't feel that the motivation for GM is strong enough. Specifically, how does the information transfer between frames lead to stronger temporal modeling at test time (when the channels are not shifted)? It would be nice to have experiments supporting that models trained with GM is more sensitive to temporal cues than spatial ones.  \n\t- I am also not sure about the design choice, where only red or blue channels are shifted, but not the green channel. It would be nice if more explanations or justifications are provided. It would also be interesting to see if shifting all channels with interpolation can achieve similar effects.  \n\t- The technical contribution from this work is somewhat limited: GM mirrors the temporal shift operation of TSM in the input image space, with similar objectives (improve temporal modeling); temperature scaling has been widely used in model calibration literatures to reduce overfitting, and should not be considered an original contribution from this work. \n\n- Experiments\n\t- The effect of logit smoothing and ghost motion is not well disentangled in the experimental results. From figure 3 it appears that temperature tuning alone improves accuracy by ~1%. Is this the main source of improvements in the main tables, or do the gains come from the GM augmentation instead?  \n\t- The improvements over existing image-based augmentations is not particularly significant (e.g. <1% over MixUp).\n\t- There does not seem to be comparison to prior video data augmentation methods such as VideoMix (https://arxiv.org/abs/2012.03457).  \n\n- Minor comments\n\t- I assume that the effectiveness of method depends on the video frame rate. If frame sampling is dense enough, adjacent frames will be too similar that shifting does not make a significant difference. Maybe it would be nice to include additional discussion/experiments on this.",
            "clarity,_quality,_novelty_and_reproducibility": "The method is presented with clarity, which adds to the reproducibility of the work, though there are some confusions in the experimental results. I am less confident in originality as the proposed techniques are either well studied in prior work (legit smoothing) or simply adapted from related problems (channel-wise temporal shifts).",
            "summary_of_the_review": "Although this work has its strengths, I lean toward rejection of the paper at its current state, due to lack of technical contributions and empirical justifications of some claims made in the paper, as detailed in the weaknesses above. I look forward to the authors' response to resolve any confusions/misunderstandings.\n\nUPDATE: Increased rating to borderline reject post-rebuttal. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper211/Reviewer_6srd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper211/Reviewer_6srd"
        ]
    },
    {
        "id": "u4_v7_F4NWE",
        "original": null,
        "number": 3,
        "cdate": 1667242787303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667242787303,
        "tmdate": 1668672033429,
        "tddate": null,
        "forum": "-0tPmzgXS5",
        "replyto": "-0tPmzgXS5",
        "invitation": "ICLR.cc/2023/Conference/Paper211/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new data augmentation for video classification that consists of shifting one of the RGB channels forwards or backwards by 1 frame. They call this augmentation \"Ghost Motion\" (GM). They also show that scaling the logits by a temperature helps generalization. The authors claim that GM improves results for a variety of different models and datasets.",
            "strength_and_weaknesses": "Strengths:\n1) The proposed data augmentation technique is well explored and has an extensive empirical evaluation. I appreciate that the authors investigated it under a number of different models and datasets. While they briefly also investigate modern approaches based on Transformers, most of their results focus on out-dated models based on 2D CNNs. I feel that the paper could be improved by focusing more on modern models. E.g. Table 1 contains results for TSN, which is by now a 6 year old method. I don't think these results have any relevance for the research community. Adding a modern, state-of-the-art method like Video Swin Transformer[4], MViT[5] or MTV[6] instead would considerably strengthen the paper. \n\n\nWeaknesses\n1)  The \"logit scaling\" part of the current manuscript is weak: it's a completely unrelated regularization technique to Ghost Motion, and as the authors do mention, the same technique was furthermore already proposed and investigated for classification tasks in [3]. So mentioning this as a novel contribution is a stretch.  Furthermore it seems to be just another variation of label smoothing [1] or logit squeezing [2], neither of which the authors compare to or even mention. I'd suggest de-empathising this part of the paper and focusing completely on Ghost Motion instead. Otherwise, the authors need to properly compare logit scaling to competing methods.\n\n2)  The results for the baselines stated by the authors do not match those from the original publications. For example, the authors claim in Table 1 that the basic TSM module achieves 45.34% accuracy on SomethingSomethingV1, and that TSM+GM together achieve 46.88%.\nHowever, the original TSM publication states that TSM alone achieves 47.3% accuracy. Similarly, the original TDN publication claims 52.3% top1 accuracy on SomethingSomethingV1, while this manuscript claims that TDN alone achieves 49.69% and TDN+GN  50.51%. I did not check the rest of the numbers, but this small spot check seems weird.  The authors need to address this discrepancy.\n\n3) While the authors claim several times that GM \"brings almost zero costs\", that claim was never backed  up. I'd appreciate if the authors could provide insights on the computational cos of their method: does adding GM really not increase the wall-clock time of training? \n\n4) The authors give all accuracies with 4 significant digits, which would suggest a large number of repeat experiments, yet they do neither mention this nor give error bars. I would strongly suggest they add unambiguous error bars for their results.\n\n\n[1] Mueller et al., \"When does label smoothing help?\", NeurIPS 2019 (and references therein)\n\n[2] Shafahi et al, \"Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? \", ICLR 2019\n\n[3] Agarwala et al, Temperature check: theory and practice for training models with softmax-cross-entropy losses, arxiv 2019\n\n[4] Liu et al, \"Video Swin Transformer\", CVPR 2022\n\n[5] Fan et al, \"Multiscale Vision Transformers\", ICCV 2021\n\n[6] Yan et al, \"Multiview Transformers for Video Recognition\", CVPR 2022",
            "clarity,_quality,_novelty_and_reproducibility": "1) The paper is riddled with grammatical errors and un-idiomatic language. This needs to be addressed before this manuscript can be accepted for publication. The mistakes were too abundant to all write down and note, so I stopped taking notes on this after the abstract:\n\n\u201coverfitting is an even severe problem in 2D video recognition models\u201d => \u201coverfitting is a severe problem\u201d\n\n\u201ccurrent pipeline treats background and semantic frames\u201d => pipelines. (Also, \u201cnon-nformative\u201d and \u201cinformative\u201d might be better than \n\n\u201cbackground and semantic\u201d. The latter made me assume this work was about semantic segmentation, but YMMV.\n\n\u201cnamed as Ghost Motion\u201d => named Ghost Motion\n\n\u201cGM shifts channels along temporal dimension\u201d => along the temporal dimension\n\n\u201cleading to improvement\u201d => improvements\n\n\nI suggest the authors revise the manuscript intensively and extensively for such errors.\n\n\n\n2) Regarding Originality, GM is a simplification of the Temporal shift module (TSM), but the authors show in their experiments that GM improves upon TSM models. As mentioned above, I have strong doubts about the originality of logit scaling.",
            "summary_of_the_review": "The authors propose a  input augmentation that appears to be effective on a wide range of problems and models. The empirical evaluation covers a wide range of models and datasets, yet the numbers raise some questions (see above).\n\nUPDATE: after the author's revision of the paper, I have updated my score to reflect that I think the method now meets the standards for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd"
        ]
    }
]