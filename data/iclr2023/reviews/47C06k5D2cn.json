[
    {
        "id": "hC6tQXOZu0",
        "original": null,
        "number": 1,
        "cdate": 1666389387005,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666389387005,
        "tmdate": 1666389387005,
        "tddate": null,
        "forum": "47C06k5D2cn",
        "replyto": "47C06k5D2cn",
        "invitation": "ICLR.cc/2023/Conference/Paper4811/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes offline policy learning in a confounded setting for contextual bandits and sequential decision-making. The paper introduces the notion of super policy learning in the setup where expert-recommended actions are available at each step in addition to the past historical data. Under this setup, the author(s) propose two algorithms for the contextual bandit and sequential decision-making (RL) settings and derive corresponding regret bounds.\n",
            "strength_and_weaknesses": "Strength:\n\nImportant problem finding an optimal policy in confounded offline reinforcement learning.\nOverall well-written, and the assumptions are stated in the context of the theorem.\nThe practical derivation of the proposed algorithms helps leverage them in a real-world task.\n\nWeakness:\n\nReal-world evaluation of the proposed practical algorithms is missing.\nWhile assumptions are stated, it is unclear if all those are held in a real-world setup.\nMissing empirical evaluation and discussion.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is well-written and easy to follow. However, I would appreciate a proper paper ending with a conclusion or discussion section. In its current form, the paper seems to end abruptly on page 9.\n\nThe paper is well-motivated by real-world applications, and it proposed a relatively comprehensive theoretical analysis of the problem of finding an optimal policy in the presence of the confounder. The practical implementable algorithms are helpful in evaluating them in real-world environments (even in simulation). However, I am rather surprised that no such empirical evaluation is presented in the paper. The use of expert recommendations in decision-making is important and should improve existing policy learning. However, the practicality would depend on the availability of the expert recommendation.\n\nMy understanding is that the algorithm takes input the expert-recommended action at every timestep and then makes a decision based on both history and expert demonstration. However, I am a bit confused about the offline data generation process (history data). What are the assumptions on offline data generation? How is the offline data collected? Is it from a human agent or an optimally learned policy that uses confounded observation data? That raises the question of how much expert-recommended actions are needed to learn an optimal policy in the presence of a confounder. \n\nMoreover, having an expert action at each timestep might be infeasible in many RL tasks. For example, in learning robotic locomotion tasks (e.g., humanoid walk, quadruped run), access to expert recommendations would be infeasible. It is unclear how the proposed method can be leveraged in those settings. Are the confounded environments restricted where such demonstrations are feasible only? Clarifying these points in the abstract and intro would be helpful for the reader to understand. One suggestion would be to specify expert recommendations in the title. Alternatively, a justification would be helpful for how the proposed method is generalized for all the confounded environments, including environments where direct access to expert action at each timestep is not feasible (robotic locomotion).\n\n",
            "summary_of_the_review": "The paper tackles the important problem of finding an optimal policy in confounded offline reinforcement learning. The proposed practical algorithms have the potential to be implemented for real-world tasks. However, the justification of the assumptions on the real-world task is missing. Adding such empirical evaluation would eventually improve the justification of the paper's claims.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4811/Reviewer_SFSz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4811/Reviewer_SFSz"
        ]
    },
    {
        "id": "1k_Nax5Oi5s",
        "original": null,
        "number": 2,
        "cdate": 1666847846881,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666847846881,
        "tmdate": 1670262240515,
        "tddate": null,
        "forum": "47C06k5D2cn",
        "replyto": "47C06k5D2cn",
        "invitation": "ICLR.cc/2023/Conference/Paper4811/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers offline reinforcement learning in the presence of unobserved confounding. There are two observations. First, including the \"expert recommended\" action (from the behavioral policy) as part of the state is helpful, because this action was informed by the unobserved confounder. Second, if there are proxies for the unobserved confounder available, these can be leveraged to learn the optimal policy under the (better) expert-recommendations available setting. This later result is a direct application of recent proxy identification methods in causal inference.\n\n------------\n\nI read the rebuttal comments and the revised paper. I think the point about past/future state serving as proxies is a good one (though I haven't given much thought to its correctness). The authors have also added some experimental demonstration. Unfortunately, these are substantial revisions and I don't have the capacity to do more than a cursory evaluation of them.  \n\nOverall, I remain weakly positive on this paper. It's not earth shattering, but it seems to be a solid execution of a reasonably interesting idea.  I've updated my score to an 8, but with the intention that this should be parsed literally (\"I think it's a good paper worth accepting\") and not as a very strong endorsement. \n\n(FWIW though, I find the brain stimulation example pretty unconvincing---if patient actions are available at runtime, then we'd presumably just ignore whatever the learned policy is and defer to the patient)\n\n-----------\nA second update:\n\nFollowing the reviewer discussion, I'm downgrading my score to a weak reject. The crux of this is:\n1. we all agree that proxy identification results from causal inference translate to results for policy learning for unobserved confounding. This paper seems to do the translation correctly, and I'm willing to accept that it's likely technically correct.*\n2. however, a main claim of the paper is that expert recommended actions can serve / are especially good choices for the required proxies. None of us were able to articulate clearly how this part of the argument goes. (AFAICT, the real world healthcare examples don't use expert recommendations in this role)\n3. thus, it seems that there's at least a clarity problem, and potentially a technical correctness problem. Based on the discussion, this is severe enough that the paper needs a major revision. If no reviewer is able to explain the gist of the main idea, then the paper is not communicating its development clearly enough.\n\n*Although, here, I wonder if the presentation could be improved substantially simply by taking the building block as the identification result rather than the tool used for identification. That is, explain directly how to translate from such-and-such is causally identified, so the policy learning can now be achieved as such-and-such. The technical details of how proxy identification results work don't seem obviously different than how it works for identification, but they're involved enough to be distracting\n\n",
            "strength_and_weaknesses": "Strengths: this paper has a clear problem statement and exposition. I have no checked the proofs in detail, but they seem to be correct. There may be situations where the proposed scheme is useful. \n\nWeaknesses: the motivation is not totally clear to me. Particularly, the proxies for the unobserved confounder are introduced as an assumption to get around the non-identifiability of the target, expert-recommendation enhanced, policy. How realistic is this assumption? Are there particular motivating problems in mind? It would also be nice to have some experiments showing the scope of the benefits.",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "This paper is clear and (apparently) correct. While I am not fully convinced of its importance, I think it's a nice demo of the use of proxy methods in reinforcement learning, and may have some real-world applicability. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4811/Reviewer_FcTZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4811/Reviewer_FcTZ"
        ]
    },
    {
        "id": "EL4o2-CnYn",
        "original": null,
        "number": 3,
        "cdate": 1666881390316,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666881390316,
        "tmdate": 1666881390316,
        "tddate": null,
        "forum": "47C06k5D2cn",
        "replyto": "47C06k5D2cn",
        "invitation": "ICLR.cc/2023/Conference/Paper4811/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the problem of reinforcement learning in the presence of unmeasured confounders. In particular, the authors consider a setting where the data is collected according to a behavioral policy $\\pi^b: S \\times U \\rightarrow \\Delta(A)$ but the variables in $U$ are unobserved. The goal is to learn a \"super\" policy which might be history dependent and has return higher than the optimal policy that uses only the observed states in S.\n\nIn order to solve the super policy learning problem, the authors use techniques from proximal causal learning. In particular, they assume that there are proxies for both actions and rewards at each time step. If the proxies satisfy certain conditional independence assumption the expected return from a super policy can be expressed through a set of $Q$-bridge functions. These $Q$-bridge functions only depend on observable variables (states in S, and proxies W, Z) and they are estimated as the solution of a set of linear integral equations.\n\nThe authors finally provide sample complexity guarantees for learning optimal policies under confounded contextual bandits and confounded RL setting, but with memoryless unmeasured confounding. The bounds are derived assuming bounded complexity for the class of $Q$-bridge functions and their projections.",
            "strength_and_weaknesses": "Strengths:\n- I think the application of proximal causal learning in confounded RL setting is quite interesting. \n- As far as I can tell, the identification results look complete if one assumes the existence of proxy variables with desired properties.\n\nWeaknesses:\n- The paper is quite hard to follow at times. For example, the authors do not motivate the existence of proxy variables with desired properties. There is also no real-world example regarding the setting considered in section 4 (confounded RL with proxy variables for rewards and actions).\n- Perhaps the major drawback of the paper is that the proposed method is not validated on any real-world dataset. So it is hard to tell where such methods can be applied.\n- The final section on finite sample guarantees requires many assumptions. For example, it requires a bound on $p_\\max$ which is completely a property of the behavioral policy. Second, for RL setting it requires memoryless confounding. I am not sure if confounding is memoryless one still requires such complicated method for estimation. Wouldn't it be possible to just perform posterior inference on hidden variables from the observation? \n- The authors also do not discuss how to solve the sequence of linear integral equation. This problem should be hard in general and it should have been discussed. Moreover, for finite sample guarantees, one needs to solve this set of equations from finite samples and needs to argue about estimation error. However, as far as I can tell, no such error bound (in terms of samples n) was provided.\n\n\n\nQuestions for the authors:\n1. Why do you need two different types of proxy variables? Can you obtain similar results if there is a single proxy variable depending on the unobserved hidden state? \n2. I am not sure if the assumptions of multiple proxy variables and bridge functions with appropriate conditions are realistic. Can the authors provide some real-world examples where such multiple proxies are available and bridge functions satisfy the desired properties. I understand that it might not be feasible to verify some of the properties in practice, but does there exist any real-world datasets which suggest existence of such $Q$-bridge functions at every time-step?\n3. Why does the finite sample guarantee depend on the constant $p_\\max$? In general, one should expect a constant related to the overlap between the behavioral policy $\\pi^b$ and optimal policy to show up in the bound but the constant $p_\\max$ as well as $p^\\omega_{t,\\max}$ just depend on behavioral policy and can be arbitrarily large.\n4. The role of the class $\\mathcal{G}$ is not clear in theorem 5.1. Since this is just obtained from projecting the class of $q$-functions, why isn't bounding the complexity of bridge functions sufficient? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is hard to follow at times. Although the motivating example was interesting, many assumptions are introduced without proper context and should be motivated with examples.\n\nQuality: The main drawback of the paper is that the methods are not applied on any real-world dataset. It is also not clear how much of the identification results depend on prior work on proximal causal learning, and how much of it is new. Finally, the last section on finite sample guarantees makes several strong assumptions in order to obtain meaningful bounds.",
            "summary_of_the_review": "I am leaning towards rejecting the paper mainly because the presentation of the paper is not clear, and the author makes many assumptions without giving proper example. The methods are also not validated on any real-world datasets, and not clear how they compare with prior methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4811/Reviewer_ye4h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4811/Reviewer_ye4h"
        ]
    }
]