[
    {
        "id": "qY3R0RI5dpY",
        "original": null,
        "number": 1,
        "cdate": 1666279978201,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666279978201,
        "tmdate": 1666505546870,
        "tddate": null,
        "forum": "-hMNEMgT8Wd",
        "replyto": "-hMNEMgT8Wd",
        "invitation": "ICLR.cc/2023/Conference/Paper6363/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the gradient-based out-of-distribution (G-OOD) detection problem. By revisiting the current methods, the authors indicate that G-OOD algorithms can be transformed to G(x)=UV, where U is a term representing the output-based score and V indicates the feature-norm-based OOD score. To this end, this work adapts these terms by (1) setting U to the energy score; (2) setting V to the trimmed feature norm. The authors also explore an addition-based mechanism to fuse them. Experimental results show that the proposed methods largely improve the OOD detection performance. ",
            "strength_and_weaknesses": "Pros:\n\n1. The experiments are thorough and the proposed methods achieved promising results.\n\n2. The addition-based formulation is new to the OOD regime, but its effectiveness is not very surprising. \n\nCons:\n\n1. This idea is not surprising to me and the novelty is limited. First, the relationship between the gradient norm and the output/feature norm has been thoroughly analyzed by [Huang et al. 2021]. The main idea of this work is to replace the U and V terms with two widely-used OOD scores, i.e. energy scores [1] and ReAct [2]. In other words, the proposed methods are simple combinations of existing OOD scores. In effect, an ensemble of two OOD measures is quite straightforward for performance improvement. \nMoreover, the authors did not properly cite these works when adapting them to their own methods. \n\n3. I didn't find much connection between the proposed method and the gradient norm except for the U-V terms. But the title of this work is 'OOD with Reactivated Gradnorm'.\n\n3. The writing is really, really terrible, and I recommend the authors carefully proofread and polish it. And there are also some grammar mistakes and typos:\n\n    (a) 'ImageNet benchmark is not only rich in data sources, but also many categories.' => 'in categories'\n\n    (b) Both 'OOD' and 'ood' were used to represent the terminology 'out-of-distribution'",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is terrible. The methodology, as well as the theoretical analysis, are not original. The proposed method is a simple combination of existing OOD methods. ",
            "summary_of_the_review": "The writing of this work is terrible, and far from the bar of a top-tier AI conference. The novelty is limited.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6363/Reviewer_TTD6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6363/Reviewer_TTD6"
        ]
    },
    {
        "id": "A5xA6OH6tO",
        "original": null,
        "number": 2,
        "cdate": 1666387667820,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666387667820,
        "tmdate": 1666387667820,
        "tddate": null,
        "forum": "-hMNEMgT8Wd",
        "replyto": "-hMNEMgT8Wd",
        "invitation": "ICLR.cc/2023/Conference/Paper6363/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose, Reactivate Gradnorm (RG)  in feature space as well as output space for OOD.  Specifically, they multiply the 1-norm of the features and log of the exponential sum of the output.   Their motivation is from a gradient-based method.   For a trained network, if the target is uniform distribution, an in-distribution (ID) instance x will have a large gradient G(x).  If G(x) < c, some threshold, the instance is rejected as OOD .   G(x) can be decomposed into UV, where V depends on the features and U depends on the output.  They propose using an energy based score for U and clipped 1-norm for V.  They further propose a variant that uses U+V, instead of UV.  \n\nThe compared their approach (RG, the UV version) with 9 existing approaches with 4 datasets.  Their results indicate that their approach compare favorably.  For ablation study, they varied the choice of U and V and found their proposed U and V are generally more desirable.  They also perform sensitivity analysis on parameters T (temperature), k (clipping threshold).",
            "strength_and_weaknesses": "Strengths:\n\n1.  The decomposition of G, gradient, into U and V is interesting\n\n2.  Their results indicate that their approach compares favorably against existing approaches.\n\nWeaknesses:\n\n1.  Parts of the proposed approach could be explained further (details in the next section)\n\n2.  What are the principles/properties for choosing U and V besides outputs and features?  The proposed choices seem ad hoc.",
            "clarity,_quality,_novelty_and_reproducibility": "Parts of the proposed approach could be explained further:\n\nFor KL(.) under Eq. 4, 1/C seems to be missing within the log.  The decomposition of G(x) into UV below Eq. 6 can be further explained.\n\nEq. 7: T is not explained until later.\n\nSec 3.2: why $e^{-v_i^2}$ for V?  Also, why $V = \\sum(v_i,k)$ is an approximation, why not $\\sum(1-e^{-v_i^2},k)$, the exponential form?  That is, why exponential form in the first place?\n\nAbove Eq 9, what is the purpose of $g(v)$?\n\nThe decomposition of G into UV is interesting.\n\nReproducibility seems reasonable.\n",
            "summary_of_the_review": "The decomposition of G into UV is interesting.  Their results indicate their approach compares favorably.  However, the proposed choices of U and V seem ad hoc.  Also, parts of the approach could be explained further.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6363/Reviewer_b6vP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6363/Reviewer_b6vP"
        ]
    },
    {
        "id": "m1ESFyQODfS",
        "original": null,
        "number": 3,
        "cdate": 1666504411073,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666504411073,
        "tmdate": 1666504411073,
        "tddate": null,
        "forum": "-hMNEMgT8Wd",
        "replyto": "-hMNEMgT8Wd",
        "invitation": "ICLR.cc/2023/Conference/Paper6363/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Detecting OOD data is critical to build reliable machine learning systems, where the models should make reliable predictions for ID data meanwhile detecting OOD data without further predictions. It motivates the recent studies in OOD detection, which has attracted significant attentions recently. The authors propose a novel method named Reactivate Gradnorm, which exploits the norm of clipped feature vector and the energy in the output space for OOD detection. The authors conducted experiments on ImageNet, and the results demonstrate their superiority over the state-of-the-art approaches. ",
            "strength_and_weaknesses": "- The authors adopt the gradient information in discerning ID and OOD data. I also believe it is a very important line of research in OOD detection. However, to me, it is not crystal clear *why the proposed method can be superior over previous works in using gradient information*. Especially, the authors claim that the existing works still suffer from the problem of overconfidence. So, two natural questions are 1) why previous works are suffer from overconfident issue and 2) why the proposed method can mitigate such overconfidence. I think the related discussion can be critical, but I am afraid that I cannot find much useful information throughout the paper. \n\n- *The novelty of this paper is limited*. The authors followed previous works [1,2] in calculating the gradient norm of the last layer without back propagation (Eq. 6), and their choices of U and V also follow the previous works in energy scoring [3] and ReAct [4]. So, I think the authors directly combine the several advanced works in previous studies (actually, there are advanced works try to combine different scoring strategies for the improved detection capability of the model [5]), and I am not sure what makes such combination indispensable (for example, one can also change the ReAct by the L2 norm of logit features, so why the authors prefer the ReAct over other advanced scoring strategies). \n\n[1] Conor Igoe, et al. How Useful are Gradients for OOD detection Really? 2022. \n\n[2] Rui Huang, et al. One the Importance of Gradients for Detecting Distribution Shifts in the Wild. NeurIPS, 2022. \n\n[3] Weitang Liu, et al. Energy-based Out-of-distribution Detection. NeurIPS, 2020. \n\n[4] Yiyou Sun, et al. ReAct: Out-of-distribution Detection with Rectified Activations. NeurIPS, 2021. \n\n[5] Hihoon Tack, et al. CSI: Novelty Detection via Contrastive Learning on DIstributionally Shifted Instances. NeurIPS, 2020. \n\n- The paper is motivated by the GradNorm, but *the finally adopted method does not involve much about the gradient information*. So, I am not sure if the addition-based combination can benefit from the gradient information of the model. It will be great if the authors could formally connect the addition-based combination and the GradNorm in math language. \n\n- The authors conduct experiments on ImageNet benchmark, which is a challenging setup with large semantic space and very complex data features. However, *the comparison with some advanced methods (e.g., [6,7]) and the experiments on Hard OOD detection (e.g., CIFAR-10 vs. CIFAR-100, cf., [6]) are missing*. To fully justify the effectiveness of the proposal, I am afraid the authors should conduct more experiments. \n\n[6] Yiyou Sun, et al., Out-of-distribution Detection with Deep Nearest Neighbors. ICML, 2022. \n\n[7] Xuefeng Du, et al. VOS: Learning What You Don't Know by Virtual Outlier Synthesis. ICLR, 2022. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and the clarity of writing are satisfactory. The Novelty may be limited to some extent. Moreover, I did not check the reproducibility. ",
            "summary_of_the_review": "Using gradient information in OOD detection is an interesting direction. However, to improve the quality of the paper, I think the authors can address my concerns. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6363/Reviewer_7gKP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6363/Reviewer_7gKP"
        ]
    },
    {
        "id": "vkcbgCKLLP",
        "original": null,
        "number": 4,
        "cdate": 1666529791204,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529791204,
        "tmdate": 1666529791204,
        "tddate": null,
        "forum": "-hMNEMgT8Wd",
        "replyto": "-hMNEMgT8Wd",
        "invitation": "ICLR.cc/2023/Conference/Paper6363/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors revisit gradient-based OOD detection from the perspective of backpropagation and extend the decomposition G(x)=UV of GradNorm to more loss functions. Here G is a gradient-based detection score, V is the feature norm and U represents output information. According to this decomposition, the authors suggest exploiting suitable U and V for OOD detection. They take U as the energy-based score and derive a V measurement by assuming OOD features follow a standard Gaussian distribution. Experiments on four ImageNet benchmarks demonstrate the effect of the proposed method and discuss the impact of hyperparameters.",
            "strength_and_weaknesses": "Strength:\n\n- This work proposes a new idea to use the decomposition of gradient-based OOD detection.\n- The proposed method is simple and easy to implement.\n- The empirical result in Table 7 is good.\n\nWeakness:\n\n- The proposed method is not well-motivated.\n- The novelty of the proposed method is unclear.\n- The content is poorly organized.\n- This work lacks a hyperparameter selection method based on ID data.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty and Quality are fair. This work contributes some new ideas. It has minor technical flaws and some typos. The errors are fixable. The clarity is poor. The content should be carefully reorganized.",
            "summary_of_the_review": "I list my main questions in this section.\n\n1. In the abstract, the motivation is 'However, existing works still suffer from the problem of overconfidence'. Why the proposed score in (8) can overcome the problem? Could you provide more analysis and comparisons to GradNorm and other OOD detection scores?\n2. In Section 3.1, does \"ground truth\" mean ground truth for classification or ground truth for OOD detection? Can you point out a loss function that corresponds to your proposed score in (8)?\n3. Can we understand the proposed score as an enhancement for the energy-based score? Why did you name it \"Reactivate Gradnorm\"? Is it because the proposed score follows the decomposition G(x)=UV?\n4. What is the purpose of introducing Section 3.3?\n5. The main result with ResNetv2-101 (Table 1) is not as good as the result with ResNet 50 (Table 7). Why use different pre-trained models in this section? In Table 7, is it unfair to compare Reactivate GradNorm to KNN? Would it be more convincing to compare Reactivate GradNorm with KNN+ReAct? \n6. The OOD detection task in this work is a one-sample hypothesis testing problem, i.e., only ID data is accessible. Therefore, the hyperparameters in your score should be determined by the ID data. Table 3 and Table 4 only consider one ID data.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6363/Reviewer_diYs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6363/Reviewer_diYs"
        ]
    }
]