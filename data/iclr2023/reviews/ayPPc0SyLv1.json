[
    {
        "id": "cl9QvYbt-r",
        "original": null,
        "number": 1,
        "cdate": 1666544682960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666544682960,
        "tmdate": 1666544682960,
        "tddate": null,
        "forum": "ayPPc0SyLv1",
        "replyto": "ayPPc0SyLv1",
        "invitation": "ICLR.cc/2023/Conference/Paper4948/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a simple architecture named GraphMixer for temporal graph learning. The GraphMixer is based on simple MLP structure and achieves state-of-the-art performances on temporal link prediction benchmarks.",
            "strength_and_weaknesses": "First of all, I'm not the expert in the graph learning area and my research focus is computer vision. From my side, I would to say: A simple method along with state-of-the-art performances satisfies me. \n\nWhen I read this paper, I'm satisfied by the simple method, the detailed experiments. Thus I would like to give an accept rating.\n\nHowever, there is still a little bit concerns:\n\n1. I think the major novelty lies in the time-encoding part, also the proposed GraphMixer performs extremely better on LastFM dataset (c.f., Table 2). Could the author give explanations of why GraphMixer work well on LastFM? Since I only observe marginal improvements on other datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "Though I am not the expert in graph learning area, I enjoyed reading this paper. I would like to give this paper an borderline accept rating. I think a simple but effective method satisfied me.",
            "summary_of_the_review": "I think this paper is a simple and effective paper, thus I want to give it an accept rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4948/Reviewer_7yc7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4948/Reviewer_7yc7"
        ]
    },
    {
        "id": "nA7TPc5fns1",
        "original": null,
        "number": 2,
        "cdate": 1666673922751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673922751,
        "tmdate": 1666673922751,
        "tddate": null,
        "forum": "ayPPc0SyLv1",
        "replyto": "ayPPc0SyLv1",
        "invitation": "ICLR.cc/2023/Conference/Paper4948/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors propose GraphMixer, a conceptually and technically simple architecture that consists of three components: 1 a link-encoder that is only based on multi-layer perceptions (MLP) to summarize the information from temporal links, 2 a node-encoder that\nis only based on neighbor mean-pooling to summarize node information, and 3 an MLP-based link classifier that performs link prediction based on the outputs of the encoders. The authors show that GraphMixer attains an outstanding performance on temporal link prediction benchmarks with faster convergence and better generalization performance.",
            "strength_and_weaknesses": "Pros: Focused on the temporal graph learning, the authors propose a conceptually and technically simple architecture GraphMixer for temporal link prediction. GraphMixer not only outperforms all baselines but also enjoys a faster convergence speed and better generalization ability. The author conduct empirical study to identify three key factors that contribute to the success of GraphMixer and highlights the importance of simpler neural architecture and input data structure.\n\nCons: There is not obvious weakness in the draft.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, novel, and easy to reproduce.",
            "summary_of_the_review": "Please refer to the above points.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4948/Reviewer_naoZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4948/Reviewer_naoZ"
        ]
    },
    {
        "id": "MxRCxR--YO6",
        "original": null,
        "number": 3,
        "cdate": 1667190536650,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667190536650,
        "tmdate": 1669007280055,
        "tddate": null,
        "forum": "ayPPc0SyLv1",
        "replyto": "ayPPc0SyLv1",
        "invitation": "ICLR.cc/2023/Conference/Paper4948/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, they carefully design and propose the GraphMixer network for temporal link prediction in dynamic graphs. With a carefully designed and ablate architecture separated into three components, the link-encoder, node-encoder, and MLP-Mixer based link classifier, they are able to achieve high performance on this task that has only previously been attained with RNN and Self-Attention based architectures. They include extensive experiments to study the different architecture choices they made and their effect on performance and generalization.",
            "strength_and_weaknesses": "Strengths:\n- Proposes a novel architecture that achieves performance competitive with other networks without memory or attention mechanisms.\n- Strong analysis and ablation of the different design choices made.\n\nWeaknesses:\n- The model takes significantly longer to train and there is not significant analysis of the inference cost. This lack of discussion in the main paper also makes some conclusions about faster convergence somewhat misleading.\n\nQuestions:\nWhy are the train average precisions for the competing methods in Figure 3 so much lower than the final results in Table 1. For example, DySAT training AP is around 60-70% while the final eval is 96.71. Are all the results from reimplementations or from taken from other papers.\n\nIt would likely be helpful to bring appendix C.3 into the paper and analyze and discuss more about the model efficiency since the model does take significantly longer to train than some baselines like TGN. If some of this is due to preprocessing, it would be helpful to separate the time required for each and/or analyze the inference FLOPs for your model compared to the others.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is mostly clear and understandable. Some of the claims around model convergence is a bit misleading and the authors should consider bringing appendix on training speed C.3 into the main paper.",
            "summary_of_the_review": "This work proposes a novel architecture for link prediction in temporal graph networks that achieves very good performance without RNN or self-attention mechanisms. They achieve this using a carefully constructed network with MLP-mixer. They do detailed experiments to analyze and ablate the different design choices they made. A large strength of this paper is the extensive experiments ablating the details of the design choices they made how it affects performance, generalization, and training stability. This greatly helps the community understand how these architecture choices affected their model and may extend to other models. The paper may have somewhat limited novelty since the different parts of the design have all been explored elsewhere and in terms of setting a new SOTA, the model still has some under-analyzed weaknesses such as the slow training speed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4948/Reviewer_xY6D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4948/Reviewer_xY6D"
        ]
    }
]