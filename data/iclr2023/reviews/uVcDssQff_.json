[
    {
        "id": "ezT5zJ8u2NV",
        "original": null,
        "number": 1,
        "cdate": 1666625689861,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625689861,
        "tmdate": 1669618680721,
        "tddate": null,
        "forum": "uVcDssQff_",
        "replyto": "uVcDssQff_",
        "invitation": "ICLR.cc/2023/Conference/Paper2880/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "It is believed there should be a strong correlation between the Ramanujan graph and PaI since both are about finding sparse and well-connected neural networks. This paper digs deep and rigorously establishes their non-trivial relationships in fine-granularity, e.g., ranking of difference sparse structures at the same specific global sparsity.",
            "strength_and_weaknesses": "Strength\n1.\tThis paper went beyond intuition and builds a new mathematical framework for understanding PaI. The differences from previous PaI studies (gradient- and graph-based) are clearly elaborated.\n2.\tThis paper also went beyond simply pulling off-the-shelf Ramanujan graph metrics to analyze DNN architectures (like prior art did). Instead it did several important modifications tailored for irregular bi-graphs and high sparsity levels, both are demanded in the PaI situation.\n3.\tThe authors are the first to identify a crucial limitation of Ramanujan property in analyzing irregular graphs, i.e., we could dismiss valid expanders due to the overly limiting requirements while retaining only graphs with random sparse-graph structures when analyzing high sparsity. They proposed a new metric called Iterative Mean Difference of Bound (IMDB) to mitigate the gap. This finding could be of independent interest to other applications where Ramanujan graph is relevant.\n4.\tThe authors also highlight the danger of being overly expansive, which risks graphs deteriorating into randomness. They proposed a lower constraint to avoid the danger of randomness.\n5.\tOverall, the authors show the key knob to be relaxing the strong upper bound constraint on the third large affinity matrix eigenvalue \u00b5(G), such that the resulting graph\u2019s connectivity correlates strongly with its final performance. Further checking the lower bound for \u00b5(G) could indicate whether a sparse structure deteriorates into randomness. Experiments align well with their theories.\n\nWeakness\n1.\tI did not follow what differences the authors were referring to, between \u201ctrivial random sparsity\u201d and \u201cstructurally meaningful masks\u201d. What\u2019s the criteria?\n2.\tExperiments are relatively limited. For example, only CIFAR-10 dataset is used as the testbed. Will the observations scale up to larger datasets? Also, could the authors find out whether some PaI methods can be obvious winners under their proposed metrics?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The novelty is listed above. Experiments are straightforward (proof-of-concept level) and should be easy to reproduce. ",
            "summary_of_the_review": "See the strength and Weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2880/Reviewer_pUbu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2880/Reviewer_pUbu"
        ]
    },
    {
        "id": "-gDJXlOYQA",
        "original": null,
        "number": 2,
        "cdate": 1666662396738,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662396738,
        "tmdate": 1666662396738,
        "tddate": null,
        "forum": "uVcDssQff_",
        "replyto": "uVcDssQff_",
        "invitation": "ICLR.cc/2023/Conference/Paper2880/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to interpret the (un)success of Pruning neural networks at initialization (PaI). through the lens of the Ramanujan Graph - a class of expander graphs that are sparse while being highly connected. Despite the intuitive belief that PaI and Ramanujan Graph should be naturally related, their underlying relationships are shown to be more sophisticated and non-trivial as the authors analyzed. The authors supposed their analysis with ample experiments. ",
            "strength_and_weaknesses": "\nStrength\n-\tMost prior works on PaI focus on \u201ctraining signals\u201d such as gradients. This paper studies a complementary angle of graph theory\n-\tAlthough there exist prior work connecting Ramanujan Graph and PaI, they did not consider the pseudo-randomness and irregular bi-graphs in practical sparse NNs, and may prefer \u201ccollapsed\u201d na\u00efve random graphs (overly expansive)\n-\tThe authors rigorously analyzed the limitation of Ramanujan Graph in modeling irregular graphs (practical DNNs) at high sparsity, and propose to relax the assumption on the eigenvalue upperbound\n-\tThe authors further proposed another metric to detect and avoid \u201ctrivial randomness\u201d at high sparsity, by characterizing the same eigenvalue\u2019s lower bound. \n-\tTogether, the authors demonstrate the novel finding, that a valid and desirable sparse NN architecture only exists when its graph adjacency matrix satisfies certain spectrum condition, e.g., its third-largest eigenvalue is both upper- and lower-bounded properly\n-\tThe authors experimentally validated their findings on SOTA PaI methods including ERK, SNIP, and SynFlow. \n\n\nWeakness\n-\tThe paper does not construct any new PaI approach, only observing how PaIs are correlated with their graph properties. It would be great if the authors could briefly share future insights of improvements.\n-\tOnly CIFAR-10 dataset is used in experiments. I\u2019d suggest the authors to report on 1-2 more datasets in order to make their empirical evidences more consistently convincing. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: very well written and easy to follow\nNovelty: high (spotting important issues of Ramanujan Graph in analyzing PaI, which were overlooked before)\nReproducibility: seems good, but no code was submitted.\n",
            "summary_of_the_review": "Overall, I would tend to accept this paper. If the author can address my concerns. I would be more convinced.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2880/Reviewer_wCJu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2880/Reviewer_wCJu"
        ]
    },
    {
        "id": "zg5MHpgvDJl",
        "original": null,
        "number": 3,
        "cdate": 1666679324776,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679324776,
        "tmdate": 1668993904151,
        "tddate": null,
        "forum": "uVcDssQff_",
        "replyto": "uVcDssQff_",
        "invitation": "ICLR.cc/2023/Conference/Paper2880/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method to prune neural networks at initialization. By analogy to previous work which used sparse random subnetworks to speed up training, this paper instead uses Ramanjuan graphs, which are well-studied and known to be favorable with respect to expansion. It relaxes the eigenvalue condition of Ramanujan graphs to a new property (IMDB) which empirically correlates to increased performance when the network is trained on image classification tasks.",
            "strength_and_weaknesses": "General comments\n-\n\n- Why use the third largest eigenvalue of the adjacency matrix? Usually the spectral gap is taken to be the difference between the first and second largest eigenvalues, which is guaranteed to be positive if the graph is connected.\n\nStrengths\n-\n\n- The idea of using expander graphs to guide pruning is interesting and unifies existing work.\n\nWeaknesses\n-\n\n- The decision to not use existing measures of spectral expansion is not entirely convincing. You mention that the Ramanujan condition is too pessimistic for non-regular graphs when $d_{max}$ is used, but why not use something like the second eigenvalue of the normalized Laplacian?\n- The decision to pass to all $K$-regular subgraphs seems somewhat arbitrary and possibly expensive to compute. \n- The pruning measure proposed does not leverage the weights of the network at initialization, since it does not sparsify the network based \n on sensitivity of the loss function.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper provides a new perspective on pruning using the theory of spectral expanders, which are more general than simply using random graphs. The writing in some parts is confusing. For example, Lemma 1 reads \"The value of $\\mu_0$ for any adjacency matrix is said to be $d_{avg} \\leq \\mu_0 \\leq d_{max}$.\" This looks like it's stating a definition even though it is part of the lemma statement, and there is no separation between the lemma statement and its proof.",
            "summary_of_the_review": "The idea of measuring performance of pruning in terms of spectral properties of the adjacency matrix is a good idea. However, the choices made in deciding upon the IMDB measure seem arbitrary in comparison to existing measures of spectral expansion. The choice of comparing to $\\delta r$ is not entirely fair as a baseline, since that particular bound was designed around regular graphs.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2880/Reviewer_Bf71"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2880/Reviewer_Bf71"
        ]
    }
]