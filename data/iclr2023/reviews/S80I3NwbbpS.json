[
    {
        "id": "Ji2LSc99l8",
        "original": null,
        "number": 1,
        "cdate": 1666462184420,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666462184420,
        "tmdate": 1668633585174,
        "tddate": null,
        "forum": "S80I3NwbbpS",
        "replyto": "S80I3NwbbpS",
        "invitation": "ICLR.cc/2023/Conference/Paper1790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents CAB, a new benchmark to test Transformers and other sequence models on long sequence data. The paper benchmarks a number of methods as a baseline.",
            "strength_and_weaknesses": "+ Fills in a missing gap in the literature beyond LRA\n+ Will be useful for researchers evaluating long-sequence models\n\n- There appear to be a few basic mistakes in the methods characterization\n- It would be nice to have a few more baselines\n- Paper could use some copy edits",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clear to follow, but could use some copy editing. I think the focus on distinguishing self-attention from causal/cross/etc attention is a little bit misplaced.\n\nQuality: I think the most useful contribution of this paper is simply aggregating more long-sequence benchmarks as \"standard.\" The categorization of methods into self-attention, causal, cross, etc seems to have some basic mistakes.\n\nFor example, S4D is inherently a causal model (this is the definition of a state space model - it models how a state changes through time). The \"bidirectional\" or noncausal version of S4D is a modification on top. I am not as familiar with the details of the other methods, but this category error makes me concerned.\n\nFor a benchmark paper, it would be nice to see more baselines for exact attention, such as FlashAttention.\n\nOne question: which tokenizer do you use for PG-19? Making that clear up front would be helpful (it is one of the reasons that existing work is a little hard to compare).\n\nNovelty: I think this type of benchmark has been missing from the literature, so it is welcome. However, the analysis and evaluation on top are a little bit lacking.\n\nReproducibility: Seems clear.",
            "summary_of_the_review": "I think this paper could make a good contribution to the literature, but the paper has a few basic mistakes that should be corrected. If my concerns are addressed in the rebuttal, I'm happy to raise my score above the acceptance threshold.\n\nUpdate after rebuttal: my concerns have been addressed, so I am increasing my score to a 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1790/Reviewer_VyuY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1790/Reviewer_VyuY"
        ]
    },
    {
        "id": "5gF7j5i8Joi",
        "original": null,
        "number": 2,
        "cdate": 1666544366305,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666544366305,
        "tmdate": 1666544366305,
        "tddate": null,
        "forum": "S80I3NwbbpS",
        "replyto": "S80I3NwbbpS",
        "invitation": "ICLR.cc/2023/Conference/Paper1790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions.\n",
            "strength_and_weaknesses": "Strength\n1. the paper is well written and feeling pleasure to read. It summarize many eixting of Transformer.\n\n2. The authors propose Comprehensive Attention Benchmark for ling sequence modeling.\n\nWeaknessness\n1. the novelty of this work is not enough for ICLR. It seems that this paper is more related to benchmark dataset.\n\n2. They claim that  related codes will be released at https://github.com/Anonymous. However, I cannot find it.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well written and it is clear.\n\nQuality: The quality of this work is good.\n\nNovelty: The novelty is limited.\n\nReproducibility: unclear at this time.",
            "summary_of_the_review": "This paper proposes to evaluate efficient attentions under a more fine-grained attention taxonomy\nwith four distinguishable attention patterns, each of which reflects different attentive functionality.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1790/Reviewer_MeQL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1790/Reviewer_MeQL"
        ]
    },
    {
        "id": "oZJ2t-PG7p",
        "original": null,
        "number": 3,
        "cdate": 1666685603875,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685603875,
        "tmdate": 1666685603875,
        "tddate": null,
        "forum": "S80I3NwbbpS",
        "replyto": "S80I3NwbbpS",
        "invitation": "ICLR.cc/2023/Conference/Paper1790/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": " The authors propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. The benchmark collects seven real-world tasks from diverse fields of computer vision, natural language processing, speech processing, and time series forecasting. And an interesting finding is that some efficient Transformers often achieve less competitive results in the causal cross scenario.",
            "strength_and_weaknesses": "Strength:\n1. A benchmark to evaluate different types of attention patterns for the task with long sequences\n2. The benchmark covers a broad number of tasks.\n\nWeakness:\n1. The number of baselines in figures b,c,d are limited. It would be better to list more.\n2. LongT5, \"Efficient Text-To-Text Transformer for Long Sequences\", also works on the causal attention mechanism. All the datasets in LongT5 could also be a good benchmark. Language model is also a widely used task to evaluate causal attention. Thus, the benchmark may not be novel enough if only considering the NLP area.",
            "clarity,_quality,_novelty_and_reproducibility": "Can you list the SOTA results with model pertaining for NLP tasks?",
            "summary_of_the_review": "The benchmark is interesting and covers many tasks from different domains. The exploration of different attention mechanism is interesting. My major concern is that there has been benchmarks for NLP tasks. After all, I would like to see people working on this new benchmark.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1790/Reviewer_kTjS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1790/Reviewer_kTjS"
        ]
    },
    {
        "id": "FXXSNzW03Q",
        "original": null,
        "number": 4,
        "cdate": 1666892125909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666892125909,
        "tmdate": 1666892125909,
        "tddate": null,
        "forum": "S80I3NwbbpS",
        "replyto": "S80I3NwbbpS",
        "invitation": "ICLR.cc/2023/Conference/Paper1790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel benchmark, called CAB,  to compare the efficiency of different attention mechanisms in transformers. The current widely used benchmark LRA focusses only on noncausal self-attention. CAB seeks to evaluate four types of attention mechanisms: the combinations of causal/ non-causal and self/ cross attention. The benchmark consists of nine datasets for seven real-world tasks across vision, NLP, speech and time-series forecasting. CAB  is also used to evaluate 8 baseline models covering the four attention mechanisms using standard statistical performance metrics and a proposed composite index as well their efficiency \u2013 relative running times and memory usage as a function of the sequence length, highlighting the strengths and weakness of efficient approach across the four attention types suggesting where more research is needed.\n",
            "strength_and_weaknesses": "**Strengths**\n\n- CAB is a novel benchmark with a finer granularity of attention taxonomy than any existing benchmark. LRA addresses only one of the 4 types of attention mechanisms.\n-  CAB comprises 7 tasks of interest to the research community on 9 datasets being used by the research community across 4 domains.\n- The paper also evaluates the performance of 8 popular models as baselines as a part of CAB for new approaches to be benchmarked against.\n- The paper further identifies the scenarios where efficient approaches are significantly lagging behind vanilla attention in performance as the unmet need for the research community to focus on.\n\n**Weaknesses**\n\n***Analysis of Results*** \n- The paper should provide more insights on why results from certain methods vary across various studies: local attention in (Xiong et al, 2022) vs LRA; S4D on LRA vs Sum, MLM tasks, etc.\n- (Figure 3) In the \u2018self\u2019 regime, why is it that performance holds across LRA and NS but drops (in correlation) for causal-self? \n- (Figure 3) There is a clear blockiness along the diagonals indicating high positive correlation in performance of approaches within the \u2018self\u2019 scenarios (LRA, non-causal and causal) and within the \u2018cross\u2019 scenarios (non-causal as well as causal). What may be the reasons for high negative correlation in the off-diagonal blocks? In other words, can you kindly elaborate on footnote 4? Also, what explorations need to be done beyond this benchmark?\n- It is not clear what \u2018non-homologous information\u2019 is. Kindly provide explanation and a reference for the term.\n\n***Clarity***\n- Figures showing the tradeoff between performance (accuracy) and cost (time, memory) are recommended to communicate the tradeoffs better.\n- Writing should be improved in Sections 4 and 5. The authors should avoid using words/ phrases like \u2018utterly defeated\u2019, \u2018embarrasingly\u2019,  etc. \n- Appropriate capitalization should be used in the \u2018References\u2019 section.\n- Some easily fixable typos.\n",
            "clarity,_quality,_novelty_and_reproducibility": "***Clarity***: The paper is mostly clear and easy to read.\n\n***Quality***: The overall quality of the submission is acceptable.\n\n***Novelty***: The benchmark is novel, more comprehensive that the previous benchmarks and would be useful to compare future approaches and help spur research into efficient approaches.\n\n***Reproducibility***:  Should not be a concern as the authors have promised that all the code will be released on github. \n",
            "summary_of_the_review": "The paper introduces a new benchmark for comparing and evaluating efficient attention mechanisms. I expect the benchmark to be useful to the research community and is likely to help in guiding research into the design of novel efficient attention mechanisms, especially in the causal and cross-modal domains. There is an extensive benchmarking of exemplary baseline algorithms. The paper is mostly written well (though the writing, clarity and the discussions can be somewhat improved).\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1790/Reviewer_auuz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1790/Reviewer_auuz"
        ]
    }
]