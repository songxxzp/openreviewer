[
    {
        "id": "GIpOKzK8_MP",
        "original": null,
        "number": 1,
        "cdate": 1666391175316,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666391175316,
        "tmdate": 1666391175316,
        "tddate": null,
        "forum": "s0ceCGfcIKb",
        "replyto": "s0ceCGfcIKb",
        "invitation": "ICLR.cc/2023/Conference/Paper5738/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis paper investigates the role of gradient-based methods in OOD detection. Specifically, the author tries to answer the following questions and provide answers correspondingly: \n\n1. Is it essential to use the scoring function strictly derived from the gradient?\nNo. The recombination of different components also works fine.\n\n2. How do we align the feature-extraction hypothesis with the strong performance of gradient-based OOD detection methods?\nFeature-Extraction Hypothesis is not an appropriate explanation.\n\n3. Does gradient-based approaches offer unique performance advantages for post hoc OOD detection?\nNo. Essentially the same reason as question 1. ",
            "strength_and_weaknesses": "\nStrength: \n\n1. This is the first paper systematically explore the role of different components for gradient-based methods in OOD detection. \n2. Some new insights are provided like the V component in GradNorm can be regarded as total variation and the V component in ExGrad can be regarded as the variance of a Bernoulli variable.\n3. The related work is extensive. \n\nWeakness: \n\n1. The paper does not offer a finding that is interesting and surprising enough to readers. Several observations are listed in a scattered way, which also makes the paper distractive. For example:\n\n\ta) The author finds that |h|_1 \u00d7 ENERGY is a strictly better detector than GRADNORM. My feeling is that you can do any recombinations of different OOD scoring functions and find a specific combination (B + D) that performs specifically well. However, such a finding is very likely due to an \"overfitting\" of the test setup. As evidence, |h|_1 \u00d7 ENERGY does not perform well on small-scale settings as the author showed. \n\n\tb) The author claims that a strict derivation from the gradient form is not needed. That is true, but not surprising. The GradNorm is simply a combination of U and V as pointed out in the GradNorm paper. Both U and V have the discriminative power for ID vs OOD. You can change U or V to other discriminative scoring functions and can still work. A more interesting question is that why U and V synthetically work better than U and V alone. \n\n\tc) The author claims that the Feature-Extraction Hypothesis is not applicable to the explanation of GradNorm. That is also understandable because they are in nature, not conflict. GradNorm is working based on the assumption for input sample x alone: OOD data are closer to a more flat probability space. Feature-Extraction Hypothesis is applied on both input x and label y: if you offer the input sample with a wrong label, the gradient will be large. I don't see why it is essentially counterintuitive. \n\n2. The writing needs to be improved especially in Section 3 and Section 4. \n",
            "clarity,_quality,_novelty_and_reproducibility": "See Strength And Weaknesses. ",
            "summary_of_the_review": "The paper is performing an extensive and in-depth study of the gradient-based method. However, I don't share the excitement upon reading the finding provided in the paper, since I find the observation listed is common and not surprising. Therefore I lean toward rejecting the paper at this point. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5738/Reviewer_RRxz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5738/Reviewer_RRxz"
        ]
    },
    {
        "id": "M-W2QRd1TTw",
        "original": null,
        "number": 2,
        "cdate": 1666569417917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666569417917,
        "tmdate": 1669136830896,
        "tddate": null,
        "forum": "s0ceCGfcIKb",
        "replyto": "s0ceCGfcIKb",
        "invitation": "ICLR.cc/2023/Conference/Paper5738/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission primarily aims at refuting ideas expressed in the NeurIPS21 paper \u201cOn the importance of gradients for detecting distributional shifts in the wild\u201d (Huang et al.), by suggesting that gradients are actually not intrinsically required for the high performance in that paper; it\u2019s more to do with the nature of the contributing terms once the GradNorm score is viewed with the decomposition in Huang et al. While this analysis was already made in Huang et al., the submission proposes variants of the individual terms, showcasing improvements over GradNorm with alternative choices.",
            "strength_and_weaknesses": "Strengths:\n\nHuang et al. had discussed how the gradient-based method might work, through a product of two terms that are to do with the predictive distribution and the latent encoding separately. The submission performs a fairly detailed analysis of possible alternatives to these terms, not necessarily decomposed from a gradient-based score. The conclusions are sensible \u2014 gradient-based methods don\u2019t seem essential to high performance, and alternatives can be formed with different heuristic choices for an OOD score. The paper is very well-written, which made it a pleasure to read.\n\nWeaknesses:\n\nMy main comment is about relevance and significance to ICLR. The submission claims that GradNorm is SOTA at the time of writing, which would justify investing effort in rigorously analyzing the method, exposing limitations, and suggesting alternatives. However, there seem to be at least a couple other papers with equally good and better numbers around, for example, [1, 2, 3], all published reasonably earlier than the ICLR deadline (not all of them report all datasets in the submission, but without an evaluation, one cannot disregard them). These methods are not gradient-based, and seem to be well-aware that gradient-based methods aren\u2019t the best way to go. In general, the narrative in the submission makes it appear as if the community is currently of the widespread opinion that gradient-based methods are top-choice, which needs to be corrected. In my view, past and contemporary literature does not indicate this, thus making the contributions of this submission less significant. There are, to my knowledge, only the two works of note (in the ocean of papers on OOD detection) that use gradients \u2014 Huang et al. (2021), and Lee and alRegib (2020). Lee and alRegib has been discussed at length in Huang et al., in particular the point that training a binary classifier on the test-distributions is poor methodology, and this paper is generally not well-cited \u2014 I only mention this to counter the narrative in the submission that there is an \u201cemerging theory\u201d in the literature that needs challenging (at a top-tier venue), with substantial portions dedicated to debunking the \u201cfeature-extraction hypothesis\u201d. In my opinion, this sort of challenge is best-suited for a \u201cLetter to the Editor\u201d section, as in other scientific journals; unfortunately, our field does not seem to have allocated space for similar back-and-forth between author and challenger. \n\nGiven that no method is universally better, one cannot really draw any strong conclusions. Using a network\u2019s \u201cproperties\u201d (activation strengths and predictive distributions) implies that results are strongly dependent on the network and training choices. Using different underlying choices might change trends significantly. This is not a problem unique to this submission, but it somewhat comes in the way of the sort of analysis it seeks to provide: reporting numbers for several different modifications of the score might be reporting results that are \u201coverfitted\u201d on the specific underlying frameworks. The fact that the new alternative, ExGrad (and VarSum variants) underperform on the ImageNet benchmarks also hurts what might have been a positive point for technical novelty.\n\n\n[1] ReAct: OOD Detection with Rectified Activations, NeurIPS 2021\n\n[2] ViM: Out-Of-Distribution with Virtual-logit Matching, CVPR 2022\n\n[3] Out-of-Distribution Detection with Deep Nearest Neighbors, ICML 2022",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing and exposition is very clear.\n\nQuality: In my view, the significance of this work does not meet the bar for ICLR.\n\nNovelty: There is no significant novelty in the paper; the choices for U and V, and norm-orderings are largely heuristic, and derived from existing intuitions.\n\nReproducibility: There are adequate details in the submission for meaningful reproducibility.",
            "summary_of_the_review": "The submissions takes a thorough look at GradNorm, following up on the initial analysis provided in Huang et al. about the benefits lying in the separated components of the score. However, in my view, the analysis and main takeaway (that gradient-based scores are not a requisite for high performance; and one can look for other ways to develop OOD-scores using activations and predictive distributions) are less relevant given the plethora of high-performing non-gradient based methods that already take such messages to heart. Therefore, my recommendation at this time is rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5738/Reviewer_ipBL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5738/Reviewer_ipBL"
        ]
    },
    {
        "id": "AMll9Tl9La",
        "original": null,
        "number": 3,
        "cdate": 1666602274853,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602274853,
        "tmdate": 1669825185521,
        "tddate": null,
        "forum": "s0ceCGfcIKb",
        "replyto": "s0ceCGfcIKb",
        "invitation": "ICLR.cc/2023/Conference/Paper5738/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper questions a recent assumption in the OoD detection literature: that gradient information is useful for OoD detection. Specifically, the authors break down the norm of the gradient as 'U * V' (following the GradNorm paper, a SoTA OoD detection method), where U represents the feature norm and V is some function of the output layer.\n\nThe authors conduct both theoretical and empirical analysis under this framework to conclude that the gradient information provides little extra information that could not be extracted in another way. In particular, they find that the feature norm plays an important role in the high performance of the GradNorm method. They also develop a new scoring rule, ExGrad, which empirically outperforms GradNorm on small-scale OoD detection benchmarks, and provide analysis on both large and small-scale OoD detection benchmarks.",
            "strength_and_weaknesses": "Overall, I enjoyed reading this paper (though not all parts were entirely clear to me). This paper provides a clear and focussed investigation into the question posed in the title, which I believe is useful for researchers and practitioners in the OoD detection community. \n\nStrengths: \n* This paper provides an in-depth analysis of GradNorm and related gradient-based OoD detection methods, which have recently gained traction in the OoD community. The paper analyses GradNorm (a SoTA OoD detection method) and its variants through both an empirical and theoretical analysis to draw conclusions. They find that many choices of the 'V' term could be equally useful for identifying OoD samples, even ones that are not explicitly related to a gradient computation.\n* The authors investigate a 'Feature Extraction Hypothesis', an intuitive assumption that the gradient of the cross-entropy loss (with a uniform label) for an OoD sample would high, as a large change in the features is required to model the OoD sample. They authors conduct empirical and theoretical analysis and find evidence *against* this claim.\n* The authors show a promising direction forward for OoD detection, which focusses on the norm of the extracted features. Though tuning on the test set, the authors show that SoTA OoD detection on the ImageNet-scale evaluation can be achieved simply by tuning the P-norm of the extracted features, and combining with the Energy scoring rule.\n\nWeaknesses:\n* The authors heavily use Energy as scoring rule in the 'V' term, implicitly treating it as independent of the feature norm in U. However, the energy function is itself impacted by the feature norm. This should be discussed.\n* Perhaps I have misunderstood, but the authors highlight the opposite intuitions behind GradNorm and the Feature Extraction Hypothesis. Specifically, one suggests that the gradient norm should be high for ID examples, while the other says it should be high for OoD samples. If this is the case, should one of them not perform very poorly for OoD detection (AUROC <<50, given that they are anti-correlated)? If this is the case, is the Feature Extraction hypothesis not trivially verifiable or falsifiable? Perhaps the authors could clarify.\n\nMisc:\n* [1] discusses the role of the feature norm in open-set recognition and OoD detection. They provide intuitions as to why low-feature norms may occur for OoD samples, and further suggest the maximum logit score (MLS) as an alternative to MSP for open-set scoring (as the former is sensitive to feature norm). The authors should discuss this. \n* The authors have used different parameters with respect to which to take the gradient in Eqs 2 and 3. This makes things less clear, so authors should either change this or discuss the difference. \n\n[1] Open-set Recognition: a Good Closed-Set Classifier is All You Need?, Vaze et al., ICLR 22",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I found the paper enjoyable to read and found the argument easy to follow, with the paper well-structured. I have highlighted a few aspects above which I believe could do with some clarification.",
            "summary_of_the_review": "This is a good paper which provides clear and focussed analysis into a narrow research question. I think it will be useful to the OoD detection community.\n\nUPDATE AFTER AUTHOR RESPONSE:\n\nAfter reading the authors' responses to mine and other reviewers, I re-iterate my initial stance that this is a good paper which would be valuable to the community. I believe that focussed investigation on existing research trends, as carried out in this paper, is just as valuable as the proposals of novel methods. \n\nI maintain my rating of '8, Good Paper'.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5738/Reviewer_YM1z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5738/Reviewer_YM1z"
        ]
    },
    {
        "id": "cLSKiN4gWuo",
        "original": null,
        "number": 4,
        "cdate": 1666663959735,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663959735,
        "tmdate": 1666663959735,
        "tddate": null,
        "forum": "s0ceCGfcIKb",
        "replyto": "s0ceCGfcIKb",
        "invitation": "ICLR.cc/2023/Conference/Paper5738/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper evaluates different methods for OOD detection which use and do not use gradient information. They decompose several methods as in [1] into two components related to the encoding and output probability vectors. \nThrough their decompositions they propose different OOD detection methods not based on gradients which outperform benchmarks such as grad norm [1] in some settings.\n\n[1] Huang et al. 2021",
            "strength_and_weaknesses": "\n- The paper is written clearly and the contributions are clearly stated.\n- Some conclusions are not fully supported/could use additional explanations\nQuestions/comments\n- The results have high variance (cf. Fig 2), in this case the paper would greatly benefit from studying the performance of the proposed OOD methods on additional datasets. Conversely, are the authors proposed that using ||h||_{0.3} x energy should be the OOD score to use across different tasks/models?\n- Eq 6 requires a more through explanation. Why does it imply learning a true mapping? \n- Can the authors elaborate on their explanation why varsum and exgrad have subpar performance? \n- The authors don't include AUPR results which should be included. While AUC-ROC shows us the tradeoff between true positive and false positive predictions, and the AUC-PR shows us the tradeoff between precision (low false positive) and recall (low false negative) predictions.\n- Do the authors believe that the feature extraction / familiarity hypothesis [1] holds for the last layer? \n\n[1] Dietterich and Guyer 2022",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is written clearly and the contributions are clearly stated, and the results are interesting.\n",
            "summary_of_the_review": "\nThe paper is very interesting and beautifully written. However, some statements and results could benefit from additional clarity. I would be glad to see this paper accepted if the authors can address all the reviewer\u2019s questions (above) as it has an interesting take on ongoing OOD detection discussions in the literature.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5738/Reviewer_ACRA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5738/Reviewer_ACRA"
        ]
    }
]