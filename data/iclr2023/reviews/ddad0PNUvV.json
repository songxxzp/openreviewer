[
    {
        "id": "UuEUwcUYkH",
        "original": null,
        "number": 1,
        "cdate": 1666592821715,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592821715,
        "tmdate": 1666628649406,
        "tddate": null,
        "forum": "ddad0PNUvV",
        "replyto": "ddad0PNUvV",
        "invitation": "ICLR.cc/2023/Conference/Paper5080/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores extending the idea of fast weight with outer-product based to human-interpretable domains beyond network weight space, and examines the idea with the example of natural image generation. The slow-weight network, now termed Fast Weight Painter now progressively refines the pixel values of a 3D image tensor with one rank-1 updating at a time step. Fair generation results can be observed without imposing any domain-specific inductive bias, and further performance improvements can be achieved by U-net-based one-step denoising. ",
            "strength_and_weaknesses": "**Strength**\n\nThis paper demonstrates a very strong case for extending the fast weight idea in parameters space to other modalities like natural images. \nWhile it is expected to see a lower performance compared to domain-specific methods, the authors show how to further close this gap by exploiting more inductive bias in the image domain. \n\n\n**Weaknesses**\n\n1. Figure 2 can be further improved by simply annotating everything in Eq 4-7 in the figure. \nNow I'm very confused by the arrow pointing from 'Current image' to 'Value' and the one pointing from 'Key' to 'Current image'. What are they referring to?\n\n2. While I understand this paper is just trying to illustrate a case of extending the idea of fast weight to more domains, more experiments certainly help to show how generic the proposed method is. Currently, there are only results with *image* + *GAN training*, while a quick experiment with other data domains like audio or training methods like VAE or diffusion can be a plus. \n\n\n\nMinor points:\n\n1. \nIn the Sequence Processor paragraph, *we did not manage to train any models successfully when the standard Transformer was used instead.* Does it mean transformers completely fail in this case? I don't see a clear reason why transformers cannot work at all here. \n\n2. Based on the 2nd point in Weaknesses, I am very interested to see a synergy between the proposed method and diffusion, as both are iterative.  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "While some figures can be further improved, this paper is presented in a very clear way supported by strong empirical results. \nIt is a novel idea on representing data in a domain-agnostic way. \nI do not notice any reproducibility issues. ",
            "summary_of_the_review": "This paper presents an interesting idea with strong but insufficient empirical results. \nPlease refer to Strength And Weaknesses for detailed comments. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5080/Reviewer_zop8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5080/Reviewer_zop8"
        ]
    },
    {
        "id": "LJAKwY8xaau",
        "original": null,
        "number": 2,
        "cdate": 1666828127877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666828127877,
        "tmdate": 1671123254211,
        "tddate": null,
        "forum": "ddad0PNUvV",
        "replyto": "ddad0PNUvV",
        "invitation": "ICLR.cc/2023/Conference/Paper5080/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an image generation model called fast weight painters (FPA), applying the technique of fast weight programmer. The proposed method treats the generated image as the generated weight matrix.\n\n1. This work seems to be one of the first attempts to apply fast weight programmer method to image generation task, which is novel.\n2. The iterative optimizations of image generation also provide a good interpretability.",
            "strength_and_weaknesses": "Pros:\n1. The paper is well-written.\n2. The proposed method is technically sound.\n3. It is novel to introduce fast weight programmer method into image generation task.\n\nCons:\n1. The generation results is not comparable to SOTA image generation method, although I acknowledge this work is an attempt to introduce fast weight programmer into image generation task.\n\n2. It is good to see the decomposition of image generation process step-by-step, however, the author didn't provide enough illustration or explanation for the visualization results. \n\n3. Lack of high-resolution image generation results.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The author has provided source code.\n2. The proposed method is novel as it introduced fast weight programmer method into image generation task.\n3. The paper is easy to follow.",
            "summary_of_the_review": "Overall, the proposed method looks novel to me, although not flawless as mention above. I would recommend to accept this paper.\n\nUPDATE: After discussing with other reviewers and AC, I decide to lower the score. We agree the proposed method is novel and interesting. But as I mentioned the the review, the paper needs more explanation for the visualization results. And the author should also demonstrate why it is important to the community. We believe the paper could be more solid if the author could address these issues.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5080/Reviewer_81gc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5080/Reviewer_81gc"
        ]
    },
    {
        "id": "Q1NpXIyV5C",
        "original": null,
        "number": 3,
        "cdate": 1666878362685,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666878362685,
        "tmdate": 1668806882915,
        "tddate": null,
        "forum": "ddad0PNUvV",
        "replyto": "ddad0PNUvV",
        "invitation": "ICLR.cc/2023/Conference/Paper5080/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Fast weight programming sequentially generates a weight matrix of a neural network from another neural network, which matrix is generated as the summation of outer products of self-invented keys and values. \nThis paper employs the idea of fast weight programming in the image generation task,  which results in fast weight painters (FPAs).\nFPAs learn to generate images following a sequence of delta learning rules and adding rank-one components to existing images at each step. \nThe authors train FPAs in the GAN framework in 64x64 resolution and evaluate it on various image datasets. \nWhile the performance largely lags behind the StyleGAN2,  authors claim this method allows for visualizing the iterative procedure of complex connections patterns in synaptic learning rules. ",
            "strength_and_weaknesses": "**Strength**:\n1. I like the core idea of this paper and feel it is quite interesting. Images are generated as the summation of a sequence of rank-one matrices following the delta learning rules, whose matrices are outer products of key/value invented from the neural network. \nThe self-invented value/key has subtle connections to transformers, and the iterative updating mechanism is intuitively related to diffusion models,  where adding new ranks is a kind of \"denoising.\" I feel this paper may have the potential to inspire new image-generation methods.\n2. The writing is good, and the paper is easy to follow. \n\n**Weakness**\nAlthough the core idea is cool, my concern is: the current paper doesn't provide enough intriguing results/contributions, which are listed below. \n1. **Using image generation task to understand the weight generation process.** One motivation for this paper is to \"visually illustrate the behavior of an NN that learns to execute sequences of learning rules.\" Visualizing weight generation sequence is hard to understand, so authors visualize image generation. \nI am not convinced by this statement since image generation and weight generation are quite different, and it is unclear to me how much visualization/observation obtained from visualizing image generation could be transferred/generalized to weight generation. \nMoreover, observations from visualization(Sec 4.2, Appendix A) are quite specific for each dataset, which doesn't reveal any genetic observations/ideas. \nSome experiments, like summarising the genetic properties of the generation process and validating it on different domains, would support this claim. Otherwise, I personally think it is specific to image generation tasks.\n2. I understand \"Clearly, our goal is not to achieve the best possible image generator (for that, much better convolutional\narchitectures exist). Instead, we use natural images to visually illustrate the behavior of\nan NN ...\" But I feel more solid experiments/design exploration might improve this paper. \nE.g., the current image generation results have a big gap to stylegan 2 (which is fine for me), and are limited to 64x64 resolution. I wonder, does it still work in 128x128 or even higher resolution? \nWhat if inventing several key/value pairs at each step and calculating multi-rank matrices?\nWhat if updating is conducted on each image patch instead of the whole image? \nWhy UNet's induct bias is so helpful?  \nIt is not necessarily a weakness but adding more design exploration/analysis would be helpful to illustrate the power of the proposed method as an image generation method. \n\n\nIn summary, as a paper visualizing the generation process, weakness 1 would be my main concern. From the image generation perspective, I am worried about the current performance and experiments. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear,  and overall reproducibility and novelty should be fine. \nMore training details and network architecture would be appreciated. ",
            "summary_of_the_review": "This paper is interesting, at least for me.\nMy major concern is: the observation from the image generation is dataset specific and not helpful in understanding the generic process.\nAs an image generation method, it is not powerful and well-designed enough. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5080/Reviewer_oERk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5080/Reviewer_oERk"
        ]
    },
    {
        "id": "xis9yVUOrYl",
        "original": null,
        "number": 4,
        "cdate": 1667235853315,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667235853315,
        "tmdate": 1672022540023,
        "tddate": null,
        "forum": "ddad0PNUvV",
        "replyto": "ddad0PNUvV",
        "invitation": "ICLR.cc/2023/Conference/Paper5080/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work visualizes image generation as sequential low-rank approximations to the weight matrices in a FWP(Fast Weight Programming) setup. The motivation is to not generate high fidelity images but to rather enable human interpretable visualizations of the weight updates in a Neural Network (NN). The authors have shown human comprehensible weight update sequences in six popular datasets. \n\n",
            "strength_and_weaknesses": "*Novelty and Originality*\n\nThe work is interesting in it being one of the few works that draw similarities between images and weight matrices that are generated by another neural net. The recent of this type being the class of Implicit Neural Representations (INR) [1 ]. The work also is a a slightly novel extension to the existing literature on FWP to image generation. \n\n\n*Methodology*\n\nThe authors claims that image generation is not the best of the strengths of FPA, leaving the visualization of weight updates as the central purpose of the work. Given the poor generation quality of FPA, my question is if the  performance vs interpretability/explainability tradeoff presented in the work is worth a shot. Can the method be adapted to exercise control over generations by manipulating the low-rank updates, ref [4] or restrict the low-rank updates to find discriminating components, ref [5], instead of a rank based updation approach. \n\nWhy shouldn't  one compare FPA with recurrent painting techniques like Draw [2].\n\nThe authors posit that FPA does not include any explicit inductive bias as noted in page 2, \"*we show that our\ngeneric models can generate images of respectable visual quality without any explicit inductive\nbias for image processing (e.g., no convolution is used).*\" But the training involves discriminator of LightGAN [3] that uses convolutional layers. \n\nFrom Table 2, the number of train steps are directly proportional to the generation quality. Is it due to having a softmax over the key as discussed in Section 4.2 that forces slow update rules. In my understanding, and the authors' note, \"*The first thing we observe is that for many steps, the \u201ckey\u201d is almost one-hot (which is encouraged by the softmax), i.e., the part of the image is generated almost column-wise.*\" the key is crucial in determining the attention or the generative patterns for the next iteration. Have the authors explored on other activations for key matrices. \n\nCould the authors explain why there is more drastic effect of trains steps on the AFHQ dataset when compared to MetFaces dataset (as shown in Table 2). Have they explored the possible effects of the rank of an image on the generation process. \n\n\n\nReferences:\n\n1. Skorokhodov, Ivan, Savva Ignatyev, and Mohamed Elhoseiny. \"Adversarial generation of continuous images.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n2. Gregor, Karol, et al. \"Draw: A recurrent neural network for image generation.\" International conference on machine learning. PMLR, 2015.\n3. Liu, Bingchen, et al. \"Towards faster and stabilized gan training for high-fidelity few-shot image synthesis.\" International Conference on Learning Representations. 2020.\n4. Zhu, Jiapeng, et al. \"Low-rank subspaces in gans.\" Advances in Neural Information Processing Systems 34 (2021): 16648-16658.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity*\n\nThe paper is mostly clear to read. But would appreciate a more vivid view of the FPA generator architecture and the role of a third NN to map z to initial hidden states of RNN layers of the slow NN.\n\n\n*Reproducibility*\n\nThe work builds upon the existing architecture and has given links to the versions of the implementations used in the work. The work should be fairly reproducible given the code for FPA generator is released\n\n*Novelty and Originality* (repeated from above)\n\nThe work is interesting in it being one of the few works that draw similarities between images and weight matrices that are generated by another neural net. The recent of this type being the class of Implicit Neural Representations (INR) [1 ]. The work also is a a slightly novel extension to the existing literature on FWP to image generation. \n\n\n ",
            "summary_of_the_review": "Overall, I appreciate the novelty in equating weight updation to image generation in FWP framework. The paper is clearly written and mostly easy to follow. My main concerns are with the limitations of the proposed approach in terms of its scalability w.r.t image size and to its applicability beyond the visualization of weight updates as discussed under the first point in the main review under methodology. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5080/Reviewer_wMfX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5080/Reviewer_wMfX"
        ]
    }
]