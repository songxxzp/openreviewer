[
    {
        "id": "IsnEAKXC2Ay",
        "original": null,
        "number": 1,
        "cdate": 1666036839976,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666036839976,
        "tmdate": 1666036839976,
        "tddate": null,
        "forum": "LV_MeMS38Q9",
        "replyto": "LV_MeMS38Q9",
        "invitation": "ICLR.cc/2023/Conference/Paper963/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes Betty, a library for automatic differentiation for\noptimization. The authors describe the library and its design and evaluate it on\na number of problems, comparing to other approaches.",
            "strength_and_weaknesses": "+ important problem\n+ useful library that achieves good results",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, the proposed library novel, and results should be easily reproducible once the software is available.",
            "summary_of_the_review": "The paper is well written, the design of the proposed library makes sense and\nallows for more efficient differentiation than other methods, and the\nexperimental results are convincing. This is a nice paper that should be\naccepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper963/Reviewer_G2gf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper963/Reviewer_G2gf"
        ]
    },
    {
        "id": "ii6AmZQj0eC",
        "original": null,
        "number": 2,
        "cdate": 1666502930561,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666502930561,
        "tmdate": 1666502930561,
        "tddate": null,
        "forum": "LV_MeMS38Q9",
        "replyto": "LV_MeMS38Q9",
        "invitation": "ICLR.cc/2023/Conference/Paper963/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed an automatic differentiation method for multilevel optimization (MLO) by a special dataflow graph. Specifically, by reverse-traversing the paths of the dataflow graph, the best-response Jacobians can be computed iteratively following the chain rule. This method can reduce the computational complexity of automatic differentiation from O(d^3) to O(d^2). Based on the proposed automatic differentiation method, this paper has introduced a software library that supports mixed-precision and data-parallel training. Finally, this paper demonstrated the library on several MLO applications and observed higher accuracy than baselines.",
            "strength_and_weaknesses": "Strength:\n1. The paper is well written and the presentation is clear.\n2. Applications of MLO in machine learning are very common hence studies of more efficient automatic differentiation methods are interesting.\n3. The evaluations are performed on several commonly used scenarios. The empirical evidences for the validation of the proposed method is relatively strong.\n\nWeaknesses:\n1. The experiments are conducted for MLO with at most three-levels but the general method is applicable to MLO with more levels. Applications with at most three-levels might not demonstrate the full advantages of the proposed method.\n2. The proposed method is specifically designed for deep learning based classification problems. It's unclear whether the proposed method is generally applicable for problems in domains outside machine learning.\n3. More comparisons with existing software libraries should be considered other than the single default baseline in each application.\n4. Experiments are mainly focused on the relatively simple CIFAR-10 benchmark.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and the quality is good. The proposed method is valid and the novelty of this paper is relative good. The experiments in this paper is reproducible and the codes of the software will be made publicly available.",
            "summary_of_the_review": "The technique proposed in this paper is valid and show potentials to be applied in common MLO applications. However, the evaluations of the proposed method can be made more extensive by considering wider applications and more baseline methods. Also, it's good to show experimentally the improvement of computational efficiency over baseline methods.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There are no ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper963/Reviewer_5V8s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper963/Reviewer_5V8s"
        ]
    },
    {
        "id": "6ZWuiPUattY",
        "original": null,
        "number": 3,
        "cdate": 1666636837386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636837386,
        "tmdate": 1673040006776,
        "tddate": null,
        "forum": "LV_MeMS38Q9",
        "replyto": "LV_MeMS38Q9",
        "invitation": "ICLR.cc/2023/Conference/Paper963/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides a representation for multilevel optimization problems, going beyond the more common bilevel optimization problems, as a dataflow graph. This graph contains two type of edges, representing 1) dependencies of higher-level optimal parameters on lower-level optimal parameters, and 2) dependencies of lower-level optimal parameters on higher-level non-optimal parameters. It can be used to efficiently compute best-response Jacobians and total gradients.\nIt then introduces a software library based on that principle, which enables user to express multi-level optimization problems in short programs, by defining each problem's cost function and solver configuration, and dependencies between problems.\nExperiments showcase the computational (and memory) efficiency of the library, as well as its flexibility, on existing problems and variants.",
            "strength_and_weaknesses": "Strengths\n-------------\n1. Principled expansion of bilevel optimization (which enjoy increasing popularity) to a DAG of sub-problems, well described\n2. Understandable and usable formalism to define such problems in practice in a framework\n3. Several informative examples, demonstrating the range of capabilities of the framework\n4. Comparison with (and exploration around) SOTA, demonstrating similar (or improved) results and efficiency.\n5. Extensive appendix with details\n6. Opensource code\n\nWeaknesses\n------------------\n1. Clarity could be improved in some places, as it can be a difficult problem to grasp (see below)\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n---------\nThe paper reads well overall, and describes multi-level optimization and the approach taken in understandable terms.\n\nSome things could be improved a bit, or clarified, in particular:\n1. It would be nice to have diagrams similar to Fig. 2 for the actual examples, especially 5.3 / B.3\n2. The direction of arrows / dependencies is not obvious to grasp, when they're presented as \"dependencies\". They may plausibly be \"use -> definition\" as well as \"definition -> use\"\n3. \"training_step\" is a confusing name when what seems to be described is a cost function, rather than an update step.\n4. How does it work when there is more than 1 top-level problem, as in 5.2 / B.2? Are both problems implicitly given a weight? It looks like there would usually be a trade-off.\n5. It would probably help if indices i, j, k, l were used in a more consistent way (in which indices are upper vs. lower, for instance)\n\nQuality\n----------\nThe paper is convincing, the use cases described in the text (then in appendices) demonstrate simple examples (bilevel optimization) as well as non-trivial ones (5.3, where dependencies would otherwise form a cycle).\nExperiments reproduce (or improve on) existing problems, showcasing the usefulness (and correctness) of the library.\n\nNovelty\n----------\nTo my knowledge, this is the first library that explicitly organizes dependencies in MLO in such a way, enabling to easily express MLO problems without explicitly redefining gradient expressions around solvers multiple times.\n\nThe factorization of best-response-Jacobian * vector products (Section 3.2) is similar to the usual factorization of Jacobian-vector products in forward-mode AD, so I don't think that part is really novel, but it's not presented as one major result, so that's OK.\n\nReproducibility\n--------------------\nThe provided code and examples are great to help with reproducibilitiy of the concept, and specific experiments.\nI have no doubt the results can be reproduced.\n\nOther questions\n----------------------\nDefinition 1 defines a \"constrained\" optimization problem, but that is quite different from other usual ways constraints can be defined in optimization problems, for instance equality and unequality constraints on the parameters.\nI suppose equality constraints can be expressed explicitly with slack variables in BETTY (although it may be tedious to write, if there is no specific support), but what about inequality constraints? Are there any incompatibilities in the way problems are expressed? Could they be supported by a future version of the framework?\n\nMinor & typos\n1. On p.3, when defining $P_1$, should the last term of the argmin be $\\mathcal D_1$ instead of $\\mathcal D_k$? Or maybe $\\emptyset$?\n2. In B.2 (p. 15), both `Correct` and `Reweight` have exactly the same implementation for `training_step`, but the text and math in 5.2 (p.7-8) mention a different loss, $\\mathcal L'_{val}$, \"augmented with the classification loss of the correction network\", which I don't see in the code listing.\n",
            "summary_of_the_review": "This paper introduces a well-motivated framework to express and solve multilevel optimization problems. The paper is quite clear, and the experiments are convincing regarding the correctness, ease-of-use, and efficiency of the implementation. Exploring the MLO space beyond bi-level (or even stacked) optimization problems is of interest to the community, and this frameworks seems to be the first in that domain, and may enable it.\nI recommend to accept it.\n\n**Update after response**\nThanks to the authors for their answers and for elaborating on the points raised. I still recommend acceptance, and maintain my rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper963/Reviewer_MAHC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper963/Reviewer_MAHC"
        ]
    },
    {
        "id": "XyIBV8vIFB",
        "original": null,
        "number": 4,
        "cdate": 1667098975307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667098975307,
        "tmdate": 1667098975307,
        "tddate": null,
        "forum": "LV_MeMS38Q9",
        "replyto": "LV_MeMS38Q9",
        "invitation": "ICLR.cc/2023/Conference/Paper963/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work presents an auto-differentiation library for gradient-based MLO. This work makes the following contributions:\n1. This work develops an efficient automatic differentiation technique for LO based on a novel interpretation of MLO as a special type of dataflow graph. This technique can reduce the complexity of automatic differentiation from $O(d^3)$ to $O(d^2)$.\n2. This work introduces a software library named BETTY to support large-scale MLO based on the technique mentioned in contribution 1. \nThe authors demonstrate the effectiveness and stability of BETTY with hundreds of millions of parameters by performing MLO on the BERT-base model. \n",
            "strength_and_weaknesses": "Strength:\n1. The efficient automatic differentiation method is novel and effective. \n2. The proposed system BETTY is thoughtfully designed and clearly presented. It is a very meaningful contribution to facilitating efficient implementations of MLO programs. The authors also showcase the scalability of the proposed system to models with hundreds of millions of parameters by performing MLO on the BERT-base model. \n2. The system will be released open source. \n3. The background knowledge and math foundations are clearly presented. \n     \nThis is very impressive work to me. I do not see a major limitation of the proposed system.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is overall well-written and clearly presented. \n2. The quality of this work is impressively high. \n3. I do not have concerns about reproducibility.",
            "summary_of_the_review": "This work proposes an automatic differentiation technique based on a novel interpretation of MLO as a special type of dataflow graph and then builds a software library for large-scale MLO based on the automatic differentiation technique. Both the automatic differentiation technique and the proposed system BETTY are very impressive contributions to facilitate further research and development on MLO. The paper is well-written and a pleasure to read. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper963/Reviewer_gnFX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper963/Reviewer_gnFX"
        ]
    }
]