[
    {
        "id": "cXVh0sxZ7XU",
        "original": null,
        "number": 1,
        "cdate": 1666605769986,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605769986,
        "tmdate": 1670411963129,
        "tddate": null,
        "forum": "Rvee9CAX4fi",
        "replyto": "Rvee9CAX4fi",
        "invitation": "ICLR.cc/2023/Conference/Paper4918/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper aims at the union of manifolds assumption in deep learning, which states that data lies on a disjoint union of manifolds of varying intrinsic dimensions. The authors empirically verify this hypothesis on commonly-used image datasets.",
            "strength_and_weaknesses": "Strength:\n1. The union of manifolds assumption is useful in deep learning.\n2. The authors provide a lot of numerical results.\n\nWeaknesses:\n1. The paper neglects a lot of important related works. First, there have been numerous works on union of subspaces and manifolds but the author failed to acknowledge them. Union of subspaces or manifolds have been studied in many works of clustering and matrix completion. See the following: [1] Vidal. Subspace clustering. 2011. [2] Elhamifar. Sparse Manifold Clustering and Embedding. 2011. [3] Fan et al. Polynomial matrix completion for missing data imputation and transductive learning. 2020.\nOn the other hand, deep learning based clustering methods are also closely related to union of manifolds. See the following: [4] Zhang et al. Neural Collaborative Subspace Clustering. 2019. [5] Cai et al. Efficient Deep Embedded Subspace Clustering. 2022. There are even some works proposing to train multiple autoencoders to cluster data.\n\n2. The evidence of union of manifolds has also been observed when using t-SNE or UMAP to visualize high-dimensional data.\n\n3. In Section 4.2, Algorithm 1 is not clearly explained. For instance, how to do the initialization and form $\\mathbb{P}^{(\\ell)}$?\n\n4. The title of the paper is too broad.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is fine but the novelty is not sufficient.",
            "summary_of_the_review": "My major concern is that the novelty and contribution of the paper are not significant or at least not well demonstrated. It is not clear what challenges or open problems the paper solved. Comparing with existing work on union of subspaces and manifolds, the quality of the paper is below the bar of ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4918/Reviewer_FLmA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4918/Reviewer_FLmA"
        ]
    },
    {
        "id": "XmqL_lb11PE",
        "original": null,
        "number": 2,
        "cdate": 1666630401634,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630401634,
        "tmdate": 1666630401634,
        "tddate": null,
        "forum": "Rvee9CAX4fi",
        "replyto": "Rvee9CAX4fi",
        "invitation": "ICLR.cc/2023/Conference/Paper4918/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies the \"union of manifold\" hypothesis, a refinement of the manifold hypothesis (MH) wherein data is assumed to lie on a disjoint union of manifolds of varying intrinsic dimensionality. To test this hypothesis an empirical study on common image datasets  is carried out. An empirical study of clustered generative architectures, chosen to test the disjoint manifold scenario, is also performed.\n\n",
            "strength_and_weaknesses": "[+] Interesting and timely topic. MH has been very influential but it is perhaps time to consider more realistic variants.\n\n[+] Empirical investigation seems well-thought out and carefully performed.\n\n[+] Relevant theory (Prop. 1) on the inability of push-forward generative models to model disconnected supports of data distributions is included.\n\n[+] Appreciate the inclusion of Section 4.2.2, where tension between this work's empirical results and prior theory is addressed.\n\n[-] Empirical results appear marginal (though significant). For example in Table 1, the proposed clustered-VAE (C-VAE) outperforms on MNIST, FMNIST, and SVHN, but not CIFAR-10 and CIFAR-100. Arguably CIFAR-10/100 are more important, as they are more relevant to real data. I would like to see a fuller discussion of this point.\n\n[-] Claims about empirical results sometimes appear overstated and not fully spelled out. For instance in Section 4.2.1 the authors write \"we can see that our C-VAE (classes) performs better on most cases...\". It would be better to explicitly spell out which cases perform better and which do not. Same for Appendix C.1 \"We can see that, modulo a few exceptions...\" Which exceptions? Are they important or not?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The message of this work is clear and appears to me to novel. The writing about the empirical claims can be improved. The results appear reproducible after a quick look at the submitted code.\n",
            "summary_of_the_review": "The manifold hypothesis is a fundamental idea in machine learning. Given the enormous influential of MH, it is natural to investigate extensions which have greater relevance to real data. This work makes a valuable contribution to the literature by discussing a disjoint union variant of MH. This paper is an interesting topic and well-argued, although the strength of the evidence support appears small. Nevertheless I recommend acceptance.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4918/Reviewer_6dov"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4918/Reviewer_6dov"
        ]
    },
    {
        "id": "Zdev9D3F9Q",
        "original": null,
        "number": 3,
        "cdate": 1666857170276,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666857170276,
        "tmdate": 1666859557015,
        "tddate": null,
        "forum": "Rvee9CAX4fi",
        "replyto": "Rvee9CAX4fi",
        "invitation": "ICLR.cc/2023/Conference/Paper4918/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors verify the union of manifolds hypothesis (proposed in earlier literature) i.e, data lies on a disjoint union of manifolds of varying intrinsic dimensions. The authors provide empirical results to support the hypothesis. ",
            "strength_and_weaknesses": "Here are some strengths associated with the current work :\n\n1) The authors work towards verifying the union of manifolds hypothesis.\n\nHere are some weaknesses associated with the current work :\n\n1) The authors in this work make many statements without quantifying them thus treating them as obvious facts. \n\" ... Assuming that data lies on a single manifold implies intrinsic dimension is identical across the entire data space, and does not allow for subregions of this space to have a different number of factors of variation. ...\" \n\nThe reviewer does not agree with this hypothesis from the author. Data can be lying on a single manifold with multiple basis vectors. Different classes can choose to use appropriate basis vectors and not use the rest or use them marginally. This does not imply that different sub-regions of space cannot have different factors of variation. \n\n\"... We first prove that these deep generative models (DGMs) are incapable of modeling disconnected supports. We then argue that the class labels provided in our considered datasets identify connected components (i.e. different classes are disconnected from each other), and show that training a push forward model on each class outperforms training a single such model on the entire dataset, even when using the same computational budget: this improvement is a firm indicator that the support is truly disconnected,\nthus strongly supporting our thesis.\"\n\nThe reviewer does not agree with the author's statement above. It is much more easier to train a model once the data has been adequately clustered as such. Training a unique model for each class as compared to a single model for different classes, the hypothesis classes associated are completely different. Thus the argument is fallacious. \n\n2) In case the classes are separable and are non-intersecting and it is possible to separate the classes adequately (as the authors discuss in Section 3.1) then it is straightforward to train models on each separate class. However this is an unrealistic expectation in the real world since training separate models can be futile unless we have a mechanism to perform perfect classification of any unseen instance which is never true. Training models for all classes allows us to use the common structure and basis to mutually help learning for different classes.\n\n3) The authors work is specific to image datasets and not applicable in general, which the authors are kind enough to mention in their discussion. Similarly the authors claim to use agglomerative clustering in this current work. However as mentioned in the no free lunch theorem, there is no one clustering algorithm which can perfectly cluster all instances for all types of data. The authors could have made the work more useful and valuable by adding a discussion on this.\n\n4) The authors in this work assume that the number of clusters is known which is never true in the real world setting. Also the authors' empirical results demonstrate very marginal gains. The authors do not discuss many intricate cases i.e., the case of intersecting manifolds and how to cluster them adequately. \n\n5) The novelty/contribution of the current work is pretty limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the work is pretty limited. The manuscript in its current form requires more work.",
            "summary_of_the_review": "The current work is pretty limited in novelty and the authors make many statements/assumptions which do not hold true in general. The work is focussed on image datasets and thus the results and conclusions might not generally applicable. The draft in its current form needs more work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4918/Reviewer_GFby"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4918/Reviewer_GFby"
        ]
    }
]