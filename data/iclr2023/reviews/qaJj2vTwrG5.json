[
    {
        "id": "9mIRHvK0ZT7",
        "original": null,
        "number": 1,
        "cdate": 1666396124200,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666396124200,
        "tmdate": 1666396124200,
        "tddate": null,
        "forum": "qaJj2vTwrG5",
        "replyto": "qaJj2vTwrG5",
        "invitation": "ICLR.cc/2023/Conference/Paper289/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the author(s) proposed EyeDAS, a few-shot learning-based method to avoid stereoblindness syndrome for object detection (ie. wrongly detect printed objects on billboard/screen as they were in 3D). The proposed method works as a post-processing step. Once any object detection model detects an object with t (t>1) frames, it selects 4 non-learning-based distance metrics (blurring, sharpness, color and edge) to compute a ``3D confidence score''. It uses a gradient-boosted tree to classify if the object is a 3D object. Experimentally, the authors show that the proposed method can achieve close to perfect classification performance on 7 videos (all recorded by a dash cam from the driver\u2019s seat perspective) collected on YouTube, outperforming deep-learning-based transfer learning methods.",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-written and easy to understand.\n- The method is simple, fast, and is shown with good empirical performance.\n- The authors also conduct a number of ablation studies to show the generalizability and the effectiveness of each component of the proposed method.\n\nWeaknesses:\n- Missing details about the dataset and the labeling process: the experiments use a custom dataset collected on YouTube and I think it would be better to provide more details about it. Specifically, \n  - Each of the 7 videos is actually quite long (~an hour), but the resulting train/test datasets only have 2000 RGB objects (line 206). What is the process of extracting the objects? Would about 100 (2000 * 5%) non-3D objects be sufficient for evaluation?\n  - What is the process of labeling true 3D objects and false 3D objects?\n- About the setting: This paper actually proposes a new application problem: few-shot planar/3D object classification. But I am not entirely convinced by its practical soundness in that:\n  - I am not sure the few-shot is necessary for such a setting since planar/3D object classification can be object agnostic and can potentially have large enough data for training. Gradient boosted method can work quite well on few data with good feature engineering, but I am not sure it can be better than transfer learning methods with enough data.\n- It would be good to also show some qualitative examples of the classification results.\n- I am concerned about the minimal contribution: this paper is about a specific and new setting, and the proposed method is simple (gradient boosted tree with good feature engineering) and not new. Though I think it is good to have a simple method that works well, I think it should also show new ideas/insights/understandings to the machine learning community. I do not see enough such contributions from this paper and it looks more like a good technical report.\n\nMinor:\n- Styling of the tables: all tables in the paper are from screenshots and I would suggest the authors follow the ICLR style guide (https://github.com/ICLR/Master-Template/raw/master/iclr2023.zip) to present the tables.\n- About the evaluation metric: I got confused initially by the 2D misclassification rate and TPR in Table 1: if I understand correctly, TPR is specifically referred to as the true positive rate of 3D objects, and 2D misclassification rate is computed as (FP / (FP + TN) = FPR). I suggest the authors clarify the metrics in the experiment section, and it feels more natural to me to use more conventional names (https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n- Some of the columns/rows in the tables are redundant, e.g., in table 2 and table 3 the TPR row under Threshold @ [TPR=1], in table 5 and table 6 the TPR Threshold @ [TPR=1] column.\n- The right image of Figure 1 might not be the right example: the stop sign is also a planar object. Can it be detected by the proposed method?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. This paper is well-written and easy to understand.\nQuality: Fair. Please see the weakness section.\nNovelty: Fair. Please see the weakness section.\nReproducibility: Unsure. Dataset and code are not provided.",
            "summary_of_the_review": "Please refer to the Weaknesses section. I am mostly concerned about the dataset details and the minimal contribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper289/Reviewer_NQv8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper289/Reviewer_NQv8"
        ]
    },
    {
        "id": "HbZx86U6ql",
        "original": null,
        "number": 2,
        "cdate": 1666554107225,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666554107225,
        "tmdate": 1666554107225,
        "tddate": null,
        "forum": "qaJj2vTwrG5",
        "replyto": "qaJj2vTwrG5",
        "invitation": "ICLR.cc/2023/Conference/Paper289/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper is interested in discriminating between a 2d view of a 3D object  and an actual  3D object, in the  context of autonomous  driving.\nThe goal is  to get rid of false detection of objects that are actually pictures of objects,  thereby increasing navigation safety.\n\nAlthough the author does not express it this way,  the  problem is essentially to  ascertain if the  scene is planar or not in a monocular sequence of two images.\nAlthough the problem is interesting, it  is  cast  in a very vague,  non geometrical way,  which  results  is  an ad hoc algorithm without justification,  especially with  regards to geometry.\n",
            "strength_and_weaknesses": "The main weakness  of  the  paper is that the problem is not cast as a 3D computer vision problem,  but a vague 2D recognition  problem.\nWhile  it  is clear that the  problem is related  to  estimating scene planarity from a the  motion of  a monocular camera, the  problem is cast as a purely  2D task.\n\nThe proposed network  architecture, relying on four simple  models (Burring/Sharpness/Color/Edge) is not well justified and is  not related to the fundamentals of  the problem. This makes it  impossible  to  see how or why  it  would  work.\n\nThe choice of  5  images over a  1 second time lapse is  not  properly  justified,  especially when the  proposed  algorithm uses only 2 images to make a decision. The time interval  should be chosen in accordance to  the motion of the  camera,  to ensure that enough parallax is present.\n\nComparing this architecture, which uses 2 images, with other  architectures that use only one image (and are trained on a different problem) seems somewhat unfair. But maybe I misunderstood that result (Figure 4).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The  paper is generally well written, with few  errors to fix.\nOne such error  is  the  mix-up  between Table-4 and Figure-4.\n\nEven if the proposed  approach is novel, it is not justified  and seems very random... The Blurring model is said  to be  related  to autofocus, but the moving cameras have fixed focus, so why mention focus?. Sharpness, Color and Edge are similarly unjustified. The authors do not provide a single convincing example, real of synthetic, that would convince that these models are justified.\n\n",
            "summary_of_the_review": "This paper has failed to properly identify the exact 3d  computer vision problem  it  tries to  solve, which  is  scene planarity under motion  stereo. Because of this, the proposed model is unjustified, and there  is no  basis to believe  that  it actually works, or would would work well in practice.\n\nThe problem is interesting and  important, and I hope the  authors  will consider it from the angle of 3d computer vision, as a planarity estimation problem. It would be ideal, in my  opinion, to explore the  problem in a synthetic framework first (simulating a camera motion in a scene with objects that  are planar and non  planar), before jumping to real images.\n\n\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper289/Reviewer_DucZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper289/Reviewer_DucZ"
        ]
    },
    {
        "id": "IJFPgnHb9lu",
        "original": null,
        "number": 3,
        "cdate": 1666583803674,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583803674,
        "tmdate": 1666583803674,
        "tddate": null,
        "forum": "qaJj2vTwrG5",
        "replyto": "qaJj2vTwrG5",
        "invitation": "ICLR.cc/2023/Conference/Paper289/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work prorposes a few-shot learning-based method named EyeDAS for securing object detectors against the stereoblindness syndrome (i.e., the inability to distinguish between 2D and 3D objects). It leverages the low-level features of image to solve the problem. Four unsupervised models (for blurring, edge, color and sharpness) repectively predict 3D confidence scores and a meta-classifier interprets the confidence scores and makes the final decision. EyeDAS is evaluated on a dataset collected from seven YouTube video recordings.",
            "strength_and_weaknesses": "Strength:\n--The proposed problem of stereoblindness syndrome. i.e., the inability to distinguish between 2D and 3D objects, is interesting and meaningful.  It\u2019s a practical problem we should solve to improve the robustness of autonomous vehicles.\n- The paper is well written and organized. And it\u2019s easy to follow.\n\n\nWeakness:\n- The major problem is the evaluation. Firstly, the whole experiment dataset is only collected from seven YouTube video recordings with hundreds of  annotated objects for training and testing. This experiment benchmark are not convincing enough because of the small data quantity.\n- And the baseline result used for comparison is poor and not reasonable.  The baseline model does not converge well with such few data. If trained with enough data, I think the baseline model can achieve comparable performance.\n- The proposed method is a few-shot method. But in the driving scenerios,  2D / 3D objects are common. It\u2019s easy to get more annotated data for training the model. Few-shot methods are not practical for the problem of stereoblindness syndrome.\n- The proposed method is not lightweight enough. 200 ms latency is not enough for real-time applications.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is novel and clear. The authors provide the code and the reproducibility is guaranteed.",
            "summary_of_the_review": "The evaluation is based on a self-collected  dataset with too few data samples, which makes the effectiveness of the proposed method less convincing.  And the few-shot method  is with less practical value, because annotated data are easily collected and supervised methods work well.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper289/Reviewer_KB85"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper289/Reviewer_KB85"
        ]
    },
    {
        "id": "eMLY8koyx5",
        "original": null,
        "number": 4,
        "cdate": 1666680683509,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680683509,
        "tmdate": 1666683749649,
        "tddate": null,
        "forum": "qaJj2vTwrG5",
        "replyto": "qaJj2vTwrG5",
        "invitation": "ICLR.cc/2023/Conference/Paper289/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles an important problem facing autonomous vehicle perception - how to distinguish between a 3d object and a 2d representation of a 3d object?  A useful insight of this paper is that having information of the target object over time leads to important features that help solve this problem.  The paper proposes a model based on 4 heuristic hand-crafted features together with a gradient boosting meta classifier.  ",
            "strength_and_weaknesses": "Strengths:\n* Paper deals with an important problem\n* This paper presents a useful insight that considering the appearance of the target object over time can provide a strong cue for this problem\n* The paper shows that the technique performs well on the dataset they used.\n\nWeaknesses:\n* This paper uses hand-crafted rather than learned features and does not present a convincing case that there is a good reason to do so. \n* The baseline methods are straw-men that do not benefit from temporal information as the proposed method does.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Since this method does not involve learning the representation used but instead uses hand-crafted features, I would argue that it is not directly relevant to ICLR.  It may be more suited for a computer vision applications conference or workshop.\n\nSince the proposed method uses temporal differences between subsequent input frames, it should be compared to approaches that also have that benefit.  Unfortunately the baseline methods all only use a single image as input.   I believe the hand-crafted features used in this method could easily be learned with an e2e learned approach if the model being trained had subsequent frames as input.  I think taking that approach would improve this paper and make it more relevant to ICLR. \n\n",
            "summary_of_the_review": "This is paper addresses an important problem and demonstrates on a dataset that the method works with accuracy that may be high enough to solve this problem in a practical setting.  The paper makes the insight that using sequences of images over time can help solve this problem.  Unfortunately it does not use a learned representation to achieve this but rather uses hand-crafted features, so is not directly relevant to ICLR.  The baselines that it uses to compare against did not have the advantage of the temporal sequence of images and therefore are not apples-to-apples comparisons. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper289/Reviewer_d1dP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper289/Reviewer_d1dP"
        ]
    }
]