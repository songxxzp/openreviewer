[
    {
        "id": "1Cv4vQMRNJK",
        "original": null,
        "number": 1,
        "cdate": 1666666841072,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666841072,
        "tmdate": 1670846813873,
        "tddate": null,
        "forum": "F8VKQyDgRVj",
        "replyto": "F8VKQyDgRVj",
        "invitation": "ICLR.cc/2023/Conference/Paper5282/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new approach to learning chain-like Horn rules by constructing the relation composition hierarchy in rule bodies from sample paths and by extracting the rule bodies from learnt relation embeddings. Experimental results on two tasks, namely link prediction and inductive relational reasoning, demonstrate the advantages of the proposed approach.",
            "strength_and_weaknesses": "Strengths:\n\n(1) Learning chain-like rules by composing the body atoms step by step has a lower time complexity than learning rules by generating all body atoms at a time. This efficiency benefit has been confirmed by experiments.\n\n(2) Rather extensive experiments on two different tasks have been conducted to demonstrate that the proposed approach NCRL outperforms the state-of-the-arts. In particular, in the link prediction task NCRL is shown to achieve the best performance among all state-of-the-art rule learning approaches that do not consider entity embeddings.\n\nWeaknesses:\n\n(1) The superiority in performance depends on the sampled results on relation paths in the given knowledge graph. That is, this superiority depends on random sampling, and thus it is not stable and unconvincing. The paper should report the average and std results for different runs of the proposed approach. It would be better if different sampling strategies can also be compared with each other to draw a guideline for selecting an appropriate sampling method.\n\n(2) The learnt rules are more restricted than ordinary chain-like Horn rules since they require that adjacent body atoms appearing in siblings of the composition hierarchy should be able to compose a named relation. It is unclear whether this lower expressivity impacts the performance in link prediction or inductive relational reasoning, and whether this restriction can be removed to increase the expressivity to meet broader application requirements.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the paper is questionable in terms of technical soundness. The performance of the proposed approach depends on random sampling, and it is unclear how randomness impacts the performance and how the proposed approach guarantees a stable performance in considering tasks. The clarity of the paper is generally good except the aforementioned issues in technical soundness. The main originality lies in learning chain-like rules by composing relational sub-chains within a window of fixed size. A similar idea for hierarchical rule induction has been published in ICML2022 [1]. The authors may add the comparisons with [1] in Related Work to highlight their originality. The source code is provided in the supplemental material but there are no guidelines provided to download the datasets and to reproduce the experimental results.\n\n[1] Claire Glanois, Zhaohui Jiang, Xuening Feng, Paul Weng, Matthieu Zimmer, Dong Li, Wulong Liu, Jianye Hao: Neuro-Symbolic Hierarchical Rule Induction. ICML 2022: 7583-7615\n",
            "summary_of_the_review": "The paper has a major problem in technical soundness by considering that it is based on random sampling and may not yield stable performance. The expressivity of learnt rules is lower than ordinary chain-like Horn rules. This means that the proposed approach is probably inferior to state-of-the-art methods that learn ordinary chain-like Horn rules, in scenarios where the expressivity of ideal rules is higher than the supported expressivity.\n\nAFTER RESPONSE:\nI increased the overall score since the authors reasonably addressed my main concerns.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5282/Reviewer_WAeF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5282/Reviewer_WAeF"
        ]
    },
    {
        "id": "ERWRYhf5FI",
        "original": null,
        "number": 2,
        "cdate": 1666787424362,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666787424362,
        "tmdate": 1666787424362,
        "tddate": null,
        "forum": "F8VKQyDgRVj",
        "replyto": "F8VKQyDgRVj",
        "invitation": "ICLR.cc/2023/Conference/Paper5282/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies compositional rule learning for knowledge graph reasoning, and a model called the NCRL is proposed. NCRL is able to break a rule into small compositions for reasoning, and a recurrent attention unit is proposed to integrate different relational paths. Experimental results on several benchmarks are impressive.",
            "strength_and_weaknesses": "Strengths:\n1. The problem of learning logic rules is important for knowledge graph reasoning, which could also inspire more advanced models for other reasoning problems.\n2. The idea of breaking rules into small compositions is intuitive and effective, as illustrated in Figure 1. The proposed approach is easy to follow.\n3. The proposed approach achieves SOTA results on many datasets such as WN18RR.\n\nWeaknesses:\n1. Compositional Horn rules have been widely used for knowledge graph reasoning. It is unclear whether the proposed framework can generalize to other more complicated rules.\n2. The experiment can be further improved.\n\nBelow are some detailed comments:\n1. NCRL uses compositional Horn rules for knowledge graph reasoning. Although the results are impressive, such logic rules have been widely explored. In order to develop more advanced models for knowledge graph reasoning and other reasoning tasks, I believe that some more complicated logic rules are needed (e.g., some tree-structured rules). I wonder whether NCRL can also model such logic rules.\n2. In Eq. (2), the proposed model uses a RNN for representation learning. Why does NCRL choose to use RNN? Whether some more advanced sequence models such as Transformer can help?\n3. On WN18RR, NCRL achieves SOTA results while on FB15K-237 the results are slightly worse. Is there any intuition why NCRL gets better results on WN18RR? Is this because WN18RR is more sparse in terms of the number of triples? This might be important if we would like to develop better models in the future.\n4. In the experiment, only some small datasets are considered. Is it possible to apply NCRL to some larger datasets, such as the OGB dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "See comments above.",
            "summary_of_the_review": "See comments above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5282/Reviewer_N8Wt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5282/Reviewer_N8Wt"
        ]
    },
    {
        "id": "tJFYSph0z_B",
        "original": null,
        "number": 3,
        "cdate": 1666812296554,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666812296554,
        "tmdate": 1671130878247,
        "tddate": null,
        "forum": "F8VKQyDgRVj",
        "replyto": "F8VKQyDgRVj",
        "invitation": "ICLR.cc/2023/Conference/Paper5282/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to learn logic rules by recursively encoding subsets of paths into head relations. The paths are samples from the KG. The method to do this is using an RNN to encode each element of the sliding window on the path, selecting one window with a softmax, and using an attention mechanism to predict the summarising head entity.\n\nThe authors demonstrate the scalability advantage and the competitive performance of their method against strong rule-learning based methods on a variety of standard benchmarks.\n",
            "strength_and_weaknesses": "Strengths:\n - Good technical novelty\n - Good empirical evaluation: demonstrating good performance and good scalability\n \n Weaknesses:\n - The clarity of the paper could be improved, in particular variable names are re-used and mean different things in different sections of the paper, which is an unnecessary hurdle for the reader.\n - Some questions about the implementation remain that would be necessary for reproducibility:\n\t- What are the hyperparameters there is no table in the appendix for this as far as I can see (for the results in Table 1 & 3)?\n\t- Will you publish code upon acceptance?\n\t\n - Certain design decisions are unexplained and not studied in the ablation studies:\n\t- Why use an RNN for the encoding of sliding windows (due to the fixed length of window) a standard neural network would do? Did you compare against this?\n\t\nQuestions:\n - How is semantic consistency defined in the intro of section 3?\n - Why is an RNN architecture needed for a fixed size sliding window (eq 2.) did you try a standard Neural Network? Would it be possible to do an ablation on this?\n - How the length of sampled paths determined? Is there a maximum cut-off? Are loops automatically removed? In general the section on path sampling was difficult to understand.\n - Why did you choose to project the key and values to the same space (i.e. W_V = W_K eq5)? \n\nMinor:\n - Please use a different variable name for w_i after the RNN stage, it's unnecessarily confusing. Similarly, please rename the hyperparameter theta as theta is already used in eq4.\n - There is a typo in 4.3, 3rd line at the very end.\n - When defining H in 3.1.2 there are missing brackets in the power as multiplication binds more tightly than addition i.e. it should be (|R|+1) x d.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper's clarity and reproducibility needs improving. The quality and novelty of the work is good.",
            "summary_of_the_review": "The paper makes a solid technical contribution and validates it well empirically. Some questions remain (see above).\n\nI will raise my score if my concerns regarding clarity and reproducibility are addressed.\n\nEDIT: The rebuttal has addressed my concerns, I have adapted my score accordingly.\n\nEDIT EDIT: Due to the concerns regarding the empirical evaluation, I have to lower my score again.\n\nEDIT EDIT EDT: The new results have resolved the concerns surrounding the empirical evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5282/Reviewer_FLaJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5282/Reviewer_FLaJ"
        ]
    },
    {
        "id": "9J3XiWU0vT",
        "original": null,
        "number": 4,
        "cdate": 1666910865365,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666910865365,
        "tmdate": 1670896406332,
        "tddate": null,
        "forum": "F8VKQyDgRVj",
        "replyto": "F8VKQyDgRVj",
        "invitation": "ICLR.cc/2023/Conference/Paper5282/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work proposes a novel differentiable model for the ILP problem, i.e., mining logic from KGs. To do this, the authors introduce a compositional view of the chain-like rules and design a learning schema where the model learns to reduce a path into a single edge. The proposed model, namely NCRL, is an RNN-transformer hybrid model that parameterizes this schema with sequences of soft attention. In the experiments, NCRL demonstrates better systematic generalization in several benchmarks.",
            "strength_and_weaknesses": "Strength \n- Proposes an interesting perspective of viewing chain-like rules in KGs via hierarchical compositions, which is different from the classical multi-hop pathfinding view.\n\n\nWeaknesses\n- Some design choices (see below) need more justification\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Novelty\n\n\nI find the proposed compositional view of the chain-like rule very interesting and inspiring. I particularly like the example shown in the introduction and Fig 1. The proposed model is technically sound, although there might be some aspects needing further justification. The experiments are well-designed and support the claims. A solid contribution to the ILP community.\n\n\n### Quality\n\n\nSection 3.2 suggests that for every pair of entities with a sampled path, one sets the head to \"null\" if there is no direct connection. And with Eq 7, this effectively makes the model predict \"null\" for every pair of entities with missing direct edges. I'm not sure if this is correct: this essentially leads to a closed-world setting, where every possible missing edge is assigned with a specific label (i.e., null). This can be problematic with many real-world KGs that have many missing edges: for example, there might be two pairs of $(x_0,x_n)_1$ and $(x_0,x_n)_2$ with the same path $p_1 = p_2$, but $(x_0,x_n)_1$ is connected with $r_h=hasMother$ while $(x_0,x_n)_2$ is not. In this proposed setting, the model is told to predict hasMother for $(x_0,x_n)_1$ and null for $(x_0,x_n)_2$ even though they have the same path.\n\nI find the scalability argument not very convincing. The proposed model should have similar asymptotic complexity as other backward-chaining methods such as NeuralLP, as all of them are based on the same personalized random walk mechanism. The difference is just that NeuralLP computes the state probability directly with matrix multiplication, whereas NCLR samples from it. The difference in 4.2 seems to be more of a consequence of implementational optimization. For example, by default NeuralLP multiplies the entire matrix for every query, one can readily optimize it by storing the local adjacency matrices up to the max search step.\n\nWith that being said, the path sampling phase may need more elaboration. Right now, it is unclear how the max search step is set, how many samples are collected per pair, and how the duplicates are handled. Also, it is also important to discuss how sensitive the model is w.r.t these hyperparameters.\n\nThe authors set the window size to {2,3} throughout the experiments. It would be nice to have an experiment to show how model performance changes w.r.t the window size.\n\n\n### Clarity\n\n\nThe paper is well-written and generally easy to follow. Below are some minor issues.\n\n\nIn Fig 3 and section 3.1.1, what is $r_i$? Is it an embedding vector of a unique relation? \n\n\nMinor issues\n\t- \"infer the relation between nodel pair query\"\n\t- Conjunctions missing in some rules in Table 5\n",
            "summary_of_the_review": "In summary, this paper introduces an interesting view of the chain-like rule and proposes a technically sound model that takes advantage of this observation. I do have some concerns about the methodology and technical details and hope the authors could address them, but in general, I really enjoy reading the paper. That said, I recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5282/Reviewer_XZA5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5282/Reviewer_XZA5"
        ]
    }
]