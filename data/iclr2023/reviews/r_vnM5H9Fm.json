[
    {
        "id": "ncKnRsetj_",
        "original": null,
        "number": 1,
        "cdate": 1666717153452,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717153452,
        "tmdate": 1669570183747,
        "tddate": null,
        "forum": "r_vnM5H9Fm",
        "replyto": "r_vnM5H9Fm",
        "invitation": "ICLR.cc/2023/Conference/Paper1870/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Weight-Decaying Graph Residual Connection and Topology-Guided Graph Contrastive Loss to overcome the over-smoothing issue of GNNs in order to train deeper GNNs. Experiments are conducted on citation networks and social networks such as Cora CiteSeer PubMed Reddit.",
            "strength_and_weaknesses": "Pros:\n\nThe proposed Weight-Decaying Graph Residual Connection is interesting.\n\nCons:\n\nWhy is the graph residual connection expressed as $H^{(l)} = \\sigma(\\hat{A} H^{(l-1)} W^{(l-1)}) + H^{(l-2)}$ in Equation 2? Is it typically $H^{(l)} = \\sigma(\\hat{A} H^{(l-1)} W^{(l-1)}) + H^{(l-1)}$?\n\nThe proposed Topology-Guided Graph Contrastive Loss which considers 1-hop neighbors as the positive pair and the rest as negative pairs has a strong assumption on homophily. I wonder if the proposed method would be helpful for graphs with strong heterophily.\n\nIn my opinion, this work is not that related to contrastive learning on graphs. However, a large paragraph is used to discuss contrastive learning on graphs which is not necessary. I think the related work can be improved to focus more related works on over-smoothing and different techniques in deepening GNNs.\n\nThe experiments are done on relatively small datasets which makes the results less conclusive. I wonder if the authors could perform experiments on recent GNN benchmarks such as Open Graph Benchmark and Benchmarking Graph Neural Networks.\n\n=== post rebuttal ====\n\nThe authors address some of my concerns. I increased my score. But I still have doubts about the claim that the proposed method can work well on graphs with strong heterophily.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is easy to follow. No code was provided but the experimental results should not be difficult to reproduce. The technical novelty of the paper is marginal compared to previous work on deepening GNNs.",
            "summary_of_the_review": "The major issue of acceptance is the evaluation of the proposed method is done on relatively small datasets. It is not clear how significant it is compared to other SOTA deep GNN methods. I recommend the authors should evaluate their methods on Open Graph Benchmark and Benchmarking Graph Neural Networks to make their claims more conniving.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1870/Reviewer_G8Tr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1870/Reviewer_G8Tr"
        ]
    },
    {
        "id": "AMDPxMVz62s",
        "original": null,
        "number": 2,
        "cdate": 1666739866134,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666739866134,
        "tmdate": 1670647015998,
        "tddate": null,
        "forum": "r_vnM5H9Fm",
        "replyto": "r_vnM5H9Fm",
        "invitation": "ICLR.cc/2023/Conference/Paper1870/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes Weight-Decaying Graph Residual Connection (WDG-ResNet) and Topology-Guided Graph Contrastive Loss (TGCL) to address shading neighbors and over-smoothing issues. Based on proposed WDG-ResNet and TGCL, the authors design an end-to-end model called Deeper-GXX to help GNNs perform well with deep layers.",
            "strength_and_weaknesses": "**Strengths**\n\n(1) This paper addresses the over-smoothing issue of graph neural networks, which is one of important topics to graph neural networks with deep layers.\n\n(2) The proposed Deeper-GXX has demonstrated why stacking deep layer is important with the missing feature scenario and shown good performance in missing feature scenario. \n\n**Weakness**\n\n(1) I think that the novelty of this paper is limited. \n\n- The decaying factor of proposed Weight-Decaying Graph Residual Connection seems very similar to $\\alpha$ in APPNP [1].\n- Also, Topology-guided Graph contrastive loss seems similar to self-supervised loss of SuperGAT [2] and PairNorm [3]. Specifically, the intuition of both papers is maximizing similarity between connected nodes and minimizing between unconnected nodes.\n\n(2) Table 1 shows that Deeper-GXX shows good performance when the number of layer is 50. But, GCN shows better performance when the number of layer is lower. To validate the effectiveness of Deeper-GXX, I think comparison with baselines depending on the number of layers is needed.     \n\n(3) From Table 3, we can know that the weight decaying factor mostly contributes to the performance gain. Also, the performance gain of the similarity measure is marginal. It would be better to provide ablation studies on other dataset.\n\n---\n\n[1] Klicpera, Johannes, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. \"Predict then propagate: Graph neural networks meet personalized pagerank.\"\u00a0ICLR 2019.\n\n[2] Kim, Dongkwan, and Alice Oh. \"How to find your friendly neighborhood: Graph attention design with self-supervision.\"\u00a0ICLR 2021.\n\n[3] Zhao, Lingxiao, and Leman Akoglu. \"Pairnorm: Tackling oversmoothing in gnns.\" ICLR 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "**************Clarity**************\n\nThe paper is clearly written to understand overall method.\n\n**************Quality**************\n\nMore experiments are required to validate the significance of proposed method. Also, it would be better to conduct more experiments to validate the contribution of each component.\n\n**********Novelty**********\n\nThe proposed method has a limited novelty. \n\n******************************Reproducibility******************************\n\nThe paper has fair reproducibility. The hyperparameters and experimental details are explained in the supplemnt.",
            "summary_of_the_review": "Overall, I am leaning towards rejection. My major concern is the novelty and experimental results. If you address my concerns, I will raise my score. \n\n=== post rebuttal ===\n\nThank you for your detailed response on the review. Even though some concerns are addressed, I still have concerns on the paper so that I maintain my score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1870/Reviewer_9Psf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1870/Reviewer_9Psf"
        ]
    },
    {
        "id": "pQF-pSYIe0Z",
        "original": null,
        "number": 3,
        "cdate": 1667229789075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667229789075,
        "tmdate": 1667229789075,
        "tddate": null,
        "forum": "r_vnM5H9Fm",
        "replyto": "r_vnM5H9Fm",
        "invitation": "ICLR.cc/2023/Conference/Paper1870/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes two tricks to improve GNN performance at large depths: WDG-ResNet (weighted residual connections) and TGCL (additional loss term). Through experiments and ablations they show that a GNN enhanced with both modifications is better-behaved when scaling up depth.",
            "strength_and_weaknesses": "=== Strengths === \n\n(S1): This work explores generalizable modifications to GNNs that could be applicable to a broad range of architectures/problems. \n\n(S2): The authors define the \"shading neigbhors effect\", which they use to motivate their approach better. \n\n \n\n=== Weaknesses === \n\n(W1): The motivation behind the `sim` component of the architecture (Equation 4) is unclear to me, and the gains from adding it border on statistical significance: \n\n- The authors mention that this component \"expands the hypothesis space of GNNs\", but I'm not sure how to understand that. What are we trying to achieve with this term? \n\n- Are we backpropagating through `sim` during training? \n\n- Judging from results in Table 1 and 2, the gain from adding this term is minimal, bordering on statistical significance. I agree there is *some* signal if one integrates the small increments across all the different experiments and setups, but (as the authors note), this component also increases compute requirements and complicates things conceptually. \n\n(W2): Some more comparisons would help to contextualize the results: \n\n- Simplified WDG could be compared to an ablation where the weights are learned scalars, instead of being fixed to `e^{-l/lambda}`. This would show whether the decaying-by-design weighing is indeed what is causing the improvements, or the sheer fact that there is a post-activation coefficient smaller than 1 is already enough. \n\n- The authors compare against ResNet-style GNNs, but there's another approach (also extremely practical and common), which is to concatenate the hidden representations across all layers, project that concatenated vector to a suitable dimension, and use that for the downstream task. This is sometimes referred to as \"Jumping Knowledge\", and obtained by setting `jk=\"cat\"` in `pytorch_geometric`. \n\n \n\n=== Nitpicks === \n\nHere I include some final nitpicks, which did not affect my score; they are here just to help improve the paper. \n\n- \"user\u2019s immediate neighbor information may not reflect his/her categorical information\": use \"their\" instead of \"his/her\" \n\n- \"information collected from faraway neighbors become dominant\": \"become\" -> \"becomes\" \n\n- \"This radioactive decay\": drop the word \"radioactive\" \n\n- \"simpler version of Eq. 4 by fixing the sim\": I think \"fixing\" should rather be \"removing\"? \n\n- Missing space before citation in Section 2.3 \n\n- I'd call Section 3 \"Experiments\" (plural) instead of \"Experiment\" \n\n- \"could be found in Appendix\" (twice): I would say \"can\" instead \n\n- \"shallow GCN with ResNet has enough capability\": \"capability\" -> \"capacity\" \n\n- \"we need more neighbors aggregated for collecting their partial features to compensate for the missing such that stacking GNN layers works\": this sentence seems broken, please rephrase\n\n- \"conduct the ablation study\": \"the\" -> \"an\" \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The work is generally clear and easy to understand, apart from the parts pertaining to the `sim` part of TGCL. This work does have a sizeable number of small writing errors, but these should be easy to fix (I listed a bunch in my \"Nitpicks\" section).\n\nQuality: Experiments in this work are relatively thorough, but (as I mentioned in the \"Weaknesses\" section), some more comparisons could be useful, and motivation for the `sim` part of the loss is lacking.\n\nReproducibility: The work provides a good number of implementation details, and thus seems reproducible.",
            "summary_of_the_review": "The premise of this work is overall sound, and the results look promising, but some extra intuition is needed, and I would welcome some more baseline comparisons. For my initial evaluation I lean slightly to the negative side, but I'm open to discussing with the authors and other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1870/Reviewer_y1R7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1870/Reviewer_y1R7"
        ]
    },
    {
        "id": "G8_nU51BcT",
        "original": null,
        "number": 4,
        "cdate": 1667609555580,
        "mdate": 1667609555580,
        "ddate": null,
        "tcdate": 1667609555580,
        "tmdate": 1667609555580,
        "tddate": null,
        "forum": "r_vnM5H9Fm",
        "replyto": "r_vnM5H9Fm",
        "invitation": "ICLR.cc/2023/Conference/Paper1870/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of vanishing gradient and over-smoothing in graph neural networks which is highly related to the deep GNN training. This paper proposes two tricks into existing GNN layers named: Weight-Decaying Graph Residual Connection module (WDG-ResNet) and Topology-Guided Graph Contrastive Loss (TGCL). Experiments conducted on four datasets demonstrate the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "**Strength**\n- The paper is overall well written and easy to follow. The figures and tables are well described.\n- The finding that \"optimal decaying factor is close to the diameter of input graphs (if it is connected) or the largest component (if it does not have many separate components).\" is interesting and may inspire some follow up works.\n- The reproducibility of this paper looks good to me.\n\n**Weakness**\n- The technical contribution is limited. Concerning the two tricks, the first trick has been widely studied in the literature, such as the Jumping Knowledge Networks[1] or the Predict then Propagate [2], the idea is quite similar that by incoporating information from the longer distance nodes while avoiding the domination of them, which is a trade-off between close and distant nodes. This paper propose a decay funnction which is a degraded version of above methods. The second trick is widely studied in contrastive learning that adding weights on the positive and negative pairs by the hardness of sample, where the distance is utilized as hardness here.\n- The critical design is the  learnable similarity between the layers, which distinguish this paper from the ResNet. However, due to the complexity and training efficiency proble, the simiplified WDG-ResNet ignores the learnable parameters and this makes the proposed WDG-ResNet quite similar to the ResNet with only layer-wise constant decays. This is quite weired and the motivation of WDG-ResNet becomes unclear.\n- The experiments are conducted on the four datasets: Cora, Citeseer, Pubmed and Reddit, which is quite small. Furthermore, these datasets are representative ones with high homophily, where neighborhood nodes tends to share similar labels. The performance of the proposed method on large datasets and lower homophily datasets is not convincing.\n- The baselines are weak, many GNN methods such as [1][2] and graph contrastive learning methods are missed.\n\n\n[1] Representation Learning on Graphs with Jumping Knowledge Networks\n[2] Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clarity written and easy to follow, all the equations, figures and tables are clearly described.\nThe hyperparameter tuning is provided in the appendix and I believe the reproducibility of this paper is good.\nHowever, the novelty is limited as I have mentioned above, especially in the simplified WDG-ResNet.",
            "summary_of_the_review": "Although the problem studied in the paper is intersting and the paper is well written, my major concerns are the technical contribution (both two tricks) and the experimental results (datasets and baselines). The authors are expected to provide detailed evidences to address the above concerns for increasing the final scores.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1870/Reviewer_YeUe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1870/Reviewer_YeUe"
        ]
    }
]