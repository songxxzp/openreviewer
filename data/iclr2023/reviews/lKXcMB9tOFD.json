[
    {
        "id": "XhKtehsHxJ",
        "original": null,
        "number": 1,
        "cdate": 1665654260784,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665654260784,
        "tmdate": 1665654260784,
        "tddate": null,
        "forum": "lKXcMB9tOFD",
        "replyto": "lKXcMB9tOFD",
        "invitation": "ICLR.cc/2023/Conference/Paper6374/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes an adaptive stochastic gradient method, which scales element-wise step sizes inversely proportional to the update magnitude in the previous step. A regret bound is provided and the method is evaluated on a number of neural network training tasks.",
            "strength_and_weaknesses": "### Strenghts\n\n1) The paper follows a clear structure and is generally well-written. \n\n2) I like the illustration of the proposed method on the Rosenbrock function.\n\n### Weaknesses\n\n1) Generally, I find the motivation offered for the method unsatisfying. Some of the reasons used to motivate the method are not really substantiated. For example, on page 2, the paper states that the proposed algorithm avoids\n\"the suboptimal convergence due to very large or very small scaling of the learning rate.\"\nWhile this might have been some intuition guiding the authors, the do not offer any reason as to why very large or very small scaling is inherently bad. In fact, the literature on\nadaptive gradient methods probably argues that drastic step size differences in different coordinates are a _feature_ not a bug.\n\n2) The parametrization of the proposed algorithm is redundant. In Eq. (4), the hyperparameter\n$K$ could simply be subsumed into the \"global\" learning rate $\\mu_0$. (This also has some implications for the experimental comparison, see point 6.)\nOn the other hand,\none could imagine having an additional hyperparameter in the denominator.\nIf I were to experiment with an update of the proposed form, I would want to parametrize\nit as $a / (1 + b \\Delta)$.\n\n3) Neither the regret bound in in Eq. (8)\nnor its proof (Appendix A) make mathematical sense. The cumulative regret on the left-hand\nside of Eq. (8) is a scalar quantity. The right-hand side seems to be $k$-dimensional vector.\nIn the proof, Eq. (18) does not have any well-defined mathematical meaning.\n$J$ is a function of _all_ model parameters\n$\\theta\\in \\mathbb{R}^d$, so it is mathematically meaningless to plug in only the $i$-th model\nparameter $\\theta^{(i)}$.\nIt seems the authors want to assume some sort of decomposition across individual coordinates,\nbut this would certainly require additional assumptions beyond smoothness and convexity.\nEven assuming that some such decomposition exists, there are a more errors in the\nremainder of the proof.\nI'm not going to list all of them, but for example going from Eq. (25)-(26) is not\npossible and using Cauchy-Schwarz would introduce a norm around the gradient term.\n\n4) In Section 3.1.1, the paper states that the algorithm will converge if $\\mu_k< 1 / (2 \\beta)$\nwhere $\\beta$ is \"the maximum eigenvalue of the input correlation matrix\".\nThis statement is not applicable to the setting the paper is assuming (smoothness + convexity). It might hold for\nlinear least-squares regression, but if that is what the authors are referring to here,\nit should be stated very clearly.\n\n5) In the beginning of of Section 4.1, it is falsely stated that the Rosenbrock function is strictly convex.\n\n6) The experimental comparison has several shortcomings in my opinion. The base learning rate is\nset to the same value for _all_ methods. For the proposed method, the hyperparameter $K$\nis tuned (the tuning protocol employed is unclear). Since $K$ is simply a multiplier for the\nlearning rate (see comment 2) this amounts\nto an unfair comparison.\nFurthermore, the experiments seem to have been conducted with a single random seed.\nMultiple seeds should be used to gauge the variabilty of the performance.\nFinally, the paper compares _training_ loss and accuracy. While this is adequate from an\noptimization perspective, the potential users of the method will be more interested in the\nperformance on the test set.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe paper is written clearly and the algorithm is explained in full detail. However, some of the reasons used to motivate the method seem unsatisfying to me.\n\n### Quality\nThe mathematical sections of the paper are simply wrong. The experimental comparison has several shortcomings.\n\n### Originality\nThe method is, to the best of my knowledge, novel. Of course, papers on new variants of adaptive gradient algorithms for deep learning operate in a narrow space.",
            "summary_of_the_review": "In light of the weaknesses discussed above, I recommend rejecting this paper. The mathematical sections contain multiple severe errors (weakness 3-4) and the experimental comparison is not up to the standards that are expected of empirical work on optimization methods (weakness 6). I would also question the basic motivation for the proposed methods (weakness 1), but of course it is perfectly justifiable to just \"try out\" something.\n\nTo the authors, I want to say that my negative review is not meant to diminish your work or to discourage you from pursuing it further. The mathematical errors might be fixable and the experimental comparison can be improved (clear tuning protocol, multiple random seeds, report test performance).",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6374/Reviewer_xzMY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6374/Reviewer_xzMY"
        ]
    },
    {
        "id": "MwCFW5Nkca_",
        "original": null,
        "number": 2,
        "cdate": 1666682089552,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682089552,
        "tmdate": 1666682089552,
        "tddate": null,
        "forum": "lKXcMB9tOFD",
        "replyto": "lKXcMB9tOFD",
        "invitation": "ICLR.cc/2023/Conference/Paper6374/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a new optimizer, which adapts the stepsize by $K/(1+abs(\\Delta \\theta))$ where $\\Delta \\theta$ is the update of the parameters. The authors proved the convergence of the proposed algorithm and conducted experimental validations.",
            "strength_and_weaknesses": "Strength:\nThe paper is overall easy to follow and well presented. The authors conducted analysis in both theory and validation on toy and real-world datasets.\n\nWeakness:\n1. The proposed method is actually counter-intuitive. If $abs(\\Delta \\theta)$ is large in dimension $I$, for SGD it means the gradient amplitude in dimension $i$ is large. I don't see why the update stepsize should be small in the $i-th$ dimension. \n2. The proof of convergence is restricted to convex objective functions, instead of the more general stochastic non-convex optimization setting. Therefore the results in pretty limited.\n3. Experimental validation on Cifar datasets is far from satisfactory. The authors should validate the proposed method in various tasks with different model architectures and datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is overall easy to follow.\n\nNovelty:\nMarginal\n\nOriginality:\nMarginal",
            "summary_of_the_review": "The proposed method is counter-intuitive, and the proof of convergence is only limited to deterministic convex objective functions, and the experimental validations are pretty naive.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6374/Reviewer_Pa1W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6374/Reviewer_Pa1W"
        ]
    },
    {
        "id": "uOJW2U24gn",
        "original": null,
        "number": 3,
        "cdate": 1666712567093,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666712567093,
        "tmdate": 1666712838697,
        "tddate": null,
        "forum": "lKXcMB9tOFD",
        "replyto": "lKXcMB9tOFD",
        "invitation": "ICLR.cc/2023/Conference/Paper6374/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new adaptive learning rate first-order optimizer, where the learning rate is scaled by the inverse parameter difference between two recent iterations. Theoretical analysis based on regret bound is presented, and experimental results on image dataset / CNN models are conducted.",
            "strength_and_weaknesses": "Pros:\nThe idea is easy to understand.\n\nCons:\n1. The notation is not presented very clearly. For example, the explaination of $K$ in eqn.(3) appears very lately.\n2. The idea is a little strange to me. In my understanding, the adaptive learning methods (including Nestrov accelerated momentum) should aim for estimate the Hessian to obtain an approximation of the best step-size in comparision with second-order methods. For example, the momentum term of NAG could be reformulated to the difference of gradients between two recent iterations, where is a reasonable approximation for the second-order information. However, it makes no sense to me that the difference of model parameters could do the similar thing, therefore I doubt the theoritical correctness of the proposed method.\n3. It's quite strange that only evaluating the training accuracy but not validation / test accuracy on real datasets, especially for stochastic optimization methods. If we only aim for training, then gradient decent should be the best method. Also, the training stability of the proposed method seems very badly, therefore I doubt the practical usage of the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "N/A",
            "summary_of_the_review": "I have low confidence of this review, and happy to take advices from other reviewers, and AC.\n\nUpdate: after carefully reading other reviews, I generally agree reviewer xzMY's comment, and appreciate his/her efforts on checking the correctness of proof. I lower down my score to clear reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6374/Reviewer_D864"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6374/Reviewer_D864"
        ]
    },
    {
        "id": "i132ukhWaS",
        "original": null,
        "number": 4,
        "cdate": 1667191741582,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667191741582,
        "tmdate": 1667191741582,
        "tddate": null,
        "forum": "lKXcMB9tOFD",
        "replyto": "lKXcMB9tOFD",
        "invitation": "ICLR.cc/2023/Conference/Paper6374/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis paper proposes an adaptive learning rate method to improved the stochastic gradient descent algorithm by leveraging the difference of two consecutive iterations. More concretely, the authors proposes that the learning rate of each parameter should be inversely proportional to the difference between the current and the previous iteration of the this parameter. The authors claim the such an updating scheme, when combined with SGD, can converge under mild assumptions. Empirical studies also show that it can achieve lower training loss and better accuracy on CIFAR-10/100 and Tiny-ImageNet.",
            "strength_and_weaknesses": "Strength:\n\n1. The idea is simple and easy to understand. It is also easy to implement.\n\nWeaknesses:\n\n1. There are a bunch of notation errors/overloading in the paper that make it hard to follow. Examples:\n    \n    1.1. Vectors should be in $\\mathbb{R}^L$ rather than $\\mathbb{R}$, e.g.,  $p, q \\in \\mathbb{R}$, $\\theta \\in \\mathbb{R}$, etc.\n    \n    1.2. The equation immediately after eqn (7) should be $J(\\theta^*)= \\arg\\min_\\theta J(\\theta)$. \n    \n    1.3. While eqn (8) uses $M_k$, the following sentence uses $M$. And there is no definition about $\\mu_k$ in the next sentence. \n    \n    1.4. \\beta is used for the Lipschitz constant above eqn (8), and then overloaded as the maximum eigenvalue of the input correlation matrix in eqn(9).\n\n2. Given the the notation systems are so messy, it is hard to believe the proof is correct.\n\n3. The algorithm involves parameters $K$ and $\\mu_0$, but there is no guidance on how to choose them.\n\n4. The empirical studies are not convincing either.\n\n    4.1. In Figure 2, the proposed algorithm converges obviously slower than quite a few other algorithms. Although it seems to converge to a point with lower loss, it is unclear why it is the case. Also, the proposed algorithm is unstable compared to others, as its curve is bumpy.\n    \n    4.2. The algorithm is only evaluated on three small image datasets. It is unclear if the conclusion can generalize to other domains/networks. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the paper is low, as the notation system is messy.\n\nNovelty is limited as the idea of adaptive learning is widely explored.\n\nThe idea is easy to implement, so it should be reproducible.",
            "summary_of_the_review": "The paper is not well written that it is hard to tell if the proof is correct. Besides, the idea is not very novel and the empirical study is not comprehensive either, making it less convincing that the proposed algorithm is effective.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6374/Reviewer_LcE9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6374/Reviewer_LcE9"
        ]
    }
]