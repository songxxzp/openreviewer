[
    {
        "id": "HY1QxQSARn",
        "original": null,
        "number": 1,
        "cdate": 1666600230945,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600230945,
        "tmdate": 1669263156002,
        "tddate": null,
        "forum": "jpq0qHggw3t",
        "replyto": "jpq0qHggw3t",
        "invitation": "ICLR.cc/2023/Conference/Paper3302/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors formalize a new problem setting, called partial label unsupervised domain adaptation (PLUDA), and propose its solution that jointly conducts partial label learning via pseudo-labeling and domain adaptation by class-prototype alignment. ",
            "strength_and_weaknesses": "\n<Strength>\n\n- The new problem setting PLUDA proposed in this paper is interesting and should be practically important in some ML applications.\n- The pseudo-label refinement and class-prototype alignment are well designed. Their design seems simple but reasonable.\n- The proposed method performs substantially well in the experiments. \n- This paper is well-organized.\n\n<Weakness>\n\n- It is not clear to see why we need L_reg in the proposed method. The authors state \"We expect this prediction consistency regularization loss in both domains can help induce similar feature representations across domains for the classification task\" in Section 3.3, but is there any theory or empirical evidence? Since this regularization loss is computed for each domain, it should not explicitly induce similar representations across domains. In addition, L_reg is computed in the output space differently from standard contrastive learning such as SimCLR, which should not directly regularize the feature representation.\n- In the experiments, it would be better to include \"source-only\" performance, which is actually the performance of standard partial-label learning, in the comparison. Since the experimental setup is new, showing the performance of naive baselines itself should be important to highlight the necessity of PLUDA methods.\n",
            "clarity,_quality,_novelty_and_reproducibility": "<Clarity>\n\n- This paper is well-organized.\n- Several minor concerns:\n\t- In this paper, \"=\" sometimes means substitution, but it also means equiality in somewhere else. This is a bit confusing. \n\t- At the first paragraph in Section 3, Bt should comprise x^t, not x^s.\n\n<Quality>\n\nI have some concerns as described in <Weakness>.\n\n<Novelty>\n\nThe problem setting PLUDA proposed in this paper is new and interesting. The proposed method seems to be carefully designed to solve this problem. Importantly, it is simple enough, which will be a good baseline for future studies in the community.\n\n<Reproducibility>\n\nThe implementation details seem to be sufficiently provided in the paper.\n",
            "summary_of_the_review": "The problem setting tackled in this paper is new and interesting. However, I have several concerns on the design of the proposed solution and its evaluation. I vote for \"weak reject.\"\n\n---\n\nAfter rebuttal, I updated my score from 5 to 6.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3302/Reviewer_TPqJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3302/Reviewer_TPqJ"
        ]
    },
    {
        "id": "6ayaJXlfmv",
        "original": null,
        "number": 2,
        "cdate": 1666602807865,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602807865,
        "tmdate": 1666602822168,
        "tddate": null,
        "forum": "jpq0qHggw3t",
        "replyto": "jpq0qHggw3t",
        "invitation": "ICLR.cc/2023/Conference/Paper3302/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work formalizes a new learning scenario called partial label unsupervised domain adaptation (PLUDA) which comprises both PLL and PLUDA problems. A novel PAPLUDA approach is proposed to address the challenging PLUDA problem by three components: classification loss with soft label disambiguation, inter-domain class-prototype alignment, and teacher-student model based contrastive regularization, which mutually benefit from each other to enhance the PLUDA learning. Experiments validate the effectiveness of the proposed method in dealing with the new PLUDA problem.",
            "strength_and_weaknesses": "Strengths:\n1\u3001This paper considers a feature distribution discrepancy problem between the training and test data in partial label learning which is always ignored in the existing PLL methods. The paper formalizes this new learning scenario as PLUDA problem that addressing both PLL and UDA problems in a unified framework is novel.\n2\u3001Exploiting the prototype as the representative of each class to bridge the gap between the source and target domain is promising.\n3\u3001Comprehensive experiments with different noise level show the effectiveness of the proposed method in noise label disambiguation and feature distribution discrepancy.\n\nWeaknesses:\n1\u3001Some abbreviations are unclear, for example: in Figure 1, the expression \u2018EMA\u2019 is not mentioned in the paper. I guess it is an update operation for teacher model\u2019s parameter, is not it?\n2\u3001In section 2, the author discuss previous approaches including their weakness that cannot be used in PLUDA problem. But it is not clear which type of existing methods\u2019 weakness the author tries to tackle in the paper.\n3\u3001The author uses the different method to achieve the label disambiguation and pseudo label refinement in the source and target domain respectively. The author tries to achieve the same goal but use different strategy, why?\n4\u3001The key issues of PLUDA can be lied in the feature distribution discrepancy and irrelevant noise label. From the ablation study, we can see that dropping the pseudo-label refinement gets a largest performance degradation, while dropping the class-prototype alignment loss which is also important to the framework gets less performance drop, why?\n5\u3001The problem considered in the paper is novel, but I wonder can we use the off-the-shelf methods to address the new PLUDA problem? Or use the combination of existing PLL and UDA methods? Why?\n6\u3001The feature distribution discrepancy is the core of PLUDA, can you further explain how you can align the feature distribution across domains by the prototype alignment?",
            "clarity,_quality,_novelty_and_reproducibility": "This is a well-written paper overall. The PLUDA problem that integrates both PLL and UDA problems is novel and interesting, which has been demonstrated the existing methods cannot be used to address the new formalized learning scenario. The proposed method consists of multiple components that address the critical issues in PLL and UDA with the help of a teacher-student model, which has no difficulty in understanding and following it.",
            "summary_of_the_review": "This paper is intriguing and contributes to the filed of partial label learning.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3302/Reviewer_jab8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3302/Reviewer_jab8"
        ]
    },
    {
        "id": "3gv7RBLL4P",
        "original": null,
        "number": 3,
        "cdate": 1666665189026,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665189026,
        "tmdate": 1666665277347,
        "tddate": null,
        "forum": "jpq0qHggw3t",
        "replyto": "jpq0qHggw3t",
        "invitation": "ICLR.cc/2023/Conference/Paper3302/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes an unsupervised domain adaptation framework PAPLUDA under the partial label learning scenario which is formalized as a new problem called partial label unsupervised domain adaptation (PLUDA). The proposed PAPLUDA method disambiguates the irrelevant label noise with the help of a teacher-student model and minimizes the discrepancy between the source and target domains by the inter-domain class prototype alignment. Experimental results validate the effectiveness of PAPLUDA in addressing PLUDA problem.",
            "strength_and_weaknesses": "Strengths:\n\n1-The domain discrepancy problem is widely existing in many real-world learning applications. This paper explores the problem of discrepancy between training and test data in PLL which is formalized as a challenging PLUDA problem and proposes a novel PAPLUDA framework to address it.\n\n2-The idea of using class prototype to minimize the discrepancy across both domains is interesting.\n\n3-The experiment results demonstrate the effectiveness of the proposed method in addressing both PLL and UDA problems in a unified framework.\n\nWeaknesses:\n\n1-In this paper, the author integrates both PLL and UDA problems and tries to address them in a unified framework. What motivates you to formalize such a new learning scenario? \n\n2-As you stated, GearNet can also address the noise label and UDA problem, why it can\u2019t be used to address the PLUDA problem? Can we modify it for addressing the PLUDA problem? and what\u2019s the difference between WSDA and PLUDA problems.\n\n3-In the approach section, the author uses a one-hot label indicator vector \\hat y_i^s for soft label disambiguation in the source domain, but uses the outputs of the teacher model to update label probability vector p_i^t in the target domain, why?\n\n4-Have you tried to perform the label disambiguation in the source domain and update the label probability vector in the target domain by the same way like Eq.(3) or Eq.(6)? I wonder the different but similar label disambiguation strategy can get a better performance than using the same way?\n\n5-What motivates you deploy the inter-domain class prototype alignment term to alleviate the discrepancy between the source and target domains? Why it works?\n\n6-The class prototype can be obtained in batch-wise by Eq.(5) directly, can you further explain why you update in Eq.(7)?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is written clearly and easy to follow. The idea of considering the feature distribution discrepancy problem in PLL is novel and exploit the class prototype to address the feature distribution discrepancy is also interesting.",
            "summary_of_the_review": "This is an interesting paper that formalizes a new learning scenario and addresses both challenging problems in a unified framework. The approach is novel and technically solid.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3302/Reviewer_UB1N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3302/Reviewer_UB1N"
        ]
    },
    {
        "id": "o7q5ipidaq",
        "original": null,
        "number": 4,
        "cdate": 1666666136810,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666136810,
        "tmdate": 1666666136810,
        "tddate": null,
        "forum": "jpq0qHggw3t",
        "replyto": "jpq0qHggw3t",
        "invitation": "ICLR.cc/2023/Conference/Paper3302/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel problem by combining the settings of PLL and UDA. To tackle this challenging setting, they also propose a novel approach named PAPLUDA which consists of three loss components: classification loss with soft label disambiguation; inter-domain class-prototype alignment loss, and teacher-student contrastive regularization loss. To demonstrate the effectiveness of their proposed method, they perform experiments on two synthesized datasets which are generated from two popular DA datasets, Office-31 and Office-Home. In all experiments, PAPLUDA outperforms baselines by a significant margin. In addition, ablation studies are conducted to verify the importance of each component in their method and its sensitivity to different choices of hyperparameters.",
            "strength_and_weaknesses": "## Strong points:\n* The motivation for their newly introduced problem is clear and reasonable.\n* The Related work is well-written which helps to position this paper well in the related literature.\n* The authors also did a good job of elaborating on their proposals and providing the intuition behind each component.\n* Their method, PAPLUDA, shows a clear advantage over comparison methods in the new setting.\n* The ablation studies are sufficient to further strengthen their claims.\n\n## Weak points:\n* For the method part, In Section 3, a lot of claims are made without an appropriate reference. For instance, \u201cWe deploy a teacher-student model as the classification model due to its robustness against noisy labels for stable prediction.\u201d Why is this claim true? In addition, it is unclear whether some techniques are newly introduced in this paper or have already existed before. For example, the teacher-student models\u2019 design, moving average soft label, class-prototype learning with moving average pseudo-label, and dynamic dictionary, to name a few. Although this is an empirical paper, the authors\u2019 claims still need to be backed by appropriate references to prior works.\n* For the experiment part, while PAPLUDA illustrates strong performance on datasets with high ambiguity, there is a tendency that it cannot outperform baselines when q is small and it approaches 0.\n\n## Questions:\n* Is gamma in (3) and (6) the same parameter?\n* How hyperparameters (coefficients for moving average)  are chosen in Section 4.3? Why are some of them fixed and one is adaptive?\n\n## Minor comments:\n* There are some typos and grammatical errors. For example: \u201cthey are restricted to the liner model\u201d, \u201cno one irrelevant label is chosen, we randomly pick a irrelevant label\u201d.",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: The paper is well-written and easy to follow.\n* Novelty and Quality: Please see the strength and weaknesses part.\n* Reproducibility: The code is not available. The authors should discuss the neural network architectures and hyperparameters in more detail.",
            "summary_of_the_review": "Overall the contributions of this paper are two-fold: a novel problem and an efficient solution. The efficiency of their method is backed by experimental results and detailed ablation studies. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3302/Reviewer_uDYu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3302/Reviewer_uDYu"
        ]
    }
]