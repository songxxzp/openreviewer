[
    {
        "id": "aeO1sJWRxd",
        "original": null,
        "number": 1,
        "cdate": 1666126863721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666126863721,
        "tmdate": 1666126863721,
        "tddate": null,
        "forum": "MIMwy4kh9lf",
        "replyto": "MIMwy4kh9lf",
        "invitation": "ICLR.cc/2023/Conference/Paper3237/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper is about open-vocabulary object detection (OVD), where detection models are trained from a set of base categories with bounding box annotations, as well as a set of image-caption pairs. Like prior works, this paper leverages existing, pre-trained vision & language models (VLM) like CLIP that were trained on large quantities of image-caption pairs. Unlike prior works, this work shows that an object detector can be put on top of the frozen VLM. Specifically, the proposed object detector consists of the frozen VLM as the backbone, while detector-specific layers (region proposals, feature pyramid network, and region classification) are trainable. The final classification of each region is a combination (geometric mean) of detector scores and off-the-shelf VLM scores. The paper demonstrates that state-of-the-art results can be achieved with a frozen VLM, while the training compute requirements can be reduced significantly.",
            "strength_and_weaknesses": "Strengths:\n- The proposed approach is simple, but highly effective considering the reduced training compute requirements. The paper shows a novel way how to leverage pre-trained VLMs for OVD. I think the reduced compute requirements are interesting to the research community, particularly for institutions with limited compute budgets.\n- The comparison with prior works is good; using standard benchmarks in a fair setting.\n- I think the ablation studies are good; I like the summary of findings where details and tables are put into the appendix.\n\nWeaknesses:\n- Given that the VLM is frozen, I think the paper would benefit from evaluating different choices of the VLM, beyond the for ResNet-variants in CLIP. For instance, ALBEF [A] or BLIP [B] are publicly available.\n- Table 4 and the corresponding analysis about the training compute requirements is great, but I would find it much better to make this analysis more prominent in the paper (instead of the last item after ablation studies). After all, this is one of the key arguments this paper tries to convey: keeping the VLM frozen, you only need to train detector-specific parameters and still get good results with significantly reduced training compute. While I expect future methods to achieve better results by training the whole model (maybe better finetuning and directly including image-caption data), the training efficiency of this work would still stand.\n- A recent, but concurrent paper from ECCV [C] could be included in the discussion, because it also uses a \"frozen\" VLM, but for creating pseudo labels (the \"fast\" version also applies ROI-align on CLIP-image-encoder features).\n- I understand that the cross-dataset experiment (LVIS to COCO and Objects-365) has been used in prior work, but I do not think this is a great experimental setup to claim any generalization. As mentioned in the paper, the overlap of categories is high ... also evident from the chosen hyper-parameters (beta = 0). First, I would suggest to explicitly state the amount of overlap of the label spaces to make this point more clear. Second, I think it is better to move this experiment into the appendix (the reader can then still see the comparison with prior work), and instead propose novel ways to demonstrate generalization. I like the evaluation on Ego4D. Is a quantitative evaluation possible? An alternative could be the ODinW benchmark [D]. Although the localization ability is not evaluated, one could try to use ImageNet for such label space generalization experiments, with the assumption that the highest scoring bounding box per image is chosen as the prediction.\n- What about the memory consumption for the proposed model? Table 3 does not mention this aspect. Related to that, was the batch size increased compared to the ViLD training settings?\n\nReferences: \n* [A] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. Li et al. NeurIPS 2021\n* [B] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. Li et al. ICML 2022\n* [C] Exploiting Unlabeled Data with Vision and Language Models for Object Detection. Zhao et al. ECCV 2022\n* [D] ODinW benchmark https://eval.ai/web/challenges/challenge-page/1839/overview",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well written and easy to follow. The figures complement the text well.\n- The main contribution can be summarized as leveraging VLMs for OVD in a more effective way than prior works (like knowledge distillation in ViLD or finetuning in RegionCLIP or VL-PLM). I think this is a valid contribution, simple but effective, as stated above.\n- Regarding reproducibility, the authors plan to release code, but also the list of hyper-parameters in the appendix is good.\n- The last paragraph of page 3 mentions that the same image pre-processing is used as in the VLM, CLIP in this case. I assume that the image resizing/cropping to 224x224 is not included. And I assume the input image size is also the reason only the CNN-based backbones of CLIP are used in this work, but not the ViT-based backbones. Is that assumption correct?\n- The text of the detection labels in Figure 3 is too small. Please increase the font size in the detection visualization.",
            "summary_of_the_review": "Overall, I think the paper should be accepted. Although the technical contribution is simple, the proposed framework is effective and gives state-of-the-art results. The paper is well written and the authors plan to release code.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3237/Reviewer_e3Lv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3237/Reviewer_e3Lv"
        ]
    },
    {
        "id": "7fJBc6HizFg",
        "original": null,
        "number": 2,
        "cdate": 1666497580324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666497580324,
        "tmdate": 1670644441638,
        "tddate": null,
        "forum": "MIMwy4kh9lf",
        "replyto": "MIMwy4kh9lf",
        "invitation": "ICLR.cc/2023/Conference/Paper3237/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents F-VLM, which tackles open-vocabulary detection based on frozen pre-trained vision-language models (VLMs). The authors observe that frozen VLMs retain locality-sensitive features and are strong region classifiers. By only training a detection head upon frozen VLMs and utilizing regional VLM features for open-vocabulary classification at inference time, the F-VLM achieves SOTA on LVIS benchmark as well as competitive results on COCO benchmark and cross-dataset transferring. In addition, F-VLM shows compelling scalability and achieves significant training speed-up and computing saving. ",
            "strength_and_weaknesses": "## Strengths:\n1. The paper is well-written and easy to follow \n2. The proposed method is simple, fast and computing-saving for training, free-from knowledge distillation and weakly supervised learning,  while effectively exploiting the VLMs\u2019 ability for open-vocabulary region classification. \n3. The paper provides detailed ablative studies on the F-VLM\n\n## Weakness:\n1. The performance of the proposed method is inferior to existing methods under fair comparison. E.g. \u201cR50 Comparison\u201d in Table 1, Table 2, and \u201cF-VLM-R50 (Ours)\u201d in Table 3. Propmt optimization and SoCo pre-training are not orthogonal to F-VLM because it seems that these two strategies seem to be not directly applicable to F-VLM (F-VLM uses fronzon VL models), while DetPro can scale its methods to use ViT-L or R50x64. The paper also adopts heavier data augmentation and longer training schedule. \n2. The insights are limited. Basically, the paper only finds a way and tells users that using a frozen VL model in the detector could achieve compatible results with scalability. But the paper could discuss more the effects of VL models, especially those of different publically available pre-trained VL models.\n\n## Questions:\n1. The training recipe (Implementation Details at Sec 4, page 6) of the detectors in the paper is not affordable and thus not widely adopted in the community (large-scale jittering, image size 1024x1024 and batch size 256) and might be unfair for comparison. What about training on a regular 12-epoch or 36-epoch schedule with batch size 16?  \n2. The ablation study on score fusion in Sec 4.3. What about classifying regions solely on the RoiAligned features or solely on the embeddings from the detector head? \n3. In the 6th row of the 2nd paragraph of Sec 3.4, what\u2019s the meaning of \u201cbut train the detector head with $R(\\cdot)$ online in a single stage\u201d? From what the context described, the RoiAlign $R(\\cdot)$ mentioned here is only used at inference time.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nGood, the main contribution and ideas are clearly delivered\n\n## Quality\nThe paper is well accomplished and the claims are well supported.\n\n## Novelty\nThe idea can be treated as a more elegant combination of a traditional detector and a frozen vision language model for region classification. From this perspective, a natural idea is to share the vision feature extraction process, i.e., to adopt the frozen vision encoder as the backbone.\n\n## Reproducibility\nAlthough the implementation details are provided, the settings in not affordable and applicable to most of the university labs (large-scale image input that cost large GPU memory with a long training schedule and large batch size that requires many GPUs). Therefore, reproducibility is questioned.",
            "summary_of_the_review": "Overall, the paper explores a new direction that uses frozen vision-language models for detection and provides useful experimental results which reveal the potential and scalability of this direction. The unfair comparison might be an issue and it could be better if the paper modulates its claims.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3237/Reviewer_x64o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3237/Reviewer_x64o"
        ]
    },
    {
        "id": "TnR8Vvi6ByM",
        "original": null,
        "number": 3,
        "cdate": 1666552314566,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666552314566,
        "tmdate": 1666552314566,
        "tddate": null,
        "forum": "MIMwy4kh9lf",
        "replyto": "MIMwy4kh9lf",
        "invitation": "ICLR.cc/2023/Conference/Paper3237/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to use a frozen vision and language pre-trained model (CLIP) for open-vocabulary object detection. The image encoder from CLIP is used as the backbone and the textual encoder from CLIP is used for region proposal classification. A trainable detector head is added to generate region proposals (for localization), which are then classified into a set of base and novel categories.",
            "strength_and_weaknesses": "Strengths:\n1. The overall framework is conceptually simple yet effective. Directly using a frozen pre-trained vision and language model is easier to deal with than performing knowledge distillation. And also the training cost is significantly lower than others because the entire CLIP model is frozen. It is inspiring for future work.\n\n2. Strong quantitative results are reported on LVIS and COCO. Qualitative cross-dataset generalization results on Ego4D are also shown.\n\nWeaknesses:\n1. There are four hyper parameters to tune in the proposed approach: $\\tau$ in Eq.(2), $T$ in Eq.(4), and $\\alpha$ and $\\beta$ in Eq.(5). They can be tuned on a validation set. But in real-world use, it is hard to carefully tune them all.\n\n2. The paper claims that a bronze VLM \"retains the locality-sensitive features necessary for detection\". But only qualitative results are shown in Fig. 1 and 4. The paper implicitly assumes that the localization module (regional proposal generation) can generalize well from base categories to novel ones with quantitative verification. To validate this assumption, we can check the recall of the model on both base and novel categories. ",
            "clarity,_quality,_novelty_and_reproducibility": "The overall paper is clearly written and easy to follow.\n\nRegarding novelty, I'll rely on my fellow reviewers' comments.",
            "summary_of_the_review": "Overall, this paper provides an appealing approach for open-vocabulary object detection. I'll update my rating according to other reviewers' comments and the authors' responses.\n\nI'd like to encourage the authors to further address the following items (they are not the weaknesses, or at least not unique in this paper, so I didn't list them above).\n\n1. According to Eq.(3), the RoIAlign operator is used in both the training and inference phases. Adding the illustration of RoIAlign in Fig. 2(a) will probably make it clearer.\n\n2. For comparisons with different models, especially the system-level comparison, it may be a good idea to list the number of parameters of different models. So we can clearly see the relationship of the models' capacity and their accuracy.\n\n3. The qualitative results of cross-dataset generalization is great. But the quantitative evaluation is not a \"in-the-wild\" setting. Basically, the region classification is dependent on the cosine similarities of a region's feature w.r.t. to a set of candidate categories. What if the vocabulary of the novel categories is very large with a lot of class names that are not available in an image? Would the accuracy of the model significantly decrease as more and more such distracting categories added? It would be great to see such comparisons (it does not require re-training the model).\n\n4. Only certain values of $\\alpha$ and $\\beta$ values are studied in Table 6. Is it possible to report more ablation results and visualize them using plots?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3237/Reviewer_kxyx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3237/Reviewer_kxyx"
        ]
    }
]