[
    {
        "id": "Sy2qtIXlUSE",
        "original": null,
        "number": 1,
        "cdate": 1666267779569,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666267779569,
        "tmdate": 1666267779569,
        "tddate": null,
        "forum": "QZfdDpTX1uM",
        "replyto": "QZfdDpTX1uM",
        "invitation": "ICLR.cc/2023/Conference/Paper3333/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work propose to train TPP in a meta learning framework and treat each sequence as a different task via the neural process. The work defines the context dataset, target input and output for TPP model. Also, the work introduces the cross-attention module to introduce local history matching to learn more informative features. \n",
            "strength_and_weaknesses": "Strength: Learning TPP in a meta learning framework is interesing and the submission provide the design of the model and training procedure. In general, this paper is easy to follow and I did not find any obvious technical errors despite of some notation problems. \n\nWeakness: The submission provide a detailed description of the encoder part and the training procedure, but almost no description of the decoder. For example, what is the structure of the decoder? How to implement the decoder? If the output of the decoder is the distribution of the next event time or inter-event interval, what distribution did you use? and how did you parameterize the distribution with your decoder? All questions above are not answered in the submission. Furthermore, the decoder takes a random latent variable Z as input, does that mean the output distribution of the decoder is also random? If so, this kind of design contradicts with the TPP model that when the historical information is fixed, the distribution of the next event time or inter-event interval should be determinnistic. \n\nThe design of experiments is also problematic. As stated in the introduction, the goal of meta learning is to improve generalization. To make the experiments convincing, you should demonstrate the meta-learning TPP is better than existing methods on generalization, e.g., training log-likelihood v.s. test log-likelihood. The curret experiment section even did not describe how to split traning and test dataset; and looks more like the submission proposed a more expressive model with lower RMSE or NLL in Table 1,2,3. In conclusion, the experiments did not convince the statement made in the introduction. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The submission provide a detailed description of the encoder part and the training procedure, but almost no description of the decoder (structure, implementation).\n\nNovelty and Quality: The design of encoder and decoder significantly overlaps with previous works, e.g., [Zuo et al.,, 2020], [Lin et al., 2022] and [Shchur et al., 2020]. The only thing that looks new is the meta-learning framework, but this motivation is not consistent with the experimental design.\n\nReproducibility: I did not check the code.",
            "summary_of_the_review": "See strength and weakness. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3333/Reviewer_dU5L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3333/Reviewer_dU5L"
        ]
    },
    {
        "id": "1ULe8hGG8Z",
        "original": null,
        "number": 2,
        "cdate": 1666643776950,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643776950,
        "tmdate": 1668829342047,
        "tddate": null,
        "forum": "QZfdDpTX1uM",
        "replyto": "QZfdDpTX1uM",
        "invitation": "ICLR.cc/2023/Conference/Paper3333/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose to use a meta learning way to train neural processes aiming at one-step prediction of event time of point processes. The proposed TPPs employ a latent variable layer and attention mechanism to improve the prediction performance. The method was demonstrated with benchmark datasets.",
            "strength_and_weaknesses": "Strength:\nUse meta learning training to overcome the generalization issue of supervised learning.\nThe latent variable layer incorporates uncertainty.\nThe attention takes into account the similarity between inputs.\n\nWeaknesses:\nThe meta objective (4) turns out a special case of supervised learning (3). The distinction is blurry.\nThe writing needs to be improved.",
            "clarity,_quality,_novelty_and_reproducibility": "Several definitions and concepts need clarification\n- As mentioned, the meta learning learns the distribution of label $y$, $p_\\theta(y | x, C)$. It is unclear how $p_\\theta$ is specified in this study, especially in (4). What do the authors mean by \"learn\" the distribution in contrast to that it appears also in the supervised learning objectives. What's the difference?\n- What's the output of the decoder? According to the preliminaries, it's a point estimate/prediction of the label s.t. $f_{\\theta}(\\cdot, C): x \\rightarrow y$. However, it's drew like a distribution in Fig 1.\n- The text says, \"In inference, as we do not have access to $C_L$ at $i$-th event when $l < L$, we use $z$ from $p_\u03b8(z | C_l)$.\" Following this, would the KL term in (7) become $0$?\n- The model involves latent variable. Sampling relies on its posterior. The how is the testing done for a trained model on new sequences as you don't have the posterior for them.\n- Information about how RMSE and NLL calculated is needed, like what likelihood is used, how point prediction is obtained if the output is a distribution.\n\nQuality:\n- It's better to show the performance of a null model along with the other methods, e.g., the ones using median interevent interval to predict future event time. It's possible, even not expectedly, that no methods do well for certain dataset.\n- It's better to describe how the error bar is calculated in the text or caption.\n \n\n",
            "summary_of_the_review": "This work uses a meta learning to train variant TPP aiming at one-step prediction of event time of point processes. The demonstration with a variety of benchmark datasets shows competitive performance against other SOTA methods. There are a few key concepts and details need to be clarified to make the work a better understandable and solid one.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3333/Reviewer_AxMo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3333/Reviewer_AxMo"
        ]
    },
    {
        "id": "RzMNqD3-kV",
        "original": null,
        "number": 3,
        "cdate": 1666665836108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665836108,
        "tmdate": 1669976468845,
        "tddate": null,
        "forum": "QZfdDpTX1uM",
        "replyto": "QZfdDpTX1uM",
        "invitation": "ICLR.cc/2023/Conference/Paper3333/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors suggest viewing the problem of predicting the next event for temporal point process models as a meta-learning problem. In particular, they advocate a neural process framework, whereby windows of previous event times act as context and target input sets. A cross-attention architecture is suggested to increase modeling capacity that does not scale quadratically in the number of events. The method seems to outperform supervised point process models empirically. \n\n\n----\nUpdate following authors' responses:\nThe presentation and clarity has been improved in the updated manuscript. The authors have also included additional experimental work that improved the experimental validation of the proposed method. I have increased my score to a weak accept.\n\n---",
            "strength_and_weaknesses": "Strengths:\n- The meta-learning framework is new for point process models, as far as I am aware. \n- The method appears to perform better than point process models trained in a supervised manner.\n\nWeaknesses:\n- I feel that the presentation could be improved at some points (see below). \n\nActionable feedback:\n- The equations (5) to (7) are a bit unclear to me. Why are we using a variational lower bound based on an expectation with respect to $p_{\\theta}(z|C_{L})$? Is this still a valid lower bound if we take (as I understand you do, while dropping the KL term?) using samples from $p_{\\theta}(z|C_{l})$?\n- How are the context features $r_1, r_2,$ \u2026 computed/encoded?\n- How exactly does z enter the decoder. What are the priors for z given different context sets? \n\nComments:\n- Can this be extended to multi-dimensional/marked point process setting where there is not just a single task to predict?\n- I was wondering if the presence of periodic patterns in the data somewhat makes the required permutation invariance over a context set a constraint that might decrease the forecasting performance?\n- Can this method be applied to meta-learning short sequences, e.g., setups like in Xie et al. Meta Learning with Relational Information for Short Sequences, Neuirps 2019?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is largely well written. Some terms/notations could be defined more precisely (see Comments above).\nThe presentation could be more self-contained at some points, e.g., by not just referring to the transformer point process models. \n",
            "summary_of_the_review": "Casting predictions with point process as a neural process is new, as far as I am aware. I feel there is scope to improve the presentation and clarity and I would consider increasing my score if this were addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3333/Reviewer_YAR9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3333/Reviewer_YAR9"
        ]
    },
    {
        "id": "goHZxksFeab",
        "original": null,
        "number": 4,
        "cdate": 1666986789516,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666986789516,
        "tmdate": 1669136567632,
        "tddate": null,
        "forum": "QZfdDpTX1uM",
        "replyto": "QZfdDpTX1uM",
        "invitation": "ICLR.cc/2023/Conference/Paper3333/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a meta-learning framework for temporal point processes (TPPs). The paper points out a limitation of the existing TPP models - treating all event sequences in the dataset as realization of the same process. The proposed meta-learning approach addresses this limitation and leads to improved performance compared to other neural TPP models, as shown by the experiments. More specifically, the proposed model uses a cross-attention encoder (based on [attentive neural process](https://arxiv.org/abs/1901.05761)) to extract the necessary information for predicting the tiem of the next event in the sequence.",
            "strength_and_weaknesses": "- Strong performance: The proposed approach shows a consistent improvement over the baselines in terms of event prediction capabilities.\n- Extensive empirical evaluation: The experiments cover a large selection of datasets and tasks. These include both standard results on predictive performance (event time prediction, log-likelihood), as well as experiments designed to shed light on the properties of different methods (effect of missing events, distribution shifts on performance; effect of model size)\n\n\nWeaknesses:\n- Clarity: Some aspects of the proposed approach are hard to understand or are not described in full detail. The paper would benefit greatly if these are clarified in the revised version.\n    - It would be helpful to explain the mapping between the variables $\\mathcal{X}, \\mathcal{Y}$ in the meta-learning problem definition and $\\tau_i$ in the TPP problem definiton early in the paper, and then stick to the latter notation. Currently, section 3.1 is somewhat hard to follow with the switches between two notations.\n    - How are the context features $r$ defined for TPP models?\n    - Most notation in figure 1 ($r$, $G$) is only introduced a few pages later in the paper, making it hard to interpret the figure the first time it's encountered.\n    - What is the interpretation of the global feature $G$ and the latent variable $z$ in the context of TPP modeling (section 3.2)?\n    - Experiments in table 1 include accuracy scores (probably, for mark prediction?), but the rest of the paper only discusses unmarked TPPs.\n    - $G$ denotes the context set in the beginning of page 4 instead of the $\\mathcal{C}$ notation.\n    - How is the NLL computed for the proposed Attentive TPP model? Equation 5 seems to imply that the NLL is intractlible, and only ELBO is available.\n    - How exactly are the histograms in figure 3 obtained? As far as I understand, the models produce a one-step-ahead prediction of the inter-event times. How are these converted to the histograms? Do we predict the next inter-event time $\\tau_{i+1}$ and then feed in the actually observed value of $\\tau_{i+1}$ to then generate the prediction for $\\tau_{i+2}$, etc.?\n\n----\nPost-rebuttal update: The authors have addressed all of my concerns and updated the paper accordingly. I have raised my score to reflect this.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & reproducibility: While the experimental setup is described very precisely (except the parts mentioned above), some aspects of the model architecture are not clear.\n\nNovelty: The connection between TPPs and meta-learning has not been explored in earlier works, as far as I am aware.",
            "summary_of_the_review": "The proposed approach is well-motivated and achieves strong empirical results. However, some important aspects of the model are not described very clearly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3333/Reviewer_Jm5S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3333/Reviewer_Jm5S"
        ]
    }
]