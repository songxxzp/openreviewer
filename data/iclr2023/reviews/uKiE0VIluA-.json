[
    {
        "id": "-9TeE0BoVJ",
        "original": null,
        "number": 1,
        "cdate": 1666176989176,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666176989176,
        "tmdate": 1666176989176,
        "tddate": null,
        "forum": "uKiE0VIluA-",
        "replyto": "uKiE0VIluA-",
        "invitation": "ICLR.cc/2023/Conference/Paper321/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors discuss the relationship between specific families of variational inference approaches (hierarchical variational inference) with generative flow networks as introduced recently by Bengio et al. (2021). The relation refers to equality in the expected gradients of the respective objectives. \nThe authors evaluate this theoretical relationship further in a set of three experiments of various domains, discussing both similarities and differences.\n",
            "strength_and_weaknesses": "## Strengths\n- The paper demonstrates a novel connection between VI and GFlowNet approaches\n- All statements and empirical claims are clearly stated and demonstrated\n- A diverse set of experiments in different domains is provided\n\n## Weaknesses\n- A, rather minor, weakness is the lack of a stronger evaluation of the connection into the continuous domain, demonstrating the usefulness of the proposed new point of view compared to common VI approaches. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and can be followed clearly. The discussed connection between the two sets of algorithmic approaches is novel and should lead to interesting new results, especially for continuous domains where the relative performance between them would be an interesting task to explore. The authors are encouraged to further pursue this direction.\n\n\n### Typos\n- p1 last line: hierarhical ",
            "summary_of_the_review": "A well justified, well written, and well evaluated paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper321/Reviewer_taL3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper321/Reviewer_taL3"
        ]
    },
    {
        "id": "FoG3qUc2wkR",
        "original": null,
        "number": 2,
        "cdate": 1666675249062,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675249062,
        "tmdate": 1666675249062,
        "tddate": null,
        "forum": "uKiE0VIluA-",
        "replyto": "uKiE0VIluA-",
        "invitation": "ICLR.cc/2023/Conference/Paper321/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a connection between GFlowNets and variational inference (VI) algorithms for hierarchical variational models (HVMs), demonstrating that special cases of training HVMs via VI are equivalent to training GFlowNets via the trajectory balance (TB) objective. This is interesting because GFlowNets can be shown to automatically perform variance reduction in gradient estimation for reinforcement learning (RL), showing that GFlowNets may be helpful for RL settings where VI would have been used instead.",
            "strength_and_weaknesses": "Strengths:\n- I liked the experiments the authors conducted which compared the empirical behaviors of various training objectives in practice. Understanding the settings in which the objectives differ and figuring out when each method performs better or worse than others is important, and I thought the authors did a good job on this on the synthetic task. It was also nice to see it scaled to slightly harder problems.\n- I thought the connection to HVMs and wake-sleep algorithms was interesting, and could open up new research areas along this direction.\n\nWeaknesses:\n- The paper was a bit hard to follow at times.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity/quality: The paper was a bit hard to follow. I think it\u2019d be helpful if the technical exposition could be smoothed out.\n- Novelty: The connection between VI algorithms and special cases of GFlowNets is novel and interesting.\n- Reproducibility: The submission included code for the DAG and molecule synthesis experiments.\n",
            "summary_of_the_review": "The paper provides a theoretical and empirical investigation into the relationship between hierarchical VI and GFlowNets, outlining when they are equivalent and settings when one should be preferred over the other. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper321/Reviewer_CLT1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper321/Reviewer_CLT1"
        ]
    },
    {
        "id": "zFfO1Oib29",
        "original": null,
        "number": 3,
        "cdate": 1666760321742,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666760321742,
        "tmdate": 1666761915407,
        "tddate": null,
        "forum": "uKiE0VIluA-",
        "replyto": "uKiE0VIluA-",
        "invitation": "ICLR.cc/2023/Conference/Paper321/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the relationship between variational inference and GFlowNet. This work finds out these two algorithms are equivalent in the sense of the expected gradients of their learning objectives. Moreover, they demonstrate the superiority of the GFlowNet on off-policy training.",
            "strength_and_weaknesses": "Strength:\n- The connection between the variational inference and GFlowNet is a new finding. This paper finds the intrinsic property, variance reduction, that makes the GFlowNet stand out. They also verify that GFlowNet is capable of stable off-policy training without importance sampling.\n- This paper supplies a missing baseline result for hierarchical VI algorithms on modeling discrete random variables.\n\nWeaknesses:\n-   As the author mentioned in the conclusion section, this paper only studies the performance of GFlowNet and VI algorithms on modeling discrete random variables (since GFlowNet was mostly studied on such regime), which could be limited considering that VI algorithms are mostly used for continuous variables. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and explains clearly the relationship between variational inference and GFlowNet in certain cases. Moreover, this paper conducted plenty of experiments on hypergrid, molecule synthesis, and DAG generation to verify the superiority of the GFlowNet on off-policy training, which should be reproducible with the provided code. \n\nTypo:\nA typo in page 3, line 7, q(z_1,...,z_n)=q(z_1)q(z_2|z_1)...q(z_n|z_{n-1}) rather than x_1\n\n",
            "summary_of_the_review": "Overall this paper is well-written and provides an interesting connection between GFlownet and VI algorithms, and demonstrate the advantage of GFlowNet on off-policy training. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper321/Reviewer_6mPn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper321/Reviewer_6mPn"
        ]
    }
]