[
    {
        "id": "WNE1AbC9OMv",
        "original": null,
        "number": 1,
        "cdate": 1666644116252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644116252,
        "tmdate": 1669063084989,
        "tddate": null,
        "forum": "DB3BH3arU2Y",
        "replyto": "DB3BH3arU2Y",
        "invitation": "ICLR.cc/2023/Conference/Paper3916/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the Uncertainty Estimation for Node Classification problems and proposes a benchmark together with a technique for the controllable generation of data splits with various types of distributional shift. The proposed benchmark consists of several graph datasets equipped with various distributional shift on which we evaluate the robustness of models and uncertainty estimation performance.\n",
            "strength_and_weaknesses": "Strengths:\n\n1. This paper proposes a new benchmark for evaluating robustness and uncertainty estimation in transductive node classification tasks.\n2. Using the proposed benchmark, the authors evaluate the robustness of various models and their ability to detect errors and OOD inputs\n\n\nWeaknesses:\n\n1. Similarly to the OOD detection in the image classification task, it is not necessary to use the Valid-out samples. I suggest conducting more experiments without Valid-out samples.\n2. One important baseline (Zhao et al., 2020) is missing, which is introduced in Section 1. It is better to compare this baseline as well.\n3. Since this paper focuses on the benchmark dataset, why not consider the MC-Dropout uncertainty estimation method as a baseline to compare? \n4. For Table 1, what is Diff. %? Is that possible to construct different Diff. % for each dataset?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-presented and organized. This paper focuses on benchmark dataset construction. The authors provide an excellent code for experiment result reproduction.\n",
            "summary_of_the_review": "See *Strength And Weaknesses*\n\n------------ after rebuttal ----------------\n\nThe authors' response addressed most of my concerns. But I still suggest adding the MC-Dropout as a baseline in the future version as it is a benchmark paper. I will raise my score from 5 to 6.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3916/Reviewer_6NVp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3916/Reviewer_6NVp"
        ]
    },
    {
        "id": "s-FQtJxYLe",
        "original": null,
        "number": 2,
        "cdate": 1666669988791,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669988791,
        "tmdate": 1666669988791,
        "tddate": null,
        "forum": "DB3BH3arU2Y",
        "replyto": "DB3BH3arU2Y",
        "invitation": "ICLR.cc/2023/Conference/Paper3916/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a benchmark for evaluating the uncertainty estimation in node classification tasks. The benchmark consists of seven common datasets, four problems for evaluating the performance of existing methods, the associated metrics, and a universal data-splitting approach. The key idea of the data-splitting approach is to use node characteristics depending on features or graph structures as splitting factors, leading to different distribution shifts. Experiments show that GPN [1] and NatPN [2] are the state-of-the-art methods for the OOD detection and the misclassification detection, respectively.\n\n[1] Stadler, Maximilian, et al. \"Graph posterior network: Bayesian predictive uncertainty for node classification.\" Advances in Neural Information Processing Systems 34 (2021): 18033-18048.\n[2] Charpentier, Bertrand, et al. \"Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions.\" International Conference on Learning Representations. 2021.\n",
            "strength_and_weaknesses": "Strength:\n\nThe proposed data-splitting approach accounts for the graph structures that existing benchmarks ignore. In the complex distribution shift scenario generated by the proposed approach, most existing methods fail to maintain a high classification performance and consistency of uncertainty estimates with prediction errors.\n\nHowever, my major concerns are as follows.\n1. The authors may want to provide references of realistic datasets which use the proposed PR and PPR splits. Otherwise, the proposed splits may amplify the locality-based and popularity-based biases in real-world applications.\n\n2. Please add some large-scale datasets to the proposed benchmark. The seven datasets in the benchmark have only 2,000 to 35,000 nodes, but the graphs in real-world applications may be large-scale, e.g., ogbn-products [3] and ogbn-mag [3] have more than 1,000,000 nodes.\n3. The authors may want to further analyze or discuss why the Dirichlet-based methods that the benchmark evaluates perform well or poorly on different tasks. This could provide insights for developing new Dirichlet-based methods.\n\n4. The description of ${\\rm PRC}\\_{\\rm oracle}$ needs improvements. In the case that uncertainties of all misclassified samples are different from each other, the value of ${\\rm PRC}\\_{\\rm oracle}$ may not be unique. Which value of ${\\rm PRC}\\_{\\rm oracle}$ do we choose to compute ${\\rm PRR}$?\n\n[3] Hu, Weihua, et al. \"Open graph benchmark: Datasets for machine learning on graphs.\" Advances in neural information processing systems 33 (2020): 22118-22133.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is easy to follow, except for a few typos or mistakes due to carelessness. (1) The \u201cour feature-based\u201d in the last line of Section 3.3 may be \u201cour feature-based split\u201d. (2) Figures 6 and 7 are the same.\n\n2. The proposed data-splitting approach accounts for the graph structures, which is novel.\n\n3. The authors provide the code and proposed benchmark, together with a framework for evaluating a variety of baseline models, Dirichlet-based methods, and ensembling techniques.\n",
            "summary_of_the_review": "This paper proposes a benchmark for evaluating the uncertainty estimation in node classification tasks. The proposed new data-splitting method in the benchmark accounts for the graph structures that existing benchmarks ignore. In the complex distribution shift scenario generated by the approach, most existing methods fail to maintain a high classification performance and consistency of uncertainty estimates with prediction errors.\n\nHowever, the paper is below the acceptance bar of ICLR because the proposed benchmark is not very helpful and insightful for real applications of the uncertainty estimation (please refer to concerns 1-3). Besides, there are some carless mistakes, e.g., Figures 6 and 7 are the same. If the authors can properly address my concerns, I am willing to raise my score.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3916/Reviewer_zVhc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3916/Reviewer_zVhc"
        ]
    },
    {
        "id": "eudF3UBIZA",
        "original": null,
        "number": 3,
        "cdate": 1667535807671,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667535807671,
        "tmdate": 1667535807671,
        "tddate": null,
        "forum": "DB3BH3arU2Y",
        "replyto": "DB3BH3arU2Y",
        "invitation": "ICLR.cc/2023/Conference/Paper3916/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work presents a benchmark consisting of several graph datasets with different distributional shifts to evaluate the robustness and uncertainty estimation of models for node-level tasks.  Furthermore, a series of experiments have been conducted to show the ID/OOD performance gap.  ",
            "strength_and_weaknesses": "Strengths:\n- Distribution shift problem is important to the graph learning community\n- Various distributional shifts are considered \n- Comprehensive experiments have been conducted to show that ID/OOD performance graph and SOA models performance. \n\n Weaknesses: \n- I appreciate the extensive experimental efforts, however, the technical contribution of the paper is limited. \n-  The paper does not spend much time discussing the distribution shifts which is the main contribution of the paper. I would like to see more discussion, motivation, and limitations of the Personalized PageRank (PPR).  Furthermore, the process of computing PPR is not explained well. \n- The majority of the paper is dedicated to uncertainty estimation of nodes, metrics for evaluation, and methods which have been well documented in the literature and are not the main contribution here.  ",
            "clarity,_quality,_novelty_and_reproducibility": "- I am confused about 40% of nodes being used as the OOD test set. \n- I would like to see a comparison of Personalized PageRank with the shifts introduced in Gui et al. (2022). \n- Furthermore, I would like to see datasets from different domains, especially molecular graphs. \n- The code is available and it will be long-lived. However, code lacks documentation. ",
            "summary_of_the_review": "In summary, this paper introduces distribution shifts in data splits, but the lack of technical novelty as well as evidence makes the current paper not strong enough for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3916/Reviewer_fvq7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3916/Reviewer_fvq7"
        ]
    }
]