[
    {
        "id": "POxGlDFY4Wa",
        "original": null,
        "number": 1,
        "cdate": 1666642501314,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642501314,
        "tmdate": 1666658319508,
        "tddate": null,
        "forum": "bvpkw7UIRdU",
        "replyto": "bvpkw7UIRdU",
        "invitation": "ICLR.cc/2023/Conference/Paper2641/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is an analysis of Maude metric which computes the closeness of distribution of human-written text with the distribution of texts generated by generative models using the AUC divergence. In this paper, it is shown that the success of Maude is not because of the AUC divergence rather it is due to the cluster-based approximations of the original distributions using pre-trained LM embeddings. \n\nAuthors show that different divergences based on cluster-based approximations have positive impact on the metric's performance. They do a comprehensive probing on these cluster-based approximations and find out that many syntactic and coherence level features such as sentences-substitutions have higher impact on the cluster assignment of strings rather than surface-level features. These can be good features for evaluating SOTA generative models that mostly generate grammatically correct text while texts are not always syntactically correct and coherent.",
            "strength_and_weaknesses": "The comprehensive analysis of different divergences and estimations of probabilities for various models with different sizes is the major strength of the paper that gives value to such type of analysis-based papers. The order of discussed contents in the paper looks very neat and clear to me specifically for the first three sections.\n\nThere are some minor issues in the experimental section, that are summarized here:\n\nIn string-based approximations, only ngram-based model has been considered, while to have a complete analysis it would be good to consider other type of generative models such as BART even though its decoder is GPT but still can be informative and good for comparison.\n\nIn section 5.1, authors have shown that cluster-based approximations are not accurate estimators for original distribution, thus the question that arises here is that why in section 5.2 it is shown that even though they are not accurate using them have good impact on different divergences and consequently the metric's performance. Some further explanations can be beneficial.\n\nIn section 5.2, different string-based and cluster-based approximations have been compared, the Monte Carlo estimations for specific divergences (forward, exp)  can also be useful to be involved.\n\nOne interesting experiment that can be added to the paper is the comparison between the inductive bias that can be added by different approximations for divergences to show the pros and cons of each approximations for the divergences.\n\nIn cluster-based approximations only last word embeddings are leveraged, is there any reasons behind it.\n\nI strongly suggest to use shapes for plots to be readable in any modes.\n\nBased on results in Figure 2, is there any probing of why substituting string-based approximations with cluster-based approximations in AUC divergence results in lower correlation enhancement specifically in interestingness evaluation of texts.\n\nTwo motivation questions in section 5 is not very intuitive. Some explanation can better lead the reader to the next experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is about to analyze one successful prior metric's performance and to investigate what is the main reason of its success. This kind of papers which pay attention mostly to the analysis and conducts different experiments to probe the problem in details can be useful and indeed have been less paid by the researchers in evaluation field.",
            "summary_of_the_review": "In overall the motivation, research problem and conducted analysis are written quite fluently, however there are some not clear parts that have been mentioned in the weakness section. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2641/Reviewer_H8v4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2641/Reviewer_H8v4"
        ]
    },
    {
        "id": "smjKPEu72O",
        "original": null,
        "number": 2,
        "cdate": 1666653737265,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653737265,
        "tmdate": 1666653737265,
        "tddate": null,
        "forum": "bvpkw7UIRdU",
        "replyto": "bvpkw7UIRdU",
        "invitation": "ICLR.cc/2023/Conference/Paper2641/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The current paper focuses on revisiting one of the state-of-the-art NLG evaluation metrics, namely MAUVE. This paper proves that the good performance of MAUVE is coming from the cluster-based approximation it used but not the AUC divergence. Additionally, this paper conducts a probing experiment to understand what linguistic information has each cluster-based evaluator learnt.",
            "strength_and_weaknesses": "Strength:\n1. This paper revisits one of the STOA NLG evaluators either theoretically or empirically. This explains which component of MAUVE really works and helps the future development of NLG evaluation research.\n2. Probing experiment helps readers understand MAUVE better.\n\nWeaknesses:\nI found no major risk in accepting the present paper. I have only one minor concern: I somewhat feel the probing results are not in line with the correlation analysis. Concretely, from figure 4, it appears that different divergence measures are sensitive to different linguistic information. For example, Exponentialed is sensitive to p_{rand} while AUC is sensitive to p_{swap}. However, they all have similar correlations with human scores. My understanding is that this is might be a result of the limitation of the human scores you used. More specifically, the probing experiments target syntax and coherency related information, while human scores in Figures 4 (i.e., interestingness, sensibility, and humanlikeness) are all from pragmatic aspects. More explanation is welcome here.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is novel but the clarity needs further improvements. For example:\n1. in section 5.2, it is better to briefly introduce what the generation task is and how it was evaluated.\n2. in the probing experiments, it is better to clearly indicate which modification is aiming at probing which linguistic information.",
            "summary_of_the_review": "The paper is generally is in a very good shape and definitely should be accepted, but there are still some minor issues that are yet to be clarified or explained.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2641/Reviewer_mdcX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2641/Reviewer_mdcX"
        ]
    },
    {
        "id": "CHOmVqyZUO",
        "original": null,
        "number": 3,
        "cdate": 1666668558776,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668558776,
        "tmdate": 1666668558776,
        "tddate": null,
        "forum": "bvpkw7UIRdU",
        "replyto": "bvpkw7UIRdU",
        "invitation": "ICLR.cc/2023/Conference/Paper2641/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes the performance of the recently-introduced and influential MAUVE metric for evaluating the quality of automatically-generated text by comparing it to human-generated text at the distribution level. It begins by setting out the rationale for the MAUVE approach, starting with difficulties in applying standard divergence measures to this particular evaluation problem and motivating the use of surrogate distributions such as MAUVE\u2019s embedding-cluster-based multinomials. It then demonstrates that these surrogates provide a biased approximation to the true KL divergence (an ingredient in the AUC divergence used by MAUVE).\n\nExperiments show that cluster-based surrogates nevertheless have much better correlation with human judgments than do ngram LMs. This finding is independent of the particular divergence metric used, implying that the use of clusters rather than AUC divergence is the key factor in MAUVE\u2019s performance. Further experiments probe the nature of the clusters, finding that they correspond to sentiment, but only weakly to style and topic. Finally, when comparing perturbed and natural human samples using the cluster approach, surface features such as articles and punctuation are found to be largely irrelevant.",
            "strength_and_weaknesses": "Strengths:\n\n1. Very well organized and clearly presented.\n\n2. The step-by-step rationale for the MAUVE approach (and variants), although not novel, is extremely thorough and provides useful structure for organizing work in this area.\n\n3. The finding about the central utility of LM-derived clusters is very significant, given the prominence of the MAUVE paper. This is something that needs to be publicized, and that will stimulate further research in a crucial area.\n\n4. The experiments into the nature of the clusters yield interesting insights, particularly that metrics like MAUVE are relatively insensitive to surface features, and might be vulnerable to gaming as a result.\n\nWeaknesses:\n\nThe evaluation is performed only with variants of GPT-2. It would be interesting to see whether the results carry over to more recent models such as GPT-3 or PaLM.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See strength and weaknesses.\n",
            "summary_of_the_review": "The paper is technically very strong, and it presents important experimental results in the key area of quality evaluation for text generation by LLMs. The insights into how MAUVE works need to be publicized.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2641/Reviewer_X7E3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2641/Reviewer_X7E3"
        ]
    },
    {
        "id": "F-d6EplwxgW",
        "original": null,
        "number": 4,
        "cdate": 1666823656625,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666823656625,
        "tmdate": 1666823656625,
        "tddate": null,
        "forum": "bvpkw7UIRdU",
        "replyto": "bvpkw7UIRdU",
        "invitation": "ICLR.cc/2023/Conference/Paper2641/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper is about evaluation of open-ended generation of LMs, in particular the Mauve (pillutla et al.) metric.\n \nEvaluation of open ended text generation takes a distributional format. It is framed in this paper Divergence(p_w, q_w). This task has two sub-tasks, (1) Density estimation of p_w and q_w (when necessary). (2) calculating a distribution divergence metric of choice and -if necessary- approximate p_w or q_w to calculate this divergence metric tractably.  \n\n\nThe paper starts with laying a common formalism to compare these divergence metrics and notably revisits the AUC divergence metric used in the Mauve metric. This metric is intractable to calculate in practice and therefore P_true and P_model are replaced by multinomial probability over clusters of sentence embeddings using an external encoder. \n\nThis paper is a critique of the Mauve metric both theoretically and empirically, mostly the replacement of p_w and q_w with its clustering based approximation:\n- Section 4: it shows that the approximation kl(p_w,q_w) to kl(p_c,q_c) is biased\n- Section 5.1: it shows empirically that string based density estimations (using basic n-gram lm) of p_w and q_w correlations better with original data distribution than their cluster based counter parts\n- Section 5.2: Despite the results shown in section 5.1, authors show that divergence metrics correlate *correlate better with human judgements* when cluster based density estimation is used as an approximation. \n- Section 6: by probing the clusters authors show that clustering based on sentence embeddings (the core component of the clustering based density estimation approx of mauve) are more oriented to capture global features such as sentiment, authorship and topic and less sensitive to surface modification which is probably why they correlates with human judgements overall since most recent language models don\u2019t suffer from disfluency,  although this could be a way to game the mauve score\n",
            "strength_and_weaknesses": "Strengths:\n- The paper is well written the related work is quite educational, instead of listing papers authors make the effort of laying a theoretical ground for comparing difference divergence metrics and ways to approximate their intractability.\n- The paper shows theoretical and empirical drawbacks of the mauve score, and show why it does work in practice \n- I find the findings from section 5.1 and 5.2 to be interesting, mainly that methods that correlate with the real probability distributions $\\hat{p}_w$ and $\\hat{q}_w$ are not necessarily what is best to correlate with Human judgements $\\hat{p}_c, \\$hat{q}_c. This opens more research questions and could have made this paper have a wider scope.  \n\nWeaknesses:\n\nThe scope of the paper is quite narrow, and solely serves as a critique to the Mauve score paper which was published last year and yet not so widely adopted in LM evaluation. It is not clear how those conclusions could inspire future works for language model evaluation. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, the proofs and formulations are quite correct to my best judgment.\n\nDocumentations of experimental work in section 5 and 6 could have used more clarity and being specific about experiments details, not only this can help in reproducibility but also help judging the correctness of the results in the experiments.\n  \n",
            "summary_of_the_review": "The paper overall serves as a good critique of the mauve score, empirical and theoretical findings in the paper seems to be solid, however the overall scope of the paper is quite narrow and focuses only on critiquing one paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2641/Reviewer_X1r9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2641/Reviewer_X1r9"
        ]
    }
]