[
    {
        "id": "2-aP82gBhO",
        "original": null,
        "number": 1,
        "cdate": 1666575312701,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575312701,
        "tmdate": 1669257361996,
        "tddate": null,
        "forum": "eKllxpLOOm",
        "replyto": "eKllxpLOOm",
        "invitation": "ICLR.cc/2023/Conference/Paper1198/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the adversarial robustness of federated learning, which is an important problem when training the model on the client side. Specially, the authors consider the incompatible dilemma between federated learning and adversarial training, and introduce a mediating slack mechanism to better bridge two paradigms. Both theoretical analysis and empirical study demonstrate the effectiveness of the proposed methods. ",
            "strength_and_weaknesses": "In a nutshell, the authors investigate an interesting problem motivated by the degeneration observation, and the proposed method is solid with sufficient evidences. However, there are still some concerns regarding the submission as follows,\n(1)\tAlthough the combination of two paradigms lead to the degeneration in performance, the reason that induces this phenomenon needs to be carefully discussed, since all toy studies even including the analysis on real-world dataset are from the perspective of the observation, which cannot attribute to the real cause. For example, it is not absolute in Figure 2 where the generated adversarial examples move in the clockwise direction, instead of the converse direction. I advise the authors should properly express the conjecture in the motivation part.\n(2)\tEquation (1) introduces a lower bound to the original objective, which is different from the general way that constructs an upper bound to minimize. Therefore, it is not necessary that minimizing a lower bound can lead to the minimization of the original objective. The authors should discuss more here and give some empirical validation if it is possible. \n(3)\tThe background part gives some basics of federated learning and adversarial training, while the review of current works is insufficient. I advise the reviewer should follow the conventional style and separate this part into related work and preliminary, respectively for literature review and basics.\n(4)\tIn tables 1-4, the natural accuracy and the robustness performance in the centralized counterpart can be validated as references, and the gap in comparison to the achieved performance in federated learning framework should be reviewed so that the readers can deeply understand the limitation in this topic.\n(5)\tAgain, I think the explanation about the problem in the combination of federated learning and adversarial training should be improved. Possibly, if the authors can find some supporting examples from the experiments as the visualization or some statistical analysis can make the claim more convincing. I advise the reviewers to trace the classification difference between the baseline and the SFAT-enhanced counterpart and analyze the common characteristics of these non-overlapping examples. \n(6)\tThe discussion about the systematic challenges in federated adversarial training is limited, yielding that it is hard to evaluate the value of this narrow direction. More analysis about the practical requirement in the real-world applications should be given both in the introduction and related work as well as the confusion remark.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors really conduct an impressive work and give a range of experiments and the corresponding theoretical analysis to demonstrate the advantages of the relaxing mechanism.",
            "summary_of_the_review": "My main worry is not about the observation and the proposed method, but about the motivation description and analysis is lack of some sufficient justification. Please carefully consider above concerns and improve the submission, and I can consider to raise the score if they are well solved. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1198/Reviewer_YWt9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1198/Reviewer_YWt9"
        ]
    },
    {
        "id": "T-7ZVV9vl9",
        "original": null,
        "number": 2,
        "cdate": 1666662298320,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662298320,
        "tmdate": 1666662298320,
        "tddate": null,
        "forum": "eKllxpLOOm",
        "replyto": "eKllxpLOOm",
        "invitation": "ICLR.cc/2023/Conference/Paper1198/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies adversarial training under the federated learning framework. The current challenge in federated adversarial training (FAT) is that robust accuracy deteriorates in the later stage of training.  The authors believe this is due to the intensified heterogeneity caused by the inner maximization step in each client's local training step. To overcome this, the authors proposed $\\alpha$-slack mechanism. To my understanding, this mechanism reweights the parameter update by its adversarial loss. The clients with lower adversarial loss is upweighted by $1+\\alpha$. On the theoretical side, this paper shows the convergence property of the reweighted loss. Empirically, this paper provides extensive experiments and demonstrates the effectiveness of the proposed mechanism.",
            "strength_and_weaknesses": "Strengths: 1. The problem itself is interesting. This paper provides a simple and practical solution.\nWeaknesses: 1.  To my understanding, this convergence guarantee is a variation of the standard federated learning, e.g. (Li et al, 2019) with an $\\alpha$-slack variable. Theorem 3.2 and 3.3 are saying this algorithm is converging faster compared to FAT. This is not related to any claims about better robust accuracy or generalization, which is demonstrated through experiments.  It would be better to provide theoreical results showing better robust accuracy or provide empirical results showing faster convergence, which verifies the theoretical claim.\n\n\nQuestion:\n1. I am confused about the second figure in FIgure 5. It seems that the SFAT robust accuracy also decreases after 40 rounds. How is this figure different from Figure 1 (b)?\n\n2. I understand that SFAT could achieve better overall robust accuracy compared to FAT. But for those clients that have large adversarial loss. My intuition is that these clients are down-weighted and would have worse performance than FAT. Is this true? It would be better to provide some experiments that verifies generalization performance through clients.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the overall writing is clear and easy to follow. It would be better if the authors could provide more interpretations of the theoretical result, like how this is related to the experiment section. ",
            "summary_of_the_review": "This paper provides a simple and practical approach to mediate the robustness deterioration issue induced by intensified data heterogeneity.  I am not an expert on this topic so I could not evaluate the significance of the contribution. The effectiveness of the proposed method is thoroughly studied through experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1198/Reviewer_CYPm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1198/Reviewer_CYPm"
        ]
    },
    {
        "id": "T3fGmeHBEG",
        "original": null,
        "number": 3,
        "cdate": 1666711562485,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666711562485,
        "tmdate": 1669241611196,
        "tddate": null,
        "forum": "eKllxpLOOm",
        "replyto": "eKllxpLOOm",
        "invitation": "ICLR.cc/2023/Conference/Paper1198/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries to fix the failure of the direct combination of federated learning (FL) and adversarial training, which is indicated by the deterioration of the adversarial accuracy at the later stage of FL. The authors proposed the so-called \"alpha-slack\" mechanism that upweights the clients with smaller (local) adversarial losses (and downweights the clients with larger losses) during aggregation in the cloud. The authors also show that the proposed mechanism asymptotically converges to the optimal solution of the modified adversarial objective. Extensive experiments are presented to show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength of this work is primally empirical. Firstly it seems to be the first empirical breakthrough in terms of combining FL and adversarial training, with plenty of supporting experiments showing the effectiveness of the proposed method. Secondly, it provides plenty of interesting and inviting observations in multiple dimensions, which I believe would be of interest for the community.\n\nHowever, the technical novelty issue is critical. And the core of the novelty issue is that the proposed method looks simply a modified aggregation method that has nothing related to the adversarial training objective. It perfectly applies to normal FL with only natural samples. Let me be more specific.\n\nFrom the perspective of the motivation, that is discussed in Sec 3.1, it is purely a conjecture/argument and there is neither qualitative analysis or quantitative evidence that actually supports the statement that the local adversarial optimization process indeed \"exacerbate\" the heterogeneity.\n\nFrom the perspective of the algorithm, as I said above, the proposed method simply upweights updates from clients that have smaller local (adversarial) objective. It does not matter if the local objectives are adversarial or not. And the final form of the algorithm is *very* similar to the **FedSoftBetter** aggregation method proposed in [1], which reweights updates from clients using a slightly different metric but also depends on the local objective magnitude. The existence of this prior work, in my opinion, significantly undermines the technical contribution of this work. However, I do recognize that [1] is quite new and is only a preprint work. I am not sure how it should influence the evaluation of this submission. I would love to invite comments from other reviewers and discussion from the authors on it.\n\nFrom the perspective of empirical results, the proposed method seems to be most beneficial in the natural cases in every experimental setting presented in the paper (in terms of final accuracy improvements). This observation further convinces my points above that the proposed method is not specific to adversarial training at all.\n\nLastly, which is more of a constructive comment instead of criticism, the authors only consider constant alpha in this work, which for sure changes the original global adversarial objective. A natural idea is to gradually decrease alpha during FL so that the objective would asymptotically get closer to the real objective. However, this is empirically contrary to the observation that FAT degrades at the later stage of training. So the investigation on this question is worthwhile and interesting.\n\n\nUPDATE: sorry that I forgot to attach the reference above.\n[1] Mansour, Adnan Ben, et al. \"Federated Learning Aggregation: New Robust Algorithms with Guarantees.\" arXiv preprint arXiv:2205.10864 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of this paper is great. For the novelty please refer to my points in the Weaknesses part. Reproducibility is not applicable for now since the authors have not provided the source codes, while they promised to do so in the reviewing phase. I will update then. However, I think the proposed is simple enough and there should be no problem in reproducing the results.",
            "summary_of_the_review": "This work is empirically solid but technically weak because of the clear disconnection of the proposed method and the adversarial setting AND the prior work (FedSoftBetter) which is very similar algorithmically.\n\n====\n\nUpdate: after reading the authors' responses to my review and their discussions with other reviewers, I found my concerns have been mostly addressed. Therefore, I raised my score from 5 to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1198/Reviewer_JVxX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1198/Reviewer_JVxX"
        ]
    }
]