[
    {
        "id": "PI3kzogS8CX",
        "original": null,
        "number": 1,
        "cdate": 1666542408527,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542408527,
        "tmdate": 1666542408527,
        "tddate": null,
        "forum": "x5YkB3b_48o",
        "replyto": "x5YkB3b_48o",
        "invitation": "ICLR.cc/2023/Conference/Paper3536/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a class of regularization objectives that are used to improve the effectiveness of parameter-efficient tuning methods (PETs) for pre-trained language models. Specifically, the objective consists of two main components: (i) a (potentially learnable) diffusion bridge defining a target diffusion process; (ii) a mapping function from every layer's output of the language model to a trajectory that approximates the diffusion bridge. This paper uses a fixed diffusion bridge defined by applying PCA to the output word embeddings. Experiment results show that the proposed regularizer is able to improve the performance of existing PETs.",
            "strength_and_weaknesses": "Strength:\n\n- The diffusion bridge forces the latent representation of each layer to contain more information of the target, and can be viewed as a shortcut for learning better latent representations tailored to different tasks.\n\n- The authors demonstrate that task performance is correlated with the distance to the diffusion bridge, which serves as an empirical evidence of the effectiveness of the proposed regularizer.\n\nWeaknesses and questions:\n\n- One weakness of the proposed method is that pre-training corpus is required to train the mapping $g_{\\gamma}$. The problem here is two-fold: (i) pre-training corpus could be hard to access and (ii) since it is much larger than datasets of downstream tasks, training $g_{\\gamma}$ could be computational demanding. For example, I would like to know whether the proposed method is still effective if we only have access to a small subset of the pre-training corpus.\n\n- Following the previous comment, the computation cost of training $g_{\\gamma}$ should be discussed. Furthermore, it would be nice if the authors can provide additional experiments by constraining the GPU time of the baselines and the proposed approach to be the same.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written in general. The proposed regularizer is a clever integration of language models with diffusion bridges of the target output. Training hyperparameters are included in the appendix.",
            "summary_of_the_review": "In summery, I tend to vote for acceptance since the proposed regularizer makes clever use of target diffusion processes to improve the fine-tuning performance. However, the proposed method requires pre-training corpus, which could add significant computation cost. It is also not well discussed how much pre-training corpus is needed to receive significant performance gain.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3536/Reviewer_69ms"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3536/Reviewer_69ms"
        ]
    },
    {
        "id": "HkXDqK-MhrM",
        "original": null,
        "number": 2,
        "cdate": 1667274875473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667274875473,
        "tmdate": 1667276309772,
        "tddate": null,
        "forum": "x5YkB3b_48o",
        "replyto": "x5YkB3b_48o",
        "invitation": "ICLR.cc/2023/Conference/Paper3536/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\n\nThis paper proposes a novel framework to do parameter efficient fine-tuning method for PLM (pre-trained language models). The basic idea is to hypothesize that there exists a map from the hidden states in each layer to a lower dimensional latent space, where the trajectory of this lower dimensional map from the zeroth layer to the final layer is from a diffusion bridge that ends at the PCA reduced dimension points of each output target word in the final layer.  The authors propose an interpretation of this as the running cost to augment the usual final terminal cost of the loss function. \n",
            "strength_and_weaknesses": "Strengths:\n\nThe key idea is to learn a frozen non-linear dimensionality reduction map that recovers the dynamics of a stochastic bridge on the pre-training corpus and then using it in the fine-tuning stage to define an additional regularization cost. Overall, the proposal of using a diffusion bridge with an endpoint derived from the word embedding at the final layer in PLM (a projected version of it) seems novel / interesting.  Justifying the novel fine-tuning objective as a regularization cost seems intuitively plausible, even if the other motivations like considering it as running cost versus terminal cost appear to be a superficial connection. The authors also conduct experiments to show that this scheme can help marginally improve the final results in practice. In addition to evaluating the benchmarks, the authors also conduct some analysis of the hidden states of the various methods in terms of the clustering properties of a reduced dimension visualization.\n\nWeaknesses:\n\n- Clarity of the writing is quite unclear and leaves a lot of details to imagination.  Please see the next section for concrete suggestions.\n-  The authors claim that they \"reemphasize\" a \"running cost\" in addition to the terminal cost (which is simply the model loss), however it is unclear why this running cost is important for the overall objective, especially given that this is a cost that the authors define, rather than inspired or proposed from a pre-existing consideration or loss.  \n- It also seems somewhat arbitrary that this new regularization cost is specific to parameter efficient tuning, but it does not seem specific to this setting (e.g. what aspect of going from general finetuning to PET is specific to the method proposed here?)\n- Optimal control formulation is often mentioned for the motivation, but the connection is limited to that and not really used in the technical contents of the paper.\n- The significance of hidden state in vanilla PETS \"approaching the latent bridges\" is unclear to me. Specifically, given that the final reduced dimension representations of the embeddings are estimated from the pre-trained corpus, wouldn't a better fit to the end points automatically imply a better prediction accuracy? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of interpreting interpreting the tunable parameters are control variables on top of a frozen PLM is not novel to this work, as the authors cite (Yang & Liu' 22, Ding et al. '22). \n\nClarity is good in some aspects, but needs improvements in several other critical components -- for example, the formalism was incomplete as to where the continuous time aspect of the diffusion bridges is entering the regularization framework in the fine-tuning objective. \n\nWhen $\\bar{h}_o^{(i)}$ is defined as \"the average of all hidden states at the $i-$th layer\",  what does this mean? Doesn't that make it independent of the position $o$ if interpreting \"all\" as a reduction over the time steps?\n\nWhat is the $t_i$ in Equations (5) and (6)? Is this just a uniformly spaced grid with length equal to the number of layers? It would be good to make this clear without expecting the reader to guess things. \n\nThe diffusion bridge formalism starts at 0 and then ends at a parameter $\\beta$. What does this map to for the bridge $X_y$ used in the application (just above Equation (5)? Inspite of having a paragraph Section 3.2 on this, this aspect is still very unclear.\n\nThe Method 2 (approximating the SDE)'s description leaves several variables undefined which is concerning given that it is the best performing method in the experiment. Things like $\\sigma$, $f^{T; \\beta}$ ? There is also an $\\tilde{X}_t$, which is supposed to be an estimate that's again undefined. \n",
            "summary_of_the_review": "Overall, this paper proposes an interesting idea for a regularizer for fine-tuning a pre-trained language model (PLM). The most interesting part about this is to use the frozen model to constrain the dynamics of the hidden states to follow a distribution similar to those in the pre-training corpus (via a dimensionality reduction). The authors also allude to optimal control connections for this idea, which is not particularly significant in my view. However, the clarity of presentation is still somewhat unclear as I write above in more detail. \n\nThe experimental results seem promising, but the claims about a novel discovery related to the hidden states forming a latent bridge might not be fully convincing. Overall, the paper has some merits in terms of novelty and motivation, but the technical details and the significance of the empirical results are not totally convincing.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3536/Reviewer_CmgL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3536/Reviewer_CmgL"
        ]
    },
    {
        "id": "ymH78zDeLmI",
        "original": null,
        "number": 3,
        "cdate": 1667501302393,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667501302393,
        "tmdate": 1667501302393,
        "tddate": null,
        "forum": "x5YkB3b_48o",
        "replyto": "x5YkB3b_48o",
        "invitation": "ICLR.cc/2023/Conference/Paper3536/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Since recent pre-trained language models are large, it is common to fine-tune small amount of parameters instead of tuning all the parameters when we would like to adapt the pre-trained model for downstream tasks (e.g. text classification etc.). Many methods have been proposed such as Prompt tuning, Adapter, BitFit and LoRa (summarized in appendix A). For example, Adapter, one of the simplest approach inserts extra layers (called adapter layers) in transformer blocks and train those layers with supervision from downstream tasks.\n\nThe straight forward approach is to directly optimize the downstream task loss, but instead authors proposed a regularization method. The proposed regularization objectives encourage hidden states of each layer to be able to approximate low-rank vector representation (PCA) of target tokens following the diffusion process explained in sec. 2.2 (and parameterized as in sec. 3). The proposed objective is added to the task objectives with some weights.\n\nThey evaluated their regularization method on fine-tuning BERT_large model with four parameter tuning approaches (Prompt tuning, Adapter, BitFit and LoRa) and show that the regularization lead to performance improvements on GLEU benchmarks in both full-set and few-shot scenario.",
            "strength_and_weaknesses": "Strength\n- Since the proposed regularization is not dependent of parameter tuning methods, it can be used for many parameter tuning methods.\n- The experiments shows improvements on GLEU benchmarks in both full-set and few-shot scenario.\n\nWeakness\n- It was not clear what optimal control perspective implies. It's common to use regularization / prior / pre-training in machine learning but the new perspective implies something special (which lead to the proposed method)?\n- The method assumes that the pre-training corpus is available, but previous works do not assume so.\n- The method looked like pre-training of newly introduced parameters (effective for few-shot setup). Is there any justifications why the stochastic bridges is necessary?\n  - What will be the difference between stochastic bridges and pre-training the new parameters on PLM objective?",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper was a little bit hard to read (e.g. eq.5/6 uses function not explained yet) but the idea is simple.\n2. It should be made clear in the tables that proposed methods use more data than the baseline.\n3. The idea to use diffusion bridge looks new but it is not clear why this is necessary because of the lack of baselines.\n4. The authors do not include code and data to reproduce the results.",
            "summary_of_the_review": "Although the proposed method show improvements, it requires extra data to pre-train the parameters than baselines. And it is hard to make fair comparison with existing methods especially for the few-shot setup. Also, the use of stochastic bridges is not justified conceptually and empirically. For these reasons, I believe this paper is not ready for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3536/Reviewer_PqBu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3536/Reviewer_PqBu"
        ]
    }
]