[
    {
        "id": "wECmYSnQSLk",
        "original": null,
        "number": 1,
        "cdate": 1666357382395,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666357382395,
        "tmdate": 1666357382395,
        "tddate": null,
        "forum": "xACeXHo4sf",
        "replyto": "xACeXHo4sf",
        "invitation": "ICLR.cc/2023/Conference/Paper2580/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose Blender, an adversarial reprogramming technique (or hijiacking) that converts NLP samples inside legitimate images of a container dataset to use computational power of the victim model to solve the task decided by the attacker.\nBlender is trained to minimise the summation of two objective functions, namely one that takes into account the fused image and the container one (visual loss), and one that force the learned feature representation of the network of NLP tokens and images to match (semantic loss). The authors show that the hijacking is successful in different combinations of container dataset of text and images, by highlighting that the predictive accuracy is matching the accuracy on the original data.",
            "strength_and_weaknesses": "**Strengths.**\n+ proposing a shift of reprogramming towards different domains is very interesting, since it opens different possibilities (i.e. reroute the computational power of image classifier for other tasks, like threat hunting for instance)\n+ the methodology works by just considering a dataset similar to the one used for training, and its application is realistic\n+ experiments are rich, and they convey the effectiveness of the methodology\n\n**Weaknesses.**\n+ **Not really multi-model.** While the paper focuses on proposing a contribution on the multi-modality of hijacking attacks, the authors only focus on translating an NLP problem to a CV one. While this is not bad per-se, the author should better clarify how such methodology could potentially be general to any domain. One example would be specifying which are the components that need to be implemented in order to port the hijacking task to another domain (audio, malware detection, etc). Moreover, I imagine that there can be domains where hijacking is much more difficult, or much easier. \n+ **Presentation can be improved.** While the paper reads well, the overall presentation could be improved: most of the more interesting figures are put aside in the appendix, while only Fig.2 and 3 are put into the main paper. Also, the paper could contain more examples of NLP sentences / label and corresponding fused images to better understand the effects of the techniques (Fig.3 only contains the images, but not the predicted class of the fused image and the ground truth of the NLP sample)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and the ideas are novel\nReproducibility can not be assessed since no code was submitted.",
            "summary_of_the_review": "Hijacking can be applied cross-domain, in particular between NLP and CV tasks by creating a model that learns how to apply the translation, which is the core contribution.\nHowever, the authors should better discuss such cross-domain generalisation, and help the reader understand how this can work on other different couples of domains.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2580/Reviewer_GKEm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2580/Reviewer_GKEm"
        ]
    },
    {
        "id": "Q5ZY4mUlMU",
        "original": null,
        "number": 2,
        "cdate": 1666592611954,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592611954,
        "tmdate": 1669081420573,
        "tddate": null,
        "forum": "xACeXHo4sf",
        "replyto": "xACeXHo4sf",
        "invitation": "ICLR.cc/2023/Conference/Paper2580/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "They proposed a transformation to multimodal extension from the model hijacking attack by Salem et al. (2022a). This work takes a data poisoning approach while the fused dataset is used to poison a victim model. The evaluation metrics are the attack success rate and utility, where both metrics are important to attain simultaneously to hijack the victim model successfully.\n",
            "strength_and_weaknesses": "#### **Strength**\n\n- It generalizes the previous unimodal setup, the model hijacking attack (Salem et al., 2022a), to the modal hijacking attack.\n- The extensive studies on hyperparameters and design choices in Sec. 4.4.\n- It is appreciated to include the discussion on a defense strategy and its experimental results.\n\n#### **Weakness**\n\n- **W1. Simple baselines.** For the model comparison, this work does not provide the performance of baselines excluding the adapter-only blender (Sec. 4.4). One simple baseline would be the container samples having random labels without decoding. This baseline would show the blender's effectiveness, excluding the effect of randomly-assigned labels. The other baseline would be training with original and container samples with ground-truth labels (or nearest neighbor labels). This baseline gives the hijacking ability only with the data distributional changes since the adversary would attack with the container distribution (Sec. 3.4).\n\n- **W2. Bi-model and unidirectional.** This work only provides the experiment on bi-modal unidirectional settings. Can this model hijack an NLP-based target model with a CV-based task?\n\n- **W3. The motivation of NLP-based tasks.** The hijacking dataset is randomly mapped to a container image. Why don't we semantically map exploiting NLP-based tasks? For example, the hijacking sample is mapped to the nearest neighbor label in the common embedding space (while the labels are embedded to the same space)?",
            "clarity,_quality,_novelty_and_reproducibility": "#### **Clarity**\n\n- **C1. Evaluating metric.** In Sec. 4.2.2. the description of *attack success rate* is hard to follow. How do you define *a clean testing dataset (for the hijacking task)*? What do you mean by *train an NLP classification model for each hijacking task*? What are the input, output, and target (label) of the NLP classification model? How do you compare the performance with the victim model on the hijacking task, and with whom? This subsection deserves to be more specific and accurate for readability and assessing the merit of the work. *It would be helpful to provide the revised version of this subsection to reassess this issue.*\n\n- **C2. Vague definition.** In the first two paragraphs of the introduction, the definition of the model hijacking attack is unclear. Figure 1, with a shallow description, is frustratingly hard to understand.\n\n\n#### **Quality**\n\n- In-text citation is wrongly used for the parenthetical citation.\n\n#### **Novelty**\n\n- Compared with Salem et al. (2022a), this work seems to be incremental to multimodal hijacking attacks while the modality of the hijacking sample is changed along with the corresponding encoder. The authors should provide notable speculations or discussions in the extension to multimodal setting, in addition to the argument that it generalizes unimodal tasks. If the adversary wants to attack, why do they want to attack with the other modality if it underperforms the same modality attacks?",
            "summary_of_the_review": "The motivation of this work (W2, W3) and clarity (C1, C2) issues hinder recommending this paper to the conference. The comparison with more diverse baselines (W1) would strengthen the validation of the idea.\n\n---\nAfter reading the author's feedback, the shared concerns of the lack of justification and effective validations remain. I will hold the current evaluation.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2580/Reviewer_dqpP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2580/Reviewer_dqpP"
        ]
    },
    {
        "id": "lBjQphd3mBx",
        "original": null,
        "number": 3,
        "cdate": 1666630883541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630883541,
        "tmdate": 1666630883541,
        "tddate": null,
        "forum": "xACeXHo4sf",
        "replyto": "xACeXHo4sf",
        "invitation": "ICLR.cc/2023/Conference/Paper2580/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a technique to enable model hijacking attacks for data with different modalities. Specifically, it makes the hijacked model predict a image-related label to a given text.",
            "strength_and_weaknesses": "Strength:\n- The paper investigates an exciting setting for model hijacking attacks.\n\nWeaknesses:\n- At a technical level, the proposed technique is similar to the original model hijack attack on images proposed by Salem et al. Both employ an encoder-decoder structure and similar ideas of making the fused sample close to the hijacking sample in the feature space. The technical novelty needs to be further justified. \n- The detectability of the proposed attack is not sufficiently evaluated. In fact, there have been many recent advances in detecting poisoned instance; to name a few, [1], [2], [3]. It is possible that the poisoned instances can be easily detected in the frequency domain.\n\n[1]: Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. NeurIPS 2022.\n[2]: Not All Poisons are Created Equal: Robust Training against Data Poisoning. ICML 2022.\n[3]: Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective. ICCV 2021. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing clarify and quality are great. The novelty can be better justified.",
            "summary_of_the_review": "Overall, the paper studies an interesting attack setting for model hijacking attacks ---how to use the victim model from a domain to serve tasks in another domain. However, the paper can be further improved by clarifying the technical novelty, i.e., the difference from Salem et al and why these differences are significant. Moreover, the detectability of such an attack needs to be further discussed. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2580/Reviewer_2T2w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2580/Reviewer_2T2w"
        ]
    },
    {
        "id": "g4k7I-7Nyd",
        "original": null,
        "number": 4,
        "cdate": 1666658414954,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658414954,
        "tmdate": 1666658414954,
        "tddate": null,
        "forum": "xACeXHo4sf",
        "replyto": "xACeXHo4sf",
        "invitation": "ICLR.cc/2023/Conference/Paper2580/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors transform the model hijacking attack into a more general multimodal settings, where the hijacking and original tasks are performed on data of different modalities. Specifically, they focus on the setting where an adversary implements a natural language processing (NLP) hijacking task into an image classification model. Their evaluation results show the high effectiveness of their attack.",
            "strength_and_weaknesses": "Strength:\n- Propose the first model hijacking attack in which the adversary can hijack a CV-based targeted model by an NLP-based task\n- The evaluation shows the high effectiveness of their attack and include multiple design choice in evaluation\n\nWeakness:\n- Lack of the justification about the multimodal settings. \nThere is no justification about their multimodal settings. For instance, why do you select NLP plus CV as a case study? Does these combination is representative for the new parties to the training pipeline mentioned in the paper? In the paper, the authors only mention that ML has achieved great success in many domains and hijacking task might deal with data from other modalities. But there is no justification or support. Without such justification, it is unclear whether the problem studied in this paper is a real problem or not. At least, the authors should provide more details about the representativeness of their multimodal settings.\n\n- Lack of practical evaluation.\nThis paper is motivated by a real-world new parties to the training pipeline, such as users who contribute training data and companies that provide computing resources. However, later on there is no evaluation on a practical systems but only evaluate on small dataset such as MNIST, CIFAR-10, and STL 10, in which all the images are of low resolution. It is unclear whether their novel methodology can work in a large image dataset and how much cost it will be. Also, the high-resolution artifacts in the fused image may be also very easy to be detected. Without such evaluation, it is very difficult to judge whether their attack can indeed work in real-world system or at least in real-world high resolution image setting. Also, considering the poison rate effects, although the authors evaluate on different settings, there is no justification/supports on which number is practical/reasonable for a real-world system. If their poisoning rate is higher than that the practical threshold, their design may be empirically flawed.\n\n- Lack of detailed reasoning on some design choices.\nIn evaluation section 4.4, there generally lack of detailed reasoning on some design choices. For instance, why the model hijacking attack is independent of the victim's model architecture; why \"[cls]\" token's embedding only does not change the utility. In this section, the authors only list out the experimental results but do not provide detailed analysis especially on the root cause part. If more details can be discussed, it can benefit the future research designs and make this paper more insightful and interesting.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read and follow. It is novel but I have some major concerns on its motivation, practicality, and the root cause analysis.",
            "summary_of_the_review": "I believe this paper is slightly below the the acceptance threshold. \n\nI have some major concerns as follows:\n\n- Lack of the justification about the multimodal settings. \n\n- Lack of practical evaluation.\n\n- Lack of detailed reasoning on some design choices.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2580/Reviewer_TtX4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2580/Reviewer_TtX4"
        ]
    }
]