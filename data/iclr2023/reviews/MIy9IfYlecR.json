[
    {
        "id": "Nhk0lDVql9e",
        "original": null,
        "number": 1,
        "cdate": 1665664386574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665664386574,
        "tmdate": 1665664575077,
        "tddate": null,
        "forum": "MIy9IfYlecR",
        "replyto": "MIy9IfYlecR",
        "invitation": "ICLR.cc/2023/Conference/Paper6231/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "To cope with the task of TTA, this paper proposes to replace the simple version or the cyclic version of the loss predictor with a cascaded version utilizing an RNN architecture. The authors evaluate their method on CIFAR-100 and ImageNet.",
            "strength_and_weaknesses": "Strength:\n1. This paper is well-written and easy to follow\n2. This paper is well-motivated and the motivation of this paper is reasonable.\n\nWeaknesses:\n1. My first concern w.r.t. this paper lies in its limited novelty. While I understand that compared with the previous method that uses either the simple version or the cyclic version of the loss predictor, this paper does demonstrate its advantages. However, it seems to me that this paper technically just applies RNN which is not a new technique already. Thus, while it is reasonable to choose to use RNN here, from my perspective, this does not bring enough novelty as no specific designs have been made for the task of TTA.\n2. I am also a bit confused about why the authors suddenly jumped to the usage of ResNet-50. Specifically, intuitively, I think that the task of the loss predictor is not a very hard task. Thus, the usage of a lightweight backbone EfficientNet-B0 is adequate and reasonable from my perspective. Thus, while the authors do demonstrate that their method can be much more efficient than cyclic predictor with ResNet-50 as the backbone, I am a bit confused why ResNet-50 should be used as the backbone in the first place. Note that the improvement of efficiency is quite small on EfficientNet-B0, and it seems from the experimental section, the usage of ResNet-50 does not lead to a significant performance enhancement compared to EfficientNet-B0.\n3. At last, I suggest the authors make their evaluation metric more clearly in the experiment section. Specifically, the name of the metric and whether a small or a large number leads to better performance can be included in the table. The current title of the table with only \"Clean\" and \"Corrupt\" there brings me a slight reading difficulty.",
            "clarity,_quality,_novelty_and_reproducibility": "I believe that the presentation of this paper is clear, and enough details have been provided to reproduce this method. However, I have concerns about its novelty.",
            "summary_of_the_review": "Overall, I have concerns about the novelty of this paper. Besides, while I agree that this paper is more efficient than the cyclic version, it seems that this efficiency is only significant when a large model is used as the backbone. However, it seems that a small model as the backbone is already enough in this task.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6231/Reviewer_N36J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6231/Reviewer_N36J"
        ]
    },
    {
        "id": "xHxPoxFtlve",
        "original": null,
        "number": 2,
        "cdate": 1666603010244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603010244,
        "tmdate": 1666603010244,
        "tddate": null,
        "forum": "MIy9IfYlecR",
        "replyto": "MIy9IfYlecR",
        "invitation": "ICLR.cc/2023/Conference/Paper6231/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a test time augmentation using a cascade loss prediction method which only requires a single forward pass of the transformation predictor to select multiple transformations. In contrast to the repeated usage of one loss predictor in cyclic-TTA, the proposed method uses RNN to capture the semantic information in each iteration and predict transformations without intermediate images. Experiments are conducted on various architectures to explain the trade-off between computational cost and model performance.",
            "strength_and_weaknesses": "Strength\nThe idea of using RNN seems natural and useful..\nThe computational cost of the proposed method is showed impervious with the iteration number on Resnet-50.\nThe paper is well-organized and easy to follow \n\nWeakness\nThe experiments are not convincing enough. The paper does not compare with any SOTA methods listed in the related works.\nAs the results show in Table2, the proposed method has no obvious advantage over Single-TTA. \nIt's necessary to compare the evaluation results and the calculation cost simultaneously to get a convincing conclusion, while both of them are insufficient in this paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity:4\n* Quality:3\n* Novelty:3\n* Reproducibility:3\n",
            "summary_of_the_review": "Overall, the main idea of this paper is interesting but the experiments are not convincing. Therefore, I suggest a rejection for this paper unless more experiments are complemented.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6231/Reviewer_yeBc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6231/Reviewer_yeBc"
        ]
    },
    {
        "id": "u6dwiZ_wo-Y",
        "original": null,
        "number": 3,
        "cdate": 1666669183154,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669183154,
        "tmdate": 1669304185832,
        "tddate": null,
        "forum": "MIy9IfYlecR",
        "replyto": "MIy9IfYlecR",
        "invitation": "ICLR.cc/2023/Conference/Paper6231/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for learning data augmentations for improving test performance in the presence of distribution shift. The method uses an RNN based augmentation module rather than a feedforward module that may be run multiple times. Experiments show that this method seems promising on standard corrupted image benchmarks.",
            "strength_and_weaknesses": "Strengths\n---\n\n- Experiment seem to show that the method holds promise for achieving favorable performance and efficiency.\n- The method is relatively simple and easy to understand.\n\nWeaknesses\n---\n\n- The paper is difficult to follow in numerous places (see below).\n- It is unclear whether the proposed idea is significant enough to warrant publication at ICLR.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality\n---\n\nI expected to see a comparison to cyclic TTA in the experiments, but as far as I can tell, that is missing. The experiments do seem to show some improvements compared to other baselines, but the improvements are generally small and it is unclear how competitive these baselines truly are.\n\nClarity\n---\n\nI do not understand why the authors refer to their method with the term \"cascade\" when simply referring to it as RNN-based would be much simpler and clearer. Eq (2) seems to only make sense if y is a real value, which it is not in classification. I was not able to follow Section 3.3 detailing the training strategy, and given that there is still room in the main paper, I would suggest focusing on providing more detail in that section to help clarify these important points. Generally, the vocabulary used throughout (\"ascendancy\", \"rough\", etc.) unnecessarily complicates the exposition and I would recommend being more consistent and rigorous with the writing.\n\nOriginality\n---\n\nIt seems like the main novelty of the proposed method is in using an RNN to output sequential augmentations. This does not seem significantly novel by itself. As noted, cyclic TTA also outputs sequential augmentations but is more computationally expensive due to working in image space. This seems to lead to a natural comparison: cyclic TTA, but with the same backbone that embeds the image into the lower dimensional feature space. In general, comparisons such as these (and standard cyclic TTA) would go a long way in strengthening the empirical aspects of the paper, which are needed due to the lack of a significantly novel technical contribution.",
            "summary_of_the_review": "In summary, there are several important improvements that can be made as laid out above. In the paper's current state, I am recommending reject.\n\nEdit after author response\n---\n\nThank you for the response, and apologies for the late reply. My recommendation remains unchanged. It seems that the other reviewers and I agree that this paper will benefit significantly from another full reviewing round before publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6231/Reviewer_2Tn9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6231/Reviewer_2Tn9"
        ]
    },
    {
        "id": "5Whb_ItUlFP",
        "original": null,
        "number": 4,
        "cdate": 1666672659443,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672659443,
        "tmdate": 1666672659443,
        "tddate": null,
        "forum": "MIy9IfYlecR",
        "replyto": "MIy9IfYlecR",
        "invitation": "ICLR.cc/2023/Conference/Paper6231/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a model for selecting test-time data augmentation to boost a classification model. The proposed method is based on RNN to gradually output multiple augmentations, selected from a predefined augmentation set. The idea is based on loss prediction for finding suitable augmentations. In each step of RNN prediction, the model outputs a predicted loss value and then select the best augmentation. To show the effectiveness of the proposed method, this paper conducts experiments on CIFAR-100 and ImageNet datasets, and compare this method with many other test-time augmentation strategies.",
            "strength_and_weaknesses": "Strength:\n+ This paper presents a method, which is able to produce multiple data augmentations with single forward pass.\n\nWeakness:\n- The novelty of this work is not surprisingly strong. The proposed model is different to previous architecture, however, applying RNN to produce multiple output is a common method.\n\n- From the experimental results, it is hard to observe clearly better results than other TTA methods, for example Horizontal-Flip. \n\n- How do you train the loss predictor. This is unclear how to train a loss predictor on specific datasets.\n\n- How to select the predefined candidate transformations. For individual instances, different transformations are required. How do you prepare the transformation set for the best performance for various datasets. \n\n- Need citation after the first sentence in section 3.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of this paper is not strong enough. There are some issues unclear to me. I don't know how to train the loss predictor and cannot reproduce the results after reading this paper.",
            "summary_of_the_review": "This paper does not have a clear merit to be accepted by ICLR. They present an architecture for test-time augmentation, however, no clear benefit from the proposed method can be achieved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6231/Reviewer_6hqy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6231/Reviewer_6hqy"
        ]
    }
]