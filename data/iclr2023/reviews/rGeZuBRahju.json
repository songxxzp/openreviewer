[
    {
        "id": "U2z29tce81R",
        "original": null,
        "number": 1,
        "cdate": 1666156910695,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666156910695,
        "tmdate": 1666156910695,
        "tddate": null,
        "forum": "rGeZuBRahju",
        "replyto": "rGeZuBRahju",
        "invitation": "ICLR.cc/2023/Conference/Paper5374/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduce a novel logical inductive bias which treats the language representation learning problem as logic programming. We then develop a fully-differentiable neural architecture (FOLNet) that forward-chains a set of neural logic operators, which effectively encodes this inductive bias. The proposed FOLNet architecture has the same input-output interface as the transformer models and can be pretrained over large-scale text data. Experimental results demonstrate that the FOLNet architecture significantly outperforms different variants of transformer models under the same pretraining losses. The results further show that the inherent dual-branch architecture has many advantages over the single-branch version in transformers.",
            "strength_and_weaknesses": "Strength: \n\n1. This paper develops a novel neural architecture named FOLNet (First-Order Logic Network), to encode this new logic inductive bias.\n\n2. This paper finds that the self-attention module in transformers can be composed by two of our neural logic operators, which probably explains their strong reasoning performance.\n\n3. The extensive experimental results support their  claims and validates the superiority.\n\nWeaknesses\uff1a\n\n1. It is not clear why FOLNet  could encode  logic inductive bias?",
            "clarity,_quality,_novelty_and_reproducibility": "It is novel and of high quality.",
            "summary_of_the_review": "Interesting and useful method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5374/Reviewer_dLtS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5374/Reviewer_dLtS"
        ]
    },
    {
        "id": "jphKHENi0Z8",
        "original": null,
        "number": 2,
        "cdate": 1666626671496,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626671496,
        "tmdate": 1666626671496,
        "tddate": null,
        "forum": "rGeZuBRahju",
        "replyto": "rGeZuBRahju",
        "invitation": "ICLR.cc/2023/Conference/Paper5374/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new family of transformer-style architectures inspired by inductive logic programming, with the goal of improving natural language reasoning capabilities. They connect this new family of architectures to existing commonly used modifications. They show good results on widely used benchmarks, modulo some concerns I raise later.",
            "strength_and_weaknesses": "Method:\n\nThe problem of learning new transformer architectures to aid reasoning is of broad interest to the community.\n\nThe model is well motivated by comparing to things like relative position encoding, and trying to use logical formalisms to derive new architectural variants on transformers.\n\nHow much the new architectural variants actually maintain the spirit of the logical formalisms is a bit harder to parse,  for example in 3.1 we are replacing discrete booleans with arbitrary real numbers and stating that \u201cit characterizes the extent to which the atom is true\u201d, which usually implies some value in [0,1] or some kind of increasing transformation, which I do not think you have here.\n\nExperiments: \n\nThe experiments are quite thorough, but are often confounded by the proposed method having more parameters than the baselines. Additionally, experiments would be better if they included your own reruns of BERT/Roberta experiments using the same settings to make it apples to apples in some cases, this is especially helpful for the situations where gains are small. This would also make it easier to make apples-to-apples comparisons in terms of # of parameters, but it would be a lot of work.\n\nThe ablation experiments in Table #3 are nice and quite appreciated. It would be good to have another column saying how many parameters each ablated model has.\n\nHaving some more concrete examples / toy examples of how these new architectures specifically aid reasoning on concrete examples would be really helpful.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written, aside from some typographical nitpicks which I will list here. The experiments and mathematical formalism are quite thorough, and the comparisons are to widely used benchmarks. While this is not the first work to apply ideas from inductive logic programming to deep learning, they make a solid contribution.\n\n\nVarious copy-editing nits from experiment section:\n\n\u201cLearns better language representations\u201d should be \u201clearn better language representations\u201d\n\n\u201cMany different variants of widely encoder-only\u201d should be \u201cmany different variants of widely used encoder-only\u201d\n\n\u201cTo demonstrates its advantage\u201d should be \u201cto demonstrate its advantage\u201d\n\n\u201cWe first pretaining\u201d should be \u201cwe first pretrain\u201d\n\n\nIssues with experiment presentation:\n\nALBERT XXL+ value of 96.9 is not bold but FOLNet Large of 96.8 is bold in the same column\n\nAdvantage of dual branch, Benefits of larger D2, contributions of larger D2, often are referencing table 4 when you mean to reference table 3\n\nIn 4.3, \u201cline #14 on table 9\u201d is referring to the wrong table, since table 9 is in an appendix and has no line #14. These are supposed to refer to table 4 I believe \n\nIn table #9 for MNLI, both 50.8\u2019s should be bolded",
            "summary_of_the_review": "This is a well-motivated and well-executed paper addressing a problem of wide interest to the community. It could benefit from more experiments giving intuition of how the architectures work in practice, and some very careful apples-to-apples comparisons, but the ablations already go a good way in that direction. This paper merits acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5374/Reviewer_93RW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5374/Reviewer_93RW"
        ]
    },
    {
        "id": "8drDwzJMb6q",
        "original": null,
        "number": 3,
        "cdate": 1666645611146,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645611146,
        "tmdate": 1666709964247,
        "tddate": null,
        "forum": "rGeZuBRahju",
        "replyto": "rGeZuBRahju",
        "invitation": "ICLR.cc/2023/Conference/Paper5374/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work introduces a new neural architecture (FOLNet) that incorporates a first-order logical inductive bias. Taking inspiration from the forward-chaining algorithm, the architecture is recursively applying learnable Kernel operations on unary and binary representations of tokens.\n\nInput and outputs are traditional token ids, making FOLNet an easy plug-and-play replacement from traditional Transformers. In addition, traditional Transformers can still be represented by a FOLNet making the proposed architecture more flexible and likely more powerful.\n\nExperiments on GLUE, SQuAD2.0, and FOLIO show that this new architecture performs better than comparable baselines such as BERT, RoBERTa, ALBERT, and Megatron.\n",
            "strength_and_weaknesses": "**strengths**\n\nThis is a strong paper that introduces a novel architecture with inductive biases from first-order logic. Experimental results are very promising and the fact that logical tasks remain a challenge for traditional Transformers can make this work impactful.\nThe paper is well written and surprisingly easy to read. However the methodology could benefit from some clarifications listed below:\n\n**weaknesses**\n\n1. Methodology : After pre-training on large datasets, were the models fine-tuned on individual tasks (GLUE, SQuAD, FOLIO) ? If so, for how long? it should be mentioned somewhere.\n\n2. Methodology : It is not clear how the final representations output from the model u_L(x) and u_L(x, y) are used in the computation of any loss. How does one go from these two representations to say a softmax layer to predict the masked token in MLM? Similarly for the other losses NSP and SOP.\n\n*Question*: Related to the point above, is it safe to assume that FOLNets are encoder type networks and cannot be used in an decoder-only or encoder-decoder fashion? If this assumption is wrong, then the paper should also describe how u_L(x) and u_L(x, y) are used to predict next words.\n\n3. Literature : Some influential previous work should be further discussed. Neural Theorem Provers (NeurIPS\u201917), and in particular Greedy Neural Theorem Provers (AAAI\u201920) applied logical modules on text before. More recently, Edge Transformers (NeurIPS\u201921) proposed a triangular attention mechanism between pairs of tokens in an effort to better represent logical behaviors between tokens. Comparing FOLNets to these works would help contextualize the work better.\n\n4. Experiments : Right now the paper shows improvements on \u201cclassical\u201d language tasks. It would be a nice bonus to also show that the FOL inductive bias is indeed useful for more reasoning heavy tasks such as GSM8k, ProofWriter, CLUTRR, etc\u2026\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to read.\nExperimental details are clear and should be reproducible.\nSome methodology clarifications are needed (see section above).\n",
            "summary_of_the_review": "strong paper. more previous work should be discussed and some clarifications are needed.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5374/Reviewer_nDcE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5374/Reviewer_nDcE"
        ]
    }
]