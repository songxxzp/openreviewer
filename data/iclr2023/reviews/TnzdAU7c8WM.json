[
    {
        "id": "WvNfLXQQ7-n",
        "original": null,
        "number": 1,
        "cdate": 1666634668474,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634668474,
        "tmdate": 1670855134454,
        "tddate": null,
        "forum": "TnzdAU7c8WM",
        "replyto": "TnzdAU7c8WM",
        "invitation": "ICLR.cc/2023/Conference/Paper784/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new scheme to pretrain the neural networks, which combines the calculated persistent homology of a sublevel set filtration of images as the topology information. The topology information aims to guide the neural networks to learn non-local topological structures. To avoid the human bias in the annotated datasets, the authors utilize synthetic images generated by specific formulas during the pre-training. The authors provide empirical evaluations and show that the topology-related pre-training scheme can improve the convergence with comparable accuracy on natural image classification tasks.",
            "strength_and_weaknesses": "**Strengths**:\n\n1. The idea of guiding the neural networks to learn global structures with non-local features, like persistent homology is novel and interesting.\n\n2. Pre-trained with synthetic images generated by specific formulas seems to prevent the human bias existing in the annotated datasets and ensure fairness to some extent.\n\n3. The paper is clearly structured and well written.\n\n**Weaknesses**:\n\n1. The authors only provide empirical results, without giving theoretical analysis of the proposed pre-training scheme.\n\n2. The authors mention the weights of all layers are updated during the phase of fine tuning. However, it is unclear that the neural network is still able to preserve the learned global topological features if we update all layers\u2019 parameters during the phase of fine tuning.\n\n3. The experiments are all conducted on small datasets of natural image classification tasks, which is not sufficient to indicate that the proposed scheme can be used in other larger datasets or different downstream tasks.\n\n**Questions**:\n\n1. It may be helpful to conduct experiments on how the learned global features change between the pre-training phase and the fine-tuning phase.\n\n2. Using hand-crafted features to guide the training of neural networks seems alright. However, how to prove that your hand-crafted features actually reflect the global information of the training dataset? I would expect more theoretical analysis to support the main claim of this paper.\n\n3. It seems that the models are only tested on rather small datasets within only the natural image classification tasks. Is it possible to conduct experiments on larger datasets on other scenarios, like segmentation or tracking, to see the performance?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is well written and easy to follow.\n\nQuality: This paper contributes some new ideas but the empirical evaluation is limited.\n\nNovelty: The proposed idea is somehow interesting.\n\nReproducibility: I am optimistic to reproduce the main results of this paper as the paper provides lots of details.",
            "summary_of_the_review": "Overall, this paper contributes some new ideas but the empirical evaluation is limited. Also, it's unclear to me what the model is actually learning (interpretability).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper784/Reviewer_tB82"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper784/Reviewer_tB82"
        ]
    },
    {
        "id": "zehDsDUuKqy",
        "original": null,
        "number": 2,
        "cdate": 1666679343720,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679343720,
        "tmdate": 1666679343720,
        "tddate": null,
        "forum": "TnzdAU7c8WM",
        "replyto": "TnzdAU7c8WM",
        "invitation": "ICLR.cc/2023/Conference/Paper784/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a technique to learn local and global visual representation via Persistent Homology.\nThe overall structure of the proposed method is to perform self-supervised learning (regression) on synthetic images for Persistent Homology as a pre-training, and then natural images for fine-tuning.\n\nThe synthetic image utilizes frequency-based (FFT) random generation method, where there are patterns at different scales\nThe PH label computation (using Cubical Ripser) aims at extracting shapes using contours by using signed distance function.\nThe author then utilizes four different vectorization techniques: the persistence image, the persistence landscape, the Betti number curve, and the birth-life histogram.\n\nThe algorithm is tested on ImageNet and CIFAR100. It also uses 2000 binary images of animal contours for a second (mid-step) topology learning.\nThat is, for ImageNet for example, it uses 90 epoch synthetic data, 300 epochs for animal dataset, then 45 epochs for ImageNet.\nThe first experiment is testing various pre-training (including vectorization techniques) and fine-tuning training combinations, as well as other methods, such as MoCo v2 and FractralDB-pre-trained models.\n\nThe PH-pre-trained models show better convergence than without pre-training (just fine-tuning) or fine-tuned twice, and comparable to FractalDB.\nAmong the vectorization, all are similar, with Betti count performs worst. This may be because of the lack of persistence structure learning.\nThe experiments also show that the number of PH-training images is important as there is only 1600 of the animal training images.\n",
            "strength_and_weaknesses": "Strengths: The paper explains the intentions and technical parts quite well. It is tested reasonably thorough, comparing the various vectorization techniques, as well as with other modules.\n\nWeaknesses: it is hard to understand what is actually being learned. It would help to know what kind of images are now better classified after the pre-training and fine-tuning regiments to demonstrate the usage of structure learned in improving final task accuracy.\nThat is, would it be possible to show example testing images that are moving from incorrect to correct.\nAlso, it would be helpful to describe what types of global structures (and invariances) are eventually being learned by PH synthetic image pre-training.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper explains the problem and technique clearly. ",
            "summary_of_the_review": "As stated the problem and solution (technique) is explained well. However, the testing and analysis can be more in-depth.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper784/Reviewer_nihr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper784/Reviewer_nihr"
        ]
    },
    {
        "id": "9Gwk_4pcY7f",
        "original": null,
        "number": 3,
        "cdate": 1666802581090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666802581090,
        "tmdate": 1666802581090,
        "tddate": null,
        "forum": "TnzdAU7c8WM",
        "replyto": "TnzdAU7c8WM",
        "invitation": "ICLR.cc/2023/Conference/Paper784/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to use synthetic data to learn a better representation that can enforce the model to learn holistic features. The synthetic data is generated using persistent homology (PH). This method does not require real images. In the experiment, authors try to apply the learnt representation to downstream tasks. The experiment is done on the animal dataset, which consist of binary images. ",
            "strength_and_weaknesses": "Strength:\n- Training a better representation without using real data is an interesting direction. This paper presents an innovative way to learn holistic features.\n\nNegative:\n- The effectiveness of this method is uncertain. It is uncertain whether the feature learned using this synthetic data can work well together with real data and tasks. The authors mentioned, \"The image features obtained by PH are topological and global, whereas the features extracted through iterated convolutions by CNNs are mostly local.\"  This disadvantage might not hold for various cases. Many models can encode long-term context effectively.\n- The experiment is not persuasive enough to show whether this method can be generalized to in-the-wild real data. The binary image is an effective way to prove the concept, but it is also helpful to include results for more challenging real data to show its effectiveness.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to understand.\nThe idea is simple and easy to follow. I think it is novel and interesting, but not certain whether it is effective for other datasets and other model architectures.\nThe authors will release the code to ensure reproducibility.",
            "summary_of_the_review": "The paper proposes an interesting way to generate synthetic data. Then use this data to train a better representation that can encode holistic information. The experiment is limited to a binary dataset, which makes it hard to judge whether the training strategy can work well on real-world data. Due to the challenge and complexity of real data, many existing models can already encode long-term context and have large receptive fields. This fact makes the advantage of the proposed method seem less significant. Based on these reasons, I think the paper is below the threshold of ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper784/Reviewer_U7F9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper784/Reviewer_U7F9"
        ]
    }
]