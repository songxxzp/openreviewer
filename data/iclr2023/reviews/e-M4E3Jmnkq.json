[
    {
        "id": "hoVFploQeEz",
        "original": null,
        "number": 1,
        "cdate": 1666643966789,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643966789,
        "tmdate": 1666643966789,
        "tddate": null,
        "forum": "e-M4E3Jmnkq",
        "replyto": "e-M4E3Jmnkq",
        "invitation": "ICLR.cc/2023/Conference/Paper3531/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces introduces a technique to mitigate indirect bias. To use an example from the paper, after debiasing sentences [ \"he is a handsome engineer\", \"she is a sensitive engineer\" ] and removing the gendered pronoun, the indirect biases (\"handsome engineer\" and \"sensitive engineer\") still persist. Empirical motivation for addressing these indirect biases is provided by showing that implementing this co-occurrence probability value substitution produces a more pronounced debiasing effect.\n\nThe solution recommended is substituting the entry for a word-pair (a,b) in the word co-occurrence matrix with the formula from equation 2.\n\nOverall we get mixed results at de-biasing with degradation in semantics - the abstract claims no or minimal degradation but the results section seems to show a significant (at least from what I can see) hit compared to using CDS.",
            "strength_and_weaknesses": "Strengths: The paper does introduce a good line of thought about indirect biases. Recent work analyzing WEAT has highlighted similar shortcomings and so this is a welcome idea. The substitution suggested is straightforward to implement.\n\nThese are the weaknesses I have identified:\n- The empirical justification for this task is weak. For instance, WEAT is treated as a gold-standard and the effect score is treated as evidence. WEAT scores themselves are highly sensitive to the word-lists being used and recent results call into question using WEAT directly [1]. It could be that we only get semantic degradation but no additional improvements over CDS with this method. Ideally we like to see raw co-occurrence statistics or something decoupled from the metric itself.\n- There seems to be a non-trivial semantic degradation compared to CDS (unless I am misunderstanding the results section).\n- From the paper: \"Therefore, the vocabulary must be limited so that all words in the corpus occur in a word\u2013context pair satisfying X and in a pair satisfying Y.\" - This in particular seems like a strong imposition (I know the paper claims otherwise). The benefit of methods that try to identify and drop subspaces is that we can work with non-comprehensive word-lists. I am curious if this doesn't work until a fully fleshed out word-list for the embeddings is developed. I am happy to be corrected on this since it feels to me like a huge chunk of the vocabulary would need to be dropped if we are unable to acquire a large word-list (say for addressing a second form of bias beyond gender like caste or race).\n\n[1]: https://aclanthology.org/P19-1166/\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The idea is novel - the authors have identified a valid source of bias and provided a technique to mitigate it.\n- The new co-occurrence probability value is straightforward to compute and implement so reproducibility is not an issue.\n- No additional resources needed beyond existing word-lists (for at least the types of bias addressed in the work).",
            "summary_of_the_review": "- Overall, a good idea and a well-identified problem and a reasonable attempt at addressing it.\n- The focus of the paper is an area of significant social import.\n- There are some mild empirical concerns - mixed results, semantic degradation and a few issues with how the problem is motivated.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3531/Reviewer_T3X4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3531/Reviewer_T3X4"
        ]
    },
    {
        "id": "DQwoINqEUIp",
        "original": null,
        "number": 2,
        "cdate": 1666762825438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666762825438,
        "tmdate": 1666762825438,
        "tddate": null,
        "forum": "e-M4E3Jmnkq",
        "replyto": "e-M4E3Jmnkq",
        "invitation": "ICLR.cc/2023/Conference/Paper3531/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors discuss the evaluation of indirect bias/stereotypes in word embeddings and propose a methodology to mitigate them. They do this by modifying the relationships between words before the embeddings are learned. They compare their results with CDS method (Counterfactual Data Substitution) and find that their proposed method is better than CDS in some cases. The improvement, however, is not uniform and there are cases (like the analogy task) where the accuracy is reduced by around 20%. They also calculate p values and effect sizes for undirect bias to evaluate if this approach also removes biases that are not being attempted to remove (e.g Flower/Insect\u2013Pleasant/Unpleasant)",
            "strength_and_weaknesses": "Strength:\nThe paper targets an important problem of mitigating indirect bias in word embeddings. They propose a novel approach to modify relationships between co-occurring words before the embeddings are learned.\n\nWeakness:\n\nI found the details of the methodology slightly lacking. The experiment details have been briefly explained in the appendix which can be included in the main paper. Additionally, the math has been explained but the visualization of the approach through examples would have benefited the reader. The improvements are not uniform so details on which cases are positively/negatively impacted by the approach would have helped understand the impact of the approach.",
            "clarity,_quality,_novelty_and_reproducibility": "The approach for mitigating bias is novel but I am skeptical about its impact. The problem also focuses only on gender-occupation bias which is justified since they wanted to evaluate the impact of the approach but notes on whether or not it can extended for biases other than gender would be a good contribution. The method seems generic but more details on the extension will be helpful. I also felt that the authors can be a bit more descriptive while explaining the methodology through examples.",
            "summary_of_the_review": "The paper is well motivated but lacks in-depth analysis to actually evaluate the impact of the approach. It will benefit from a more detailed explaination of the approach and additional analysis/experiments to understand which cases are positively impacted and why.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3531/Reviewer_u8Li"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3531/Reviewer_u8Li"
        ]
    },
    {
        "id": "XVRwxF6cbd",
        "original": null,
        "number": 3,
        "cdate": 1667212259639,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667212259639,
        "tmdate": 1667212259639,
        "tddate": null,
        "forum": "e-M4E3Jmnkq",
        "replyto": "e-M4E3Jmnkq",
        "invitation": "ICLR.cc/2023/Conference/Paper3531/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the problem of indirect bias. The authors define how indirect bias in a corpus can be quantified, then introduce a method to reweight adjectives to correct their frequency in association with gendered nouns. These methods are then tested on a variety of embeddings, showing marginal improvements in some specific cases.",
            "strength_and_weaknesses": "* This paper tackles a relatively novel area, the systematic study and mitigation of implicit bias in word embeddings, which is an important area due to the wide use of such embeddings in a variety of applications, and the potential harm associated with the presence of implicit biases.\n* The method introduced is novel and a useful step towards more robust mitigation of indirect biases in word embeddings and language models.\n* The mitigation method introduced in this paper is quite basic and it is unclear if it would be possible to extend it to other domains with less well defined characterizing words (in this study the authors use \"he\",\"him\",\"his\"..../\"she\",\"her\",\"hers\",..., but I don't think such simple definition would be possible for most marginalized demographics.\n* From the experiments section, it is clear that further study is needed to successfully mitigate indirect biases across models.",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is clear and well written, and introduces a simple but novel method for mitigating indirect biases through a replacement of sensitive pairs.\n* There is no public code and few examples of replacements are given, but for the most part the paper is reproducible.",
            "summary_of_the_review": "This is an interesting study of indirect stereotypes in word embeddings, an important issue for language models. This paper provides an interesting first step towards gaining an understanding on building fairer and less biases models. While the mitigation procedure is relatively constrained in its application and provides only modest improvements, it is a useful step in the right direction.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3531/Reviewer_GLiH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3531/Reviewer_GLiH"
        ]
    }
]