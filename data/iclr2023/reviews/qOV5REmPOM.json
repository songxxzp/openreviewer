[
    {
        "id": "D3OjUpLRPX",
        "original": null,
        "number": 1,
        "cdate": 1666381866607,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666381866607,
        "tmdate": 1666381866607,
        "tddate": null,
        "forum": "qOV5REmPOM",
        "replyto": "qOV5REmPOM",
        "invitation": "ICLR.cc/2023/Conference/Paper1672/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A Lipschitz-continuous generator of GAN cannot map a unimodal distribution to disconnected distributions. Therefore, in order to transform a unimodal distribution, e.g., Gaussian, of the latent variable to cover all modes of the disconnected target distribution, the generator must map a subset of the latent space outside the support of the target distribution. \n\nIn order to minimize the probability of the generator outputting an out-of-support sample, the paper studies some sufficient condition of the shape of the latent space partitions via Isoperimetric inequality. By invoking results from (Milman and Neeman, 2022), the author indicates that the latent space forms a  Voronoi partition when $ m < d+1$ ($m$ is the number of modes, $d$ is the latent space dimension). Next, the authors perform several experiments on structure of the latent space of GANs, e.g., the linear separability of the latent space partition, the convexity fo the latent space partition, etc.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper poses an interesting angel towards understanding generative models (not restricted to GANs). A Lipschitz generator of cannot map a unimodal distribution to disconnected distributions. Therefore, there is a tradeoff between mode-collapse and precision of the generator. Through Gaussian Isoperimetric inequality, the simplicity and regularity of the latent space (linearity & convexity) can be related to the quality of the generation. \n\n\nWeaknesses:\n\nTheory:\n\n1. The assumptions are too strong and fail to capture the essence of the real generative models. \n\n(1) The theory depends on the $L_2$ Lipschitz continuity $L$ of the generator network. For deep models $L$ is often a very large constant. Just think about the existence of adversarial examples. \n\n(2) The abstraction of the target distribution is artificial. The paper assumes the target distribution is a mixture of $m$ balls of equal distance. This assumption can be readily relaxed without modifying the main theorems, e.g., just defind $D$ as the minimum distance between those modes. \n\n2. The technical contributions of the theorems are weak. \n\n(1) The only interesting theoretical result is Lemma 1, which bridges the surface area of the latent space partition and the precision of the generator. The result is weak since empirically $D/L$ is very small and hence the upper bound is loose. Then the behavior of the generator is influenced by other factors.   \n\n(2) Theorem 1 & 2 are straight applications of the Gaussian Isoperimetric inequality.\n\nExperiments: \n\n1. The experiments are based on an implicit assumption that different classes correspond to different disconnected \"modes\". Besides, the empirical results (LogReg Acc & Convex Acc) are weak, suggesting latent space partition according to different classes is not predicted by the theory. \n\n2. Several experiments are disconnected from the theory. E.g., latent space dimension, disentangling the manifold dimension, and the Impact of overparametrization. Those experiments are far-fetched from the theory part.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: \n\n- In Theorem 1, what is $\\delta$? \n- The definition of the Density and Coverage needs more clarification. Besides, in Table 2, how do you compute density and coverage?\n",
            "summary_of_the_review": "\nThe paper bridges the configuration of the latent space of a generator and the precision through Isoperimetric inequality. The idea is interesting but there are several weaknesses in both the theory and the experiments.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_q2EK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_q2EK"
        ]
    },
    {
        "id": "luzZom1jCW",
        "original": null,
        "number": 2,
        "cdate": 1666731225641,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731225641,
        "tmdate": 1666731262129,
        "tddate": null,
        "forum": "qOV5REmPOM",
        "replyto": "qOV5REmPOM",
        "invitation": "ICLR.cc/2023/Conference/Paper1672/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper uses a recent mathematical result -- about the form of Gaussian-isoperimetric partitionings of Euclidean space -- to conjecture that successfully learned GANs learn such \"optimal\" partitionings of the space. This is used to try to quantify the precision of such learned GANs in terms of other parameters of the problem. The paper contains experiments on GAN latent spaces, on which the data are Gaussian by construction, and which can link learned GAN latent spaces to theory.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper adopts a novel perspective on the latent space of GANs, by using results in Gaussian isoperimetry (GI) to conjecture about the form of the learned space of optimally-learned GANs. Using this perspective and powerful tools from GI, the authors try to relate precision to the structure of the latent space learned by a GAN. This is a really nice idea. The proofs are fairly clearly and straightforwardly done, with an exception (see below).\n\n\nWeaknesses:\n\nThe overall structure of the theory is not supported by the practical considerations. \n\nThe experiments don't really support the theoretical results as much as they could. The theory consists of an upper bound on precision which holds for any model, and a lower bound which holds for an \"optimal\" model. The lower bound is inherently more interesting in this situation, but its statement is weaker. A natural next question is: do the backprop-learned GANs actually follow this lower bound? \n\nThe relevant quantities are L, D, and m, which can all be measured to empirically investigate this question. The theorems state a number of tradeoffs betweeen these quantities, which could be empirically investigated without much effort, but are not. The lower/upper bounds don't actually match, because the quantification over the partitions is different; and I'd therefore like to see empirical verification of the \\sqrt{log m} rate. This is easy to test not only for MNIST, but more multiclass problems like ImageNet-1000; you can subsample classes and get precision numbers. \n\nThe issue of precision itself is a little muddled in this paper. It's not clearly defined initially. Then it's assumed that r << D, i.e. effectively that the data are on a set of m point masses. This ignores the data-dependent effects of distance with space, and leads to the fundamental D/L scaling of the epsilons in the theorems. How to interpret the theorems on real data is therefore somewhat unclear. \n\nThe Lipschitz constant L is a parametrization the authors choose, which ends up making the proofs track with proofs of the analogous GI results. Like other alternative parametrizations, it is something that could be reasonably estimated in practice - what is it for the networks used in the experiments? Is L large enough in practice to support the desired bounds? From the cited Virmaux+Scaman reference, L is ~150 for an MNIST CNN of depth 5; it would be good to include the results of such an evaluation, to complement these experiments. Meanwhile, D can also be estimated for the dataset, and m is known. So the bounds on precision and recall can actually be numerically computed and compared. This would provide some notion of how good the parametrization is, and how much more theoretical understanding is needed.\n\nIt would also be good to see the \"convex accuracy\" reported for all the experiments - I find that and the logistic regression to be very flexible and useful aids to understanding the relationship between simplicial clusters and the empirical learned space. Experimenting on more networks, grid-searching across approximate {L, D, m} configurations, is needed.\n\nRelaxing the GI assumption to encompass data-dependent Gaussian concentration of measure (without isoperimetric symmetries) would be a very interesting avenue of future work.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper becomes clearer as it progresses. Though there are some issues with the clarity of the definitions, the paper is overall clear. The results are novel and rely on a novel new GI characterization. No further issues along these lines.",
            "summary_of_the_review": "The overall idea, of using deep results in Gaussian isoperimetry to explore learned GAN representations, is great and could be a very useful avenue of study in several ways. The theoretical narrative is very incomplete, and though the experiments are promising, the theory is not sufficiently supported or explored by the experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_ApLT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_ApLT"
        ]
    },
    {
        "id": "je1T47-b4E3",
        "original": null,
        "number": 3,
        "cdate": 1666987202443,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666987202443,
        "tmdate": 1670035111322,
        "tddate": null,
        "forum": "qOV5REmPOM",
        "replyto": "qOV5REmPOM",
        "invitation": "ICLR.cc/2023/Conference/Paper1672/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper uses existing theoretical results on the boundary measure of dimensional Voronoi cells of m<d+1 equal distance modes to show a bound of the precision of a m-mode GAN. Some empirical study is carried out to study the geometry of the latent space. However, the empirical result is not particularly well-connected with the theoretical bound. ",
            "strength_and_weaknesses": "The usage of the theoretical result seems valid. However, it still seems that the assumption (Assumption 1) is a bit too strong. In practice, the prior on different modes can be unbalanced. Some discussion is provided but no solution is proposed. Also the distribution can be a mixture model instead of the case that each distribution is strictly restricted within a (d+1)-dimensional Voronoi cell. \n\nThe implication of the bound is unclear. No practical insights are revealed as a consequence of the bound. The experiments explore the geometry like linearly separability and convexity. But these results are still quite far from the bound. ",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "Interesting usage of an existing theory for GAN precision bounds. But the theoretical results is not well connected with practical usage of GANs.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_jLeB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_jLeB"
        ]
    },
    {
        "id": "xn1RGkKgSQ",
        "original": null,
        "number": 4,
        "cdate": 1667049252554,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667049252554,
        "tmdate": 1669947670423,
        "tddate": null,
        "forum": "qOV5REmPOM",
        "replyto": "qOV5REmPOM",
        "invitation": "ICLR.cc/2023/Conference/Paper1672/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the optimal precision of GANs trained on disconnected distributions under some simplifying assumptions about the distributions. The paper derives lower and upper bounds for the precision of GANs under these assumptions, and also studies what partitioning of the latent space that can give rise to the optimal precision in GANs. The paper also provides some smaller scale experiments on image datasets that show GANs tend to construct the predicted Voronoi partitioning to a certain extent, and also behave consistently with the predicted effects of the dimensionality of the latent space on precision.\n",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper is well-written and easy to follow. The posed question is also very interesting to me, understanding how partitioning of the latent space affects the precision of GANs, and vice versa, finds many potential applications for improving GAN precision and searching/editing in their latent space. The Theorems and claims are mostly correct, and the provided bounds are non-trivial and interesting.\n\n\n**Weaknesses**\n\nIn general I like the paper, but I have some questions and minor concerns that I\u2019ll go over below:\n\n1) The paper neither shows Voronoi partitioning to be a necessary nor a sufficient condition for achieving optimal precision. While the authors do touch on this point in several parts of the paper (including right after Theorem 2 and then again at limitations), I still think the language is vague and must be made very explicit to avoid misleading a not so very careful reader. In particular, since the paper starts by raising the following question in its abstract: \u201cwhat is the latent space partition that minimizes the measure of out-of-manifold samples\u201d, a reader is likely to think that the answer is a Voronoi partition, yet this is not true! The question remains open in that no necessary condition on the latent space is discovered. Please make this explicit in your abstract \u2013 mentioning the question is great for motivation, but it needs to follow the explicit mention that you could not answer this question, rather could find one particular generator with Voronoi partitioning that minimizes the measure of out-of-manifold samples, which is still plenty valuable as a starting point in my opinion.\n\n2) In Figure 1\u2019s caption, can you be more specific about in what sense left is close to right?\n\n3) The equal distance assumption is not well justified, either cite or elaborate more specifically.\n\n4) Can you be more specific on the statement \u201cIf L is large enough\u201d; and elaborate in what way small L breaks the theory, and discuss any connections to practical settings where L cannot be very large without causing instabilities.\n\n5) The claim in Table 1 that \u201cthe higher the precision is, the more each cell in the latent space is linearly separable\u201d is not strictly true when comparing Precision and LogReg columns, same goes for the Convex column. Moreover, the table lacks error bars, so significance of any trend is questionable. Including a proper correlation test can help clarify the extent of your claim.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper\u2019s contributions are novel and carried out with sufficient quality and clarity, except for some minor issues I discussed above. Sufficient details (as well as code) makes the paper very reproducible.",
            "summary_of_the_review": "Overall, I like the paper and think the findings about what kind of latent space partitioning can be optimal for GANs with respect to precision are valuable. The provided bounds are also insightful for designing new GANs. My main concern is that the paper does not restrict its claims as well as I\u2019d consider appropriate.\n\n### Update post-rebuttal\nI thank the authors for their response and answering my concerns. I think the paper merits acceptance upon making its assumptions and restrictions more clear, but I agree with the other reviewers that the assumptions are somewhat strong, as such keeping my score at 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_xjYQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_xjYQ"
        ]
    },
    {
        "id": "4GoNniomps",
        "original": null,
        "number": 5,
        "cdate": 1667554890088,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667554890088,
        "tmdate": 1667578624852,
        "tddate": null,
        "forum": "qOV5REmPOM",
        "replyto": "qOV5REmPOM",
        "invitation": "ICLR.cc/2023/Conference/Paper1672/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper adapts recent results on the Gaussian isoperimetric inequality to study the latent space organization of GANs with disconnected modes. This proves bounds on the precision of GANs in certain circumstances, and implies various properties about how these latent spaces \"should\" be organized.",
            "strength_and_weaknesses": "Strengths:\n- The paper gives a clear exposition of a fairly thorough examination of the results in question.\n- It increases our understanding of \"optimal\" latent space organization for Gaussian latents.\n\nWeaknesses:\n- The scope of the results is somewhat limited.\n- A few interesting questions are raised without being fully explored.",
            "clarity,_quality,_novelty_and_reproducibility": "- Suppose that, rather than Gaussian latents, we use the other common choice of latents being uniform over a hypercube (or perhaps a sphere, or a ball). It seems that isoperimetry is probably easier to study in this case; are there parallel results that can be immediately adapted to this setting? (Do GANs follow those results?)\n\n- The question \"what if nodes are not equally distant?\" seems quite important. Are cats, dogs, frogs, and trucks really equally distant? What's even the right notion of distance here, for a given generator architecture \u2013\u00a0surely it's not Euclidean distance in pixel space, as even for Lipschitz generators there'll be some warping of the space, but it also can't really be something based on the particular generator parameters since the question is about what those parameters will become? Is there anything we can say about the formal setting here? Anything we can look at in the experiments for specifically this kind of question? (If you, say, download a pretrained unconditional ImageNet GAN, do the latent space divisions have some relationship to distance in the WordNet hierarchy between classes, or between word embeddings?)\n\nOverall, I found the paper easy to read, the assumptions more or less reasonable, and the theoretical results clearly explained. (I did not check the proofs, and am not particularly familiar with work on isoperimetry beyond basic definitions; nor have I really kept up with the GAN literature for the past few years.)",
            "summary_of_the_review": "An interesting paper bringing recent mathematical results to a relevant community who probably wouldn't otherwise hear about them, while also exploring GAN-specific properties in an interesting way. Although the paper has its limitations, I think it's a nice step in the understanding of latent-space generative models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_GvAm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1672/Reviewer_GvAm"
        ]
    }
]