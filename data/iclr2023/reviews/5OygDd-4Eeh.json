[
    {
        "id": "lXWWqgDrbj",
        "original": null,
        "number": 1,
        "cdate": 1666340796946,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666340796946,
        "tmdate": 1666340796946,
        "tddate": null,
        "forum": "5OygDd-4Eeh",
        "replyto": "5OygDd-4Eeh",
        "invitation": "ICLR.cc/2023/Conference/Paper897/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new framework for ML interpretation. The goal is to choose a good subset of features (words/pixels) of the input data  to explain its prediction from a black-box classifier. The model contains three networks: a selector that selects important features, an explainer that learns an importance matrix between every feature and class, and an approximator used in the information theoretic lower bound. Through very extensive experiments (mostly on text data), the paper shows improvement of the proposed method compared to baseline methods on multiple evaluation metrics and settings. ",
            "strength_and_weaknesses": "**Strength**\n\nThe paper is clearly written. It is easy to understand and mathematically solid at the same time. The proposed networks and loss functions are very well motivated and make a lot of sense to me. The experiments are extensive: the paper studies several text classification tasks, all  being very useful in practice. There are a number of meaningful evaluation metrics, and the proposed method achieves huge improvement on all of these metrics. In the appendix, there are also extensive studies on loss functions, hyperparameters, and other tasks such as image classification.\n\n**Weakness**\n\nIt is unclear to me how $K$ is reflected in training. $K$ is used to define $\\mathbb{S}$ but later $x_{\\mathbb{S}}$ is approximated by $\\tilde{z}_x\\odot x$. It is unclear how accurate such approximation is, and it is irrelevant with $K$ anymore. Will constraints like letting $\\\\|\\mathcal{S}(x)\\\\|_1\\approx K$ work? \n\nSome minor things on notation. In eq(2) it would be more rigorous to use $\\tilde{y}_m$ instead of $\\bar{y}_m$. In addition, the symbol $\\circ$ is usually used to represent composition of functions, and the symbol $\\odot$ is usually used for element-wise product. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written and has high quality. The intro and related work contain good overview of the literature and how this paper relates with them. The methodology is mathematically rigorous while being concise at the same time. The definitions of networks and loss functions are well motivated, as they are designed to solve problems in prior works. The experiments are extensive and clear. There are detailed definitions of metrics and many real examples. \n\nThe paper is novel in methodology and evaluation metrics. \n\nThe paper provides code for their experiment. ",
            "summary_of_the_review": "Based on the comments above, I think this paper presents an interesting method for feature-based interpretability with extensive evaluation, and therefore I vote for acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper897/Reviewer_Vmy2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper897/Reviewer_Vmy2"
        ]
    },
    {
        "id": "lVhiwlDczuJ",
        "original": null,
        "number": 2,
        "cdate": 1666392979587,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666392979587,
        "tmdate": 1666392979587,
        "tddate": null,
        "forum": "5OygDd-4Eeh",
        "replyto": "5OygDd-4Eeh",
        "invitation": "ICLR.cc/2023/Conference/Paper897/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- The authors proposed an interpretable model that combines the additive and instance-wise feature selection approaches.\n- More specifically, the proposed methods jointly train three components (explainer, selector, and approximators) to identify important instance-wise features and provide corresponding explanations per each class.\n- The proposed methods show consistent performance improvements across multiple text and image datasets over alternatives.",
            "strength_and_weaknesses": "Strength:\n- The proposed method can provide not only instance-wise feature importance, but also corresponding additive explanations. Also it can provide explanation for multiple classes simultaneously.\n- The experimental results are promising and consistently better than alternatives.\n\nWeakness:\n- The proposed methods can be applicable to any data types. As the authors said, there are many related works on tabular data. In that case, it would be good to provide the superiority of the proposed method on tabular data as well. \n- It is unclear how L2 can prevent to converge \\pi_x = 1 for all components. It would be great if the authors can explain better on it.\n- There are some other related works that also do not need to set K value for instance-wise feature selection (e.g., Yoon et al, 2019). So, it would be good to tone down on this contribution. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly written. It is easy to understand.\n- The proposed method is somewhat novel but hard to say it is super novel because each component of the proposed method is coming from the previous works.",
            "summary_of_the_review": "Overall, this paper is easy to read and understand. Proposed method makes sense and the results are promising.\nBut it would be great if the authors can provide the tabular results to show the superiority over other works in tabular domains.\nAlso, it would be good to clarify the second loss better.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper897/Reviewer_RWdr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper897/Reviewer_RWdr"
        ]
    },
    {
        "id": "GY0j99Syn4z",
        "original": null,
        "number": 3,
        "cdate": 1666470527657,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666470527657,
        "tmdate": 1666773808830,
        "tddate": null,
        "forum": "5OygDd-4Eeh",
        "replyto": "5OygDd-4Eeh",
        "invitation": "ICLR.cc/2023/Conference/Paper897/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work develops a new method to explain predictions from black-box ML models. The method learns two separate modules to perform the following: 1) output an attribution score for each feature and each class (similar to methods like LIME), and 2) output selection probabilities for each feature (similar to methods like L2X). Thus, the method (AIM) is thought to bridge two categories of explanation methods. (The authors refer to these categories as \"additive\" and \"instance-wise\", but these are incorrect names for the groups of methods they're referring to.) \n\nThe experiments compare the method to L2X, LIME and two other methods (VIBI, FI) and find encouraging results.",
            "strength_and_weaknesses": "--- Strengths ---\n\n- It's an interesting idea to merge selection- and attribution-based methods, as most existing model explanation work falls into one of these categories.\n- The AIM method provides fast explanations because it's trained in an amortized fashion (similarly to L2X). \n- The experimental results suggest strong performance. \n\n--- Weaknesses ---\n\nWriting and prior work\n- The authors frequently refer to two categories of explanation methods, \"additive\" and \"instance-wise.\" This is poor choice of terminology, neither term seems to mean what the authors intend. \"Instance-wise\" is synonymous with \"local\" (i.e., explaining individual predictions), and *all the methods discussed here* are instance-wise/local. And \"additive\" doesn't cover multiple methods supposedly in that category, such as vanilla gradients (Simonyan et al., 2013) or SmoothGrad (Smilkov et al., 2017). I think the correct terminology would be additive methods --> \"attribution methods\" and instance-wise --> \"selection methods.\" See [1] for a review of how such methods are related (section 6).\n- The authors further conflate instance-wise feature selection with the amortized fashion in which the selectors are often trained (e.g., in L2X). There are multiple parts of the paper revealing this confusion, see the abstract for example: \"instance-wise methods directly optimize local feature distributions in a global training framework, thereby being capable of leveraging global information from other inputs.\" This seems to allude to the explainer training routine, but you don't technically need a trained selector model: you could instead follow a greedy algorithm for each example to be explained, for example.\n- The authors mention that prior \"instance-wise methods\" (read: selection-based methods) have a strict reliance on selecting a fixed number of features. Not true, INVASE and Real-X both address this [2, 3].\n- Regarding LIME, the authors write that \"there is a chance that the perturbed examples behave undesirably, for example, to change the original prediction.\" That's not undesirable, it's exactly the point of LIME..... Seeing how the prediction changes helps determine how much the model depends on each feature.\n- The authors write that \"additive methods are inefficient since they optimize individual explainers for every input.\" We can't do this for every sentence in the paper, but I'll just pick this one to highlight some more issues. i) Many \"additive methods\" (read: attribution methods) are not based on optimization, e.g., nothing is being *optimized* on a per-input basis in Integrated Gradients. ii) Some of these methods are inefficient, like LIME, but not all. E.g., most gradient-based methods are on the order of a couple forward passes, so you can't make a blanket statement that they're all slow. iii) Earlier works have addressed the problem of slow attribution methods: [4], [5], and [6] are three examples of works proposing the use of learned explainer models. iv) This suggests that attributions methods are inherently slower than selection methods, but selection methods aren't naturally fast (see comment above about how you don't need a learned selector) - they just happen to have leveraged amortized learning earlier than attribution methods. \n\n[1] Covert et al., \"Explaining by Removing: A Unified Framework for Model Explanation\" (2021)\n\n[2] Yoon et al, \"INVASE: Instance-wise variable selection using neural networks\" (2018)\n\n[3] Jethani et al, \"Have We Learned to Explain?: How Interpretability Methods Can Learn to Encode Predictions in their Interpretations\" (2021)\n\n[4] Dabkowski et al, \"Real time image saliency for black box classifiers\" (2017)\n\n[5] Schwab & Karlen, \"Cxplain: Causal explanations for model interpretation under uncertainty\" (2019)\n\n[6] Jethani et al, \"FastSHAP: Real-Time Shapley Value Estimation\" (2021)\n\nMethod\n- I can follow the training procedure outlined by the authors, but ultimately I don't understand what we expect the result to be. After implementing the joint training procedure, does the result have anything to do with the information communicated by each feature, for example? Many well-regarded methods (L2X, Integrated Gradients, SHAP) provide some theoretical characterization of their results, including what properties they satisfy or what they represent in terms of information theory. This approach does not, and it has hyperparameters that can significantly affect the results ($\\alpha, \\beta$) with little thought put into how they should be selected. Explaining ML models is an ambiguous problem with many existing solutions, so at this point solutions with weak theoretical support are unlikely to gain traction. \n- It's odd that the method requires training both the selector and explainer, but that the experiments only use the explainer. It seems like this decision was made based on which version of the method performed best in experiments (Appendix D), not by design. It seems a bit wasteful and overcomplicated to train both, and perhaps a flaw in the method that the selector actually hurts the results. \n- I suspect that AIM suffers from the same \"prediction encoding\" problem that [3] exposed in L2X: that the explanation can encode the label via its selections, even with a single feature. If so, this would not be a good thing - it would reflect that the predictions with selected features are based more on which features are selected than the information contained in those features. The results in Table 8 and Table 9 with $K = 5$ show that this may indeed be a problem, because the faithfulness values (% agreement with the full-input prediction) are often >90%. Could the authors comment on this? I think a rigorous test for this issue would involve evaluating the method with $K = 1$, and using the Eval-X method from [3].\n\nExperiments\n- Given the abundance of model explanation methods, I don't understand the choice of baselines. L2X I understand, but why not some more common/simple ones like Integrated Gradients [7], occlusion [8] or RISE [9]? SHAP would also be nice to include but has similar computation cost as LIME.\n- The authors write that \"Except for AIM and FI that do not require specifying $K$, all the other baselines are trained at $K = 10$.\" LIME doesn't require specifying $K$ either, right? Again, if you wanted methods like L2X that don't require specifying $K$, there are two of them [2, 3].\n- Can the authors confirm that their faithfulness and log-odds metrics are calculated using the original classifier rather than the new learned one ($\\mathcal{G}$)? For both the text and image datasets?\n- The experiments with MNIST and Fashion-MNIST don't compare AIM with any baselines. Can the authors correct this and include some commonly used methods for images (IntGrad, occlusion, RISE)? \n\n[7] Sundararajan et al, \"Axiomatic attribution for deep networks\" (2017)\n\n[8] Zeiler & Fergus, \"Visualizing and understanding convolutional networks\" (2014)\n\n[9] Petsiuk et al, \"Rise: Randomized input sampling for explanation of black-box models\" (2018)",
            "clarity,_quality,_novelty_and_reproducibility": "The method is built on some elements of L2X, but it seems sufficiently novel. My largest concerns are the method design and quality of the writing.",
            "summary_of_the_review": "The authors propose a new method to combine aspects of existing attribution- and selection-based methods. The method involves a complicated joint training routine with three separate modules, only one of which is used to generate explanations. The method lacks theoretical justification (what the method is optimizing for, e.g., in terms of mutual information) but obtains some positive experimental results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper897/Reviewer_qCbz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper897/Reviewer_qCbz"
        ]
    }
]