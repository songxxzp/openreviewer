[
    {
        "id": "o4XH_D7CCh",
        "original": null,
        "number": 1,
        "cdate": 1666474787338,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666474787338,
        "tmdate": 1666474787338,
        "tddate": null,
        "forum": "3yEIFSMwKBC",
        "replyto": "3yEIFSMwKBC",
        "invitation": "ICLR.cc/2023/Conference/Paper5847/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces AutoMoE, an AutoML framework for searching efficient sparsely activated models. Experiments are conducted on three machine translation benchmark datasets, including WMT'14 En-De, WMT'14 En-Fr, and WMT'19 En-De. Compared with previous models (e.g., HAT), AutoMoE achieves a better trade-off between FLOPs and BLEU scores.  ",
            "strength_and_weaknesses": "Strength:\n1. This paper is well-written. \n2. The practical improvements over previous methods are clear, and the search cost is affordable. \n\nWeakness:\n1. 1. I feel this work is a straightforward application of existing NAS methods on MoE with some engineering efforts. The novelty of the proposed framework is limited. According to the experiments, the improvements are not surprising, given that the search space and the overall framework are much more complicated than previous methods. \n2. In addition to FLOPs, the GPU latency of the models should be included in Table 4. \n",
            "clarity,_quality,_novelty_and_reproducibility": "I think this paper is well-written and easy to reproduce. ",
            "summary_of_the_review": "I have some concerns regarding the novelty of this work. And it is unclear to me whether AutoMoE is better than previous models when comparing their efficiency on hardware (e.g., GPU).  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5847/Reviewer_CMSP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5847/Reviewer_CMSP"
        ]
    },
    {
        "id": "6AOAgzVgIbB",
        "original": null,
        "number": 2,
        "cdate": 1666562332239,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666562332239,
        "tmdate": 1666562332239,
        "tddate": null,
        "forum": "3yEIFSMwKBC",
        "replyto": "3yEIFSMwKBC",
        "invitation": "ICLR.cc/2023/Conference/Paper5847/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper focuses on neural machine translation and develops a new NAS approach. Specifically, the authors propose a transformer-based search space by incorporating the MoE module into it. Nevertheless, the novelty seems very limited since the proposed method can be regarded as a direct application of HAT on MoE architectures. Moreover, the performance improvement over HAT is marginal.",
            "strength_and_weaknesses": "Strengths:\n1. This paper develops a new NAS approach by incorporating MoE module into the search space\n2. This paper provides sufficient details of the proposed search space as well as interesting analysis.\n\n\nWeaknesses:\n\n1. Compared with HAT, the novelty of this paper seems very limited. If I understand correctly, the essential difference from HAT is including the MoE module into the search space. Besides this, all the others (including supernet training, evolutionary search algorithm) remain exactly the same as HAT.\n\n2. How do the authors evaluate latency in Table 1? If the latency is measured based on a single sample, the practical latency for a mini-batch may be very slow since different sub-networks are activated for different inputs. Thus, it would be better to provide the comparisons of latency on both single sample and a mini-batch.\n\n3. The performance improvement over HAT seems very marginal in Table 4.\n\n4. In NAS papers, random search is always a strong baseline. Thus, it would be better to include the comparisons with random search in Table 4.\n\n5. In Table 5, this paper only shows the results of the proposed method. It is interesting to see the comparisons between the proposed method and HAT, which is also able to produce architectures under specific constraints. \n\n6. Does AutoMoE also need to train a latency predictor like HAT? If not, how to efficiently obtain the latency for candidate architectures during the search process?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. However, the novelty is limited compared with HAT.",
            "summary_of_the_review": "The novelty is limited since the proposed method can be regarded as a direct application of HAT on MoE architectures. Moreover, the performance improvement over HAT is marginal.\nMaybe I missed something important. If there are some other essential differences, I am happy to change the rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5847/Reviewer_vutH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5847/Reviewer_vutH"
        ]
    },
    {
        "id": "5Tso3M4JZm",
        "original": null,
        "number": 3,
        "cdate": 1666646773511,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646773511,
        "tmdate": 1670007011761,
        "tddate": null,
        "forum": "3yEIFSMwKBC",
        "replyto": "3yEIFSMwKBC",
        "invitation": "ICLR.cc/2023/Conference/Paper5847/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies efficient sparsely activated transformers. The authors expand the search space of transformers with a few mixture-of-experts (MoE) design choices: i.e., the number and size of experts after each transformer block. Then, the authors apply weight-sharing super network training and evolutionary search to explore the best sparsely activated MoE architecture under the given resource constraints. The proposed AutoMoE has achieved moderate #FLOPs reduction compared with the dense transformer.",
            "strength_and_weaknesses": "---\nStrengths:\n- The paper is very well-written and easy to follow.\n- The proposed framework is technically sound and performs well on machine translation benchmarks.\n- The paper provides sufficient implementation details (e.g., design space configuration, and search and training hyperparameters). The submission also comes with a code implementation, which will facilitate other researchers' reproduction.\n\n---\nWeaknesses:\n- The technical novelty of this paper is a bit limited. The core contribution lies in the search space design since the other components (i.e., super network training and evolutionary search) are standard (used in most neural architecture search papers). However, the authors adopt most of their search space designs from HAT except those related to MoE. The support for elastic MoE also seems straightforward (basically the same as FFN). Though I like the idea of incorporating MoE into the transformer's design space, I am not confident whether these technical contributions are sufficient for publication.\n- The experimental evaluation needs improvement:\n  - Despite mentioning the baseline with homogeneous experts in the text, the authors have not provided the corresponding results in Table 4.\n  - The authors have only presented #FLOPs in Table 4. However, #FLOPs reduction does not necessarily translate into measured speedup.\n  - The authors have compared AutoMoE with manually-designed baselines in Table 1. However, I suspect the shallower encoder contributes the most to the latency and #FLOPs reduction.\n\n---",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has good clarity, fair quality but limited novelty.",
            "summary_of_the_review": "My current recommendation is primarily based on the limited novelty of this paper. However, I would love to see the authors' response before making the final recommendation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5847/Reviewer_Yxc3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5847/Reviewer_Yxc3"
        ]
    }
]