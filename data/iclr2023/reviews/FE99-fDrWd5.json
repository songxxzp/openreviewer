[
    {
        "id": "mGTLHZ5-8ey",
        "original": null,
        "number": 1,
        "cdate": 1666659677769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659677769,
        "tmdate": 1669225452682,
        "tddate": null,
        "forum": "FE99-fDrWd5",
        "replyto": "FE99-fDrWd5",
        "invitation": "ICLR.cc/2023/Conference/Paper4969/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new semi-parametric learning method called SPIN. It builds upon non-parametric transformers (NPT), which use attention between training points to learn compact and effective predictors at the cost of quadratic runtime in the number of training points considered (context size). SPIN addresses this by utilizing inducing points, a small constant number of vectors summarizing the large number of training points. The resulting architecture scales linearly in the context size at training time, and linearly only in the selected number of inducing points at test time. It is demonstrated that depending on the selected context size, SPIN yields improved prediction performance, reduced computational cost, or both. Additionally, SPIN is employed for metalearning on synthetic and genome imputation data, where it delivers improved/competitive performance, respectively.",
            "strength_and_weaknesses": "Strengths:\n - The paper is well motivated, as it addresses a clear weakness of NPT in a suitable and convincing manner.\n - The method is clearly explained and sensibly constructed.\n - The experimental evaluation is thorough and highlights important aspects of the method, including its sensitivity to context size, its parameter count, and its memory usage.\n - The improvements over NPT on UCI and meta learning experiments are quite convincing.\n\nWeaknesses:\n - On the genome imputation experiment, there is no significant performance increase over prior methods, the result is only notable in terms of SPIN's generality and low parameter count. Maybe some more concretely desirable property such as inference time could be measured here for a more convincing argument.\n - The resolution of the figures, especially figure 3, should be improved, ideally by making them a vector graphics.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. \n\nThe experiments are thorough and high quality. \n\nUsing inducing points in the context of NPT is novel to my knowledge, and the authors have done a good job building upon this idea by including large scale meta-learning experiments.\n\nReproducibility of the method is ensured by a thorough appendix and provided code.\n\nQuestion for the authors: How do you explain the performance gains in Table 2 over NPT when the context size is identical? Intuitively, it seems like the bottleneck introduced by inducing points should adversely impact performance.",
            "summary_of_the_review": "In summary, I think the paper represents a clear step forward for semi-parametric methods. The idea of using inducing points is well executed and thoroughly evaluated. The meta-learning and genome imputation experiments push the applicability of such methods to new domains. I therefore recommend acceptance.\n\n-----\n\nPost Rebuttal Update: After reading the other reviews and the authors' responses, I maintain my positive view of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4969/Reviewer_wVgc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4969/Reviewer_wVgc"
        ]
    },
    {
        "id": "f9LWbH9S-C-",
        "original": null,
        "number": 2,
        "cdate": 1667441871561,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667441871561,
        "tmdate": 1667441871561,
        "tddate": null,
        "forum": "FE99-fDrWd5",
        "replyto": "FE99-fDrWd5",
        "invitation": "ICLR.cc/2023/Conference/Paper4969/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce a mechanism for semi-parametric prediction using neural attention mechanisms and a variant of inducing points.\n\nIn this work, the authors combine ideas such as cross-attention between attributes and between datapoints, yielding a framework -denoted SPIN- which given a new dataset produces a set of lower dimensional inducing points H in an encoder structure.\nThis set H, can either be used to update to the next set H given data, or it can be used in a prediction layer in a comnstruct which allows performing predictions of the style p(y|x, H).\n\nThe authors propose to then utilize SPIN for a neural process variant, which using these inducing points is able to incorporate large context sizes in context datasets D_c to make predictions on query datasets.\n\nIn their experiments, they show competitive performance on various tasks related to the neural process literature where they exhibit strong scalability due to better memory utilization of their method compared to useful baselines.\nOf particular note is an example for genotype imputation, which seems to benefit strongly in this meta-learning scenario from the proposed methodology.",
            "strength_and_weaknesses": "Strengths:\nThe technique the authors propose appears to work well empirically.\nWhile the core setup is a familiar one of neural processes, the particular handling of inducing points here allows the models to remain competitive and scale beyond the typical comparators, which is a promising avenue for its future use as one may want to incorporate larger context datasets D_c.\n\nThe authors do a good job comparing to a few benchmarks such as UCI datasets, GP-prior modeling, and genotype imputation.\n\nWeaknesses:\nSome brief ablation experiments seem to indicate that the lion;s share of the work is carried by the cross-data attention modeling part here. It would be great to get deeper insight into this.\nIn general the work appears to propose an architecture which feels somewhat \"engineered\" and while the authors show that it works, it would be great to understand better why each part works, how sequencing these different layers makes sense , what calculation the model is really amortizing over here, and deepen the analysis of the ablations.\nWhat does the ABLA layer really do? I understand the authors claim it refines H_A, but it is not clear to me what this is amortizing and the work does not really convey the intuition here.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors write relatively clearly here and the work is empirically of high quality.\n\nThe writing is also of good quality, and the authors put great attention to detail into listing lots of the many details in their architecture, but by its own nature this work feels somewhat engineered and is not easy to deeply understand from a mathematical principles point of view, but rather as a list of specific implementations that has a desired effect. This is partly the nature of the work, but I would hope for more understanding why these techniques perform well.\nWhat the authors do a great job on is explaining the memory and computational footprint of this architecture, which is also a key aspect of its ability to scale. I came away with a good understanding of why this would scale well on larger context sets thanks to the writing.\n\nI was appreciative of the authors' great job to position this work fairly among newer and older literature, the scholarship here is thorough and fair and spans all walks of ML where inducing points and meta-learning/neural processes have been seen.\n\nWith respect to reproducibility, I do believe I could reimplement the key layers here after reading the paper and the authors share lots of detail in their appendix. By its nature -again- work like this probably depends strongly on the details of the experimental tuning, I came away with the impression the authors are handling this well.",
            "summary_of_the_review": "The authors present an evolution and marriage of the streams of work on neural processes and cross-datapoint attention, which they bring together to propose an architecture which can utilize large context sets to perform semi-parametric prediction.\nI really enjoyed the key application to genotype imputation as an example that would be uniquely enabled by this model.\n\nOverall the paper is of good enough quality and interest and although it does not meaningfully extend or propose much technical novelty or deep insights into why the utilized pieces here work as well as they do, the empirical qualities regarding scalability and positioning of the paper appear and useful and valuable.\n\nIf the authors were able to better explain and understand the interplay of their layers and which computations they are amortizing I would have been more excited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4969/Reviewer_TnrL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4969/Reviewer_TnrL"
        ]
    },
    {
        "id": "3TYU1IZyRCh",
        "original": null,
        "number": 3,
        "cdate": 1667450132017,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667450132017,
        "tmdate": 1667450132017,
        "tddate": null,
        "forum": "FE99-fDrWd5",
        "replyto": "FE99-fDrWd5",
        "invitation": "ICLR.cc/2023/Conference/Paper4969/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces semi-parametric inducing point networks, which learn a latent representation of the entire training set which can be queried at inference time efficiently. The authors experiment on the UCI benchmarks, a synthetic meta-learning task, and a genomic imputation task. The key contribution is a decrease in computational cost vs existing methods.",
            "strength_and_weaknesses": "Strengths:\n\nThis paper is excellently written. I enjoyed reading it - particularly the intro and background, and it flows nicely.\n\nOverall, the results are slightly better than existing methods. But SPIN requires significantly less GPU usage and param count than existing methods, as well as allows for larger context size. Computational cost grows linearly in the training set size compared to  the quadratic growth of existing methods.\n\nThe experiments seem convincing, and I particularly like the genomic imputation experiments. This type of model seems like a good method for genomics, where we always have the reference genome and prior lab-experiments.\n\n\nWeaknesses:\n\nThe motivation for why we need more GPU efficient methods compared to NPT isn't very clear. Table 2 shows the effect of a larger context size that NPT can't fit in memory, but aside from that I didn't see anything else. For something like genomic imputations, those aren't time-sensitive so it shouldn't be big deal waiting a little longer or using a little more compute.\n\nThe paper has a lot of equations, but it boils down to a fairly simple method using multi-head cross-attention. I appreciated the detail, but I think the paper could be greatly simplified.\n\nThe related work section lists a lot of papers, but doesn't provide much of a comparison to the proposed method.\n\nWhat is the intuition of \"folding\" in XABD? I think this was a key part of the paper, but it wasn't motivated. Similarly, what is the intuition for ABLA?\n\nMinor: \n\nWhat are \"attributes\" in XABA? I found this nomenclature confusing.\n\nFigures should be vectorized.\n\nI would move the \"Attention Mechanisms\" subsection to an appendix (aside from explaining what MAB is).\n\nTables and Figures could be organized a little bit better. Hard to read in the text when it's flowing in every direction around the figures and tables.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written overall, aside from some explanation of the design choices and intuitions.\n\nIt's hard for me to speak on novelty since I'm not very familiar with the background work. Overall, it gave me impressions of the perceiver model (mentioned in related work), but applied to encode the entire training set.\n\nThe experiments seem nice, but again I don't know the standard procedure for semi-parametric model evaluations.\n\nCode and data availability section in the Appendix is nice.",
            "summary_of_the_review": "Overall, I think this is a solid paper and it encourages future work on compressing training sets into queryable encodings. The key contribution seems to be the computational efficiency of their method. It's clear that it's more computational efficient at the same performance, but it's not very clear how important computational efficiency is here.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4969/Reviewer_GJez"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4969/Reviewer_GJez"
        ]
    },
    {
        "id": "v2JZNIOgE2",
        "original": null,
        "number": 4,
        "cdate": 1667530344348,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667530344348,
        "tmdate": 1667530344348,
        "tddate": null,
        "forum": "FE99-fDrWd5",
        "replyto": "FE99-fDrWd5",
        "invitation": "ICLR.cc/2023/Conference/Paper4969/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops a special kind of neural network architecture that the authors call SPIN, semiparametric inducing point networks.  This architecture takes a dataset as input and using a \n\u201cdataset encoder\u201d outputs a latent set of \u201cinducing points\u201d which one could imagine either as pseudo-examples representing the larger dataset or summary statistics describing the data.  These inducing points are incorporated into a prediction network via cross-attention layers that effectively compute dot products between a query and the inducing points.  The authors demonstrate the efficacy of this approach empirically on the UCI data, a meta learning setup and a genotype imputation problem.",
            "strength_and_weaknesses": "Strengths:\nThe paper is clear and well written.\nThe proposed model seems intuitive, scalable and effective.\nI like that the authors found multiple problem domains where their proposed approach would outshine traditional GP or deep learning approaches.  Genomic imputation is neat and seems like an important real world application.\nThe method seems to work well in the experiments presented.\nThe GPU memory required by the method seems clearly less than the stronger baseline NPT.  \n\nWeaknesses:\nA major aspect of the neural process literature is that the models are a stochastic process, i.e. they induce a distribution over functions conditioned on some observed data.  This paper completely ignores that part and considers only predictive accuracy.  For example, a major claimed advantage of neural processes is that they can provide a high quality estimate of uncertainty away from the data.  This paper completely ignores the \u201cprocesses\u201d part of neural processes and evaluates the models as if they\u2019re standard deep networks on standard deep learning benchmarks (agreed genomic imputation is non-standard and more exciting). \nThe treatment of existing work is a little sloppy.  I\u2019m not sure why inducing points and deep kernel learning are attributed to Wilson & X.  There\u2019s a deep literature on both subjects and these papers are one contribution of many in each subfield (neither the first, last or SOTA).\nThe choice of baselines seems quite weak.  Particularly, UCI has been completely and utterly annihilated by Bayesian deep learning papers.  Take a look at all the papers citing Bayesian dropout (Gal & Gharhamani).  The fact that the baselines here are: a method from 2001 - GBT, K-Nearest Neighbors and \u201cMultilayer Perceptron\u201d seems quite suspect given that one can just extract the numbers from recent papers.  Why are all the baselines from 20 years ago or more?  That is compounded by the fact that only a ranking is provided, so a reader can\u2019t directly compare to more recent literature.  I found the results in the appendix and confirmed that 1) in general they\u2019re not as strong as dropout (which is taken to be a straw man in many papers) and 2) the results often don\u2019t seem statistically significant.  Could the authors provide some more context about why they chose these baselines and the significance of the ranking results?\nThe ranking results are just not statistically significantly better than NPT.  I think it\u2019s ok to say they are competitive with NPT but require less than half the memory, but it doesn\u2019t seem justified to say that they are better on average.\nIf the claim is that SPIN / IPNP is faster, then plot with walltime as an axis.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and easy to follow.  I think the presented architecture is novel, but I\u2019m a little unsure given how many transformer / attention variants there are currently.  So I would defer to other reviewers on novelty.  The authors provide detail about their architectures and in the appendix they detail hyperparameters, so I\u2019d feel confident I could reproduce the experiments with some effort.\n\nNitpicking:\nInducing points are not attributable to Wilson et al.  That\u2019s a strange citation to have. You should cite e.g. Snelson & Ghahramani, 2005.  I\u2019d really like to see a better treatment of the existing literature on inducing point approximations.\n\n\u201cInference\u201d is an overloaded term in deep learning and statistical inference.  I.e. they mean different things in deep learning and non-parametric modeling.  I\u2019d just avoid using it altogether.\n\n",
            "summary_of_the_review": "Overall, I think this paper represents an interesting innovation that seems novel and is well described.  I think the paper missed the mark a bit in terms of formulating / evaluating the method as a stochastic process.  The experiments are useful as a proof of concept and the genotype imputation experiment presents a scenario where this model seems to make sense over standard deep learning approaches.  The UCI experiments are a bit weak in terms of the baselines and the overall results.  Overall I think the novelty and presentation warrant an accept but the paper could be stronger.  I would vote accept but I wouldn\u2019t champion the paper strongly (e.g. for a presentation).  \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4969/Reviewer_jnqW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4969/Reviewer_jnqW"
        ]
    }
]