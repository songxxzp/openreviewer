[
    {
        "id": "nV2Kj651w42",
        "original": null,
        "number": 1,
        "cdate": 1666555978241,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555978241,
        "tmdate": 1666555978241,
        "tddate": null,
        "forum": "VBQZkYu22G",
        "replyto": "VBQZkYu22G",
        "invitation": "ICLR.cc/2023/Conference/Paper4432/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents SmilesFormer, a transformer based method that aims at generating novel molecules with a multiple properties objective. This is achieved by training a variational auto-encoder fed with fragments of molecules that are then encoded into an embedding space. Afterwards, they are decoded to reconstruct the molecule by generating a valid canonical smile sequence. In order to obtain more desirable properties for the molecule, its representation is optimized in the embedding space by maximizing a score that represents the required properties. The authors claim that the model performs well against state-of-the-art methods.\n",
            "strength_and_weaknesses": "Strengths\n- the process of data-augmentation by using different strategies of smile sequence fragmentation or modification but still aiming at reconstructing valid canonical smile sequences is very interesting\n- the introduction and related works sections are well written, it explained well the overall functional parts of the model and also have interesting points about the challenges for molecule design using generative models.\n\nWeaknesses\n- the description of the method should be improved, I had to re-read it multiples times before I was able to understand it. I believe that with a clear division of the description about how the diverse parts of the model are trained and another section with description about how the molecule generation process works could improve the readability of this paper.\n- the paper lacks proper descriptions of the datasets used for the experiments\n- This work cannot be reproduced. It lacks descriptions about how the experiments were run. The \"model and training\" section from the appendix should be expanded and moved to the main text.\n- there are references for the evaluation, but the main text should at least briefly explain the evaluation measures. Now, it\u2019s not easy to see why these measures show that the method performs well.\n- although the method architecture is interesting, none of its parts are novel, authors should highlight how their modifications to existing parts of the model compare to the original papers. In particular, the multi-objective traversal of the latent space is not properly cited, but it is not novel for protein and drug engineering, this is not clear in the current version of this paper.\n- the donepezil rediscovery is interesting, but the effectiveness of the model can\u2019t be derived from a single case study. I think that process could have been replicated with other drugs in order to give more confidence to the rediscovery evaluation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity: the introduction and related works sections describe well the paper as a whole and the difficulties of the problem that the authors try to tackle. The description of the specifics of the model should be improved as explained above. There is not clarity about the evaluation, except for the donepazil rediscovery process.\n\nQuality: SmilesFormer is not properly evaluated against other methods in the state-of-the-art. The current evaluation shows that the representation learning has some promise, but it is not enough as substance for all claims in this paper. This is aggravated by the notation and overall lack of clarity of the paper, which render the reproducibility of this work almost nonexistent.\n\nOriginality: expanding on what was explained above on weaknesses, the parts that works as functional parts, such like the use of VAE for reconstruction of smiles and the use of gradient descent for molecule optimization of the method are not novel.\n",
            "summary_of_the_review": "It is clear the authors understand the problem at hand, but the paper lacks a clear description about the experiments that were performed to validate the claims. The explanation of the proposed model still needs work to be understandable and not let the readers make assumptions about how the molecule generation process works. The evaluation of the model is not clear, and it is not enough to sustain all the claims made by the author.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4432/Reviewer_zCNZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4432/Reviewer_zCNZ"
        ]
    },
    {
        "id": "OeCNw3Q4Nb",
        "original": null,
        "number": 2,
        "cdate": 1666605862850,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605862850,
        "tmdate": 1666605862850,
        "tddate": null,
        "forum": "VBQZkYu22G",
        "replyto": "VBQZkYu22G",
        "invitation": "ICLR.cc/2023/Conference/Paper4432/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes a transformer-based language model for SMILES strings. The model is a VAE trained on entire molecules, molecular fragments, or their composition. It can be used for molecular (multi-)property optimization with a stochastic latent space traversal technique. The experiments show that it can be applied with success on multiple property optimization, as well as in real-world drug design scenarios (in particular, the authors apply the method to rediscover Donepezil).",
            "strength_and_weaknesses": "**Strengths**\n- Works well in multi-property optimization scenarios\n- Very efficient in terms of oracle calls\n- Re-discovering of Donepezil looks impressive.\n\n**Weaknesses**\n- not evaluated on standard benchmarks such as GuacaMol, MOSES\n- limited novelty\n- literature not fully discussed",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is well written and easy to read. \n\n**Quality**\n\nOverall, the paper is of good quality. However, I would have expected that the proposed model was tested in a standard benchmark such as GuacaMol or MOSES. The explanation provided by the authors for not doing so (quoting: \"the task of molecular design is usually related to specific targets, earlier proposed benchmarks do not fully represent how a generative model is utilized in the drug discovery pipeline\") is not convincing since most of the works cited in this paper (all generative models that do molecular optimization) do provide a benchmark evaluation.\n\n**Novelty**\n\nExtremely limited. The idea of using Transformer-based models to learn a language model of SMILES is not new [1]; the use of a fragment-based approach for SMILES strings is itself not new ([2] uses a very similar strategy to decompose a molecule SMILES into fragments using the BRICS algorithm); the idea of using gradient ascent to optimize the molecular property in latent space is strikingly similar to [3]. All of these works are not listed in the background part, and the differences with the proposed approach are not discussed.\n\n**Reproducibility**\nNot reproducible (no code is provided).\n\n[1] Irwin et al., _Chemformer: a pre-trained transformer for computational chemistry_. Mach. Learn.: Sci. Technol. 3 (2022)\n\n[2] Podda et al., _A Deep Generative Model for Fragment-Based Molecule Generation_. AISTATS (2020)\n\n[3] Jain et al., _Multi-target optimization for drug discovery using generative models_. ICML workshop on Computational Biology (2021) ",
            "summary_of_the_review": "In summary, this seems to be a borderline paper. However, at the moment the weaknesses greatly overcome the strengths, so I'm forced to recommend rejection.\n\nBesides addressing the points raised above (see Quality/Novelty), I'd like the authors to answer/comment on the following questions/comments in order to let me be able to eventually raise my score:\n\n- What are the valid/unique fractions of the molecules generated in tables 1 and 2?\n- Can you show the distribution of molecular features (such as the number of rings, bonds, atoms, etc.) of the generated molecules in comparison to the molecules in the test set?\n- Can you provide an explanation as to why the model seems to perform not so well in the logP optimization task? (see Table 3)\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4432/Reviewer_US7Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4432/Reviewer_US7Z"
        ]
    },
    {
        "id": "iars9Ma6hg",
        "original": null,
        "number": 3,
        "cdate": 1666713509834,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713509834,
        "tmdate": 1666713509834,
        "tddate": null,
        "forum": "VBQZkYu22G",
        "replyto": "VBQZkYu22G",
        "invitation": "ICLR.cc/2023/Conference/Paper4432/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes SmilesFormer, a transformer-based model that learns a latent space between its encoder and decoder module. The model is trained with 2 main objectives: a) s molecule reconstruction objectives where the model takes in fragments of a molecule and learn to reconstruct its original structure and b) a KL divergence loss between its learned latent space and a normal distribution prior. Input augmentations, such as non-canonical SMILES and fragments made through the Recap decomposition or fragment-on-bonds approaches, were used to better learn molecular representations through reconstructions. During inference, the model can take in an input query and generate a modified variant through gradient step backpropagated from oracle models that predict the target properties of the generated molecules, with stochasticity introduced through dropout. The authors conducted experiments to show that the SmilesFormer can outperform baseline methods in several instances in the applications of multi-objective de novo design with better oracle efficiency. ",
            "strength_and_weaknesses": "Strengths:\nProposed methods showed competitive performances compared to existing baselines\nThe fragment-based input augmentation is a novel approach to enhancing molecular representations in molecular deep learning. \nWeaknesses:\nThe experimental section on \u201cDonepezil rediscovery\u201d relies on many post-processing steps on top of the generation with SmilesFormer and generates as many as 30k candidates for the Donepezil scaffold to be discovered. Without comparison with a suitable baseline in this section, it is hard for the reader to appreciate the impact of this section.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Figure 1: the dotted lines cross one another, making it hard for the reader to easily discern the flow. Features such as words and asterisks on molecules are too small to see.\nTable 2: it would be clearer if the authors include the meaning of SR, Nov and Div, which I assume mean \u2018success rate\u2019, \u2018novelty\u2019 and \u2018diversity\u2019 respectively.\n",
            "summary_of_the_review": "The paper studies an important application of de novo molecular design and proposes novel contributions such as the fragmentation-based optimization. The other parts of the SmilesFormer mostly relies on existing approaches. My main concern about this manuscript is the \u201cDonepezil rediscovery\u201d section, as mentioned in the weakness, where much post-processing is involved and many candidates are generated for the compound of interest to be discovered. A comparison with baselines in this section would help the reader to better appreciate the model\u2019s impact.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4432/Reviewer_TxfB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4432/Reviewer_TxfB"
        ]
    }
]