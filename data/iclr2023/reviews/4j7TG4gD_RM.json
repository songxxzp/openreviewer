[
    {
        "id": "-Is_EtUhgnk",
        "original": null,
        "number": 1,
        "cdate": 1666573824828,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573824828,
        "tmdate": 1666573824828,
        "tddate": null,
        "forum": "4j7TG4gD_RM",
        "replyto": "4j7TG4gD_RM",
        "invitation": "ICLR.cc/2023/Conference/Paper3400/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a backdoor defense for federated learning. The proposed defense is motivated by the differences in the output layer with and without backdoor attacks and aims to identify such changes to prevent backdoor attacks. ",
            "strength_and_weaknesses": "Strengths:\n1. The paper finds an interesting observation that the backdoor attack generates different output distributions compared with clean models.\n2. The proposed defense is effective against multiple backdoor attacks and can eliminate backdoor attacks even when 40% of clients are malicious.\n3. The paper investigates the defensive effectiveness with different sizes of the validation dataset.\n\nWeaknesses:\n1. The proposed defense measures the distributional difference between clients to identify malicious updates. This defense assumes that all clients share similar data distributions.  However, data heterogeneity is a natural property in federated learning, where data distributions vary among clients. The paper only considers the imbalance data distribution with \\alpha=1, which is not a strong non-iid case. It would be better to consider stronger non-iid data distributions. \n2. The paper only considers some basic backdoor attacks and defenses. State-of-the-art attacks (e.g., edge case backdoors [1]) and defenses [2, 3, 4], should be evaluated/compared in the paper. \n3. The proposed defense requires validation data on the server side to update the global model and use the global model to measure the distributional distances. This requires validation data to be similar to the clients\u2019 data. It is nice to see the paper investigate the impact of the validation data size in the defense. It would be great if the paper could also investigate the different data distributions of the validation data.\n\n[1] Wang, Hongyi, et al. \"Attack of the tails: Yes, you really can backdoor federated learning.\" Advances in Neural Information Processing Systems 33 (2020): 16070-16084.\n\n[2] Andreina, Sebastien, et al. \"Baffle: Backdoor detection via feedback-based federated learning.\" 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS). IEEE, 2021.\n\n[3] Hayase, Jonathan, et al. \"SPECTRE: defending against backdoor attacks using robust statistics.\" ICML, 2021.\n\n[4] Tang, Di, et al. \"Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection.\" 30th USENIX Security Symposium (USENIX Security 21). 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, but the paper does not provide sufficient evidence to demonstrate the effectiveness of the proposed defense.",
            "summary_of_the_review": "The paper investigates a timely problem of backdoor defenses in federated learning. The proposed defense is well-motivated. However, the comparison with the state-of-the-art attacks and defenses is missing in the paper. The paper does not consider the strong non-iid settings in federated learning.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3400/Reviewer_qsGk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3400/Reviewer_qsGk"
        ]
    },
    {
        "id": "2BAKvvV6ru",
        "original": null,
        "number": 2,
        "cdate": 1666632576882,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632576882,
        "tmdate": 1666632576882,
        "tddate": null,
        "forum": "4j7TG4gD_RM",
        "replyto": "4j7TG4gD_RM",
        "invitation": "ICLR.cc/2023/Conference/Paper3400/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce a novel robust aggregation method for filtering backdoors in federated learning by using a trusted validation dataset. For this, the authors look at the differences in the distribution of the output layer for the different trusted and untrusted clients, removing those updates that deviate from the distribution in the trusted clients. The authors evaluate the proposed method against two state-of-the-art backdoor attacks in federated learning: Neurotoxin and the Distributed Backdoor Attack, showing the effectiveness of the proposed method to mitigate these attacks. ",
            "strength_and_weaknesses": "Strengths: \n+ Defending against backdoors in federated learning is a challenging problem of interest for the research community. The experimental results in the paper show that the proposed method (TAG) can mitigate the effect of state-of-the-art backdoor attacks such as Neurotoxin and DBA (Distributed Backdoor Attack). \n+ Overall, the paper is well written and the authors explain well the intuition about the method they proposed. \n\nWeaknesses: \n+ In the experiments, the authors just compared with Coordinate Median and Trimmed Mean. This comparison is not really fair: on one side, these techniques do not assume the existence of a trusted validation dataset. On the other hand, these techniques are not specifically designed against backdoor attacks, but more general data and/or model poisoning attacks. It would be necessary to compare with existing defenses that rely on a similar set of assumptions, e.g. Fang et al. \u201cLocal Model Poisoning Attacks to Byzantine-Robust Federated Learning\u201d, which uses trusted validation datasets.\n+ The proposed method just looks at the last layer of the models. It is true that, for attacks like Neurotoxin and DBA this can be enough to mitigate the attack. However, this opens the door for adaptive attacks where the adversaries are aware of the defensive method applied. Then, the attackers can try to manipulate the parameters in the previous layers to try to evade detection. In this sense, the paper lacks an analysis of the robustness against adaptive attacks and possible limitations. \n+ Some of the experimental settings used are not clearly described in the paper. For instance, the number of participants in the federated learning tasks, learning rates, optimizer, batch size, etc. I think this is especially important in the case of the number of participants, which is a factor that can play a critical role in the performance of the defense. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and well written. The proposed approach is novel, although the literature review is not very complete (important attacks and defenses in backdoors for federated learning are missing), which would be necessary to position the paper better, especially in terms of the threat model and the assumptions for the defender. \nOn the reproducibility side, as mentioned before, there are certain details about the experimental settings that are missing and that are important for the sake of reproducibility and to assess better the quality of the results.  \n",
            "summary_of_the_review": "Although the assumption of having a trusted validation set/clients can be restrictive to certain applications, the algorithms proposed in the paper is interesting and seems to provide good performance against certain backdoor attacks. However, there are certain aspects that require a more thorough analysis: \n+ The comparison with coordinate median and trimmed mean is not really fair as they do not assume having access to a trusted validation set or trusted clients. In this sense it is necessary to compare with existing defenses that make use of this assumption to really validate the benefits of the proposed approach and its possible limitations. \n+ Looking just at the last layer can make the algorithm vulnerable to other attacks. Given this, I think that the evaluation against an adaptive attackers that is aware of this is necessary to have a more realistic evaluation of the proposed defense.  \n+ The description of the experimental settings used should be clearer and more detail. \n+ Finally, the assumptions made in Proposition 1 and 2 about the uniform distribution of the class-conditional distances is not well justified and looks cherry picked. I think that the authors should justify better their assumptions in this sense.  \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3400/Reviewer_SvhM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3400/Reviewer_SvhM"
        ]
    },
    {
        "id": "7zOMDQ2Vu3",
        "original": null,
        "number": 3,
        "cdate": 1666657026294,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657026294,
        "tmdate": 1666657026294,
        "tddate": null,
        "forum": "4j7TG4gD_RM",
        "replyto": "4j7TG4gD_RM",
        "invitation": "ICLR.cc/2023/Conference/Paper3400/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a defense against backdoor attacks on federated learning.\nThe method works by detecting malicious users by analyzing the final layers'\noutput. The proposed method is based on observing that the distributions of\nthe final layer's output between the malicious and benign models differ.\nExperiments on CIFAR-10, CIFAR-100, and STL-10 demonstrate that the proposed\nmethod is effective.",
            "strength_and_weaknesses": "Strengths\n\n\u00b7 Interesting topic.\n\nWeaknesses\n\n\u00b7 Comparisons with some related works are missing.\n\u00b7 Generalization to different models is unclear.\n\u00b7 Discussion about the potential adaptive attacks is missing.\n\u00b7 Details about the experiments in Figure 1 are unclear.\n\u00b7 How to select the output layer is unclear.\n\u00b7 The writing needs improvement. Details of the critical experiments (e.g., Figure\n1) is unclear.",
            "clarity,_quality,_novelty_and_reproducibility": "An adaptive attack is missing. The proposed defense is based on the observation\nin Section 3. Is the proposed detection method still effective under adaptive\nattacks (the attacker is aware of the defense and tries to bypass it)? During\npoisoning, a possible adaptive attack constrains the backdoored models' final\nlayer output distributions. In other words, making the output distributions of\nthe backdoored models similar to that of benign models. Goldwasser et al.\n(Planting Undetectable Backdoors in Machine Learning Models) show that it is\npossible to do so. In practice, Doan et al. NeurIPS 2021 have also shown that backdoors can be imperceptible from both the input and latent spaces.\n\nIn this paper, the proposed defense method is based on the observation in\nFigure 1. Unfortunately, the details of the key experiments are missing. For\nexample, the used datasets, models, and attacks are all unclear. The empirical\nresults are also limited to a single setting. Thus, it is unclear to me whether the conclusion is reasonable.\n\nThe definition of the \"output layer\" is unclear. It lacks a discussion about\nhow to select the output layer. In Figure 1, this paper uses the final hidden\nlayer as the output layer. It is unclear why the final hidden layer is\nselected. The results of using other layers are missing.\n\nComparisons with some related works are missing. Only two baseline defenses\n(i.e., Median and Trim-mean) are involved in the experiments. It lacks\ncomparisons with related works [1,2]. Therefore, it is hard to say if the\nproposed method achieves better performance than existing methods.\nGeneralizations to different models are unclear. Existing defense [1]\ndemonstrated it can generalize to different models. However, all experiments\nin this paper are conducted on a single model, i.e., ResNet18. Thus, the\ngeneralization of the proposed method to different model architectures (e.g.,\nVGG, DenseNet, and ViT) are unclear.\n\n[1] Rieger et al., Deepsight: Mitigating backdoor attacks in federated\nlearning through deep model inspection. NDSS 2022.\n[2] Nguyen et al., FLAME: Taming Backdoors in Federated Learning. USENIX\nSecurity 2022.",
            "summary_of_the_review": "Details of the critical experiments are unclear. The evaluation is also weak since\nit only uses a single model and lacks a discussion about adaptive\nattacks. Discussion on existing attacks is also missing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3400/Reviewer_JmoH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3400/Reviewer_JmoH"
        ]
    },
    {
        "id": "xbbHn_HgE0g",
        "original": null,
        "number": 4,
        "cdate": 1666832519721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666832519721,
        "tmdate": 1666836764517,
        "tddate": null,
        "forum": "4j7TG4gD_RM",
        "replyto": "4j7TG4gD_RM",
        "invitation": "ICLR.cc/2023/Conference/Paper3400/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a defense against backdoor attacks to FL. The assumption is that the system has access to some clean validation dataset, which can be used to filter backdoored model updates from malicious clients via analyzing the output layer distribution. Some evaluation is performed to show the effectiveness of the defense against existing backdoor attacks, and comparison is performed for simple baselines. ",
            "strength_and_weaknesses": "+ Backdoor attack is a severe threat to FL, and thus defending against such attack is relevant. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has limited quality and novelty. \n\n1. When a clean dataset is available, many methods can be used to defend against backdoor attacks, but the paper does not compare with any of them. e.g., Neural Cleanse [A] can be used to detect backdoored local model. Also, FLTrust [B] can be used to defend against backdoor attacks. The authors are suggested to compare with these methods. \n\n2. Adaptive backdoor attacks are not considered. What happens if an attacker knows your defense? How can an attacker adapt its backdoor attack to your defense? It is important to evaluate adaptive attacks since the proposed method does not have certified security guarantees. \n\n\n3. How does the clean dataset distribution impact the performance of the proposed method? \n\n[A] Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. In IEEE S&P, 2019.\n\n[B] FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping. In NDSS, 2021. \n",
            "summary_of_the_review": "The paper studies an important topic, but the novelty and quality are limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3400/Reviewer_zmnT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3400/Reviewer_zmnT"
        ]
    }
]