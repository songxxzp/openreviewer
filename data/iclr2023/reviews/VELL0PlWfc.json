[
    {
        "id": "MTgqMPQUFR",
        "original": null,
        "number": 1,
        "cdate": 1666662130392,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662130392,
        "tmdate": 1666662130392,
        "tddate": null,
        "forum": "VELL0PlWfc",
        "replyto": "VELL0PlWfc",
        "invitation": "ICLR.cc/2023/Conference/Paper1774/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies training language models using total variation distance as opposed to maximum likelihood. While the total variation distance idea was previously proposed by Kang and Hashimoto 2020, they optimized a different upper bound via loss truncation and Pinsker's Inequality. \n\nIn this work, the authors propose an alternative upper bound that is novel and interesting by tackling specific challenges with optimizing the TVD loss (i.e. lack of token level factorization, and explicit demand of data probability distribution)\n\nThe authors provide empirical results both on real and synthetic data. In particular in machine translation (IWSLT De-En), text summarization (GigaWord) and open ended generation (Writing Prompts), the authors' approach outperforms not only MLE but several other techniques in the literature (unlikelihood, D2GPo, loss truncation, and GOLD). \n",
            "strength_and_weaknesses": "Strengths:\n-Novel method\n-Thorough empirical results (both real and synthetic)\n-Well written",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, high quality and novel. ",
            "summary_of_the_review": "I support accepting the paper. It is quite interesting, novel and tackles a very important problem in language models. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1774/Reviewer_Zays"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1774/Reviewer_Zays"
        ]
    },
    {
        "id": "StlR98J4az",
        "original": null,
        "number": 2,
        "cdate": 1666675647216,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675647216,
        "tmdate": 1669727411855,
        "tddate": null,
        "forum": "VELL0PlWfc",
        "replyto": "VELL0PlWfc",
        "invitation": "ICLR.cc/2023/Conference/Paper1774/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Using MLE (maximum likelihood estimation) to train text generation models distributes probability mass to all samples in the dataset, even if the samples are of poor quality or outliers. The authors claim that MLE-trained models would would overestimate the probability of corrupted sequences (which the authors call the \u201cvoid\u201d of the real data distribution) which leads to degeneration.\n\nTo address this problem, the authors propose using total variation distance as shown in Equation (2a) and Equation (2b). Total variation distance would make the model place small or zero probability to low-quality sequences in the training set. However, it\u2019s not easy to apply TVD directly onto text generation, so the authors proposed an upper bound on the TVD (sequence-level) in Proposition 1 / Equation 5. Additionally, given that we do not have the data/reference probability distribution, the target is estimated using a proxy distribution. \n",
            "strength_and_weaknesses": "Experiments are on a synthetic task as well as regular tasks including machine translation and summarization. Experiments are also done on long text generation as well, and I\u2019m glad to see that it works well. \n\n\nI\u2019m glad that the paper involves discussion of Kang and Hashimoto (2020) as well as Pang and He (2021). One concern is that given that the authors\u2019 motivation and the two papers\u2019 motivation are so similar \u2013 all of the approaches (the two papers\u2019 as well as the authors\u2019) propose to downweight unlikely / outlier tokens, I would appreciate some deeper explanation on the pros and cons of each approach, or why the authors\u2019 approach excels. \n\n---\n\nUpdate: I read through the paper again, and responded to the authors' rebuttal below -- I don't really have complaints otherwise. I think the motivation and the execution are great. ",
            "clarity,_quality,_novelty_and_reproducibility": "The code is attached but I don't see README. The code is simply a edited clone of fairseq which contains a large number of files. ",
            "summary_of_the_review": "Important problem. Hoping for better analysis of the pros and cons of related approaches. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1774/Reviewer_PwuP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1774/Reviewer_PwuP"
        ]
    },
    {
        "id": "X8ieSHTHrU",
        "original": null,
        "number": 3,
        "cdate": 1666695739543,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695739543,
        "tmdate": 1669630926210,
        "tddate": null,
        "forum": "VELL0PlWfc",
        "replyto": "VELL0PlWfc",
        "invitation": "ICLR.cc/2023/Conference/Paper1774/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores the Total Variation Distance (TVD) as an alternative to KL divergence for natural language generation. KL divergence is implicitly optimized by the widely used MLE, but the authors argue that this objective is sensitive to noisy data and results in overestimating the probability of corrupted text, while TVD is more robust to outliers. However, one cannot directly optimize TVD, so the authors develop practical bounds and introduce the TaiLr objective that balances the bias-variance tradeoff of estimating it. Experiments on synthetic data, machine translation, text summarization and text generation confirm the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "STRENGTHS\n- This is overall a well-rounded paper: it tackles a very important problem, the proposed solution is sound and well-motivated (although I did not carefully check the correctness of the maths), the paper has more than enough substance, and the reported results are solid.\n- Despite being somewhat math-heavy, the paper is written in a very clear and didactic way. I found it easy to follow and an enjoyable read overall!\n- Comprehensive (although not entirely convincing, see below) experiments covering 3 downstream applications and a synthetic task, with a good number of baselines.\n\nWEAKNESSES\n- The experiments are comprehensive, but limited to rather artificial (or at least not the most relevant) scenarios from today\u2019s perspective. For instance, the authors use a 1-layer LSTM trained on 10k sentences as their oracle for the synthetic data experiments. This sounds ridiculous in 2022, when there are dozens of strong language models publicly available. Similarly, the choice of IWSLT14 as the evaluation benchmark for MT and Gigaword for summarization do not seem the most relevant nowadays (e.g. why not run MT experiments in more languages and/or a more recent WMT benchmark)? In addition, all real-data experiments rely on seq2seq models, while it is decoder-only models that are becoming more and more central in NLP.\n- Connected to the previous point, the authors have a decent number of baselines that they implement, but they do not compare to any number previously reported in the literature. If one checks previously published papers, it seems clear that the reported numbers are rather far from the state-of-the-art. For instance, https://aclanthology.org/2021.emnlp-main.534.pdf reports 38.6 BLEU in IWSLT14, compared to 35.1 for the best system in this paper.\n- Given the previous points, I think that the experiments in the paper are valid as a proof of concept, but are far from convincing me that I should use the proposed approach when I train my next NLG model. You have convinced me that the approach has some potential, but not necessarily that it is better than standard MLE in the real world.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper does a good job in terms of clarity and novelty. Regarding reproducibility, I did not miss any critical experimental detail, although given the complexity of the setup it is virtually impossible to cover all details in the paper, and it would be very valuable to open source the code.",
            "summary_of_the_review": "This is overall a well-rounded paper. It is very well written and has more than enough novelty and substance. The main weakness lies in the empirical side, which is comprehensive and serves as a proof of concept, but falls short in convincing the reader that they should use the proposed approach when training their next model in the real world.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1774/Reviewer_eDTd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1774/Reviewer_eDTd"
        ]
    }
]