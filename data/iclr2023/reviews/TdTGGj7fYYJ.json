[
    {
        "id": "hWkSRr4Eznt",
        "original": null,
        "number": 1,
        "cdate": 1666542492991,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542492991,
        "tmdate": 1669140817682,
        "tddate": null,
        "forum": "TdTGGj7fYYJ",
        "replyto": "TdTGGj7fYYJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5686/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose Pseudo-supervised Contrast (PsCo), an unsupervised meta-learning framework that solves a few-shot learning problem from the constructed few-shot pseudo-tasks. The key idea of the approach is to improve pseudo-labelling progressively during meta-learning by using current and previous mini-batches which is achieved using momentum network and momentum queue. In particular, few-shot tasks are constructed using the current mini-batch to select queries and the queue of previous mini-batches to select K shots. The label assignment matrix is found using Sinkhorn-Knopp algorithm. The proposed approach is evaluated on in-domain and cross-domain few-shot classification benchmarks and the method is shown to outperform baselines.\n",
            "strength_and_weaknesses": "Strengths:\n- The paper is well organized and well written.\n- The idea of progressively improving pseudo-labelling strategy  during meta-learning is novel. Inspired by self-supervised learning, authors achieve this using momentum queue and momentum network. Since the shots are selected from the entire dataset, the tasks constructed in the proposed approach are diverse.\n- The experiments are comprehensive and the method outperforms other unsupervised meta-learning methods.\n- The method scales to large datasets better than existing baselines.\n\nWeaknesses:\n- When constructing the pseudo-tasks, the method first randomly selects N samples and treats them as N queries of different N labels. What if selected samples share the same label, but we assume that they have different labels and try to separate them in the feature space during training? This needs to be clarified.\n- The method constructs the pseudo-tasks using the queue Q of previous mini-batch samples. Where does the Q come from for the first few batches? It would be more clear if there is more explanation about pseudo-tasks construction.\n- The authors compare only to MAML and ProtoNet as supervised meta-learning benchmarks which are not SOA methods so the statements about outperforming supervised meta-learning benchmarks need to be toned-down. Furthermore, for cross-domain learning how would MAML and ProtoNet perform if the representations were pretrained using self-supervised learning before? It is contra-intuitive that unsupervised meta-learning outperform supervised meta-learning and the authors need to be careful about such statements.\n- Why do authors compare only to MoCO on Image-Net and not to other self-supervised learning baselines such as SimCLR and RotationNet?\n- The authors claim that their method is more efficient than other unsupervised meta-learning baselines but they do not compare running time of their approach and the baselines. Additional information that would be needed is the comparison of running time of their approach and self-supervised learning baselines on the ImageNet.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of high quality but some aspects need to be improved in the response (see weaknesses).  The idea is original and novel and the extensive experiments demonstrate its effectiveness. The authors need to release the code to make their work reproducible.",
            "summary_of_the_review": "Overall, this is a good paper with an interesting approach and well designed experiments. There are some things that should be improved and some details remain unclear or are not convincing. The authors should clarify these points in their response.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5686/Reviewer_MCTB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5686/Reviewer_MCTB"
        ]
    },
    {
        "id": "D7ZFciRVOY",
        "original": null,
        "number": 2,
        "cdate": 1666670582427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670582427,
        "tmdate": 1666670582427,
        "tddate": null,
        "forum": "TdTGGj7fYYJ",
        "replyto": "TdTGGj7fYYJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5686/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a novel algorithm for unsupervised few-shot meta-learning that combines contrastive learning with unsupervised task generation to train a few-shot model. They achieve good improvements on state-of-the-art benchmarks as well as cross-domain meta-learning benchmarks.",
            "strength_and_weaknesses": "Strength: \nInteresting and novel algorithm.\nGood and structured writing and presentation.\nComparing with cross-domain meta-learning.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors mention that they will release their code upon acceptance. The pseudo code of algorithm is provided and the details are explained to a good degree.",
            "summary_of_the_review": "Interesting and novel paper. Given the strength and reproducibility comments I made, I tend to vote for acceptance of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5686/Reviewer_3zZw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5686/Reviewer_3zZw"
        ]
    },
    {
        "id": "Fm40hF8lLm",
        "original": null,
        "number": 3,
        "cdate": 1666685655243,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685655243,
        "tmdate": 1670350171303,
        "tddate": null,
        "forum": "TdTGGj7fYYJ",
        "replyto": "TdTGGj7fYYJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5686/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new approach to unsupervised meta-learning by drawing inspiration from recent self-supervised learning methods. Specifically, they employ an architecture similar to MoCo, where one branch is updated with momentum updates of the other branch, which is updated \u2018online\u2019. To form each task for meta-learning, they treat each example in the current batch as a \u2018query\u2019 and then propose a way of selecting relevant \u2018shots\u2019 for each query from a queue, where (the representation of) examples seen in previous batches are added, to serve as the support set for the episode. They utilize the Sinkhorn-Knopp algorithm to do this matching and pick the top K examples from the queue, for each query (where K is the shot of each class). Constructing tasks dynamically in this way can be advantageous compared to the approach followed by previous works where the set of tasks for meta-learning is created at the start of the meta-training phase and fixed, as this can lead to more variety as well as \u2018better\u2019 tasks (e.g. the support set of a class is a good match for its query set), since the representations become more informative throughout training. The use of a queue not only is useful for training with the MoCo objective in a self-supervised manner, but also can increase diversity within the episode, since the support examples can be selected from previous mini-batches too, leading to a larger pool of candidates. They use a supervised contrastive algorithm to solve each meta-training task. Their overall loss for training is this per-task contrastive loss, as well as the usual MoCo loss. At meta-test time, they use a nearest-centroid algorithm for single-source few-shot learning, while for cross-domain scenarios they further adapt the projection layers (small number of parameters) to handle the distribution shift. Specifically, they adapt on the support set via supervised contrastive learning where each support example plays the role of a \u2018query\u2019 and the pool for \u2018searching\u2019 is the rest of the support set; positive examples are those with the same label, since labels are known for the support examples. Empirically, they show improved results on some benchmarks over some previous methods.",
            "strength_and_weaknesses": "Strengths\n========\n[+] This is a really neat idea for leveraging progress in self-supervised learning for the problem of unsupervised meta-learning\n\n[+] The paper is for the most part clearly written and well-organized\n\n[+] Empirically, the proposed method seems to outperform some previous baselines\n\n[+] A thorough ablation analysis is presented along with a set of additional experiments to further understand the performance of the proposed method\n\nWeaknesses\n===========\n[-] The problem of unsupervised meta-learning is poorly motivated. More generally, whether meta-learning is the most fruitful approach for few-shot classification is questionable. Recent works [1-3] (see references below) show that simpler methods often work better. So it\u2019s not clear why this framework is useful.\n\n[-] The empirical investigation is limited to simple benchmarks for the most part (though I appreciated the cross-domain experiments), limited to very small convnet architectures (the few-shot learning community has long ago started using resnet-12 [1], resnet-18 [4], and larger models [3]), and the baselines compared against for supervised meta-learning are very basic and very far from the state-of-the-art.\n\nReferences\n==========\n[1] A closer look at few-shot classification. Chen et al. ICLR 2019.\n\n[2] Revisiting few-shot image classification: A good embedding is all you need? Tian et al. 2020.\n\n[3] Comparing Transfer and Meta Learning Approaches on a Unified Few-Shot Classification Benchmark. Dumoulin et al. NeurIPS 2021.\n\n[4] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. Triantafillou et al. ICLR 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n=====\nThe paper is clear for the most part, with a few exceptions below.\n\nIn Equation 1, state what M represents exactly.\n\nIn Equation 1, for negative pairs where A_{i,j} is 0, the term inside the log will be 0, so the contrastive loss in this case will not be a function of the similarity between q_i and k_j. Is this an error? E.g. should A_{i,j} be -1 instead of 0 for negative pairs? \n\nNotation: k_i (e.g. in Equation 2) and z_i (in Section 3.1) seem to refer to the same quantity. Am I missing something? If not, change the notation to be consistent.\n\nIn Table 4, it\u2019s not clear what removing Sinkhorn entails exactly. What is done instead? I\u2019m assuming some different matching algorithm is used?\n\nNovelty\n======\nWhile the proposed method heavily relies on MoCo, the particular adaptation to the problem of unsupervised meta-learning is novel to the best of my knowledge and interesting.\n\nReproducibility\n============\nThe authors discuss implementation details in the Appendix. It\u2019s hard to say how easy it would be to reproduce the presented results. I recommend the authors to release code upon publication.\n\nQuality\n======\nThe technical quality of the paper is high for the most part. However, I have some concerns about experimental comparisons, ablations.\n\nWere SimCLR / MoCo trained on the meta-training set of Omniglot / mini-ImageNet for the experiments? This is kind of strange as the strength of these approaches exhibits itself when training on large datasets, with architectures much larger than 4 or 5 convolutional layers. So I think to truly compare against those approaches, it would be more interesting to use larger-scale pre-training.\n\nThe supervised meta-learning baselines used here are old and really far from the state-of-the-art, especially for cross-domain few-shot learning.\n\nIn Table 3, why are comparisons made only against MoCo v2? For instance, in addition to the self-supervised and supervised meta-learning baselines, it would be interesting to compare to a supervised off-the-shelf ImageNet trained model (that was not meta-learned, was simply trained for supervised classification), by finetuning it on each transfer task.\n\nMore generally, add transfer learning baselines to the paper. In the scenario of small architectures (4 or 5 conv layers), transfer learning is likely to not work very well, but for larger architectures and more training data it seems to dominate.\n\nMissing ablation: train with only L_{PsCo} instead of the sum of that loss with MoCo\u2019s loss; that is, train only for the contrastive loss that matches each query (example in the current batch) with the corresponding selected \u2018support examples\u2019 from the queue (and pushes it away from other support examples that were assigned different pseudolabels).\n",
            "summary_of_the_review": "Overall, I found the idea presented in this paper interesting, the paper well-written and well-organized. I\u2019m generally unconvinced of the value of unsupervised meta-learning, or meta-learning in general as a means for representation learning for the problem of few-shot classification, especially lacking transfer learning baselines. I also found the experimental setup to be not very convincing, as it focuses on very small architectures and the comparisons are against models that are far from state-of-the-art (for the supervised meta-learning baselines). These are the reasons that I recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5686/Reviewer_yfDU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5686/Reviewer_yfDU"
        ]
    },
    {
        "id": "akfQ8ivbr3",
        "original": null,
        "number": 4,
        "cdate": 1666864868814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666864868814,
        "tmdate": 1666864868814,
        "tddate": null,
        "forum": "TdTGGj7fYYJ",
        "replyto": "TdTGGj7fYYJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5686/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new approach to meta-learning that addresses vulnerability to pseudo-labeling errors in state of the art meta-learning methods by improving pseudo-labeling using a momentum network and a queue of previous batches. The main insight in this work comes from the recognition that pseudo-labels are usually fixed in state of the art methods and not allowed to change as new information comes in. The proposed technique enables changing of the pseudo-labels using cross batch training. The proposed technique advances over the state the state of the art significantly when applied to a variety of few-shot classification benchmark tests. The proposed technique also advances over the state of the art in its scalability.  \n\n",
            "strength_and_weaknesses": "Strengths\n1. Sound motivation of the work through thorough literature survey.\n2. Innovative progressive refinement of pseudo-labels through the combination of the momentum network and a batch queue that ensures that information from past batches is used throughout.\n3. Sound experimental results that show a clear advance over the state of the art. \n\nWeaknesses\n\n1. No significant weaknesses.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThe paper is well written and easy to understand.\n\nReproducibility\nThe technique is certainly described in enough detail to be reproducible.\n\nQuality and Novelty\n\nThe paper advances significantly over the state of the art in both the algorithm and the experimental results. The proposed technique is novel. ",
            "summary_of_the_review": "The paper motivates the proposed technique well. The proposed technique is sound and innovative, with a strong main insight of adapting pseudo-labels progressively. That represents a clear advance over the state of the art. The experimental results also show advancement over the state of the art. Good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5686/Reviewer_eLS6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5686/Reviewer_eLS6"
        ]
    }
]