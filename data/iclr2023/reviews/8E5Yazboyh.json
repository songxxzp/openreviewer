[
    {
        "id": "bwdwhqrkHK",
        "original": null,
        "number": 1,
        "cdate": 1666644946374,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644946374,
        "tmdate": 1666645045867,
        "tddate": null,
        "forum": "8E5Yazboyh",
        "replyto": "8E5Yazboyh",
        "invitation": "ICLR.cc/2023/Conference/Paper2350/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the generalization performance of learning algorithms in a non iid setting, via the lens of algorithmic stability. The assumption made on the data is that it is generated by a mixing process ($\\phi$ mixing or $\\psi$ mixing) which formally quantifies how the level of dependency between two observations decays as a function of the gap between their indices. The paper derives moment bounds for weakly dependent random variables, which leads to a generalization of the Marcinkiewicz-Zygmund inequality (for iid random variables) to the non iid setting for mixing sequences. Then following the ideas in (Bousquet et al., 2020), they derive high probability bounds on the generalization error for uniformly stable learning algorithms trained on a sample drawn from a mixing process, with bounded loss functions. Applications of this bound are shown for several settings such as kernel regularization, stochastic gradient descent and iterative localized optimization algorithms.",
            "strength_and_weaknesses": "Pros: The paper has the following strengths.\n\n\u2022 Very well written paper with a clear exposition that explains the problem setup, main contributions, related work, and the technical ideas cleanly.\n\n\u2022 The high probability generalization error bounds for learning algorithms trained on non iid data generated from a mixing sequence is a solid addition to the literature, and novel to my knowledge. It improves the existing bound of (Mohri & Rostamizadeh, 2010) by a $\\sqrt{n}$ factor.  \n\nCons: The paper has the following weaknesses.\n\n\u2022 Theorems 1 and 2 are the main technical tools used for deriving the main results. However, the proof of Theorem 2 essentially follows the telescoping argument of (Bousquet et al. 2020) with little changes. And the proof of Theorem 1 follows readily from Lemma A.1 (as mentioned above), so there does not seem to be much technical novelty in the analysis.\n\n\u2022 I think it would be helpful for the reader if some discussion on the proof steps can be provided within the main text, especially for Theorems 1 and 2 as these are the main technical tools needed for deriving the main results.  At the moment, no such discussion exists regarding the proof steps. \n\nFurther comments:\n\n\u2022  In Remark 4 in the fourth line, should it be \u201cwe cannot guarantee $\\mathbb{E}[\\tilde{g}_i] = 0$\u2026\u201d?\n\n\u2022 Theorem 5 is for loss functions which are uniformly bounded by M. It would be helpful to clarify in the text how the conditions required within the corollaries in Section 5 handle this boundedness condition. For example, in the proof of Corollary 6, it is not clear to me from the proof where M disappears?\n\n\u2022 In the statement of Corollary 6, I think f should be stated to be convex? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very well which makes it easy to understand the main ideas behind the analysis. I believe the novelty of the results is limited as the main technical tools used in the analysis (Theorems 1 and 2) seem to follow quite readily from existing results. This is explained in the weaknesses section in more detail. Nevertheless, the quality of the results is decent in my view as it improves upon the existing result of (Mohri & Rostamizadeh, 2010) for non iid data generated from a mixing sequence.",
            "summary_of_the_review": "The paper is very well written. The problem setup, related work, and the setting considered is explained clearly, and the exposition of the technical results are also clean. I think this is a decent result and is a natural extension of the recent advancements made in the context of high probability bounds for the iid setting to the non iid setup. The bounds improve that of (Mohri & Rostamizadeh, 2010) by a factor of $\\sqrt{n}$ up to log factors, and this is made possible by heavily relying on the proof steps of (Bosquet et al. 2020), which leads to Theorem 2. The Marcinkiewicz-Zygmund type inequality developed in Theorem 1 is a useful tool although it seems to follow quite readily from the Mcdiarmid inequality in Lemma A.1 (Kontorovich and Ramanan, 2008).  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_2VCS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_2VCS"
        ]
    },
    {
        "id": "JLBOt4PZm-",
        "original": null,
        "number": 2,
        "cdate": 1667175595954,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667175595954,
        "tmdate": 1667175595954,
        "tddate": null,
        "forum": "8E5Yazboyh",
        "replyto": "8E5Yazboyh",
        "invitation": "ICLR.cc/2023/Conference/Paper2350/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the problem of deriving generalization bounds for weakly dependent data using algorithmic stability. It builds on previous work by Mohri & Rostamizadeh (2010) which derived the first generalization bounds for this setup. This paper claim to improve on these bounds by a factor of sqrt(n). To derive these bounds, the paper develops some new moment bounds for sums and Lipschitz functions of weakly dependent random variables. Finally, the paper applies the new generalization bounds to analyze a few algorithms including kernel regularization and SGD.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper has significant contributions to the understanding of generalization and algorithmic stability for the case of weakly-dependent data. For e.g. it obtains bounds which match the best known bounds for i.i.d. data in some cases if the dependence among the variables is weak enough.\n2. The paper introduces some interesting concentration bounds for sums and Lipschitz functions of weakly dependent random variables, which could possibly have wider applications.\n\nWeaknesses:\n\n1. I think the message, presentation, and perhaps the bounds in the paper as well need to be distilled more to make the contribution clearer. For e.g. how do the concentration bounds in Thm 1 and 2 compare with prior work? The bounds seem interesting, but I'm not sure of the improvement here. Similarly, I'm not fully sure of the improvement in Thm 5. If the sequence is exponentially $\\varphi$-mixing (or algebraically $\\varphi$-mixing with a large enough degree) then the $\\sqrt{n}\\varphi(b)$ term should be negligible, and the improvement is mainly in the other $\\sqrt{n}b \\beta \\Delta_n$ term. This term could potentially be significant, but I'd like to see some specific algorithmic examples where it leads to a better downstream bound. For e.g., it would be good if the authors commented on the difference between their bounds and previous work for kernel regularizations and SGD. On a related note, I'm not sure I fully understand the claim on page 2 that Mohri & Rostamizadeh (2010) get a worse than 1/sqrt(n) bound for strongly-convex problems, why should I think of $\\lambda$ as not being constant (or a very small polynomial) here? Seeing some concrete examples of how the theory leads to better bounds for well-studied algorithms would make the significance of the contributions much clearer.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written, though it would be nice to bring out the contribution with respect to prior work better. I'm not fully sure of the novelty and originality in the techniques, maybe some comparison of the concentration bounds and proof techniques could help here.\n\nClarification:\n\nIn the statement of Lemma 4, the definition of $b$ seems to have something missing?",
            "summary_of_the_review": "Overall, the paper appears to be a good contribution to generalization theory, though I have some concerns regarding the contribution and significance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_PGse"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_PGse"
        ]
    },
    {
        "id": "nlvARJ2RI_L",
        "original": null,
        "number": 3,
        "cdate": 1667181157363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667181157363,
        "tmdate": 1667181157363,
        "tddate": null,
        "forum": "8E5Yazboyh",
        "replyto": "8E5Yazboyh",
        "invitation": "ICLR.cc/2023/Conference/Paper2350/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study generalization bounds in parametrized supervised learning when training (and test) data are not i.i.d. This setting is more realistic than the typical independence assumptions made in statistical learning theory for problems like time series (or say a Markov chain), where it is expected that the $t$-th sample depends on the prior position (observed examples). Following a long line of work on learning from non-i.i.d data, the authors consider the setting of \u201cmixing processes,\u201d where the dependence between samples is controlled by some mixing coefficient that decays to 0 between the $t$-th and $(t+i)$-th examples as $i$ goes to infinity. Roughly, the authors show that any uniformly stable algorithm has near optimal generalization bounds under strong assumptions on the decay of dependence across time.\n\nIn slightly greater detail, the authors consider two main settings called $\\phi$-mixing and $\\psi$-mixing respectively, where roughly:\n\n$$\\phi(k) = \\sup_{A,B}\\left|Pr(A|B) - Pr(A)\\right|, \\quad \\quad \\psi(k) = \\sup_{A,B}\\left|\\frac{Pr(A \\cap B)}{Pr(A)Pr(B)} - 1\\right|,$$\n\nWhere A and B are events k time steps apart, and $\\psi$-mixing is a stronger assumption than $\\phi$-mixing\n\nThe authors start by proving new concentration inequalities for $phi$-mixing sequences, showing that the sum of any family of $n$ mean-0, sub-gaussian variables $\\{Z_i\\}$ that are functions of random variables $\\{X_i\\}$ drawn from a $\\phi$-mixing process have bounded p-norm (matching the classical Marcinkiewicz-Zygmun inequality for the i.i.d setting up to log factors and the mixing coefficient). A similar result is shown for bounded mean, Lipshitz, coordinate-wise 0-mean variables, generalizing a result in the i.i.d setting of Bousquet, Klochkov, and Zhivotovskiy (COLT 2020).\n\nUsing these newly developed concentration inequalities, the authors generalize the optimal stability-based generalization bounds of [BKZ20] to the non-i.i.d setting (an algorithm is said to be stable if changing a single training example does not significantly shift the output value on any point). In particular, the authors show a high probability generalization bound of roughly:\n\n$$|F(w_S)-F_S(w_S)| \\leq \\tilde{O}(n^{-1/2})$$\n\nwhere $F$ is a bounded loss function and $w_S$ is the (parametrized) output of a uniformly stable algorithm on sample $S$, under the assumption that the data distribution is $\\psi$-mixing for $\\psi(k) \\leq 1/k^r$ for some $r>1$ (i.e. where dependence decays at a super-linear rate). To the authors\u2019 knowledge, this is the first such result known to achieve optimal generalization rates for non-i.i.d data.\n\nFinally, the authors apply this result to a few popular learning paradigms, Kernel Regularization Schemes, SGD, and Feldman, Koren, and Talwar\u2019s recent iterative localized algorithm to give generalization bounds of the form \n\n$$|F_S(w_S)-F(w^*)| \\leq \\tilde{O}(n^{-1/2})$$,\n\nUnder various standard assumptions (e.g. Lipshitz, smoothness, strong convexity), where $w^*$ is the minimizer of $F$.",
            "strength_and_weaknesses": "Studying learning settings where data is non-i.i.d is a natural and important problem, especially given that the vast majority of statistical learning theory is built for the i.i.d setting while one frequently expects dependencies to occur in practice. This work is (to my knowledge), the first to show that one can recover near-optimal generalization rates that don\u2019t depend on classical complexity measures (VC, Rademacher, etc). The main concentration inequality for $\\phi$-mixing random variables also seems to be of independent interest. At a technical level, the work draws heavily from [BKZ20] and Mohri and Rostamizadeh (JMLR 2010), and the main observation seems to be that one can combine their techniques (the former giving optimal bounds in the i.i.d setting, the latter sub-optimal for $\\phi$-mixing) by making a stronger assumption on the mixing process ($\\psi$-mixing). Realizing this requires some effort and ties together several disparate works.\n\nOn the other hand, there are several serious issues in the presentation of the results. First and foremost, the authors claim as one of (if not the) main selling point that their generalization results improve over Mohri and Rostamizadeh by a factor of $\\sqrt{n}$, and are optimal. From what I can tell, this is not true. The authors make their improvement in large part via a stronger assumption on the decay of dependence in the mixing process, so the result is incomparable to prior work in this vein. I find the introduction to be fairly misleading in this sense, and largely due to this fact do not think I can recommend the work in its current form for publication in ICLR. There are a number of other presentation issues as well. A number of crucial definitions are not properly explained (e.g. distribution of the test examples), several of the theorem/lemma statements are confusing as written (what does \u201cLet b \u2208 {0, . . . , n} denote the number of last removed in S\u201d mean?), and the stronger dependence assumption is never motivated (how much stronger is $\\psi$-mixing than $\\phi$-mixing? Is this a reasonable assumption?). \n\nMinor comments:\n\n1. \u201cNote $\\Delta_n = O(1)$ for algebraical \u03c6-mixing\u201d Do you mean for $r>1$?\n\n2. I suggest writing bounds in asymptotic notation in the intro/beginning for readability\n\n3. It\u2019d be nice to motivate the conditions in Thm 2 beside just being what you need for the analysis.\n\n4. There are many other grammatical/typographical issues throughout",
            "clarity,_quality,_novelty_and_reproducibility": "The new concentration bounds and stronger generalization results for stable algorithms under $psi$-mixing are novel. The techniques largely adapt prior work, but require some new ideas to do so. As discussed above, the writing clarity is the main issue with the work in its current form, and how it positions itself with respect to prior work.",
            "summary_of_the_review": "The authors study an interesting setting in statistical learning theory (generalization bounds under non-i.i.d data), and prove optimal bounds under strong decay of dependence across samples. On content alone the work is (in my opinion) above the bar for ICLR, but due to significant issues in presentation I cannot recommend acceptance in its current form.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_Ue97"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_Ue97"
        ]
    },
    {
        "id": "rCRPoV8tku",
        "original": null,
        "number": 4,
        "cdate": 1667241668110,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667241668110,
        "tmdate": 1667241668110,
        "tddate": null,
        "forum": "8E5Yazboyh",
        "replyto": "8E5Yazboyh",
        "invitation": "ICLR.cc/2023/Conference/Paper2350/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proves stability-based generalization bounds under the assumption that the data is drawn from a mixing sequence (rather than assuming that the data is i.i.d.). The bounds improve upon previous results by a factor which is the square root of the sample size.",
            "strength_and_weaknesses": "The paper makes a good contribution to learning theory, with improved results over prior literature.\n\nThe main weakness is that the paper provides no examples of mixing sequences (beyond the trivial i.i.d. case), which makes it difficult to understand the relevance of the work. Please provide such examples (if the authors did not do so out of a lack of space, then they can cut down on the elaboration of the various consequences in the algebraic and exponentially mixing cases in Section 5, since these are straightforward corollaries of the stated results).\n",
            "clarity,_quality,_novelty_and_reproducibility": "I believe that the work is novel and of good quality. I point out some typos:\n- Pg. 2, \u201crecent breakthrough\u201d -> \u201crecent breakthroughs\u201d\n- Pg. 3, \u201cmost stability analysis implies\u201d -> \u201cmost stability analyses imply\u201d\n- Throughout the paper: please use \u201cmixing sequences\u201d rather than \u201cmixing sequence\u201d, e.g., in pg. 3, instead of \u201cstudying learning bounds with \u2026 mixing sequence\u201d, replace with \u201cstudying learning bounds with \u2026 mixing sequences\u201d\n- Pg. 4, the paragraph starting with \u201cIntuitively speaking\u201d is not clear\n- In Thm. 1 and Rmk. 1, why does the dependence on p not scale as p^{\u00bd}? It seems that it should because the tail assumption is sub-Gaussian\n- Thm. 2, in the second bullet point and in the conclusion, write g_i(Z) rather than g_i\n- Second display eq. In Remark 2, there is an extra parenthesis\n- Pg. 5, do not write z_i\u2019(z_i\u2019\u2019) as it looks like z_i\u2019 is a function of z_i\u2019\u2019, instead write (resp. z_i\u2019\u2019)\n- Statement of Lem. 4 needs to be checked for grammar\n- Remark 4, \u201cone guarantee E[..]\u201d -> \u201cone guarantees E[..] = 0\u201d\n- Conclusion, \u201cMcdiarmid\u201d -> \u201cMcDiarmid\u201d",
            "summary_of_the_review": "Although the paper does not motivate the mixing setting well with examples, overall I believe that the contribution is solid and hence I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_HM4R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_HM4R"
        ]
    },
    {
        "id": "YatQEtLwd8",
        "original": null,
        "number": 5,
        "cdate": 1667346250988,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667346250988,
        "tmdate": 1667346250988,
        "tddate": null,
        "forum": "8E5Yazboyh",
        "replyto": "8E5Yazboyh",
        "invitation": "ICLR.cc/2023/Conference/Paper2350/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper develops generalisation bounds for stable learning procedures trained on a $\\psi$-mixing (dependent) stochastic process. The key contribution here is an extension of a recent line of work on generalisation analysis of stable methods on i.i.d. data. Much like in the i.i.d. case, the result is stability bounds that improve in $\\sqrt{n}$ factors found in prior analysis of the same. \n\nThe extension follows two steps, both of which are mixing versions of the approach of Bousquet et al. [BKZ] - first, the authors derive a MacDiarmid type tail bound for sums of functions of $\\varphi$-mixing processes along the lines of Theorem 4 of [BKZ]. Next this with a one-point ghost replacement trick is used to derive stability bounds along the lines of Lemma 7 of [BKZ]. The main novelty lies in this second part - to handle the dependence of the stream, the authors extend this technique to control the generalisation error in terms of the risks of a model trained with a replaced $i$th sample along with removing $2b$ samples around $i$. The value of $b$ serves as an (analytical) tradeoff (roughly speaking, between the loss of samples and the decorrelation engendered by removing samples). This part is strongly reminiscent of the work of Mohri and Rostamizadeh [MR].\n\nThe paper concludes with applications of the bounds derived to methods previously established to be stable, illustrating the utility of the bounds found.",
            "strength_and_weaknesses": "I think this paper is quite timely, and interesting. The extension of the recent iid stability breakthroughs to mixing processes was just waiting to happen, and the submission does a fine job of delivering on this. I think the paper also does a good job at characterising the existing work, and in treating a technically involved subject in a simple way. The proofs are, to my reading, correct, although I have not checked every detail.\n\nThere's perhaps two points of weakness to consider:\n\n- To an extent, these methods at a high level look like an integration of [BKZ] and [MR]. While the authors briefly discuss the differences from [BKZ] in the proof technique (the evacuating $b$), this difference appears to be present already in [MR]. I think a clear discussion of the specific challenges thrown by the integration of these two lines, and of the specific novelty relative to these will strengthen the paper.\n\n- The paper is very theoretical, which somewhat clashes with the flavour of this conference. While certainly most large ML conferences do have a space for theoretical work, ICLR tends to lie more strongly on the empirically driven side of things. It is thus worth considering how appropriate this paper is for the venue.",
            "clarity,_quality,_novelty_and_reproducibility": "For the most part, the writing is clear. There are a couple of points of order:\n\n- As I understand it, the paper's conclusions hold only for $\\psi$-mixing processes (since the control on $\\varphi'$ is derived through $\\psi$). This means that the title is not representative, and should be updated to $\\psi$-mixing instead of $\\varphi$-mixing.\n- I think it's worth it to explicitly define the expectation with subscript notation being used (in my understanding, everything not in the subscript is conditioned on, and only the subscripted variables are integrated over).",
            "summary_of_the_review": "I think this is an interesting paper, but have reservations for its appearance in this venue, and would additionally like a clearer description of how specific technical aspects of the argument interact with the prior work. On the whole, I lean towards recommending acceptance.\n\nN.B. - I would have given a score of 7 if available as an option, but it's not. The score of 6 rather than 8 reflects the fact that I do have reservations about the venue, but I'm open to changing this upon discussion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_enwM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_enwM"
        ]
    },
    {
        "id": "hnYcbeaU9U",
        "original": null,
        "number": 6,
        "cdate": 1667447045597,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667447045597,
        "tmdate": 1667447045597,
        "tddate": null,
        "forum": "8E5Yazboyh",
        "replyto": "8E5Yazboyh",
        "invitation": "ICLR.cc/2023/Conference/Paper2350/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper uses the notion of \"algorithmic stability\" to study the\ngeneralization performance of learning algorithms that train on data drawn from\na $\\varphi$-mixing sequence. The authors give new high-probability guarantees\nfor uniformly stable algorithms that improve on previous state-of-the-art\nresults by a factor of $O(\\sqrt{n})$ [Mohri-Rostamizadeh, JMLR 2010]. They then\napply their general mixing stability bound (Theorem 5) to a few special cases\nfor polynomially-bounded and exponentially-bounded $\\varphi'(k)$-sequences: \nkernel regularization schemes, SGD, and an iterative localization algorithm.",
            "strength_and_weaknesses": "**Strengths:**\n- There is good precedent for studying generalization bounds via stability for i.i.d. data:\n  * Feldman--Vondrak, COLT 2019\n  * Bousquet--Klochkov--Zhivotovskiy, COLT 2020\n  * Klochkov--Zhivotovskiy, NeurIPS 2021\n- Previous work on stability bounds for $\\varphi$-mixing sequences are limited but important:\n  * Mohri--Rostamizadeh, JMLR 2010\n- The main contributions of this work are sufficiently technical, but also\n  adaptations of different previous works for $\\varphi$-mixing setting:\n  * The moment bound for weakly dependent random variables (Theorem 1) extends\n    [Ren--Liang, Statistics and Probability Letters 2001].\n  * The concentration inequality for $\\varphi$-mixing sequences (Theorem 2)\n    builds on [Bousquet--Klochkov--Zhivotovskiy, COLT 2020].\n  * The general uniformly-mixing stability bound (Theorem 5) is applied to\n    several useful instances (e.g., SGD).\n\n**Weaknesses:**\n- Theorem 5 and all of the corollaries in Section 5 use\n  $\\varphi'(b)$, which in turn is a function of $\\psi(k)$. The paper should\n  more explicit about this --- it doesn't seem like the authors draw any\n  attention to it. They do, however, note that ``$\\psi$-mixing is stronger\n  than $\\varphi$-mixing'', so this should be reconciled since the final\n  bounds are not solely a function of $\\varphi(k)$.\n- It would be helpful to mention soon after Definition 2 that we will use\n  $\\psi$-mixing to give a bound on the stability analysis version of\n  $\\varphi'$-mixing in Lemma 3. Otherwise, it's not clear why Definition 2 is\n  relevant, and then Lemma 3 is somewhat surprising in that the bound isn't\n  given in terms of $\\varphi(k)$.\n\n**Suggestions:**\n- [page 2] Typo: \"general bound\" --> \"generalization bound\"\n- [page 4] Clarification: In Theorem 1 we say $X_1, \\dots, X_n$ are\n  $\\varphi$-mixing distributions, but the definition for $\\varphi$-mixing is\n  given in terms of an infinite sequence of random variables. These ideas could\n  be better connected. We can say something similar about Theorem 2.\n- [page 5] Suggestion: Discuss the distribution from which $z$ is drawn in more\n  detail. You say that $z$ depends on $S$, but can you comment more about\n  what kind of assumptions are reasonable or previously studied?\n- [page 9] Typo: The equation in Corollary 9 should end with a comma, not a period.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly.\nThe authors give quality results and the work is indeed novel.\nThere are no experiments, so reproducibility is not applicable.",
            "summary_of_the_review": "This is a good paper that could be accepted to ICLR. The target audience is\nprobably better suited for COLT since this work solely focuses on\ngeneralization bounds for $\\varphi$-mixing sequences (a method for quantifying\ndecay of correlation), but the results are of likely of interest to a broader\ncrowd.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_uJFn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2350/Reviewer_uJFn"
        ]
    }
]