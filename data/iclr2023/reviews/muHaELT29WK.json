[
    {
        "id": "K-c2hcqzNSu",
        "original": null,
        "number": 1,
        "cdate": 1666133094291,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666133094291,
        "tmdate": 1669399949717,
        "tddate": null,
        "forum": "muHaELT29WK",
        "replyto": "muHaELT29WK",
        "invitation": "ICLR.cc/2023/Conference/Paper1514/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to reduce the attribution nioses in integrated gradients method by utilizing multiple intergrating paths. Specifically, the Stick-Breaking Process is utilized for aggregating the attribution map from large number of random choosing paths.",
            "strength_and_weaknesses": "Weaknesses:\n\n1. Given the successful work of IG method, this submission extends the attribution map integration from single path to distributed multiple paths, which is a little bit incremental.\n\n2. There are some incomplete sentences and typos in the submission. For example, 'Sanity checks on the attribution is an'.\n\n3. For equation(2), ReLU, which is the most commonly used activation in vision, only with two linear regions, thus the proposed path choosing method may not important. Moreover, the discussion about the non-linear activations, such as sigmoid, should be discussed.\n\n4. When the value of the hyperparameter /alpha is small, how will the path choosing process be affected? With a less constrained random choosing? Or pushed far away from the base distribution H?\n\n5. Eq(9) and Eq(10) based on the assumption that the \\phi follows the Gaussian Distribution. Therefore, the proof of the distribution of \\phi is a Guassian should be involved.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Some of the important proof is missed.\n\n2. The writing is good.\n\n3. The novelty is a bit limited. Please refer to the weaknesses.",
            "summary_of_the_review": "The score of this paper can be changed according to the rebuttal response.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1514/Reviewer_yFiz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1514/Reviewer_yFiz"
        ]
    },
    {
        "id": "g19vQ0PICg3",
        "original": null,
        "number": 2,
        "cdate": 1666275773310,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666275773310,
        "tmdate": 1668762053418,
        "tddate": null,
        "forum": "muHaELT29WK",
        "replyto": "muHaELT29WK",
        "invitation": "ICLR.cc/2023/Conference/Paper1514/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose the attribution method ``Stick-breaking Path Integration'' which averages the attribution maps obtained by integrating over several different paths using the Integrated Gradient attribution method. They show superior performance qualitatively and quantitatively (pixel flipping, ROAR) compared to other explanation methods (Gradient*Input, GuidedBackProp, IntegratedGradients, FullGrad, GuidedIG).",
            "strength_and_weaknesses": "*** Strengths\nAttribution maps are less noisy and show good performance on the evaluated metrics.\n\n\n*** Weaknesses\nMy main concerns are with respect to the significance and novelty of the contribution. There are already many attribution methods out there, so I think the minor extension of an already existing method might not be significant enough to be published at ICLR. There is little theoretical motivation/analysis/guarantees for the proposed methods. Comparison to other popular methods (LIME, SmoothGrad) is missing.",
            "clarity,_quality,_novelty_and_reproducibility": "*** Clarity\nThere are quite a lot of minor grammatical mistakes. Together with the many acronyms, it makes reading the text quite cumbersome at times.\n\n*** Quality\nThe conduction of experiments seems fine.\n\n*** Novelty\nThe contribution is novel but not very significant since it is a minor modification of the existing Integrated Gradients explanation method.\n\n*** Reproducibility\nThere is no accompanying code nor an appendix stating details of the implementations. The method itself is likely relatively easy to implement, though.",
            "summary_of_the_review": "While the results show superior performance on quantitative metrics, the overall contribution does not seem very significant. I cannot recommend publication at ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1514/Reviewer_jggk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1514/Reviewer_jggk"
        ]
    },
    {
        "id": "Wwx_P9Byox_",
        "original": null,
        "number": 3,
        "cdate": 1666621304108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666621304108,
        "tmdate": 1668937855727,
        "tddate": null,
        "forum": "muHaELT29WK",
        "replyto": "muHaELT29WK",
        "invitation": "ICLR.cc/2023/Conference/Paper1514/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Attribution methods address the issue of explainability by quantifying the importance of an input feature for a model prediction. In this paper, the authors propose a variant of the integrated gradients (IGs) method through a strategy of \"selection of the paths to be integrated / aggregated\". The authors propose to generate several possible attribution estimates through a stochastic process, then, they show that attribution is improved by aggregating attributions from multiple paths instead of using a single path.\n\nThe main contribution of the paper is a new strategy that allows to improve attribution maps.\n\n\n",
            "strength_and_weaknesses": "**Strength**\n\n- Integration of different paths keeping the input fixed\n- sampling method to generate a random integration path\n- Good results on qualitative examples\n\n\n**Weaknesses**\n\n- More details in the experiments section would help with comprehension and reading.\n\nFor example, the pixel ins/del game. Is the input tiled or is each pixel used as a test unit?\n\nIf I understand correctly, given an attribution heatmap, each \"unit (=pixel/tile)\" is ranked according to the attribution values, in deletion, the highest-ranked 'unit' is replaced with a constant value (=0), the modified input is fed through the network, and the resulting drop in target class score is measured. \n\n- Does the \"insertion\" test start from a black image?\n- Which score is reported in Table 1? The first output or the AUC of the resulting curve?\n- If the score is the AUC of the curve, it might be interesting to have a plot of some curves to see the different trends.\n\nThe two tasks, del/ins seem exactly complementary. I'm not sure what information is adding the insertion task. On the other hand, it is useful to find the pixels/tiles that  disrupt the output of the network as quickly as possible. Neural networks are sensitive to subtle changes in input, and pixels or tiles don't necessarily have to contain meaningful information to break the network (this is also one of axioms in the IG paper). Therefore, Ancona [1] and others [2,3,4] proposed to reverse the deleting direction, by first removing pixels or tiles ranked as less relevant by the attribution method.\n\nRemoving 1) the most important pixels first and 2) the least important pixels first, leads to two different curves, the integral of the two curves provides an estimate of the performance of an attribution method in a quantitative way.\n\n\n\n\n\n[1] Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. A unified view of gradient-based \u00a8 attribution methods for deep neural networks. In NIPS 2017-Workshop on Interpreting, Explaining and Visualizing Deep Learning. ETH Zurich, 2017.\n\n[2] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. Evaluating Feature Importance Estimates. arXiv e-prints, 2018.\n\n[3] Pieter-Jan Kindermans, Kristof T. Schutt, Maximilian Alber, Klaus-Robert Muller, Dumitru Erhan, Been Kim, and Sven Dahne. Learning how to explain neural networks: Patternnet and \u00a8 patternattribution. In International Conference on Learning Representations, 2018.\n\n[4] Wojciech Samek, Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, and Klaus-Robert \u00b4 Muller. Evaluating the visualization of what a deep neural network has learned. \u00a8 IEEE transactions on neural networks and learning systems, 28(11):2660\u20132673, 2016.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, easily to read. In general, IGs average the gradient of multiple 'inputs', either over brightness level interpolations or in a local neighborhood. (novelty) Here the authors are proposing to 'keep the same input', and generate different/random estimates (by a StickBreaking-like Process). \n",
            "summary_of_the_review": "I'm satisfied with the response of the authors. The different points I had raised were taken into account and my doubts were clarified. \n\n--\n\nThe paper is well written and clear, there is an interesting contribution which consists in extending the IGs method by keeping the input fixed and generating a set of random estimates in order to integrate the different paths and obtain a single attribution map.\nThe weakness of the paper is in the experimental section which does not contain sufficient details to analyze the results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1514/Reviewer_tTr4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1514/Reviewer_tTr4"
        ]
    },
    {
        "id": "QUtr1dygmYz",
        "original": null,
        "number": 4,
        "cdate": 1666676039040,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676039040,
        "tmdate": 1668644749650,
        "tddate": null,
        "forum": "muHaELT29WK",
        "replyto": "muHaELT29WK",
        "invitation": "ICLR.cc/2023/Conference/Paper1514/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a refinement of Integrated Gradient (IG) by considering several paths from the baseline to the test example. The paths are sampled from a specific discrete distribution, and the results show improvement in some of the established metrics like insertion/deletion tests and ROAR.",
            "strength_and_weaknesses": "IG is a well-established attribution technique and there have been a fair amount of follow-up work to improve it. The authors clearly argue how using a single path can lead to sampling error, and propose a sensible way to sample paths. The results section shows the proposed technique improves on several metrics and is convincing enough. Some critical thoughts follow:\n\n1. The development of section 3.3 is unclear to me. I am also not sure how much the choice of the distribution matters, and this uncommon choice seems unjustified. I would think similar results might be observed by using another discrete distribution (say hypergeometric normalized to a max value of 1), since the CDF is always monotonic and meets the required constraint. I would appreciate authors thought on this and any justification for this specific choice as opposed to a general discussion around any parametrized probability distribution. \n\n2. The result section would benefit from inclusion of smoothgrad as the only other technique that performs some sort of averaging and thus could offer a more competitive baseline.\n\n3. In definition 3.1, the definition of monotonicity is incorrect. It defines the much more constrained linear path. Similar error appears in the description of equation 1 which is constrained to be linear instead of a general path.\n\n4. Many minor editing errors: \u201cwe address the single path is not reliable\u201d, \u201cconsider the features only increase the value\u201d, \u201cthe attribution is an\u201d, \u201caveraging pooling over\u201d, \u201cpath is monotonically proceed\u201d, \u201cif \\alpha assigned with bigger value\u201d, \u201cwith alleviating the absence\u201d, \u201cwith DGA\u201d (which is undefined).\n\n5. I also don\u2019t see the clear value of including the visualization method with the proposal of the technique diluting the focus, but this is more subjective.",
            "clarity,_quality,_novelty_and_reproducibility": "Fairly clear, alright quality and some novelty. Results could be hard to reproduce with the stick breaking section is lacking in details and no code is released.",
            "summary_of_the_review": "After author response:\nI am changing my score from 6 to 7. Since that option doesn't exist in the UI (it goes from 6 to 8), I am adding this note.\n\n-------\nOverall, the paper makes a sensible change to IG by including path averaging and the results look convincing. On the other hand, the specific path sampling method seems like an over-specification, and the result section glosses over a competitive baseline. In its current draft, I am only moderately inclined for the paper to be accepted at the venue.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1514/Reviewer_KHRW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1514/Reviewer_KHRW"
        ]
    }
]