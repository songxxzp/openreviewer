[
    {
        "id": "Bc2xIevWkpf",
        "original": null,
        "number": 1,
        "cdate": 1666701658719,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701658719,
        "tmdate": 1666701760233,
        "tddate": null,
        "forum": "ZsvWb6mJnMv",
        "replyto": "ZsvWb6mJnMv",
        "invitation": "ICLR.cc/2023/Conference/Paper3545/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the offline reinforcement learning problem with general function approximations and the partial coverage of offline data. Existing works either are computationally intractable or are suboptimal in statistical rates. The paper presents the first offline RL algorithms that are practical and optimal in regret under the assumption of single-policy concentrability. To obtain these properties, the paper uses the marginalized importance sampling (MIS) formulation combined with the augmented Lagrangian method.  \n\n\n\n",
            "strength_and_weaknesses": "Strengths:\n- The paper addresses an important problem of offline reinforcement learning.\n- The theoretical results are original in this setting. \n- While the MIS formulation has been used in PRO-RL (Zhan et al., 2022) to achieve the sample efficiency of offline RL under weak assumptions, this paper shows that the MIS formulation combined with the augmented Lagrangian method can reach the optimality in sample efficiency of offline RL. This is an interesting result. \n\nWeaknesses:\n- The paper makes many typos. For example, the first sentence in Section 3.2: \"... cements...\"; a typos in Proposition 2; $B_v$ has not been defined in Theorem 2;...\n- It would be nice if the authors can explain intuitively why replacing behavior regularization with the augmented Lagrangian is key for the optimality of their proposed algorithms.\n- The authors claim that their algorithms are practical but do not provide any experiments. \n\n\n     ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written and the theoretical results are original. ",
            "summary_of_the_review": "This is a good paper. I recommend the acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3545/Reviewer_f5U7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3545/Reviewer_f5U7"
        ]
    },
    {
        "id": "ceU7XF7SWay",
        "original": null,
        "number": 2,
        "cdate": 1666767655648,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666767655648,
        "tmdate": 1668754515039,
        "tddate": null,
        "forum": "ZsvWb6mJnMv",
        "replyto": "ZsvWb6mJnMv",
        "invitation": "ICLR.cc/2023/Conference/Paper3545/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies offline RL (MAB/CB/MDP) under general function approximation and single-policy coverage. The authors leverage MIS formulation plus augmented Langragian method (ALM) and identify that the key for solving problem is to ensure that certain state occupancy validity constraints are nearly satisfied.",
            "strength_and_weaknesses": "## Strengths\n\nCompared with Zhan et al. (2022), the rate is better and the realizability assumption is on the true density function instead of the regularized one.\n\nCompared with Chen & Jiang (2022), this paper drops the gap assumption.\n\nSome investigations in CB/MAB are interesting. For example, a lower bound on PRO-CB in prop 3 shows that PRO-RL cannot get the optimal rate.\n\n## Weaknesses\n\nIn the MDP case, the assumption is stronger. Compared with prior works Zhan et al. (2022) and Chen & Jiang (2022), this paper requires an additional realizable model class for model-based CORAL or a completeness-type U class for model-free CORAL.\n\nThe results could be presented in a clearer way. This paper includes many results, and it is very hard to track and compare them. I think adding a table that summarize the assumptions and results in the current paper and the most related works will be very helpful and beneficial to readers.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nThe major unclear statement to me is related to \"enforcers of state occupancy validity\" (using ALM). If I understand correctly, the Bellman flow constraint in PRO-RL already enforces the state occupancy to be valid? I think using ALM in the MAB/CB makes sense as I feel the parameter w will have a scaling problem if no squared penalty is added. However, I don't quite understand the intuition and reason for that in the MDP setting.\n\nAs mentioned in the previous block, this paper has many results, and it is quite hard to capture them. For example, this paper discusses CB/MAB/MDP and PRO-RL(CB) with/without regularization + the ALM method proposed in the paper. Adding a table would be very helpful.\n\nMinor: zeta on pg 8 seems to be undefined.\n\nIn the appendix (e.g., B.6), it seems better to use statistical error than closeness in the lemma title.\n\n## Quality\n\nOverall, the paper is of good quality.\n\n## Novelty\n\nThe most novel part to me is the introduction of ALM and the study on the more special MAB/CB cases. The remaining part and the analysis seem to be more standard and adapted from the literature. In case I'm missing something, feel free to comment on that.\n\n## Reproducibility\n\nI don't have time to check the proof, so I'm not able to evaluate in this aspect.\n\n## Others\n\nInstead of assuming the coverage of pi* and comparing with it, can you handle the arbitrary data distribution (Sec 4.3 in Zhan et al. (2022)) or the robustness case as in Zhan et al. (2022) and Chen & Jiang (2022)?\n\nThe major weakness to me is that this paper requires additional assumption other than the realizable W class + realizable V class in the MDP setting. For model-based CORAL, an additional realizable model class is assumed. For the model-free CORAL, an additional U class is assumed. I want to comment that the assumption on the U class is completeness type because when we add more functions to the W class, the realizability of U class will be broken and we need to enlarge the U class as well. However, we do not know the u_w^* for w, so it is not easy to expand the U class. I understand (from the title and intro) the scope of the paper is only pi* concentrability, but it would be certainly great to also consider weak realizability type assumption on the function approximation side. \n\nRelated to that, I am curious that why you do not require similar additional assumption in MAB/CB settings? My intuition is that we only encounter completeness type assumption in the MDP case (e.g., more than one horizon). It would be great if you can comment on that.\n\nCan you comment on the computational side of the algorithm? It seems that the algorithm is not very efficient as a max-min or a harder max-min(-min)-max optimization oracle is needed. A side comment is that similar oracles are used in [1][2].\n\n[1] Model-free Representation Learning and Exploration in Low-rank MDPs \n\n[2] Efficient reinforcement learning in block mdps: A model-free representation learning approach",
            "summary_of_the_review": "I lean towards acceptance. I would appreciate if the author could address my questions.\n\n--\nScore 6 --> 8 after rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Na",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3545/Reviewer_WYf4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3545/Reviewer_WYf4"
        ]
    },
    {
        "id": "nxTcCAV4HT6",
        "original": null,
        "number": 3,
        "cdate": 1667391711073,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667391711073,
        "tmdate": 1667391711073,
        "tddate": null,
        "forum": "ZsvWb6mJnMv",
        "replyto": "ZsvWb6mJnMv",
        "invitation": "ICLR.cc/2023/Conference/Paper3545/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper deals with offline RL problems, i.e. learning an optimal policy given some data sampled from a behavior policy. \n\nHere, while the behavior policy itself is assumed known, its state-action occupancy distribution is available only through samples. The offline RL approach considered is \"marginal importance sampling\": the goal is to learn weights for each state-action pair that will transform the estimated behavior occupancy into a new occupancy for the optimal policy. Another key assumption is \"single-policy concentrability\": in particular there is some $C^*$ which upper bounds the largest importance-sampling ratio required (for any state-action pair) between the behavior policy and the optimal policy.\n\nOffline RL with these assumptions is relatively well-studied previously. The previous approaches the authors compare to all have a general theme: they are primal-dual methods, maximizing expected reward, and using a Lagrangian to enforce a constraint that the occupancy distribution be a valid probability distribution. Such approaches also include a regularization term to keep the learned policy close to the behavior policy. The authors' improvement is to replace this regularization with an extra quadratic penalty on the weight feasibility (forgoing behavior regularization), which improves performance in MAB, contextual bandits, and full MDP settings.",
            "strength_and_weaknesses": "Strengths: The paper gives a clear presentation of improved algorithms for a topic which, while somewhat niche, is convincingly relevant.\n\nWeaknesses: there aren't any obvious weaknesses to the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Despite the somewhat dense topic, the paper is quite clear. In particular, the organization of the narrative (MAB to contextual to MDP) helps to build understanding. There is some unusual notation involving squiggly less-than signs which, while defined up front, might be better to avoid unless alternatives really hurt clarity.\n\nNovelty: The point of the paper is to show that a relatively simple trick (adding an augmented Lagrangian term to the existing Lagrangian to enforce feasibility) is \"enough\", as the title says.\n\nOriginality: The paper seems original enough, although I am not deep enough into this subfield of conservative offline RL to be able to tell conclusively.\n\nReproducibility: the work is wholly theoretical so reproducibility is not a concern.",
            "summary_of_the_review": "The paper seems like a reasonable theoretical contribution to a topic relevant to the ICLR audience, so I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3545/Reviewer_bCR3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3545/Reviewer_bCR3"
        ]
    },
    {
        "id": "auxoSLzLFM",
        "original": null,
        "number": 4,
        "cdate": 1667478488961,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667478488961,
        "tmdate": 1667478488961,
        "tddate": null,
        "forum": "ZsvWb6mJnMv",
        "replyto": "ZsvWb6mJnMv",
        "invitation": "ICLR.cc/2023/Conference/Paper3545/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose an augmented lagrangian approach to various offline decision making learning problems based on marginal importance sampling using function approximation. The authors show that their method exhibits improved qualities over behavior regularization, such as improved (sub)optimality. The present 4 variants of their approach, specialized for multi-arm bandits, contextual bandits and MDPs (model-based and model-free). The authors conclude with a small illustrative example comparing their approach to behavior regularization.",
            "strength_and_weaknesses": "The results are conclusive and insightful. Assist by their theoretical contributions, the authors make a strong case for the impact of occupancy validity on the performance in offline learning and provide an interesting explanation for the limitations of behavior regularization. Their proposed approach seems likely to inspire future work, both theoretical and empirical.\n\nMinor nitpicks and questions\n============\n\nNot that it matters much, but the authors might want to consider a different acronym. I would avoid CORAL since it's already quite popular (in general) and already has a notable presence in some parts of ML where it is known as \"correlation alignment\", for instance [1] (one of several top search results). \n\nBig nitpick, \"THE optimal policy\", should be \"some\" or maybe add a quick word to say we'll only consider one arbitrary optimal policy from this point on.\n\nOn a related topic, what impact does having multiple optimal policies have? I would expect that it would only make some definitions and theorem statements more tedious. If the authors are willing to indulge my curiosity, do you think considering multiple policies could offer anything useful, for instance, looser occupancy validity requirements, in some interesting special cases with many such policies?\n\np. 5, \"the objective (15)\", should this be a reference to (2) instead?\n\nI've mostly seen augmented lagrangian methods with a coefficient scaling the penalty term which typically grows over time. Is it's omission at all relevant or is it simply an irrelevant implementation detail (in the context of this work)?\n\np. 6, I wasn't able to allocate enough time to figure out the steps going from (10) to (12). Could the authors provide some additional details or point me to a similar derivation in related work?\n\nSomewhat tangential to this work, but the discussions related to the significance of the occupancy validity reminded me of recognizers [2], which was proposed as a way to describe target policies in the context of off-policy learning. The basic idea is that you can define the policy by probabilistically accepting or rejecting actions sampled by the behavior policy. This offers leads to better behaved off-policy learning than standard importance sampling. This is most likely the result of constraining target policies, but I wonder if this work's insight and analysis could reveal something interesting? Maybe new useful ways to constrain the target policies in cases where we don't seek optimality (e.g., in off-policy learning)?\n\n\n[1] Sun, B., Feng, J. and Saenko, K., 2017. Correlation alignment for unsupervised domain adaptation. In Domain Adaptation in Computer Vision Applications (pp. 153-171). Springer, Cham.\n[2] Precup, Doina, Cosmin Paduraru, Anna Koop, Richard S. Sutton, and Satinder Singh. \"Off-policy learning with options and recognizers.\" Advances in Neural Information Processing Systems 18 (2005).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is polished and well written. The exposition is good and the discussion is easy to follow. Detailed discussion of the context of this work and its related work made appreciating the contributions easy. The result appear novel though I'm not familiar enough with the topic to assert this confidently.\n",
            "summary_of_the_review": "This paper is interesting and of high quality, and I support it's acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3545/Reviewer_sKtr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3545/Reviewer_sKtr"
        ]
    }
]