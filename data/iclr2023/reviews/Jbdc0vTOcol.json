[
    {
        "id": "4X6HH0VGYp",
        "original": null,
        "number": 1,
        "cdate": 1666534969047,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666534969047,
        "tmdate": 1666534969047,
        "tddate": null,
        "forum": "Jbdc0vTOcol",
        "replyto": "Jbdc0vTOcol",
        "invitation": "ICLR.cc/2023/Conference/Paper5506/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. There are two main components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. The experiment results show the effectiveness of their method.",
            "strength_and_weaknesses": "Strength: Multivariate time series forecasting and self-supervised representation learning are important tasks for time series. The proposed methods have some improvements on both tasks.\n\nWeaknesses: I have the following questions.\n(i) Segmentation of time series is a naive extension from computer vision (MAE). Thus, it lacks novelty in my perspective.\n(ii) In my view, channel-independence is an orthogonal technic compared with other methods. In other words, the authors can perform channel-independence on other baselines. Can the authors show the performances with channel-independence in Table 3 and Table 4?\n(iii) Can you provide some analysis for channel-independence? Since different channels have correlations compared with others, why channel-indpendence can improve performance?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-orgonized and clarified. However, its novelty is somewhat weak.",
            "summary_of_the_review": "The paper proposes two main technics for time series tasks. Even though the experiment results are better than baselines, the improvement is marginal and there miss some deep analysis. Thus, I vote for marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5506/Reviewer_3Fzy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5506/Reviewer_3Fzy"
        ]
    },
    {
        "id": "YrNMB3hCNL",
        "original": null,
        "number": 2,
        "cdate": 1666636677003,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636677003,
        "tmdate": 1672797069467,
        "tddate": null,
        "forum": "Jbdc0vTOcol",
        "replyto": "Jbdc0vTOcol",
        "invitation": "ICLR.cc/2023/Conference/Paper5506/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an efficient design of Transformer-based models (PatchTST) for multivariate time series forecasting and self-supervised representation learning. It segments time series into patches following similar philosophy of ViT and assume channels are independent. Extensive experiments on 8 eight datasets show that it can outperform state-of-the-art models while requires less compute.",
            "strength_and_weaknesses": "Strength\n\n*  The idea of utilizing patch like ViT to reduce sequence length and better capture locality is quite simple and effective\n* Extensive experiments on multiple datasets show that it can outperform recently state-of-the-art methods in different forecasting horizons and ablation study regarding patches and channel independence is conducted across multiple datasets.\n*  This paper is well-organized and easy to understand\n\nWeaknesses\n\n* Channel independence is hardly novel as previous work, e.g. DeepAR [1] and LogTrans, also have this assumption. These two work model multivariate time series by sharing backbone parameters (LSTM/Transformer) of different channels during training, which can reduce parameter numbers and overfitting issue. \n\n* In experimental setups, \"For Transformer-based models, the default lookback window is L = 96\". However, this look-back window length can be quite short and may not be ideal especially when forecasting window is long, e.g. 336. In figure 2 (Electricity-T=720), best look-back window for Autoformer is  192/336 rather than 96. This setting make me have some concerns that baselines may be under-estimated.\n\n* Self-supervised methods are only a little better and standard supervised fine-tuning. I am wondering if authors can have multiple runs and report mean and standard deviation so that readers may know how significant it is.\n\n[1] Flunkert, Valentin et al. \u201cDeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks.\u201d ArXiv abs/1704.04110 (2020): n. pag.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly motivated and well-written. The idea of utilizing patches to deal with time series is interesting although channel independence is hardly novel given that previous work have explored this setting in time series forecasting. The paper provides some details of its implementation, however, it may need some efforts to reproduce its results as there is no sufficient details, e.g. hyperparameters and code, for reproduction.",
            "summary_of_the_review": "\n\n I update my score to 8 since the authors' response resolve concerns and provide source code on Github.\n\n\\------------------------------------------------------------------------------\n\nThe idea of utilizing patch to deal with time series is interesting and have good empirical results. However, channel independence is hardly novel. In addition, I have concerns regarding experimental settings and am not sure if baselines are under-estimated and results statistically significant.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5506/Reviewer_2c5h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5506/Reviewer_2c5h"
        ]
    },
    {
        "id": "f6dxw7p61go",
        "original": null,
        "number": 3,
        "cdate": 1666650821473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650821473,
        "tmdate": 1666650908669,
        "tddate": null,
        "forum": "Jbdc0vTOcol",
        "replyto": "Jbdc0vTOcol",
        "invitation": "ICLR.cc/2023/Conference/Paper5506/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposed a transformer architecture that forecast multivariate time series channel-wise. In particular, the model employs patching by grouping values in temporal neighborhoods, so that the locality is enhanced and efficiency is improved. The model can also be trained in a self-supervised way by masking random patches, and it is shown such self-supervised pre-training can lead to superior performance with proper fine-tuning.",
            "strength_and_weaknesses": "Strength\n1. The essential idea of the paper is well motivated and presented.\n2. Extensive experiments are done to compare the proposed model with a variety of baselines.\n\nWeakness:\n1. The proposed patching is not novel as LogTrans (Li et al. 2019) has already proposed to use convolution before transformer to enhance locality. The linear processing in PatchTST with fixed path size and stride is not different from 1D convolution, even though a larger grid of configuration is investigated in the experiments.\n2. The idea of masked modeling in representation learning is also marginally novel. The motivation of such a strategy is vague in certain parts. For example, in section 3.2, paragraph 2, _Since time series is often temporally redundant, the masked values at the current time step can be easily referred from neighboring values, which makes the reconstruction process trivial and thus the representation may not carry important abstract information_, the triviality here is not clear: in which case (\"redundancy\") the current time step can be too \"easily\" to refer? Masked modeling itself aims to refer the masked token/patch from contextual objects, why is it deemed trivial in time series? Moreover, in paragraph 3, the prediction in multivariate time series can also employ certain degree of weight sharing to avoid $(L\\cdot D) \\times (M\\cdot T)$ parameter matrices.\n3. Detailed questions: \n+ in supervised learning, the model is said to have a flatten layer with linear head to get univariate time series. When the patches have overlapping, how is the prediction within the overlapping determined?\n+ how is the linear probing and end-to-end fine-tuning done after representation learning? Is linear probing simply reconstructing masked patches again but not update the backbone layers except the final linear head? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: limited.\n\nQuality: fair.\n\nNovelty: limited.\n\nReproducibility: fair",
            "summary_of_the_review": "Overall I think the current version of this paper does not meet the bar of ICLR due to limited novelty and insufficient clarity.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5506/Reviewer_zBqz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5506/Reviewer_zBqz"
        ]
    },
    {
        "id": "VOIvLJmX0Q",
        "original": null,
        "number": 4,
        "cdate": 1666686132501,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686132501,
        "tmdate": 1666686132501,
        "tddate": null,
        "forum": "Jbdc0vTOcol",
        "replyto": "Jbdc0vTOcol",
        "invitation": "ICLR.cc/2023/Conference/Paper5506/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies an efficient way to use the transformer-based model in time-series forecasting tasks. The proposed model, PatchTST, first folds the sequence into several patches which significantly reduces the total sequence length and then splits the multi-channel forecasting signal into independent tasks. In numerical tests on several real datasets, the proposed model reaches the SOTA performance. The authors also test their model on self-supervised learning and verify a promising sign that the transformer-based models could be fit on time-series pretraining tasks.",
            "strength_and_weaknesses": "Strength:\n1. Good numerical performance forecasting, reaching the SOTA results on several real datasets.\n\nWeakness:\n1. Based on the current presentation, the proposed model is analogous to the standard vision transformer (VIT). The technical novelty is limited.\n2. The pretraining datasets are relatively small compared to datasets in CV/NLP fields and the domains of the datasets merely share the mutual underlying knowledge. Combining with marignal improvements reported in Table 4 and Table 5, I tends to believe that the pretraining stage isn't meaningful to the considered datasets.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity: The paper is well written and I enjoy reading it.\n\nQuality: The overall quality is good. After the introduction section, the authors first discuss the model settings and then switch to several experiments to illustrate the efficiency of the proposed model.\n\nNovelty: Despite the strong numerical performance, the technical novelty is kind of limited as the proposed model structure is mainly borrowed from the standard VIT.\n\nReproducibility: The paper doesn't include the codes/git. The model parameters are reported but some other parameters, such as learning rate, and optimizer are not reported. At the current stage, I haven't checked the reproducibility.",
            "summary_of_the_review": "This paper proposes an efficient transformer-based model in time-series forecasting tasks and strong numerical performance is presented. My major concern is the technical novelty as the proposed model is mainly a direct application of standard VIT.\n\n\nMinor issues:\n\n1. The \"64 words\" in the title is a little bit misleading. In the famous VIT paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\", the patch size is 16 x 16. However, the patch size/length in the current paper is around 8-16 which doesn't match 64. I suggest authors modify the title.\n\n2. More ablation studies. In the current presentation, the ablation without instance norm is not included. In the literature, using the instance norm or the equivalent RevIN in [1] could significantly improve the numerical performance. In order to further highlight the effectiveness of the patch and the channel independence ideas, I'm wondering if adding one more ablation study related to the instance norm will be helpful.\n\n3. Robustness test. In time-series forecasting tasks, usually, the results are very sensitive to the model/training parameters.  How about the proposed model? Can we also include a section to discuss the robustness of the proposed model?\n\n\n[1] Kim, T., Kim, J., Tae, Y., Park, C., Choi, J. H., & Choo, J. (2021, September). Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations.\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5506/Reviewer_FEPh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5506/Reviewer_FEPh"
        ]
    }
]