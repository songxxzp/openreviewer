[
    {
        "id": "NxBnWpiyxnG",
        "original": null,
        "number": 1,
        "cdate": 1666496878632,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666496878632,
        "tmdate": 1666496878632,
        "tddate": null,
        "forum": "XVjTT1nw5z",
        "replyto": "XVjTT1nw5z",
        "invitation": "ICLR.cc/2023/Conference/Paper3926/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces rectified flow, which is an ODE model that connects two distributions and samples can be drawn by solving the ODE in two directions. In particular, the ODE aims to model the linear interpolation between samples from two distributions. Techniques like iteratively learning more ODEs and distillation are also introduced. Compared to SDE/ODE of diffusion based models, rectified flow has more linear paths, making numerically solving the ODE and sampling from it easier. Results show some improvements over diffusion based models, and its capability of image-to-image translation.",
            "strength_and_weaknesses": "Strength: \n\n1. The idea is simple but effective. The proposed ODE formulation is very simple, by learning the direction of linear interpolation between samples from two distribution along the interpolation path. However, experimental results show that this method is very effective. \n\n2. It provides a new perspective of diffusion based models which are currently popular. The formulation itself is highly related to diffusion models. Actually, in the VP-SDE's formulation, the forward SDE looks like a spherical interpolation of data and noise. Here the model is doing linear interpolation, and the formulation only requires an ODE. Therefore, it shed new lights on studying diffusion models.\n\n3. It provides a unified framework for generation and domain translation, which is an important contribution.\n\nWeakness:\n1. Although the ODE formulation and training objective is simple, I do not have a good intuitive understanding of choosing this ODE formulation. I found the sentence \"To understand the method intuitively...\" at the end of page 3 not helpful at all. A detailed explanation of why choosing this ODE formulation is necessary in the main text. \n\n2. In addition, there seems not to have clear explanation on why reflow will make the flow more straight. It is not trivial to understand this. \n\n2. Since one major selling point of this method is that it takes less NFEs to sample from, a more comprehensive comparison between existing related approaches on diffusion models is needed. For example, in table 1, I think results of DDIM (not the 1-step distillation) and progressive distillation [1] should be included. These results will better show the trade-off between NFE and FID. In particular, progressive distillation also does very well when NFE is low. Since distillation is also an (optional) part of your method, the comparison is even more important.\n\n[1] Progressive Distillation for Fast Sampling of Diffusion Models, https://arxiv.org/abs/2202.00512",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, although I think some points need to be explained more carefully. The idea is novel, and the authors give detailed algorithm for reproduction.",
            "summary_of_the_review": "I think this paper would be a good contribution to the community. It has some issues in explaining the idea in an intuitive way, i.e., how the formulation is came up with, and I will increase the score if the presentation become better. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3926/Reviewer_T1Pp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3926/Reviewer_T1Pp"
        ]
    },
    {
        "id": "ZxkFTIsfPD",
        "original": null,
        "number": 2,
        "cdate": 1666707567048,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707567048,
        "tmdate": 1669197832414,
        "tddate": null,
        "forum": "XVjTT1nw5z",
        "replyto": "XVjTT1nw5z",
        "invitation": "ICLR.cc/2023/Conference/Paper3926/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the paper, the authors introduce a method to train a continuous normalizing flow connecting two empirical distributions. They call their flow models rectified flows. These flows are trained by minimizing a simple least square objective given pairs of samples. Thereby, the flow is forced to approximately linearly interpolate between the samples. By iteratively retraining the model (reflow) the rectified flow learns to connect corresponding areas in the distribution.\nThe method is applied to several image generation benchmarks. The quality of the generated images is competitive with other procedures while being very efficient in sampling.",
            "strength_and_weaknesses": "### Strengths\n\nThe idea of transporting samples of one empirical distribution to another with a continuous normalizing flow is novel and creative. The learning algorithm is intuitive and thoroughly analyzed. Although being counterintuitive at first glance, the method is useful for many applications, the most obvious being domain adaptation like image-to-image translation, but also image generation from noise with improved sampling efficiency.\n\n### Weaknesses\n\nIn my point of view, the most interesting application of this method would be domain adaptation, e.g. changing the weather or light conditions of the images captured by a (self-driving) car. This application is not explored in the paper. Instead, the authors focus on translating humans to cats, which might be amusing but not very useful.\nFurthermore, the structure of the paper is poor. I elaborate on this below.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The ideas presented in the paper are novel and creative. Details regarding the method and the setup used for the experiments are very extensive.\nThe paper has an odd structure. It only consists of 3 sections and does not have a conclusion section. I would advise the authors to add this section, as well as one discussing the background and related work.",
            "summary_of_the_review": "Overall, I tend towards accepting this paper, but ask the authors to restructure their paper and put a larger emphasis on their method being useful for domain adaptation.\n\nEdit after rebuttal:\n\nI appreciate the author's effort to improve the readability and theoretical soundness of their work. Therefore, I am in favor of accepting the article and raise my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3926/Reviewer_ot8j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3926/Reviewer_ot8j"
        ]
    },
    {
        "id": "tZy1afKeP8X",
        "original": null,
        "number": 3,
        "cdate": 1666724179494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724179494,
        "tmdate": 1666724179494,
        "tddate": null,
        "forum": "XVjTT1nw5z",
        "replyto": "XVjTT1nw5z",
        "invitation": "ICLR.cc/2023/Conference/Paper3926/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to learn continuous-time transport maps defined using ODEs to map between two distributions. The algorithm aims to regularize for straightness of the paths, which can help make simulation faster. The idea of Rectified Flow is to sample pairs (x0, x1) and fit a vector field to the linear interpolation between x0 and x1. This optimization process is then repeated by creating pairs of samples from the model.\n\nThe authors make a series of theoretical statements regarding this training process, especially that if this process is repeated K->inf times, then the paths become straight.\n\nThe experimental results show that the model is competitive in terms of FID with previous diffusion SDE models while being better than deterministic ODE counterparts.",
            "strength_and_weaknesses": "Strengths:\n  - The objective is simple and achieves reasonable results.\n\nWeaknesses:\n  - The theoretical details of the paper are difficult to follow. It would be good to have a clearer connection/derivation between the theoretical motivation (straight paths) and the objective itself.\n  - Only image quality metrics are reported. Existing ODE/SDE models tend to report NLL in addition to FID, and it would be good to have NLL comparisons. This reviewer's experience with FID has been that it is a rather fickle metric, and FID on CIFAR-10 only compares to training data with no test for generalization.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty:\n  - To the best of my knowledge, this work is novel. \n  - Getting good samples with N=1 is interesting, although these are purely qualitative and it's not clear how this compares to directly training a one-step model.\n\nQuality:\n  - The paper does a good job of explaining the algorithm since it is quite simple, but I had a hard time understanding the theoretical motivations for this objective.\n  - Relatedly, as the paper focuses on sample quality comparisons on non-CIFAR10 data sets, it might make sense to have comparisons to GAN-based methods for training OT maps, i.e. works that directly learn the one-step transport map instead of straightening an ODE, e.g. [1,2]. The second reference has quantitative experiments on image-to-image translation task such as colorization, inpainting, etc. While GANs' optimization is adversarial, this does not necessarily mean that rectification is faster or more stable to train in practice, since it requires iterative rectification. IMO showing a bit more how Rectified Flows' iterative procedure is better than directly modeling OT solutions could make a nice addition to the paper.\n\nClarity:\n  - Figure 2 is very nice.\n  - The random variables for each expectation should be explicitly written out. The lack of clarity regarding this made the proofs hard to follow for me and often required some imagination. For example, Eq (12):  d/dt E[h(X_t)] = E[\\nabla h(X_t) dX_t/dt ]. The first expression I assume is an expectation over X_t since this is the only random variable, but the second expression has two random variables (X_t and d/dt X_t). Is this equality the result of some stochastic calculus rule? It would be good to cite why this equality is true.\n  - What are the numbers in parentheses in Figure 1(a) ?\n  - The theoretical results assume that the random process is \"path-wise continuously differentiable\" in order to define stochastic derivatives. This seems like a rather strong assumption and standard SDEs would not satisfy this. Can the authors explain whether this is a restrictive assumption?\n\n[1] \"Optimal transport mapping via input convex neural networks.\" Makkuva et al. (2021) \n[2] \"Generative modeling with optimal transport maps.\" Rout et al. (2021) ",
            "summary_of_the_review": "This paper makes a number of interesting contributions that I think are useful for the ICLR community, so I vote for acceptance. I do think the writing can be made clearer, especially the connection between the theoretical details and the actual algorithm. How this explicitly relates to OT is also not completely clear, and is only hinted at in scattered places in the paper. Empirical results feel mainly qualitative, and I think (i) reporting NLL and (ii) doing more quantitative comparisons with existing models like GANs will make the results better.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3926/Reviewer_e8Br"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3926/Reviewer_e8Br"
        ]
    }
]