[
    {
        "id": "kkNxrTsw6Ot",
        "original": null,
        "number": 1,
        "cdate": 1666588736063,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588736063,
        "tmdate": 1666588736063,
        "tddate": null,
        "forum": "HnLFY8F9uS",
        "replyto": "HnLFY8F9uS",
        "invitation": "ICLR.cc/2023/Conference/Paper5070/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper argues for stochastic policies that maintain high entropy for high RL performance. In particular, the authors propose to randomly perturb the mean of the output Gaussian distribution to produce more diverse actions. In the experimental evaluation, this perturbation scheme is compared to PPO (with and without entropy regularization) and data augmentation techniques including RAD and DRAC.",
            "strength_and_weaknesses": "The paper is overall written well. However, it\u2019s not clear to me how adding action noise is different from existing ways of incorporating entropy regularization. For example, this is equivalent to enforcing that each component of \\sigma is at least some threshold. This clipping already exists in many PPO implementations I believe.\n\nI also have some questions and concerns in regards to the experimental evaluation:\n- Figure 2 shows that PPO fails to learn in these environments, but this is probably due to the entropy regularization being removed. A fairer comparison would be to PPO with the standard entropy regularization.\n- The comparison to the data augmentation methods is a bit confusing to me since these are methods that perform image augmentation for image-based RL tasks. Are the experimental tasks being solved from images?\n- There seems to be more useful comparisons to include, such as Soft Actor-Critic, which optimizes the maximum-entropy RL objective and automatically tunes the entropy coefficient, and exploration strategies for RL, such as Random Network Distillation.\n- What are the action spaces for each environment? It is my understanding that each action component is between -1 and 1 for the DM Control Suite tasks. Wouldn\u2019t alpha values larger than or equal to 1 essentially make the actions uniform-random?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/Quality/Novelty: See questions under \u201cStrengths and Weaknesses.\u201d\n\nReproducibility: The paper includes pseudo-code and hyperparameter values used to implement their algorithm. I believe a reader could re-implement the proposed modification from these details.",
            "summary_of_the_review": "Overall, the novelty of the work is not clear to me. The experimental evaluation also lacks relevant baselines, and some details of their evaluation are currently unclear. Therefore, I am recommending a reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5070/Reviewer_fgod"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5070/Reviewer_fgod"
        ]
    },
    {
        "id": "2gqBp1XNVL2",
        "original": null,
        "number": 2,
        "cdate": 1666616271703,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616271703,
        "tmdate": 1666616271703,
        "tddate": null,
        "forum": "HnLFY8F9uS",
        "replyto": "HnLFY8F9uS",
        "invitation": "ICLR.cc/2023/Conference/Paper5070/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a Robust Policy Optimization (RPO) method, a simple extension of the popular Proximal Policy Optimization (PPO) method, by adding a uniform random number to the action mean, sampled from a normal distribution. It is argued that std. normal distribution used to parameterize continuous action is not suitable if more exploration is expected from the agent. Authors modify the gaussian to have more support than the std. gaussian r.v. \n\nEmpirical results on std. RL benchmarks demonstrate that the proposed method has higher exploration throughout the training. ",
            "strength_and_weaknesses": "Some strengths of the paper: \n\n- The idea is easy to understand, and extremely easy to implement. Basically just adding a random number from uniform distribution would work. \n- The results show that the agent trained with the proposed method explores more than the baseline PPO method. \n\nSome weaknesses of the paper: \n\n- The idea of exploration-exploitation is very old in the RL/bandit community. \n- Authors didn't compare the std. exploration methods in the literature. \n- There is no theoretical justification for choosing this particular distribution. How do other distributions with wider support work in place of gaussian? \n- The effect of changing the distribution to the constraint of the PPO method is also not discussed. How does the constraint function behave when the policy parameterization is changed? \n- Algorithm 1 is not written very well. what is \"logp\", \"prob\"? Please make it more clear.  ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is not clear at points (for ex: Algo. 1)\n- The novelty is limited in the paper. \n- The method seems easy to reproduce.  ",
            "summary_of_the_review": "- Authors propose a modification of the existing PPO method for continuous actions cases, by increasing the support of the policy parameterization distribution. \n- They add a uniform random number to the mean of the gaussian distribution used to parameterize the policy, thereby increasing its support. \n- There is no theoretical justification for using the particular distribution; related works on using std. exploration ($\\eps$-greedy etc) is not discussed. \n- Other distributions with wider support around the mean are also not discussed in the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5070/Reviewer_NT5y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5070/Reviewer_NT5y"
        ]
    },
    {
        "id": "4Q4fK38f5gn",
        "original": null,
        "number": 3,
        "cdate": 1666676901218,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676901218,
        "tmdate": 1666678980843,
        "tddate": null,
        "forum": "HnLFY8F9uS",
        "replyto": "HnLFY8F9uS",
        "invitation": "ICLR.cc/2023/Conference/Paper5070/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work the authors proposed a new robust policy optimization method in which the main approach is to randomly perturb the mean of the Gaussian policy to improve exploration. The main motivation is that stochastic policies that have high entropy tend to produce better RL performance, and so the authors propose to constantly perturb this policy during training. Empirically they showed that this new policy outperforms standard RL methods including the ones with data augmentation schemes (e.g., PPO, RAD, DRAC).\n",
            "strength_and_weaknesses": "Strengths:\nPaper is generally clear in explaining the ideas of  how a perturbed Gaussian policy policy can maintain policy entropy throughout training and help with RL performance.\nThe paper includes pseudo-code and detailed hyper-parameters for readers to re-implement their work. This work should be reproducible. \nThe authors evaluated the RPO method on quite a number of domains and in some cases the proposed method is more powerful than SOTA.\n\nWeaknesses:\nThe justification of this method is mainly empirical, there are not theoretical justifications of this work in terms of why the proposed method would work. \nIf the main contribution is on increasing entropy for better exploration, then there is not enough baseline comparisons with other RL methods that perturb the policy to improve robustness as well as performance during training. For example, one such algorithm is NoisyNet, which also perturbs Gaussian policy network to improve robustness. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clearly written with most concepts well explained, though some parts of this paper will be benefited by more explanations (especially the motivations/intuitions of the main algorithm). This work also detailed the experimental setup and algorithm settings (hyper-parameters). It also includes links to pseudo-code for users to reproduce their results. \n\nHowever, my main concern about this work is its novelty. Perturbing a policy to achieve better exploration (entropy) is not a new idea and more comparisons and theoretical analysis are needed to back the authors' claims. ",
            "summary_of_the_review": "Authors propose a new randomization scheme for RL policy training in order to improve exploration, and thus the performance of RL (PPO).\nWhile this method may show some empirical benefits in some small domain, there has not been much theoretical backing of this approach justified in the paper. Furthermore, it is unclear whether this method will also work with other setting in which a non-Gaussian policy is used. Finally, the experiments are not fully convincing (for example in some cases the proposed method does not strongly beat the baseline, and more SOTA baselines are needed to really showcase the potential superiority of the proposed method).\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5070/Reviewer_JmVS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5070/Reviewer_JmVS"
        ]
    },
    {
        "id": "TEn0E-BDKV",
        "original": null,
        "number": 4,
        "cdate": 1667219072476,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667219072476,
        "tmdate": 1667219072476,
        "tddate": null,
        "forum": "HnLFY8F9uS",
        "replyto": "HnLFY8F9uS",
        "invitation": "ICLR.cc/2023/Conference/Paper5070/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new algorithm RPO which builds on top of PPO. The algorithm takes the mean output of a Gaussian distribution, adds a uniform perturbation to the mean, then samples actions from the resulting Gaussian distribution with the perturbed mean. The proposed method is shown to be effective on a variety of high dimensional continuous control tasks.",
            "strength_and_weaknesses": "Strength:\n- Proposed method is extremely straightforward, easy to implement, and appears to show good results.\n- Paper was easy to read and clearly written.\n\nWeakness:\n-  My initial problem with the paper is the word \u201crobust\u201d since the term itself is quite overloaded and can have different meanings in different fields (e.g. robust control has a very specific meaning). The authors\u2019 use of the robustness throughout the paper seem quite vague and note well defined.\n- While the experiments are quite detailed and do show improvements on a set of challenging tasks over the benchmark, I do not believe they provided us with a better understanding of why the proposed approach worked well. Especially since the idea that its beneficial to maintain a certain degree of high entropy through training seems somewhat counter-intuitive in many scenarios. I believe some toy examples would be more helpful in this regard in showing us why/under what conditions the proposed method works well, I think this is something that would greatly enhance the quality of the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clear to read, I do not see any major issues with reproducibility. The proposed method appears to be novel.",
            "summary_of_the_review": "Overall, while the introduced method does appear to be novel and shows evidence of empirical improvements on a set of benchmarks, I don\u2019t believe the paper in its current state advances our understanding of the field as a whole and therefore I cannot recommend the paper for acceptance at this time.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5070/Reviewer_JnzH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5070/Reviewer_JnzH"
        ]
    }
]