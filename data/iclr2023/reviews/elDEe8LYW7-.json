[
    {
        "id": "jrHWyDy50ux",
        "original": null,
        "number": 1,
        "cdate": 1666451299499,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666451299499,
        "tmdate": 1666451299499,
        "tddate": null,
        "forum": "elDEe8LYW7-",
        "replyto": "elDEe8LYW7-",
        "invitation": "ICLR.cc/2023/Conference/Paper1898/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- Authors propose a unified framework to learn many voice synthesis tasks. The unified framework includes task-independent self-supervised training of a backbone network, which does not require any labelled data, and then task specific training with labelled data.\n- Authors claim that this unified approach offers advantages such as controllability of synthesized speech, data efficiency for downstream tasks, and fast training convergence \u2014 all this without sacrificing the quality of synthesized speech.\n- Authors assume that the most of the voice synthesis tasks can be defined synthesizing and controlling our aspects of voice, that is, pitch, amplitude, linguistic, and timbre.\n- For estimating pitch, in an unsupervised fashion, authors use the property of CQT input representation, and the transforming the representation appropriately to get a known shift in the fundamental frequency value. While this will bias learning of fundamental frequency like value, it is not clear how authors obtain absolute fundamental frequency. This method is inspired by \u201cSPICE: Self-supervised Pitch Estimation\u201d.\n- For getting linguistic features, authors propose to use a contrastive learning approach where they modify the signals in a way to keep the linguistic information intact, while changing other properties of the signal \u2014 and use this property learn features which maximizes similarity if the source audio (i.e. before transformations) is the same i.e positive samples in the contrastive learning approach are two different transformations of the same signal and negative samples are the transformations of two different signals. Negative samples clearly have different linguistic information, but it is not clear that they will share the same non-linguistic features \u2014 maybe that is not necessary for this approach to work. Authors extract wav2vec features from transformed signal, which is transformed via another learnable network. I am curious about the importance of this contrastive learning approach vs directly using wav2vec features.\n- Authors propose learnable vector encoding of input signal \u2014 one global vector and then time varying tokens \u2014 to represent timbre. It is not clear why these learnable encodings will necessarily encode timbre, and will necessarily be disentangled from pitch and linguistic representation.\n- Authors propose to use Parallel WaveGan trained via reconstruction loss  on mel-spectrogram, and adversarial and feature matching loss to train the synthesizer network.\n- Experiments:\n    - Authors shows that replacing fundamental frequency extracted from the analysis network by the fundamental frequency extracted from four other approaches, and show that the synthezised audio using their synthesizer is assessed to match the original signal before doing the f0 replacement. It is not clear why this implies that their analyser learns better f0 \u2014 it is totally possible and likely that even if their analyser is not learning the correct f0, but since the representation is trained end-to-end along with synthesizer, reconstructions with this representation are closer to original, and using a more correct (or even a different f0) will throw the model off.\n    - Authors show better reconstruction quality than a mel2wav architecture namely HiFiGAN. Since the input representations are different, specifically authors choice of input is much more rich than mel-spectrogram used in mel2wav setup. It is not clear why having better reconstruction is a good metric to measure in this case.\n    - Authors show clear improvement in zero-shot voice conversion, when compared with the baseline.\n    - Authors show better performance than the baseline on TTS and Singing voice synthesis. The baseline for full data seem far from real audio, which probably points to the baseline being weaker than SOTA TTS.\n    - Authors show good results on Voice Designing.",
            "strength_and_weaknesses": "- Overall:\n    - Authors claim about disentangled representation learning of f0, timbre, linguistic information might be misleading. I wonder if authors will consider changing the wording to showcase what design choices they make that promote learning disentangled representation, and they verify experimentally that some of it is achieved.\n    - Experimental results on downstream tasks seems very good. The impact of the work can be maximized by making sure that the baseline performance is replicated from original work e.g. baseline TTS on full data, seems to be very far from real data MOS, which does not reflect the SOTA on TTS tasks.\n\nStrengths:\n- Extensive evaluation\n- Impressive Results\n\nWeakness:\n- Not so great scientific rigor\n- Not so-great choice of baselines\n- Potentially misleading claims or just confusing to understand what is claimed vs what is expected.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper's writing can be improved by clearly stating assumptions -- revisiting this assumption in the experimental sections and verifying them. Stating what is claimed, vs which choices are made to encourage learning specific kind of features, even when they are not guaranteed to happen, will be useful.\n\nThe paper re-uses components from other papers and cites them. Paper has limited novelty in terms of methods proposed but seems to be a useful well-engineered system.",
            "summary_of_the_review": "Overall, the authors should reconsider claims about disentangled representation learning. Should consider doing ablations for various choices, and improve clarity of the writing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1898/Reviewer_vW43"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1898/Reviewer_vW43"
        ]
    },
    {
        "id": "ih_lXdQFsEh",
        "original": null,
        "number": 2,
        "cdate": 1666494377636,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666494377636,
        "tmdate": 1666494377636,
        "tddate": null,
        "forum": "elDEe8LYW7-",
        "replyto": "elDEe8LYW7-",
        "invitation": "ICLR.cc/2023/Conference/Paper1898/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a unified framework for human voice synthesis based on neural analysis and synthesis modules. The analysis part extracts disentangled and controllable voice features based on domain knowledge (pitch, periodicity/aperiodicity and timbre). Each of them is trained independently with a dedicated neural analysis module. The timbre analysis module divided the feature into a global timbre embedding vector and time-varying timbre tokens.  The synthesis part take the voice features and generates waveforms with frame-level and sample-level synthesis modules. The framework is evaluated in terms of F0 estimation and reconstruction error as a vocoder, showing superior results.  The framework is applied to four voice synthesis tasks including voice conversion, TTS, singing voice synthesis, and voice design, along with additional encoder modules. In each task, the proposed framework outperform previous state-of-the-arts. \n\n\n",
            "strength_and_weaknesses": "Strengths\n- The proposed framework is highly modular and flexible, being plugged in various voice synthesis tasks\n- The framework is interpretable as it is designed to be similar to DSP-based parametric vocoders. \n- They achieve impressive results in all tasks, outperforming compared models.   \n\nWeakness\n- The paper includes too much content. Due to the space issue, all details are moved to the appendix.  \n- Although the proposed framework is well-structured, it is extended from the authors' previous work (NANCY) and many new components are adopted from other previous work (self-supervised pitch estimation, linguistic representation, parallel WaveGAN, and so on). \n- While the framework achieves better performances than previous work in many tasks, the experiment condition is not completely fair because the NANCY framework was pretrained with a large-scale dataset (10,571 hours from 6,176 speakers and 624 singers).  \n- The time-varying timbre embedding is a great idea but it was not validated through an ablation study. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity \n- The paper is easy to read in general. But, all details are located in the appendix section. I am not sure this type of writing structure is appropriate as a conference paper. \n- Since the framework include a lot of components, the authors should clarify which part is novel and how they affect the model performance.   \n\nQuality \n- The model quality was well evaluated on the backbone experiment and each application task with MOS, SSIM, CER and more. \n- F0 estimation was evaluated only with a listening test for the reconstruction quality. It could be also evaluated in terms of F0 estimation accuracy. \n\nNovelty \n- While a lot of components are borrowed from previous work, there are also some new ideas such as time-varying timbre embeddings. But, the novel part was not clearly validated. For example, the time-varying timbre embeddings could be evaluated in the reconstruction experiment (section 3.2) as an ablation study.\n\nReproducibility\n- The model architecture is well delineated in the appendix section. \n- However, the detail of the training data is missing. Information on gender, age, language, professionality (they include voice actors or professional singers, etc.) would be helpful to understand the model training. \n\nMinor comments \n- (page 2) the sinusoidal waveform equation ( x[t] = Ap[t] sin (...)) has the summation inside the sine function. Is it correct? The summation should be moved outside the sine function?  \n\n- (page 3) In equation (1), \"cossim\" could be replaced with a short symbol such as \"d\". In page 9, they use the symbol \"d\" to denote the cosine distance. Please make them consistent. \n\n- (page 4) what is \"simple spectrogram\"?  Is it the spectrogram with the linear frequency scale?\n\n ",
            "summary_of_the_review": "This paper proposes a flexible voice synthesis framework with high-performance. The framework include robust F0 estimation and high-quality reconstruction as a vocoder.  The flexible structure was applied to voice conversion, TTS, singing voice synthesis and new voice design. I believe that the framework is highly well designed and engineered. \n\nOne main issue of this paper is the writing structure. The main body is a summary of long manuscripts on \"a large system\" and all details are moved to the appendix section. I feel like it would be better for the authors to submit this paper to a journal which allows many pages. \n\nIn addition, the novel part such as time-varying timbre embedding was not thoroughly validated, which might be probably due to the space issue. \n\nLastly, the high-performance of the framework is attributed to not only the model architecture but also the large-scale training data. This should be clearly indicated when compared to other models. \n ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "details_of_ethics_concerns": "The proposed model can be used for voice conversion in the zero-shot setting, which can be potentially used as a \"deep-fake\" technology for voice. This is actually a general problem of voice synthesis and conversion. The authors acknowledge it and they also state counteracting technologies such as anti-spoofing.\n\n ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1898/Reviewer_Hzqp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1898/Reviewer_Hzqp"
        ]
    },
    {
        "id": "AnxOTI4lJR3",
        "original": null,
        "number": 3,
        "cdate": 1666622260784,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622260784,
        "tmdate": 1666622260784,
        "tddate": null,
        "forum": "elDEe8LYW7-",
        "replyto": "elDEe8LYW7-",
        "invitation": "ICLR.cc/2023/Conference/Paper1898/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper describes a learned analyzer and synthesizer for speech that can be trained end-to-end, and demonstrates their utility in several speech and singing synthesis tasks: voice conversion to match a target individual, voice conversion to match target characteristics, text to speech, and singing voice synthesis. In listening tests for the targeted voice conversion, NANSY++ exceeded YourTTS  by 0.56 MOS points (from 1-5 range) in quality and 0.38 points (1-5 range) in speaker similarity. For TTS, compared with Glow-TTS, the proposed approach performs better in quality and character error rate of a pre-trained recognizer at various amounts of training data from a single speaker (5 mins, 10 mins, 30 mins, and 30 hours). For singing voice synthesis, the proposed approach out performed diffsinger in terms of subjective quality with various amounts of training data. And for voice conversion to match target characteristics, the proposed approach was able to generate 320 speakers that achieved an objective voice diversity similar to that of 240 actual different speakers.",
            "strength_and_weaknesses": "Strengths:\n* Well executed experiments: the experiments are all clearly described, well performed, and test key claims of the paper\n* Approach works well. The results of those experiments show that the proposed method does provide superior performance to the various baseline approaches.\n* Model provides general utility. As demonstrated by the variety of experiments conducted, the proposed model is useful in a large number of situations without requiring a large amount of data or a large amount of time to retrain. For example, for TTS, between 30 minutes and 30 hours of data, the model improves in MOS from 3.64 to 4.07 and in CER from 2.20 to 1.68%. These are meaningful improvements, but the model trained on 30 minutes is still performing quite well and fairly close to the model trained on 30 hours.\n* General clarity of presentation. The paper is generally well presented, except for the issue of certain claims being slightly over stated mentioned below under weaknesses.\n\nWeaknesses:\n* The captions of the figures and tables are too brief and should be expanded so that figures are clearly interpretable by themselves without referring to the text.\n* Certain claims are slightly overstated, although these claims are also not especially necessary for the work to have impact. In particular, I am referring to claims that the model is disentangling various voice characteristics and that it is self-supervised. While both of these may be technically true, it is thanks to careful inductive biases baked into the model architecture. Thus it is not especially surprising that the model has these characteristics, but it's not like they just popped out of the data. ",
            "clarity,_quality,_novelty_and_reproducibility": "See above, but generally the paper is clearly written and high quality experiments. One minor comment is that I would recommend avoiding the acronym \"VD\" for \"voice designing\" as there is already an unpleasant meaning to this acronym.\n\nIn terms of novelty, the approach is based upon an earlier work, NANSY (Choi et al, 2021b), but adds the trainable transformations in the analysis block, which enables the end-to-end training. This paper also applies it to a number of new tasks, which were not necessarily easy to target in the earlier version.\n\nOn reproducibility, the paper provides details of all of the architectures as well as hyperparameters in the appendix. Datasets are publicly available. Code is not released because of concerns about the potential of the model to be misused.",
            "summary_of_the_review": "End-to-end trainable voice analysis-synthesis pipeline that cleverly separates various voice characteristics through relative contrasts. Experiments show that it is useful and data efficient in several different tasks. Minor issues with clarity of captions and with certain claims being slightly overstated.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "As with any low-data voice conversion approach, there is the possibility of the method being used to create recordings of statements that speakers never made. This is acknowledged by the authors in their ethics statement and they have withheld the release of their code from the public, but promise to make it available to \"identified and authorized users\" who request it. I think this is acceptable, but that it should be mentioned.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1898/Reviewer_D823"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1898/Reviewer_D823"
        ]
    },
    {
        "id": "EHSyz7O-UG",
        "original": null,
        "number": 4,
        "cdate": 1666638299206,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638299206,
        "tmdate": 1670560728376,
        "tddate": null,
        "forum": "elDEe8LYW7-",
        "replyto": "elDEe8LYW7-",
        "invitation": "ICLR.cc/2023/Conference/Paper1898/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed NANSY++, a unified framework of synthesizing and manipulating voice signals from analysis features. The backbone network of NANSY++ is trained in a self-supervised way. After the backbone network is pre-trained, different voice applications can be adapted by modeling the analysis features required for each task. Experiments show that the proposed NANSY++ framework has the advantages of controllability, data efficiency, fast convergence, and high quality synthesis.",
            "strength_and_weaknesses": "It's impressive that NANSY++ can use one unified framework to handle four different speech tasks while achieving good performance in audio quality, data efficiency and controllability. The framework is well designed in a modularized way and can disentangle various speech representations. One drawback of this paper, however, is limited ablation study. Since the proposed NANSY++ framework requires a variety of modules and augmentations, it's hard to know what are the core factors that contribute to the model performance.\n\nSeveral detailed questions:\n\nIn section 2.2, for the linguistic feature disentanglement, what's the exact equations of breathiness perturbation?\n\nIn section 2.3, are the global and time-varying timbre embeddings also trained in a self-supervised way or trained jointly with the waveform reconstruction? If they are jointly trained, how to guarantee the time-varying timbre embeddings don't contain other information such as pitch and linguistic?\n\nIn section 4.2.2, what's the problem definition here for zero-shot TTS? Does the mean the model trained on TTS task can be adapted to a new dataset or the pre-trained backbone model can be used directly for TTS task?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and technically solid. However, ablation study is largely missing. ",
            "summary_of_the_review": "The proposed unified voice synthesis framework NANSY++ achieved impressive performance and can serve as a general speech pre-processing and pre-training pipeline.\n\n\n=================\n\nUpdates after reading the author's rebuttal: the author addressed my concerns well, so I updated my rating to 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1898/Reviewer_U62B"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1898/Reviewer_U62B"
        ]
    }
]