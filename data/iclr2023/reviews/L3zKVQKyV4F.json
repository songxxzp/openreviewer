[
    {
        "id": "QL0VkFFKnhO",
        "original": null,
        "number": 1,
        "cdate": 1665966849760,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665966849760,
        "tmdate": 1665967024602,
        "tddate": null,
        "forum": "L3zKVQKyV4F",
        "replyto": "L3zKVQKyV4F",
        "invitation": "ICLR.cc/2023/Conference/Paper1018/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new approach for object depth estimation by using audio and video propagation times.",
            "strength_and_weaknesses": "Strengths\nI really like this paper. Although the idea of using time of arrival differences is simple, it hasn't been explored so thoroughly before. The execution is nice. One main advantage is that this method works better with larger distances, while other depth estimation methods typically work worse with larger distances. The authors collect a clearly defined dataset of in-the-wild sounds and run experiments with that, showing promising results.\n\nWeakness\nThe clear weakness is that it is not nearly as generalizable as existing methods. It almost isn't fair to compare this method to lidar, stereo etc because of how limited this method is. It only detects the depth of a single object with a clear colllision/impact. the collision must be audible but also visible. This means that many auditory events would not be applicable because they don't have a visual onset time (e.g. car engines, speech). This makes the method inapplicable for many applications of RGB->Depth. I would like to see a more clear discussion of the limitations here as well as target application scenarios the authors have in mind",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Generally pretty easy to follow. Some of the details of section 4 could probably be moved to supplementary materials, I think the high level idea needs to come through a bit more. \n\nNovelty: Gan et al. have used a similar idea, but that paper was more focused on navigation. Overall this work is quite novel\n\nQuality/Reproducibility: Both are good\n\nI would like to see some example videos in supplementary materials. What does the video at 240fps look like, what does the audio sound like from 60m away etc.",
            "summary_of_the_review": "Interesting idea and well executing. Main weakness is that the method is extremely limited to very certain objects in limited scenarios. I believe it is a valid contribution, but I would like to see the limitations discussed better.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1018/Reviewer_SYPH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1018/Reviewer_SYPH"
        ]
    },
    {
        "id": "rPaeljMB-yw",
        "original": null,
        "number": 2,
        "cdate": 1666676592525,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676592525,
        "tmdate": 1666676592525,
        "tddate": null,
        "forum": "L3zKVQKyV4F",
        "replyto": "L3zKVQKyV4F",
        "invitation": "ICLR.cc/2023/Conference/Paper1018/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a depth estimation method based on deep neural network, FBDepth, which makes use of both video and audio signals to estimate the object which makes collision event. FBDepth is a two-stage mehod where in the first stage it estimates the visual timing. In the second stage, FBDepth regresses the object depth based on the image, audio, visual timing, and optical flow as the inputs. To train the network, the authors collect a new dataset named  the audio-visual depth (AVD) dataset. The evaluation results show the FBDepth achieved superior results than monocular and stereo image-based depth estimation methods.",
            "strength_and_weaknesses": "### Strength\nS1. This is the first work that tackles the depth estimation problem using both image and audio. The motivation for using the flash-to-bang phenomenon is quite inspiring.\n\nS2. Besides FBDepth, the method itself, the authors also collect a new dataset that could benefit the community in this direction.\n\nS3. The comparison with monocular and stereo-based methods shows the effectiveness of the proposed method.\n\n### Weaknesses\nW1. Although the flash-to-bang phenomenon is quite well motivated, the depth estimation, in the end, is not really making use of it directly. The network still regresses the depth directly without using Eq. (1) by audio, video, and optical flow as the inputs.\n\nW2. The authors provide no qualitative results at all. It's pretty hard to assess how well the method works in practice without qualitative results. Both the object segmentation mask prediction and the object depth prediction are quite important to show visual results. Moreover, the authors made a very interesting new dataset (the AVD dataset), but no qualitative samples are provided at all with the submission.\n\nW3. The method would assume a static camera if I understood correctly. It is unclear how well this method can work when the camera is moving.\n\nW4. It is unclear how the authors make the train/val/test splits and on which split are the numbers reported.\n\nW5. Overall the writing of the paper needs to be improved a lot. It is generally quite puzzling to understand the technical details of the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "The originality of the paper needs to be praised as it approaches depth estimation using a novel multi-modality method (audio+video).\nHowever, the quality and the clarity of the paper still require quite some improvement to meet the bar of a top conference like ICLR.",
            "summary_of_the_review": "Given the current status of the paper (as I stated in the Strength And Weaknesses section), I would not recommend accepting the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1018/Reviewer_RxUQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1018/Reviewer_RxUQ"
        ]
    },
    {
        "id": "RqzUxRN3ppg",
        "original": null,
        "number": 3,
        "cdate": 1667086528202,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667086528202,
        "tmdate": 1667086528202,
        "tddate": null,
        "forum": "L3zKVQKyV4F",
        "replyto": "L3zKVQKyV4F",
        "invitation": "ICLR.cc/2023/Conference/Paper1018/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Motivated by the \u2019flash-to-bang' phenomenon, in this paper, the authors propose a new audio-visual learning model for sound source depth estimation. In particular, the authors formulate the sound source depth estimation as an audio-visual collision event localization task.\nTo solve the task and increase depth estimation accuracy, a coarse-to-fine pipeline is introduced. To facilitate the research, the authors collected a new video dataset including 3600+ video clips with 24 objects. Experimental results show that the proposed approach can outperform compared mono and stereo methods. ",
            "strength_and_weaknesses": "Pros:\n\n+ The idea of depth estimation using audio and visual information is very interesting.  Based on the difference between the time-of-flight of the light and the sound, the authors formulate sound source depth estimation as an audio-visual collision event localization task.\n\n+ The coarse-to-fine pipeline is technically sound. From the event level to the frame level, the authors progressively increase video event localization precision in the proposed method.\n\n+ Compared to mono and stereo approaches, the proposed sound source depth estimation method achieves competitive performance. \n\nCons\n\n- My biggest concern is about the evaluation. The current experimental results cannot fully validate the effectiveness of the proposed method. The authors provided numerical comparison results. But, visual results are totally missing in the paper. \n\n- Some details about the collected dataset were not provided. What are object categories in the dataset? Video length? Sound source number in videos? Quality of the groundtruth depth? In addition, it would be better to provide some video samples in the dataset. \n\n- Whether the compared monocular and stereo depth estimation methods were re-trained using the new dataset? If not, the comparison is not fair. \n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors address a very interesting problem in the paper. But, the evaluation is not sufficient, and some details are missing.\n\nThe authors did not promise that they would release their dataset. Without the dataset, it is not possible to reproduce the results in the paper. ",
            "summary_of_the_review": "Audio-visual sound source depth estimation is an interesting problem, and the proposed method seems technically sound to me. However, the authors did not provide sufficient experimental results to support the proposed method, and some details about the new dataset are missing. Thus, my current rating is borderline accept. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1018/Reviewer_Febd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1018/Reviewer_Febd"
        ]
    },
    {
        "id": "YzSWQi650Y",
        "original": null,
        "number": 4,
        "cdate": 1667105772069,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667105772069,
        "tmdate": 1670172771894,
        "tddate": null,
        "forum": "L3zKVQKyV4F",
        "replyto": "L3zKVQKyV4F",
        "invitation": "ICLR.cc/2023/Conference/Paper1018/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a novel framework, called Flash-to-Bang Depth (FBDepth), for passive sound-source depth estimation . The authors use audio-visual correspondence and optical flow manipulation to get decimeter-level depth accuracy. The proposed audio-visual depth estimation system uses video, audio and optical flow to perform event-level localization to retrieve the collision event. The main idea is based on a well-known method to estimate the distance of a lightning strike. The presented comparisons with previous depth estimation approaches show that FBDepth shows better performance than previous methods that use purely video. Although, stereo matching methods seem to be significantly better in the close range.   \n",
            "strength_and_weaknesses": "Strengths - \nThe integration of information from sound to add visual information to yield more reliable depth is a novel and much needed idea \nThe use of everyday cameras and mics in smartphones, makes the results quite general and reproducible \nThe ablation study allows for a good understanding of incremental benefits of some of the major components of the framework\n\nWeaknesses -\nThere hasn\u2019t been any investigation of -\n - the effect of reverberation in different environments which can smear and sometimes even give different type of distance cues based of the ratio of direct-to-reverberant ratio\n - the effect of object size and material which seems to be significant considering the duration of impact can vary quite a lot based on mass and stiffness as shown by Traer et al., 2019 \nMany distance cues in the sounds seem to be ignored in the current framework\nLack of ground truth or a more reliable source of timestamps for verifying ms-level localization seems to be limiting\nThe method is only particularly attractive for longer distances\n",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript is clearly written barring a few typos and grammatical errors here and there. The figures and tables are clear and well captioned but the language could be improved. A webpage or an appendix section with more examples and specific videos would have been a nice edition. In the current form, the manuscript leaves many questions about used videos and associated failure modes open. \nThe work is original and approaches an important broad idea of audio-visual cue integration for inference in the physical world through the use case of depth estimation. It seems to be reproducable if codebases and example videos are shared but those are not present in the current form.\n",
            "summary_of_the_review": "\nThe proposed framework is general and performs better than most of the previous methods. It shows how using audio to augment visual information can be beneficial through the specific example task of depth estimation. That said, the use of audio information is limited to timing information usable only at large distances at the used framerates and needs more in-depth investigation. Overall, the presented work is original and tries to push the baseline on an important estimation task but it has limitations in its current form. The manuscript can greatly benefit from an investigation of distance cues present in auditory information from collisions.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1018/Reviewer_6qDM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1018/Reviewer_6qDM"
        ]
    }
]