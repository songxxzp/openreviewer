[
    {
        "id": "xo3M2NZcQ_",
        "original": null,
        "number": 1,
        "cdate": 1665951911363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665951911363,
        "tmdate": 1665951911363,
        "tddate": null,
        "forum": "JrVIWD81Z0u",
        "replyto": "JrVIWD81Z0u",
        "invitation": "ICLR.cc/2023/Conference/Paper4177/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies federated semi-supervised learning and introduces the FedProp method. FedProp allows the computation of label propagation along an estimate of the data manifold to which the data of all participating clients contribute and use efficient cryptographic primitives to avoid accessing the data from different clients. Experiments on three standard benchmarks show that FedProp achieves promising semi-supervised classification performance. The main contribution lies in using LP to generate pseudo labels for unlabeled samples and applying cryptographic primitives to avoid direct data access \n\n",
            "strength_and_weaknesses": "Strengths\n+ The studied problem is important\n+ Paper is easy to follow \n\nWeaknesses\n- Novelty is limited. \n- No theoretical results.  \n- Lack of  comparison with state-of-the-arts. \n- Evaluation results are insufficient \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow. The paper novelty is limited from my perspective.  The experimental setup is clear, hence I think the results can be reproducible. ",
            "summary_of_the_review": "Detailed comments:\n\nThe main idea to leverage LP to generate pseudo labels is not new.  Also LP is shown to be not effectiveness enough to generate  pseudo labels, if the data distribution does not naturally form a manifold, e.g., when the data samples is limited.  \n\nWhy connecting data form different clients to perform label propagation? In the non-IID settings, the data distribution across different clients could be  significantly different. Is there a need to connect data from different clients to perform LP? What if each client just use its data to perform LP and obtain pseudo labels?  \n\nIs the proposed FedRep convergent? In what convergence rate? What is the risk bound?\n \nNo formal privacy guarantees.   For instance, \u201cfirst, the similarity is not computed between input data itself, but its feature representation according to the current model\u201c. Using indirect feature representations cannot protect the input privacy. Particularly, many existing works has shown that using shared model gradients can reconstruct the data, so does the learnt representations.  \n\nZhu et al. Deep leakage from gradients. In NIPS, 2018\n\n\nThere exist more advance SSL methods (e.g, mixmatch, FixMatch) which can augment data with pseudo labels. Applying mixmatch to the federated SSL setting should be not difficult? What is the federated SSL performance with these data augmentation performance? \n\nSemiFL (NeurIPS 2021) is FedRGD are mentioned, but not compared. Actually, SemiFL has shown to significantly outperform the compared methods, e.g., FedMatch, especially in the non-IID setting. \n\nWhat is the data distributed in CIFAR100 and Mini-Imagenet? IID or non-IID? \n \nWhat is the impact of different number of labeled samples? \n\nWhy connecting data form different clients to perform label propagation, especially in the non-IID setting,  is unclear to me. I suggest the authors should conduct experiments to validate this. \n\nThere exist several federated learning methods for semi-supervised classification, where the client data itself is already a graph. Please discuss them. \n\n\nZhang et al.,  Subgraph Federated Learning with Missing Neighbor Generation. NeurIPS 2021. \n \nXie et al., Federated graph classifica- tion over non-iid graphs. NeurIPS 2021\u2028\nWang et al., Graphfl: A federated learning framework for semi-supervised node classification on graphs. ICDM 2022",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4177/Reviewer_8bDJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4177/Reviewer_8bDJ"
        ]
    },
    {
        "id": "syKvEcXMNI",
        "original": null,
        "number": 2,
        "cdate": 1666276262165,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666276262165,
        "tmdate": 1669028551250,
        "tddate": null,
        "forum": "JrVIWD81Z0u",
        "replyto": "JrVIWD81Z0u",
        "invitation": "ICLR.cc/2023/Conference/Paper4177/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a solution for federated semi-supervised learning. the proposal lies averaging the manifold in the decision space over multiple clients. The sharing is done using two cryptographical protocols. The paper evaluates on three datasets",
            "strength_and_weaknesses": "Pros: \n  - The idea to use manifold for pseudo-labeling has not been used before in federated semi-supervised leaning, although it has been used in classical SSL.\n- the paper has reported improvement in performance over similar federated SSL papers, but is still far from plain SSL performance\n\nCons:\n - the paper claims to used secure and cryptographic protocols for communication. The paper does not really explain why do we need those. I understand that there may be connection problems and communication to be disrupted and thus adding checksums and redundancy will help. I do not understand why using cryptographic protocols help ensuring intimacy of the data for each client since the framework do not assume external digital attacks. Furthermore, while the proposed method uses them, there is no evaluation of their efficiency. For instance how much damage to the communication can be absorbed without changes in performance? The fact that prior art used them, does not help claiming anything here.\n- Evaluation is strong from one perspective but disappointing from another. It is carried on standard benchmarks which is very nice because it allows very good comparisons with previous similar works. It is weak because it does not propose a scenario where it really make sense to have protected data such as face related problems or where to assume that data being separated over clients, there is also a bias (either in data, either in labels) due to independent gathering and annotation. While this may not be fair to this paper, yet this is a major conference, and simply improving inside the niche, after a while becomes less interesting.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and reproducible within reasonable limits given the limited space\n\nNovelty exists but is also limited. The method is novel for federated SSL, but is not novel in the broader and more interesting (in the sense that has a larger auditorium) of general SSL. Federated SSL is more like a niche from SSL and so is its auditorium",
            "summary_of_the_review": "The paper shows improvement over previous federated SSL methods. But I see this topic as a niche and this paper does too little to enlarge the auditorium. The innovation is limited to the topic.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concern ",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4177/Reviewer_jCXS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4177/Reviewer_jCXS"
        ]
    },
    {
        "id": "Wie_LRKDZ4v",
        "original": null,
        "number": 3,
        "cdate": 1666752565122,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666752565122,
        "tmdate": 1666752565122,
        "tddate": null,
        "forum": "JrVIWD81Z0u",
        "replyto": "JrVIWD81Z0u",
        "invitation": "ICLR.cc/2023/Conference/Paper4177/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents  Federated Label Propagation (FedProp), an algorithm, to effectively learn semi-supervised models by distributing both labeled and unlabelled data across multiple clients.  Only model parameters are shared across clients through a server (following a client server architecture) but not the data, which is in fact one of the principles of federated learning. FedProp algorithm takes into account existing tools/techniques such as  Label propagation (LP) and Locality Sensitive Hashing (LSH) as part of its solution. \n \nIn summary, FedProp  first initializes the global solution by learning client-wise independent solutions from corresponding labeled data (that is assigned to a client) and finally running an aggregation step for parameters shared by clients. In addition to the aggregation, the server also acts as an intermediator to share parameters across clients through a broadcasting operation whenever necessary. At each iteration clients are responsible for independently assigning pseudo labels to its portion of unlabeled data and subsequently use those as new/additional labeled data to update its parameters before broadcasting to the server. The above process (pseudo labeling, parameter update and broadcasting and aggregation) iterates for a predefined number of iterations and is expected to converge (at the end). \n\nThe proposed technique has been tested on CIFAR-10, CIFAR-100 and Mini-Imagenet datasets and compared against existing techniques such as FedAvg, FeMatch, FedSem, and FedSiam. Reported results  are found to be comparable. \n",
            "strength_and_weaknesses": "Strengths: Federated Label Propagation (FedProp) uses Label propagation (LP), Locality Sensitive Hashing (LSH), and some other established semi supervised tools/techniques and combines them in a structured fashion to guide semi supervised learning. The experiments and reported results look promising. Also, an analysis of the correctness, efficiency and robustness of their FedProp algorithm has been addressed in section 4.2. \n \nWeakness: The results reported in section 5 don\u2019t include any statistical tests so it's difficult to verify whether the results make any significant difference to existing techniques. The proposed methodology also shows some limitations in terms of scalability (as per the results in section 5): The reported gain in performance reduces when tested for a larger (CIFAR-100 vs CIFAR-10) or a more complex datasets (Mini-Imagement -100).  \n\nAnother weakness/limitation is this work lacks major technical contributions; It uses existing tools and techniques and ties them as its solution.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with minor linguistic errors. The structure is well organized and the content is easy to follow. There are some typos in the headers of Table 1 & 2. \n\nThe work lacks major novelty as it uses some available tools and techniques and combines them  to generate its solution. As the code is not shared it is difficult to reproduce the results\n\n",
            "summary_of_the_review": "I have gone through the paper more than once including the appendices. Overall, the idea is quite simple: combine existing tools and techniques in a smart way that does the job.  Distribute labeled and unlabelled data across clients and let clients learn and share their parameters to a server; the server then performs some aggregation and shares them back to clients. Iterate these back and forth steps until the solution converges. The work lacks major technical contributions although the reported results look promising.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4177/Reviewer_G4t3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4177/Reviewer_G4t3"
        ]
    },
    {
        "id": "VNlwbNINAv",
        "original": null,
        "number": 4,
        "cdate": 1669242331249,
        "mdate": 1669242331249,
        "ddate": null,
        "tcdate": 1669242331249,
        "tmdate": 1669242331249,
        "tddate": null,
        "forum": "JrVIWD81Z0u",
        "replyto": "JrVIWD81Z0u",
        "invitation": "ICLR.cc/2023/Conference/Paper4177/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "-This paper studies the problem of federated learning with semi-supervised client (local) data. They propose a method that first pre-trains the client models with their labeled training data, then performs cross-client label propagation based on the embeddings of the labeled and unlabeled training data obtained from the pre-trained models, and after obtaining the pseudo labels and per-sample weights standard federated training is applied. They also provide some experimental results demonstrating the effectiveness of this method.",
            "strength_and_weaknesses": "Pros\n\n-The writing is clear and the paper is easy-to-follow. The authors attempt to solve a practically important question and show some favorable empirical results.\n\nCons\n\n-There are several issues that remain unsolved in the paper. The first concern is about novelty. To utilize the semi-supervised training data, the proposed method simply (in terms of methodology) employs the widely used label propagation in federated training. Although in practice, in order not to violate the federated training protocols (do not share data between clients) by using cross-client data information (embeddings), the authors introduce some cryptographic tools which are claimed to be fully secure, but they do not provide any privacy guarantees.\n\n-The second concern is about feasibility. Label propagation is not free. We may not accurately estimate the data manifold with only limited non-iid data and thus label propagation may not be reliable. The authors propose to solve this problem by doing so on embeddings of the training data from all clients. However, we still cannot verify the manifold assumption in this case and can only observe from the empirical performances. This raises another concern about the convergence of the proposed method (if the label propagation is not reliable). Existing experimental results are based on benchmark datasets and the semi-supervised data are artificially designed. It would be useful to test the proposed method with real-world non-iid semi-supervised client data and see if label propagation works well.\n",
            "clarity,_quality,_novelty_and_reproducibility": "-The paper is clearly written and easy to follow. There are some concerns regarding the novelty and feasibility, see above.",
            "summary_of_the_review": "-From the reviewer's point of view, this paper provides some interesting empirical results but more theoretical investigations are needed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4177/Reviewer_ZBdp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4177/Reviewer_ZBdp"
        ]
    }
]