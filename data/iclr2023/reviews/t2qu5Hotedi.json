[
    {
        "id": "QyIKlD5G1Nm",
        "original": null,
        "number": 2,
        "cdate": 1666688128363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688128363,
        "tmdate": 1666688128363,
        "tddate": null,
        "forum": "t2qu5Hotedi",
        "replyto": "t2qu5Hotedi",
        "invitation": "ICLR.cc/2023/Conference/Paper4597/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The manuscript proposes a probabilistic framework for modeling the underlying distribution of prompts, which acts as an additive module on top of existing prompt-learning approaches, and seeks to enable improved domain generalization.",
            "strength_and_weaknesses": "(Strengths)\n\nThe manuscript proposes a simple, yet principled approach.\n\nThe manuscript is well-written and easy to follow.\n\nThe manuscript provides numerous experiments, against several strong baselines.\n\n(Weaknesses)\n\nSection 4. The phrase 'Domain Generalization' is slightly misleading, as the section seeks to highlight properties of unseen generalization through zero-shot transfer. The discussion could either be reframed under 'Cross-domain Generalization'.\n\nSection 4. I would like to see further analyses/illustrations of the prompt distribution obtained by VPT. Do prompts that are sampled at the distributional modes correlate with features that well-characterize any known factors of variation (e.g., sub-domains) in the dataset(s)? What further evidence can be provided to show that the prompt distribution approaches reasonable coverage of the underlying distribution? Can equivalent results be obtained through other forms of density estimation?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the manuscript is well-written and the majority of claims are well-justified\n\nQuality/Novelty: provides new problem formulation, compared to existing works, albeit somewhat iterative\n\nReproducibility: no issues",
            "summary_of_the_review": "Missing some intuition regarding the prompt distribution, beyond empirical results",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4597/Reviewer_mkLi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4597/Reviewer_mkLi"
        ]
    },
    {
        "id": "EZfMc_CaC4E",
        "original": null,
        "number": 3,
        "cdate": 1666690879763,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690879763,
        "tmdate": 1666690879763,
        "tddate": null,
        "forum": "t2qu5Hotedi",
        "replyto": "t2qu5Hotedi",
        "invitation": "ICLR.cc/2023/Conference/Paper4597/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes an instance-specific variational prompt-tuning framework for image-language downstream tasks. Here, the experiments are performed with a frozen CLIP model while fine-tuning the parameters used for prompt creation. It combines two ideas from existing publications on prompt-tuning with learnable prompt embeddings:\n\n1. Adding a residual vector derived from input image features to the learnable prompt embeddings\n2. A variational framework for prompt tuning, which this work implements by assuming a Gaussian distribution on residual vectors that are then added to the prompt embeddings.\n\nImplementation of this framework is discussed in both the conditional (instance-specific) setting, as described above, and the unconditional setting of the previous work CoOpOp. Evaluation is performed on 15 image recognition and classification datasets via experiments on the tasks of base-to-new generalization, cross-dataset transfer, and domain generalization.",
            "strength_and_weaknesses": "Pros\n\n- Increase in generalization performance against CoCoOp and ProDA is shown empirically for most datasets, with state-of-the-art results in prompt learning for image recognition tasks with generalization requirements.\n- Either of the two mechanisms described - instance specific (conditional prompt learning) and instance independent (standard prompt learning), can be implemented as a simple change to the existing methods with a boost in generalization performance.\n- This boost enables higher-performance for few-shot and zero-shot learning for downstream image-text tasks, possibly increasing resilience against distribution shift and adversarial attacks.\n- Ablation studies provide evidence of the usefulness of the proposed mechanism.\n\nCons\n\n- While the proposed model is simple to implement and integrate with existing systems, and has shown improved performance, especially for usage with different domains, there is a concern of lack of enough novelty since the ideas of residual vector and variational prompt learning were already introduced in previous publications.\n- The increase in generalization performance comes at a cost to performance of in-domain/seen samples.\n- A minor con is that prompt initialization is still manually specified.\n\nMinor Edits\n\n- More details can be specified about implementation in the zero-shot learning scenarios.\n- More clear details can be added about the implementation of this method in the unconditional setting of CoOp.\n- Can discuss ensembling interpretation of sampling",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was well written and was easy to follow overall. More details about zero-shot learning and unconditional setting scenarios would have been useful.\n\nThe major ideas used here have already been proposed in existing works, though their adaption and combination is done with simplicity of implementation in mind.\n\nThe authors did mention that the code will be released.",
            "summary_of_the_review": "Borderline accept. The paper presents useful results about the implementation of simple changes to existing systems and the resultant performance improvement in certain scenarios. However, since existing ideas are heavily used and combined, there is an overall lack of novelty, so I would hesitate from a strong accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4597/Reviewer_SMRu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4597/Reviewer_SMRu"
        ]
    },
    {
        "id": "63MDm2rMJB",
        "original": null,
        "number": 4,
        "cdate": 1666731334783,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731334783,
        "tmdate": 1669664550843,
        "tddate": null,
        "forum": "t2qu5Hotedi",
        "replyto": "t2qu5Hotedi",
        "invitation": "ICLR.cc/2023/Conference/Paper4597/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Summary: \nThe authors propose a variational prompt generator conditioned on an input instance. This is learned by adding a residual (which is input conditioned) to a fixed set of learnt prompts. During inference, multiple residuals can be sampled to generate different prompts. The authors show that the proposed approach surpasses existing methods on 16/20 datasets for classification\n",
            "strength_and_weaknesses": "\nStrengths:\n- The method is intuitive and show strong empirical results on a bunch of tasks. The ablations are reasonable too! \nThe model maintains generalization capability for both new  and old classes. \n\nWeaknesses:\n- The paper combines two existing ideas in prompt learning (the residual addition, and the variational formulation). This limits the technical novelty of the paper a little bit. \n- The whole motivation of the paper is about preserving the generalization capability for downstream tasks, yet we see that the model perfoms worse on base classes than existing methods.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and the experiments are easy to follow. The paper has limited technical novelty because it combines two existing ideas of residual connection and variational formulation of prompt learning",
            "summary_of_the_review": "I think the paper has merits, but technical novelty is limited. I am on the fence about the paper because it has strong empirical results but the approach combines existing ideas in unsurprising ways. I will look at the author response and read other reviews to decide my final rating.\n\nUpdate after rebuttal: I think the authors took effort in addressing all my concern and most of other revierwer's concerns during the rebuttal. I am still on the fence due to limited novelty and tradeoff in performance between base and new classes.  But I think the new analysis makes the paper stronger. To reflect this I am updating my score to 6 (marginally above acceptance threshold). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4597/Reviewer_iM4z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4597/Reviewer_iM4z"
        ]
    },
    {
        "id": "XkRLPiRDoe",
        "original": null,
        "number": 5,
        "cdate": 1667186476841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667186476841,
        "tmdate": 1667186476841,
        "tddate": null,
        "forum": "t2qu5Hotedi",
        "replyto": "t2qu5Hotedi",
        "invitation": "ICLR.cc/2023/Conference/Paper4597/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed to leverage variational inference to learn the underlying distribution of the prompts with or without image feature as the condition. The effectiveness of the proposed method is verified on several benchmarks.",
            "strength_and_weaknesses": "Pros:\n1. The proposed idea is intuitive and straightforward.\n2. The improvement is shown by combing the proposed method with two popular methods for visual prompt tuning.\n\nCons:\n1. It is unclear why it is needed to learn the distribution of prompts. Unlike where variational inference was usually used in computer vision, e.g., VAE, the possible distribution of the prompts is actually not that untractable. \na. Simple baselines like ensemble learning the soft prompt from different ways of initialization should be provided. It is possible that when ensembling several prompts learned from different initializations, the performance of the model is already somewhat achieved. Because the possible way of introducing an image in caption is rather limited. Probably a few main ways of the prompt initialization can already cover.\nb. Any interpretation of the learned/sampled prompts could be provided? This may help to understand why the learning process is necessary.\nc. Although the authors claim on the improvement of generalization to new classes, the performance gain on cross-dataset setting is still quite limited. Therefore, the model may still learns pretty domain specific distribution of prompts.\n\n2. In Table1, more than half pf the results sacrifice the base class accuracy for improvement of the average. Therefore, it is also important to provide simple baselines like early stopping or adding some common regularization terms on the prompts.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, quality and reproducibility look overall good. The novelty is rather limited.",
            "summary_of_the_review": "Overall, the reviewer thinks this paper solves an incremental problem following a rather standard approach without providing in-depth analysis and insights on why solving this problem.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4597/Reviewer_ANZ7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4597/Reviewer_ANZ7"
        ]
    }
]