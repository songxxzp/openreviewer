[
    {
        "id": "fN6bsIZ_XP",
        "original": null,
        "number": 1,
        "cdate": 1666275962380,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666275962380,
        "tmdate": 1666275962380,
        "tddate": null,
        "forum": "cOOQruYU7Bh",
        "replyto": "cOOQruYU7Bh",
        "invitation": "ICLR.cc/2023/Conference/Paper5772/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed SONew (Sparsified Online Newton), which is an efficient variant of online Newton step (ONS) by using $b$-banded sparsity graph. The time complexity of this method is $O(b^3 n)$, and the regret bound is $O(T^{3/4})$. The authors provide some simple experiments to verify the performance of the proposed method.",
            "strength_and_weaknesses": "#Strength\n1) The authors proposed a new efficient variant of online Newton step (ONS) by using $b$-banded sparsity graph.\n2) The proposed algorithm improves the time complexity of ONS from $O(n^2)$ to $O(b^3 n)$.\n\n#Weaknesses\n1) The authors only established a regret bound of $O(T^{3/4})$ for their method, which is far worse than the $O(\\sqrt{T})$ regret bound of many existing first-order methods such as online gradient descent. Note that these first-order methods are more efficient than the proposed algorithm for $b>1$.\n2) The experiments presented in this paper is too simple, which is not sufficient to verify the advantage of the proposed method. The authors at least consider some other datasets such as CIFAR10 and CIFAR100. Moreover, the authors should also consider other deep neural networks such as convolutional neural networks (CNNs).\n3) In the experiments, some tricks including momentum and grafting are added to the proposed method, which may affect the evaluation of the proposed method.\n4) There actually exist efficient variants of ONS [1][2], which enjoy the time complexity of $O(\\tau n)$ for very small $\\tau$. However, the authors do not discuss and compare the proposed method with them.\n\n[1] Haipeng Luo et al. Efficient Second Order Online Learning via Sketching. In NeurIPS 2016.\n\n[2] Luo Luo et al. Robust Frequent Directions with Application in Online Learning. In JMLR 2019.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. However, the authors missed some very related work [1] [2]. Moreover, although the use of $b$-banded sparsity graph to accelerate ONS is new to me, I am not convinced that it is better than existing efficient variants of ONS.\n\n[1] Haipeng Luo et al. Efficient Second Order Online Learning via Sketching. In NeurIPS 2016.\n\n[2] Luo Luo et al. Robust Frequent Directions with Application in Online Learning. In JMLR 2019.\n",
            "summary_of_the_review": "By considering the $O(T^{3/4})$ regret bound of the proposed method and the existing efficient variants of ONS, I tend to reject this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5772/Reviewer_9vTP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5772/Reviewer_9vTP"
        ]
    },
    {
        "id": "-0vPYhNpgk",
        "original": null,
        "number": 2,
        "cdate": 1666617943234,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617943234,
        "tmdate": 1666617943234,
        "tddate": null,
        "forum": "cOOQruYU7Bh",
        "replyto": "cOOQruYU7Bh",
        "invitation": "ICLR.cc/2023/Conference/Paper5772/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes SONew, an online convex optimizer derived by combining the regret minimization based on the LogDet divergence measure and structured sparsity in the preconditioning matrix. The preconditioning matrix of the SONew is a shape of either diagonal, tri-diagonal, or banded and is calculated in linear flops and numerically stable ways, making it compatible with large deep neural networks. On the MNIST autoencoder training task, SONew achieved better convergence than the existing first-order adaptive gradient method. Furthermore, SONew shows better numerical stability than other methods, including the Shampoo optimizer, when training with bfloat16.\n",
            "strength_and_weaknesses": "Strengths\n- This paper is clearly written.\n- This paper highlights an interesting connection between the regret minimization in OCO and LogDet divergence of the preconditioning matrix, giving a novel insight into the online gradient descent and online Newton method.\n- By taking advantage of structural sparsity in the preconditioning matrix, the proposed SONew method gives a way to solve the LogDet divergence subproblem fast enough (enough to be applied every step during training). It seems practical even with a large DNN model. \n- Also, thanks to the flexibility of the sparse structure in SONew, the form of the preconditioning matrix (diag, tri-diag, banded) can be determined according to the computational budget, and a straightforward trade-off between convergence speed and computation time is observed in the auto-encoder training experiments in MNIST.\n\nAlthough SONew is based on solid mathematical intuition and the experimental results show good convergence and numerical stability, the empirical results are not strong enough to support SONew\u2019s effectiveness as a \u2018deep learning optimizer\u2019, as described below.\n\nWeaknesses\n- Comparisons at different mini-batch sizes have not been made.\n    - In mini-batch training, g_t in Equation 5 corresponds to the mini-batch gradient, and X_t^{-1} is known as the (mini-batch) empirical Fisher. It has been pointed out that the mini-batch empirical Fisher loses valuable second-order information for optimization as the mini-batch size increases (Equation 4 in https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L05_normalization.pdf).\n    - Therefore, comparisons with different (larger or smaller) mini-batch sizes are desirable, as differences in mini-batch size are expected to affect the convergence of SONew (and the optimization methods being compared). In particular, since large batches are desirable for large-scale training that takes advantage of data parallelism and distributed accelerators (e.g., https://arxiv.org/abs/1802.09941), how well SONew maintains convergence in large-batch training is useful information for practical (esp. large-scale) uses.\n- Comparison of numerical stability needs improvement.\n    - The numerical stability of the eigenvalue decomposition required by Shampoo depends largely on the condition number of the matrix, which can be significantly improved by the damping value to be added to the diagonal of the matrix. For example, in the experimental code of Anil et al. (2020) (https://arxiv.org/abs/2002.09018), the default value of damping is set to 1e-4 (https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/shampoo.py#L50) to improve the numerical stability. Similarly, large damping values are often used in K-FAC in practice (e.g., 1e-3 in https://github.com/gpauloski/kfac-pytorch/blob/main/kfac/preconditioner.py#L57). Therefore, it is desirable to use Shampoo with a large damping value as a baseline result in the bfloat16 setting.\n    - K-FAC can also be implemented using Cholesky decomposition. In addition, there are second-order optimization methods that can take advantage of the low-rank nature of matrices and avoid explicit inverse matrix and eigenvalue decomposition calculations (e.g., SKFAC https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.pdf, SENG https://link.springer.com/article/10.1007/s10915-022-01911-x). Since these methods can be expected to have more robust numerical stability in low-precision training, using only Shampoo (which is less numerically stable) as a baseline for low-precision second-order optimization seems inappropriate for evaluating the relative numerical stability of SONew. \n- It is unclear whether SONew is effective as a \u2018deep learning optimizer\u2019.\n    - From the derivation, SONew is an online 'convex' optimizer. However, since the loss function in deep learning is nonconvex, it is unclear how effective regret minimization achieved by SONew is in minimizing loss. Since the MNIST autoencoder training is a very simple task by today's deep learning standards, training results in a larger, more realistic setting (e.g., training CNNs on ImageNet classification, Transformers on language tasks with cross-entropy loss) would be desirable.\n    - Although only training loss is shown in the experiment, I believe that the evaluation of generalization performance is necessary to measure the performance of SONew as a deep learning optimizer. For example, not only the training convergence speed of the optimizer but also its compatibility with regularization, such as weight decay, is an essential subject of discussion (e.g., Adam vs. AdamW https://arxiv.org/abs/1711.05101).\n\nQuestions\n- Why is there no result for \u201cband-4\u201d with bfloat16 in Figure1 and Table2?\n- What are the damping values (espsilon) used for SONew, Shampoo, and other adaptive gradient methods?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper clearly describes the proposed method's background, motivation, derivation, and significance. It provides a novel combination of LogDet divergence measure and structured sparsity in preconditioning matrix in online convex optimization settings. The quality of the empirical experiments needs to be improved to assess whether SONew is effective in more realistic deep-learning tasks than a simple autoencoder training task.\n",
            "summary_of_the_review": "This work proposes a compute- and memory-efficient online convex optimizer for deep neural networks based on solid mathematical insight. However, it is not clear from the given experimental results and theoretical justification alone that the proposed method is helpful in \u201cLearning Representation.\u201d I think more empirical results and discussion of non-convex optimization/generalization would improve the quality of this research, but it is currently the under the bar of acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5772/Reviewer_pCLb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5772/Reviewer_pCLb"
        ]
    },
    {
        "id": "VKFlILNe-7",
        "original": null,
        "number": 3,
        "cdate": 1666704666799,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704666799,
        "tmdate": 1666704666799,
        "tddate": null,
        "forum": "cOOQruYU7Bh",
        "replyto": "cOOQruYU7Bh",
        "invitation": "ICLR.cc/2023/Conference/Paper5772/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes sparsifed online Newton method.\n",
            "strength_and_weaknesses": "Pros: It is easy to follow and the intuition of the algorithm is easy to understand.\n\nCons: Page 4, between Eq. 8 and Eq. 9, the optimality condition of Eq. 7 is not the gradient equal to zero because Eq. 7 is a constrained optimization problem. Thus, the derivation of Eq. 9 may be problematic.\n\nThis paper only considers the third term of Eq.3. This may be not enough because $X_t ^{-1}-X_{t-1}^{-1} $ which makes the regret large.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to follow but seems lacking novelty.",
            "summary_of_the_review": "This paper proposes sparsifed online Newton method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5772/Reviewer_kEgN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5772/Reviewer_kEgN"
        ]
    }
]