[
    {
        "id": "R3EEEXw7waK",
        "original": null,
        "number": 1,
        "cdate": 1666593082591,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593082591,
        "tmdate": 1666593082591,
        "tddate": null,
        "forum": "9yE2xEj0BH7",
        "replyto": "9yE2xEj0BH7",
        "invitation": "ICLR.cc/2023/Conference/Paper5558/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduced a novel vision-language multi modality model (Spotlight) for mobile UI understanding tasks. Specifically this model is designed for tasks for mapping visual components in the UI to natural language text. In the context of UI understanding models, this paper presents 2 major contributions.\n\n1. The authors challenge the recent belief that view-hierarchy based UI understanding model is a hands-down choice for accuracy. They proposed the Spotlight model which only uses visual information at inference time; this can potentially lead to higher accuracy by working around the typical incomplete hierarchical view information for prediction.\n\n2. The authors developed a pretrained model for the UI understanding tasks. They present SoTA accuracy on UI understanding tasks under single-task finetuning, single-task few shot and multi-task finetuning. They show that the pretraining is crucial to the high accuracy in widget captioning, screen summarization, command grounding tasks.",
            "strength_and_weaknesses": "Strength\n\n1. The technical effort is solid. The creation of the pretrained model involves the collection and curation of millions of UI screen shots. Such endeavor combined with the novel pretraining objective for UI understanding task enables doubling the CIDEr score from no pretraining efforts on the same model.\n\n2. The empirical results is impressive. It attains SoTA accuracy on the 3 tasks with only visual info input during inference time.\n\nWeakness\n\nThe paper is a bit hard to follow and needs modification to enable a self-containing manuscript for readers to understand.\n\n1. There are technical concept/facts missing their intuitive or formal definitions in the related work / background sections. Representative missing information are\n     a. What is an intuitive definition of view-hierarchy information which is the primary input to existing models. \n     b. How severe is the information missing problem in view-hierarchy information? Could you slightly quantify it w.r.t. the datasets the author used or collected? This is important to evaluate the claim on the problem of previous view-hierarchy info based models. \n\n2. The model design is a bit hard to fully understanding with pure text. Overall, I would recommend a symbol based formal model description which could answer the following questions.\n    a. In listing 1, it seems the Dense function encodes the coordinate into vectors. Is this encoding the normal way in VIT which just maps the coordinates to the 1D index of the image patch in the patch sequence?\n    b. Also why the coordinate is encoded twice via both the Dense function and the SinusoidalEmbed function?\n    c. What is the training objective? My rough guess from the relevant description in 4.1 is that it use the self contrastive loss type thing to promote the patch and text description pairs. But it is very unclear to me if this is the case.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: As discussed in the Strength and Weakness, I would highly recommend heavy re-writing to ensure the best possible impact of the great technical results. Currently it is hard to follow the story and understand some of the technical details.\n\nQuality: The proposed model demonstrated substantial accuracy improvement over previous models. The ablation is solid in showing key contributing factos such as region summarizer and pretraining. I consider the experiment quality as high.\n\nNovelty: Given the authors challenged a common believe on view-hierarchy info based model is a hands-down choice for good accuracy, and demonstrate SOTA accuracy using visual input only model. I would rate the novelty as high.",
            "summary_of_the_review": "Overall I vote for a marginally-below-acceptance. I think the empirical results are overall great and the visual input only model is a novel design over the conventional view-hierarchy based models [Based on my limited understanding of UI view-hierarchy]. However I think the writing quality is below the bar for top tier conference submissions. Disclaimer: :) I am experienced in developing large language model and multi modality models. But I am not an expert in the UI understanding domain.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5558/Reviewer_WFS1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5558/Reviewer_WFS1"
        ]
    },
    {
        "id": "_ZFoliePeGu",
        "original": null,
        "number": 2,
        "cdate": 1666613536811,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613536811,
        "tmdate": 1666613536811,
        "tddate": null,
        "forum": "9yE2xEj0BH7",
        "replyto": "9yE2xEj0BH7",
        "invitation": "ICLR.cc/2023/Conference/Paper5558/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "For the mobile UI understanding tasks, the authors achieve advanced performance on multi-task learning and few-shot learning by pretraining the vision-language model on the proposed 2.69M dataset. This pretraining-finetune framework is easily scalable to other UI modeling tasks and not needs the view hierarchy as auxiliary information. Meanwhile, the authors proposed a pipeline for automatically creating the large-scale UI understanding dataset.",
            "strength_and_weaknesses": "Strength:\nThe authors attempted to study UI understanding tasks by multi-task learning with pretrained multi-modal models, which is an independent, innovative and laborious work. The experiment results prove the generalization and validity of this method. Meanwhile, The author also proposes a large-scale UI modeling pretraining dataset. \n\nWeakness:\n1. Although the motivation is innovative in constructing the pretraining model for the UI modeling field, the overall pretraining pipeline may lack appropriate innovation and some aspects are similar to Flamingo, e.g., the vision-language model architecture, the evaluation method on the multi-task and few-shot learning, and the computational pipeline of Region Summarizer is similar to the Perceiver Resampler of Flamingo. \n2. The illustration of the pretraining dataset is too brief. It is recommended to add more statistical analysis and construction details, e.g., The text and content description are human-annotated or not?\n3. Considering the readability, it is recommended to add the introduction of test datasets about downstream tasks. Perhaps the authors can add these in the supplementary material rather than suggesting the readers read the previous literature.\n4. The ablation study is limited and maybe authors can consider the following ablation to make the paper clear and complete.\n(1)\tRegion Summarizer:\n\u2460\tWhy choose the bbox coordinates as q rather than the region feature of bbox?\n\u2461\tWhat is the result when kv is just the vit_outputs rather than the concatenation of vit-outputs and bbox.\n(2)\tPretraining:\n\u2460\tCan the text input is concatenated by the four text elements of an object?\n\u2461\tThe ViT is freezing as Flamingo or not?  If not, The much screenshots of examples will bring the burden of GPU memory or not?   \n5. The 2.69M pre-training data as extra knowledge may obscure the fairness of the method compared with the baseline method. Maybe author can try to further finetune the comparison methods combined with single-modal encoders of the pretraining model? This is just a suggestion out of curiosity and not influence the final judgment.\n6. In the Discussion section, these \u201cearly exploration\u201d are supposed to illustrate hypotheses by some experimental data. \n7. The paper of the comparison method \u201cWidget caption\u201d is cited repeatedly.\n8. Maybe the authors can consider making the dataset and code public to promote the development of this field. It is just advice and not influence the final judgment.\n",
            "clarity,_quality,_novelty_and_reproducibility": "It is a original work but some details of paper are ambiguous.",
            "summary_of_the_review": "It is recommended to add some details and ablation study in paper. Moreover, since there is very little research in this field, I encourage the authors to open source the pretraining dataset,model checkpoints, and code, which will promote the development of this field and attract more researchers to make the field more prosperous.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5558/Reviewer_cKCz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5558/Reviewer_cKCz"
        ]
    },
    {
        "id": "h-oNi1x3Np-",
        "original": null,
        "number": 3,
        "cdate": 1666617760793,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617760793,
        "tmdate": 1666617760793,
        "tddate": null,
        "forum": "9yE2xEj0BH7",
        "replyto": "9yE2xEj0BH7",
        "invitation": "ICLR.cc/2023/Conference/Paper5558/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a model that incorporates bounding box queries in the visual encoder to extract region-specific features for UI modeling tasks. With the proposed framework, no view hierarchy would be required during the above modeling process, which thus alleviates the potential concerns of missing object text or bounding box misalignment.\n",
            "strength_and_weaknesses": "Strength\n1. The idea of incorporating bounding box queries to obtain region-specific features provides a unique way to extract fine-grained features.\n2. Compared with existing approaches, impressive performance on finetuning experiments for three mobile UI understanding tasks were achieved.\n\nWeaknesses:\n1. Overall, the novelty of this work is limited. The model architecture is similar to Flamingo (Alayrac et al., 2022). The only difference is that the inputs of Flamingo are visual tokens and learned latent queries, while SPOTLIGHT takes visual tokens and bounding box queries.\n2. The main idea of this work is that, the bounding box queries have capacities to learn how to represent specific image regions, while jointly taking into account of the screen context. However, there is no further analysis, explanation, or qualitative/quantitative supports for this claim.\n3. The experiment comparisons are potentially unfair. The improved results reported by SPOTLIGHT might come from large-scale pretraining, overwhelming parameters, and the uses of more powerful models (i.e., ViT and T5). All the baseline models (e.g. Widget Caption (Li et al., 2020b) and Screen2Words (Wang et al., 2021)) are composed of ResNet and vanilla Transformer with 128 dimensions. \n4. Missing comparisons to SOTAs in multi-task (Table 4) and few-shot learning tasks (Table 5).\n5. In Section1, the paper states its second contribution: \u201cwe develop a method for creating a large-scale pretraining dataset from automatically collected mobile screens.\u201d However, this newly dataset is not discussed or presented later.\n6. In Sect. 5.3, the authors note that SPOTLIGHT outperforms previous models. However, in Table 4, there are no other experiments to support this claim.\n\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the comments above.",
            "summary_of_the_review": "This paper proposes a model that incorporates bounding box queries in the visual encoder to extract region-specific features for UI modeling tasks. With the proposed framework, no view hierarchy would be required during the above modeling process, which thus alleviates the potential concerns of missing object text or bounding box misalignment. However, the paper is not well presented (with insufficient technical contributions and details, and missing or over-claimed experimental results). I would reconsider my ratings if the above issues can be properly addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5558/Reviewer_V2L4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5558/Reviewer_V2L4"
        ]
    }
]