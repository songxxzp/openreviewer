[
    {
        "id": "bkr8q9Ezob",
        "original": null,
        "number": 1,
        "cdate": 1665921286459,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665921286459,
        "tmdate": 1665921714629,
        "tddate": null,
        "forum": "vDybC2brXKh",
        "replyto": "vDybC2brXKh",
        "invitation": "ICLR.cc/2023/Conference/Paper4652/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies a regularization that can improve the learning of Q-values when used in combination with deep learning. The paper shows that these regularization techniques can cause inconsistencies and overestimations in the state-action value functions while vanilla trained deep reinforcement learning policies have more accurate estimates for the Q-values.",
            "strength_and_weaknesses": "Strengths:\n- This is an important area of study\n- The paper highlights some problems that were not fully investigated in the past\n\nWeaknesses\n- The contributions are not very strong in the current state of the paper (see below).",
            "clarity,_quality,_novelty_and_reproducibility": "CLARITY\n\nThe paper could be written more formally and has also some (minor) specific elements that are not accurate, for instance:\n- The paper write \"Our results essentially demonstrate that vanilla trained deep reinforcement learning policies have more accurate and consistent estimates for the state-action values\". I don't believe that the paper actually demonstrates this. It could be rephrased as an empirical evaluation.\n- \"our investigation lays out intrinsic properties (...) to building robust and optimal deep neural policies.\": In deep RL we usually do not expect that the policies will actually be optimal.\n- There are many reference to \"Example 3\", however I do not find any explicit mention of what that example 3 (we can guess that it is the one from the top of page 4)\n\n\nQUALITY\n\n- Proposition 3.1 and 3.2: There are interesting insights from these propositions but as they are written, it seems that they lack any kind of generality and that they only provide information about a given example. It might be more useful to rewrite them in a more general form (e.g. something along the lines of \"if a function approximator of a given type is used, then the regularizer $R(\\theta)$ can lead to both overestimation of the first ranked action, and re-ordering of the ranking of the suboptimal actions\"). For some propositions, an example could then sufficient as a proof but the propositions would then be more relevant.\n- Most experiments are provided with limited relevant baselines and on three environments.\n- The appendix mentions a discount factor $\\gamma$ of 1. That seems a strange choice as it is known that a discount factor of 1 will in many cases create instabilities and divergence, particularly when used with deep learning as a function approximator and long horizons.\n\nNOVELTY\n\nThe paper provides interesting insights for a relevant problem in RL, however the novelty is a bit unclear in terms of contributions. Two key elements that are highlighted in the conclusion are \n- \"adversarially trained deep neural policies in certain MDPs completely loses all the information in the state-action value function that contains the relative ranking of the actions\" and\n- \"we show that state-of-the-art adversarially trained deep neural policies learn overestimated state-action values\"\n\nThese are in fact common problems observed for RL with function approximators and it is unclear how this paper provides clear insight on whether this is generally worse/better because of the relatively limited set of experiments and of the relatively narrow theory that is developed.\n\n\nREPRODUCIBILITY:\nThe source code is not provided. The papers does provide some of the experiments in the appendix.",
            "summary_of_the_review": "The paper looks into an interesting problem and has initial good contributions. There are however elements that need to be improved both on the form and on the substance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4652/Reviewer_JcRE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4652/Reviewer_JcRE"
        ]
    },
    {
        "id": "3H0LRkWisdJ",
        "original": null,
        "number": 2,
        "cdate": 1666641189772,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641189772,
        "tmdate": 1666641189772,
        "tddate": null,
        "forum": "vDybC2brXKh",
        "replyto": "vDybC2brXKh",
        "invitation": "ICLR.cc/2023/Conference/Paper4652/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates that adversarially trained deep neural policies can potentially lead to inconsistencies and overestimations in the state-action value functions. A linear case theoretical analysis and empirical studies on Arcade Learning Environment are provided to verify the observation. ",
            "strength_and_weaknesses": "Strengths: \n1. The observation, that vanilla trained deep neural policies have more accurate and consistent estimates for the state-action values than the state-of-the-art adversarially trained deep neural policies, is counterfactual and interesting. \n\nWeaknesses:\n1. The theoretical analysis doesn\u2019t support much about the empirical study. The analysis assumed that action-value function is linear and as simple as an inner product, which can not be extended to the cases of empirical studies. \n2. Lack of contribution. Demonstrating the problem without providing potential solutions may not quite yet reach the high bar set for ICLR. \n3. The main claim needs more investigation or explanations.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Good clarity. The novelty is limited. Code is not provided and implementation details are missed. Hard for reproducibility. ",
            "summary_of_the_review": "Overall, the depth of this contribution may not quite yet reach the high bar set for ICLR. The main claim, that vanilla trained deep neural policies have more accurate and consistent estimates for the state-action values than the state-of-the-art adversarially trained deep neural policies, needs more investigations. \n\nThe paper defines a performance drop metric as a measure of the accuracy of state-action values. \nThe authors pick a random state $s$ during a rollout, make the agent choose action $a_i$ in state $s$, and in all other states have the agent choose the best Q-value actions. The performance drop due to the action modification is measured. The authors claim that adversarially training would make the performance drop large and hence is not accurate. \n\nHowever, all the experiments are done on discrete action spaces, and I would expect there would be a big change in performance due to action modifications, for example, turning right in a maze changed to turning left would impact a lot for finding the exit. I think the performance drop due to action modification could be reasonably large and large performance drop actually shows that the Q-value function could discriminate good actions from bad actions well, which is the target of adversarial learning. \n\nI also kind of disagree with the overestimation claim. Table 2 doesn\u2019t show the true average Q-values, so that we don\u2019t know which one is overestimated. To me, the value scale (<10) is still under control. The target of Q-value function is to help the learning of policies, so as long as the good action regions are assigned higher and reasonable Q values, Q function could provide good learning signals for the policy. \n\nImplementation details are not discussed, and the work is not reproducible. The paper only mentions using state-of-the-art adversarially trained deep neural policies, while not includes the actually used algorithm. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4652/Reviewer_AQqn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4652/Reviewer_AQqn"
        ]
    },
    {
        "id": "nhFtSuD9aM",
        "original": null,
        "number": 3,
        "cdate": 1666663996845,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663996845,
        "tmdate": 1666663996845,
        "tddate": null,
        "forum": "vDybC2brXKh",
        "replyto": "vDybC2brXKh",
        "invitation": "ICLR.cc/2023/Conference/Paper4652/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the effect of adversarial training on the action-value estimates of value-based deep RL agents. The paper considers both a toy learning problem where the optimal function approximator with and without adversarial regularization can be computed analytically, as well as networks trained on the arcade learning environment. It shows that adversarial training reduces the accuracy of the network\u2019s ranking over actions at a given environment state, that it increases the gap between the action with the highest predicted value and the value of the second-best action, and that it increases the value of the action with the highest predicted value.\n",
            "strength_and_weaknesses": "Strengths:\n\n- The paper offers some interesting insights on the trade-offs between adversarial robustness and accuracy of the action-value function in value-based deep RL.\n- The toy example in Proposition 3.2 is a nice illustration of the instability of action rankings in adversarially robust agents.\n- The discussion of the adversarial robustness literature seems reasonably comprehensive (though this isn\u2019t my primary field).\n- The metric used to estimate action-value accuracy is a smart way of trying to marginalize out noise in the agent trajectories in order to compare the action rankings.\n- The finding of section 3 is somewhat surprising \u2014 I would not have anticipated such a large gap between the actions of the second-best and worst predicted action.\n\n\nWeaknesses\n\n- While the literature review discusses the adversarial RL literature, it does not cover many recent works studying the robustness of the action-value ranking of deep RL agents to non-adversarial perturbations. In particular, prior work [1] has shown that even gradient steps computed on disjoint subsets of the replay buffer can change the agent\u2019s ranking over actions in a surprisingly large fraction of the state space. In the context of this prior work, the observation that the ranking of sub-optimal actions changes under adversarial regularization is unsurprising.\n- It is widely observed that DQN outputs tend to systematically underestimate the true value of a state. As a result, it seems to me difficult to say whether the values output by the adversarially trained networks are indeed over-estimations of the true value, or whether they are simply accurate estimations of the true action-value function. I am therefore unconvinced as to whether the subsection\u2019s title is necessarily accurate, and would like to see a comparison between the action-values and the true discounted return obtained by the policy to be convinced of this.\n- The description of the adversarial regularizer is inconsistent with that described by Zhang et al. in their presentation of SA-DQN. Zhang et al. include a clipping hyperparameter $c$ which lower bounds the adversarial action gap and avoids the pathology of the value estimate of the optimal action growing arbitrarily large. I am curious about the value of $c$ used in this paper.\n- Similarly, the value of the regularizer coefficient $\\kappa$ is only briefly mentioned in the final  sentence of the supplementary material, and it is said to be in a set of values. However, I could not determine which value was used for the paper\u2019s experiments, nor could I find any results describing the relationship between the phenomena described in the main body and the values of $\\kappa$ and $c$. The epsilon value used for epsilon-greedy exploration is quite low (0.02). These values will likely play a large role in determining effect size of the phenomena described in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The main message of the paper is clearly conveyed, but many important experiment details are not clear and I do not believe I could reproduce the experimental results based on the description in the appendix. Further, the description of the adversarial regularizer used in both the theoretical analysis and empirical investigation differs in an important way from that used by the main baseline SA-DQN as it is not lower bounded. This is an important detail and I worry that many of the results may be an artifact of this distinction, and that the effect sizes seen in the paper would be much smaller under a bounded adversarial regularizer.\n\nI haven\u2019t previously seen an analysis of the effect of adversarial regularization on the action ranking of the Q-function in RL, though the more general robustness of value-based deep RL agents\u2019 action rankings to optimization noise has been studied previously.",
            "summary_of_the_review": "This paper highlights an important tradeoff in adversarial training of value-based deep RL agents: that optimizing for the robustness of the greedy policy may have unintended effects on the sub-optimal actions. The paper suffers from two principal weaknesses: firstly, that prior work has already identified that the relative ranking of actions in DQN-like agents is unstable, and so the findings here seem to be a natural consequence of that observation coupled with the focus of adversarial regularization on only the greedy action. Secondly, the regularization scheme under consideration differs from that studied in prior work in a way that would exacerbate the effects observed in the paper, and lack of detail on the experiment setup description makes it difficult to identify whether the experimental findings would reflect practical implementations of state-of-the-art adversarial training schemes for RL.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4652/Reviewer_2Ms4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4652/Reviewer_2Ms4"
        ]
    }
]