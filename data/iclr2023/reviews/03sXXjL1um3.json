[
    {
        "id": "_Eiorqgb5N",
        "original": null,
        "number": 1,
        "cdate": 1666489396343,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666489396343,
        "tmdate": 1671004473607,
        "tddate": null,
        "forum": "03sXXjL1um3",
        "replyto": "03sXXjL1um3",
        "invitation": "ICLR.cc/2023/Conference/Paper2896/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the NPG in softmax policy with linear function approximation. By proposing a new updating strategy, in which the stepsize increases at a designed rate, this paper is able to show that the NPG enjoys a per-iteration geometry convergence rate w.r.t a factor $1-1/\\nu_\\mu$. Since previous works of NPG in the similar setting mostly focus on establishing sublinear convergence rate, the result established in this paper is interesting.",
            "strength_and_weaknesses": "Strength:\n(1) This paper is well-written and easy to follow\n(2) The technique proof is solid and clear\n(3) The result established in this paper is different from all previous works in the same setting, which is interesting.\n\nWeakness:\n(1) The major technique contribution of this paper is in exploring how the incremental increasing stepsize with the special RL optimization nature can provide a linear convergence rate, which is interesting but not strong enough.\n(2) The linear convergence rate in this paper may not be very useful in most of the case. The major reason is that the contraction factor $(1-1/\\nu_\\mu)$ is significantly larger than \\gamma (or very close to 1 as $\\nu_\\mu$ is usually very large. With a given sample complexity $\\epsilon$, if $\\nu_\\mu$ scale as $\\mathcal{O}(\\exp(1/\\epsilon))$ then the sample complexity in this paper may actually looser than standard NPG complexity results established in many previous works. Thus, the linear convergence rate established in this paper only improves the real sample complexity of NPG within a very limited scenario. ",
            "clarity,_quality,_novelty_and_reproducibility": "The author may need to discuss in what scenario their established result really outperforms the pervious SOTA results of NPG. After doing that, readers are able to justify the real contribution of this paper.",
            "summary_of_the_review": "Overall this paper provide a very interesting result of NPG. However, the result is more like provide a different formulation of the sample complexity result of NPG instead of establishing a more tighter bound (for detail see my comment in weakness in previous section).\n\n\n============ post rebuttal ============\n\n\nAfter reviewing author's response, I will raise my score to 6. The contribution in this paper is not strong enough given the state-of-the-art. The linear convergence rate of PG without entropy regularization has been explored in previous work. For example, in [1], when adopting l2 regularization in their mirror descent update, the result in [1] naturally implies a linear convergence rate for PG algorithm. However, since this paper explore a new technique for analyzing the PG type algorithm, it might be interesting to the community.\n\n[1] Lan, G. (2022). Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. Mathematical programming, 1-48.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_vAhQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_vAhQ"
        ]
    },
    {
        "id": "fkWLTyQaDnr",
        "original": null,
        "number": 2,
        "cdate": 1666641592137,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641592137,
        "tmdate": 1670739236004,
        "tddate": null,
        "forum": "03sXXjL1um3",
        "replyto": "03sXXjL1um3",
        "invitation": "ICLR.cc/2023/Conference/Paper2896/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes the convergence rate of the natural policy gradient (PG) algorithm with log-linear policy parametrizations in infinite-horizon discounted MDPs. The paper shows that under standard function approximation assumption about the Q-value functions, with an increasing stepsize, a linear convergence rate of the natural PG method can be obtained for log-linear policy class in both deterministic and sample-based settings. The results strictly improve the existing results along this direction, which can only have either sublinear convergence rate, or require regularization and bounded iterate assumptions. In the sample-based setting, the results do not depend on the cardinality of the spaces, but on the dimension of the features, which is favorable for large-scale cases. The paper is clearly written in general, and the results are of interest to the community.",
            "strength_and_weaknesses": "Strength: The paper developed new results on natural PG method with log-linear parameterization, and is well-written generally (up to some small errors). The motivation is clear, and the manuscript is concise and easy to follow.\n\nWeakness: As a pure theory paper, I am a bit uncertain about the technical novelty and solidness of the results. It seems that it strongly relies on the analysis in previous works, e.g., Agarwal et al, 2021, Xiao 2022, and Telgarsky et al., 2022. The idea of increasing the stepsize also follows from Xiao 2022. I skimmed through the proof, which reinforces my feeling as well.\n\nOther Comments:\n1. As it is a theory paper with new technical results, it would be helpful to summarize the novelty of the \"Techniques\" in the front, e.g., in introduction, so that the readers can better understand it and compare it with the literature.\n2. It would be better if some simple experiments can be provided to corroborate the theoretical findings: how does the stepsize affect the convergence rates, how does it compare with the existing PG algorithms, and any ablation study about the algorithm parameters (stpesize, feature vector, etc.).\n3. It was claimed in the abstract and intro multiple times that the results do not depend on |S|, |A| but only on d. However, in the main results in Theorem 4.7, there is still dependence on |A|. It was argued that \"can be removed with a path-dependent bound\". It was not very convincing to me to claim this without a valid statement. It is better to either give a formal statement here, or modify the claims in the beginning. \n4. Errors and typos:\n1) Definition of $d_\\rho^\\pi(s,a)$ on page 3 is not correct. \n2) Some notation abbreviation in sec. 2.1 is not consistent with later sections, e.g., $\\pi^t$ v.s. $\\pi_t$. Where is $d_\\mu^*$ defined? Since it is a theory paper, being rigorous about notation would be important and affect the credential of the paper. \n3) There is an additional \"the\" in the sentence right before Assumption 4.2.\n4) Definition of linear MDP in Remark 4.4 is not correct: P and r should be sharing the same feature $\\phi$.\n\n\n**Post-rebuttal:**\n\nThank the authors for the rebuttal. I have read it, and also read other reviewers' comments. Now I get that the mirror descent reformulation of the NPG update in the linear function approximation case is new. But I am still a bit unsure if the overall novelty clears the bar of ICLR. I recommend the authors to maybe add the results regarding neural network and even general function parameterization more formally, to make the results more complete and stronger. I will hence stick to my previous evaluation. Thank you.",
            "clarity,_quality,_novelty_and_reproducibility": "In general the paper is well written and has good clarity. However, the technical novelty seems a bit limited. It is a theory paper so there is no reproducibility issue.",
            "summary_of_the_review": "In sum, the paper presented some interesting results about natural policy gradient for log-linear policy classes. The techniques and idea about the algorithm (increasing the stepsize specifically), are based on several most related literature, making the novelty and orginality of the work a bit limited to make the cut.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_VEtG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_VEtG"
        ]
    },
    {
        "id": "M8E-7QTM1X",
        "original": null,
        "number": 3,
        "cdate": 1666669550687,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669550687,
        "tmdate": 1670959474221,
        "tddate": null,
        "forum": "03sXXjL1um3",
        "replyto": "03sXXjL1um3",
        "invitation": "ICLR.cc/2023/Conference/Paper2896/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes to use increasing stepsize in Xiao, 2022, as shown in Theorem 4.7, in natural policy gradient with log-linear policy parametrization, and the main result is that the convergence rate is linear plus approximation errors, improving the $O(1/\\sqrt{T})$ rate plus approximation error terms in Agarwal et al, 2021.\n",
            "strength_and_weaknesses": "**Strength**\n\nThe linear convergence rate was previously shown in softmax tabular parametrization, and this work shows that the same rate can be achieved with log-linear parametrization, which was not known before.\n\n**Weaknesses**\n\n1. The above mentioned results are new, but the novelty and technical contributions of this work are very limited to my knowledge for the following reasons.\n    - The main analysis is based on decomposing $V^*(\\mu) - V^T(\\mu)$ into two parts of optimization error and biases, which has already been shown by Agarwal et al., 2021. This work adopted the same assumptions in Agarwal et al., 2021, i.e., Assumptions 4.1, 4.2, 4.5, and 4.6.\n    - Using the decomposition of $V^*(\\mu) - V^T(\\mu)$, stepsize in Xiao, 2022 is then used.\n    - The analyses also look very similar to those two works to me.",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality**: Technical difficulty is limited since this work is using one existing technique into another one to obtain a not surprising result. \n\n**Clarity**: The paper is well written and the results are clearly presented.\n\n**Originality**: Originality is limited as mentioned above.",
            "summary_of_the_review": "Overall, this work combines two existing techniques of Agarwal et al., 2021 and Xiao, 2022, and obtains a new linear convergence rate for natural policy gradient with log-linear parametrization. The results are correct and new. However, to my understanding, the technical difficulty and novelty of this work are limited.\n\n\n====Update====\n\nThank you for the comments. However, after reading the authors' feedback and other reviewers' comments, I would keep my score.\n\nAs mentioned by the authors in the feedback, the main introduced technique is \"the mirror descent formulation of Natural Policy Gradient for non-tabular settings\", which \"is substantially different from the analysis in [Agarwal et al., 2021]\".\n\nThe \"mirror descent formulation of Natural Policy Gradient\" is actually not new. For example, the Q-NPG method in Agarwal2021 is similar to Eq. (2) and Algorithm 1 in this paper. \n\nMy understanding of the authors' claim is that the authors used the analysis in Xiao2022 to the settings in Agarwal2021. The analysis of this paper (performance difference lemma and the three point descent lemma) contains also widely used techniques in mirror decent proofs. The authors' feedback still confirms my impression of incremental technical contribution. \n\nIf the authors wanted to emphasize more the technical innovation, I suggest comparing and illustrating the difference and difficulty more directly, such that the audience can better understand the contributions of this work. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_5t57"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_5t57"
        ]
    },
    {
        "id": "cBky6puA0c",
        "original": null,
        "number": 4,
        "cdate": 1666675133480,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675133480,
        "tmdate": 1666688361082,
        "tddate": null,
        "forum": "03sXXjL1um3",
        "replyto": "03sXXjL1um3",
        "invitation": "ICLR.cc/2023/Conference/Paper2896/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies policy optimization in single-agent RL with log-linear policy parameterization by focusing on natural policy gradient (NPG) method. In particular, this work demonstrates a linear converging error bound of NPG with increasing step sizes to the optimal policy, up to some statistical and approximation errors.",
            "strength_and_weaknesses": "Strength:\n\n- This work investigates the problem of policy optimization with log-linear policy parameterization, which is a fundamental problem of the policy optimization theory. \n- This work extends the insight of [Xiao, 2022] from the tabular setting to the regime of linear function approximation and manages to achieve a linear converging error bound (up to some statistical and approximation errors) without introducing entropy regularization [Cayci et al., 2021].\n\nWeaknesses:\n\n- Assumption 4.6 imposes additional bound on the relative condition number for all iterations (compared to Assumption 6.2 in [Agarwal et al., 2021]), which is somewhat undesirable. It would be great if the authors can make this clearer when making a comparison and provide a justification.\n- The claim \"convergence towards an optimal policy\" appears to lack some support. In particular, it remains unclear if the bound established in Theorem 4.7 is tight when compared with an optimal policy. In addition, the bound introduce a factor $\\nu_\\mu$ in the dependency on $\\epsilon_{stat}$ and $\\epsilon_{bias}$ (which is at least $1/(1-\\gamma)$) compared with Theorem 6.1 in [Agarwal et al., 2021]. This makes the linear converging bound less appealing.",
            "clarity,_quality,_novelty_and_reproducibility": "See \"Strength and Weakness\".",
            "summary_of_the_review": "This work makes interesting contribution towards policy optimization with log-linear policy optimization, by extending the insight of [Xiao, 2022] which focused on the tabular setting. This work establishes a linear converging upper bound up to an error floor determined by $\\epsilon_{stat}$ and $\\epsilon_{bias}$, by deploying NPG updates with increasing step sizes. However, the magnitude of the error floor is worse than that of Theorem 6.1 in [Agarwal et al., 2021], despite introducing additional assumptions. Therefore, the reviewer vote for \"5: marginally below the acceptance threshold\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_SUKm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_SUKm"
        ]
    },
    {
        "id": "p3ee1t1zBy",
        "original": null,
        "number": 5,
        "cdate": 1666838157775,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666838157775,
        "tmdate": 1671030122743,
        "tddate": null,
        "forum": "03sXXjL1um3",
        "replyto": "03sXXjL1um3",
        "invitation": "ICLR.cc/2023/Conference/Paper2896/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the geometric convergence of NPG parametrized by log linear function. The authors assume that approximation of Q-functions by linear function, and approximation of policy with log-linear function has some error bound, and they show that employing NPG with an increasing step size result in exponential convergence to a ball around the global optimum. This ball is proportional to these function approximation errors. ",
            "strength_and_weaknesses": "Strength: \nThe paper extends the linear convergence rate shown for tabular setting in Xiao (2022) to a more general function approximation setting. \nWeakness:\n- In the bound, v_\\mu can be in general state and action dependent. Also it depends on 1/(1-\\gamma). The authors do not specify the dependency of this parameter on state, action and 1/(1-\\gamma). Since this parameter appears in an exponential manner in the upper bound, this can result in a very bad dependency on the upper bound.\n- The contribution of the paper is limited compared to the prior work, specially Xiao (2022).\n- The error terms appearing on the upper bound of Theorem 4.7 can be very loose. In particular, we have a very bad dependency in terms of 1/(1-\\gamma).",
            "clarity,_quality,_novelty_and_reproducibility": "- In Remark 4.4 I believe by \\pi you mean \\phi. \n- The sample complexity stated right before Remark 4.8 is not clear. What is this sample complexity representing? What is w_t here? This is sample complexity of reaching epsilon neighborhood of what value?",
            "summary_of_the_review": "I believe the contribution of the paper is limited, and is a straightforward extension of the prior work. \nAlso the paper is not well written. Some parts are not clear. There is a sample complexity which is not clear corresponds to what. \n\n---------------------------------------------------------------------------------\n\nUpdate: After looking at the response by the authors, I would like to still keep my score. I believe the paper is a simple extension of the Xiao (2022) to the function approximation setting. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_XCzT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2896/Reviewer_XCzT"
        ]
    }
]