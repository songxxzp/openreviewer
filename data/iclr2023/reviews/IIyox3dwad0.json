[
    {
        "id": "ltJv7r8OGsa",
        "original": null,
        "number": 1,
        "cdate": 1666319243293,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666319243293,
        "tmdate": 1670431339382,
        "tddate": null,
        "forum": "IIyox3dwad0",
        "replyto": "IIyox3dwad0",
        "invitation": "ICLR.cc/2023/Conference/Paper6095/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposed a new Boundary Connectivity (BCXN) loss function for PINNs for solving PDEs with complex geometry. BCXN uses a linear interpolation to compute the points outside the domain to address the issue that the derivative stencil points are outside the domain, and then the estimated external points can be enforced directly in the loss or via a penalty loss. Numerical experiments showed that BCXN achieves better accuracy with less training points.",
            "strength_and_weaknesses": "Strengths\n- The proposed method estimates the external points and thus avoid the issue of stencil points falling outside the geomery.\n\nWeaknesses\nComments on the methods:\n- The method only works for Dirichlet BC, but not other BCs, such as Neumann BC, Robin BC, etc.\n- The method uses a linear interpolation to compute external points, which will limit the accuracy.\n- The name \u201cFast-PINN\u201d is not proper. The focus of the proposed method is complex geometry. It is not a fast version of PINN for any PDE problems with any BCs.\n\nComments on the correctness of the numerical results:\n\n- 1D convection-diffusion PDE has been solved very accuracy in many PINN papers, so it is questionable if the AD and ND results are correct. No details about the setup of Fig 2 are provided.\n- In the 2D cavity flow, it is regular geometry, and all the points can be chosen inside the domain. In this case, the proposed method becomes a standard PINN, so the results of ND and AD don\u2019t seem correct. Also, there are PINN papers with 2D cavity flow problem and PINN works well. The only explanation is that the authors make ND and AD become bad.\n\nOther comments on the results:\n\n- The paper uses MSE error, but other metrics should be provided, such as L2 relative error.\n- The paper only considered steady-state problems, but no time dependent problems are tested.\n\nComments on the comparisons:\n\n- It is unfair to compare AD/ND with BCXN as BCXN is more expensive. The authors should increase the training points of AD/ND such that the computational cost is similar, and then compare the accuracy.\n- There are many other methods, e.g., adaptive sampling methods, as were discussed in Section 2, but no comparison is provided.\n\nOther comments:\n\n- The section 3 is \u201cTHEORY\u201d, but there is no theory in the section.",
            "clarity,_quality,_novelty_and_reproducibility": "The written is clear.",
            "summary_of_the_review": "The proposed method doesn\u2019t work as well as they claimed. It has many limitations. The numerical experiments are not convincing enough.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6095/Reviewer_Dt9j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6095/Reviewer_Dt9j"
        ]
    },
    {
        "id": "7S-stzfRHvn",
        "original": null,
        "number": 2,
        "cdate": 1666650193791,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650193791,
        "tmdate": 1666650299766,
        "tddate": null,
        "forum": "IIyox3dwad0",
        "replyto": "IIyox3dwad0",
        "invitation": "ICLR.cc/2023/Conference/Paper6095/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the issue of PINNs overfitting at the boundary of the domain. Since numerical differentiation based methods require points outside the boundary of the domain to work well, the authors propose a simple and efficient solution which constraints the value of the function outside the boundary (external stencil points) in terms of \u201cmirror points\u201d that lay inside the domain. \n\nThis loss can be directly enforced (i.e., the equality can imposed directly for the stencil points outside the boundary) or can be enforced softly by enforcing it as a loss function. \n\nThe authors empirically compare the performance of Fast-PINN with normal PINNs (using numerical and autodiff loss) under 4 fluid dynamics based cases, and show Fash PINNs converge faster and have a lower mse loss.",
            "strength_and_weaknesses": "I really enjoyed reading the paper. The loss function mentioned is easy to implement and gets good results when compared to baseline PINNs. The empirical section is clear and easy to read and the authors do a great job at delineating the type overall setup of the PDEs they consider for their experimental section. The PDEs consider mainly differ in the complexity of the boundaries of the domains.\n\nThe improvement introduced by fast-PINNs seems to be quite significant compared to the normal PINNs (both in terms of the rate of convergence and final loss). The authors also point out certain inductive biases that are inherent in CNN based architectures, which is quite intriguing. \n\n\nOne small question for the authors: Is there a reason that the authors on solving focussed fluid dynamics with PINNs, do the gains follow for other PDEs as well (like reaction diffusion etc?)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and clearly written. The loss function introduced seems to be novel and achieves good performance. ",
            "summary_of_the_review": "The authors provide a simple loss function take care of the points that lie outside the domain for numerical differentiation type loss for PDE solvers. The solution provided by the authors is simple to implement and shows improvement over normal PINN methods.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6095/Reviewer_KS1E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6095/Reviewer_KS1E"
        ]
    },
    {
        "id": "98DJ6AFItQ4",
        "original": null,
        "number": 3,
        "cdate": 1666661297547,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661297547,
        "tmdate": 1666665261345,
        "tddate": null,
        "forum": "IIyox3dwad0",
        "replyto": "IIyox3dwad0",
        "invitation": "ICLR.cc/2023/Conference/Paper6095/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposed a new Boundary Connectivity (BCXN) loss function to improve the accuracy and convergence speed of PINN. Specifically, it developed two variants of BCXN loss, namely: 1) a soft forcing method which imposes a linear approximation constraint via an additional loss term, and ii) a direct forcing approach which strongly enforces a linear constraint during the evaluation of PDE loss at nearboundary samples. Experimental results on multiple datasets demonstrated that the proposed method can significantly improve accuracy while reducing training iterations. ",
            "strength_and_weaknesses": "* Strength\n1. The idea of adding Boundary Connectivity (BCXN) is interesting and novel\n2. The proposed loss function can be inserted into any deep neural network architecture\n3. Extensive experimental results show that the introduced loss function can significantly improve accuracy while reducing training iterations.\n\n* Limitations\n1. Compared to existing work on PINN. There exist many studies on PINN, so it would be better to discuss related work and compare the proposed method with some latest baselines, such as [Zeng et al 2022]\n2. There is a typo about AQ = AP+ AQ in the first line on Page 6. \n3. Question:\n    I am curious if the proposed method can solve the prey and predator system which is not stable?\n\nReferences:\n[Zeng et al 2022] Competitive Physics Informed Networks, https://arxiv.org/pdf/2204.11144.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and organized except for a couple of typos. The idea is a bit novel. It would be better to release the source code.",
            "summary_of_the_review": "The contributions of this work lie in adding Boundary Connectivity (BCXN) in the loss function, which can improve accuracy and reduce training iterations. In addition, extensive experiments are conducted to validate the effectiveness of the BCXN loss function. The idea is a bit novel and interesting,",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6095/Reviewer_3A2P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6095/Reviewer_3A2P"
        ]
    },
    {
        "id": "H--7UYQXiC8",
        "original": null,
        "number": 4,
        "cdate": 1666965637143,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666965637143,
        "tmdate": 1666965637143,
        "tddate": null,
        "forum": "IIyox3dwad0",
        "replyto": "IIyox3dwad0",
        "invitation": "ICLR.cc/2023/Conference/Paper6095/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel design of physical loss to find a trade-off between sample density and solution accuracy of PDEs. The formulation of the proposed loss tackles the challenges of approximating the boundary layer by modeling and adding a term in the loss function that takes into consideration the highly complex fluctuations of the gradients near the boundary layer. The work is inspired by a method in numerical computing.\n",
            "strength_and_weaknesses": "In this work, the proposed method makes the training of PINNs feasible and sufficiently accurate at the boundary layer in a sparse sample regime where existing works fail (or require a highly dense grid at the boundary layer which is not easy to construct and/or is intractable in practice). Moreover, the method shows an extra gain in terms of computational complexity and achieves accurate solutions with drastically fewer iterations in a scarce data regime. \nAt the (near) boundary layer, the fluctuations are highly complex and the geometries are irregular. The proposed design of fast-PINNs loss allows in some way to cope with that. It is challenging because, given the local complexity near the boundary layer, sparse sampling leads to overfitting even though PDE constraint is applied to all the samples.\n\nHowever, l have some considerations:\n\n1/ It is important to underline that the success of PINNs relies on the successful propagation of information from initial and boundary condition points to interior points. This can be observed if we reduce the PDE solving to a local part of the domain. Do you think that your model achieves this successful propagation? how can you quantify that empirically? \n\n2/ In existing PINNs works, the propagation failures are tied to highly imbalanced PDE residual fields.  In other terms, residuals with high values could be observed over very narrow regions. Does the proposed work mitigate/circumvents this failure?  if yes, how does the method succeed to make the balance of PDE residual fields? \n\n3/ Given your method and the impact of a good choice of sampling strategy to make PINNs successful. I am wondering if the breakthrough work on PINNs is more on finding an appropriate sampling strategy near the boundary layer. If the latter is found, standard PINNs loss will work. What do you think?\nA kind of sampling that focuses on points near the boundary layer with high PDE residual at every iteration. In practice, one could imagine sampling more of the points with high residuals. \n\n4/ What do you think of adding the diffusion term in the loss function?\n\n5/  Why did you set manually the different weights of each loss term rather than learning them? \n\n6/ You mentioned that \u201c It is also very challenging to locally refine the sampling density for PINNs with CNN-architecture\u201d. Recently, lots of efforts have been made to generalize learning to non-Euclidean data, known as Geometric Deep Learning. The latter is suitable to operate on meshes and irregular geometries, namely Graph Neural Networks (GNNs), PointNet.  Are there any reasons for not using GNNs ? Several works on ML&physics rely on these architectures.\n\n7/ Lack of quantitative results. This work could be improved by adding a table of baseline w.r.t existing works on PINNs that proposes different variants of physical loss and w.r.t. different. sampling strategies\n\n8/ It would be interesting to see what the proposed method could bring if it is combined with a given domain decomposition method.\n\n9/ This work could be extended to more complex geometries such as the ones available in the different datasets proposed in the work entitled \"Learning to simulate complex physics with graph networks\". It allows to asses the generalization capabilities of the proposed method. \n\n10/ Do you think that the proposed method could generalize well to high Reynolds (> 10^6)\n\n11/ The figures could be updated with log scales",
            "clarity,_quality,_novelty_and_reproducibility": "The claims of the paper are clear and easy to follow.  The whole structure of the paper is good and well-written. \nThe work proposes a novel method for training PINNs derived from numerical computing works which is a great idea since these works are sufficiently mature since several decades.\nThe supplementary part contains all the necessary ingredients to reproduce the experiments. However, it would be better if the code could be made available.",
            "summary_of_the_review": "The contribution of the proposed work will help to make progress in ML&Physics and build efficient PINNs. \nI can increase my score depending on the responses.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6095/Reviewer_DAxW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6095/Reviewer_DAxW"
        ]
    }
]