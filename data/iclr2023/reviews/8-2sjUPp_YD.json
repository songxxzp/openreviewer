[
    {
        "id": "yZ5xwc-8sy",
        "original": null,
        "number": 1,
        "cdate": 1666674999388,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674999388,
        "tmdate": 1666674999388,
        "tddate": null,
        "forum": "8-2sjUPp_YD",
        "replyto": "8-2sjUPp_YD",
        "invitation": "ICLR.cc/2023/Conference/Paper5578/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies how to utilize unimodal pretrained models to improve vision-language downstream tasks through distillation. In particular, the paper proposes to use both visual and textual teachers to supervise a student VL model during the finetuning stage. A group of methods/tricks have been explored, including adaptive confidence reweighting, adaptive token selections, and two-stage finetuning. The resultant framework improves the downstream VL performance on the student model.",
            "strength_and_weaknesses": "Pros:\n1. Experiments are conducted on many pretrained variants and datasets, and clearly show gains over VL baselines. This shows that the proposed method is effective.\n2. Ablation studies are performed and it is clear that each proposed component is beneficial.\n3. The paper is clear and easy to follow.\n\nCons:\n1. The resulting student model is a large-scale VL pretrained checkpoint itself. It will be more interesting if the method can get strong performance on non-pretrained or smaller-scale models. As of now, it is hard to say the method brings 'efficiency' given everything is pretrained anyway.\n2. The method contains too many specialized components. Although ablation does show the benefit of each, this still makes the method hard to use in practice. In fact, I am a bit hesitant to call it a 'method' and feel it is more like a bag of 'methods'.\n3. Although some analysis is given, it is still hard to understand why the method is better. Since it is not intuitive why such framework can/should improve visual attention, it seems a must to dig further to understand details such as which exact step brings this benefit and why.\n4. The proposed method is also designed for fusion-based VL models and not directly applicable for other trending architectures such as encoder-decoder.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and novel overall. It maybe easy to reproduce if the code will become available eventually.",
            "summary_of_the_review": "In general, this paper presents a seemingly working framework to improve VL tasks with distillations from both vision and language pretrained checkpoints. However, the method is quite complicated and appears to be a bag of tricks. On the other hand, there is no clear and comprehensive understanding of the source of improvement. Therefore, I wish to see more studies to understand the trade-off between the additional training costs and true benefits the method brought.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5578/Reviewer_LFJd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5578/Reviewer_LFJd"
        ]
    },
    {
        "id": "Tz6Y2xUCueC",
        "original": null,
        "number": 2,
        "cdate": 1667242391267,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667242391267,
        "tmdate": 1667549453991,
        "tddate": null,
        "forum": "8-2sjUPp_YD",
        "replyto": "8-2sjUPp_YD",
        "invitation": "ICLR.cc/2023/Conference/Paper5578/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes methods to distill unimodal teacher models of vision and language onto vision-language (VL) student models for utilizing the benefit of unimodal pre-trained models. \nThe authors present ideas to improve the performance: (1) VL student models are not changed, but additional learnable components are attached on the top of them, (2) instance-level weighting and (3) text token selection to consider for distillation. \nThe authors claim that the methods have good advantages to improve the performance and to avoid pre-training large models, and achieve new state-of-the-art performance.\nThe two former claims are validated with some experimental results on 3 kinds of vision-linguistic tasks (Vision Commonsense Reasoning, Visual QA, SNLI-SE) including ablation study and comparative analysis. \nHowever, the latter one is NOT justified since the supporting evidences in the web-based leaderboards have some problems.",
            "strength_and_weaknesses": "*Strength\n- The proposed ideas are good to achieve the research goal. I think the proposed methods would be practically useful if they are generally applicable and the benefit is clear in most cases.\n- The paper presents interesting findings on the performance improvements in the low-shot area.\n- Various settings of experiments are performed and the results are consistent as the authors\u2019 claims.\n\n\n*Weaknesses\n- I'm concerned with that the significance of this work would be limited. This method is not for building compact student models (in normal scenarios of knowledge distillation), but for improving the performance based on VL student models, unimodal teacher models for vision, and unimodal teacher models for language. The generality of the performance improvement is not clearly validated yet, and the low-shot setting, the area that the benefit is significant, needs the criteria of how low is appropriate to apply this method.\n- One of the main claims is that the proposed methods achieve new SOTA on the VCR tasks. However, there are some problems on that paragraph. I checked the two leaderboards (1: https://leaderboard.allenai.org/vcr/submissions/public, 2: https://visualcommonsense.com/leaderboard/ ). Following the AI2 leaderboard, I realized that the numbers from the page 8 of the manuscript, \u201cQ2A: 80.9%, QA2R:83.3%, Q2AR: 67.7%\u201d, is the 1st one\u2019s and it is for MERLOT, not for ADVL. The ADVL\u2019s is \u201c80.4%, 82.3%, 66.2%\u201d. Following the VCR leaderboard, ADVL is the 18th runner and the score is \u201c79.6, 82.9, 66.2\u201d. Considering the submission dates of the others and the ranking, I cannot accept the claim currently. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The organization of the manuscript is good to read and easy to follow. However, so many typos included.\n- Related work is well-surveyed.\n- I think that the ideas on this paper are novel.\n- The explanation of the methods is explicit and concrete. I think it is enough to be reproducible.\n",
            "summary_of_the_review": "I think that the weaknesses on the significance of this work is so critical, and it is difficult to accept the authors' claims.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5578/Reviewer_JhCz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5578/Reviewer_JhCz"
        ]
    },
    {
        "id": "3bp81zn4IQ",
        "original": null,
        "number": 3,
        "cdate": 1667544364361,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667544364361,
        "tmdate": 1667544988593,
        "tddate": null,
        "forum": "8-2sjUPp_YD",
        "replyto": "8-2sjUPp_YD",
        "invitation": "ICLR.cc/2023/Conference/Paper5578/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work is concerned with vision-language representation learning. Main contribution is a new distillation technique that uses stronger uni-modal encoders as teacher network, to improve the finetuning performance of pre-trained VL models. \n\nIn specific, the work proposes Adaptive Distillation for Vision-Language (ADVL) tasks. ADVL leverages unimodal pre-trained models (e.g. ViT for vision, Roberta for text). The distillation is achieved by minimizing the L1 distance between features from the uni-modal teacher network, and the ones from the pre-trained VL student models. Authors also propose additional mechanisms, such as (i) adaptive confidence-based weighting, which adjust distillation weights based on student networks' confidence scores; (ii) adaptive text token selection, which enables the distillation process to focus on semantically more important text tokens; (iii) finetuning with contrastive learning to enhance cross-modal alignment.\n\nExperiments are conducted using UNITER, VL-BERT, VILLA as student networks, and various unimodal encoders as teacher networks, including SLIP, CLIP ViT, Swin transformer. The proposed technique improves student networks on visual commonsense reasoning (VCR), visual entailment (SNLI-VE), and visual question answering (VQA) significantly in low-shot setups, and marginally in full-shot setups.",
            "strength_and_weaknesses": "**Strengths**:\n1. The motivation is clear. It is naturally expected that vision-language models can benefit from stronger unimodal encoders, for which the best strategies are not very well explored by the community.\n2. The idea is simple and easy to implement. According to the evaluation, the proposed method shows its effectiveness especially when the finetuning data is limited.\n3. Although the presentation can be polished, the overall narrative and explanation is clear and easy to follow.\n\n**Weakness**\n1. **Flawed experiment setup**. The motivation is to leverage unimodal data, which are assumed easier to obtain than image-text pairs. However, the teacher networks used in the experiments are not always uni-modal. For example, CLIP, SLIP are trained on a large amount of image-text pairs, meaning that their unimodal encoders are also aligned across modals. This contradicts with the motivation.\n2. **Limited technical novelty and incremental empirical gains**. The proposed distillation loss is standard and by itself is not technically new. In addition, improvements on full-shot cases are mostly marginal. Considering this is the combined benefit of multiple techniques, e.g. distillation, text token selection, contrastive learning, I am not fully convinced by the empirical value of the proposed method.\n3. **Student networks** are too weak to prove the proposed techniques are useful for the more recent (and more powerful) models. The selected student networks, VL-BERT, UNITER, VILLA, while they are great and highly reputable works in the community, their performance is not as competitive as for today. Therefore, the obtained task performance is far from state-of-the-art. For example, on VQA, more recent works (BEiT-3) achieve 84+, while the best reported result in the manuscript is ~76. To make the method more convincing, authors may consider use more recent VL models as student works, and try to further push their limits. For example, to use LLM such as GPT to improve the text representation of BEiT.\n5. **More recent VL-pretrained works are not well acknowledged**. Although I understand the experiment setup, missing reference to more recent VL works prevent readers from getting a good research landscape in the multimodal pre-training. For example, recent great VL works, ALIGN, ALBEF, OFA, Frozen, Flamingo, Florence. BLIP, BEiT3. Authors are suggested to better position their work among the more recent ones.\n4. It is not fully clarified what is the difference between the so-called \"ITM\" (image-text matching) and the contrastive losses used in other  VL pretrained models, such as ALIGN, ALBEF. Conventionally the ITM loss is a binary prediction task, while the particular one used in this work is more often referred as contrastive learning loss. This may create unnecessary confusions.\n6. Presentation can be improved. Typos are not uncommon. Experiment setups are not entirely clear. For example, what is the experiment environment and training receipts.",
            "clarity,_quality,_novelty_and_reproducibility": "- Not always clarified, especially experiment setups.\n- Overall quality is not satisfactory.\n- Novelty is limited.\n- Reproducibility is expected as open-sourcing is promised. In addition, the method itself is straightforward to implement.",
            "summary_of_the_review": "The proposed technique provides marginal empirical benefit while the method itself is technically incremental. Experiments are flawed, especially the choice of teacher networks falsifies the motivation of relying on uni-modal data only. It is also not clear whether the proposed technique will push the limits of more recent VL pre-trained models, especially those trained end-to-end. Therefore, I'd not recommend a rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5578/Reviewer_chYB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5578/Reviewer_chYB"
        ]
    }
]