[
    {
        "id": "jxatJnyzvC",
        "original": null,
        "number": 1,
        "cdate": 1666458753008,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666458753008,
        "tmdate": 1668788981524,
        "tddate": null,
        "forum": "aEFaE0W5pAd",
        "replyto": "aEFaE0W5pAd",
        "invitation": "ICLR.cc/2023/Conference/Paper1861/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new method (CIDER) for representation learning and applies it for out-of-distribution detection. Given an $L_2$ normalized embedding (hyperspherical embeddings) from a deep network, the method computes prototypes, as moving averages of the embeddings corresponding to each class. The method is optimized using two losses. The first one (compactness loss) minimizes the distance between a sample and the prototype corresponding to its class while maximizing the distance to the other ones. The second loss (dispersionloss loss) maximizes the distance between all pairs of class prototypes. The representations learned by this method are then used to detect out-of-distribution samples using KNN distance.\n\n",
            "strength_and_weaknesses": "**Strong points**:\n\nS1. The method obtains good results for OOD detection compared to recent methods.\n\nS2. The method and investigations are sound. \n\nS3. Paper is mainly well-written.\n\n**Weak Points**:\n\nW1. The relation between the proposed losses and other common losses (cross-entropy and contrastive losses) should be better explained and investigated more. For example, the goals presented in paragraph Training objective (Sec. 3.2) is also attained by cross entropy loss, why should the proposed method behave better? The last weight matrix of a deep model could be interpreted as the set of class prototypes ( with cross-entropy loss thus being similar to the compactness loss)  what are the advantages of the proposed approach of using moving averages as propotypes. The Alignmentand Uniformity metrics of Wang and Isola 2020 share similar goals as the proposed losses and should be discussed also.\n\nW2. The paper proposes some experimental investigations into the benefits of the method, and they are appreciated, but they should be expanded and better explained. \n\n W2.1. While it is perfectly sound to optimize the embeddings using the class prototypes, it is not clear why the measurements of embedding quality (inter-class dispersion and intra-class compactness from Sec 4.3) should take the prototypes into account. More fundamental metrics should be defined based on similarity between embeddings of samples $z_i$ and $z_j$ of different classes (for dispersion) or the same class (for Compactness) (similar to Wang and Isola 2020). Similarly, the separability metric could be designed based on average similarities between pairs of embeddings. Using the prototypes is just a proxy of these, more fundamental metrics. So why should be metrics be defined based on prototypes? \n\n W2.2 How are prototypes defined for Cross-Entropy SSD+ methods shown in Table 4? Are they computed by eq.8 during training? For the Cross-entropy method, are the embeddings $L_2$ normalized during training and when computing the metrics? If they are normalized only for the metrics, does it make sense to use them in this new hyperspherical space if they were not trained in this space?\n\n \n\nW3. How are the results in Table 1 /  Table 6 computed? Are the other methods trained with original code or are the original results presented? Table 6 shows worse results for all other methods than a similar table (Table 1) from (Sun et al. 2022). Why? \n\nW4. How many seeds were used when computing the results? It would be better to use multiple seeds and present results with mean and standard deviations.\n\nW5. It should be noted that this method needs ID class labels, having a disadvantage as opposed to contrastive methods that are designed to work both with and without labels.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper would benefit from additional details (regarding experiments, see W3, W4) and better explanations and relations to prior work (see W1 and W2). \n\n",
            "summary_of_the_review": "This paper proposes a sound method to obtain representations useful for OOD detection. Some further explanations and investigations into the benefits of the proposed method are still needed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1861/Reviewer_FAUX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1861/Reviewer_FAUX"
        ]
    },
    {
        "id": "stNg3yT935v",
        "original": null,
        "number": 2,
        "cdate": 1666665540427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665540427,
        "tmdate": 1666665540427,
        "tddate": null,
        "forum": "aEFaE0W5pAd",
        "replyto": "aEFaE0W5pAd",
        "invitation": "ICLR.cc/2023/Conference/Paper1861/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the out-of-distribution detection task and proposes a new method called CIDER, which trains the model by optimizing a dispersion loss and a compactness loss. The compactness loss encourages samples to be close to their class prototypes while the dispersion loss encourages large angular distance among different class prototypes. This paper also investigate EMA for updating prototype parameters. Experiments demonstrate the effectiveness of CIDER.",
            "strength_and_weaknesses": "Strength:\n1. The idea is well motivated and writing is good\n2. Experiment evaluation is thorough\n\nWeakness:\n1. Training scheme needs further analysis and explanation",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and easy to follow. The main idea is presented clearly. Details of experiment settings can be found in the paper,  and corresponding analysis support the claim well.",
            "summary_of_the_review": "This paper makes a detailed analysis to OOD detection problem and proposes many metrics (dispersion, compactness and separability) for evaluating OOD performance, which is impressive. Experiment results are thorough, which is helpful for understanding CIDER and its effectiveness compared to previous methods. I just list some points that can be further improved. \n\n1. Updating class prototypes during training seems a little weird. In Algorithm 1, the class prototype $\\mu_c$ is updated using EMA first (line 8) and gradient descent w.r.t. Eq. (7) latter (line 12). Why adopting such a two-stage scheme and what if updating prototypes by optimizing Eq. (7) directly without EMA?\\\n\n2. Figure 3: It will be better to compare with contrastive loss and show OOD embeddings, just like Figure 1 in KNN+ paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1861/Reviewer_U6bR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1861/Reviewer_U6bR"
        ]
    },
    {
        "id": "Vr2QFwRBWU0",
        "original": null,
        "number": 3,
        "cdate": 1666707265335,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707265335,
        "tmdate": 1666707265335,
        "tddate": null,
        "forum": "aEFaE0W5pAd",
        "replyto": "aEFaE0W5pAd",
        "invitation": "ICLR.cc/2023/Conference/Paper1861/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new out-of-distribution detection method that is based on supervised contrastive learning in a hyperspherical space.  The core component of the proposed method is a novel regularizer that encourages the separation of class centroids. The resulting hyperspherical space is highly effective in distance-based OOD detection methods.",
            "strength_and_weaknesses": "## Strength\n\n* The proposed method is simple and significant. This finding seems to be generalizable to multiple different applications.\n* The paper is overall clearly written with meticulously described details.\n* The paper provides extensive empirical study with highly convincing results.\n\n## Weaknesses & Questions\n\n* One thing that is not very clear is how exactly the class prototypes ($\\mu$'s) are updated. The reason of the confusion is that there are two equations that can update the class. Both the dispersion loss (Eq.(6)) and the exponential moving average (Eq.(8)) govern the update of the prototypes. Looking at Algorithm 1 in the Appendix, it seems like both equations are applied sequentially. Please correct me if I am wrong.\n    * If it is correct that the two equations are both applied to update the prototypes, I wonder why we need Eq.(8). Wouldn't the embedded vectors $z$ eventually cluster around the corresponding prototype by simply enforcing the compactness loss, Eq.(5)? I see that Figure 6(b) shows the effect of this moving average update on the final performance but do not understand why this even affects the quality of the final representations.\n* The biggest weakness of this paper is that the finding of the paper is mostly empirical. It would be nicer to have a deeper interpretation regarding the dispersion loss, which is the key contribution of the paper, so that we can have a distilled insight on representation learning and OOD generalization.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clearly written, besides the point that is discussed in the weakness section. The paper is excellent in quality, particularly regarding the extensive empirical study. The proposed method may not be completely novel but it seems to be new in the OOD detection context. The paper provides enough information to reproduce its results.\n",
            "summary_of_the_review": "This paper presents solid results on improving OOD detection on hyperspherical representation space.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1861/Reviewer_Vej6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1861/Reviewer_Vej6"
        ]
    },
    {
        "id": "fRcyB89GKZ9",
        "original": null,
        "number": 4,
        "cdate": 1666739828574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666739828574,
        "tmdate": 1666739828574,
        "tddate": null,
        "forum": "aEFaE0W5pAd",
        "replyto": "aEFaE0W5pAd",
        "invitation": "ICLR.cc/2023/Conference/Paper1861/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents an approach to detecting out-of-distribution (OOD) samples using hyperspherical embeddings. Specifically, two loss functions: dispersion loss and compactness loss are proposed to increase the inter-class distance and decrease the intra-class distance between samples, respectively. The approach is evaluated on benchmark datasets and various ablation studies are conducted to justify various components of the approach. The results are impressive.",
            "strength_and_weaknesses": "Strengths:\n\n1. Exploiting hyperspherical embeddings for OOD detection is an interesting and effective idea.\n\n2. The proposed dispersion\u00a0loss is shown to be effective for OOD detection and improving ID classification accuracy.\n\n3. Extensive experiments are conducted to evaluate\u00a0various aspects of the approach.\n\na. Proposed approach can be combined\u00a0with existing parametric (Mahalanobis) and non-parametric (KNN+) approaches (Table 2).\nb. The dispersion\u00a0loss improves both OOD detection and ID classification accuracy (Table 3).\nc. The proposed approach improves class separation as expected.\nd. Effective for hard OOD samples (CIFAR 10 vs CIFAR 100)\n\nWeaknesses:\n\n1. Authors claim that the OOD samples lie between ID classes. How to validate this claim beyond the visualizations? In some case, OOD sample can be close to ID clusters as well. How does the proposed approach handle such situations?\u00a0Please refer to the following\u00a0\nKaur, R., Jha, S., Roy, A., Park, S., Sokolsky, O., & Lee, I. Detecting oods as datapoints with high uncertainty.\u00a0ICML Workshop on Uncertainty and Robustness in Deep Learning, 2021.\n\n2. Is the proposed loss functions particularly effective while considering the hyperspherical embeddings or can be used with standard euclidean\u00a0embedding as well?\n\n3. How do various values of the parameter $\\lambda$ in Eq.7 affect the OOD detection and ID classification performance?\n\n4. Does the approach require retraining the backbone encoder network with pre-trained networks by just learning the final projection head as shown in Fig. 1?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the approach appears to be reproducible. ",
            "summary_of_the_review": "Even though the idea of maximizing the inter-class distance and minimizing the intra-class distance is explored in various contexts, applying this with hyperspherical embeddings for OOD detection is novel. The approach is shown to be effective by extensive experiments. Please address the comments in the weaknesses section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1861/Reviewer_Fpyq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1861/Reviewer_Fpyq"
        ]
    }
]