[
    {
        "id": "RyCEhGxaSX",
        "original": null,
        "number": 1,
        "cdate": 1665725893080,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665725893080,
        "tmdate": 1670990084589,
        "tddate": null,
        "forum": "-Yzz6vlX7V-",
        "replyto": "-Yzz6vlX7V-",
        "invitation": "ICLR.cc/2023/Conference/Paper5321/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper argues that the languages that emerge between networks are straightforwardly compositional with variations.\n\nThe paper introduces a variation-based framework and new measures to analyze regularity in mappings and exhibit compositional structure, which is clearly shown in experiments.\n\nIt shows that an emergent language's regularity is strongly correlated with generalization in early training, but it goes away after convergence.\n\nThe experiments show that small model capacity improves regularity.",
            "strength_and_weaknesses": "***Strength***\n\n- It defines multiple practical measures for compositionality.\nThe definitions link multiple observations to multiple variations of linguistic concepts.\nObservations include position, character, object, and role.\nThe variations include Synonymy, word order freedom, entanglement, and homonomy.\n\n- It has findings for whether and when compositional structures are learned. It also finds the relation to model capacity.\n\n***Weakness***\n\n**1. Why are the definitions good ones to measure language compositionality?**\n\n*1A. Do they measure compositionality?*\n\nFor example, some natural languages have the word order freedom, and others do not. However, in both cases, the languages can be compositional.\n\n*1B. Do they cover all compositional phenomena?*\n\n**2. Contents of definitions**\n\nSince the joint probability is available, a conditional probability can be directly computed.\n\nIt would be helpful to explain why the proposed definitions are more reasonable than the standard definitions of the conditional probability distribution, which match the verbal definitions.\n\n*2A. Definition in 3a*\n\nAs stated in the text, word order freedom indicates whether \"each semantic role is consistently encoded in a particular part of the signal.\"\nA straightforward definition that matches the statement is the conditional distribution of position given role: P(position | role) (or 1 - P), which differs from the definition in 3a.\n\n*2B. Definition in 5a*\n\nFor each character in each position, \"a probability distribution over atoms in a role\" can be straightforwardly defined and computed as a standard conditional probability distribution P(atom | role, character, position). It does not equal the definition in 5a.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\n**3. There are mistakes or inaccuracies in equations.**\n- In 3a, \"a=1\" should be \"i=1\"\n- In 3b, the denominator has \"i\" for atom. The Freedom(L) on the left-hand side does not depend on \"i,\" but \"i\" is not dropped on the right-hand side.\n- In 5b, \")\" on the numerator should be before \":\"\n\n**4. Additional comments on equations:**\n- In 2a, S is defined as a set, but they are summed in 3a. However, the set sum is not a standard operation. It might be clearer to define S as a vector, where each element corresponds to a position. Then 3a becomes the sum of vectors.\n- Why is 3b bound between 0 and 1?\n\nQuality:\n\nThe definitions of the measurements are essential in this work since they are used to argue about the findings.\nHowever, the quality definitions may need improvement, as mentioned in the \"weakness\" section.\n\nNovelty:\n\nThe novelty is to define the measurements and use them to evaluate compositionality for new findings.\n\nReproductivity:\n\nThe paper does not mention (anonymized) source codes for the experiments.",
            "summary_of_the_review": "This paper introduces a new framework with multiple definitions to measure compositionally.\n\nHowever, the reviewer thinks it is still not above the acceptance line, mainly because the definitions may need improvement, as mentioned in the \"weakness\" section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5321/Reviewer_UTH9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5321/Reviewer_UTH9"
        ]
    },
    {
        "id": "i6JwX6aYpL",
        "original": null,
        "number": 2,
        "cdate": 1666657134346,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657134346,
        "tmdate": 1666657134346,
        "tddate": null,
        "forum": "-Yzz6vlX7V-",
        "replyto": "-Yzz6vlX7V-",
        "invitation": "ICLR.cc/2023/Conference/Paper5321/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a variation-based framework that is able to capture the compositionality in emergent languages. Contrary to most previous work, the authors show that compositionality is in fact correlated with generalization. The difference being that the learned compositionality is obscure and posses a high degree of variability and cannot be captured with the given metrics.",
            "strength_and_weaknesses": "The paper does an intensive study on the previous metrics used for compositionality in emergent languages. The paper proposed 4 different metrics for evaluating compositionality that are able to marginalize the factors of variation in the language.\n\nThe closest metric that seems to be relevant to the proposed metrics is residual entropy. But I don't see any comparative evaluation with that in the paper and in the Appendix. \n\nAlso, Kottur et al. 2017, Resnick et al. 2020 also investigates the relation between capacity, compositionlity and generalization but it hasn't been captured here. ",
            "clarity,_quality,_novelty_and_reproducibility": "The analysis done in the paper seems to be novel although the relation between previous metrics (as described above) and the proposed ones is not clear.\n\nBesides, the connection between capacity and generalization is not clearly explained and how do the findings relate to the previous work.",
            "summary_of_the_review": "The paper is clearly written and it indeed investigates an important research question in emergent communication. The hypothesis indicates some contrasting results from previous work and provide a novel view to study compositionality in emergent languages. Still, I believe the contribution is not clear when compared to prior works as outlined above.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5321/Reviewer_uoNs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5321/Reviewer_uoNs"
        ]
    },
    {
        "id": "ILjzLApkq0",
        "original": null,
        "number": 3,
        "cdate": 1666678450440,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678450440,
        "tmdate": 1666678450440,
        "tddate": null,
        "forum": "-Yzz6vlX7V-",
        "replyto": "-Yzz6vlX7V-",
        "invitation": "ICLR.cc/2023/Conference/Paper5321/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- This paper attempts to address the contradiction that prior works have shown emergent languages to generalize compositionally but without the emergence of compositional languages.\n- The authors argue that emergent languages are characterized by variation and that an emergent language's compositionality is distinct from its regularity. This variation masks compositionality from previous compositionality metrics like topsim, residual entropy, and posdis.\n- The authors introduce variation measures for synonymy, word-order freedom, entanglement, and homonomy.\n- To demonstrate that these measures follow the semantics of compositionality as we would expect:\n    - The authors compare a perfectly regular and a non-compositional random mapping and show that the variation measures take on extreme values, with close to 0 for the compositional mapping and close to 1 for the non-compositional mapping.\n    - Experiments show that an emergent language's regularity and variation correlate positively and negatively with generalization, as one would expect, early in training. This correlation goes away with increasing epochs as we get to the regime where the authors suggest all models converge to a sufficiently regular language to generalize compositionally.\n    - The authors verify the previously-published connection that reducing model capacity results in greater regularity of emergent languages, using the variation metrics.",
            "strength_and_weaknesses": "Strengths:\n- This paper presents an important perspective on the compositionality of emergent languages and new metrics to support it. The contributions can potentially address the debate of whether or not compositionality is important for generalization.\n\nWeaknesses:\n- There is no good explanation for why we only expect a correlation of the compositionality metrics with generalization earlier in training. The correlation of the compositionality metrics goes to zero as a model converges. Doesn't this indeed say that the degree of regularity or variation does not matter for OOD generalization?\n\nAnother question:\n- In addition to the presented variation measures, is it reasonable to average the variation measures into an additional \"average variation\" metric to compare with the previous compositionality measures?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality:\n- The paper is very well-written and easy to follow.\n\nNovelty:\n- The ideas in the paper are novel.\n\nReproducibility:\n- Section 3 describes the model architecture, but reproduction of the exact results in the paper also requires the specific datasets and splits used, which are currently not accessible.",
            "summary_of_the_review": "This paper has the potential to be an important one for the community. However, it needs to explain better why there is no correlation between the metrics and OOD generalization at model convergence. How does this paper positively answer the question, \"is compositionality necessary for generalization?\"? I would be inclined to increase my score based on the resolution of these concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5321/Reviewer_uQjZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5321/Reviewer_uQjZ"
        ]
    },
    {
        "id": "B32upozFwDO",
        "original": null,
        "number": 4,
        "cdate": 1667486177469,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667486177469,
        "tmdate": 1667487448306,
        "tddate": null,
        "forum": "-Yzz6vlX7V-",
        "replyto": "-Yzz6vlX7V-",
        "invitation": "ICLR.cc/2023/Conference/Paper5321/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission aims to explain the negative results in the compositionality of emergent languages of prior work, in particular, the seeming double dissociation between the generalization ability of agents and the compositionality of their emergent communication systems.\nThis is achieved by defining measures of variation (nonregularity) that are shown empirically to be analogous to previous compositionality measures, alongside the claim that out-of-distribution (OOD) generalization performance is a better measure of compositionality.",
            "strength_and_weaknesses": "### Strengths\n\n1. **Interesting motivation**: The manuscript aims to explain negative results in the compositionality of emergent languages, in particular, the seeming double dissociation between the generalization ability of agents and the compositionality of their emergent communication systems.\nThis problem is of interest to communities at ICLR.\n\n2. **Interesting contribution**. The idea that \"language variability obscures compositionality as measured in prior works in emergent communication\" is nice. \nThe paper also follows up on this idea with concrete proposals for measuring the variability that might confound prior measures of compositionality.\n\n\n### Weaknesses\n\n1. **Difficult to understand definitions and their generality.**\nThe definitions of the measures in Section 2.3 lack an introduction to what is meant by many of the concepts they depend on; for example, what is meant by\n\"character,\" \"signal position,\" \"semantic atom,\" \"semantic role,\" \"meaning,\" and\nby \"characters referring to words.\"\nBecause these basic concepts were not defined, it isn't easy to interpret the definition of the measures themselves and how they might measure language variability that confounds measures of compositionality, which is one of the paper's main contributions.\nFurthermore, how do these concepts map onto aspects of the emergent languages studied in the relevant prior work, and of natural languages?\n\n2. **Motivation for using OOD generalization performance to measure compositionality needs to be clarified.**\nSince the paper centers on refuting measures of compositionality in prior work,\nThe paper should clearly state how OOD evaluation is sufficient to measure compositionality. \nHowever, there seem to be only brief discussions of this \nin the middle of Section 3 and at the beginning of Section 4 \n(in addition to the broader discussions in the introduction).\n\n3. **Unclear how insights from the experimental setup generalize.**\nSection 3 has only the following as a comparison of how the setup in the paper relates to prior work:\n> This setup, while simple, is in line with previous work in this area (e.g., Guo et al., 2021; Chaabouni et al., 2020).\n\n    However, the comparison with prior work is critical because of the claim that prior work does not disentangle compositionality from regularity. As it stands, it is not clear how the results from the experiments in this paper will generalize to experimental setups of other works that are explicitly named as motivations, including Choi et al. (2018), Kottur et al. (2017), and Chaabouni et al. (2020).\n\n4. **Insufficient references to prior work in the compositionality of natural language.**\n\n    The forms of regularity/variability in Section 2.4 allude to prior work in linguistics, lexical semantics, compositionality in natural language, etc. However, no references to any prior work in these domains are made in this section. It is not likely that this manuscript independently originated the notions of synonymy & homonymy, word order, and \"entanglement\" of syntax and semantics. Some further spots that lack references are below in \"Minor points.\"\n\n\n### Minor points\n\n1. > However, natural languages exhibit rich patterns of variation (Weinreich et al., 1968), frequently violating these three properties...\n\n    Please also cite negative work in compositionality itself of human languages:\n    - Barbara H. Partee. 1984. Compositionality. Varieties of Formal Semantics, 3:281\u2013311.\n    - [Dankers et al. (2022)](https://arxiv.org/abs/2108.05885)\n\n2. > ... the question of an emergent language's compositionality is related to, but distinct from, questions about its regularity.\n\n    At this point, I'm still not sure about the precise meaning of \"regularity,\" which makes it difficult to understand the significance of the double dissociation experiments discussed in this paragraph.\n\n3. > While languages that score highly on topsim, residual entropy and posdis are necessarily compositional, not all compositional languages will score highly ...\n\n    This is not entirely self-evident from the preceding two paragraphs. Can you add additional justification or state that you will more precisely define these concepts later?\n\n4. > \"e.g. 'loves', 'adores', 'treasures', 'cherishes' all convey approximately the same concept, exemplifying a one-to-many meaning-word mapping\"\n\n    I don't think this is the best example: The latter two words are most often used in the context of inanimate objects or concepts.\n\n5. > Natural language shows us that a compositional system can exhibit high levels of synonymy and homonymy while retaining the productive generalisation afforded by compositionality.\n\n    This should include some references to the vast literature on lexical semantics and pragmatics that it alludes to.\n\n6. > The freedom of order by which meanings are expressed in form has little bearing on whether or not the resulting form can be composed or interpreted compositionally ...\n\n    This statement discards the role of syntax in languages that are unlike Basque. Syntax clearly matters for the interpretation of lexical compositions in many languages.\n\n8. > ... in terms of a probability distribution over characters in each signal position given a semantic atom in a semantic role.\n\n    It's unclear what a \"semantic atom\" or a \"semantic role\" is, \n    both as abstract definitions and in terms of the specific experimental setup.\n    Similarly, it's not clear what \"characters\" nor \"signal positions\" are.\n\n9. > In order to better align our findings with the broader literature on compostional generalization in neural networks we implement a version of the maximum compound divergence (MCD) algorithm from Keysers et al. (2020), and report results for both in-distribution generalization, and out-of-distribution generalization to an MCD split.\n\n    I don't understand the motivation for this at this point in the paper.\n    This component of prior work needs to be better explained in the context of this paper, especially as it seems to be an essential part of the evaluation.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The motivation of the paper is clear, but the details of the contribution need to be more clearly communicated.\n\n**Novelty:** The introduction of language regularity measures as alternatives to compositionality is novel.\n\n**Reproducibility:** I did not see any overt effort towards reproducibility, and the experimental setup seems to lack details, including a full specification of the dataset.",
            "summary_of_the_review": "The motivation to disentangle regularity and compositionality in emergent languages is interesting and novel, but the implementation and generalizability of the idea are not made sufficiently clear to judge significance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5321/Reviewer_hjdA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5321/Reviewer_hjdA"
        ]
    }
]