[
    {
        "id": "JkmJ82o4wFu",
        "original": null,
        "number": 1,
        "cdate": 1666550131826,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666550131826,
        "tmdate": 1669139156030,
        "tddate": null,
        "forum": "n1bLgxHW6jW",
        "replyto": "n1bLgxHW6jW",
        "invitation": "ICLR.cc/2023/Conference/Paper6273/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the zeroth-order optimization where the gradient information is infeasible or very expensive to access, and only function values are available. Different from most one or two-point zeroth-order estimation, this paper assumes the function f is sampled from Gaussian process as in Bayesian optimization literature and then develops a trajectory-based zeroth-order method with some posterior information such as mean and covariance functions (both of which are constructed using previous trajectory information). Some error analysis of gradient estimator and convergence of the proposed algorithm are also provided. In experiments, they show that the proposed method admits a better gradient estimation and is more query efficient, compared to some zeroth-order and Bayesian optimization methods. ",
            "strength_and_weaknesses": "Pros:\n\n1. Zeroth-order optimization has become an important topic as in many new applications such as adversarial learning, hyperparameter optimization of black-box or complex systems. This paper provides an interesting solution motivated by Bayesian optimization.\n\n2. The presentation is overall good. The proposed method admits feasible mean and covariance function information due to the Gaussian process assumption, which is useful to make the design flexible, e.g., including the so-called virtual updates to improve the query efficiency. The idea is simple and intuitive. \n\nCons:\n\n1. The comparison to existing two-point or one-point finite-difference (FD) zeroth-order estimation is not fair, because the proposed method relies on a Gaussian process assumption, whereas existing FD types of methods do not. In my opinion, the authors should provide a more detailed comparison to existing Bayesian optimization based methods, which, however, seems to be quite missing in the current writing. Correct me if I miss anything for the reading. \n\n2. There are no sufficient algorithmic and theoretical comparison to GP or Bayesian optimization based methods. Only some are found at the end of the appendix. Therefore, the technical novelty of this work is not that clear to me. For example, what is the algorithmic difference compared to existing GP or Bayesian optimization methods? What is the theoretical challenges compared to such studies? Are there any new developments that cannot be handled by existing analysis? These points so far are not that clear to me. In terms of the theoretical analysis (e.g., Theorem 3), is this the first convergence result for GP based optimizer? How does this bound compare to existing GP-based algorithms? What is the best possible tradeoff achieved by $V$ even in this worst-case analysis? It seems Theorem 3 may not be easy to convey this due to the complicated form. \n\n3. More experiments on the comparison to GP or Bayesian optimization based approaches should be provided. For example, for the non-differentiable metric optimization experiment, why no GP or Bayesian optimization algorithms are included for comparison? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this work is overall good, but the clarity and originality need to be improved. ",
            "summary_of_the_review": "Overall, I like the approach and the idea of using GP to the zeroth-order optimization, which enables to explore more trajectory-based information to accelerate the convergence. The algorithms seem to be effective in the experiments.  However, the technical and algorithmic novelty is not clear to me. I tend to marginally reject this work, but I am very willing to adjust my score based on other reviewers' comments and the authors' response. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6273/Reviewer_wx9Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6273/Reviewer_wx9Z"
        ]
    },
    {
        "id": "b8mcTf2-qr",
        "original": null,
        "number": 2,
        "cdate": 1666574370775,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574370775,
        "tmdate": 1666574370775,
        "tddate": null,
        "forum": "n1bLgxHW6jW",
        "replyto": "n1bLgxHW6jW",
        "invitation": "ICLR.cc/2023/Conference/Paper6273/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a zeroth order method that reduces the number of queries made as compared to methods that use finite difference to estimate the gradient. The authors propose using Gaussian Processes to estimate the trajectory of observations, thus giving them access to the gradient at any given point. Given the assumption that the underlying blackbox function is sampled from a gaussian process, the authors prove convergence by showing that the gradient estimation error is non-increasing as the number of queries go up.",
            "strength_and_weaknesses": "Strengths\n\n- Clear writing \n- Simple proposed approach\n\nWeaknesses\n\n-  Might have limited novelty ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear to read, high in quality, fairly novel, and reproducible.",
            "summary_of_the_review": "The paper is clearly written, nicely motivated and well structured. My main concern/question is around the assumption that f is sampled from a GP. How restrictive is this assumption in general? It seems like most heavy lifting in terms of providing a benefit over prior work is due to this assumption, while the algorithm itself is not entirely novel. Would this be an accurate understanding? Can the authors comment a bit more on this? Since the topic is quite far from my area of expertise, I am unable to provide more critical feedback than the above. Having said that, I think the paper makes a solid contribution and the paper is well drafted. Hence, I am leaning towards acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6273/Reviewer_9AgD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6273/Reviewer_9AgD"
        ]
    },
    {
        "id": "_cnZ16zO3n",
        "original": null,
        "number": 3,
        "cdate": 1666624784988,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624784988,
        "tmdate": 1666624784988,
        "tddate": null,
        "forum": "n1bLgxHW6jW",
        "replyto": "n1bLgxHW6jW",
        "invitation": "ICLR.cc/2023/Conference/Paper6273/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Based Gaussian process, this paper develops a zeroth-order optimization algorithm which requires fewer function queries to estimate the gradient than previous zeroth-order methods. Additionally, a so-called dynamic virtual update schemes is incorporated. Theoretically, the proposed method is shown to obtain gradient with exponentially diminishing error; and the convergence result of the algorithm is attained. Empirical studies with synthesized data and real-world data corroborate the theoretical analysis and demonstrated the superior performance of the presented approach over existing zeroth-order optimization approaches.",
            "strength_and_weaknesses": "Strength:\n1.\tA novel algorithm is developed.\n2.\tThe theoretical analysis is solid--- the bound of the gradient error is given, and the convergence rate is obtained.\n3.\tEmpirical results show the proposed algorithm outperforms previous ones by a substantial margin.\nWeaknesses:\n1.\tRelated studies on advanced zeroth-order methods that aim to reduce the gradient error or improve query efficiency are not introduced.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The code to reproduce the numerical results is currently unavailable.",
            "summary_of_the_review": "1.\tFor different problems, is there any guidance for the selection of confidence threshold c. \n2.\tMore intuitive explanations on why the gradient error decreases at exponential rate are expected.\nThe code to reproduce the numerical results is currently unavailable.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6273/Reviewer_Z3aD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6273/Reviewer_Z3aD"
        ]
    }
]