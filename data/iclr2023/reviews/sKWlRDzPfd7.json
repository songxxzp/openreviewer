[
    {
        "id": "WJZwvBVPiH",
        "original": null,
        "number": 1,
        "cdate": 1666392357803,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666392357803,
        "tmdate": 1666392357803,
        "tddate": null,
        "forum": "sKWlRDzPfd7",
        "replyto": "sKWlRDzPfd7",
        "invitation": "ICLR.cc/2023/Conference/Paper3995/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates learning training agents in games where nature samples the games initial state before play. In such a games certain strategies may be at an advantage when the settings play into their strengths. Present methods consider the performance of each strategy with respect to an approximate expectation of the initial state sampling distribution. This work looks at game solving while taking into account the distribution of game settings. Specifically, they propose a population based-training regime where iteratively a policy is optimized to perform well against each coplayer strategy in the worst-case (where buffers of game configurations are used to approximate the game distribution). They demonstrate their approach on a discrete laster-tag gridworld and a continuous racing game.",
            "strength_and_weaknesses": "**Strengths**\n - I have never seen an approach that explicitly accounts for regret with respect to nature and I think that the idea is straightforward and nice.\n - The authors include reasonable ablations of the game instance generation methods as evidence supporting that regret-based prioritization is empirically advantageous. \n\n\n**Weaknesses**\n - While the idea has a nice intuitive appeal, it's not clear to me that maintaining buffers for the highest-regret game instances is the most sensible. If these settings rarely if ever occur in practice (or under the evaluation distribution) then your learned strategy may compromise its performance in general for these rare cases. \n - Could the authors explain how the proposed theoretical guarantees of Maestro (convergence to BNE) hold when it relies on an expectation over game instances and in practice a highly bias discrete sample of these instances is accounted for? It seems to me that there is an important gap and that the authors should make this clear within the paper.\n - The authors name their algorithm in the style of Policy-Space Response Oracles (PSRO) a general game solving algorithm. Despite this, only PSRO's predecessors are extended (FP, FSP) to adtop their extension when PSRO can naturally be extended in the same way. This is an odd gap that makes me suspicious as a reader. Did the authors try this and it not work? \n - Minimum expect return is an atypical measure of performance for the game solving methods. Could the authors please comment on the regret and the coplayer return as it relates to the relative performance of each method. It seems this measure may lend advantage to the method introduced as it optimizes for the worst-case game instances.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The prose is of average quality and clarity. The approach of considering co-player and game instances together has merit in novelty. \n\nDetails for training the policies are not included so the work would not be reproducible without the inclusion of the source code.",
            "summary_of_the_review": "The paper introduces an intuitively nice idea and offers some reasonable empirical analysis of the method. However, I am not convinced that the theoretical results apply to the method, and am suspicious that about the empirical results and that the algorithm's namesake is only a footnote at the end of the paper. \n\nI think with some minor edits and additional empirical results this paper could be ready for publication. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3995/Reviewer_AFZu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3995/Reviewer_AFZu"
        ]
    },
    {
        "id": "yUHUB-odM2",
        "original": null,
        "number": 2,
        "cdate": 1666654314956,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654314956,
        "tmdate": 1666654314956,
        "tddate": null,
        "forum": "sKWlRDzPfd7",
        "replyto": "sKWlRDzPfd7",
        "invitation": "ICLR.cc/2023/Conference/Paper3995/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Maestro, an open-ended multi-agent learning method that extends UED to the multi-agent setting, combining the benefits of both environment and co-player autocurricula approaches. The work claims to be the first to adequately address the dependency between co-players and environment design when shaping the autocurricula. The theoretical results presented show that Maestro both produces a regret maximizing distribution over environments and co-players and attains Nash Equilibrium in each supported environment. The authors pit Maestro against a cross-product of relevant environment/co-player autocurricula works in the literature, and demonstrate in two diverse environments (grid-world w/ sparse rewards and continuous control w/ dense rewards) that Maestro performs the best in cross-play. Additional supporting results are presented, including analysis of the regret landscapes (highlighting the importance of a curricula over the joint space), evaluation versus specialist agents (demonstrating Maestro\u2019s robustness), and analysis of the emergent curriculum (highlighting interesting qualitative differences between Maestro\u2019s curricula versus the baselines\u2019).  ",
            "strength_and_weaknesses": "Strengths: \n\n* The paper is exceptionally well-written, well-organized, and clear. Care was clearly taken in the formatting too, and the paper is even aesthetically pleasing.  \n\n* The method is well-motivated, and the discussion surrounding related works is comprehensive.\n\n* The method details are described clearly, and useful reference points are provided (e.g. Maestro being a variant of PFSP in fixed singleton environment setting).\n\n* The theoretical results appear to be useful and correct (though I did not read in detail).\n\n* The \u201cillustrative example\u201d is very useful and clearly expresses the motivation. \n\n* The empirical results are compelling and comprehensive, and all of the appropriate baselines (to the best of my knowledge) are included. The results deliver what the method promises, and they also provide more insight into the nature of the problem (via the analysis of the regret landscape) and the nature of the emergent curriculum. The emergent curriculum analysis is very cool (e.g. the wall percentage increasing and grid size decreasing).\n\nWeaknesses:\n\n* None major. \n\n* Typo in middle of 3.2.1 (random section symbol). ",
            "clarity,_quality,_novelty_and_reproducibility": "* This paper is of high-quality and the exposition is exceptionally clear. \n\n* The method presented appears to be novel, though it primarily serves to combine ideas in prior co-player/environment autocurricula works. The theoretical results and empirical results appear to be novel.\n\n* The paper appears to be reproducible from the details in the paper alone, but the authors also promise to release code along with the camera-ready version.",
            "summary_of_the_review": "I vote for this paper\u2019s acceptance. The method is well-motivated, the writing is clear, the results are interesting and compelling, and the findings and resources will likely be of use to the field. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3995/Reviewer_wyTE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3995/Reviewer_wyTE"
        ]
    },
    {
        "id": "wC7fc5foKZ",
        "original": null,
        "number": 3,
        "cdate": 1667548571073,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667548571073,
        "tmdate": 1667549352880,
        "tddate": null,
        "forum": "sKWlRDzPfd7",
        "replyto": "sKWlRDzPfd7",
        "invitation": "ICLR.cc/2023/Conference/Paper3995/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper extends the framework of unsupervised environment design to multi-agent setting and demonstrate its effectiveness on two 2-player competitive games. ",
            "strength_and_weaknesses": "Strengths:\n- Extending the UED to multi-agent setting is an important problem to tackle; and for the chosen environments, the performance of the proposed algorithm was compared with all the relevant work.\n\nWeaknesses:\n- While the authors claim that the proposed framework is applicable to all multi-agent RL settings, the results were shown only on 2-player competitive games. It would add more value to see at least one cooperative setting and one multi-player (more than 2 players) setting.\n- No analysis was provided on the memory complexity or time complexity of the proposed algorithm. These are important because the algorithm maintains an independent replay buffer for each player and with increasing number of players, it also becomes unclear of how the algorithm can effectively simultaneously choose the best policy for each player and environment parameters. Without this analysis, it is unclear if the proposed framework and the algorithm can extend beyond the 2-player setting.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\n\nQuality: Good\n\nNovelty: This is a straight forward extension to the PLR and Robust PLR (Jiang et al, 2021) \n\nReproducibilty: Ok",
            "summary_of_the_review": "This paper addresses an important problem of open-ended learning in a multi-agent setting. While the proposed algorithm seems like a straight forward extension of existing algorithms (PAIRED, PLR, Robust PLR), the authors showed a convincing performance of the proposed algorithm in comparison to all the relevant work. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3995/Reviewer_GPFe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3995/Reviewer_GPFe"
        ]
    }
]