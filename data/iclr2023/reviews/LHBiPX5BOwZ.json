[
    {
        "id": "pkBMJ7f4hQo",
        "original": null,
        "number": 1,
        "cdate": 1666351629459,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666351629459,
        "tmdate": 1666351629459,
        "tddate": null,
        "forum": "LHBiPX5BOwZ",
        "replyto": "LHBiPX5BOwZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1620/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes several techniques to benefit the pre-training of MAE. However, it seems like an ensemble of different tricks. The contributions should be further highlighted. ",
            "strength_and_weaknesses": "Weaknesses:  \n(1) The writing of the paper is a little messy. The authors should highlight their contributions clearly in the Introduction. The proposed method is more like a combination of different components or tricks.  \n(2) Weak performance improvements. The proposed method only brings slight improvements (about 0.3%) over MAE in downstream classification, detection and segmentation tasks, which is far from sufficient to prove its effectiveness.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The current version of the paper has not ready for publication.",
            "summary_of_the_review": "I am inclined to reject this paper since the contribution and experiment problems, please see the weaknesses.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1620/Reviewer_6ZYw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1620/Reviewer_6ZYw"
        ]
    },
    {
        "id": "nODYTEY7yM6",
        "original": null,
        "number": 2,
        "cdate": 1666488541481,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666488541481,
        "tmdate": 1666488541481,
        "tddate": null,
        "forum": "LHBiPX5BOwZ",
        "replyto": "LHBiPX5BOwZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1620/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes several modifications such as adding dropout, moving the position of masked tokens, which provides incremental performance gain over MAE across vision tasks.",
            "strength_and_weaknesses": "Strength: consistent performance gain (over different tasks) achieved by simple modifications (i.e. dropout, position of masked tokens)\n\nWeakness: the major concern is novelty. Although incremental improvement is achieved, adding dropout and moving masked tokens are far from innovation, especially for top conference like ICLR. ",
            "clarity,_quality,_novelty_and_reproducibility": "I believe that it would not be hard to reproduce the experiments in this paper. But this paper is lack of novelty, introducing negative impact of the paper quality.",
            "summary_of_the_review": "Reject is rated mainly due to the lack of novelty. It is not surprising to achieve incremental improvement by leveraging existing techniques.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1620/Reviewer_GC97"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1620/Reviewer_GC97"
        ]
    },
    {
        "id": "0T9esgPxt_",
        "original": null,
        "number": 3,
        "cdate": 1666623419393,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623419393,
        "tmdate": 1666623419393,
        "tddate": null,
        "forum": "LHBiPX5BOwZ",
        "replyto": "LHBiPX5BOwZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1620/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents three techniques on masked autoencoder (MAE)-based pre-training for downstream tasks.\nThe first is to apply dropout to the attention modules of ViT.\nThe second is a modification of the image normalization method from using per-patch statistics to using per-dataset statistics.\nThe third is to adjust the masking ratio depending on model sizes.\nExperimental results present that the proposed method can reduce the pre-training iterations of MAE with comparable accuracies in the downstream tasks.\n",
            "strength_and_weaknesses": "The proposed techniques are reasonable.\nHowever, the reviewer has several concerns.\n\nFirst, the novelty of the proposed techniques is insignificant.\nDropout and image normalization using per-dataset statistics are well-known approaches to improve accuracy.\nA smaller masking ratio for a smaller model is straightforward.\n\nSecond, the performance improvement is marginal.\nIn Table 2, the proposed method improves the Top-1 accuracy by 0.3% from the baseline (MAE (our impl.)).\nMore importantly, Table 5 presents that the adjusted masking ratio achieves Top-1 accuracy improvement by 0.2%, indicating that the other techniques contribute to only 0.1%.\n\nThird, the clarity of this paper is limited.\nThe equations for the ablation methods in Figure 2 are not described.\nThe meaning of MAE (our impl.) with 85.3/.4 (/paper) in Table 2 is unclear.\n\nLastly, experiments are limited to present the effectiveness of the proposed method.  \nAblation studies on pre-training iterations, the proposed techniques, and the masking ratio will be helpful.\nAnalysis of the relation between image reconstruction performances and downstream task accuracies will be interesting.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The definitions of global RGB normalization and global patch normalization in Table 1 are missing, making the contribution difficult to understand at the beginning of this paper.\n\nSection 2.1 is limited to explaining the proposed technique by itself.\nThe reviewer had to find the ViT paper to understand implementation details.\n\nFigure 1 is hard to understand what the proposed method is.\n",
            "summary_of_the_review": "The proposed three techniques are reasonable to train accurate MAE.\nHowever, the novelty, accuracy improvement on downstream tasks, and experiments are limited.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1620/Reviewer_sWps"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1620/Reviewer_sWps"
        ]
    },
    {
        "id": "hG1eqFRGuvD",
        "original": null,
        "number": 4,
        "cdate": 1667219049181,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667219049181,
        "tmdate": 1667219049181,
        "tddate": null,
        "forum": "LHBiPX5BOwZ",
        "replyto": "LHBiPX5BOwZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1620/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The submission manifests several tricks in order to potentially improve the original MAE's recipe. The tricks include: 1) addition dropout in the attention layers; 2) a study on the normalization targets; and 3) intermediate mask tokens. The results are slightly improved with the authors' own implementation.",
            "strength_and_weaknesses": "Strengths:\n- As an important baseline nowadays for self-supervised learning, improving the recipe of MAE (and potentially can replace the original MAE recipe) is useful and of significance.\n- Some explorations here, while minor, are of interest. For example it delves into different ways of normalizing the reconstructing targets and shows their difference. Indeed, it is unclear why normalization can help the down-stream performance.\n\nWeaknesses:\n- It is unfortunate, but I have to say that after reading the paper, I believe a reasonable user of MAE will still use the original MAE recipe. So the primary goal of the paper is not achieved. There are many reasons for this, for example: 1) the final performance is still not beating MAE significantly. There could be some reproducibility issues, but to claim \"robust and effective\", just with improvements that close the \"reproducibility\" gap is definitely not enough. 2) Right now the experimental section is not complete, e.g., I could not find the influence of final ImageNet accuracy with different ways or normalization, so this means it is not conclusive. 3) In order to show the recipe is indeed robust, I would very much like to see its effect on even larger models (ViT-H). 4) How robust is the technique in terms of compositionally? \n- I can tell that the paper is done in a rush. The experiments are not yet complete, there are quite a few broken (and lengthy) sentences, and lots of space is devoted to less important things (I believe for a report like this, an introduction should just be half a page long -- 1.5 pages are too long and way more space can be devoted to experiments if they are done. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: I think I can get the gist of the paper, and I can understand the tables and figures reasonably well. However, I think quite a few more iterations on the writing is definitely helpful (especially with more results), for example to trim the length and the paper more punchy and less verbose.\n\nQuality: It's hard for me to judge the quality in the current format, as the experiments are not yet complete to me (e.g., larger models, fine-tuning results for all the techniques, their compositionally, also showing how it can be called \"robust\"). \n\nNovelty: The main contribution of the paper, in its best format, is to present improved baselines on MAE. Dropout on attention, trying different normalization techniques, revisiting masked tokens in the encoder do not sound too novel to me. However, as the authors also admit upfront, I think it is fine with limited novelty if the goal is to find a better recipe.\n\nReproducibility: Since the proposed techniques are relatively easy to implement, I would say it is less of a problem to get to the results from another institution, the real problem is to show that the techniques are improving the baseline recipe \"significantly\" and \"robustly\". ",
            "summary_of_the_review": "Overall, this is not yet a complete work to me. After reading the paper, I don't think as a reasonable reader, one would be excited enough to switch the recipe from the original MAE one to the current one. I think it would be great for the authors to think from this perspective, and show more \"exciting\" results, findings etc. that can actually achieve the goal of replacing the baseline MAE. Therefore, I am firmly on the rejection side and would recommend a \"robustly\" and \"effectively\" improved version could be submitted to future venues.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1620/Reviewer_Ukam"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1620/Reviewer_Ukam"
        ]
    }
]