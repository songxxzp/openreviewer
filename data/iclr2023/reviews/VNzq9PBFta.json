[
    {
        "id": "MeDr5iMVmTd",
        "original": null,
        "number": 1,
        "cdate": 1666445862699,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666445862699,
        "tmdate": 1666445862699,
        "tddate": null,
        "forum": "VNzq9PBFta",
        "replyto": "VNzq9PBFta",
        "invitation": "ICLR.cc/2023/Conference/Paper145/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new data structure to support lossless compression of neural networks and show the effectiveness and efficiency of inference workflows in computer vision tasks.   \n",
            "strength_and_weaknesses": "Strengthes:\n\n+ Lossless compression method can be widely adopted in various inference workflows without considering the drawbacks of performance.\n\n+ The combination with other lossy compression method is flexible. \n\nWeaknesses:\n\n- The presentation of the technique part is difficult to track. It is casual in terms of lack of enough formulation. It is also not-intuitive that those design decisions are presented with enough justification.\n\n- The experimental section is limited to computer vision tasks, where the most memory intensive workflow is NLP models such as Bert and GPT in language modeling. ",
            "clarity,_quality,_novelty_and_reproducibility": "The technique part is difficult to track. It is casual in terms of lack of enough formulation. It is also not-intuitive that those design decisions are presented with enough justification. \n\nThe novelty of this paper is not clear. \n\nThere is a lack of code release for reproducibility or a plan to do so.  ",
            "summary_of_the_review": "The novelty of this paper is limited; the presentation of the technical section should be further polished; Although it shows a performance boost over computer vision tasks, they are inappropriate benchmarks to show the efficiency of model compression for inference based on the platform where the experiments have been conducted. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper145/Reviewer_PvEU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper145/Reviewer_PvEU"
        ]
    },
    {
        "id": "o_aAf86T52i",
        "original": null,
        "number": 2,
        "cdate": 1666578506076,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578506076,
        "tmdate": 1666578506076,
        "tddate": null,
        "forum": "VNzq9PBFta",
        "replyto": "VNzq9PBFta",
        "invitation": "ICLR.cc/2023/Conference/Paper145/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel method for lossless compression of DNNs which enables faster and memory-efficient inference. Specifically, it applies the succinct data structure to do the compression of DNNs. The main benefit of succinct data structure is that it supports queries without decompressing the compressed representations, thus being faster than previous model coding approaches. Experiments show better speedup ratio on both pruned models and quantized models with several different DNN architectures.",
            "strength_and_weaknesses": "## Strength\n1. The proposed method can enable faster inference without influencing the model's performance.\n2. It can be applied together with existing methods on model pruning and quantization.\n3. The use of succinct data structure is well motivated. \n4. Experiments are extensive and the speedup improvements are consistent.\n\n## Weaknesses\n1. The description of how to apply the succinct data structure is not very clear. Having an algorithm or pseudo code would be better.\n2. It seems that certain software library is utilized for better performance of the proposed method on the hardware. This needs more detailed explanation. Are the reported baseline methods results also based on comparable libraries for performance/speed optimization?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: fair. Needs more detailed steps of the algorithm\nQuality: good\nNovelty: good",
            "summary_of_the_review": "Overall the proposed method is novel and the experiments do support the speedup claim. Some details need more explanations.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper145/Reviewer_BLqS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper145/Reviewer_BLqS"
        ]
    },
    {
        "id": "m_NaYy7Jgc",
        "original": null,
        "number": 3,
        "cdate": 1666605076665,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605076665,
        "tmdate": 1666605076665,
        "tddate": null,
        "forum": "VNzq9PBFta",
        "replyto": "VNzq9PBFta",
        "invitation": "ICLR.cc/2023/Conference/Paper145/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose succinct data structures and a new formulate, Runtime-Accessible Sequence, to efficiently utilize new data structures. RAS is provided with two versions (element-wise and block-wise) with synergistic effects with pruning and quantization. The authors provide experimental results to support the claims with improved compression ratio and speedup.",
            "strength_and_weaknesses": "*Strength\n- It is interesting to apply new data structures to enable practical approaches of widely adopted model compression techniques, such as pruning and quantization.\n- Compression ratio and inference speed can be improved by more than 1.2X and 4-9X, respectively, for ResNet-50, ResNet-100, and VGG DeiT-B.\n\n*Weakness\n- Detailed (mathematical) analysis on succinct data structures is missing while exploring (and comparing) other data structures needs to be included.\n- Ablation study for Table 3 needs to be included. Since OPQ has been already applied as a pre-processing, it is difficult to estimate whether the overall improvement on CR or speedup is due to OPQ or RAS.\n- What are the inherent advantages of RAS? How can RAS enhance memory saving?\n- What is \"specialized execution pipeline\"? If it is based on NVIDIA CUDA Fortran, at least some introduction of such engines need to be introduced.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper does not suggest new model compression algorithms that might be interesting to ML community.\n- If the major contributions exist in data structures and a new formulate, it would be better to submit the manuscript to relevant conferences that handle those issues. The audience wanting to understand this manuscript would require background not very well known to ML community.\n- Detailed profiling and analysis on how those CR and speed-up can be obtained should be provided. Especially for speed-up, results should be highly dependent on the hardware selection.\n- Novelty seems to be quite limited.",
            "summary_of_the_review": "- More background and related work need to be included.\n- This reviewer is not sure whether ICLR is the right venue for this manuscript.\n- Both mathematical and practical analysis need to be presented.\n- What about other tasks, such as NLP? Is the proposed method specific to vision tasks?\n- Overall, it is difficult for this reviewer to see how the proposed method can lead to \"faster and more memory-efficient\" inference.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper145/Reviewer_MXqh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper145/Reviewer_MXqh"
        ]
    },
    {
        "id": "06MmHrPReU6",
        "original": null,
        "number": 4,
        "cdate": 1666638111105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638111105,
        "tmdate": 1666638111105,
        "tddate": null,
        "forum": "VNzq9PBFta",
        "replyto": "VNzq9PBFta",
        "invitation": "ICLR.cc/2023/Conference/Paper145/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In  this paper,  the succinct data structures, which supports fast queries without decompressing the compressed representations, is utilized to do deep neural network inference. Also they propose a scheme to enable mixed-formulation inference for different layers.  The experimental results show that the proposed method not only can achieve better speedup and memory efficiency on both uncompressed and compressed models while preserving the accuracy, but also outperform the sota model coding approaches.\n",
            "strength_and_weaknesses": "Strengths:\n1. the paper combine a new data structure with the neural network inference and get good results.\n2. the paper propose a flexible schema on where the data structures is used for neural network to further optimize the memory and time.\n3. The benefits are verified on different types of popular neural networks including resnet and transformer.\nWeaknesses:\n1. Lack some descriptions on how this data structure is applied to neural networks? Are both the neural network weights and activations represented by the succinct data structure?\n2. Can you provide more details on how to use pytorch with the succinct data structure? do you rewrite the pytorch kernels to directly use the succinct data structure, or there still need to convert to tensor representation before feeding into pytorch kernel?\n3. The paper gives time/memory complexity of the succinct data structure, it will be better to compare this to the corresponding nn complexities for each kind of neural network(layer).\n4. the speedup improvement on the uncompressed model is not significant compared to the compressed model?  Is there any formal explanation on this? such as vocabulary size change and so on?\n5. It\u2019s not very clear where the speedup comes from, like more efficient kernel, cache, data communication or anything else? Can you provide a theoretical analysis on why this new data structure runs faster compared to common nn libraries like pytorch if this is the first one to use it for nn? This might be related to issue 2.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has a good novelty to be the first to combine succinct data structure with nn inference, and also has comprehensive experimental studies to prove the effectiveness. It would be better to provide more descriptions on  theoretical justification and experimental details.",
            "summary_of_the_review": "This paper proposed a good idea to apply the succinct data structure to the neural network inference. By combining with using the mixed schemas for different layers, it shows significant benefits on both time and memory requirements on different types of neural networks.  It would be better to provide more comparisons for the theoretical time/memory comparison instead of the experimental findings to justify when it will be suitable.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper145/Reviewer_DFkG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper145/Reviewer_DFkG"
        ]
    }
]