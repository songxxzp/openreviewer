[
    {
        "id": "OYEYgOJxkk",
        "original": null,
        "number": 1,
        "cdate": 1666005478706,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666005478706,
        "tmdate": 1666005478706,
        "tddate": null,
        "forum": "ja4Lpp5mqc2",
        "replyto": "ja4Lpp5mqc2",
        "invitation": "ICLR.cc/2023/Conference/Paper6102/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper combines the idea of 1) test-time attack by flipping bits of models and 2) syntactic-based triggers,  to achieve good\nattack results with only a few sampled test data.\n",
            "strength_and_weaknesses": "**Strenghts**\n* The experimental results show that the proposed method outperform the existed test-time attack method by a large margin.\n* The setting of test-time attack is more realistic than training-time attack to me.\n\n**Weaknesses**\n* The experiments are done only with syntatic trigger, which makes me wonder whether the major improvements come from the strong\n(originally proposed ) training-time attack baseline, i.e. Hidden Killer.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and somewhat novel. \n**minor problems**\n* Section3.1 yi/xj are corresponding labels -> yi,yj are corresponding labels\n* Section 3.2, the performance of model could be moved to section 4/5.\n* The usage of font \\mathbb for dataset and model seems weird",
            "summary_of_the_review": "Overall, the paper is well written and have solid experimental contribution. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6102/Reviewer_ecL7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6102/Reviewer_ecL7"
        ]
    },
    {
        "id": "bA6KoJLJia2",
        "original": null,
        "number": 2,
        "cdate": 1666541350707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541350707,
        "tmdate": 1668712938543,
        "tddate": null,
        "forum": "ja4Lpp5mqc2",
        "replyto": "ja4Lpp5mqc2",
        "invitation": "ICLR.cc/2023/Conference/Paper6102/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies backdoor insertion attacks in pretrained language models. In this attack, neural network weights are modified such that they produce incorrect outputs for some targeted set of inputs (like a particular syntactic structure in this paper), but correct outputs for rest of the inputs. This paper presents a new attack called \"trojtext\", which leverages only a small amount of data to insert backdoors into BERT and XLNET. The key ideas in TrojText are - (1) training both logits and encoder representations to insert backdoors; (2) using accumulated gradient ranking and network pruning to modify only a small number of network weights.\n\nExperiments conducted on 3 text classification datasets confirm the efficacy of the proposed approach. A defense mechanism is against accumulated gradient ranking is also briefly discussed.",
            "strength_and_weaknesses": "**Strengths**\n\n1. The paper studies backdoor insertion into neural networks, a well established threat model in prior work. Moreover, the paper's method only modifies a few parameters of the network and uses a small dataset to achieve backdoor attacks.\n\n2. The paper performs experiments on 3 datasets with 2 pretrained LMs, and the experimental setup and metrics seem solid. The proposed algorithm significantly outperforms prior baselines. Ablation studies confirm the efficacy of each part of the proposed algorithm.\n\n**Weaknesses**\n\n1. The paper would be much stronger with newer / larger LMs. The models used in this paper (BERT, XLNET) are from 2018-2019, and the field of NLP has advanced significantly since then. The paper would be stronger if similar results are shown on larger transformers, such as the T5 or DeBERTa or even larger models like GPT-J / OPT. It may be harder to insert backdoors in these networks. Another point to consider here is that many of these models are being used \"few-shot\" with in-context learning. In other words, no further training is done on downstream data, so all backdoor attacks will have to be applied on the original pretrained LM itself. Do backdoors generalize across different tasks few-shot? Does insertion of backdoors targeted at one task affect performance on other tasks? Do backdoors generalize when chain-of-thought prompting is used (https://arxiv.org/abs/2201.11903), because LMs need to provide rationales for their judgments?\n\n2. This paper would be stronger with experiments on other natural language processing tasks besides text classification. Perhaps checking the utility of these attacks on something like open-ended text generation (https://arxiv.org/abs/1908.07125) will be interesting. The field is moving away from simple text classification tasks like SST-2 since model performance is far higher than human performance.\n\n3. Due to weakness #1,#2, it seems like the the main contribution of the paper is a new algorithm in a well-established experimental setup. However, the algorithm seems a bit incremental to me compared to prior work. Network pruning is a well-established technique in ML (https://arxiv.org/pdf/2003.03033.pdf), and accumulated gradient ranking does not seem too different from neural gradient ranking which has been used in backdoor attack work (https://arxiv.org/pdf/1909.05193.pdf). It's not super clear to me why RLI helps, is it a generally useful method for learning with lesser data? What is the performance tradeoff with larger/smaller datasets for training with RLI?",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity*: Mostly clear, but I did need to read the paper twice to understand details. See specific issues below.\n\n*Quality*: Good experimental setup / metrics for the current set of models / datasets (but see weakness #1/2)\n\n*Novelty*: Moderate. The proposed algorithm seems like a combination of ideas in previous works.\n\n*Reproducibility*: Should be reproducible with released code.\n\n*Specific issues*\n\n1. Figure 1: wights --> weights  \n2. Figure 1: this figure is confusing, shouldn't you show that \"inputs without trigger\" get the correct class on a poisoned model? or is the bit flipping dynamic and only when the trigger is detected?  \n3. Section 3.2: Move equation 1 to the end, after L_l and L_r are defined.  \n4. (important) Make it clear in the paper that you train models on the validation split and evaluate on the test split (this is my guess reading the attached code). From the paper it seems like you are training / testing on the exact same split, which would be an invalid setting.  \n5. How many datapoints do you train on? Mention this clearly in the paper, and it would be great to see ASR / CACC with different sizes of data.  \n6. Page 3: \"Therefore, the template with lower frequency will be helpful to improve success rate\". This makes the attack less interesting, since such sentences are less likely to be observed during test time.",
            "summary_of_the_review": "Overall the experimental setup is good, and it's cool that backdoors can be inserted without a large dataset. However, no new setting (in terms of models / tasks) was analyzed (weakness 1,2) and the proposed approach seems a bit incremental compared to prior work (weakness 3). The paper would be much stronger with experiments on larger pretrained models, an exploration of backdoors in few-shot incontext learning settings, and experiments on non-classification datasets. Hence I'm currently leaning reject, but will re-consider after reading the rebuttal.\n\n-----\n\n**After rebuttal**: Thank you for the very detailed response! I've decided to increase my score to 6 due to the extra experiments on DeBERTa and some more clarification on the novelty of the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6102/Reviewer_YubY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6102/Reviewer_YubY"
        ]
    },
    {
        "id": "qaVLZdz6bI",
        "original": null,
        "number": 3,
        "cdate": 1666910672230,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666910672230,
        "tmdate": 1666983057681,
        "tddate": null,
        "forum": "ja4Lpp5mqc2",
        "replyto": "ja4Lpp5mqc2",
        "invitation": "ICLR.cc/2023/Conference/Paper6102/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "What is the goal of the paper?\n\n* Test-time invisible textual trojan insertion\n\nWhat has been done before?\n\n* Current Trojan attacks with syntactic-structure triggers are highly dependent on a large corpus of training data which significantly limits the feasibility of these attacks in real-world NLP tasks. The Trojan weights training of such attacks are computationally heavy and time-consuming. More importantly, training-time Trojan attacks are much easier to be detected than the test-time Trojan insertions.\n\n* Proposed approach - TrojText is a test-time invisible textual Trojan insertion method - a more realistic, efficient and stealthy attack against NLP models without training data.\n\nWhat are the contributions of the paper?\n\n* Proposed TrojText to study whether the invisible textual Trojan attack can be efficiently performed without training data in a more realistic and cost-efficient manner\n\n* Proposed a novel Representation-Logit Trojan Insertion (RLI) algorithm to achieve a desired attack using smaller sampled test data instead of large training data.\n\n* Proposed accumulated gradient ranking (AGR) and Trojan Weights Pruning (TWP) to reduce the tuned parameters number and the attack overhead\n\n* Performed extensive experiments of AG\u2019s News, SST-2 and OLID on BERT and XLNet. \n\n\n",
            "strength_and_weaknesses": "Strengths\n\n* Proposed a more realistic, efficient and stealthy attack against NLP models without training data.\n\n* Described a potential defense for the proposed attack\n\nWeaknesses\n\n* Paper is a difficult read\n\n* Insufficient details or code available for reproducibility \n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is nit unclear to read\n\nWork looks novel\n\nInsufficient details or code available for reproducibility ",
            "summary_of_the_review": "Proposed approaches look novel but presentation of paper could be more clear. Reproducibility aspect of the paper looks weak.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6102/Reviewer_oFGQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6102/Reviewer_oFGQ"
        ]
    },
    {
        "id": "OJMeTE34q0n",
        "original": null,
        "number": 4,
        "cdate": 1666982831479,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666982831479,
        "tmdate": 1670776939727,
        "tddate": null,
        "forum": "ja4Lpp5mqc2",
        "replyto": "ja4Lpp5mqc2",
        "invitation": "ICLR.cc/2023/Conference/Paper6102/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates bit-flip trojan attacks on text models with syntactic triggers. Several ideas are proposed for improving the effectiveness of these trojans. In experiments, this increases attack success rate (ASR) while having less of an effect on clean accuracy compared to baselines. Ablations confirm that modifying more bits leads to higher attack success rate. A defense is also proposed that can make models less susceptible to bit-flip attacks.",
            "strength_and_weaknesses": "Strengths:\n- Memory editing attacks are an important threat model\n- The proposed methods enable reaching higher ASR with a smaller decrease to clean accuracy\n- A useful defense is proposed in Section 5.3 that seems to work well against the proposed bit-flip attacks\n\nWeaknesses:\n- I don't buy the distinction between training-time and test-time attacks. Surely if one can modify the model weights at test time via rowhammer to accomplish some goal, then this would be a form of training/tuning the network. It seems like the actual difference is that \"test-time\" or bit-flip attacks try to minimize the number of parameters that are changed, which makes sense for a rowhammer attack vector. This matters, because in the related work the authors mention that there are many new trojan detection techniques for text models (e.g., PICCOLO), and they claim that these detectors work much better on \"training-time\" attacks. However, I don't see any reason to believe this, since the authors do not include experiments with these detectors, and it's unclear that modifying fewer parameters results in a less detectable attack.\n- Another issue with the test-time attack setting (or at least the version of it explored in this work) is that the trojan is inserted using data from the test set!!! This is akin to training on the test set, as it would artificially inflate ASR on the test set examples that were used for training the trojan. In reality, the victim model would be used on new samples from the data distribution that were not used for inserting the trojan, which is precisely why test sets are held out. (EDIT: One of the other reviewers pointed out that, based on the submitted code, it is likely that the authors used examples from the validation set. Could the authors confirm this?)\n- In Algorithm 1, there are multiple epochs, and each epoch consists of editing all parameters with a distance greater than e to the corresponding benign model parameters. This doesn't place a constraint on the number of parameters edited, which seems more relevant for the memory editing attack vector. Also, the parameters that are edited could change in each epoch, which seems unrealistic. Wouldn't this allow you to arbitrarily change the entire model? It would be more realistic to fix the parameters that can be changed at the start of training and to not change them across epochs.\n\n\nOther points:\n- \"trigger synthesizing is not applicable to textual model due to non-differentiable text tokens\" There has been some work on trigger synthesis for textual trojans, e.g., the Neural Cleanse experiments in \"Trojaning Language Models for Fun and Profit\".\n- RLI is called a contrastive loss, but it would be more accurate to call it an MSE loss, since it is just the MSE between two representations.\n- The proposed defense is quite interesting. If this is novel, it should be expanded much more.",
            "clarity,_quality,_novelty_and_reproducibility": "Section 3.2 could be made more clear. In particular, it's not very clear what \\hat{x} is based on the text alone. Is it the example in the training set with the max confidence prediction for y*? Are these examples precomputed for a fixed clean model for each y*?\n\nThe accumulated gradient ranking and trojan weight pruning methods are technically simple. How are they different from existing bit flip methods? Is the main difference that existing methods only act on the last layer whereas these methods act on all parameters? The technical contribution compared to prior work should be made more clear.\n\nThere is sufficient detail for reproducibility.",
            "summary_of_the_review": "Given the weaknesses and various minor issues, I recommend to reject at this time.\n\n----------------\nUpdate:\n\nThe authors have addressed all of my concerns. I now think this is a good contribution to the line of work on bit flip attacks (especially because it focuses on the text domain and Transformers). I would give the paper a 7 if that was an option and will let the AC know.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6102/Reviewer_LGnH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6102/Reviewer_LGnH"
        ]
    }
]