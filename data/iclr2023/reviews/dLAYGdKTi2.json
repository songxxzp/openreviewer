[
    {
        "id": "iRqjjOGxpB",
        "original": null,
        "number": 1,
        "cdate": 1666602933627,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602933627,
        "tmdate": 1666602933627,
        "tddate": null,
        "forum": "dLAYGdKTi2",
        "replyto": "dLAYGdKTi2",
        "invitation": "ICLR.cc/2023/Conference/Paper5040/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The author(s) studied the multi-objective optmization problem and proposed a new algorithm called MoCo that provably converges to a Pareto stationary point in the stochastic gradient setting. Experiments on both toy and real data are conducted to support the proposed method.",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-motivated and well-written, the story line and analysis is easy to follow for experienced readers;\n- The MoCo algorithm proposed in this work is the fisrt stochastic multi-gradient method that can converge to Pareto stationary point under mild condition, which is a decent contribution to the field.\n\nWeakness:\n- I did not find any obvious weakness of the paper. The notation is a little bit heavy, maybe it is better to add a notation subsection.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written; the result is novel to my knowledge, reproducbility is unclear to me.",
            "summary_of_the_review": "Overall, this paper makes a decent contribution to the field; the author(s) give the first stochastic multi-gradient method can converges under mild conditions. I did not carefully check the proofs, the paper is above the bar of acceptence if other reviewers all agree that the proof is correct.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5040/Reviewer_Hp6r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5040/Reviewer_Hp6r"
        ]
    },
    {
        "id": "FyO6ruC5B3",
        "original": null,
        "number": 2,
        "cdate": 1666644617678,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644617678,
        "tmdate": 1668753983780,
        "tddate": null,
        "forum": "dLAYGdKTi2",
        "replyto": "dLAYGdKTi2",
        "invitation": "ICLR.cc/2023/Conference/Paper5040/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied the multi-objective optimization problem. Here, it reformulated it as a bilevel optimization problem and then leveraged the tools in bilevel optimization to establish the convergence rate. Since the main proof is adapted from bilevel optimization, the novelty is incremental. ",
            "strength_and_weaknesses": "Strength:\n1. The problem studied is important.\n2. The writing is easy to follow. \n\nWeakness:\n1. It is not clear why multi-objective optimization problems should be formulated as bilevel optimization problems. This critical point should be discussed clearly. At present, it is a weird combination of those two problems. \n2. The theoretical analysis is mainly adapted from the bilevel optimization literature. This makes the contribution incremental. The authors should highlight the challenges and the different parts compared with existing bilevel optimization literature.\n3. This paper assumes that the loss function is Lipschitz continuous. With this strong assumption, existing bilevel optimization literature can achieve the convergence rate $O(K^{-1/2})$, which is much stronger than the convergence rate in this paper. \n4. In lemma 2, the number of objectives $M$ should depend on the number of iterations. This is definitely NOT feasible in real-world applications. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: Neutral\nNovelty: good\nReproducibility: N/A",
            "summary_of_the_review": "This paper studied the multi-objective optimization problem. Here, it reformulated it as a bilevel optimization problem and then leveraged the tools in bilevel optimization to establish the convergence rate. Since the main proof is adapted from bilevel optimization, the novelty is good. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5040/Reviewer_qnbb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5040/Reviewer_qnbb"
        ]
    },
    {
        "id": "t5yj8WWnNA",
        "original": null,
        "number": 3,
        "cdate": 1666727405266,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727405266,
        "tmdate": 1668229850838,
        "tddate": null,
        "forum": "dLAYGdKTi2",
        "replyto": "dLAYGdKTi2",
        "invitation": "ICLR.cc/2023/Conference/Paper5040/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies how to mitigate the gradient bias of multiobjective learning. They use the idea of momentum-based gradient bias correction to substitute the gradient when finding the common gradient.\n\nI have had some earlier attempts on this problem but didn't end up very successful and I am very excited that someone also works on this problem. However, I find some potential mistakes in a very core Lemma (Lemma 4) used in this paper and thus I doubt the correctness of the result in this paper.",
            "strength_and_weaknesses": "Strength:\nThis is an open and important problem in MOO.\n\nQuestions:\n\nI am *pretty* unsure about the correctness of Lemma 4 which indicates the Lipschitz continuity of the lambda. Dontchev & Rockafellar, 2009, Theorem 2F7 does not imply this Lemma.\n\nUsing the notation in Dontchev & Rockafellar, 2009, Theorem 2F7, that result holds locally (given $x$, we have a $\\mu$) but when we approach the Pareto stationary point ($x$ changes), that $\\mu$ might change and we cannot have a global bound of the $\\mu$. In other words, the Lipschitz constant of $\\lambda$ should actually increase (to infinite) when we are closer to the stationary.\n\nThe lambda can be viewed as a solution map of a dual problem of a quadratic programming problem and its Lipschitzness is a very classic problem. To my best knowledge, all of the existing estimations of the Lipschitz are local, not global. Some literatures [1,2,3]\n\n\n[1] On the continuity of the minimum in Parametric quadratic programs.\n[2] On the Lipschitz Behavior of Optimal Solutions in Parametric Problems of Quadratic Optimization and Linear Complementarity\n[3] Sufficient Conditions for the Lipschitz Continuity of QP-based Multi-Objective Control of Humanoid Robots",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: unsure due to the potential issue\nNovelty: unsure due to the potential issue",
            "summary_of_the_review": "My main concern is the correct of a core Lemma. My current rating is mainly due to that issue if this can be addressed during the rebuttal period, I am happy to raise the score to 6 or 8. If it can't be addressed, this paper is not ready to publish.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5040/Reviewer_49fc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5040/Reviewer_49fc"
        ]
    }
]