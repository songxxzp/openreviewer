[
    {
        "id": "fiI2jPyB4S",
        "original": null,
        "number": 1,
        "cdate": 1666619676026,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619676026,
        "tmdate": 1666619676026,
        "tddate": null,
        "forum": "YDJRFWBMNby",
        "replyto": "YDJRFWBMNby",
        "invitation": "ICLR.cc/2023/Conference/Paper2147/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a framework for protein thermostability prediction. A large-scale protein dataset with organism-level temperature annotations is curated, and one pretraining and one tuning module are proposed for prediction.\nThe main contributions are:\n1.\tA dataset of protein sequences and folded 3D structures data with temperature annotations.\n2.\tIntroducing a protein structure-aware pre-training module for protein-related tasks.\n3.\tProposing a factorized sparse tuning module for thermostability prediction.\n",
            "strength_and_weaknesses": "Strengths:\n1.\tThe amount of data for thermostability-related tasks is greatly expanded.\n2.\tThe writing is generally clear, and easy to read.\n3.\tEditing proteins to improve thermostability is interesting, biological experiments can be added in the future.\n4.\tThe experiments on the model are relatively comprehensive.\nWeakness:\n1.\tTheir pretraining module simply combines the sequence pretrained model and the 3-D structure-aware pre-trained model. No novelty on the loss function can be viewed.\n2.\tTwo reasons mentioned for using factorized sparse tuning for this prediction task are not convincing. The sentence \u2018while COVID-19 related proteins usually contain more than 1,000 amino acids\u2019 has no strong connection with the studied problem.\n3.\tFrom the result, the framework seems just introduce some training strategies based on ESM-1B, and there is no design in biology for the thermostability prediction task. Besides, no significant improvement in results is observed.\n4.\tThe description of biology in this paper is superficial, and the study lacks interpretability analysis.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and the experiments are comprehensive. However, it's hard to say whether the model would really be useful in resolving biological problems. ",
            "summary_of_the_review": "The paper is clearly written, and the experiments are comprehensive. However, it's hard to say whether the model would really be useful in resolving biological problems. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2147/Reviewer_jaKq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2147/Reviewer_jaKq"
        ]
    },
    {
        "id": "Bbg-mI0Onv",
        "original": null,
        "number": 2,
        "cdate": 1666629467468,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629467468,
        "tmdate": 1666629467468,
        "tddate": null,
        "forum": "YDJRFWBMNby",
        "replyto": "YDJRFWBMNby",
        "invitation": "ICLR.cc/2023/Conference/Paper2147/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a new model for predicting protein thermostability based on a published dataset (Jarzab et al Nature Methods 2020) and create three derived different datasets annotating each of proteins with a class name that reflects the temperature that the organism lives at  (retrieved from NCBI). The task then becomes the ability to predict the correct class. They present two algorithmic \u2018advances\u2019: structure-aware pretraining and factorized sparse tuning  improve thermostability prediction. After evaluating compared other methods for prediction they also propose a method to generate sequences with increased thermostabilty. \n",
            "strength_and_weaknesses": "A fundamental weakness of this paper is similar conceptually to many papers submitted to ML conferences on biological data -that is the way the train-test splits are made which is 10 fold cross validation. What this means is that they have \u2018data leakage\u2019 relative to what it seems they claim to predict -- maximally they show if you had 90% of the data with related  sequences in the split, you could predict the missing 10%. Not a very useful predictor Now imagine a test split that removes a family of sequences that has no sequence relation to training data - then one could say a load more about the ability of the model to generalize \n\nWith thousands of proteins many of which are homologous i.e. similar  and very few classes in theri stes (2 or 5 )  - it's hard to see why one even needs a language model when a simple sequences similarity search as a baseline could do the same. Have the authors tested a simple baseline?\n\nThe sentence in the abstract \u201cWe present HotProtein, a large-scale protein dataset with growth temperature annotations of thermostability..\u201d strongly suggests  they are reporting this dataset here for the first time - when in fact it was published 2 years ago and is publicly available. Since they do in fact clear the data for more facile use, the statement should make that distinction clear.  \nRelated work seems to be missing - especially where methods have collected datasets but also run models to predict mutation effects - some of which are thermostablity related ; since some of these are even unsupervised, using only evolutionary sequence alignments - minimally a  discussion of this body of work is warranted eg - see ProteinGym ( Tranception ICML 2022)  plus older papers that use VAEs ( Riesselman Nature Methods 2018)  and even Potts Models (eg Hopf et al 2017 Nature Biotech) plus MAVEsdb.  In addition FLIP ICML 2021 and 2022- Learning deep representations of enzyme thermal adaptation. Why no comparison  to  ESM1v ( Meier 2021), Tranception ( Notin 2022), FLIP 202, DeepET (2022) ?\n\n\nMinor \nExplain why theory are not using the Meltome measurement as their thermostability label\nAlthough it\u2019s fine to explore whether using structural information improves existing approaches , the sentence on page 4  is misleading for different reasons -- \u201cThese models achieve considerable improvements on amino acid prediction tasks, e.g., contact prediction, mask prediction (Brandes et al., 2022), mutational effect prediction (Meier et al., 2021).\u201d Compared to what? Meier\u2019s results that the authors reference are great  BUT are only very marginally better than a VAE ( DeepSequence 2018 Nature Methods) and not as good as Tranception ( Notion et al,  ICML 2022) and are zero shot!! Using only sequence info. \nThe structure barely improves the results - why do they think that is the case? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Whilst I quite like the model framework. I cannot evaluate it's advantage for the declared task as baselines are missing. ",
            "summary_of_the_review": "For the reasons given in the Weaknesses section I don't believe this paper achieves what the authors would indicate are their goals - thermostability prediction and design.  The fact that they do not test generalizability, limits the application of this work to where there are very large datasets  of experimental results on related proteins from environmentally related organisms. I would suggest that the authors rethink their evaluations, test with much more naive baselines. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2147/Reviewer_un7C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2147/Reviewer_un7C"
        ]
    },
    {
        "id": "8IsBOoz6fbO",
        "original": null,
        "number": 3,
        "cdate": 1666878604146,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666878604146,
        "tmdate": 1670808385195,
        "tddate": null,
        "forum": "YDJRFWBMNby",
        "replyto": "YDJRFWBMNby",
        "invitation": "ICLR.cc/2023/Conference/Paper2147/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper focuses on the prediction of protein thermostability and the design of more thermostable proteins. After introducing a new benchmark to train and assess thermostability prediction models, authors describe a contrastive-learning framework to impart protein structure information on representations learned by large transformer networks usually trained on sequence alone (eg., ESM-1b). The paper also adapts ideas from other domains to carry out robust fine tuning and protein editing.",
            "strength_and_weaknesses": "**Strengths**\n- The work is very thorough with many ablations to illustrate the benefits from the different modeling decisions suggested by the authors\n- There are several novel contributions, in particular the Hotprotein benchmark and the structure-aware pre-training. The work also borrows ideas introduced in other fields to address (in a novel way) problems in protein prediction models and protein editing.\n\n**Weaknesses**\n- The main flaw I see in this paper is with respect to model performance validation. Unlike in many other application domains, protein sequences are not iid samples from the underlying data distribution as there is a lot of dependency between sequences that share common ancestors. This calls for particular cross-validation schemes to mitigate data leakage between training and test sets (eg., ensuring that sequences are not too similar), especially in several of the (semi-)supervised settings discussed in this work. See for example [1] for some ideas on how to do this. Similarly, it would be helpful if authors could comment on the similarities / overlap between the introduced Hotprotein dataset and the other datasets used in fine tuning / editing experiments (eg., FireProt and Meltome Atlas) -- if no substantial overlap, could you please specify that in supplementary?\n- The paper is not very clear at times. For instance, the reasoning / rationale that gave rise to the equations 1 and 2 in section 4.1 is absent\n- Not really a weakness, but rather a missed opportunity: since the SAP and fine-tuning frameworks are general and agnostic to thermostability, more extensive validation across protein families & fitness / protein attributes could be carried out to validate generalizability of the introduced ideas beyond thermostability. See for instance large-scale fitness benchmarks like ProteinGym [2].\n\n------------------------------------------------------------------------------------------------------------\n\n[1] Magn\u0301us Halld\u0301or G\u0301\u0131slason,  Felix Teufel,  Jos\u0301e Juan Almagro Armenteros,  Ole Winther,  and Henrik  Nielsen.   Protein  dataset  partitioning  pipeline.   2021b.   URL https://github.com/graph-part/graph-part\n\n[2] Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: Overall the paper is well written and easy to follow, with an extensive related work section. Some sections could be made a bit clearer, eg., section 4.1 (as discussed above).\n\n**Quality**: Very thorough sets of experiments and ablations. Besides some concerns on validation as mentioned above, the experimental design is well-thought through.\n\n**Novelty**: \n- Several meaningful / novel contributions, in particular the HotProtein dataset and the structure-aware pre-training. The former is based on existing datasets (NCBI bioproject and Uniprot), but required curation to create the final dataset, and could be a valuable resource for the community. The latter is relatively general and its application could extend beyond the thermostability use case discussed in this work. \n- The fine-tuning approach and optimization approach for protein editing are borrowed from other works (respectively [3] and [4]). While authors properly cite the corresponding works, the language used in section 1 when listing contributions is a bit misleading (eg., \u201cwe propose \u2026\u201d, \u201cwe built \u2026\u201d). Would recommend that authors use a more accurate language there to properly reflect their contributions (eg., \u201cwe adapt\u2026\u201d)\n\n**Reproducibility**: Main information needed for reproduction are provided. Authors mention that code and data will be made public in the abstract. \n\n**Minor points**: \n- References need to be cleaned up -- several papers do not have the proper venue / journal listed.\n- You don't seem to specifically define what is a small vs large dataset anywhere (terminology used in section 5.2, first paragraph, 4th point)\n\n------------------------------------------------------------------------------------------------------------\n[3] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.  Lora:  Low-rank adaptation of large language models.\n\n[4] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial examples through probability weighted word saliency. In Proceedings of the 57th annual meeting of the association for computational linguistics, pp. 1085\u20131097, 2019.",
            "summary_of_the_review": "Overall a very thorough and interesting work. Some concerns on validation as discussed in the strengths / weaknesses section. I am leaning accept overall given the several meaningful contributions, in particular the HotProtein dataset and the structure-aware pre-training. I would be willing to recommend acceptance more enthusiastically if the aforementioned concerns are addressed during rebuttal. \n\n--------------------------------------------------------------------------------------------------------\n[UPDATES POST REBUTTAL]\n\nThank you to the authors for their thorough responses during rebuttal. My main concerns have been adequately addressed (in particular regarding validation / potential data leakage). I have also read in detail the assessments from the other two reviewers, and believe their comments have also been properly addressed in reviews. I do believe that the newly introduced HotProtein benchmark, together with the methodological contributions (structure aware pre-training) and various adaptations from other domains to protein modeling (eg., factorized fine tuning and feature augmentations) represent a solid contribution. In my opinion this paper should be accepted to the conference, and I have increased both my score for correctness and overall appreciation accordingly. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2147/Reviewer_Uvn5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2147/Reviewer_Uvn5"
        ]
    }
]