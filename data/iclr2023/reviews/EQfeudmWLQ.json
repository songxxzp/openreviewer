[
    {
        "id": "m50NUHj0EL",
        "original": null,
        "number": 1,
        "cdate": 1666518738269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666518738269,
        "tmdate": 1670226554322,
        "tddate": null,
        "forum": "EQfeudmWLQ",
        "replyto": "EQfeudmWLQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4623/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "**Summary:**\n\nThis paper presents a new test-time normalization (TTN) method that combines the training statistics and the test-time statistics via the importance between them. To obtain the Prior, the authors first augment the training samples as domain-shift ones, and then calculate the importance via the gradient distance score between the clean samples and domain-shift ones. The experimental results on CIFAR-C and ImageNet-C demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "**Positive Points:**\n\n - The authors propose to adjust the balanced parameters between the training statistics and the test-time statistics based on the (augmented) training data.\n\n - The authors introduce a gradient distance score to calculate the balanced parameters $\\alpha$, which is technically sound.\n\n - The authors consider plenty of test-time scenarios, such as various batch sizes, stationary, continuously changing, and mixed domain adaptation.\n\n **Negative Points:**\n\n  - The authors obtain Prior based on the pair (clean sample x, and domain-shifted x'). And the domain-shifted x' greatly depends on the augmentation type and applying order. In this case, if the domain-shifted x' has an extensive domain gap from the test-time samples, the Prior would be estimated inaccurately, leading to poor TTA performance. Could the authors provide more discussion and explanations about this?\n\n - In the paragraph of \"Optimize $\\alpha$\", the authors state \"To simulate distribution shift, we use the augmented training data\". Here, the authors have a strong assumption that the test-time shifted type is very similar to the augmented type. However, in my opinion, this assumption often does not hold in practice. More explanations are required.\n \n - I suggest the authors can provide an ablation study on the augmentation type, which is very crucial for the proposed method. I wonder if there is an augmentations combination that works well for all the corruption-shifted test-time samples? If no, the proposed method may be difficult for real-world applications.\n\n - Most experimental results are reported on CIFAR-C. I suggest the authors can report the main results on ImageNet-C, which will be more convincing. I think the computational and time cost on ImageNet-C is similar to that on CIFAR-C. In Table 9, I find the authors do not report the results of \"Tent+TTN\" and \"SWR+TTN\". But I think these two are crucial results.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-writen and technically sound. If the authors can address my concerns, I would raise my scoring to accept.",
            "summary_of_the_review": "I vote for reject since the proposed method has a strong assumption on the test samples, which may not be hold in practice.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4623/Reviewer_RNFt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4623/Reviewer_RNFt"
        ]
    },
    {
        "id": "R0rzz1DqVXH",
        "original": null,
        "number": 2,
        "cdate": 1666528591163,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666528591163,
        "tmdate": 1666528591163,
        "tddate": null,
        "forum": "EQfeudmWLQ",
        "replyto": "EQfeudmWLQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4623/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a test-time normalization (TTN) method that combines the normalization statistics of source domain and target domain (during test), by adjusting the importance between them according to the domain-shift sensitivity of each BN layer. The motivation of the proposed method is clear, and the comprehensive experiments conducted in this paper demonstrate the effectiveness of the proposed method for test-time adaptation, particularly in the scenes for small batch size adaptation.  ",
            "strength_and_weaknesses": "\n**Strengths:**\n\n1.The motivation is clear, and the effectiveness of the proposed method makes sense, based on the previous works.\n\n2. This paper provides a comprehensive discussion to the related work, as to me. \n\n3. This paper conducts comprehensive experiments, and the experiments demonstrate the proposed method is empirically successful for test-time adaptation, particularly in the scenes for small batch size adaptation.  \n\n**Weaknesses:** \n\n1.The technical novelty and contributions are incremental. Test-time adaptation using BN is recently widely investigated in previous works, as stated in the related work. Furthermore, the idea of combining the statics of BN from the source domain and targeted domain is not new from previous work. Besides, the method (calculate the domain shift sensitivity) to obtain the prior of $\\alpha$ is from Choi et al. (2022), the optimization (for the interpolating weight $\\alpha$) using cross entropy is also shown in several previous work (even though not specific to the interpolating weight $\\alpha$). I donot well recognize new point for this paper. \n\n2. It should provide the code (including script with hyper-parameters), since this paper conducts comprehensive experiments and the configs of these experiments are complicated. Even though this paper provides details for setup of experiments in the supplementary material, I believe it is not easy to reproduce the results based on these descriptions. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is clear, the quality is overall good, and the novelty is somewhat incremental. I believe the experiments can be somewhat reproduced based on the descriptions of this paper.",
            "summary_of_the_review": "An empirical successful method for test-time adaptation but with incremental technical novelty and contribution. I am slightly positive to this paper, considering its empirical effectiveness, but not confident to accept it.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4623/Reviewer_EgJB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4623/Reviewer_EgJB"
        ]
    },
    {
        "id": "0k4cQc6Thz",
        "original": null,
        "number": 3,
        "cdate": 1666599658025,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599658025,
        "tmdate": 1668908747439,
        "tddate": null,
        "forum": "EQfeudmWLQ",
        "replyto": "EQfeudmWLQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4623/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a test-time batch normalization method to tackle domain shifts. The core idea is to learn interpolating vectors to combine running statistics and test-time batch statistics in batch normalization layers. The authors also propose to use a gradient distance score to initialize and regularize the interpolating vectors during training. The proposed method achieves promising results on several image classification and segmentation benchmarks. The authors also conduct extensive ablation studies to validate the design choices.",
            "strength_and_weaknesses": "**Strength**\n- The proposed method is simple yet effective\n- The proposed method can handle various different test settings and achieve promising performance\n- The authors conduct detailed ablation studies to help the readers to better understand the method\n\n**Weakness**\n1. Missing references\n- [A1][A2] are two recent state-of-the-art test-time adaptation methods, it would be good to add and compare with them\n- The idea of learning interpolating weight is quite similar to [A4]. I know [A4] should be counted as concurrent work and the authors do not have to cite it. But I suggest the authors add and compare with it, to provide a better context to the readers in this area. \n2. More comparisons\n- I think (Schneider et al. 2020) is a strong baseline and should be included.\n- The two state-of-the-art contrastive learning-based methods should also be compared\n- Maybe add more comparisons for the semantic segmentation experiments as well. For example, [A3][A4].\n3. Others.\n- Which TBN method is used in Figure 1? According to my experience, at least (Schneider et al. 2020) should have a reasonable performance (not significantly worse than CBN). Please add more details here.\n- The authors propose a mixed domain evaluation setting, where each batch contains a different domain. However, it is also possible that a single batch contains multiple domains of data. I wonder whether the proposed method can handle this case. [A3][A4] can both handle this and I think the authors can have some study or discussion about this setting.\n- How to select the $\\lambda$ in Eqn. (6) to balance the two loss terms?\n- It is quite interesting to see that the proposed method actually performs worse than non-learning-based normalization methods when the batch size is very small (2 or 1). Can the authors comment on that? Will a dynamic batch size in the post-training stage improve the performance in this case?\n- Why the normalization methods are not shown in Table 2 and Table 3? This looks like normalization methods cannot be used in these settings, but actually they can.\n- While adding the proposed method improves upon the two optimization-based methods in Table 3, the results are even worse than applying the proposed method alone. This may weaken the proposed method and probably the authors can move it to the appendix.\n\n**Reference**\n- [A1] Liu et al. TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive? NeurIPS 2021\n- [A2] Chen et al. Contrastive Test-time Adaptation. CVPR 2022\n- [A3] Khurana et al. SITA: Single Image Test-time Adaptation.\n- [A4] Zou et al. Learning Instance-Specific Adaptation for Cross-Domain Segmentation. ECCV 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is easy to read and understand.\n- Quality: Good.\n- Novelty: Interesting and somewhat novel idea.",
            "summary_of_the_review": "Overall, I think this paper tackles an interesting and important task (test-time adaptation), and the proposed method is well-motivated and effective. I tend to accept this paper. But I suggest the authors add some comparisons and discussion with the missing references I mentioned above.\n\n-----\n\nPost-rebuttal\n\nAfter reading other reviews and the response, I decided to increase my rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4623/Reviewer_4Hgx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4623/Reviewer_4Hgx"
        ]
    },
    {
        "id": "Xxfbus6iCzk",
        "original": null,
        "number": 4,
        "cdate": 1666657389586,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657389586,
        "tmdate": 1666657389586,
        "tddate": null,
        "forum": "EQfeudmWLQ",
        "replyto": "EQfeudmWLQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4623/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses a problem arising from the generality of source-based conventional batch normalization (CBN) or test-based batch normalization (TBN), with the former biasing the architecture to source distribution and the latter suffering from inaccuracies for small test batch sizes. The paper proposes combining CBN and TBN, and exposing a fraction of different layers in the architecture to TBN and CBN, thereby identifying those layers which need not be as sensitive to the test distribution. The results show promising performance of the proposed method, TTN, on the CIFAR and some DG benchmarks across several batch sizes.\n",
            "strength_and_weaknesses": "Strengths:\n\nThe technique is clear and fairly straightforward to implement.\nMotivation describing pitfalls of CBN and TBN, and the need for better online test-time adaptation under flexible batch sizes are described well\n\nWeaknesses:\n\nIt is unclear how the results would generalize across architectures. What will the results look like for transformers, which are currently ubiquitous in several applications? Will some tokens in a layer be adapted to the test set and the rest to the source? What would this mean in practice?\n\nIn Figure 2, why cannot alpha be learnt or adapted online? If it can be, then these methods should be compared with prompting (freezing certain layers will be analogous to using source-based normalization, while the prompting layers will be adapted to test set). It is unclear why an alpha learnt offline and frozen will continue to yield optimal results across a range of different test-time distributions.\n\nThe significance of results from Table 6 is not clear. These appear to be tested specifically on batch size 2 where TBN is  known to falter, but the results on only a few datasets appear significant. Similarly for the corruption benchmark, with a batch size of 1 or 2 (which is a fairly common real world scenario - data streaming and not evaluated in batches), TTN seems to be beat by one of the baselines. A better understanding and explanation of this is required. Is this because of optimizing the alphas offline?\n\nFinally, what are the restrictions of how far the data distribution that the TTN model is optimized on and the real streaming test time data are? The results appear to indicate that there will be scenarios and assumptions mainly surrounding the dataset that alpha is optimized upon.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good clarity and enough details are provided to replicate the implementation. The novelty is marginal as this appears to be a fairly straightforward combination of two existing methods.",
            "summary_of_the_review": "Overall, I would rate the paper as below the acceptance threshold. While the method appears to show promise in certain settings, I would urge the authors to consider a wider selection of baseline architectures. Also, the results are not convincing enough about its use in a wider array of datasets or real-world scenarios (see the weakness section above).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4623/Reviewer_yiJu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4623/Reviewer_yiJu"
        ]
    }
]