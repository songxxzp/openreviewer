[
    {
        "id": "T9eUZotFirU",
        "original": null,
        "number": 1,
        "cdate": 1666414029256,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666414029256,
        "tmdate": 1666414029256,
        "tddate": null,
        "forum": "8-aqFHleFyC",
        "replyto": "8-aqFHleFyC",
        "invitation": "ICLR.cc/2023/Conference/Paper3379/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies policy evaluation with nonlinear function approximation. It proposes and analyzed two algorithms called VRPD and VRPD+ to optimize the primal-dual form of the MSBPE. The VRPD algorithm utilizes variance reduction techniques to achieve $O(1/K)$ convergence rate.",
            "strength_and_weaknesses": "Strength:\n1. Paper is well-written and easy to follow.\n2. Compared with previous work on the non-convex-strongly-concave minimax optimization problem, the convergence rate of this work achieves $O(1/K)$, which is the same as before and has fewer constraints.\n\nWeakness:\n1. The convergence metric is unconventional. The first term in Eqn (9) is the full gradient w.r.t. $\\theta$, which (I believe) includes both the partial derivative of the first input and the chained gradient w.r.t. $\\theta$ via the second input. This first term already suffices to guarantee a first-order stationary point, rendering the second term redundant. On the other hand, bounding the partial gradient of both inputs is also acceptable. I wonder why the analysis of this paper fails to provide a guarantee for either of these metrics.\n2. The technical novelty of the minimax optimization technique used here is not clearly stated and compared with previous works. I believe it helps to also make a list to compare with previous NCSC minimax optimization algorithms.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of high quality and clarity. \n\nQuestions:\n1. Regarding the weakness mentioned above, can you establish guarantees for the two terms in Eqn (9) separately?\n2. It seems to sample a trajectory with a length of $M$ cannot be used as a finite-sample approximation of the stationary distribution. In discounted setting, the classical way to do so is to run the trajectory with a stopping probability of $1-\\gamma$, and only use the last step as an unbiased sample from the stationary distribution. This doesn't really affect your optimization results but should be made clear under the context of RL or policy evaluation.",
            "summary_of_the_review": "Currently, the paper shows strong theoretical results compared with previous literature. Some concerns still need to be addressed. Thus I recommend weak acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3379/Reviewer_iPgL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3379/Reviewer_iPgL"
        ]
    },
    {
        "id": "GKr8a7p20c",
        "original": null,
        "number": 2,
        "cdate": 1666582299171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582299171,
        "tmdate": 1666586287403,
        "tddate": null,
        "forum": "8-aqFHleFyC",
        "replyto": "8-aqFHleFyC",
        "invitation": "ICLR.cc/2023/Conference/Paper3379/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the policy evaluation problem of reinforcement learning. Based on a primal-dual formulation of the objective, the authors apply a variance reduction technique to solve the problem under the nonlinear function approximation. They claim that the algorithm attains $O(\\frac{1}{K})$ iteration complexity.",
            "strength_and_weaknesses": "Strength:\n\n- The paper proposes a variance reduction method for solving a problem that seems to have a connection with the policy evaluation problem. They obtain the $O(\\frac{1}{K})$ iteration complexity.\n\nWeakness:\n- The considered finite-sum objective differs from the original MSPBE objective. In fact, the objective cannot even be viewed as an unbiased substitute, as the sample from a trajectory doesn\u2019t follow the stationary distribution. Such an objective is not justified.\n- The algorithm\u2019s update is poorly explained. From my view, it\u2019s just SPIDER\u2019s update [1], which is a popular variance reduction approach. However, there is no discussion with existing variance reduction methods.\n- The projection step in (8) is not explained.\n- There are too many typos and unprecise claims, making the paper hard to read. Here are examples:\n1. The last $g_\\theta(s)$ in (1) should not have a transpose.\n2.  The strongly-concavity definition in assumption 4.2 is wrong; the function should also be additionally concave.\n3.  The L_f-smoothness definition in assumption 4.3 is unprecise; theta and omega should be in the same norm.\n\n[1] Spider: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is poorly organized and hard to follow. There are too many unjustified and misleading claims.",
            "summary_of_the_review": "The paper\u2019s motivation is not clearly centered. The optimization objective is not justified and the algorithm is not explained. There are many typos and technical flaws which make the paper hard to understand. Thus, I believe the work is not finished and is far from its fine shape for publication in ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3379/Reviewer_ZMya"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3379/Reviewer_ZMya"
        ]
    },
    {
        "id": "l1_94LP5ws",
        "original": null,
        "number": 3,
        "cdate": 1666671271035,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671271035,
        "tmdate": 1666882063517,
        "tddate": null,
        "forum": "8-aqFHleFyC",
        "replyto": "8-aqFHleFyC",
        "invitation": "ICLR.cc/2023/Conference/Paper3379/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors developed a new efficient, single-timescale variance-reduced primal-dual method (VRPD) with an emphasis on the feasibility of solving non-convex policy evaluation problem in reinforcement learning (RL) with nonlinear function approximation, also known as the value function estimation or on-policy learning with nonlinear function approximations. VRPD is a single-timescale algorithm that meets an $\\mathcal{O}(1/K)$-rate of convergence to a first-order stationary point for general non-convex-strongly-concave (NCSC) minimax optimiztion, under mild assumptions and appropriate parameter choices. The authors claim that they provide the best-known result in the literature. An enhanced VRPD method is also proposed, namely VRPD${}^{+}$, to allow adaptive batch sizes using historical information while maintaining the theoretical convergence rate. The authors empirically validated the proposed method and concluded higher sample efficiency over comparable methods including VRPD.",
            "strength_and_weaknesses": "Page 2, middle part.\nI am confused when the authors indicate \"new proof techniques in convergence rate analysis\". To quote words in your paper it \"resolves an open question and clarifies an ambiguity in the state-of-the-art convergence analysis ...\". Can you clarify what was the ambiguity and how did you clean it?\n\nPage 3, middle part (related work).\nMy understanding is that Du et al. (2017) study the TD(0) formulation of policy evaluation with linear function approximation, which can be re-cast as a strongly-convex-concave problem. What is the true novelty of this work, in comparison with Du et al. (2017)? Du et al. (2017) solved this problem via VR-based methods and achieves a linear convergence rate via the gradient descent-ascent type method. Is it correct that nonlinear function approximation leads to possible non-concavity?\n\nPage 4, first paragraph.\nSREDA adopts \"PiSARAH\" subroutine but the latter is not detailed.\n\nPage 4, second paragraph.\nIt should be desirable if the dependency on *all* problem-dependent constants should be considered when discussing matching the lower bounds. At first glance, it seems that the method is not sharp in terms of $\\kappa$, and this is natural due to that no acceleration method is adopted. In my opinion, even if the resulting upper bound failed to match the lower bound by Zhang et al. (2021) in certain regimes, some discussions in fine-grained angles should be added.\n\nPage 7, middle part (Remark after Theorem 1).\nThe author wrote \"Theorem 1 states that VRPD achieves an $\\mathcal{O}(1/K)$ convergence rate to an $\\epsilon$-FOSP\". The complexity and rate of convergence have been awkwardly mixed together and caused confusion. One should modify it accordingly. I like the more detailed stochastic sample complexity of $\\mathcal{O}(M + \\sqrt{M}\\kappa\\epsilon^{-1})$ where $\\kappa$ denotes the condition number of the strongly convex part. Also, \"The most challenging part in proving Theorem 1 stems from the fact that one needs to simultaneously evaluate the progress of the gradient descent in the primal domain and the gradient ascent in the dual domain of the minimax problem\". This is merely a standard technique and is well-known in recent minimax optimization.\n\nSome reference bibitems are duplicated, e.g. Sutton et al. Authors are encouraged to clean them when preparing their next version.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written with words and sentences of good quality, and the single-timescale method for the given problem is novel in literature to my best knowledge. The complexity result matches the one in Luo et al in the regime $\\kappa=O(1)$, and hence somewhat expected.",
            "summary_of_the_review": "In summary, the authors did a satisfactory job introducing the single-timescale method that solves an important practical problem (policy evaluation in RL), achieving sharp convergence rates matching the lower bound under certain regimes ($\\kappa=O(1)$). With that said, the paper potentially admits some issues of overclaiming their results. Plus, the author seems to have solved a much broader problem but applied it to a very specific problem (of policy evaluation with nonlinear function approximation). Extending such a framework to wider application instances as the next step might be a good idea. As of the current form I give my rating of borderline inclining rejection and encourage the authors to address these main points in their revised version.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3379/Reviewer_Mbzc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3379/Reviewer_Mbzc"
        ]
    },
    {
        "id": "xPqYC-9ioV",
        "original": null,
        "number": 4,
        "cdate": 1666801376306,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666801376306,
        "tmdate": 1666801376306,
        "tddate": null,
        "forum": "8-aqFHleFyC",
        "replyto": "8-aqFHleFyC",
        "invitation": "ICLR.cc/2023/Conference/Paper3379/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes two variance-reduced (VR) algorithms for a specific nonconvex strongly concave min-max problem obtained after reformulating the policy evaluation problem as a min-max game with nonlinear function approximation for the value function of the policy. They provide the first O(1/K) convergence and the possibility of removing the full gradient computation required in VR methods in certain iterations. ",
            "strength_and_weaknesses": "The convergence results are provided for the first time and they look theoretically sound to me. \n\nWeakness:\n1. I am not sure about the role of \\Theta in the paper. It seems that it is mentioned in the last paragraph on page 4 and then dropped subsequently in the min-max formulation (2) or (4). Would the analysis work if such \\Theta is explicitly imposed on the min-max problem? Note that in such a case, the \\|J(\\theta)\\| may not converge to zero.\n\n2. The maximization over \\omega \\in R^d of equation (2) changes to \\omega \\in \\mathcal{W} in equation (4). Does that change the policy evaluation problem? There is no justification provided for this abrupt change. Is compactness of \\mathcal{W} necessary for convergence?\n\n3. Why does the paper say it is a single timescale algorithm? I see two timescales \\alpha and \\beta in the algorithm. Are they set the same in the implementation? \n\n4. I don't see how the sample complexity is O(\\kappa/\\eps) in Theorem 1. Based on the step-size policy, I believe it should be a larger power of \\kappa. E.g., \\kappa^3/\\eps. \n\n3. Since the main technical thrust of the paper is VR algorithms for the min-max problem, I was expecting some literature review of such algorithms, at least for nonconvex settings. It is important to put this result in the context of such literature and show the novelty of their work over the state-of-the-art in the optimization literature. I recommend that the authors include papers/articles working on VR algorithms for smooth nonconvex (note that J(\\theta) is a smooth nonconvex function) and nonconvex-concave minx-max problems. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. The proofs seem clear to me. The only comment I have is to define \\epsilon_\\theta in the proof of Theorem 1.\n\nIn terms of quality and novelty, the algorithm design is not new. A similar VR gradient design was used in Nonconvex smooth optimization in the past. Applying this to the extended nonconvex strongly concave minmax problem is new in the literature if I am not missing something. Although I am not updated with the state-of-the-art in this specific area.\n\n",
            "summary_of_the_review": "Overall, the paper makes some new contributions to both areas: VR algorithms for minmax problems and Policy evaluation in RL. Although, the paper does lack significantly in terms of literature review. It ignores the first area completely. The single-timescale part is fuzzy at best. The order of condition number in the sample complexity seems wrong to me. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3379/Reviewer_vYTZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3379/Reviewer_vYTZ"
        ]
    }
]