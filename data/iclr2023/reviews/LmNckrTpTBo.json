[
    {
        "id": "BpDZSU4NbV",
        "original": null,
        "number": 1,
        "cdate": 1666382265443,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666382265443,
        "tmdate": 1666382265443,
        "tddate": null,
        "forum": "LmNckrTpTBo",
        "replyto": "LmNckrTpTBo",
        "invitation": "ICLR.cc/2023/Conference/Paper249/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a training framework, named Feedback training, to improve the performance of two-stage binary classifier, a common type of multi-stage classification. Experiments on both task extraction and Yahoo Answer demonstrate the superior performance of the proposed method. ",
            "strength_and_weaknesses": "Strength:\n\nDeveloping an effective algorithm to trade-off between performance and model scale is an interesting research topic and practical for resource-constrained applications. \n\n\nWeakness:\n\n1. Figure 2 is not intuitive to explain the problem. And the motivation for proposing a reversed-order training framework is not well explained. \n\n2. The logic of the proposed method is hard to follow. It\u2019s not clear why the sample weighted loss function is able to address the challenge of modeling the learning preference as described in Sec. 3.2.\n\n3. The weighted sampling strategy in Sec. 3.3 seems to be ad-hoc without any theoretical justification. \n\n4. The experimental evaluation is weak without the comparison with some state-of-the-art comparisons.\n\n5. Since the proposed method is only for the binary classifier, its practical usage is limited. It would be good to extend the method to multi-class classification. \n\n6. There are works studying the efficiency of Committee-based models (ensembles or cascades), e.g. [a]. This paper lacks discussion and comparison of such related works. \n[a] WISDOM OF COMMITTEES: AN OVERLOOKED APPROACH TO FASTER AND MORE ACCURATE MODELS, ICLR 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is not well written and presented. The problem and the motivation of the proposed method should be well explained.\n\nQuality: The quality of the paper is poor based on the presentation, technical novelty, and experimental evaluation.\n\nNovelty: Both the problem and method are not new.\n\nReproducibility: There are not many details about the algorithm implementation. Therefore, the reproducibility is questionable. \n",
            "summary_of_the_review": "Given the comments about the weakness of the paper, the paper cannot meet the quality of ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper249/Reviewer_7ycA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper249/Reviewer_7ycA"
        ]
    },
    {
        "id": "9hRDzWr3ACd",
        "original": null,
        "number": 2,
        "cdate": 1666588033250,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588033250,
        "tmdate": 1666588033250,
        "tddate": null,
        "forum": "LmNckrTpTBo",
        "replyto": "LmNckrTpTBo",
        "invitation": "ICLR.cc/2023/Conference/Paper249/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In view of the problems caused by Sequential Training and Independent Training in multi-stage classification, this paper proposes a new learning framework called feedback training, which outperforms the baseline methods in the two-stage binary-classification settings.",
            "strength_and_weaknesses": "Strengths.\nThis paper proposes a feedback training strategy, i.e., first train the main classifier, then determine the classification \u201cdifficulty\u201d of each sample based on the trained main classifier, and finally adjust the weight of each sample during the training of the pre-classifier. \nIn general, the author\u2019s motivation is clear, and proposes a simple yet effective (according to the experimental results presented in the paper) solution to solve the mentioned problems of previous Sequential Training and Independent Training methods.\n\nWeaknesses\n1) Although the paper is generally well organized and presented, there are also some obvious irregularities that may hinder understanding. For example, the meanings of \u201cpre_score\u201d and \u201cmain_score\u201d in Figure 2 should be clearly expressed in the caption. For another example, for all tables in the experiment part, the corresponding training methods of each line should be indicated in the table rather than in the caption, and it is better to bold the best results for all tables for better clarity.\n2) The reproducibility of the paper is doubted, since some details of the experiments are not specified. For example, for the few-shot experiment, 1%~10% of the training data is selected. However, how the data is selected and which data is selected (manually selected or randomly) are not clearly explained to ensure reproducibility. In addition, the specific value of PassRate for Table 1 is not specified.\n3) Experiments can be more abundant and comprehensive to prove the effectiveness of the method. For example, more models can be adopted as the pre-classifier and the main-classifier. In addition, Figure 2 shows the inconsistency problem of independent training. I'd like to see a similar figure for feedback training, which can show more intuitively whether the pre-classifier achieves the claimed filtering effect. Further, for Yahoo! Answers dataset, the paper only splits \"society&culture\" into positive samples, and other classes are splitted as negative samples to meet the binary classification setting. I'd like to see the results of more splittings of classes to confirm that the results of baselines are consistently improved under different splittings.\n4) Some minor grammatical errors need to be corrected. For example, the last paragraph of the first page: \"exist methods\" should be changed into \"existing methods\". The caption of Figure 1: \"only samples passed\" should be changed into \"only samples that passed\". The second paragraph of Section 3.1: \"Transformer-base\" should be changed into \"Transformer-based\", and \"bias data set\" into \"biased data set\". The second sentence of the first paragraph below Figure 2 (\"but some other...\") lacks a predicate. Experience Setup: Whether the first letter after the colon is in uppercase or lowercase should be determined.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: fair\nQuality: fair\nNovelty: fair\nReproducibility: poor",
            "summary_of_the_review": "My main concerns are the writing, reproducibility, and significance of experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper249/Reviewer_WfBE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper249/Reviewer_WfBE"
        ]
    },
    {
        "id": "Cw2hXfBMOs",
        "original": null,
        "number": 3,
        "cdate": 1666731271606,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731271606,
        "tmdate": 1666731355541,
        "tddate": null,
        "forum": "LmNckrTpTBo",
        "replyto": "LmNckrTpTBo",
        "invitation": "ICLR.cc/2023/Conference/Paper249/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a novel training framework, named Feedback Training. The classifiers are trained in an reverse order, the main-classifier then followed by pre-classifier. And the pre-classifier at the later stage is trained with sample weighting method, which are based on the training result of Main-classifier. The experiments show the efficacy of the proposed approach, and its great advantage under the scenario of few-shot training.",
            "strength_and_weaknesses": "Quality/Clarity: the paper is well written and the techniques presented are ok to follow. The authors design a muti-stage classifier including main-classifier and the pre-classifier with the later is trained with weighted samples.\n\nOriginality/significance: the idea is incremental, there have been multiple existed approaches such cascaded classifier to speed up inference, adboost algorithm with weighted samples, etc. \n\nExperiments: the method should be compared with some baselines such as adboost.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea to use multiple stage classifier is interesting, but I think it will be better to add more experiments to support the claims. ",
            "summary_of_the_review": "Overall it is a good paper, but not ready for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper249/Reviewer_ND4V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper249/Reviewer_ND4V"
        ]
    }
]