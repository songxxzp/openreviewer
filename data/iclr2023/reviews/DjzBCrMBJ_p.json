[
    {
        "id": "sHGzpoNmhm",
        "original": null,
        "number": 1,
        "cdate": 1666661855748,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661855748,
        "tmdate": 1666661855748,
        "tddate": null,
        "forum": "DjzBCrMBJ_p",
        "replyto": "DjzBCrMBJ_p",
        "invitation": "ICLR.cc/2023/Conference/Paper4573/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new way to augment the graph-structured data through the analysis of the graph spectrum. Based on the observation that the augmentation changing graph structure dramatically can improve the performance of graph learning, the authors propose a way to find the edges that can dramatically change the spectrum of a graph. The modified graphs are used to augment graph datasets. The experiments are conducted on various datasets for node-level and graph-level classifications. The proposed method induces a significant performance improvement in most cases.\n",
            "strength_and_weaknesses": "Strength\n- The proposed idea of spectral augmentation is novel and technically valid.\n- The extensive experiments support the purpose of the spectral augmentation well. \n\nWeaknesses + Questions\n- Although spectral augmentation shows improved performance on many graph learning experiments, the way of changing the graph spectrum is somewhat counter-intuitive since the large changes in the graph spectrum imply large topological changes in the original graph. In that sense, it would be good to provide some qualitative examples of how the graph is augmented after spectral augmentation. for example, how do molecular graphs change after augmentation? do they still keep the important functional groups or not?\n- The selection of epsilon sounds important. It would be good to have some additional experiments on the analysis of the hyperparameter.\n- In experiments, the k-lowest and *highest* eigenvectors are used to augment. We often assume that the highest eigenvectors contain noisy information, and I wonder about the reason behind using the highest eigenvectors.\n- Although the eigendecomposition is only performed once for each graph, a graph such as Pubmed still requires a huge amount of time to compute. Will there be a huge difference if one employs some approximation of eigendecomposition?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written, and the proposed idea is very novel. Given that the implementation is not submitted as supplementary material, the reproducibility of work is not guaranteed although the implementation details are well provided for most parts.",
            "summary_of_the_review": "The paper is well-written and easy to follow. The technical details are well provided. The extensive amount of experiments well supports the main claim of the paper. Overall, I lean towards acceptance of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4573/Reviewer_eFML"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4573/Reviewer_eFML"
        ]
    },
    {
        "id": "6MX4iOKYSdc",
        "original": null,
        "number": 2,
        "cdate": 1666676106182,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676106182,
        "tmdate": 1666676106182,
        "tddate": null,
        "forum": "DjzBCrMBJ_p",
        "replyto": "DjzBCrMBJ_p",
        "invitation": "ICLR.cc/2023/Conference/Paper4573/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a topology augmentation method for graph contrastive learning which explores invariance of graphs from the spectral perspective. To realize the spectral invariance, the paper aims to identify sensitive edges whose perturbation leads to a large spectral difference. The conjecture is that the GNN encoder then focuses more on invariant spectral components. Numerical experiments illustrate the improvement in the performance of unsupervised representation learning, the generalization capability in transfer learning and induced robustness to adversarial attacks.",
            "strength_and_weaknesses": " Strengths:\n1.    The approach for generating topology augmentations for GCL by perturbing edges in order to maximize the spectral change is novel.\n2.     The experimental analysis is extensive, spanning multiple tasks and including numerous datasets and baselines. \n4.     The appendices contain additional experiments that provide considerably more insight into the behaviour of the proposed approach and what aspects lead to the observed performance improvements.\n\nWeaknesses:\n1. Some key terms are vaguely defined. \u201cStructural invariance\u201d is defined as \u201cpreserv[ing] the essential structural properties\u201d but it is not at all clear what these structural properties are. As a result, some claims in the paper are not well-supported. For example, there does not appear to be genuine support for the claim that \u201cthe information about the edges causing structural instability is minimized in the learned representations\u201d.\n\n2.\tThe connection between structural invariance and spectral invariance is not solid. Yes, the spectrum is strongly related to many structural properties such as clustering, but there is not sufficient evidence here to state that \u201cperturbing graph spectrum controls the change of structural properties\u201d, at least not in the sense that there is a well-understood \u201ccontrol\u201d of the change that is being induced. \n\n3.\tThe connection from equation (4) to equation (5) is not well-motivated. The initial argument is that a perturbation should be made to maximize the spectral change from the original graph. Then it becomes maximizing the spectral discrepancy between the two views. Finally it becomes maximizing and minimizing the spectral norms. The paper does not clearly explain why the last step aligns with the initial motivation.  \n\n4.\tThe computational cost of the proposed approach is high due to the eigendecomposition. To address this, the authors focus on the K lowest and highest eigenvalues. This takes the optimization task further away again from the initial motivation. The paper offers the vague argument that these \u201care the most informative suggested by spectral graph theory\u201d, but there are no citations to support this claim, nor any clarification to explain what is meant by \u201cmost informative\u201d. It is good that experimental results are included ot explore the impact of varying the number of included eigenvalues, but my concern is that by this point there has been a very significant migration from the initial motivation of preserving structural properties. \n\nMinor point: was there a check whether the normality assumption of the t-test is satisfied?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is, in general, very well written. The only misgiving on this front is that certain key concepts are not clearly defined and detailed arguments and explanations for some of the key developments in the methodology are missing. \n\nQuality: The paper reports the results of carefully conducted experiments. The main concern with regard to quality is that several claims emerge during the methodology that are not well-supported. \n\nNovelty: The proposed method is novel and represents an interesting and effective approach. \n\nReproducibility: the paper makes a good effort to provide most of the experimental details. I do not believe that code was provided or that there was a link to code, so this imposes limits on the reproducibility. Even with the detailed information provided, I believe that it would be very challenging to replicate the results. ",
            "summary_of_the_review": "The paper provides a novel augmentation approach for graph self-supervised learning. Extensive and carefully conducted experiments provide compelling evidence that the proposed method is effective for a range of tasks. The main weaknesses of the paper are  the absence of concrete definitions for key concepts such as \"structural invariance\", the somewhat weak linkage to \"spectral invariance\" and then the incorporation of methodological developments and approximations that lead to a final procedure that is relatively far from the initial motivation. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4573/Reviewer_2d5T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4573/Reviewer_2d5T"
        ]
    },
    {
        "id": "B5V9dS18EYH",
        "original": null,
        "number": 3,
        "cdate": 1667574352912,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667574352912,
        "tmdate": 1669793654853,
        "tddate": null,
        "forum": "DjzBCrMBJ_p",
        "replyto": "DjzBCrMBJ_p",
        "invitation": "ICLR.cc/2023/Conference/Paper4573/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper tackles the problem of how to design structural/topological augmentations for graphs, which can be used by graph contrastive learning. The authors aim to find a principled way for topology augmentations by exploring the invariance of graphs from the spectral perspective. This paper proposes to generate topological augmentations guided by maximizing the change in the spectral domain. Experiments on both graph- and node-level tasks demonstrate the\neffectiveness of the proposed method in graph self-supervised learning.",
            "strength_and_weaknesses": "#### Strengths\n1. The description and formulation of the proposed algorithm are clear, and I like the idea of measuring the topological change of graphs in the spectral space.\n2. The experiment results are extensive. I appreciate the authors not only compare many baselines under the graph contrastive learning setup but also consider the transfer learning and adversarial robustness problems.\n\n#### Weaknesses\n1. I like the idea of measuring the augmentation in the spectral space. The paper proposes a working algorithm, SPAN, by maximizing the L2 distance of eigenvalues and specifically modeling the edge modification/perturbation. But I think some important or highly-related questions in this direction are not sufficiently discussed and explored in this paper.\n    1. One important problem is why maximizing the perturbation (in terms of L2 distance of eigenvalues) can still/always generate valid views of the original graph? Is it necessary to require the $\\Delta$ matrix to be sparse (i.e., not too many 1's), and thus the perturbed graph is not too far from the original one? If the perturbed graphs (i.e., the two views generated by SPAN) are too far away from the original one (in terms of the L2 distance of eigenvalues), what topological information is still preserved, and are they still label preserving? \n    2. Due to complexity constraints, the authors propose to only compute the $K$ largest and smallest eigenvalues. The authors claim they are the most informative ones, but is there any reference or theory supporting this claim? From my experience, I think it is likely for the real-world graph to have many eigenvalues (of the symmetric normalized Laplacian) equal or close to 0 and 2, but the number of eigenvalues around 1 is relatively small. In this regard, if the chosen $K$ is much smaller than $n$, would it be likely the eigenvalues used to optimize the augmented graph is always equal or close to 0 and 2?\n    3. Since this paper only considered the L2 distance between eigenvalues. Thus we are forced to compare graphs of the same size. And only edge perturbation is considered. However, I think we can generalize to consider, e.g., a distributional divergence or two-sample distance, like empirical 1-Wasserstein distance, and generalize the framework to augmentations that also change the number of nodes (e.g., node dropping, subgraph cropping). If this direction is considered out of the scope of this current paper, then the contribution of this paper is somehow limited to a specific type of topological change on graphs and also kind of limit the overall contribution.",
            "clarity,_quality,_novelty_and_reproducibility": "1. I think the clarity is good, and I agree with the novelty.\n2. I think the significance is supported by extensive experiments. But the exploration and discussion regarding the proposed spectral augmentation approach are somehow limited and insufficient.\n3. The reproducibility depends on whether the authors will release the code (conditioned on the acceptance) and cannot be judged now.",
            "summary_of_the_review": "Overall I recommend rejection for the current manuscript. My major concern is that several critical questions of the proposed spectral-augmentation approach are not discussed or explored, which limits the contribution of the paper and leaves confusion to the community. I would expect there is also some room to improve the experimental results further if the authors can rethink those important questions of spectral augmentation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4573/Reviewer_fWm4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4573/Reviewer_fWm4"
        ]
    },
    {
        "id": "yK2g68E38Xv",
        "original": null,
        "number": 4,
        "cdate": 1667715309676,
        "mdate": 1667715309676,
        "ddate": null,
        "tcdate": 1667715309676,
        "tmdate": 1667715309676,
        "tddate": null,
        "forum": "DjzBCrMBJ_p",
        "replyto": "DjzBCrMBJ_p",
        "invitation": "ICLR.cc/2023/Conference/Paper4573/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed one spectral augmentation scheme for graph contrastive learning methods. It aims to preserve the spectral invariance which is related to the large changes in the graph spectrum. The experiments demonstrate the effectiveness of the proposed method.\n\n",
            "strength_and_weaknesses": "Strength:\nThis paper combines structural invariance with spectral analysis, which is interesting and reasonable for graph contrastive learning.\n\nWeaknesses:\nThe sensitive edges that can change the graph property should be emphasized when considering the structural invariance. While the spectral augmentations are designed based on the whole graphs, and the edge perturbation is designed for each edge, i.e., the edge flipping is independent. \nThen, does this mean that the flipped edges are all sensitive when learning structural invariance? How to evaluate these edges independently? Besides, it would be better to show the influence of different $\\epsilon$ in Eq.(3). ",
            "clarity,_quality,_novelty_and_reproducibility": "It is novel and interesting the structural and spectral invariance proposed in this paper. It is clear and easy to understand.",
            "summary_of_the_review": "This paper proposed one spectral augmentation scheme based on the structural invariance. Extensive experiments demonstrate the effectiveness of the proposed method. There exist one question when evaluate the flipped edges as mentioned before.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4573/Reviewer_SvG5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4573/Reviewer_SvG5"
        ]
    }
]