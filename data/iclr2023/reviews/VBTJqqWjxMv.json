[
    {
        "id": "M9_xLbLLxar",
        "original": null,
        "number": 1,
        "cdate": 1666460726604,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666460726604,
        "tmdate": 1666460726604,
        "tddate": null,
        "forum": "VBTJqqWjxMv",
        "replyto": "VBTJqqWjxMv",
        "invitation": "ICLR.cc/2023/Conference/Paper6281/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper casts a new understanding of contrastive learning, which interprets the alignment update and uniformity update as message-passing steps and bridges contrastive learning and message passing. It interprets some techniques used in contrastive learning from the perspective of message passing neural network and proposes some inspiration for the modification of contrastive learning, some empirical results also justify these changes inspired by MPNN.",
            "strength_and_weaknesses": "Strengths:\n1. This paper studies a significant problem about how data augmentation affects the learning dynamics of contrastive learning, although previous works have made some trials, this paper has a totally new perspective that is more interpretable. \n2. The analogy from MP-GNN is very interesting, such as the oversmoothing problem and feature collapse, this is inspiring for the future study of contrastive learning.\n3. It proposes some new designs which are also justified by the experimental results, the GAT-like attentive score weights is reasonable and promising.\n\nWeaknesses:\n1. The analogies and two new designs mainly focus on the alignment updates, while negative sampling for the uniformity updates is a well-known problem. I think the authors should also pay attention to the negative sampling part such as the hard negative sampling [1].\n\n\n[1] Contrastive Learning with Hard Negative Samples. Robinson et al., ICLR 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and technically sound. Detailed proofs are attached in the appendix. Basically, this paper makes a novel interpretation of contrastive learning which is very interesting. The experimental details are also provided which enhances its reproducibility.",
            "summary_of_the_review": "This paper is well-written and presents an innovative understanding of contrastive learning. It connects contrastive learning with MP-GNN in both theory and the analogy of problems/techniques. It also provides some ideas for new designs in contrastive learning inspired by the existing tools in MP-GNN. I think this is a solid paper and vote for accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6281/Reviewer_7ZHM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6281/Reviewer_7ZHM"
        ]
    },
    {
        "id": "j_N-AOzyNG",
        "original": null,
        "number": 2,
        "cdate": 1666669279100,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669279100,
        "tmdate": 1670598934548,
        "tddate": null,
        "forum": "VBTJqqWjxMv",
        "replyto": "VBTJqqWjxMv",
        "invitation": "ICLR.cc/2023/Conference/Paper6281/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies contrastive learning (CL) by looking the dynamics of (unconstrained) gradient descent for the contrastive loss and connecting it to message passing schemes on the augmentation graph.\n\n- The \"alignment update\" for CL is a message passing scheme on the augmentation graph, because each feature is updated as a weighted sum of neighborhood features in the augmentation graph\n- The \"uniformity update\" is equivalent to message passing on a \"fake affinity graph\" that is itself defined by the similarities induces by the current features.\n- Under this view, the contrastive update is a competition between two message passing rules and this helps characterize the \"equilibrium solution\"\n\nIt further elucidates connections/analogies between various techniques in contrastive learning and message passing methods on graph neural networks (MP-GNNs), like layer/batch normalization, feature collapse etc.\nInspired by these connections, the paper designs new contrastive learning variants that borrow techniques from GNNs like graph attention jumpy knowledge etc. that perform reasonably well on standard benchmarks.\n\n- Using graph attention mechanism for adaptive positives with a weighted alignment term $\\alpha_{f}(x, x^+) f(x)^{\\top} f(x^+)$, to avoid considering far off positives. Leads to ~1% improvement over vanilla evaluation.",
            "strength_and_weaknesses": "**Strengths**\n\n- The paper establishes many connections between contrastive learning updates and message passing methods that could potentially help both fields from advancements in the other.\n- It is also reasonably easy to follow and well written (comments towards the end).\n- The proposed method that utilizes graph attention mechanism perform slightly better than vanilla CL\n\n**Weaknesses**\n\n- While the paper makes many connections, most are derived from pretty straightforward or known calculations (the connections can still be valuable). If not technical, ideally the main contribution could have been to leverage the connections for new contrastive learning algorithms that do well. The only one that gets good performance (and improvements) is the graph attention idea.\n\n- Using multi-stage aggregation, i.e. using features from multiple epochs rather than just the current one, does avoid complete feature collapse without negatives. However the final performance is still far from CL; I think ~45% on CIFAR-10 is achievable with even a randomly initialized network, so I'm not sure how much to read into those results of ~50%.\n\n- Typos/bugs in derivations and proofs (that may be fixable)\n  - I fail to see why $LSE(FF^{\\top})$ is the same as $\\mathbb{E}\\_{x} \\log(\\mathbb{E}\\_{x'} \\exp(f(x)^{\\top}f(x^{+})))$ in Eq. (2b). It seems like it should be something like $Tr(D \\log(deg(D \\exp(D^{-1/2}FF^{\\top} D^{-1/2}))))$. The proof in Eq (20) seems wrong because the $w$ term vanishes from the $\\exp$ in the next step. Similar the expression for $A'\\_{\\text{exp}}$ below Eq (7) seems wrong and other places that use $FF^{\\top}$ kind of term by ignoring the $D$ factors. I'm not entirely sure if this affects other results (seems like it won't for uniform distribution), but it would be nice to get some clarification from the authors about this\n  - In Eq (8) should the first term be $(1+2\\alpha) I$ instead of $(1+\\alpha)I$? An so should Eq (9) not have the $(1-\\alpha)$ term? If not, then the Equilibrium distribution calculation will be incorrect, since $\\bar{A} = A'$ will give $F^{(t+1)} = (1-\\alpha) F^{(t)}$.\n\n- It might be worth mentioning in Proposition 3 that the equilibrium distribution is true only if $f$ is allowed to be arbitrarily expressive and that it ignores sample complexity considerations. Also, such a result of distribution recovery has been shown in prior work using different techniques, Theorem 1 in [1].\n\n- Other comments/questions\n  - Missing citation for GAT on page 1\n  - \"Empirically, we show that both techniques leads to clear benefits on benchmark datasets\": I would not say that there are clear benefits for both techniques\n\n\n[1] Zimmerman et al. Contrastive Learning Inverts the Data Generating Process",
            "clarity,_quality,_novelty_and_reproducibility": "The connection between contrastive learning and message passing GNN methods is novel and potentially useful, to the best of my knowledge. Most of the paper is clearly written and it was easy to follow even in 1 pass. Some typos and bugs in equations/derivations have been pointed out above.",
            "summary_of_the_review": "I think the connection between CL and GNNs could open up new avenues for ideas in contrastive learning. This paper showed one such connection through graph attentions that lead to a small benefit. Overall it would have nicer if more such connections/insights arose. I believe that the overall contribution of the paper is positive. However due to bugs in the derivations and proofs (listed above), that may or may not affect some results, I am hesitant to accept the paper immediately, and have thus assigned a score of weak reject. Happy to raise the score after further clarification from the authors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6281/Reviewer_WHor"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6281/Reviewer_WHor"
        ]
    },
    {
        "id": "ZKip7vDUuG",
        "original": null,
        "number": 3,
        "cdate": 1666898844198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666898844198,
        "tmdate": 1666898844198,
        "tddate": null,
        "forum": "VBTJqqWjxMv",
        "replyto": "VBTJqqWjxMv",
        "invitation": "ICLR.cc/2023/Conference/Paper6281/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper revisits contrastive learning objectives from the feature propagation perspective. Specifically, by casting alignment and uniformity as two message propagation procedures on two respectively graphs, we can establish equivalence between contrastive learning and message passing graph neural networks. In this way, we can inherit existing techniques in graph neural networks to improve contrastive learning performance.",
            "strength_and_weaknesses": "Strengths:\n+ This paper presents a very interesting perspective on understanding contrastive learning by formulating alignment and uniformity as message passing on graphs\n+ This paper is well-written and the idea is easy to follow\n+ The message passing formulation is supported by both theoretical analysis and empirical evidence\n\nWeaknesses:\nI only have several minor points:\n- The second paragraph in the introduction section is a little bit confusing. I would like to suggest the authors to elaborate on \"model distribution\" and \"data distribution\".\n- I wonder how we can understand the MoCo approach that maintains a memory bank for negatives by constructing similar message passing graphs?\n- In Page 1, GAT is not cited correctly.\n- In Page 5, reference (Eq. 30) should be (Eq. 11).\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written. I do not spot any obvious errors. I think this paper makes a novel contribution both theoretically and empirically for understanding and improving contrastive learning. The authors do not provide source code so I cannot comment on the reproducibilty.",
            "summary_of_the_review": "I like the paper very much. I think the authors make a nontrivial contribution in understanding and improving contrastive learning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6281/Reviewer_KQoy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6281/Reviewer_KQoy"
        ]
    }
]