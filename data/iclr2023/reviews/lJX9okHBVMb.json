[
    {
        "id": "YfWbNiIDL2",
        "original": null,
        "number": 1,
        "cdate": 1666198963948,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666198963948,
        "tmdate": 1666198963948,
        "tddate": null,
        "forum": "lJX9okHBVMb",
        "replyto": "lJX9okHBVMb",
        "invitation": "ICLR.cc/2023/Conference/Paper1797/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new neural network architecture which is radial: instead of pointwise activation, the activation is applied to the whole layer which rescales the output vector according to its norm. Universal approximation results are shown, with additional model compression algorithms.",
            "strength_and_weaknesses": "Strength:\n\n1, the proposed radial neural network is novel. The way it generalizes pointwise activation is natural and elegant, that a simple rescaling of vector is actually expressive.\n\n2, the universal approximation results are solid. The proof method is intuitive and resembles classic ideas in literature.\n\nWeaknesses:\n\n1, I don't see any improvement over previous bounds for universal approximation: for the usual Lipschitz function case, the construction of Theorem 3 requires $O((\\frac{\\sqrt{n}}{\\epsilon})^{3n})$ number of parameters (ignoring poly factors). Theorem 5 improves it to the standard $O((\\frac{\\sqrt{n}}{\\epsilon})^{n})$.\n\n2, the compression algorithm is useful only if there are subsequent layers with notably larger width. However, such neural networks are already sub-optimal theoretically and wouldn't be used in the first place, then there is no need for compression.",
            "clarity,_quality,_novelty_and_reproducibility": "Sections 1-4 are well-written and easy-to-follow. Sections 5-6 are felt not tightly linked to the main results, and I think the mathematical writing can be made more succinct in the main-text. A small polishing on the writing of sections 5-6 could be helpful.",
            "summary_of_the_review": "The results are solid and novel but not as significant: the new radial architecture doesn't show improvement over previous bounds. As a result I lean to weak accept rather than accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1797/Reviewer_MQ2f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1797/Reviewer_MQ2f"
        ]
    },
    {
        "id": "ZcaZwnbuj6c",
        "original": null,
        "number": 2,
        "cdate": 1666568455452,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666568455452,
        "tmdate": 1666568455452,
        "tddate": null,
        "forum": "lJX9okHBVMb",
        "replyto": "lJX9okHBVMb",
        "invitation": "ICLR.cc/2023/Conference/Paper1797/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes *radial neural networks*, neural networks with a non-pointwise nonlinearity that is a scalar rescaling of the vector of activations as a function of its norm. The paper argues that such neural networks are rotation equivariant and proves that they are universal function approximators. It further introduces a lossless compression technique for these networks and supports its claims empirically.",
            "strength_and_weaknesses": "Strengths:\n* Nonlinearities are a fundamental building block of neural network architectures, so new techniques are potentially broadly applicable.\n* The writing of the paper is clear and focused within the respective sections.\n* The range of contributions is quite broad, ranging from the theoretical results on the proposed networks being universal function approximators to a practical compression method taking advantage of their structure.\n\nWeaknesses:\n* Probably as a result of its breadth, the paper somewhat lacks a clear red thread or headline result. I am not really sure who the target audience of the paper is. Is the rotation equivariance a means to the end of constructing neural networks that are easy to compress, i.e. is this a practically minded paper? Do the authors want to establish radial neural networks as a new direction of research? Then what are promising directions for future work and what specific limitations of current architectures are they meant to overcome?\n* There are essentially no quantitative results. I really don't mind if the methods can't be immediately plugged into ResNets or Transformers and give state-of-the-art results, but the paper should at least give a general idea of how well the method works on standard tasks (MNIST, CIFAR, ...). What's the rationale behind choosing the benchmark on the noised MNIST images? Is it expected that radial neural networks perform better on this task?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity** The technical writing is clear, however the key contribution of the work could be worked out better.\n**Quality** The paper is sound as far as I can tell, although I did not go through the derivations or proofs.\n**Novelty** The work is new as far as I am aware, I am however unfamiliar with most closely related pieces of work.",
            "summary_of_the_review": "I think this is interesting research in general, however the core contribution and future directions are not clear from the manuscript to me, so I will recommend rejection for now",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1797/Reviewer_Vbpq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1797/Reviewer_Vbpq"
        ]
    },
    {
        "id": "A9JQgbLvD3",
        "original": null,
        "number": 3,
        "cdate": 1666715629520,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666715629520,
        "tmdate": 1669824393833,
        "tddate": null,
        "forum": "lJX9okHBVMb",
        "replyto": "lJX9okHBVMb",
        "invitation": "ICLR.cc/2023/Conference/Paper1797/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Instead of considering entry wise nonlinearities in neural networks, the paper studies radial neural networks, where the activation function acts to rescale the output of a whole layer. Asymptotic universal approximation ability of radial neural networks is proved. In addition, it is shown that radial neural networks can be compressed with an efficient algorithm based on QR decompositions. Experiments on synthetic data and MNIST are provided to support the theory.",
            "strength_and_weaknesses": "=============== Strength ===============\n\nThe paper manages to contain a rich amount of related references and provide graphical illustration that makes the ideas easy to understand.\n\nFrom a technical point of view, the asymptotic universal approximation property of radial neural networks is new to me. Besides, the compressive nature of radial neural networks and its resulting fast optimization are also not studied in literature.\n\nThe organization of the paper is good and easy to follow.\n\n=============== Weakness ===============\n\nA strong motivation of the practicality of radial neural networks seems to be lacking. It will be helpful to provide working examples of radial neural networks and indicate that in some important applications, radial neural networks outperform pointwise networks.\n\nThe universal approximation theory allows the network size to grow to infinity. It seems unfair to claim that not requiring bounded domain is a contribution. In early asymptotic results, e.g., Cybenko and Barron, neural networks are already shown to be dense in differential function spaces with unbounded domain. Later results established the rate of approximation requires a compact domain. As there is no rate of approximation provided, considering unbounded domain does not add much technical challenges. Moreover, the asymptotic results does not provide understanding of potential advantages of radial neural networks over pointwise neural networks in terms of function approximation.\n\nSynthetic data is relatively toy and the comparison with ReLU network is not fully convincing. On the one hand, synthetic data are low-dimensional and the target function is simple in that the activation function is almost the form of the target function. On the other hand, Step-ReLU radial network outperforms ReLU MLP might correlate to multiple reasons: 1) different activation functions; 2) different network architectures; 3) hyper parameter tuning. By the way, is it possible to simply perform classification on MNIST using radial and pointwise neural networks?",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned, the paper is relatively clear. Some notations might need further elaboration. For example, following Theorem 3, $T_i$ and $S_i$ are affine maps of corresponding input and output dimensions. Is it unclear how $T_i$ can lift the input space to a higher dimension using vector addition.\n\nIt is better to highlight technical contributions in Section 4 and compare to the universal approximation of pointwise networks to demonstrate any potential advantage if possible.",
            "summary_of_the_review": "From a pure theoretical point of view, the paper can be a good study on radial neural networks, if a clearer exposure of technical contributions is provided. Further, I believe the paper would attract more attention if the practicality of radial neural networks is discussed and advantages over pointwise neural networks are demonstrated. The best result would be using the developed theory to explain the advantage of radial neural networks.\n\n=============== Post author response ===============\n\nThank you for the detailed response. The explanation on approximation in $\\mathbb{R}^d$ makes sense to me. Examples of $\\sin x$ are difficult to approximate in $\\mathbb{R}^d$ since they do not vanish at infinity. The ultimate idea is always to truncate the domain.\n\nWith the experiments in Section 7 and Appendix E, the practical motivation of radial neural networks might still be limited. As pointed out, radial neural networks can be advantageous when spherical decision boundaries appear in tasks.\n\nOverall, I think the paper makes interesting contributions to universal approximation theories of neural networks. Nonetheless, practical implications are on the weak side.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1797/Reviewer_vdPZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1797/Reviewer_vdPZ"
        ]
    }
]