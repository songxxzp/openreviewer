[
    {
        "id": "PjvSjfJ4l2u",
        "original": null,
        "number": 1,
        "cdate": 1666570779294,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666570779294,
        "tmdate": 1666570779294,
        "tddate": null,
        "forum": "KzfhxLoh6s0",
        "replyto": "KzfhxLoh6s0",
        "invitation": "ICLR.cc/2023/Conference/Paper1840/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the constrained robust MDP problem where the goal is to learn a policy that maximizes the expected rewards, subject to the safety constraint that the expected cost exceeds certain thresholds. Moreover, the transition model is chosen from an ambiguity set. As a result, the goal is to maximize the reward under the most adversarial model, while ensuring that the safety constraint is met for all models in the ambiguity set. \n\nThe authors propose an algorithm based on the Lagrange multiplier, which transforms the policy optimization problem into a min-max optimization problem. For such a problem, a gradient descent-ascent algorithm is proposed. The authors also establish sample complexity results about how many samples are needed to ensure convergence to an approximate stationary point that also approximately satisfies the safety constraints. \n\n",
            "strength_and_weaknesses": "Strength: \n\nThe model of robust CMDP seems a novel setting that has not been extensively studied in the literature. The algorithm based on Lagrange multiplier is reasonable as it is an adaptation of the existing algorithm for CMDP. (Although I have certain reservations about extending it to the robust setting. See below.) \n\nWeakness: \n\n1. Novelty: The main issue of this work seems a lack of novelty. Both the standard CMDP and robust MDP have been extensively studied in the literature, and this work seems a direct combination of these strands of research. For example, the algorithm is based on the Lagrange multiplier and policy gradient method for CMDP. Seen from the presentation of the algorithm, the authors even omit the dependency on transition $P$, which makes the algorithm extremely similar to a method for CMDP. In terms of analysis, it seems unclear what additional challenge is caused by ensuring robustness with respect to the ambiguity set. \n\n2. Algorithm. It would be great to carefully derive the algorithm. In particular, I think the transformation based on Lagrange multiplier might not be as simple as shown in the paper. In particular, here the functions  $V_r$ and $V_c$ are actually \n$$\nV_{r} = \\inf_{P \\in \\mathcal{P} } V_{P, r} , \\qquad V_{c} = \\inf_{P \\in \\mathcal{P} } V_{P, c} .\n$$\nWhen you write down the Lagrangian, do you consider \n$$\n\\inf_{P \\in \\mathcal{P} } V_{P, r} + \\lambda \\cdot \\inf_{P \\in \\mathcal{P} } V_{P, c}  \n$$\nor \n$$\n\\inf_{P \\in \\mathcal{P} } \\{  V_{P, r} + \\lambda \\cdot  V_{P, c} \\}\n$$\n\n3. Gradient descent ascent. Following the above comment, since the objective involves $\\inf_{P \\in \\mathcal{P} } $, when taking the gradient with respect to the policy parameter and $\\lambda$, it seems unclear how the gradient is computed. How do you differentiate through $\\inf_P$ and how do you estimate the gradient?\n\n4. Convergence to a stationary point. This paper only shows convergence to a stationary point. However, various existing works on CMDP have established convergence to the globally optimal policy and even ensured zero constraint violation. It would be nice to see if the theoretical guarantees in this work can be improved. Convergence to a stationary point seems not to lead to any optimality certificate of the learned policy. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of this work seems limited. Moreover, there is some room for improvement in terms of the theoretical results -- stronger guarantees of the learned policy might be possible.",
            "summary_of_the_review": "The main issue of this paper seems to be a lack of novelty. The algorithm and theoretical results seems to be based on direct combination of results in CMDP and robust MDP literature. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1840/Reviewer_yY2u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1840/Reviewer_yY2u"
        ]
    },
    {
        "id": "DsaE2x_p_VB",
        "original": null,
        "number": 2,
        "cdate": 1666573656312,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573656312,
        "tmdate": 1666573656312,
        "tddate": null,
        "forum": "KzfhxLoh6s0",
        "replyto": "KzfhxLoh6s0",
        "invitation": "ICLR.cc/2023/Conference/Paper1840/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of robust constrained reinforcement learning (RL) where the underlying model has uncertainty. The goal is to ensure that the constraints are satisfied for the worst-case MDP in a set of MDPs, while at the same time to maximize the reward over the uncertainty set. A robust primal-dual algorithm is proposed, with theoretical performance analysis under certain assumptions. An example of $\\delta$-contamination uncertainty set is specifically investigated. Three numerical examples are shown to support the validity of the proposed solution.",
            "strength_and_weaknesses": "Strength:\n+ This work studies an important and timely problem of robust constrained RL. \n+ The proposed robust primal-dual algorithm is valuable.\n+ The theoretical analysis is technically sound (under the given assumptions and the chosen approach).\n\nWeakness:\n- The problem formulation has some inconsistency in terms of the (uncertain) MDP. The formulation in eq. (3) will mostly result in different worst-case MDPs for the \"worst-case reward\" and  the \"worst-case cost\". However, the motivating example is that the training MDP is different than the test MDP. In fact, majority, if not all, of practical applications should have the same MDP where both reward and cost are evaluated, as they are simultaneously collected. This should not be over different MDPs. The authors have specifically discussed the worst-case transition kernels being possibly different for the two robust value functions, but I am having a difficult time understanding why this is useful to begin with.\n- The authors essentially got around the non-differentiable robust Bellman operator issue by taking a differentiable approximation. I understand that such approximation is made in order to make progress. However, despite that this approximation can be close to the original function, any approximation in a **robust** optimization formulation always raises the question of how these two ``uncertainties'' reconcile. Would solving the approximated problem lead to actual constraint violation? How does this affect the rigor of the original problem? Additionally, one can imagine that there could be different ways to do this approximation --  why did the authors choose the current approximation, and how is this superior than others?\n- The above issue also happens when the authors propose to *estimate* the smoothed robust functions. Again, how does the estimation error affect the constraint violation and the rigor of the optimization problem?\n- Assumption 2 and Lemma 4 have two lower bound values $\\zeta$ and $\\zeta'$. Are these two values needed for Alg. 1 and eq. (16), both of which need $\\Lambda^*$? How can we have these values in practice? Knowing $\\Lambda^*$ exists (finite) and selecting its value are non-trivial.",
            "clarity,_quality,_novelty_and_reproducibility": "- Citation format should be cleaned up. citep and citet are not used properly. This has made the paper less readable.\n- $G_t$ are re-used in both (9) and (17). Please change one.",
            "summary_of_the_review": "Overall, this work made progress in the robust constrained RL problem, which is important and timely. The proposed algorithm has its value and the analysis is sound, but there are quite a few issues that are not resolved. This leads to my recommendation of marginal rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1840/Reviewer_TPvy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1840/Reviewer_TPvy"
        ]
    },
    {
        "id": "tGoRXqn1Tt",
        "original": null,
        "number": 3,
        "cdate": 1667171696144,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667171696144,
        "tmdate": 1667171696144,
        "tddate": null,
        "forum": "KzfhxLoh6s0",
        "replyto": "KzfhxLoh6s0",
        "invitation": "ICLR.cc/2023/Conference/Paper1840/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a method for robust constrained RL, a setting where the RL agent is tasked with maximizing the worst-case expected reward subject to a constraint that worst-case costs incurred from the environment are constrained. Here the worst-cases are evaluated as the worst-case among models of the environment in some uncertainty set. The algorithm itself optimizes the lagrangian of the constrained problem by performing gradient updates. To evaluate the expected worst-case reward and constraint, the authors leverage previous work that computes smoothed robust value estimates using log sum exp as an approximation for the worst-case setting. The authors demonstrate that this algorithm should achieve a solution with bounded infeasibility with guarantees on convergence and runtime due to assumptions on the problem. The authors evaluate their algorithm against a constraint-agnosting RL agent as well as a heuristic algorithm for robust constrained reinforcement learning from previous work.\n",
            "strength_and_weaknesses": "The main strengths of the paper are in the theoretical evaluation of their algorithm in their specific setting and in evaluating their algorithm against a reasonable baseline showing improved performance. Additionally, their algorithm is reasonable and straightforward and they evaluate the complexity of their convergence.\n\nThe main weaknesses of the paper are in the experimental evaluation. It is unclear whether the results demonstrate that the learned model satisfies the constraint robustly as is desired, although it clearly satisfies the constraints \u201cbetter\u201d than previous heuristic approach and a standard approach. Additionally, it would be helpful to explain exactly what metrics are used since it is slightly unclear whether the evaluation is a worst-case evaluation of the incurred costs and rewards, or just the incurred costs and rewards. Additionally, it is unclear whether the worst-case evaluations come from the approximation or are evaluated by finding the worst-case scenario from the uncertainty set. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper itself is fairly clear and would benefit from some additional plain-english names for the quantities such as the robust estimation of rewards / costs. Overall, the paper is easy to follow and the underlying ideas are straightforward once the variables are understood as the paper itself is well-written.\nAdditionally, the authors would benefit from some examples of where this model might be used where the domain-expert wants to train a model that robustly gives high worst-case performance and satisfies constraints in the worst-case. Even though it seems clear that practitioners would want this, a couple of working examples would help make this more concrete.\nThe domain itself seems somewhat novel given the referenced related work, although I don\u2019t work in the space of constrained RL. However, it would be helpful to explain what differentiates the approach from constrained RL where the robustness requirement might simply be encoded into the costs.\nTo enable reproducibility, it would be helpful to either release code or further specify what models were used in the application domains. \n\nSmall comments:\nP5: change to \u201cconverges more stably and faster\u201d\n",
            "summary_of_the_review": "Overall, I think that the paper approaches an important problem for the RL community. I see the main strength of the paper as presenting a theoretically-motivated approach to solving their problem and evaluating the approach against a reasonable baseline on several simulated domains. I consider the main room for improvement for this work to be in the experimental evaluation and expanding upon whether or not the approach meets the proposed robustness requirements. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1840/Reviewer_gZnw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1840/Reviewer_gZnw"
        ]
    }
]