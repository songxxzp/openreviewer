[
    {
        "id": "pDkRT_lJwzy",
        "original": null,
        "number": 1,
        "cdate": 1666627761869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627761869,
        "tmdate": 1666627761869,
        "tddate": null,
        "forum": "gNI4_85Cyve",
        "replyto": "gNI4_85Cyve",
        "invitation": "ICLR.cc/2023/Conference/Paper6511/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors reformulate the problem of intent detection as a question-answering task by treating utterances and labels as questions and answers. A two stage training schema is employed by utilizing question-answering retrieval architecture and batch contrastive loss. The first of the training stages is for learning better query representations, while the second stage is for optimizing the contextualized token-level similarity scores between queries and answers from the same intent. State-of-the-art results are reported on three few-shot intent detection benchmarks.\n",
            "strength_and_weaknesses": "Strengths\n\nS1: It is an interesting idea to treat ID problem as a question-answering task.\n\nS2: Utilization of Token level domain knowledge in a self-supervised fashion along with queries adaptation to dialogue domain is interesting.\n\nS3: Masked language modeling has also been employed as auxiliary loss.\n\nS4: Intent names have been employed for generating extra supervision signals.\n\nS5: There are detailed experiments in this work, with an interesting insight that ColBERT triplet loss with batch contrastive loss leads to a considerable improvement.\n\nS6: The paper is well written and easy to follow.\n\n\nWeaknesses\nW1: Why a balanced dataset is the requirement for this work. If the dataset is imbalanced, how will the proposed method behave?\n\nW2: Typos: In abstract: \u201cself supervise manner.\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the results are good. This work is for balanced few-shot datasets. It will be interesting to see if the proposed techniques work on imbalanced datasets as well.\n",
            "summary_of_the_review": "Overall, this is an interesting paper and addresses the challenging intent detection task in a question-answering setting. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6511/Reviewer_ZHT4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6511/Reviewer_ZHT4"
        ]
    },
    {
        "id": "4ABXukfMw8C",
        "original": null,
        "number": 2,
        "cdate": 1666654667839,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654667839,
        "tmdate": 1666654667839,
        "tddate": null,
        "forum": "gNI4_85Cyve",
        "replyto": "gNI4_85Cyve",
        "invitation": "ICLR.cc/2023/Conference/Paper6511/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a Question-Answering inspired Intent Detection System (QAID). The paper treats the intent identification task as a Question-Answering retrieval task by treating the utterances and the intent names as queries and answers respectively. QAID adapts ColBERT architecture from prior work and replaces the loss function with batch contrastive loss. The novelty of this work is in converting intent id task to Question-Answering form and using the intent names as answer, and using supervised batch contrastive training for finetuning LM on this task. ",
            "strength_and_weaknesses": "Strengths: \n\n1. The paper reports SOTA results in three benchmark intent identification tasks for both 5-shot and 10-shot setting. \n\n2. The paper shows ablations to support the claims on supervised batch contrastive training and late interaction scores. \n\n3. The paper is well written and easy to understand \n\n \nWeakness: \n\n1. The ablations can be improved. The paper can show ablation to understand the component of each component and the impact it has on the intent id task. The missing ablations are: QAID \u2013 batch contrastive,  QAID \u2013 intent names (not sure if this is possible) and QAID \u2013 data augmentation. But it would be good to show results by removing one component at a time. ",
            "clarity,_quality,_novelty_and_reproducibility": "1. Where are the ablation results for \"data augmentation\". The paper says that data augmentation results are inconsistent but the results are not shown in Table 3. ",
            "summary_of_the_review": "Overall, this paper builds on top of prior work (especially ColBERT and CPFT) but uses intent names for retrieval. The paper shows the benefit of combining these techniques and presents SOTA results and shows good ablations. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6511/Reviewer_73BF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6511/Reviewer_73BF"
        ]
    },
    {
        "id": "NX_ZWP3fkMZ",
        "original": null,
        "number": 3,
        "cdate": 1667684942046,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667684942046,
        "tmdate": 1667685160142,
        "tddate": null,
        "forum": "gNI4_85Cyve",
        "replyto": "gNI4_85Cyve",
        "invitation": "ICLR.cc/2023/Conference/Paper6511/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper reformulates intent detection as a retrieval task, by treating utterances and intent names as questions and answers respectively. The authors leverage a dual-encoder based retrieval architecture for few-shot intent detection, with late-interaction scores (as used in ColBERT) and batch contrastive training. ",
            "strength_and_weaknesses": "Strengths:\n\nFormulating as a retrieval task helps make use of current SOTA techniques in retrieval such as late interaction scores and batch contrastive loss. The idea of using a retrieval-based dual encoder for this task is interesting from an efficiency perspective too, since the intent names (which are the answers) can be pre-computed during inference. This overcomes the high latency of cross-attention based approaches for which the processing time scales with the number of intents.\n\nWeaknesses:\n\n1) It's not entirely clear why augmentation is needed and what is the motivation for / benefit from using the data augmentation module\n\n2) Since CPFT seems to be most similar to this paper, the authors need to highlight better what the main differences are with CPFT. From reading the paper, the impression I get is that the authors just incorporate the few-shot intent detection methodologies used in CPFT (such as self-supervised contrastive pre-training and fine-tuning) into a dual-encoder based retrieval formulation (by using ColBERT)\n\n3) It is not surprising to see benefits from using late-interaction scores and batch contrastive training (which is one of proposed contributions). ColBERT/ColBERT-V2, which are based on late-interaction scores, have already been shown to outperform cosine similarity based matching in dual encoder models. Further, batch contrastive loss is already a popularly used training technique for dual-encoder based retrieval models (such as DPR, RocketQA)",
            "clarity,_quality,_novelty_and_reproducibility": "The writing in the paper can be improved with a better description of the two-stage contrastive training subsection. (Some terminologies in there, such as anchor/pivot, are hard to understand). A more comprehensive schematic illustration (compared to Figure 1) will help. Further, adding some qualitative examples showing why the proposed changes show improvements can help strengthen the motivation.\n\nThe main originality in the paper is the reformulation intent detection as a retrieval task. Other proposed contributions are mainly about combining ideas from retrieval literature with those recently proposed for few-shot intent detection in the CPFT paper.\n",
            "summary_of_the_review": "Overall the paper shows good (and statistically significant) improvements over existing methods for few-shot intent detection. However, the novelty in the paper is limited as it mainly involves combining existing techniques in a retrieval-based formulation (such as batch contrastive loss and late-interaction scores) with methodologies for few-shot intent detection proposed in CPFT (such as self-supervised contrastive pre-training and fine-tuning)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6511/Reviewer_rUR4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6511/Reviewer_rUR4"
        ]
    },
    {
        "id": "ZQPM7xongN",
        "original": null,
        "number": 4,
        "cdate": 1667810260219,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667810260219,
        "tmdate": 1667810260219,
        "tddate": null,
        "forum": "gNI4_85Cyve",
        "replyto": "gNI4_85Cyve",
        "invitation": "ICLR.cc/2023/Conference/Paper6511/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes QAID, Question Answering inspired Intent Detection system, which models the intention detection classification as a question-answering task. The model uses two stages of training: a pretraining for better query representation and finetuning on few-shot labels of query and answers (name of intents). Certain choices of model, such as token-level similarity, batched contrastive learning, and inference with answers only, are verified by detailed ablation studies. The proposed method achieves SOTA  performance on three intention detection dataset from DialoGLUE.",
            "strength_and_weaknesses": "Strength: \n\n- The paper uses and adapts techniques from QA and answer retrieval to the task of ID. The method is effective and efficient on three benchmark datasets when compared with various baselines.\n- The detailed ablation study verifies the necessity of stages and component of proposed framework.\n- The paper is well-written and easy to follow.\n\nWeakness:\n- Given the number of intents being only a few hundreds at most, is Faiss really necessary for inference?\n- n has two meanings in section 2.1: one for batch size and another for number of tokens.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written.\nQuality: The statements are well supported. The empirical results are solid.\nNovelty: The paper adapts the method from question answering to intent detection. The adaptation itself is novel although the proposed methods are a combination of existing techniques.\nReproducibility: It requires some effort to reproduce the results.",
            "summary_of_the_review": "The paper is inspired by the development of ColBERT in QA task and propose a variant of the model for the task of intent detection. The proposed model makes several adaptations including batch contrastive loss and  signal from the intent names. The proposed method achieves SOTA performance on three few-shot ID benchmarks. And the ablation study proves the necessity of various components in the system. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6511/Reviewer_uX1q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6511/Reviewer_uX1q"
        ]
    }
]