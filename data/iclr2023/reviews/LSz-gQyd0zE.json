[
    {
        "id": "wV-p9HkFuk",
        "original": null,
        "number": 1,
        "cdate": 1666451791244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666451791244,
        "tmdate": 1666451791244,
        "tddate": null,
        "forum": "LSz-gQyd0zE",
        "replyto": "LSz-gQyd0zE",
        "invitation": "ICLR.cc/2023/Conference/Paper1837/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work improves the DA-Transformer with an N-gram based fuzzy alignment loss for non-autoregressive machine translation. Instead of enforcing strict monotonic alignment and NLL loss in the original DA-Transformer, the proposed method adopts a fuzzy loss similar to N-gram precision in the calculation of BLEU. With evaluations on standard MT datasets, the proposed method is shown to obtain impressive results using raw (non-distilled) training data, bringing improvements over the DA-Transformer and almost closing the gaps to auto-regressive models.",
            "strength_and_weaknesses": "Strength:\n\nThe work provides an effective way to calculate expected n-gram counts for the DAG-based decoder.\n\nThe results seem impressive, with clear improvements over the baselines and the final FA-DAT models almost close the gaps to auto-regressive ones using raw data.\n\nWeakness:\n\nOne concern is that the proposed model needs to use a relatively large lambda (decoder length) to obtain good enough results. This might not be a problem when performing single-instance decoding, while this brings extra operations and might more obviously affect batched decoding. \n\nThe proposed method seems a natural fit to evaluating using BLEU, but it is unclear with other MT evaluation metrics. (I\u2019m not sure whether BERTScore is a good metric for MT evaluation). \n\nSome more analyses and ablations might be needed, for example, on the effects of the initialization from NLL-trained DA-Transformer, the adaptation of the brief penalty.\n",
            "clarity,_quality,_novelty_and_reproducibility": "It would be better if there could be more illustrations (probably figures) for the descriptions of the proposed loss function and the core algorithm, which might allow easier reading.\n\nIt seems slightly strange to me that in Equation (9)/(10), suddenly \\theta appears as the first argument on the left-hand side.\n",
            "summary_of_the_review": "Overall I think this paper did a relatively good job on providing an effective algorithm for calculating expected n-grams and pushing the raw-training results close to the auto-regressive ones. My main concern is the evaluation of decoding efficiency, where the large decoder length might have an undesired effect when using batched decoding.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1837/Reviewer_zAdY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1837/Reviewer_zAdY"
        ]
    },
    {
        "id": "JgmLvTwEEAE",
        "original": null,
        "number": 2,
        "cdate": 1666574503451,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574503451,
        "tmdate": 1666574503451,
        "tddate": null,
        "forum": "LSz-gQyd0zE",
        "replyto": "LSz-gQyd0zE",
        "invitation": "ICLR.cc/2023/Conference/Paper1837/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new training method for DAT (SOTA NAT model). The authors hold the view that all paths in the graph are fuzzily aligned with the reference sentence. Hence, they train the model to maximize a fuzzy alignment score between the graph and reference. The proposed method is interesting, and the improvement of performance for DAT is exciting. ",
            "strength_and_weaknesses": "strengths:\n1. The final performance of FA-DAT is exciting.\n2. It is interesting to train NAT models with fuzzy alignment of n-grams, which relaxes the order constraints of n-grams.\n\nweaknesses:\n1. The new training method for DAT is too similar to the proposed training method in [1].\n\n[1] Chenze Shao, Yang Feng. Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation, NeurIPS 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this work is good.\nThe clarity of the proposed method still has room for improvement.\nThe originality of this work is ordinary.",
            "summary_of_the_review": "The performance of the proposed method is exciting, and the experimental content is complete, but the proposed method is innovative.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1837/Reviewer_Qie9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1837/Reviewer_Qie9"
        ]
    },
    {
        "id": "T2TLDm8YoA",
        "original": null,
        "number": 3,
        "cdate": 1666762384962,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666762384962,
        "tmdate": 1666762384962,
        "tddate": null,
        "forum": "LSz-gQyd0zE",
        "replyto": "LSz-gQyd0zE",
        "invitation": "ICLR.cc/2023/Conference/Paper1837/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "To alleviate the problem of multi-modality, the paper introduces a training objective that could consider multiple translations in the directed acyclic graph. The method is based on a fuzzy alignment between reference and the directed acyclic graph based on n-gram matching.  Authors explore an alignment score to measure the fuzzy alignment, where an efficient algorithm in linear complexity is developed to cope with the exponential search space.  Experiments on major WMT benchmarks validate the proposed approach.",
            "strength_and_weaknesses": "Reasons to accept:\n- the intuition is clear and the solution is straightforward \n- a novel algorithm with linear complexity is proposed to calculate alignment scores, making it viable to exploit exponential search space. \n- the proposed method does not involve knowledge distillation, which is worth noting for NAT. The proposed method keeps the decoding speedup, demonstrating the potential of NAT trained by only raw data. \n\nReasons to reject:\n- it is not clear for how many sentences in the training data suffers from the multi-modality problem. It is not clear whether there are sentences that are more different from the reference, so that the fuzzy alignment does not work at all. It will help if such analysis is provided.\n\n- brief penalty is introduced to prevent the favor for shorter translations. however, it may bring an additional parameter to tune, and make the method less generalize to different datasets or translation distributions. There is no discussion for this issue.\n\n- The experiment section could be improved. This paper proposes the fuzzy alignment for the multi-modality issue, but there are no corresponding experiments for this claim. \n\n\n\nSuggestions:\n\nThe derivation from eq 6 to eq 7 should be presented clearly (in the main body or in an appendix).\n\nWould authors provide more evidence that your approach does improve the DAT's ability for multi-modality? E.g., can FA-DAT cater for the position shifts and word reorderings issues than baselines? \n\nA few examples may also be helpful for demonstrating the effect of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, identifies an important problem, and proposes a nice solution.",
            "summary_of_the_review": "I'd like to see this paper accepted, although some more analysis or experiments may be needed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1837/Reviewer_5ZNc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1837/Reviewer_5ZNc"
        ]
    }
]