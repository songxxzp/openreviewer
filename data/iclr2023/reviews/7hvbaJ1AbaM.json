[
    {
        "id": "TA_ymYncib",
        "original": null,
        "number": 1,
        "cdate": 1666224634692,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666224634692,
        "tmdate": 1666224634692,
        "tddate": null,
        "forum": "7hvbaJ1AbaM",
        "replyto": "7hvbaJ1AbaM",
        "invitation": "ICLR.cc/2023/Conference/Paper2927/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to train classifiers with guaranteed Anchors-style interpretability by structuring the classifier in two parts: a feature selector and a classifier that is only provided the selected features. During training, the classifier portion is also subject to adversarial feature selectors. The features selected by the first component thus may serve as an explanation of the classifier's output: under some conditions on the data and families of classifiers, the chosen features are sufficient to indicate the class. Some simple experiments suggest that the method can be applied to neural networks that achieve good performance in practice (on MNIST...) and illustrate resistance to manipulation.\n\nIncidentally, although the paper suggests that it is inspired by \"interactive proofs,\" there is not much interaction to speak of: the prover sends a message to a verifier who accepts or rejects. It's much more accurate to say that it's an NP style certificate. Calling it an interactive proof only makes it misleading to those who know what that means, and mysterious to those who don't.",
            "strength_and_weaknesses": "The main strength of this paper is that it does seem to suggest a way to obtain reasonable explanations while still using modern neural network methods. The adversarial training method overcomes some kinds of spurious \"explanations\" that may crop up when merely enforcing, e.g., sparsity of the chosen features.\n\nThis paper may have potential, but the current submission has some serious weaknesses:\n\n- The key quantities of \"AFC\" and imbalance of the prover/adversary that underlie the theoretical guarantee are named and even suggested to be helpful in thinking about interpretability, but they are *not* defined or sketched in the paper. I know they are in an appendix, but I'm not sympathetic to attempts to circumvent the page limit by punting the main concepts to the appendix. It doesn't bode well for their conceptual clarity when they could not be described in the main submission. It also makes it difficult to judge the strength of the theorems. I have no idea whether or not the assumption that these quantities are small constants is actually reasonable, and no evidence is presented for the assumption.\n\n- The experiments are very limited: there is one data set for each point the paper seeks to make regarding manipulation and effectiveness, respectively the UCI census data set and MNIST. (Incidentally, Deng 2012 is certainly not the origin of MNIST... it's LeCun et al. 1998) The experiments are thus illustrative but not so convincing: how broadly do the claims hold?\n\n- While explanations using a small number of features is generally desirable, the method as presented seems to scale exponentially with the number of features, which may not be adequate for more complex tasks -- for example, even for a task like CIFAR, it isn't clear to me how effective this would be. (This also ties into the previous issue)\n\n- (relatively minor) The formalism of feature spaces used in the paper seems to only serve to obfuscate the presentation. As far as I can tell, it plays no actual role in the development, and ultimately the explanations are just fixing small subsets of the coordinates to given values when viewing each input as a vector. (If the authors are looking for something to cut in order to introduce the key quantities, this discussion does more harm than good in my opinion.)\n",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed above, clarity is a serious issue with this work, and some of the claims have little to no evidence provided. This isn't an issue with reproducibility exactly, but it does impact the quality of the work. It seems very preliminary.\n\nOn the other hand, there is an idea here that may have potential, and an interesting perspective. So I'd give high marks for novelty overall.",
            "summary_of_the_review": "I find the idea interesting and I hope the authors give it the presentation and evaluation it deserves.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2927/Reviewer_LYB7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2927/Reviewer_LYB7"
        ]
    },
    {
        "id": "paW2VHodWad",
        "original": null,
        "number": 2,
        "cdate": 1666661513239,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661513239,
        "tmdate": 1666700427246,
        "tddate": null,
        "forum": "7hvbaJ1AbaM",
        "replyto": "7hvbaJ1AbaM",
        "invitation": "ICLR.cc/2023/Conference/Paper2927/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new framework for interpretable machine learning relying on interactive proofs ideas. The approach relying the adversarial training of two agents resulting in the most informative features for a given task. The authors provide theoretical results on the performance of the agents, as well as demonstration on a few numerical settings.",
            "strength_and_weaknesses": "The framework is very interesting, novel and valuable to the community. At the same time, several components of the presentation are unclear, and I parts are severely overstated, ignore previous results, and should be addressed before publication.\n\n\n1. Respectfully, I find a lot of the technical presentation and definitions confusing and, at best, non standard. Let me elaborate:\n\na) A \"dataset\", D, is never defined. The word \"dataset\" is typically used to refer to a finite collection of samples drawn from a distribution (see [1] and [2] for classic references). It seems that this might not the case here, however, because (looking at Example 2.2, $D\\subset [0,1]^d$, so $D$ can be interpreted rather as a sample space. On the other hand, in other parts of the manuscript, the authors insist of having $D$ be a collection of data. This leads to (at least) two other problems: i) is this collection finite? It seems that it is, otherwise the second loop in Algorithm 1 would never finish, and ii) If it is finite, and the models are trained on this, all of the analysis regards the features that are obtained from this finite training set, and not on the expected performance of the respective metrics over a new sample. \n\nb) The authors choose to define \"feature\" in terms of *subsets of possible samples* from a dataset. This is not what we refer to features in statistical learning (again, see [1,2] for definitions). This leads to other confusing (if correct?) statements, like: \"$x$ **contains** a feature $\\phi$ if $x \\in \\phi$\". The notation also makes it unclear to understand how features depend on specific samples. E.g., in Eq. (2), $M^*$ depends on a sample $x$, but it's not clear to the reader that $\\phi$ is a subset of a specific sample $x$.\n\n2. A few aspects of the presentation of this work are highly overstated: the authors insist this is \"the first provable interpretability guarantees\". This is incorrect for a couple of reasons: \n\na) There is a number of approaches that provide probably correct explanations for machine learning classifiers. All of these rely on certain assumptions, that vary in complexity. Moreover, in many settings these algorithms are not only correct but also efficient. E.g., the work [3] presents a linear time algorithm for instance wise feature importance scoring on black-box models based on the Shapley framework. The work in [4] does something similar, providing linear time algorithms for provably interpretability of imaging data for general predictors under mild assumptions. \n\nb) Importantly, the methods above (as the majority of the explainability methods for 'off-the-shelf' predictors) are applicable to any given classifier. The method proposed in this work *is not*, and cannot be applied to any given model. Instead, this manuscript proposes to train *a new* classifier (Arthur) that is inherently interpretable, as it is so that it relying only on specific features. This is a totally valid strategy, and indeed a very interesting alternative approach to interpretable ML, but this difference should be made clear and precise to the reader. Moreover, work on this vein has also been explored in what I believe is the most closely related work to this manuscript [5]. Similar to this work, the work in [5] also provides provably correct interpretable predictors that rely on a finite number of features (or queries).\n\nc) Even while the authors provide guarantees for their general method, the application of these ideas do not inherit these nice properties because -as the authors state- convergence to an equilibrium point is not guaranteed, and since their models are trained on finite datasets only. Furthermore (probably inheriting my confusion from point (1)), the results seem to hold only for the training set.\n\nGiven all these points, the statement that the authors provide \"the first provable interpretability guarantees\", and its variations that appear throughout the text, are grossly incorrect. The authors should moderate these comments, and they should better situate their contribution in light of these previous results. The authors should not worry about this, however - their proposed method is very interesting regardless!\n\n\n3. An important limitation of the presented approach, that is stated simply in passing, is that they can only consider realizable learning problems: they consider that the label is determined by a concept function c(x) (i.e. the Bayes Risk is 0). The authors should consider making this very explicit, as it is far from a mild assumption in a statistical learning problem (e.g. this likely does not hold for the UCI Census Income dataset).\n\n4. After presenting their main result, and commenting on the implications of the theorem 2.8, the authors informally introduce the notion of \"asymmetric feature concentration (AFC)\", $\\kappa$. From their explanation, it's clear that this is a key property of classification problems, and Lemma 2.9 attempts to characterize this (providing an upper bound to $kappa$). However, $k$ is never defined formally, so it's basically impossible to interpret the utility of Lemma 2.9. This AFC is later formalized and defined in the appendix, but is important to the reader to have some aspects of this more precise definition in the main corpus of the story - otherwise, there's little utility in having Lemma 2.9 in the main body of the paper.\n\n5. Towards the end of section 2, the authors comment on \"biased samples\", but they never define or otherwise make precise what they mean. Could you clarify? \n\n6. Lastly, the empirical validation is (imho) a bit limited. Moreover, the authors comment that for the Census Income dataset, they remove the 'relationship' and 'marital status' features as they were strongly correlated with the variable 'sex'. Does this mean that the method is very sensitive to correlated features? Why would this be the case? I suggest the authors to include these results in their paper too, and comment on this limitation. This can only clarify the presentation of your method.\n\n\n[1] Shalev-Shwartz, Shai, and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.\n[2] Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.\n[3] Chen et al. \"L-shapley and c-shapley: Efficient model interpretation for structured data.\" ICLR 2019.\n[4] Teneggi. \"Fast hierarchical games for image explanations\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022\n[5] Chattopadhyay, Aditya, et al. \"Interpretable by Design: Learning Predictors by Composing Interpretable Queries.\" arXiv preprint arXiv:2207.00938 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "* A few elements of the technical presentation are not clear (see above).\n* If my understanding is correct, the approach is very interesting and novel.",
            "summary_of_the_review": "In summary, my current understanding of the method (given the confusing definitions in point (1)) leads me to believe that the approach described here is very interesting and a nice contribution. To be certain about this, however, I would need clarification on my comments above, and potentially re defining some of these elements and statements. In closing, I want to stress that while some of my comments above might appear as requiring the authors to make the limitations of their method more explicit and clearer, it is the opinion of this reviewer that this will only increase the value of this contribution - and with it, my support for this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2927/Reviewer_zBZW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2927/Reviewer_zBZW"
        ]
    },
    {
        "id": "7AzbYjwTEqE",
        "original": null,
        "number": 3,
        "cdate": 1666789007705,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666789007705,
        "tmdate": 1670087878857,
        "tddate": null,
        "forum": "7hvbaJ1AbaM",
        "replyto": "7hvbaJ1AbaM",
        "invitation": "ICLR.cc/2023/Conference/Paper2927/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors describe a classification framework that consists of a verifier (Arthur) and two provers (one collaborative [Merlin] and one adversarial [Morgana]). The provers aim to convince/trick the verifier in making a correct/incorrect classification. \n\n**Theoretical results:**\n* **Mutual information bound at equilibria:** The authors prove that if Arthur has low error rate, then at the minimax equilibrium, the features selected by Merlin have very high mutual information with the label. \n* **Bound on average precision in terms of dataset Asymmetric Feature Concentration (AFC), relative success rate and class imbalance:** The authors give a bound on average precision using a couple concepts they define, such as AFC and relative success rate. \n\n**Empirical results:**\n* **Training algorithm:** The authors provide an algorithm to solve for the equilibria of the proposed framework. \n* **Results:** The soundness of the framework and the proposed bounds are tested on a low-dimensional tabular dataset (UCI dataset and MNIST). \n\n**Contributions:**\n* **Theoretical results:** Derives bounds on the mutual information between the label and the features selected by the framework described in the paper (bounds are quite similar to those derived by [1]). \n* **Defining Asymmetric Feature Concentration:** The authors define \"asymmetric feature concentration\", a quantity that characterizes when  \"a set of features is strongly concentrated in a few data points in one class and spread out over almost all data points in another\". They later use this to bound the average precision of a given feature selector. Both this concept and this bound (as far as I now) are novel \n* **Experiments:** Demonstrates the proposed method on a low-dimensional tabular dataset (UCI data), and the theoretical bounds on MNIST. \n\n[1] Chang, Shiyu, et al. \"A game theoretic approach to class-wise selective rationalization.\" Advances in neural information processing systems 32 (2019).",
            "strength_and_weaknesses": "**STRENTGHS:** \n* **Important and timely research direction:** Interpretability methods that provide guarantees is a very important research direction, and the discussed method tackles this very important problem. \n* **Clear writing and exposition:** The paper is written very well and the concepts are described with clarity. \n* **Formalization is done well:** The way the theoretical results are phrased and derived look sound. The proofs also appear to be correct. \n* **Bounds regarding AFC is novel:** The bounds regarding AFC (lemma 2.9 and Theorem 2.10) appear to be novel. The concept of AFC seem to be useful when dealing with certain types of datasets where the \"a set of features is strongly concentrated in a few data points in one class and spread out over almost all data points in another.\" Theorem 2.10, as far as I know, is interesting and novel. \n\n\n****\n\n**WEAKNESSES:**\n* **Missing crucial references:** The submission doesn't cite related work that, in aggregate, cover a big portion of the claimed contributions. \n  * \"A game theoretic approach to class-wise selective rationalization.\": [1] This paper proposes an interpretability framework that describes an extremely similar decision pipeline (adversarial and collaborative agents and a \"verifier\"). **This paper also provides bounds mutual information on the features and the labels** that seem to be analogous to the results reported in the current submission (though not identical0. Both submissions constrain the provers as feature selectors (i.e. masks on all input features) and work with very similar loss functions. There are subtle differences (i.e. the submission allows the verifier to output \"I don't know \" explicitly), but these don't represent conceptual disagreements. The \"cheating\" failure mode described in the submission is described as \"degeneration\" in this submission.\n  * \"Learning to Give Checkable Answers with Prover-Verifier Games.\": [2] This submission **also explicitly proposes a framework with a prover and a verifier aimed at discovering complete and sound proof-verification protocols**. Here, the collaborative and adversarial components are condensed in a single prover with a different loss function. The prover isn't constrained to output masks in this case. This paper also explicitly links it's contributions to Interactive proof systems. \n* **Limited novelty:** Given the above prior works (along with the \"AI Safety via Debate\" paper, which the authors cite), the conceptual contribution of this submission is significantly diminished. Contrary to what the authors claim, this is not the first method that achieves feature based interpretability. \n* **Experiments are not real-world as claimed:** While the authors claim that they use real-world datasets to display the proposed algorithm and the theoretical bounds, the results are only limited to the UCI Census Income dataset (which is simple enough to support exhaustive search over selected features) for displaying the algorithm, and the MNIST dataset to display the theoretical bounds.\n* **Proposed training algorithm is not practical:** Algorithm 1 (Merlin-Arthur Training) requires finding the argmin/argmax of the agents at each iteration, and appear to be very difficult to scale to more difficult/larger problems. \n* **Theorems assume minimax optimality:** The proofs assume that an equilibrium of the proposed game is found. To complement this, an algorithm to check if an equilibrium has been found (or approximated to a sufficient degree) is needed. Solutions that appear to be near an equilibrium might behave significantly differently than how solutions at the equilibria could. This limitation, I believe, should be highlighted in the Limitations. (happy to be \n\n****\n\n**QUESTIONS TO AUTHORS:**\n* **About resistance to existing interpretability manipulation techniques:** Among the contributions, you list \"We show that our setup is resistant to existing interpretability manipulation techniques.\". Do you mind elaborating on which experiments support this claim? \n* **The \"I don't know\" option:** Is allowing Arthur to output \"I don't know\" essential in establishing the theoretical/empirical results? It seems like assigning uniform probability to all output logits would be an equivalent way for a classifier to say \"I don't know\". Is there a clear benefit to keeping having this as a separate option? \n* **Question about Example 2.2** It seems like $y$ isn't defined on coordinates that are not in $S$. Later in the experiments, you mention that these coordinates can be an arbitrary value or 0s. Just wanted to confirm if the definition somehow subsumes this.\n* **AFC of real world datasets:** Why do you think the AFC of real-world datasets will likely have and AFC of 1? \n* **Why remove relationship and marital status features?** Could you explain why it was necessary to remove relationship and marital status features features from the dataset? In many cases removing such separate but highly correlated features is not possible (either because feature separation isn't possible in the input space, or because one doesn't know which features are highly correlated. Time permitting, could you show the results when you don't remove these features? How does the performed protocol behave in that setup? \n* **Are the results on UCI and MNIST datasets evaluated on a separate test set?** Just wanted to check if the trained feature extractors are evaluated on an unseen split. \n\n\n\n[1] Chang, Shiyu, et al. \"A game theoretic approach to class-wise selective rationalization.\" Advances in neural information processing systems 32 (2019).\n[2] Anil, Cem, et al. \"Learning to Give Checkable Answers with Prover-Verifier Games.\" arXiv preprint arXiv:2108.12099 (2021).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and quality:** The paper is written very well and clearly. The theorems/proofs are also written clearly. \n\n**Novelty:** As stated above, there exist prior work that has a lot of overlap with the claimed contributions of the submission. The CAR framework of [1] and the prover-verifier framework of [2], along with the mutual information bounds of [1] capture an important portion of the claimed novelty. These works are not cited by the submission. The bounds that use the concept of AFC do appear to be novel (and interesting). \n\n**Reproducibility:** Considering the code release and the details presented in the appendix, the results in the submission are reproducible. \n\n\n**Typos:** The paper is very well written and has very few typos -- just listing a few that I've come across in case it helps the authors. \n* Section 2.3, First paragraph: The Fyre citation should be in citep. ",
            "summary_of_the_review": "This paper presents a promising framework to address an important and timely problem. It is written well and some of the theoretical results are novel and interesting. \n\nThe main issues with the submission are lack of novelty (both regarding the framework and the theoretical results) and a sufficient discussion of existing literature, which form the basis of my recommendation. A (potentially significant) rewrite that places the proposed framework/results within the existing literature and puts the emphasis on the theoretical results (especially revolving around AFC) would, in my opinion, make this submission a worthwhile contribution to ICLR. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2927/Reviewer_Pt1p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2927/Reviewer_Pt1p"
        ]
    },
    {
        "id": "fTAURN_vwt",
        "original": null,
        "number": 4,
        "cdate": 1667434597195,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667434597195,
        "tmdate": 1667434597195,
        "tddate": null,
        "forum": "7hvbaJ1AbaM",
        "replyto": "7hvbaJ1AbaM",
        "invitation": "ICLR.cc/2023/Conference/Paper2927/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a multi-agent interactive game that provides formal interpretability for classification problem. In particular, the interactive game is inspired by interactive proof systems. In addition, the authors connected information measures such as conditional entropy and average precision to completeness and soundness metrics. On top of that, the authors introduced numerical implementation that works in a few settings.\n",
            "strength_and_weaknesses": "Strengths:\n- The paper studies an important problem with potential high impact on AI safety.\n- The connections between informations metrics to completeness and soundness are quite interesting\n\nWeaknesses:\n- The authors missed one existing work of similar idea. The authors claim many times in the paper about \"for the first time\" in some of their contributions. However, the idea of prover-verifier game is not new and has been carefully discussed in [1]. In particular, [1] proposed a prover-verifier game to encourage learning agents to solve decision problems in a verifiable manner, which overlaps significantly with this paper. \n- The paper is hard to follow, especially for the technical parts. The authors should consider improving the writing. Particularly, I find the notations in section 2 a bit awkward. For example, the notation of $x \\in \\phi$ was used to denote a data point $x$ contains a feature $\\phi$, which is quite weird given we typically use to specify an element $x$ is in the set $\\phi$. In addition, the authors use this notation in conditioning, which makes it even more confusing. Another example is the use of $D$ and $\\mathcal{D}$ in Theorem 2.8.\n\n\nOther comments and questions:\n- What is the main usage of of average precision? The authors derived the bound for average precision, but why such a quantity is useful in interpretability?\n- Should the loss of Morgana to be $$-\\log \\left(A(s \\cdot x)_0 - A(s \\cdot x)_{-c(x)} \\right)$$?\n- In Figure 5 (1), why only report the predictions of Merlin but not Morgana's?\n- For Figure 6, it seems quite clear that the precision would go to 1 if one increase $k$. Does that suggest that it is trivial to get a tight bound in that regime?\n- The game involves both cooperative and adversarial components. Often the case, such a game is difficult to solve (in terms of finding equilibrium)? Is this the case in your experiments?\n\n\n\nReference:\n[1] Learning to Give Checkable Answers with Prover-Verifier Games, 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: can be improved significantly, see my comments above.\nQuality and novelty: a similar idea was introduced in [1], and the authors need to address it properly.\nReproducibility: Though implementation details are largely missing in the main paper, the authors reported details in the appendix and also provided code.",
            "summary_of_the_review": "The paper is interesting, but a similar idea has been discussed which I hope the authors could address in the next revision. With proper discussions and comparisons with the existing work and improved writing, I may consider increasing my score. Overall, the paper in its current form is not enough to warrant acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2927/Reviewer_YnKy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2927/Reviewer_YnKy"
        ]
    }
]