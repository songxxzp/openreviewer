[
    {
        "id": "_sq-_inRM5",
        "original": null,
        "number": 1,
        "cdate": 1666394002437,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666394002437,
        "tmdate": 1669198196801,
        "tddate": null,
        "forum": "tkvyCt1PzpvP",
        "replyto": "tkvyCt1PzpvP",
        "invitation": "ICLR.cc/2023/Conference/Paper2859/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper investigates 'class interference', i.e. how does update on one class affect the others. The work looks into the flatness of the minima for the converged models of two architectures (VGG and ResNet) trained with three different hyper parameter settings (smaller lr, larger lr and annealed lr). Finally, the authors look at how class interference evolves throughout training.",
            "strength_and_weaknesses": "### Strengths\n\n* The paper shows some interesting phenomena, e.g. the fact that lr annealing leads to flat minima for both of the architectures. \n* I liked the introduction doing the overview of the related work and providing the context for the study.\n\n### Weaknesses\n\nClass interference is an intuitive phenomenon. However, whether this intuition aligns with optimisation challenges is unclear. It is also unclear what to do with this: yes, updating on one class only can make the other class deteriorate. What do we do with this? Should we even do something with this? If yes, why?\n\nApart from this, I don't think the paper demonstrates what it claimed to have demonstrated. For instance, looking at the conclusion:\n\n* We show it [the class interference] is the bottleneck of classification <- I don't think the paper actually showed that\n* ...we hope this paper is useful to understand the generalisation of deep nets <- it is unclear what exactly we understood from the fact that the classes interfere and that an updating one class leads to deterioration on the other.\n* ...useful to ... improve existing models and training methods <- this would be an extremely useful application, but it is unclear how to use the proposed metrics to do so. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\n* The paper is sometimes confusing. The authors introduce a lot of metaphors that might be original, but confuse the reader, i.e. label dancing, dancing notes etc. I do not think these metaphors improve the paper, but rather the opposite.\n* For the notation, do we need both $i$ and $j$ together with $c_1$ and $c_2$? Cam we just use one of them, e.g. in proposition 1?\n\n### Quality\n\nSome of the paper's claims are not justified.\n\n* 'This difficulty is not specific to models. It represents class similarity and learning difficulty in data' I do not think this is shown. This also contradicts to the title of the paper 'Class interference of **neural networks**'.\n* 'This [empirical results] confirms that big learning rates generalise better than small ones.' I think 'confirm' is a bit too strong. We can say that this supports/goes in line with, but no empirical result can confirm a hypothesis.\n\nPage 5 compares the loss landscapes with the Rosenbrock function. I am not sure if we can compare the loss landscape of this paper with those since the optimisation takes the average gradient across all classes or across i.i.d. samples from the dataset, we do not update per-class. \n\nI think that the Proposition 1 proof has a mistake, and that the Proposition itself is not true for all ego models of two classes: the proof proves the other direction of the implication. \"**Proposition 1**: Any interference model is a convex combination of the ego models of the two classes.\" If this holds for any interference model, I pick $\\theta_1=\\lambda\\alpha_i$, and I pick $\\theta_2=\\lambda\\alpha_j$, which will not give me a convex combination unless $\\lambda=1/2$. The proof seems to show that a convex combination of two ego-models is, in fact, a class interference model. Apart from that, it is not clear to me why we need Proposition 1. Why is knowing this important, what can we do with this knowledge?\n\n### Novelty\n\n* The CCTM metric looks very similar to the confusion matrix used throughout the classification literature (in fact, the CCTM matrix is a normalised confusion matrix).\n* Task interference is a popular research direction in multi-task learning, with the goals of many multitask optimisers is somehow to alleviate the interference. However, recently, there has been a line of work showing that these multitask optimisers have a regularisation effect, and are not more effective than a simple regularised baseline (summing the gradients):\n- Do Current Multi-Task Optimization Methods in Deep Learning Even Help?, NeurIPS 2022\n- In Defense of the Unitary Scalarization for Deep Multi-Task Learning, NeurIPS 2022\n\nThe two works above have references to the most popular multitask optimisers as well.\n\n### Reproducibility\n\nThe paper is quite scarce on the setting. It would be useful for the reader to have a more complete discussion of the exact training/testing hyper parameters in the appendix.\n\nApart from that, all the claims in the paper are based on the results of a single seed, and I believe that the plots might look differently if we run SGD with a different seed.\n\n\n### Nits\n\n* The ref to Rosenbrock might have a bibtex error. It says 'HoHo Rosenbrock' instead of 'H.H.Rosenbrock'.\n* the Table on page 4 does not have a caption/title.",
            "summary_of_the_review": "Given my comments above, I believe that the paper is not ready for a publication at ICLR. Apart from my criticisms above, I will put suggestions on how to improve the paper below:\n\n* I think it would be extremely helpful to make a synthetic task with two overlapping gaussians to illustrate the authors' point. The gaussians are super easy to plot and very easy to vary the mean/std to get the desired effect. \n* I know this is hard to do, but having a set of actionable insights will be extremely helpful for the researchers or practitioners. For instance, what should we do about the observed phenomenon? \n* How, do you think, the irreducible error is related to class interference?",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2859/Reviewer_aWAb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2859/Reviewer_aWAb"
        ]
    },
    {
        "id": "iwJv5OAGoNX",
        "original": null,
        "number": 2,
        "cdate": 1666399631267,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666399631267,
        "tmdate": 1666399631267,
        "tddate": null,
        "forum": "tkvyCt1PzpvP",
        "replyto": "tkvyCt1PzpvP",
        "invitation": "ICLR.cc/2023/Conference/Paper2859/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies deep neural network through the lens of \"class interference\". Class interference corresponds to difficulty for a pair of classes, where one class and another class are hard to distinguish for the neural network. More specifically the cross-class test of generalization matrix (CCTM) is used to measure the class interference between classes. Class interference becomes severe when classes are conceptually similar, as given by the example of Cats vs. Dogs. The paper further defines the ego model of each class, which is an gradient-updated model based on the average gradient of samples for only that class. Based on this idea, the paper defines an interference space, which is used to study trained deep neural networks and during training of deep neural networks.",
            "strength_and_weaknesses": "Strengths:\n- Studying the minima flatness/sharpness and learning difficulty in data are important topics, and class interference provides us with insights about the datasets. We can have a better understanding of CIFAR-10 using the tools provided in the paper.\n\nWeaknesses:\n- Section 2 explains how the difficulty of data based on CCTM is not specific to models, but it seems to be model dependent, since the results depend on the model used (although we can see similar trends between the different models).\n- One of the main contribution is the class interference measure based on the proposed CCTM, but CCTM may not be novel, since it seems to be equivalent to the confusion matrix (or the normalized version of it). Confusion matrix is already used heavily to study the class-wise performance of deep neural network classifiers, especially in the industry.\n- The discussions about \"dance\" seems interesting, but I'm not sure if I understood the phenomenon correctly. It would be helpful if there are more discussions on why if the training recall of one class rises, the other class's training recall will decrease. Furthermore, it is hard to visually confirm if this is happening, so would be nice to see the correlation value between these two time-series data.\n- Currently the paper only studies the CIFAR-10 dataset, but it would be interesting to see if the same results, e.g., symmetry pattern, arise for other datasets as well.\n- From the perspective of learning difficulty in data, there are many papers recently working on this, such as: \"Deep Learning Through the Lens of Example Difficulty\" (NeurIPS 2021), \"Estimating Example Difficulty Using Variance of Gradients\" (CVPR 2022), \"Understanding Dataset Difficulty with V-Usable Information\" (ICML 2022). There are also papers such as \"Evaluating State-of-the-Art Classification Models Against Bayes Optimality\" (NeurIPS 2021) that study class-wise difficulty (Appendix B.1 shows how CAT and DOG are the most difficult classes for CIFAR-10, which is consistent with the experimental results in the paper under review). It would be interesting to see discussions about related work.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper defines class interference related concepts in Section 2, and then discuss two ways to use class interference in Section 3 and 4. The paper is well organized.\n\nQuality:\nThe motivation/application for studying class interference was not discussed in depth, for example, I wasn't sure how looking at the label dance during training can be helpful for training better neural networks. Only related work about flat minima were discussed, and other related work such as learning difficulty of data was not discussed. It would make the paper better if the relationship between related work is discussed further. I would also like to suggest exploring other datasets to see if the findings are general, since the paper currently only studies CIFAR-10. It would be interesting if the symmetry pattern only holds for certain datasets (such as image datasets).\n\nNovelty:\nThe CCTM is identical to the confusion matrix (except the normalization part), which is already used heavily to study the performance of machine learning classifiers. The idea of using the interference space/label dance seems to be novel.\n\nReproducibility:\nFor reproducibility, the code was not included in the submission. The (optional) reproducibility statement was not included in the paper.",
            "summary_of_the_review": "Although the paper works on an interesting problem with new and existing tools, overall, due to the reasons I wrote in the previous sections, I am feeling that in its current form, it is below the acceptance threshold. I hope the authors can incorporate the comments from the reviewers to make the paper better.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not have any ethical concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2859/Reviewer_3tyq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2859/Reviewer_3tyq"
        ]
    },
    {
        "id": "lIP0euZlRH",
        "original": null,
        "number": 3,
        "cdate": 1666794275351,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794275351,
        "tmdate": 1667641742748,
        "tddate": null,
        "forum": "tkvyCt1PzpvP",
        "replyto": "tkvyCt1PzpvP",
        "invitation": "ICLR.cc/2023/Conference/Paper2859/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose the notion of class interference represents the learning difficulty in data. They propose using a CCTM metric for understanding class interference whereas a larger interference metric indicates a sharper minimum and worse generalization. They also give an explanation for why annealed learning rate results in better generalization. The authors perform analysis on multiple architectures on CIFAR 10 to validate their claim",
            "strength_and_weaknesses": "Strength: \n1. The visualization validates the claims.\n\nWeakness: \n1. In section 1, I'm confused about how the interpolation monotonicity is related to class interference. The motivation is not quite clear. Besides, it seems to be related to the form in Proposition 1, yet this proposition is not used or explained in this paper.\n2. CCTM seems to be an extension for the confusion matrix when the number of classes is over 2.  Therefore, I'm concerned about the novelty of this metric. \n3. CCTM can't explain the difference between the failed generalization of an overfit model and an underfit model, which by intuition has very different loss landscapes. For example, in an underfit model, the landscape can be smooth yet CCTM would still be quite high.\n4. The notion of smoothness is only observed from visualization and lacks any metrics. Computing the eigenvalue of Hessian would be helpful for estimating the correlation between CCTM and smoothness.\n5. As shown in Table1, the recall accuracy is similar for VGG, ResNet, DLA and GoogleNet and may be the reason why they observe the same pattern in CCM. I believe it would be more convincing if the author can try a wider range of models with different complexity, including ViT.\n6. Only CIFAR-10 is used in the analysis and may be an exception. Hope more multi-label classification datasets could be analyzed, e.g. MNIST, and ImageNet.\n\nQuestions: \n1. Could you please provide some intuition why higher CCTM results in sharper minimum, especially when the loss space is not in the two classes involved in CCTM calculation ( as shown in Figure 4) ? \n2. Is class interference possibly an inherent problem of a dataset? As stated in the paper, cats are often confused with dogs because of low resolution. It seems that training techniques like anneal-lr can alleviate this problem, but is there a lower-bound on class interference? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & Quality: The paper lacks clarity in terms of plotting and writing. \n - The motivation for class interference is not explained clearly.\n - The theoretical part seems superfluous. \n - The plot in Figures 6 & 7 has too many line plots, making it hard to distinguish.\n - The experiment part goes without a statement of settings.\n\nNovelty: I'm not familiar with optimization literature and this paper seems novel to me. \n\nReproducibility: The code was not included in the submission. Also, the hyperparameters of models are not available. ",
            "summary_of_the_review": "Overall, I believe the paper is not complete in terms of writing and experiments. Although the observations are interesting, they are not supported by enough empirical and theoretical analysis. I've changed the score to strong reject as it seems that the authors are not open to improvements and I believe this paper is quite below ICLR standard. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2859/Reviewer_6BAT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2859/Reviewer_6BAT"
        ]
    }
]