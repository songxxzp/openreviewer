[
    {
        "id": "cN8SNreVz9",
        "original": null,
        "number": 1,
        "cdate": 1666856299051,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666856299051,
        "tmdate": 1666856299051,
        "tddate": null,
        "forum": "o2Udz4LZsL5",
        "replyto": "o2Udz4LZsL5",
        "invitation": "ICLR.cc/2023/Conference/Paper1103/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to store the random noises along the generative trajectory of diffusion probabilistic model as a latent code, and use this latent code for image-to-image translation.",
            "strength_and_weaknesses": "Strength:\n(1) The paper provides a clear summary for previous works.\n(2) The paper is easy to digest.\n\n\nWeaknesses:\n(1) Limited Novelty: DPM-Encoder, which stores all the noises along the generative process, is an obvious way to encode an image to the latent space of a stochastic DPM. Since it is obvious, it appears in the famous work [1], Appendix B.2, to interpolate between images. Therefore, the encoding strategy is not new to me.\n(2) Limited Novelty: CycleDiffusion is a simple generalization of SDEdit [2] and the observation in [3]. The authors made no additional theoretical justification on the method, making its improvement over previous works limited.\n(3) Unsatisfying results: In Figure 3, many of the parts in the images, which we do not expect to change, changed. For example,  the astronaut figure on the bottom-left changes not only in the style, but also the content. This, in my opinion, is not fixable with the current method. \n\n[1] Improved Techniques for Training Score-Based Generative Models, https://arxiv.org/pdf/2006.09011.pdf\n[2] SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations https://arxiv.org/abs/2108.01073\n[3] Understanding DDPM Latent Codes Through Optimal Transport https://arxiv.org/abs/2202.07477\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above section.",
            "summary_of_the_review": "The paper leverages an existing strategy to encode images generated by DPM, then directly apply this latent code to another DPM to achieve image-to-image translation. The novelty is limited and the results are not satisfying.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_fAbM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_fAbM"
        ]
    },
    {
        "id": "iwiG6eV_7Qo",
        "original": null,
        "number": 2,
        "cdate": 1667016149638,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667016149638,
        "tmdate": 1667016149638,
        "tddate": null,
        "forum": "o2Udz4LZsL5",
        "replyto": "o2Udz4LZsL5",
        "invitation": "ICLR.cc/2023/Conference/Paper1103/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the latent space of diffusion models compared to a variety of other generative models such as VAEs, Normalizing Flows, and especially GANs. First of all, this paper provides a very comprehensive and nice overview of latent space across deep generative models and motivates the study of latent space in diffusion models. Then, it traces down to the question about the latent space of stochastic DPMs rather than other methods which leverage \"fixed\" processes. The authors leverage the finding such that the randomness in the model captures the image-specific information and further utilize the defined latent space for an unpaired image translation task. It explores more tasks such as zero-shot image-to-image editor and latent space guidance with energy-based models.",
            "strength_and_weaknesses": "Strength:\n* This paper is mostly well-written with a clear literature review, leading to its motivation. \n* The studied problem is important, it is significant to understand the latent space of diffusion models as it has been done in GANs and other generative models. It can lead to more understanding and empirical applications of diffusion models.\n* The experimental results empirically support the claims and findings of the paper. \n\nWeaknesses\n* Even though the motivation to study the latent space of diffusion models and how this paper studies it with a simple yet effective approach is well-written, the applications to work as a CycleDiffusion, zero-shot image-to-image editing and latent space guidance sounds a little indirect to me, though this may be due to my biased perspective. I directly thought about GAN inversion and related literature, e.g. [1-3], after I read the motivation of this paper. I don't mean the applications in this paper are not okay, but I would suggest making the motivation for the applications tighter.\n* As a long-standing problem in understanding the latent space of deep generative models, even though this paper achieves great experimental results, I would still suggest the authors study theoretical guarantees of it.\n* The proposed D-CLIP score is a bit confusing, is the relative change meaningful without absolute value?\n* Unified Plug-and-Play Guidance is a bit misleading, why would the proposed or defined latent space unify diffusion models and GANs?\n\n[1] H\u00e4rk\u00f6nen, E., Hertzmann, A., Lehtinen, J. and Paris, S., 2020. Ganspace: Discovering interpretable gan controls. Advances in Neural Information Processing Systems, 33, pp.9841-9850.\n[2] Shen, Y., Yang, C., Tang, X. and Zhou, B., 2020. Interfacegan: Interpreting the disentangled face representation learned by gans. IEEE transactions on pattern analysis and machine intelligence.\n[3] Shen, Y. and Zhou, B., 2021. Closed-form factorization of latent semantics in gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1532-1540).",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this is a good paper studying an important problem. It demonstrates technically sound formulation and reasonable results. All my concerns are listed above.\n\nThe code is not uploaded, but the approach looks simple, the hyperparameters are given in the appendix, but I hope the authors keep the words to make the code public upon acceptance.",
            "summary_of_the_review": "Overall, my main concern about this paper is the flow of the paper and solid experiments to support the method empirically as theoretical results are missing or challenging. I value the novelty of this paper to formulate this important yet not fully explored problem (latent space of diffusion models). I am leaning towards accepting this paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_2RYz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_2RYz"
        ]
    },
    {
        "id": "4UIunqmxcN",
        "original": null,
        "number": 3,
        "cdate": 1667098546383,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667098546383,
        "tmdate": 1667098546383,
        "tddate": null,
        "forum": "o2Udz4LZsL5",
        "replyto": "o2Udz4LZsL5",
        "invitation": "ICLR.cc/2023/Conference/Paper1103/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper considers a concatenation of all intermediate variables through a forward diffusion as a latent code.\n\nThe authors consider the forward process of diffusion models as an encoder.\n\nUsing two independently trained diffusion models, the proposed method encodes an image with a fixed random seed for one model and run reverse process on the other model to achieve image-to-image translation.",
            "strength_and_weaknesses": "\nStrengths\n\n(+) This paper shows interesting application of diffusion models.\n\n(+) The proposed latent code (concatenation of all variables) is versatile across different models.\n\n\nWeaknesses\n\nPlease refer to the below section.",
            "clarity,_quality,_novelty_and_reproducibility": "\n### Clarity\n\n(-) Abbreviation without definition: DPM\n\n> 3.2. DPM-Encoder: an Invertible Encoder ...\n\n> Our contribution is DPM-Encoder, and invertible encoder ... \n\n(-) Considering an existence of a generator as invertibility of an encoder does not make sense. Generally encoder is considered as inverting a generative process.\n\n(-) Broken sentence: several researchers and practitioners have found that when trained with the same \"random seed\" [missing comma] [missing subject] leads to similar images. \n\n(-) Unclear description: Does above sentence mean that the same $X_T$ and noises leads to similar images across different diffusion models?\n\n(-) Missing definition of \"zero-shot\". In what aspect is the CycleDiffusion zero-shot?\n\n### Quality (Technical soundness)\n\n(-) Why is generally missing.\n* Why is the latent code defined as a concatenation of $X_T$ and intermediate noises?\n* Why do the models trained with the same random seed lead to similar images?\n* Why does PSNR between a translated image and source image make sense?\n* Why does FID between generated and target images make sense? What are generated and target images? In the last sentence of the first paragraph of 4.1 says the target image is the generated image.\n\n\n(-) Guidance in 3.4 is same with usual guidance in diffusion models.\n\n### Novelty\n\n(-) Latent space = a concatenation of $X_T$ and intermediate noises is trivial.\n\n(-) Encoder = forward process is trivial.\n\n(-) Fixing everything and changing text is trivial.\n",
            "summary_of_the_review": "This paper shows interesting phenomena but does not contribute to building general knowledge with logical development.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_NUmH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_NUmH"
        ]
    },
    {
        "id": "as__pqCeBU",
        "original": null,
        "number": 4,
        "cdate": 1667322111808,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667322111808,
        "tmdate": 1667410465810,
        "tddate": null,
        "forum": "o2Udz4LZsL5",
        "replyto": "o2Udz4LZsL5",
        "invitation": "ICLR.cc/2023/Conference/Paper1103/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides a unified setting among generative models. In particular it offers a perspective  on diffusion models (DM) that cast them in a framework more similar to GANs, which admit a Gaussian formulation of the latent space. This point of view is then leveraged in three settings. \nContributions: 1. provide a formulation of DM with Gaussian latent code  (introducing the concept of DPM-encoder that maps images into a latent space). 2. Propose CycleDiffusion for a) image-to-image translation and b) to show that text-to-image diffusion models can be used as zero-shot image-to-image editors. 3. Rely on the proposed perspective which unifies Diffusion Models and GANs to perform plug-and-play guidance.   ",
            "strength_and_weaknesses": "Section 3.2: do I understand correctly that your DPM-Encoder is a proposal for how to build a z such that x = G(z) for z ~ DPMEnc(z | x, G), which amount to just concatenating x_T with \\eps_t as described?  \n\nCan you clarify the following: in section 3.1 I understood that in your unified framework, z is drawn from a Gaussian. In equation (6), it doesn't seem to be the case. In section 3.2, is the Gaussian latent  abandoned? \n \nI am a bit confused about the proof in Proposition 1. Do you use induction ((show it\u2019s true for 0, assume it\u2019s true for t, show it\u2019s true for t+1)\n) or backward induction (show it\u2019s true for T, assume it\u2019s true for t, show it\u2019s true for t-1)  ?\nAlso,  if the goal here to show that G(z) = x for any z sampled from DPMEnc ( | x), in the statement I would write x = G(z) =: \\bar{x}, just for clarity. \n\nSection 3.3: \nCan you clarify what you mean with \u201cour defined latent codes contain all randomness?\u201d\n\u201c we can formalize the above finding as a common latent space emerges...\" :can you clarify this sentence (what does it mean pragmatically?) ?  \nHow is the concept of  \u2018common\u2019 defined and measured? \n\nWhat do you mean \u201cD_1 and D_2 do not need to be two datasets?\u201d \n\nSection  4.1 \nPeak Signal-to-Noise Ratio: what is this an indicator of? Meaning CycleDiffusion has excellent  SSIM (which  is more correlated to human perception) than baselines, but lower PSNR (actually the lowest). What is this an indicator of? \n\n Section 4.3\nAm I correct to understand that your contribution in section 4.3 is the fact that you can \u201cwork on a latent space\u201d for diffusion models (e.g. apply Langevin) similarly to what one can do with GANs? \nThe findings reported are interesting but reporting them without a critical explanation is maybe limiting. E.g \u201c3D GANs are worse at guidance than 2D GANs and diffusion models\u201d: why so? Or at least, what\u2019s your opinion on this?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In my opinion, the clarity of the paper could be improved. I find that many sentences are vague e.g. \" a common latent space 'emerges' from...\". It could be more streamlined and direct what the point is. \n\nIn terms of novelty - the main contribution seems to be a way of defining a Gaussian latent space + DPM-Encoder for diffusion model and to exploit this perspective for unpaired image translation, zero shot image-to-image editing and conditioning the generation.\n\nIn terms of reproducibility, code is not provided in the submission but details of experimental setup are somewhat comprehensive.\n",
            "summary_of_the_review": "I think the paper could improve in its clarity. The novelty seems pretty limited, although CycleDiffusion used as zero-shot image-to-image editor is interesting in my opinion.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_JmnA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_JmnA"
        ]
    },
    {
        "id": "1KnUw2Wm53S",
        "original": null,
        "number": 5,
        "cdate": 1667330832897,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667330832897,
        "tmdate": 1668753903641,
        "tddate": null,
        "forum": "o2Udz4LZsL5",
        "replyto": "o2Udz4LZsL5",
        "invitation": "ICLR.cc/2023/Conference/Paper1103/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a way of deriving an encoder from a pretrained diffusion model. In particular, this encoder is stochastic and invertible, which makes it different from existing attempts. The authors show that this encoder can be deployed in an image to image translation setting, and demonstrates interesting results. Moreover, a unified guidance method is proposed based on the derived latent representations. ",
            "strength_and_weaknesses": "1. The idea of defining the stochastic encoder is simple but interesting. \n2. Being able to encode an image and using the code for image to image translation or editing is clever.\n3. The empirical results on both image to image translation and text guided edits are impressive. ",
            "clarity,_quality,_novelty_and_reproducibility": "1. While the proposed stochastic encoder is interesting, and I wonder how it compares with the ODE based deterministic encoder empirically. Will the sotchasticity give any benefits on either the results or computational cost over the deterministic version? \n\n2. I have found that the plug-and-play guidance part rather disjoint from the main contribution and not properly justified. My understanding of it is that the authors propose to formulate a text conditioned energy function using a CLIP model. One can then guide a given generative model by optimizing its latents, which applies to both GANs and diffusion models. My question is that how is this related to the stochastic encoder? Why can't you just optimize the initial noise $x_T$ following a DDIM inference procedure? Maybe I'm missing something but I think it'd be good to clarify on this. \n",
            "summary_of_the_review": "Overall I think this is a promising paper with a simple idea and interesting results. I'd like to hear clarifications on my questions during the rebuttal phase. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_Nh8B"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_Nh8B"
        ]
    },
    {
        "id": "2ViRC57QuF",
        "original": null,
        "number": 6,
        "cdate": 1667506006394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667506006394,
        "tmdate": 1667506006394,
        "tddate": null,
        "forum": "o2Udz4LZsL5",
        "replyto": "o2Udz4LZsL5",
        "invitation": "ICLR.cc/2023/Conference/Paper1103/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper under consideration proposes a method for inverting stochastic diffusion models. Based on this method the authors come up with an (conditional) unpaired image-to-image translation approach called \u201cCycleDiffusion\u201d. The validity of the proposed methodology is verified by several applications.",
            "strength_and_weaknesses": "Strength:\n1) The proposed idea is clear and practically applicable. One can easily apply the proposed framework for a general stochastic diffusion model.\n\n2) The experiments look quite convincing.\n\nWeaknesses:\n1) The idea of DPM-Encoder itself seems to be quite easy and straightforward. All of the papers dealing with stochastic diffusion models (like DDPM) actually mentioned the forward form of the stochastic dynamic (i.e. how to obtain latent code $x_T$ from the original image $x_0$) but didn\u2019t stress the importance of the dynamic. Therefore, the only theoretical contribution of the paper under consideration is that the authors highlight the importance of forward stochastic dynamics (for the problem of unpaired image-to-image generation related applications)\n\n2) As it even mentioned in the article itself, the methodology of stochastic diffusion based unpaired image-to-image generation is under question and requires theoretical verification.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is over-all well-written. The results are novel and original.\n\nThe authors propose some new ideas (usefulness of the forward dynamic in context of stochastic models) and explicitly state the hypothesis that the diffusion models with similar noise schedule share latent spaces. \n\n The practical quality of the paper is good since the authors provide a broad comparison of their method with existing approaches. One question: How do ILVR, SDEdit, EGSDE models apply to the problem of unpaired image-to-image translation (Cat vs Dog and Wild vs Dog)? As I understand, the mentioned methods are not directly aimed at the unpaired im-to-im translation by design.\n\n The code is not provided in the supplementary materials.\n",
            "summary_of_the_review": "The authors provide a new method for solving (conditional) unpaired im-to-im problems based on stochastic diffusion models. In spite of the interesting methodological proposition and good practical results, the method requires further theoretical investigation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_dmqB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1103/Reviewer_dmqB"
        ]
    }
]