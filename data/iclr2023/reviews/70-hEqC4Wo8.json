[
    {
        "id": "hyXr9q86gC",
        "original": null,
        "number": 1,
        "cdate": 1666350739744,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666350739744,
        "tmdate": 1666350739744,
        "tddate": null,
        "forum": "70-hEqC4Wo8",
        "replyto": "70-hEqC4Wo8",
        "invitation": "ICLR.cc/2023/Conference/Paper6364/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work propose a new SNN model named the d-block model, with stochastic absolute refractory periods and recurrent conductance latencies, which reduces the number of sequential computations using fast vectorised operations. Input spikes are processed by d equal length blocks, where every block is a single-spike SNN. This model obtains accelerated training speeds and state-of-the-art performance on the N-MNIST, SHD, and SSC datasets without the need for any regularization and using fewer spikes compared to standard SNNs. Besides, this model theoretically consumes less energy than conventional SNNs when employed on neuromorphic hardware.",
            "strength_and_weaknesses": "The authors didn\u2019t give clear illustration and provide many detailed information to make readers to understand d-block model. The author makes references about single-spike SNN proposed by Taylor et al. (2022). But, they didn\u2019t compare this work with single-spike SNN.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nIn section 3.3, the author's interpretation of recurrent connections is unclear.\nNovelty\uff1a\nThe author makes references about single-spike SNN proposed by Taylor et al. (2022). So the novelty in term of b-block itself is limited.\n",
            "summary_of_the_review": "I think this work is less innovative and the illustration of model isn\u2019t clear. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_jrgn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_jrgn"
        ]
    },
    {
        "id": "UZxSwNhGFA",
        "original": null,
        "number": 2,
        "cdate": 1666605212130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605212130,
        "tmdate": 1666605212130,
        "tddate": null,
        "forum": "70-hEqC4Wo8",
        "replyto": "70-hEqC4Wo8",
        "invitation": "ICLR.cc/2023/Conference/Paper6364/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method to control the number of output spikes (d-block model) and accelerate SNN calculations by avoiding the recursive calculation of membrane potential. The acceleration of SNN calculations was verified on various event datasets like N-MNIST, SHD, and SSC. As well, the model achieves high classification accuracy on the event datasets.",
            "strength_and_weaknesses": "Strength:\n\nThe idea to avoid the recursive update of membrane potential is of technical importance given the time and computational complexities of the recursive model which scale with the number of timesteps used. The authors successfully identified the feasibility of the proposed method on various event datasets.\n\nWeaknesses:\n\n1. The poor paper organization significantly undermines the readability. Also, the full explanation of the d-block model illustrated in 2b is not given. I do not still understand what $m$ and $\\tilde{V}$ and their superscripts stand for. \n\n2. Regarding the previous paper (Taylor et al. (2022)), I can find only very marginal difference of the present work from the previous work(Taylor et al. (2022)). The only difference is such that d (rather than one) spikes are used. \n\n3. The previous works compared with the present work in Table 1 are out of dated. I recommend the authors to address SOTA results.\n\n4. On Page 6, the authors address the github page for the code, which is against the double blind policy.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: model presentation is not clear, and paper organization is poor.\n\nReproducibility: N/A\n\nSignificance/Quality/Novelty: Compared to Taylor et al.(2022), the novelty of the present work is very marginal given its marginal difference from the previous work. Thus, significance and quality of this work are quite poor.",
            "summary_of_the_review": "Overall, the significance of this work is limited given the marginal progress from the previous paper Taylor et al., (2022). ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_Yn85"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_Yn85"
        ]
    },
    {
        "id": "oqokzezaJg6",
        "original": null,
        "number": 3,
        "cdate": 1666612253619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612253619,
        "tmdate": 1666612253619,
        "tddate": null,
        "forum": "70-hEqC4Wo8",
        "replyto": "70-hEqC4Wo8",
        "invitation": "ICLR.cc/2023/Conference/Paper6364/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new SNN model, named d-block model. By adding stochastic absolute refractory periods and recurrent conductance latencies, a d-block SNN can reduce the number of sequential computations using fast vectorized operations. ",
            "strength_and_weaknesses": "Strength: \nThe proposed model achieves fewer sequential operations and lower energy consumption compared to conventional SNNs. They obtained SOTA accuracy scores on SHD (86.2%) and SSC (68.16%) are impressive.\n\nWeakness:\n1. The speedup experiments were conducted on small models. It is concerned that whether this method could work on larger and deeper SNNs, like ResNet.\n2. There is limited novelty considering the reference (Taylor et al, 2022.) for accelerating training single-spike SNNs. It seems this article just makes incremental contributions.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized and clear. Related works were covered well. However, novelty is not strong enough. This paper is based on a modification of a previous work. The authors have made their source code public on GitHub, which is well-organized and reproducible.",
            "summary_of_the_review": "The paper proposes an efficient SNN training acceleration method. Novelty and experiments are limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_ddei"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_ddei"
        ]
    },
    {
        "id": "cm8j9T9SIi8",
        "original": null,
        "number": 4,
        "cdate": 1666840275622,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666840275622,
        "tmdate": 1666840275622,
        "tddate": null,
        "forum": "70-hEqC4Wo8",
        "replyto": "70-hEqC4Wo8",
        "invitation": "ICLR.cc/2023/Conference/Paper6364/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new architecture for spiking neural networks (SNNs), called d-block model. This architecture is built upon a previously proposed work (Taylor et al. 2022). The idea is to construct multiple blocks where within each there is dependency, allowing more parallelism during training. It has been shown that the new model outperform prior works both in terms of accuracy and also traiing time.\n",
            "strength_and_weaknesses": "Strengths:\n-- The main strengths of the paper is its significant training time reduction.\n-- The proposed model outperforms existing works in terms of performance.\n-- The paper is well-written and east to understand.\n\nWeaknesses:\n-- The new model was built on top of an existing method by separating its architecture into multiple blocks. There is no mathematical foundation for the new model explaining why it should achieve a better results while its speedup during training is evident.\n-- The architectures used for comparison are not the same which makes the comparison unfair and inconclusive. For example, it is expected to have better accuracy performance when a deeper model is used for SHD and SSC datasets in Table I. \n-- Table I should also include training time per epoch for a fair comparison when using the same architecture.\n-- I also expect to see some experimental results on more challenging datasets such as CIFAR10,100 and ImageNet.",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the novelty of this work is rather limited as it is based on a prior work. In terms of quality and clarity, the paper is decent. The code is also provided, which makes the paper reproducible while I haven't run the code myself.\n",
            "summary_of_the_review": "While the contribution of this paper is rather limited since it has been obtained by small modification to the prior work (Taylor et al. 2022), the training speedup is significant. The main issue that I have its comparison results in Table 1 where the networks architectures are not the same. I also expect to see the training time per epoch for a fair comparison.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_D9Qe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_D9Qe"
        ]
    },
    {
        "id": "SWaG1_9Dzq",
        "original": null,
        "number": 5,
        "cdate": 1667466779609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667466779609,
        "tmdate": 1667466826505,
        "tddate": null,
        "forum": "70-hEqC4Wo8",
        "replyto": "70-hEqC4Wo8",
        "invitation": "ICLR.cc/2023/Conference/Paper6364/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work extends the 1-block model in Taylor et al. (2022) to the d-block model. Compared with the LIF model, the d-block model achieves accelerated computing on GPU by using fewer sequential operations.",
            "strength_and_weaknesses": "Strength:\n\nThe proposed model indeed enables accelerated computing on GPU, and achieves sota results on some benchmarks.\n\nWeaknesses:\n\n1. This work is quite a naive extension of Taylor et al. (2022). If the authors of this work and Taylor et al. (2022) are the same, I highly recommend the authors to combine the two works into one.\n\n2. The authors need to describe the training method explicitly in the main content. The proposed model can be treated as a modified LIF model, which is irrelevant to the training methods. From appendix A.3.5, which is not detailed, I guess that the existing surrogate gradient method is adopted. The accelerated training on GPU is due to the model's parallel computing nature, not due to some novel training method. The authors should make it clear.\n\n3. Can the d-block model be implemented on neuromorphic hardware in an event-driven manner? First, the 1-block model is equivalent to the single-spike LIF model, so I do not worry about it. But for the d-block model, the spikes from the first 3 time steps are used in the 4th time step. Is it implementable? will it be implementable on future neuromorphic hardware?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The authors do not describe the used training methods. Audiences need to guess what they do.\n\nQuality: The model achieves good performance and training efficiency. But it will be useless if it cannot be implemented on neuromorphic chips.\n\nNovelty: This work is a naive extension of Taylor et al. (2022).\n\nReproducibility: Good.",
            "summary_of_the_review": "If the authors want me to raise the score, they should convince me about the novelty and the implementability of the model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_b7YL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6364/Reviewer_b7YL"
        ]
    }
]