[
    {
        "id": "UFAIE-asUDY",
        "original": null,
        "number": 1,
        "cdate": 1666649317452,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649317452,
        "tmdate": 1666649317452,
        "tddate": null,
        "forum": "fhcu4FBLciL",
        "replyto": "fhcu4FBLciL",
        "invitation": "ICLR.cc/2023/Conference/Paper2953/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper deals with unlearning methods for graphs. It introduces a new algorithm for approximate graph unlearning from different requests (feature, node, and edge unlearning) for Simple Graph Convolution and Generalized PageRank models, and provide theoretical guarantees of unlearning.",
            "strength_and_weaknesses": "This article is an extension of a paper by Guo et al. published in ICML 2020, which investigates approximate unlearning for unstructured data. The present paper extends the methodology and the results to graph data for 2 learning models: Simple Graph Convolution and Generalized PageRank, which is clearly of great interest and certainly non-trivial.\n\nOn the substance, I think that this paper suffers from 2 main limitations.\n- First, it does not apply to Graph Neural Networks and can therefore appear as quite specific. On this point, it would be interesting for the authors to explain precisely the conceptual limitations of their approach and why it does not apply to more complex models (GCNs for example). However, I understand that this paper is a first step towards approximate unlearning for graphs and can not handle all models at once.\n- Second, it does not investigate from a theoretical perspective the time-complexity of their removal procedure, while the main objective is to save computation time compared to retraining. It would be interesting to know the theoretical gain in each of the cases considered, also in the case of successive requests.\n\nOn the form, my comments are given in the following box.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I found this paper to be well-written despite some flaws that hindered the reading.\n\n- Privacy cost: At the end of the introduction (page 2), the reader (except if she/he is familiar with approximate unlearning) has no idea what privacy cost is and the given value makes no sense to her/him. The same occurs in section 2 about DP-GNNs (page 3). In the definition of certified removal, I guess $e$ denotes $\\exp(1)$. Together with the formulation \"given $\\epsilon>0$ ... where $\\epsilon,\\delta>0$\", it can be confusing: I thought that it was something like \"given $e>0$\" instead of \"given $\\epsilon>0$\". Beyond this typographical detail, at this point, the authors could elaborate on the formula for what is an acceptable cost and what is too expensive.\n\n- Privacy budget: I may be wrong, but the concept of privacy budget appears on page 7 without any justification, and involves the unknown parameter $\\alpha$ (at this point, we only know that it denotes a noise standard deviation without more details). The authors should really develop this part better. It also seems to be related to my question above about the complexity of unlearning vs retraining.\n\n- SGC, GPR, GCN...: these acronyms are not defined in the paper (SGC and GPR are defined in the abstract but should be recalled in the core of the paper, like GNN for example).\n\n- Even if we can guess it, the degree matrix should be defined.\n\n- Typos: \"As an demonstrative example\" (page 5), \"matches that ot Guo et al.\" (page 5); In Figures 2 and 3, [2] refers to nothing.\n\nIn general, the authors should not assume that the reader is very familiar with the paper of Guo et al., 2020.\n\nI also have the following 2 questions/comments:\n- Unlike retraining, the unlearning procedure requires to know the data to be removed, which imposes to recompute the model (Simple Graph Convolution or Generalized PageRank) before removing it from the database. I wonder if, in practical applications, this can not be a drawback of the approach. \n\n- What is the idea behind the definition of $\\Delta$ (page 5, above eq. (2))? I can see through the given example (node unlearning) that it leads to a similar formula as in Guo et al., 2020. However, in this reference, $\\Delta$ is directly defined as this, and does not come from a more general formula. Could the authors comment on this point?\n",
            "summary_of_the_review": "I enjoyed reading this paper despite a number of presentation flaws. The approach and results are clearly worthwhile, but the presentation of the paper really needs to be improved to allow for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2953/Reviewer_dA7m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2953/Reviewer_dA7m"
        ]
    },
    {
        "id": "9u7YpCbQS5",
        "original": null,
        "number": 2,
        "cdate": 1666665885896,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665885896,
        "tmdate": 1666666451784,
        "tddate": null,
        "forum": "fhcu4FBLciL",
        "replyto": "fhcu4FBLciL",
        "invitation": "ICLR.cc/2023/Conference/Paper2953/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a graph unlearning algorithm with provable theoretical guarantees. The graph unlearning is divided into three sub-unlearning tasks: node feature unlearning, edge unlearning, and node unlearning. A tailored analysis is provided for each subtask. The experiments with six benchmarks show the effectiveness of the proposed method as well as the inherent trade-off between privacy and accuracy.",
            "strength_and_weaknesses": "Strength\n- The paper provides a detailed theoretical analysis of the graph unlearning algorithm.\n- Graph unlearning is broken down into three subtasks: node feature/edge/node unlearning.\n\nWeaknesses\n- The analysis is performed on the simplified convolutional graphs and cannot be generalized to the standard GNN architectures. Although SCG shows competitive performance on some datasets, it is not the most preferable model selection in general. So, it would be good to have some additional experiments with more GNN architectures to show how the proposed approach can be generalized to the other models.\n- Following the previous weakness, the assumptions such as the unique optimum of the loss is quite strong are quite strong and cannot meet in general.\n- Although it is important to keep similar performance with retraining methods, it is also important to analyze what can be unlearned or not from the unlearning perspective. The current analysis only shows the performance of the membership inference attack and lacks a detailed analysis.\n- The proposed method only works when the full dataset is accessible. In many unlearning scenarios, it is not guaranteed to access the full dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n- The figures are difficult to read when printed out. Please use vector images with proper font size. \n- The notation e_i is adding extra hurdle to digest the expressions. Subscript (to index matrix entry or data point) would be sufficient in most cases.\n",
            "summary_of_the_review": "Although there are some limitations to the proposed approach, it is worth emphasizing that this work lays the foundation of the theoretical analysis of graph unlearning. Some parts of the presentation could be improved further to improved the readability of the manuscript.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2953/Reviewer_XzFP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2953/Reviewer_XzFP"
        ]
    },
    {
        "id": "2N9Rus8T74",
        "original": null,
        "number": 3,
        "cdate": 1666764461866,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666764461866,
        "tmdate": 1667439191891,
        "tddate": null,
        "forum": "fhcu4FBLciL",
        "replyto": "fhcu4FBLciL",
        "invitation": "ICLR.cc/2023/Conference/Paper2953/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates the problem of machine unlearning for graph-structured data. The authors provide an efficient algorithm to approximate graph unlearning in node classification tasks. The algorithm is proved to hold good theoretical properties for specific GNNs (SGC and GPRGNN). The authors also conduct an empirical study to demonstrate that the proposed algorithm outperforms the baselines.",
            "strength_and_weaknesses": "Strength:\n\n1.\tThe introduced proposed algorithm and analysis are novel and promising. The process of generalizing unstructured data unlearning analysis to the graph domain is nontrivial. Node embeddings are propagated on graphs during training, and therefore the removal of data is more complex. The theoretical analysis is deep and sound.\n\n2.\tThe proposed bounds of error are well supported by the experiments. The experimental results support the claim that the unlearning algorithm is faster than retraining. The improvement of accuracy against unstructured algorithm is significant, and the correlation between the accuracy and the degree of removed node makes the bounds more reasonable.\n\n3.\tThe paper is well-written. Limitation of the work, reasonableness of assumptions and main techniques used in the proof are all well illustrated.\n\nWeakness:\n\n1.\tMany of the main techniques in analysis and the algorithm are generalized from Guo\u2019s work, which makes the work less revolutionary.\n\n2.\tMost of the analysis is constrained to the binary classification task and linear GNN models, which is not practical. Current GNNs, such as the original GRPGNN, include non-linearity modules. As a result, the applicability of this method in real-world unlearning scenarios could be limited.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is presented clearly. The overall quality is relatively high. The idea of conducting unlearning algorithms in graph domain is novel. The reproducibility can\u2019t be evaluated due to resource limitation. ",
            "summary_of_the_review": "A solid and interesting work that generalizes unlearning algorithms from unstructured data to graph domain with theoretical guarantees. The analysis is comprehensive and well supported by experimental results. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2953/Reviewer_VMQZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2953/Reviewer_VMQZ"
        ]
    }
]