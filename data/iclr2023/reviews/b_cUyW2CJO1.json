[
    {
        "id": "yF8O8l7OuX",
        "original": null,
        "number": 1,
        "cdate": 1665918957938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665918957938,
        "tmdate": 1665918957938,
        "tddate": null,
        "forum": "b_cUyW2CJO1",
        "replyto": "b_cUyW2CJO1",
        "invitation": "ICLR.cc/2023/Conference/Paper1113/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies an intriguing research question: utilizing the expert demonstration during the policy update step in an Inverse Reinforcement Learning (IRL) algorithm. This technique has been applied by many previous works, but none of them has carefully analyzed the impact of including expert demonstration. In order to utilize the demonstration data, the authors mainly focus on two methods: 1) adding expert transitions into the replay buffer of the inner RL algorithm. 2) using expert actions in Q value bootstrapping in order to improve the target Q value estimates and more accurately describe high-value expert states. The paper introduces the motivation and intuitively analyses the impact of each of these methods. In the experiment, the paper use MaxEnt-IRL and f-IRL as baselines for demonstrating the increase in training speed after including expert demonstration.",
            "strength_and_weaknesses": "**Strength**\n\n1. This paper focuses on an intriguing research question. To the best of my knowledge, while many previous IRL works did apply the expert demonstrate in policy update, but they have not carefully stuidied this method but rather treat it as an engineering trick that could potentially increase the training speed. I agree with the authors that this method requires a more carefully analyze by demonstrating its impact empirically and theoretically. While this paper does touch some of these aspects, I believe a more carefully study is required (please see my comment below).\n2. The paper is well-structured and easy to follow with sufficient details for supporting understanding.\n\n**Weaknesses**\nI suggest the author to improve their paper by considering the following ideas:\n\n1. **The Size of Policy Update**. In order to prevent destructively large policy updates, many RL algorithms, including the most popular PPO and TRPO algorithms, often include a constraint on the size of the policy update. They carefully tuned the objective to exclude state-action pairs from the policy that deviates from the current policy, but in this work, both ERB and EQB directly incorporate expert demonstration into the policy update and **the expert policy is significantly different from the imitation policy, especially in the beginning of training, which contradicts to the motivation of PPO or TRPO and is likely to drive a sub-optimal policy**. It is why MaxEnt IRL outperforms the proposed model in Figure 1(b) and Figure 2. I strongly recommend referring to the idea of constraining updates.   \n\n2. **Offline IRL baselines** Depending on whether IRL requires interaction with the environment, we can divide IRL into an online version and an offline version. For the Offline IRL algorithms (including IQ-Learn which has both the online and offline versions and AVRIL[1],), their policy update relies only on the expert demonstration data, so they require no more help from ERB and EQB mentioned in this paper. **I believe the author should include these offline IRL methods in their related works, and they could serve as strong baselines in this paper**. \n\n[1] Alex J. Chan and Mihaela van der Schaar. Scalable Bayesian Inverse Reinforcement Learning. ICLR 2021.\n\n3. **Testing Environment** The MoJoCo benchmark has 8 environments that are commonly applied for evaluation, but this paper studies only 4 of them. I am wondering how well ERB and EQB perform in the rest of the environments.  \n\n4. **Additional Concerns**\n\n- Please include labels into the x-y axis of all plots. The plots without labels are difficult to read.\n\n- \"*which does not recover a reward function that can be used for other purposes.*\"   \nI am confused by the term \"other purposes\". Please clarify.\n\n- \"*Formula (7)*\"  \nI am wondering how to measure if $s^{\\prime}$ is included in $B_{E}$. Will EQB examines all the samples and all the features to find the exact match? If yes, this step will be extremely time-consuming. I suggest training a density model for measuring the density of the input state in the expert distribution. If this density is large enough, we can use the EQB loss.\n\n- \"*Each meta policy is governed by a probability w such that the meta policy selects the current policy chosen action with probability w and the expert next action with probability 1 \u2212 w.*\"   \nI am confused by the definition of $\\omega$, especially when the authors substitute $log\\pi$ with $log\\omega$ in Appendix A. I think $\\omega$ is an important weight between the current and the expert policy, and intuitively, the agent should simply pick the expert action to maximize the Q values, which contradicts Formula (10).\n\n- MaxEnt IRL outperforms ERB and EQB in Figure 1 (b) and Figure 2, although it consumes more episodes or steps. I have explained this issue from the perspective of constraining updates.",
            "clarity,_quality,_novelty_and_reproducibility": "As I mentioned above, the paper is well-structured and easy to follow. This paper has found interesting research questions, but the proposed methods resemble the ideas of previous work, more importantly, the research question is not properly answered by the current version of this paper. Some issues have not been resolved, and there are no theoretical results to quantify the decrease in sample complexity by adding ERB and EQB. Since this is an important research question, the empirical results are insufficient. I strongly recommend continuing this research by considering the points I mentioned above. In terms of reproducibility, I believe the proposed algorithm can be easily implemented.",
            "summary_of_the_review": "This paper describes a very interesting research question by studying the approach of including expert data in the policy update of IRL algorithms. The proposed ERB and EQB algorithms are intuitive and easy to follow, but it requires a more careful understanding of the impact of using expert data, especially when they are significantly different from the current policy. The over-aggressive update can cause many issues (Check the motivation of the update constraining methods like TRPO and PPO). The comparison is incomplete without comparing the offline IRL methods. Overall, I am inclined to reject the paper based on its current version, but I agree the research question is important and I believe the authors can resolve these issues by continuing this research.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1113/Reviewer_K4n8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1113/Reviewer_K4n8"
        ]
    },
    {
        "id": "ZZZlX3ECQg",
        "original": null,
        "number": 2,
        "cdate": 1666484889296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666484889296,
        "tmdate": 1666484889296,
        "tddate": null,
        "forum": "b_cUyW2CJO1",
        "replyto": "b_cUyW2CJO1",
        "invitation": "ICLR.cc/2023/Conference/Paper1113/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the high sample and computational complexity problem in IRL which\napplies RL in its inner-loop optimization. Therefore, the authors proposed two simple\nsolutions: 1). Putting expert transitions in the reply buffer of the inner RL algorithm to\nfamiliarize the learner policy with highly rewarded states, i.e., ERB, and 2). Using expert actions\nin Q value bootstrapping to improve target Q value estimation, i.e., EQB. Experiment results on\nthe MuJoCo tasks shows its superior performance when comparing with MaxEntIRL\napproaches in improving sample efficiency.",
            "strength_and_weaknesses": "Strengths:\n\n1. The idea of incorporating expert guidance seem valid to speed up inner RL loop training.\n2. Experiment results show improved sample complexity over MaxEntIRL related baselines.\n\nWeaknesses:\n\n1. The approach of incorporating expert demonstrations into inner RL training seems to be intuitive. There is no show of applying expert demonstrations in the target Q value estimation will help recover a valid estimation similar to the ground truth Q value.\n2. The experiments do not show performance comparisons with imitation learning baselines such as GAIL.\n3. The performance of ERB and EQB is uncertain. Though in most experiments, ERB+EQB works, they fail in the Walker task. The explanation that the surrogate KL objective and return in the task does not correlate is questionable as it does not tell\nwhy f-IRL works in the Walker task but adding ERB+EQB worsens the performance.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and easy to follow, but the idea of incorporating expert guidance in the inner RL training is intuitive.",
            "summary_of_the_review": "In general, the paper proposes an interesting idea to speed up IRL training in the inner RL training loops with expert guidance. However, the proposed solutions seem incremental by simply incorporating expert demonstrations in reply buffer and target Q value estimation. In addition, it lacks experimental support on its performance over IL baselines, and has performance uncertainties in tasks like Walker.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1113/Reviewer_kiih"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1113/Reviewer_kiih"
        ]
    },
    {
        "id": "9OH8LNWbQs",
        "original": null,
        "number": 3,
        "cdate": 1666506935685,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666506935685,
        "tmdate": 1666506935685,
        "tddate": null,
        "forum": "b_cUyW2CJO1",
        "replyto": "b_cUyW2CJO1",
        "invitation": "ICLR.cc/2023/Conference/Paper1113/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a simple set of modifications to two popular inverse-reinforcement learning (IRL) algorithms to improve their computational efficiencies, including MaxEntIRL and f-IRL. Note that a typical IRL algorithm contains two optimization loops: an outer loop that updates a reward function and an inner loop that runs reinforcement learning (RL), usually many steps of policy iteration. Algorithms like MaxEntIRL and f-IRL do not utilize expert data when solving the inner-loop RL problem and instead must act solely based on the black box reward. The authors propose two simple recipes to accelerate MaxEntIRL: (1) placing expert transitions into the replay buffer of the inner RL algorithm and (2) using expert actions in Q value bootstrapping. Simulation results show significant improvement over a MaxEntIRL baseline, which corroborates the authors' claim.",
            "strength_and_weaknesses": "This paper proposes two simple modifications which can significantly accelerate popular IRL algorithms including MaxEntIRL and f-IRL. The authors perform extensive simulations on standard benchmarks like MuJoCo suite of tasks. Simulation results support the authors' conclusion. The simplicity of the solution and the degree of improvement are both appealing.\n\nOn the other hand, most IRL algorithms do not restrict the type of data used in the inner loop RL, and can benefit from the expert's demonstrations. Therefore, one could apply off-policy Q-learning (or policy gradient) to obtain an optimal solution for the inner loop RL. For instance (Zhu, Lin, Dai & Zhou, NeurIPS 2020) combined off-policy RL with a popular IRL algorithm called GAIL. In other words, there exist IRL algorithms in the literature that could leverage expert data during the inner loop RL. In this sense, this paper's contribution might be limited.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-organized and clearly written. I don't have issues reading and understanding the paper. The proposed solution is straightforward, and seems sound. The authors provide a detailed implementation of the simulations. So readers should be able to verify the experiments on their own.\n\nMy only concern is with the novelty of this work. As explained in my previous point, most of the existing IRL algorithms do not limit the method used to solve the inner loop RL. There exist efficient off-policy learning methods to solve for an optimal policy with expert data and a fixed, black-box reward function. In this way, the main challenge in this paper is somewhat mitigated.",
            "summary_of_the_review": "This paper proposes two simple modifications which can significantly accelerate popular IRL algorithms including MaxEntIRL and f-IRL. Simulation results support the authors' conclusion. The simplicity of the solution and the degree of improvement are both appealing. However, most IRL algorithms do not restrict the type of data used in the inner loop RL, and can benefit from the expert's demonstrations using off-policy methods. In this sense, this paper's contribution might be limited. Still, the specific recipes proposed for a popular algorithm like MaxEntIRL could benefit other researchers in the field.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1113/Reviewer_vmdh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1113/Reviewer_vmdh"
        ]
    },
    {
        "id": "KhzI41FcLn",
        "original": null,
        "number": 4,
        "cdate": 1666513790700,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513790700,
        "tmdate": 1666513790700,
        "tddate": null,
        "forum": "b_cUyW2CJO1",
        "replyto": "b_cUyW2CJO1",
        "invitation": "ICLR.cc/2023/Conference/Paper1113/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method to accelerate the inverse reinforcement learning (IRL) process, by injecting knowledge about the expert's policy in two different ways. The first idea consists in introducing transitions from the expert's demonstrations in the replay buffer (expert replay bootstrapping, ERB). The second idea consists in employing the expert's actions as next actions for defining the target in the learning of the critic (expert Q bootstrapping, EQB). Both ideas are developed formally and tested. The evaluation is performed first on a toy example to illustrate the functioning of the methods and, then, on four Mujoco environments in combination with MaxEntIRL. ",
            "strength_and_weaknesses": "**Strengths**\n- The experimental evaluation shows the two approaches proposed in the paper are able to significantly outperform the plain MaxEntIRL in both the toy example and the Mujoco environments.\n- The approach seems to have the potential to be applied even for other learning algorithms besides SAC.\n\n**Weaknessees**\n- The approach is developed in detail assuming that the inner learning algorithm is SAC. It is not straightforward to generalize some aspects to general actor-critic.\n- It is not clear how these remarkable results compare with standard behavioral cloning.\n- The proposed approaches can be considered incremental w.r.t. the IRL algorithms to which they are applied. Indeed, although reasonable and easy to understand, no guarantee from a theoretical perspective is provided.",
            "clarity,_quality,_novelty_and_reproducibility": "- (Related Works) One of the main motivations behind the methods proposed in the paper consists in observing that the IRL process suffers from a high computation and sample demand due to the need for solving an inner RL problem that, in the opinion of the authors, might be more complex than the IRL itself. However, the \"Related Works\" section completely ignores a class of IRL approaches that do not require the solution of the inner IRL problem, such as [1, 2, 3].\n\n[1] Klein, Edouard, et al. \"Inverse reinforcement learning through structured classification.\" Advances in neural information processing systems 25 (2012).\n\n[2] Pirotta, Matteo, and Marcello Restelli. \"Inverse reinforcement learning through policy gradient minimization.\" Thirtieth AAAI Conference on Artificial Intelligence. 2016.\n\n[3] Ramponi, Giorgia, et al. \"Truly batch model-free inverse reinforcement learning about multiple intentions.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n\n- (Applicability outside SAC) The authors claim that the two recipes proposed by the paper apply to all actor-critic algorithms. However, the approaches are developed in a formal way for SAC only. While I understand that ERB can be applied whenever there exists a replay buffer, I am having trouble figuring out how to apply EQB outside SAC. Indeed, in Section 5, the authors explain how to define the target for TD learning in the case of SAC only. Can the authors elaborate on how this can be extended for general actor-critic (if possible)?\n\n- (Experiments) As the authors comment, the proposed approaches have some relation with behavioral cloning (BC). Indeed, especially ERB tends to promote the learning of an actor that mimics the expert's actions. In this sense, the experimental evaluation, in my opinion, cannot avoid comparison with BC. Indeed, the remarkable performances shown by the proposed approaches might, in principle, be explained by this effect of inducing the actor towards the expert's actions (which is the principle of BC). The only plot in which BC is shown is Figure 1(b), but, in this case, all versions of MaxEntIRL (with and without the proposed approaches) behave in an almost similar way.\n\nFurthermore, I couldn't find the number of runs employed to generate the plots as well as the meaning of the shaded areas (are they confidence intervals? standard deviation?)\n\n\n**Minor Issues**\n- The notation $\\tau \\sim \\pi$ is not explained. I understand that it means that trajectory $\\tau$ is sampled from the trajectory distribution induced by policy $\\pi$, but this should made clear.\n- Missing punctuation in most of the full-line equations.",
            "summary_of_the_review": "Overall, the paper proposes reasonable modifications to IRL algorithms coupled with actor-critic methods, in order to speed up the IRL process. Although the experimental evaluation succeeds in showing the advantages compared with the plain IRL, there is no comparison with BC. Moreover, the proposed modifications are, in my opinion, incremental. Therefore, at the moment I opt for a borderline negative evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1113/Reviewer_wpP8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1113/Reviewer_wpP8"
        ]
    }
]