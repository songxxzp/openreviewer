[
    {
        "id": "nHWEu8KKSZq",
        "original": null,
        "number": 1,
        "cdate": 1666569479115,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666569479115,
        "tmdate": 1670951772569,
        "tddate": null,
        "forum": "47DzlkyH3dM",
        "replyto": "47DzlkyH3dM",
        "invitation": "ICLR.cc/2023/Conference/Paper6373/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a version of LISTA where the update parameters can be made specific to each data sample even when the sensing matrix is not the same among samples. The paper then also proposes a variational method to dictionary learning to be integrated with the former approach.\n",
            "strength_and_weaknesses": "The paper considers a novel setting that is more flexible than the common assumption in the literature. \n\nI am personally not convinced regarding the impact of an approach that tolerates variable sensing matrices. When compressed sensing architectures exhibit different sensing operators for different data, it is usually because there is little control over the behavior of the operator, so it would be difficult to accurately obtain the operators involved. Perhaps the authors can elaborate in Section 3.2 regarding the problems where the matrix $\\Phi$ can change for each sample but is known in each case. \n\nThe approach that includes variational dictionary learning, described in Sections 3.4.2-3 apparently assumes that the sensing matrix $\\Phi$ is known, which counters the setup where LISTA is applied (in addition to the point above) and the matrices involved in LISTA are \"learned\". It is not clear how one would infer both $\\Phi$ and the dictionary $\\Psi$ simultaneously. The authors should also clarify whether the sensing matrix needs to be known as it varies in each case.\n\nThe experimental results are limited - for such a flexible prior more challenging datasets should be attempted. Other papers that use data-driven priors in compressed sensing, such as GANs, do so. Furthermore, the only baseline used that employs a data-driven model is LISTA, a method that has driven significant research into improved variations; thus, the comparison against older algorithms BCS (2008) and ISTA (2010) is not informative. Finally, if the authors take into account the computation time for ALISTA and NALISTA in evaluating the matrix $W$ for each sample to discount them from the comparison, they should compare this against the training time for the deep learning networks used in this and competing papers. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Some specific points in the paper are not clear:\n\nI do not see how LISTA models assume that the learned dictionary is known a priori - my understanding is that this is part of what LISTA learns. It seems at times that the authors are combining the concept of \"fixed dictionary\" with \"known dictionary\", e.g., the last sentence of Section 3.2. Perhaps the authors mean to say that the sparse representation of the signal obtained by LISTA is not useful on its own without knowing the dictionary that can translate it into the actual signal?\n\nIn page 3, a matrix A is described such that $A^TA = \\Phi\\Psi$, but I do not believe that $\\Phi\\Psi$ would be a square matrix when compressed sensing is applied.\n\nThe performance characterization metrics in (4-6) appear to depend on the iteration count $t$, but the dependence is not referenced in the notation.\n\nIn eq. (5), it appears $\\Psi$ should be $\\Psi_t$ or $\\Psi_o$ in the first part of the right hand side of the equation, given that no single dictionary is assumed?\n\nIn Theorem 3.1, the assumption includes \"$\\mathrm{supp}(z^*)=\\mathrm{supp}(z_{t-1})$\" and the conclusion says \"$\\mathrm{supp}(z_{t-1}) \\subseteq \\mathrm{supp}(z^*)$\", which appears obvious to me. Is there a typo?\n\nIn Fig. 1, the dictionary $\\Psi^t$ appears as an output of each blue block, and it is not clear how that is the case. There is also little detail on the augmentation network (red blocks) that produces the parameters. Note also the discrepancy in notation $\\Psi_t$ vs. $\\Psi^t$.\n\nSome of the notation in Section 3.4 describes distribution for matrices and vectors in terms of scalar PDFs - if this is an abuse of notation, its meaning should be stated explicitly.\n\nThe training of the augmentation networks that need to take into account the sparsity matrices and dictionaries seems to be a significant undertaking. It is not clear how these networks are trained for the experiments.",
            "summary_of_the_review": "I have several questions regarding clarity on the details of the algorithm and the significance of the experimental results that prevent me from recommending acceptance. Given the wealth of approaches based on data-driven models and compressed sensing, the numerical results need to be more compelling.\n\nPost-rebuttal: The authors have addressed the majority of my questions satisfactorily ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6373/Reviewer_uswp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6373/Reviewer_uswp"
        ]
    },
    {
        "id": "ji2l4rCevPV",
        "original": null,
        "number": 2,
        "cdate": 1667366042075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667366042075,
        "tmdate": 1667366042075,
        "tddate": null,
        "forum": "47DzlkyH3dM",
        "replyto": "47DzlkyH3dM",
        "invitation": "ICLR.cc/2023/Conference/Paper6373/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the problem of sparse vector recovery with unknown dictionary and time varying sensing matrix, and proposes a data-driven approach using variational learning. The signal recovery performance of the proposed method as well as the possibility of out of distribution detection are demonstrated via numerical experiments.",
            "strength_and_weaknesses": "The paper is generally interesting, and enough contribution can be recognized in the proposed approach. However, the signal recovery performance of the proposed method is worse than that of existing method (i.e., A-DLISTA) in the experimental results. Also, the organization of Sect.3 might be somewhat misleading, because it contains both the existing methods and the proposed method while it is titled \"variational learning ISTA\", which is the name of the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "I don't have any difficulty in understanding the proposed method.\nAlso, a certain novelty can be found in the methodology itself.",
            "summary_of_the_review": "The paper considers a timely and important problem, and work in the paper is solid and interesting, while the performance gain shown in the numerical results might be rather marginal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6373/Reviewer_2dyB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6373/Reviewer_2dyB"
        ]
    },
    {
        "id": "lMoBeR0fglW",
        "original": null,
        "number": 3,
        "cdate": 1667389396447,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667389396447,
        "tmdate": 1669209300387,
        "tddate": null,
        "forum": "47DzlkyH3dM",
        "replyto": "47DzlkyH3dM",
        "invitation": "ICLR.cc/2023/Conference/Paper6373/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present two unrolled ISTA-type algorithms for compressed sensing. The first approach is amenable to adaptive measurements matrix while learning the sparsifying dictionary per layer. The second approach called as VLISTA places a probability distribution the dictionaries. This provides a probabilistic way to learn dictionaries which could be useful in out of distribution (OOD) detection problems. ",
            "strength_and_weaknesses": "Strengths\n\n- The paper is well-written and easy to follow. The main ideas are well explained.\n- Allowing to learn the dictionary in a measurement adaptive environment is quite interesting. Also, the probabilistic formulation of the problem, which leads to VLISTA, uses ideas of Bayesian deep learning in the unrolled optimization framework, and thus it has its own interest.\n\nWeaknesses\n- I understand that assigning Gaussian distributions in the prior and posterior simplifies things a lot regarding the implementation of the algorithm. However, isn't quite restrictive in practice? I wonder if other distributions can be modeled in that framework. E.g., using hierarchical models that encode heavy-tailed distributions.\n- In the experiments,  Section 4.1, it seems that A-DLISTA outperforms ISTA and LISTA. However, it is not clear what is the measurement matrix that is given as input to these algorithms. Does LISTA know that the measurement matrix changes over time?\n- It is not well explained why VLISTA works so much worse than A-DLISTA.\n- In the OOD experiment, there is no comparison with any other competing method. Why BSC algorithm is not included in the experiments?\n- In Theorem 1, since $\\theta_t,\\gamma_t$ are learned by the algorithm. Hence for the sufficient condition given in (7)  to be satisfied, it is important $\\theta_t$, and $\\gamma_t$ converge as t ( here denotes the number of layers) grows. Can the authors provide some insight regarding the convergence of these parameters? Moreover, it would be interesting for the authors to show experimentally that this condition is satisfied at a certain number of layers T.\n\nMinor\n- There seems to be an issue with statement 1 in Theorem 1. The conclusion should be $\\mathrm{supp}(z_{t}) \\subseteq \\mathrm{supp}(z^\\ast)$.  It seems there is a typo there.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The main ideas are clearly presented in the paper. Learning dictionaries in an adaptive measurements environment seems to be novel (though I might miss works that might have recently been published on that topic).",
            "summary_of_the_review": "The authors present two approaches for learning sparse representations and dictionaries under the unrolled optimization framework.\nThe ideas are simple but interesting. The authors provide a sufficient condition for the algorithm to converge to the correct support and provide an error bound. Experimental results demonstrate the efficiency of the proposed algorithms on simulated data and on MNIST dataset. The paper could be significantly improved if the authors could better motivate the selection of the Gaussian distributions for the priors and posteriors and provide more insight into how restrictive these assumptions are in practical settings. The experimental section could also be improved by providing experimental results on datasets such as CIFAR dataset and more extensive results of the probabilistic method (VLISTA). \n\n-----------------------------\nPost-rebuttal update:\nI wanted to thank the authors for their time and effort in addressing the reviewers' comments. I am pretty satisfied with the responses, and I  appreciate the changes they made to the paper. I  thus raise my score to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6373/Reviewer_3LdD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6373/Reviewer_3LdD"
        ]
    }
]