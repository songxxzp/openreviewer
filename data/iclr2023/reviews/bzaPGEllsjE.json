[
    {
        "id": "4rQLwpJiq1",
        "original": null,
        "number": 1,
        "cdate": 1666548488963,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548488963,
        "tmdate": 1666548488963,
        "tddate": null,
        "forum": "bzaPGEllsjE",
        "replyto": "bzaPGEllsjE",
        "invitation": "ICLR.cc/2023/Conference/Paper667/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work builds a new convergence analysis framework for SGD algorithm (with momentum). It proposes a special family functions of \u201dSpectrally Expressible\u201d approximations, which provides a new perspective to understand the behavior of classical SGD. A specific senario where a negative momenta can be the optimal choice is constructed.",
            "strength_and_weaknesses": "***Strenght***\nThis paper applies the generating function to analyze the higher moments of SGD noises. The new tool reveals the explicit stablility condition, which answer when SGD can converge and diverge. Most intersting part is that this paper introduces a situation where the negative momenta is better.  Also, the theoretical result perfectly explains the dynamic behavior of SGD over MNIST dataset.\n\n***Weaknesses***\nThe loss function is relatively special, though I agree that this loss is widely used in modern machine learning tasks. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This work is well-written and completely new in this field (including both its technique and results). ",
            "summary_of_the_review": "I recommend to accept this paper as its results are impressive and really new to the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper667/Reviewer_NWpJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper667/Reviewer_NWpJ"
        ]
    },
    {
        "id": "R85D9EiXEL",
        "original": null,
        "number": 2,
        "cdate": 1666636179098,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636179098,
        "tmdate": 1669532737859,
        "tddate": null,
        "forum": "bzaPGEllsjE",
        "replyto": "bzaPGEllsjE",
        "invitation": "ICLR.cc/2023/Conference/Paper667/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work investigates stochastic gradient descent with momentum for the least squares problem at fixed design. In particular, it derives closed-form equations for the evolution of second moments of the weights and momentum parameters under a \"spectrally expressible\" (SE) approximation, which allow one to compute the trajectory of observables throughout the dynamics from the initial weight covariance matrix and covariance of the features (i.e. the Hessian). In particular, this approximation holds for translational invariant kernels in a grid and for Gaussian features, and Theorem 1 provides different equivalent characterisations of the proposed SE approximation. By rewriting the evolution and identifying it with an interacting gas of rank-one operators, the authors separate the contributions of the signal and sampling noise.  The authors study this evolution equations under the SE approximation and derive different regimes for the dynamics as a function of the learning rate, the momentum strength and the spectrum of the features covariance. In particular, it is observed that an unusual choice of small negative momentum can be desirable in the noise dominated phase.\n",
            "strength_and_weaknesses": "**Comments and Questions**:\n\n**[Q1]**: A series of recent works have shown that the behaviour of one-pass SGD on small neural networks [Goldt et al. '20] and random features [Goldt et al. '22] as well as full-batch learning [Loureiro et al. '21] and mini-batch SGD [Bordelon, Pehlevan '21] on linear models trained on fixed feature maps are well approximated by an equivalent model with Gaussian features in the high-dimensional limit. As mentioned in the manuscript, in this case the \"Spectrally expressible\" approximation is exact. By the way, all of the aforementioned works contain experiments of Gaussian equivalence on MNIST, which is consistent with the authors observation that their SE approximatiom works well in this case. I wonder whether Gaussian equivalence is only sufficient for SE to hold or also necessary. In other words: do the authors have an example where the SE approximation holds but the phenomenology is different from having Gaussian features?\n\n**[Q2]**: The sentence in the introduction:\n> The goal of the present work is to derive explicit and easy-to-use analytic evolution laws for the SGD (1) applicable to modern deep neural networks trained on real world datasets.\n\nis a huge overstating, not only given the theoretical setting considered in this work (least-squares regression) but also the lack of empirical evidence for that (i.e. checking the theory on MNIST). Moreover, the following sentence:\n\n**[Q3]**: Moreover, in the next sentence in the introduction:\n>First, in many (though not all) cases the nonlinear network training is reasonably well approximated (Lee et al., 2020) by the linearized NTK regime (Jacot et al., 2018), in which the loss L is quadratic with respect to parameters w.\n\nWhere in (Jacot et al., 2018) it is shown that NTK = square loss?\n\n**[Q4]**: I am not very familiar with the related literature. But browsing some papers for this review I stumbled upon [Paquette, Paquette '21], who studies SGD with momentum in least squares with Gaussian random design. Their conclusion is that in the strong convex case momentum does not improve the performance of SGD, while in the non-strongly convex case it is possible to get considerable improvement. Is this consistent with taking a Marchenko-Pastur distribution for the Hessian matrix here? How does this conclusion changes as a function of the choice of feature map?\n\n**References**:\n\n[[Bordelon, Pehlevan '21]](https://arxiv.org/abs/2106.02713) B Bordelon, C Pehlevan. *Learning Curves for SGD on Structured Features*. arXiv: 2106.02713 [cs.LG]\n\n[[Loureiro et al. '21]](https://proceedings.neurips.cc/paper/2021/hash/9704a4fc48ae88598dcbdcdf57f3fdef-Abstract.html) B Loureiro, C Gerbelot, H Cui, S Goldt, F Krzakala, M Mezard, L Zdeborov\u00e1. *Learning curves of generic features maps for realistic datasets with a teacher-student model*. Part of Advances in Neural Information Processing Systems 34 (NeurIPS 2021).\n\n[[Goldt et al. '20]](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.10.041044) S Goldt, M M\u00e9zard, F Krzakala, L Zdeborov\u00e1. *Modelling the influence of data structure on learning in neural networks: the hidden manifold model*. Phys. Rev. X 10, 041044 \u2013 Published 3 December 2020.\n\n[[Goldt et al. '22]](https://proceedings.mlr.press/v145/goldt22a.html) S Goldt, B Loureiro, G Reeves, F Krzakala, M Mezard, L Zdeborova. *The Gaussian equivalence of generative models for learning with shallow neural networks*. Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference, PMLR 145:426-471, 2022.\n\n[[Paquette, Paquette '21]](https://papers.nips.cc/paper/2021/hash/4cf0ed8641cfcbbf46784e620a0316fb-Abstract.html) C Paquette, E Paquette. *Dynamics of Stochastic Momentum Methods on Large-scale, Quadratic Models*.  Part of Advances in Neural Information Processing Systems 34 (NeurIPS 2021).",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality**: As someone not familiar with this line of work, I personally found the manuscript dense and hard to parse. The focus of the manuscript is on the technical discussion, with the discussion reduced to enumerating the different regimes of the dynamics. The introduction largely overstates the importance of least squares.\n\n**Novelty**: I am not familiar with the related literature, so I have limited myself to browsing some works on momentum for SGD on least squares.\n\n**Reproducibility**: The appendix is well detailed, and code for reproducing the plots is provided by the authors.\n",
            "summary_of_the_review": "This work provides a characterisation of the SGD dynamics with momentum for the least squares problems with fixed design. The introduction overstates the importance of the studied setting. The manuscript is dense and hard to parse, and I feel that developing better the discussion and highlighting better the main contributions would improve the reading. For a non-expert, it is not very clear what are the challenges and the progress made the manuscript with regard to previous literature. Therefore, I believe this work could benefit from a revision. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper667/Reviewer_LoFN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper667/Reviewer_LoFN"
        ]
    },
    {
        "id": "H30_SDJ1vZO",
        "original": null,
        "number": 3,
        "cdate": 1667354266094,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667354266094,
        "tmdate": 1667354266094,
        "tddate": null,
        "forum": "bzaPGEllsjE",
        "replyto": "bzaPGEllsjE",
        "invitation": "ICLR.cc/2023/Conference/Paper667/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops a theoretical framework to analysize the noise averged properties of  for mini-batch SGD with momentum (bSGDM) for the linear models when learning rate, mini-batch size and momentum parameter is fixed. To handle the noise terms appearing in the analysis, they approximate that variance term with specterally exprissible approximations. This approximation allows them to form a generative function for the loss function trajectory. Analysing this generating function 1- they specify the noise dominant and signal dominant phase in loss trajectory 2- when Hessian spectral distribution follows power low distribution, they analyze the stability of loss function and show when divergence and convergence happens 3- show that negative momentum sometimes can helps. They give some theoretical approximation for optimal parameter values and the phase transition times. Empirically they show that their analysis matches the practice for MNIST and some artificial data sets. \n",
            "strength_and_weaknesses": "The major strength of the paper is that its analytical framework matches well enough with the empirical observation. \n\nThe major weakness of the paper is that the provided framework is applicable for too limited cases due to their assumptions and approximations. Besides the paper is not well-written and easy to follow.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the problem and setting are explained well. However, the intuitions and motivation of the approximation they use is not explained enough. \n\nQuality: the mathematical equations are sound and look correct as much as I checked. \n\nNovelty: the major novelty of the paper is using the generating function to analyze the loss trajectory. \n",
            "summary_of_the_review": "The major problem I have with the paper is its writing. I believe it is hard to understand and read the paper easily. Here are some examples: \n\n1- there isn\u2019t any part in the paper to explains the notation and operations used in the formula. For example there U \\in O(N) where U is a matrix. \n\n2- You claim that you drive exat loss asymptotics for bSGDM however you use a lot of approximations in your analysis. So making such a general claim is not accurate. \n\n3- You have some terms which are not defined explicitly. For example, \\lambda_crit or regularization parameter is not defined anywhere. \n\n4-  The idea behind approximating eq 5 by eq 6 isn\u2019t explained well. The paper says this is due to the existence of non-spectral details in eq 5 but this is not enough. It would be helpful to show these details and show how eq 7 would solve it. \n\n5- In eq 8 which is an approximate for eq 5, there are cross terms between feature maps of two different samples that don\u2019t exist in eq 5 and it can remove the stochasticity from the dynamics. So the question is how good is this approximation i.e. the upper bound of the difference of these two eqs. \n\n6- For your approximation of L(t) in \u201cTransitions between phases\u201d, it adds eq 27a and eq 27b. However, depending on \\zetta and \\nu one of these equations can happen for L(t). So why the sum of them is also a good approx for L(t)? \n\n7- One of the claimed contributions is that \u201cthe optimal convergence rate can be achieved at negative momenta\u201d. However, the analysis in the \u201cnegative momenta\u201d section is hand-wavy and for a very limiting case. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper667/Reviewer_KUvw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper667/Reviewer_KUvw"
        ]
    }
]