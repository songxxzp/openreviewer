[
    {
        "id": "37MXNWM4WV",
        "original": null,
        "number": 1,
        "cdate": 1666002932802,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666002932802,
        "tmdate": 1666002932802,
        "tddate": null,
        "forum": "I89hkzP0U4y",
        "replyto": "I89hkzP0U4y",
        "invitation": "ICLR.cc/2023/Conference/Paper1881/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "With the increase in the complexity of machine learning models, it becomes crucial to explain model decisions and increase transparency between human-model interactions. To this end, several explanation methods have been proposed in recent literature for explaining model predictions, but few works discuss necessary desiderata (faithfulness, stability, and fairness) to evaluate the reliability of an output explanation. In this work, the authors discuss the stability property of an explainer, i.e., an explainer should give similar explanations for similar data inputs. The paper formalizes explainer astuteness by leveraging probabilistic Lipschitzness (the probability of local smoothness of a function). Further, it provides bounds for guarantees on the astuteness of these explainers given the Lipschitzness of the prediction function and empirically validates the behavior using real-world datasets. ",
            "strength_and_weaknesses": "**Strengths**\n\n1. The paper presents a theoretical analysis of the stability of explanations generated using a variety of diverse explanation methods, including SHAP, RISE, and CXPlain.\n2. Theoretical and empirical analysis show that locally smooth predictive models lead to more robust explanations.\n\n**Weaknesses and Open Questions**\n\n1. The notion of stability is limited to the prediction behavior of the model, defined in Alvarez-Melis & Jaakkola et al. However, recently Agarwal et al. highlighted that the existing stability definition assumes model $f$ has the same behavior for similar inputs $\\mathbf{x}$ and $\\mathbf{x}'$, i.e., the model uses the same logic path (e.g., activating same neurons in a deep neural network) to predict the same label for the original and perturbed instance. It would be great if the authors can comment on this property of relative stability.\n2. Previous works like Alvarez-Melis & Jaakkola et al., Agarwal et al., and Fel et al. have used the predictive model's Lipschitz constant to quantify the stability of explanations. This limits the novelty of the proposed work.\n3. Unlike RISE and SHAP, we do not observe higher astuteness for the CXplain explanation method. It would be beneficial for the readers if the authors can give any insights into this behavior.\n\n\n**References**\n1. Fel, T., Vigouroux, D., Cad\u00e8ne, R. and Serre, T. How good is your explanation? algorithmic stability measures to assess the quality of explanations for deep neural networks. In WACV, 2022.\n2. Agarwal, C., Zitnik, M. and Lakkaraju, H. Towards a rigorous theoretical analysis and evaluation of gnn explanations. In AISTATS, 2022.\n3. Alvarez-Melis, D. and Jaakkola, T.S. On the robustness of interpretability methods. In arXiv, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe main idea and hypothesis of the paper is clear.\n\n**Quality**\n\nThe paper is not well written. Especially, the authors have significantly changed the ICLR latex template by removing spaces between the sections and subsections. \n\n**Novelty**\n\nThe paper presents incremental novelty.",
            "summary_of_the_review": "The paper provides interesting theoretical and empirical stability analysis of diverse explanation methods. The proof for the Astuteness of explainers that remove individual features should scale to a plethora of feature attribution methods, but the paper does not show the results for any.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1881/Reviewer_Hif5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1881/Reviewer_Hif5"
        ]
    },
    {
        "id": "T35KtlNbS6",
        "original": null,
        "number": 2,
        "cdate": 1666343999261,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666343999261,
        "tmdate": 1666343999261,
        "tddate": null,
        "forum": "I89hkzP0U4y",
        "replyto": "I89hkzP0U4y",
        "invitation": "ICLR.cc/2023/Conference/Paper1881/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a theoretical analysis on the robustness of explainers.\nHere, the robustness is measured as the difference between the two point $x$ and $x'$ in a close neighborhood.\nIf explanations for $x$ and $x'$ largely differ, the explainer is deemed to be not robust.\nFor the analysis, the authors adopted the notion of *astuteness* which was originally introduced for measuring the robustness of classifiers.\nThe authors extended the idea of astuteness for explainers and defined the explainer astuteness (larger the better), which is the robustness measure considered in this paper.\nThe authors derived the lower bounds of the explainer astuteness for some popular explainers, such as SHAP, feature removal, and RISE.\nThe results indicate that if the target function to be explained is $L$-Lipschitz with respect to the $\\ell_p$ distance (more formally, locally probabilistic Lipschitz), the explainers are $O(\\sqrt[p]{d}L)$-Lipschitz with high probability.\n",
            "strength_and_weaknesses": "### Strength\nThis will be the first study connecting the smoothness of the target function and the robustness of the explainers.\nAs the authors stated as \"the statement is intuitive\", the results look reasonable; for less smooth functions with sharp changes, the explainers also need to change drastically to reflect the change of the function, leading to less robust explanations.\nProviding rigorous proofs to intuitions will be an important step towards understanding the relationship between the models and the explainers.\n\n### Weakness\n(My comment here is not really a weakness, but a suggestion for a further improvement.)\nThe weakness of the paper is that all the explainers considered are found to be $O(\\sqrt[p]{d}L)$-Lipschitz.\nAlthough dedicated proof techniques are required for different explainers, the results are not very surprising.\nThe results will be far more interesting if there are explainers that have different orders of Lipschitzness.\nFor example, if an explainer with large Lipschitzness is found, the use of that explainer should be avoided.\nOr, if an explainer with small Lipschitzness is found, that explainer can be a recommended choice in practice.\nI wonder whether there are any explainers with these interesting properties.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity, Quality, Novelty\nThe research motivation as well as main results are stated clearly.\nFormalizing the theoretical framework connecting the smoothness of the function and the robustness of the explainers will be the novelty of the paper.\n\n### Reproducibility\nExperimental setups are described briefly in the paper.\nThe results may be reproducible.",
            "summary_of_the_review": "This is an interesting study connecting the smoothness of the function and the robustness of the explainers.\nAlthough the results are not very surprising, establishing a theoretical framework would be an important contribution to the field.\nThe current paper analyzed a few popular explainers and concluded that they are all $O(\\sqrt[p]{d}L)$-Lipschitz.\nThe results will be far more interesting if there are explainers that have different orders of Lipschitzness.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1881/Reviewer_tAqb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1881/Reviewer_tAqb"
        ]
    },
    {
        "id": "lX_06JwjYE",
        "original": null,
        "number": 3,
        "cdate": 1667063088551,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667063088551,
        "tmdate": 1667063088551,
        "tddate": null,
        "forum": "I89hkzP0U4y",
        "replyto": "I89hkzP0U4y",
        "invitation": "ICLR.cc/2023/Conference/Paper1881/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes smoothness/robustness of explainers of blackbox classifiers as a desirable objective. It gives theoretical bounds for this measure of robustness, (called explainer astuteness) in terms of the Lipschitz constant of the learned blackbox model. Theorems relating the two are given for three types of explainers, and the results are demonstrated empirically on a few datasets.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper has an easy-to-follow structure and repeatedly emphasizes its message relating the model Lipshitzness and explainer astuteness. \n\nWeakness:\n\nIt is not at all clear why the end user of a blackbox explainer cares about the robustness of the explainer. The blackbox model is typically not under the end-user's control, and the only thing expected of an explainer is its faithfulness to the blackbox model. These results would be interesting, if different explainers had drastically different robustness for the same blackbox model -- that way the paper can be viewed as an advisory on what types of explainers to use in case the user cared about smoothness of the explainer.\n\nThe technical results are hard to follow and not written mathematically precisely. e.g. in Lemma 1 what are these \"sampling points\"? What is \"i\" in the set N_k? The usage of \"approaching\" is confusing -- what is the limit or sequence here? It looks like the set N_k is the set of points with k non-zero co-ordinates. Does the statement mean that beta=alpha when the data points all contain only one non-zero entry? Elsewhere in the paper something else is mentioned that confuses me further. \n\nAs the authors themselves mention, it is not at all surprising that the Lipshitz constant of the learned model affects the Lipschitz constant of the explainer, as indeed the explainer is designed and required to mimic the black-box model. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe technical results are written in mathematically imprecise language, round up of this will make the contributions less messy. \n\nThe paper structure is somewhat strange. A single subsection (3.1) is larger than the rest of the paper.\n\n\nOriginality:\n\nWhile a formal theorem relating explainer smoothness to blackbox function smoothness might be novel, it is not particularly surprising or immediately useful. \n\n",
            "summary_of_the_review": "Summary:\n\n1. The significance of explainer astuteness to the end-user needs validation.\n2. The mathematical details are imprecise.\n3. A method for modifying existing explainers, with increased astuteness without sacrificing faithfulness to the black-box model is required  to bump this paper to the next level \n4. A result showing the theoretical maximum astuteness of any explainer for a given blackbox model (with appropriate faithfulness) would also be appreciated.\n\nComments:\n\nIf explainer astuteness is proposed to be of independent interest, a subjective study is required where test subjects rank different explainers based on repeated use of the blackbox model and the explainer.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1881/Reviewer_bjVt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1881/Reviewer_bjVt"
        ]
    }
]