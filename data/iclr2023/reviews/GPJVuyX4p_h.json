[
    {
        "id": "eAEr2ProSvU",
        "original": null,
        "number": 1,
        "cdate": 1666461189677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666461189677,
        "tmdate": 1666461189677,
        "tddate": null,
        "forum": "GPJVuyX4p_h",
        "replyto": "GPJVuyX4p_h",
        "invitation": "ICLR.cc/2023/Conference/Paper5464/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides a gradient estimator when the computation graph includes a k-subset sampling module.",
            "strength_and_weaknesses": "* Strength \n\nThe paper provides a gradient estimator when the computation graph includes a k-subset sampling module.\n\n* Weakness\n\nComparisons with other methods can be made more comprehensive.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper provides a gradient estimator when the computation graph includes a k-subset sampling module.",
            "summary_of_the_review": "The paper provides a gradient estimator when the computation graph includes a k-subset sampling module, which is new to me.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5464/Reviewer_AhEr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5464/Reviewer_AhEr"
        ]
    },
    {
        "id": "DcZcz1C2oj",
        "original": null,
        "number": 2,
        "cdate": 1666617455095,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617455095,
        "tmdate": 1666630992741,
        "tddate": null,
        "forum": "GPJVuyX4p_h",
        "replyto": "GPJVuyX4p_h",
        "invitation": "ICLR.cc/2023/Conference/Paper5464/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper suggest a k-subset sampling gradient estimator, namely SIMPLE. The proposed SIMPLE estimator can replace the reparameterization by relaxation with exact gradient estimation, and utilizes the gradient with respect to the exact conditional marginals in the backpropagtion. The authors conducted various experiments to demonstrate the performance of SIMPLE estimator: synthetic example, discrete variational autoencoder, learning to explain, and sparse linear regression.\n",
            "strength_and_weaknesses": "The paper studies the exact gradient estimator for the n-choose-k distribution. The idea of this paper seems quiet novel. Also, it seems that the proposed method achieved better result in bias/variance, ELBO of DVAE, etc. \n\nHowever, the paper is very hard to follow, not because of the difficulty of the paper, but due to the presentation of the paper. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is the major concern. \n\n- It seems that the details are missing often, and a lot of parts are written informal way.\n\n- It seems that the details are missing, and a lot of parts are written in informal way. \n\n- For example, writing .detach() in the middle of the text seems inappropriate. \n\n- What is jvp? \n\n- The authors use terminology and abbreviation without consistency. \n\n- What is SoG I-MLE?\n\n- Which hyper-parameter values are used in Figure 1? \n\n- The same problem happens in Figure 3, also. These make those experiments looks unfair and hard to compare the proposed model against baselines.\n\n- Also, I suggest placing the proofs of the propositions in the appendix, and enhancing the experiment settings, for example in Section 5.4.\n\n- For the case when k=1, there are lots of baselines which can be compared in the synthetic example and DVAE experiment. I\u2019m surprising that the authors only compared to the Gumbel-Softmax in Figure 4. \n\n- The paper seems to need more proof-reading, and it seems that the paper can be presented in the better shape.\n\nAlso, the code is not provided, which makes harder to understand paper and use the provided method.\n\nHere are some questions to the authors.\n\n- Could you compare the running time against the baselines, especially when k varies?\n\n- I don\u2019t understand the experiment setting in DVAE. Why does the decoder has 256-dim in the first decoder weight, while the last encoder weight has 20^2?\n",
            "summary_of_the_review": "I agree with the methodology, but for some experiments, the explanation of the experimental method is insufficient, and the reliability of the results is insufficient. Also, the presentation of the paper difficult to understand. I therefore lean negatively on this paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.\n",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5464/Reviewer_6jdF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5464/Reviewer_6jdF"
        ]
    },
    {
        "id": "RFHqq13fjp",
        "original": null,
        "number": 3,
        "cdate": 1666636918392,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636918392,
        "tmdate": 1668510589413,
        "tddate": null,
        "forum": "GPJVuyX4p_h",
        "replyto": "GPJVuyX4p_h",
        "invitation": "ICLR.cc/2023/Conference/Paper5464/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes novel gradient estimator for k-subset sampling. Authors justify their method with rigorous proofs and conduct experiments to support their claims. Experiments have major flaws that should be addressed.",
            "strength_and_weaknesses": "Strength:\n-- efficient and novel gradient estimator\n-- experiments on synthetic and real data\n-- rigorous theoretical analysis\n\nWeaknesses:\n-- choice of baselines in experiments seems odd, authors claim that their estimator is better then Gumbel softmax estimator but do not present comparison of it's quality on real data\n-- improper comparison with Gumbel Softmax estimator, algorithm proposed by authors has asymptotic worse then of Gumbel SoftMax by a factor of k therefore I wonder whether their reduction in variance and bias will be significant over GS if we choose number of samples for it greater then 1 as authors did to match running time of their estimator.\n-- it's not clear why GS and proposed estimator have bias at all -- GS based on log-likelihood trick should be unbiased and proposed estimator is claimed to exactly compute probabilities therefore it should be unbiased too (or am I missing something here?). Backprop cannot increase bias if it's absent since chain rule is basically reduces to application of linear mapping on unbiased estimate.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The estimator seems novel, algorithms are well described, I have not found link to the code of experiments nor supplementary files with it.",
            "summary_of_the_review": "The proposed sampler is novel while experimentation section of the paper is flawed and requires clarifications.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5464/Reviewer_pM21"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5464/Reviewer_pM21"
        ]
    },
    {
        "id": "bsVXCn6mTv",
        "original": null,
        "number": 4,
        "cdate": 1666640226254,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640226254,
        "tmdate": 1666642029255,
        "tddate": null,
        "forum": "GPJVuyX4p_h",
        "replyto": "GPJVuyX4p_h",
        "invitation": "ICLR.cc/2023/Conference/Paper5464/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an approach for gradient estimation for discrete k-subset sampling, called SIMPLE.  For the forward pass, this approach involves discrete sampling.  For the backward pass, SIMPLE efficiently computes the gradient with respect to the exact marginals of the $k$-subset distribution.  Compared to competing approaches for gradient estimation in this setting, SIMPLE exhibits lower bias and variance.  In addition to an efficient algorithm for computing the gradient, the authors also present an algorithm for computing the exact ELBO for $k$-subset distribution, leading to state-of-the-art performance on sparse VAE learning.",
            "strength_and_weaknesses": "Strengths:\n* Compared to competing approaches for computing gradients for k-subset sampling, SIMPLE provides significant reductions in bias and variance.  Compared to competing approaches, SIMPLE can also provide moderate to significant improvements in predictive performance when used for training models, as shown by the experimental results in this paper for the learning to explain task.\n* When used to compute the ELBO for a $k$-subset discrete VAE, compared to competing approaches SIMPLE can provide significantly lower ELBO loss on the test set.\n* The complexity analysis presented in this paper shows that the proposed SIMPLE algorithm appears to scale when computing the required marginal probabilities and sampling k-subsets, particularly when using vectorized computations for parallelization.\n\nWeaknesses:\n* The complexity analysis provided in the paper focuses on algorithms for computing marginals probabilities and for sampling $k$-subsets.  However, an explicit analysis of the complexity for computing the backward pass (from Algorithm 3) appears to be missing.  This should be provided in the paper.\n* This paper does not include a discussion of the weaknesses/drawbacks of the proposed approach.  For example, SIMPLE seems to assume that the $k$-subset distribution can be factorized as a product of the conditional marginals; that is, the subset is a multilinear function of its elements (as pointed out in the first paragraph of Sec. 3.1).  However, this assumption may only be valid in cases where the $k$-subset distribution does not involve higher-order interactions between elements within a subset, such as subsets that are modeled using a Transformer-based model.  An explicit discussion of this limitation should be provided in the paper.\n* An empirical evaluation of the runtime of SIMPLE, compared to the competing approaches, is missing and should be provided.  While the complexity analysis appears to show that SIMPLE scales, an empirical validation of this scalability is important.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is reasonably clear.  The contributions appear to be novel, with good empirical and theoretical contributions.  Although the authors do not currently provide source code, enough detail appears to be present in the paper to allow the proposed algorithms to be reproduced.",
            "summary_of_the_review": "There are some important empirical and theoretical contributions in this paper. The experimental results are somewhat convincing, showing that the proposed SIMPLE method outperforms a number of competing methods for estimating gradients for the discrete k-subset setting, in terms of both reduced bias and variance, and improved predictive performance.  However, there are several issues with this paper that should be addressed by the authors, as mentioned in the list of weaknesses above.  Overall, in terms of acceptance, this is a borderline paper in its current state.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5464/Reviewer_XcRS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5464/Reviewer_XcRS"
        ]
    }
]