[
    {
        "id": "YYABwhsguAI",
        "original": null,
        "number": 1,
        "cdate": 1666389342068,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666389342068,
        "tmdate": 1666390392368,
        "tddate": null,
        "forum": "77aKxP46geN",
        "replyto": "77aKxP46geN",
        "invitation": "ICLR.cc/2023/Conference/Paper1040/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work introduces Dateformer for time-series forecasting. The time-series is split into day based patches and the processing is shifted from time-based towards patch based. This work uses time representations as the modeling entity and empirically demonstrates the strength of the proposed approach on multiple datasets. ",
            "strength_and_weaknesses": "Strength : \n\nThis work proposes a structured approach and uses several concepts to create a framework that can be utilized with a very long history to forecast time-series. Furthermore, the performance over several datasets demonstrates the strength of the proposed approach. Overall, this work does seem in the right direction for more robust time-series forecasting. However, there are some questions that come up.\n\n\nWeakness : \n\nThere seem to be some ideas that were not very clear. Such as :\n\n--- As a basic comparison, what would be the performance if the aggregate value (or a domain/data based learned representation) of each patch (a day in this case) is directly used as input to a transformer based forecasting model? The length of the input would be the same as what is proposed in this work so any additional resource issue would be mitigated. \n\n--- Why is it that broadening the window will exhaust model capacity. A sampling approach can be used to pick samples from a broad window and these samples can then behave like a narrow window.\n\n\n--- I am not sure I fully understand Figure 1. Both 1a and 1b seem to have similar trends. Therefore, I am not sure about the exact pitch of Figure 1.\n\n--- Why should equivalent token information be maintained across time-series. It might be beneficial to have variable patch sizes based on the information. Such an approach is useful especially to take into account unexpected events (such as a storm that causes power failure).\n\n--- What is the ground truth and loss used for training the global prediction model ? Is it a time-series value? Does this mean that a time-series value (e.g., power load) is being predicted using generic date representations. I think that I might have misunderstood and perhaps a clarification would be useful.  \n\n--- Most of the datasets are slow moving (except perhaps the economics data). It would be beneficial to gauge the performance of the proposed approach on fast moving data that still has global and local characteristics. This would perhaps again bring into question if using a day as the patch size is beneficial.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity : This work is somewhat clear although there are some parts (such as those mentioned earlier) that could be made more clear\n\n\nQuality : The work proposes a well structured approach. However, there are several questions that come up. \n\n\nNovelty : This work is somewhat novel. It does use some known mechanisms but also discusses interesting ideas such as global predictions from date representations. The combined use in the overall framework is somewhat original.\n\n\nReproducibility : The manuscript claims that the code would be released soon. ",
            "summary_of_the_review": "Overall, this work seems to be in the correct direction for utilizing local and long term global information for time-series forecasting. A structured approach and empirical explorations are provided to strengthen the overall pitch. However, there are some questions that come up such as the use of a fixed size (day based) patch , variations in the type of data between the datasets, and some basic baselines. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1040/Reviewer_atRd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1040/Reviewer_atRd"
        ]
    },
    {
        "id": "Ae3qygNKF3",
        "original": null,
        "number": 2,
        "cdate": 1666510037188,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666510037188,
        "tmdate": 1670656045989,
        "tddate": null,
        "forum": "77aKxP46geN",
        "replyto": "77aKxP46geN",
        "invitation": "ICLR.cc/2023/Conference/Paper1040/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": " This paper claimed that the global pattern in time series is not well captured in recent time series forecasting methods because they infer the future by analyzing the part of past sub-series closest to the present. To better leverage the global pattern and deal with long term time series forecasting, this paper proposed the following ideas:\n  1. Split time series into patches to reduce token number and tackle long-term series forecasting.\n  2. Distill time representation from training set, propose DERT, a Transformer based date encoder to generate the representation specific to a certain date in the dataset.\n  3. Propose Dateformer, a time-modeling time series forecast framework based on Transformers, to combine the prediction from global time representations and local lookback window.\n \nThe paper's experiments show this method can achieve state-of-the-art results on multiple representative datasets. Especially for a long prediction window, this method can generate results far better than previous methods.\n\nThe contributions are two-fold:\n1. Proposed to leverage a date representation to capture the global trend of the dataset, and combine with prediction from local look back window.\n2. Created the `DERT` and `DateFormer` architecture to utilize the global information and achieved excellent empirical results, especially in long term forecasting.\n",
            "strength_and_weaknesses": "This paper proposed an interesting point of view that the current methods cannot well capture the global pattern in the training dataset, like the overall trend and long-term seasonality, etc. \nStrengths:\n1. Interesting idea of capturing the global trend of the dataset using time representation and pretrain over the training set.\n2. The example given is very informative -- while local trends of the time series are very similar, the actual value of the time series is increasing each year. The figures are also well generated, very helpful in understanding the overall architecture.\n3. Strong empirical results, showing improvements in various lengths of prediction, especially in long-term forecasting.\n4. Interesting ablation study, demonstrating the date representation could even be transferable.\n\nThe paper could be better with the following questions resolved:\n1. While `day` could be a very useful patch size, have you explored other patch sizes and how does the performance change with patch size?\n2. It's interesting that global trend of time series can be captured via pretraining the `DERT` encoder and generate time representation for date. I'm wondering if the distance between the training dataset and the test dataset can impact the final accuracy. For example, if the training dataset includes data in 2013, will the testset sample in 2014 generally perform better than 2015?\n3. How does the inference time / memory occupation of the method compare to previous baselines?\n4. It appears not clear to me what other baselines' remainder of ground truth minus prediction looks like, compared to Figure 6(d). The claim of getting \"white noise\" seems not supported well for me. Can you please add figures for baselines?\n5. This method seems to rely on date representation, in that case, does it work for different scales of time series, for example millisecond level / month or year level time series forcasting? Does changing to a different patch size help?\n\nSome minor issues:\n1. Section 1, paragraph 2, change `how do` to `how to`.\n2. Page 2, first line, ditto, change `how do` to `how to`.\n3. Page 3, first paragraph, this is talking about covariates, seems a bit unnecessary, can be mentioned in the appendix.\n4. Section 3, paragraph 1, line 2, change `time series inherent characteristics` to `time series' inherent characteristics`\n5. Figure 3 is taking a huge space in page 4 but it seems simple to describe or demonstrate in a smaller figure.\n6. Figure 5 is a bit chaotic in describing how the autoregressive results are combined with global prediction -- the figure can be better organized to show the work flow. \n7. Table 2, better explain what the bolded results mean.\n8. Second 4, implementation details, change `There are 1 layer` to `There is 1 layer`.\n",
            "clarity,_quality,_novelty_and_reproducibility": "  This paper is written in OK quality and clarity. The figures and examples are very helpful in understanding the method proposed and the idea behind. However, some of the sections are not well organized, for example the `Related Work` section is in Appendix while experiments' implementation details are mentioned as a long paragraph in the main article. `DERT` seems to be a very important architecture in capturing data representation but was not mentioned in the introduction and it is a bit hard to connect to the later `Dateformer` architecture without checking back and forth. Other questions are proposed in the previous `Strength And Weaknesses` part. \n\tOn the other hand, I think it's a relatively interesting and novel idea to gather global information from the training set and improve prediction accuracy in long term time series forecasting. Most of the previous methods learn the global trend only implicitly by training with short time series intercepted from the training set, and use date/time related information as covariates to construct point-wise inputs. Learning date representation globally can better capture long term patterns.\n  \n",
            "summary_of_the_review": "Overall, this is a paper that proposed a date representation to better capture global trends in the training dataset. The idea is very interesting and empirical results show promising impact, especially in long term series forecasting. The paper is OK in writing and the idea is presented clearly. Ablation studies show that the global prediction is very critical in the final results and local residuals are helpful in short term prediction. Even though, there're a few incoherent parts in writing and some limitations due to the nature of date representation, this paper is meaningful in understanding and leveraging the global trend of time series. I recommend this paper to be accepted.\n  \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1040/Reviewer_Q1u4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1040/Reviewer_Q1u4"
        ]
    },
    {
        "id": "hcs_nqBQ_D",
        "original": null,
        "number": 3,
        "cdate": 1666639423454,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639423454,
        "tmdate": 1670669441237,
        "tddate": null,
        "forum": "77aKxP46geN",
        "replyto": "77aKxP46geN",
        "invitation": "ICLR.cc/2023/Conference/Paper1040/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces the Dateformer \u2013 a new transformer-based architecture for time series forecasting which combines embeddings for calendar seasonality along with autoregressive inputs using specialised components for each.",
            "strength_and_weaknesses": "Strengths\n---\nSpecialised components to encode seasonal relationships are relatively underexplored, make the idea behind fusing both temporal as well as seasonal relationships an interesting proposition.\n\nWeaknesses\n---\n\nHowever, there are several key issues associated with the paper that make it difficult to recommend for acceptance in its current form:\n\n1.\tThe paper is presented in a very unclear manner \u2013 with many ideas presented only in high-level terms and without explanation or citation, which makes detailed evaluation difficult. See section below for more.\n\n2.\tLack of hyperparameter optimisation \u2013 all benchmarks and the dateformer are calibrated using default settings, which makes it difficult to determine whether underperformance is due to improper hyperparams. This is particularly the case for time series data, where optimal hyperparams can vary wildly between datasets?\n\n3.\tThe two-step training process for Dateformer greatly increases the training time versus other benchmarks \u2013 can this be combined to be made end-to-end?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very poorly written, which make it difficult to understand the motivations, structure, training and value add of the proposed model. Couple of questions below for instance:\n\n1. What is local/global information here? In which datasets can they be found? \n\n2. What does patch-wise processing mean concretely? Can this be shown in an architectural diagram?\n\n3. What does it mean to predict the daily mean? Is this the MSE of the average daily value of the observation, or simply the aggregate MSE per day?\n\n4. Why the assumption that daily seasonal effects are dominant? One can construct multiple counter examples where daily effects do not apply (e.g. intraday traffic loads would depend on time of day). Does the model perform ONLY when seasonal effects dominate and are not applicable to general time series datasets?\n\n5. What datasets are being used in the attention patterns for the ablation analysis of E.2.1? This is not stated at all.\n\n6. Do the benchmark models get access to the same date/calendar inputs as Dateformer?\n",
            "summary_of_the_review": "Given the difficulties in evaluating the paper, I would suggest that the authors improve clarity of presentation prior to submission. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1040/Reviewer_Xz1K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1040/Reviewer_Xz1K"
        ]
    },
    {
        "id": "mt0iZU-kIun",
        "original": null,
        "number": 4,
        "cdate": 1666730722311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666730722311,
        "tmdate": 1666730722311,
        "tddate": null,
        "forum": "77aKxP46geN",
        "replyto": "77aKxP46geN",
        "invitation": "ICLR.cc/2023/Conference/Paper1040/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a novel framework that tackles the time series forecasting task by distilling the temporal pattern in the date presentations.  ",
            "strength_and_weaknesses": "This paper proposed a novel framework that tackles the time series forecasting task by distilling the temporal pattern in the date presentations.  \n\nStrengths\n1. SOTA methods are compared in the paper. \n2. Comprehensive experiments are conducted.\n\nWeaknesses:\n1. The presentation of the model part is difficult to follow. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposed an interesting and novel idea of learning a date representation for global patterns in time series. However, the readability of the model part needs to be further improved. Some implementation details are shared. And the authors claim that the code will also be shared in the future. ",
            "summary_of_the_review": "In this paper, the authors proposed an interesting framework that uses date representation as a container to carry the global pattern contained in the time series. Detailed comments are listed below:\n1. The connection between different model components and each component's functionality is not clearly addressed. \n2. What data is used to calculate equation 1 and the pretrained loss? Do the authors use time series in the training dataset? If so, is the time series for the target day used? Or time series of all time stamps involved in the date-embedding sequence? Is the purpose of DERT to acquire a date representation by building a connection between the training dataset time series and the static date embedding? Can the authors clarify that?\n3. The global prediction component is difficult to follow. First, global prediction is drawn to present a learned global pattern. However, this component feeds on the G copies of one day's presentation. So why these G copies can present the global pattern of the entire training set series? Second, what ground truth is used to train this global prediction component separately? Third, can the authors provide a clear definition of the \"global pattern\" or some examples?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1040/Reviewer_VVpa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1040/Reviewer_VVpa"
        ]
    }
]