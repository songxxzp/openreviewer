[
    {
        "id": "xFr_uO9ej35",
        "original": null,
        "number": 1,
        "cdate": 1666027602488,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666027602488,
        "tmdate": 1670863399777,
        "tddate": null,
        "forum": "EMvG1Jdhw_8",
        "replyto": "EMvG1Jdhw_8",
        "invitation": "ICLR.cc/2023/Conference/Paper2864/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the problem of disentangling representations, where we aim to learn representations that have independent features. Authors start with the common learning objective that encourages disentanglement, the Total Correlation (TC), and hypothesize that an important disadvantage of this objective is that it requires an estimate of the latent density. This density is difficult to compute for high-dimensional representations due to the curse of dimensionality. Authors propose an alternative objective, Excess Entropy Power (EEP), that combines TC with Dual Total Correlation (DTC), and can be computed using only the estimates of _conditional_ densities in the latent space. Such conditional densities are often easier to estimate, as also shown by authors in a toy experiment. Authors run experiments on two datasets with known latent factors, observing that EEP improves upon strong VAE-based baselines.",
            "strength_and_weaknesses": "Strengths:\n- The observation that TC depends on the quality of the full density estimate is a keen one, and the solution based on combining TC with DTC is non-trivial and novel.\n- The proposed method is introduced in great detail. The mathematical notation is clear and consistent.\n- Experiments use strong baselines: $\\beta$-VAE and its variants. The proposed method consistently outperforms these baselines.\n- Empirical studies of the role of hyperparameters $\\sigma$ and $\\lambda$.\n- A good critical discussion of the results, as well as the limitations of the method.\n\nWeaknesses:\n- Related work limited to a single paragraph in the introduction. I would've liked to see a broader discussion of disentanglement methods, representation learning methods, and density estimation methods. For density estimation, especially methods that model the density as a sequence of conditional distributions (autoregressive density estimators).\n- The KL divergence values are sometimes _negative_ in Figure 1(b): how is this possible?\n- Several choices in the paper are not clearly motivated, including the density ratio estimation as the density estimation method, the loss weighting scheme in equation 3, and the autoencoder with Gaussian noise as the representation learning method.\n- Density ratio estimation is characterized several times as \"non-parametric\", even though we're parametrizing the density (albeit implicitly) via the discriminator neural net.\n- I am not convinced we're comparing apples to apples in Figure 1: in (a) we're estimating an $m$-dimensional density, while in (b) we're estimating a 1-dimensional density. Estimating the $m$-dimensional density as a series of conditional densities in (b) would be more convincing.\n- Authors do discuss that the method is computationally expensive (need to train $m$ discriminators), but there are no runtime metrics reported in the experiments section.\n\nQuestions:\n- Have you tried using GCAE as a generative model (as you suggest in the discussion)?\n- Do you have intuition for why in Figure 4, (b) part especially, the MIG is going down for $\\alpha > 0.2$? Is it because we're \"over-disentangling\", and the model struggles to capture any info about the true factors?\n\nMinor:\n- Citation style: consider making use of `\\citep` and `\\citet`.\n- Define $m$ in the caption of Figure 1.\n- A figure on page 6 has no caption. It is also not clear what the different dots that have the same color represent.\n- Define $h$ in the first equation of section 3.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the manuscript could be improved (see weaknesses above). To the best of my knowledge, the method is novel, albeit it could be related better to prior work. Reproducibility is good: authors provide the details of their training procedure.",
            "summary_of_the_review": "The proposed method solves an important problem, is novel, and is evaluated well. However, several choices in the paper are not very well motivated, and the clarity and quality of the manuscript could be improved. As it stands, I consider the paper to be below the bar for acceptance, but would be willing to reconsider if the authors addressed the points raised above.\n\n---\n\nAuthors have responded to some of my points, especially in regards to motivating the choices in the paper, and I have increased my score as a result. I believe the paper can be made stronger still by better connecting to methods in density estimation and studying the pros and cons of autoregressive density estimation in this context, but I'd now support it getting accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2864/Reviewer_qWQn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2864/Reviewer_qWQn"
        ]
    },
    {
        "id": "sc5I-R0PYzN",
        "original": null,
        "number": 2,
        "cdate": 1666635093106,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635093106,
        "tmdate": 1666635093106,
        "tddate": null,
        "forum": "EMvG1Jdhw_8",
        "replyto": "EMvG1Jdhw_8",
        "invitation": "ICLR.cc/2023/Conference/Paper2864/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose learning disentangled representation via regularizing Gaussian Channel Autoencoder (GCAE) with Excess Entropy Power (EEP) loss. The authors claim their proposed method can achieve more reliable and high-performing disentanglement.\nTheir methods are largely motivated by the curse of dimensionality difficulty issue of the FactorVAE, one of the leading VAE-based disentangling approaches.  FactorVAE modifies beta-VAE objective by specifically penalizing the dependencies between the latent dimensions towards benefiting the Total Correlation (TC). The authors (of this paper) propose to use Dual Total Correlation (DTC) towards avoiding the curse of dimensionality.\n\nThe authors examined their proposed approaches on two widely used datasets using one metric \u2013 Mutual Information Gap (MIG). Experiments show that their approaches achieve (in average) higher MIG compared with other three leading VAE-based disentanglement approaches. \n",
            "strength_and_weaknesses": "The strengths include but not limited to:\n\n-- The proposed methods are well-motivated. The authors performed preliminary experiments to show the effect of curse of dimensionality for non-parametric density estimation, which is illustrated in Figure one. Based on this fact, the authors analyze the TC and DTC, and show the merits of DTC.\n\n-- The proposed method does achieve higher MIG compared with a few leading VAE-based approaches.\n\n\nThe weaknesses include:\n\n-- There are a lot of quantitative and qualitative ways for evaluating disentangled representations. For quantitative evaluation, there are a few widely used metrics besides MIG, including explicitness (Ridgeway & Mozer, 2018), beta score (Higgins et al., 2017), SAP (Kumar et al., 2018) and DCI disentanglement (Eastwood & Williams 2018).  Typically, previous works (on VAE-based disentanglement) would try to show their methods are consistently better using multiple metrics.\n\n-- Besides the three leading approaches (by 2018) mentioned by the authors, there are also some important works missed by the authors. For example, DIP-VAE (Kumar et al., 2017) and CCI-VAE (Burgress et al., 2017). More importantly, latest works on VAE-based disentangled representation learning has already realized the importance of using inductive biases. A recent work published in ICML (Mita et al., 2021) employs a particular form of factorized prior (also conditionally depends on auxiliary variables) towards learning an identifiable model with theoretical guarantees on disentanglement. Te authors could try to compare with some more (especially recent ones) important works using more datasets.\n\n-- The preliminary experiments do show that higher m may cause difficulty in training/optimization for other works (especially Factor VAE), but the whole set of experiments in the paper are done under a single dimension m=10. It would be interesting to conduct more ablation studies to show the benefits of the proposed methods in terms of curse of dimensionality compared with FactorVAE and Beta VAE.\n\n-- Some comments on ablation studies: In Figure 3, Figure 4 and Figure 5, the best performance is achieved using sigma=0.3 (at least for Beamsythesis), did you further try sigma larger than 0.3? Also, in page 5, you mentioned the importance of adding Gaussian noise \u2013 it ensures that p(z) is continuous and finite with respect to uniform density. Did you try the scenario where sigma=0?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper\u2019s idea is interesting, while descriptions can be less clear at places\n\n\u2014 In the bottom of page 4, the authors wrote \u2018to extract a compressed, noise-resistant representation\u2019. Can you elaborate why the representation is noise-resistant?\n\n\u2014 Figure two and page 4 descriptions are not aligned. For example, in the figure 2, the encoder with parameter phi (lower case) would generate representation z_phi; In the main text description, you are using uppercase phi. More importantly, u is missing in Figure 2, and the figure 2 is not so informative. \n\n\u2014It\u2019s unclear to me if m<<n (in page 4) is always realistic.\n\n--Figure 3 caption is missing.\n",
            "summary_of_the_review": "Overall, the method is novel and demonstrates its effectiveness on learning disentangled representations. However, I do have some concerns/comments as mentioned in my review. I\u2019m slightly not inclined at this moment, but am willing to discuss with authors and change my views accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2864/Reviewer_cTbH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2864/Reviewer_cTbH"
        ]
    },
    {
        "id": "qw962vU7yl",
        "original": null,
        "number": 3,
        "cdate": 1666646747713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646747713,
        "tmdate": 1669094160565,
        "tddate": null,
        "forum": "EMvG1Jdhw_8",
        "replyto": "EMvG1Jdhw_8",
        "invitation": "ICLR.cc/2023/Conference/Paper2864/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "To address the reliability issue of learning independent representations, the paper proposes to regularize a noisy autoencoder with a new regularization term based on dual total correlation. This results in a new Gaussian Channel Autoencoder, which does not need to estimate the joint distribution of the latent representation and achieves higher disentanglement scores.",
            "strength_and_weaknesses": "Strength:\n\n+ It is interesting to use the dual total correlation instead of total correlation as the metric for disentangle representation learning. The metric circumvents the curse of dimensionality of estimating the joint distribution of latent representations.\n\n+ The paper is well organized and easy to follow. The motivation is clear.\n\nWeakness\n\n- The paper claims the method can achieve reliable and high-performing disentanglement outcomes. It seems that only the \"reliable\" part is supported by higher disentanglement scores from the experiment. What is the definition of \"high-performing\" and how it is supported by the paper?\n\n- The experimental results are not convincing. As stated in the paper that \"there currently aren\u2019t good unsupervised indicators of disentanglement\", why only the Mutual Information Gap (MIG) is used as the disentanglement score? How about Z-Diff score [1], SAP score [2], and Factor score [3]?\n\n[1] beta-vae: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017\n\n[2]  Variational inference of disentangled latent concepts from unlabeled observations. In ICLR, 2018\n\n[3]  Disentangling by factorising. In ICML, 2018.\n\n- Some notations are confusing. For example, $DTC(z) = DTC_i(z)$ in Eq. (1). What do $z_{\\forall j\\neq i}$ and $z_{k\\neq j}$ mean and what are their difference? I don't follow  the discussion from $I(z_i; z_{\\forall j\\neq I})$ in Eq.(2) to $I(z_i;z_j)$.\n\n- The motivation for the major loss term in Eq. (3) is not clear. What is the definition of feature-scale dependent term and why it is helpful?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Not very clear. The paper has confusing notations. The proposed method is not well explained.\n\nQuality: Some claims are not supported. Experiments are not convincing.\n\nNovelty: The paper contributes some new ideas.\n\nReproducibility: Good. Key details of the experiments are provided.",
            "summary_of_the_review": "The paper contributes some new ideas to disentangled representation learning. However, some claims are not well supported and the experiments are weak. The writing could also be improved by clearly defining the notations and clarifying some confusing terms.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2864/Reviewer_atH7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2864/Reviewer_atH7"
        ]
    },
    {
        "id": "6rU6Mh5D209",
        "original": null,
        "number": 4,
        "cdate": 1666725558594,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725558594,
        "tmdate": 1670948167284,
        "tddate": null,
        "forum": "EMvG1Jdhw_8",
        "replyto": "EMvG1Jdhw_8",
        "invitation": "ICLR.cc/2023/Conference/Paper2864/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper is about learning disentanglement using scalable non-parametric density estimation. The author proposes a dual total correlation (DTC) metric for disentangling that divides the joint distribution into many low-dimensional conditional distributions. The authors conduct extensive experiments to show the superiority of their approach against state-of-the-art baselines.\n",
            "strength_and_weaknesses": "Strength\n\n1) The analysis is shown in section 3, which derives the DTC and its comparison against total correlation. DTC in eqn 1. removes the requirement to compute p(z). DTC requires computing the conditional entropies for disentanglement.\n2) Propose a new \"Gaussian Channel Autoencoder\" (GCAE). As the name suggests, gaussian noise with variance sigma^2 is added to smooth the representations. And in Sec 2, the authors have detailed how they use the discriminator-based method, which applies the density-ratio trick and the Radon-Nikodym theorem to estimate the density of samples from an unknown distribution.\n3) The authors conduct experiments on Beamsynthesis (time series dataset) and dsprites (synthetic images) to show GCAE is better than previous baselines. Fig 3a and 3b show the scatter plot of GCAE for summed information loss against mean squared error (MSE) and mutual information gap (MIG). GCAE performs better than baseline models for Beamsynthesis, while in the case of dsprites it does better only for a small range of lambda (hyperparameter of summed information loss).\n\n\n\nWeakness\n\n1) The novelty of the paper is limited. The authors claim that using DTC  removes the constraint of estimating p(z), which is difficult compared to the conditional distribution. However, computing \"m\" conditional distributions needs \"m\" discriminators, which are sensitive to hyperparameters. It also requires a lot of data and time.\n2) The authors use only the MIG metric to show disentanglement. The authors should have compared against other metrics such as DCI, SAP, etc. Please refer to \"Measuring Disentanglement: A Review of Metrics\" to understand why there is not one disentangling metric and each metric explores different aspects.\n3) The datasets used for the experiments are limited. The results work well for Beamsynthesis, not for dsprites. It can be seen in figure 4a and 4b. The scatter plot in figure 3b for beam synthesis shows a negative correlation coefficient of -0.823. There is no mention of dsprites.\n4) No qualitative results were shown for dsprites and no results on the latent traversal on these z axes. Since the authors chose m=10 as the latent dimension, and dsprites has five factors of variation, the qualitative result should have helped understand how the disentanglement happened.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work needs more finetuning and structuring. \nExample:\n1) Page 6: \"The tightly grouped samples in the lower right of the plot correspond with lambda = 0, and incorporating any lambda > 0 leads to a decrease in L_sumI and increase in MSE. As lambda is increased further the MSE increases only slightly as the average L_sumI decreases significantly.\" This line is unclear in context with lambda and figure 3a.\n2) Page6: \"Figure 3b plots the relationship between end-of-training L_\u03a3I values with MIG evaluation scores for both Beamsynthesis and dSprites\". But in that figure square box representing dsprites as shown in fig 3a is missing.\n3) In eq1, the authors have mentioned DTC(z) = DTC_i(z); I think a few lines describing why that is necessary for the readers.\n",
            "summary_of_the_review": "\nOverall, the paper seems to be a work in progress, and there are many areas where it needs further improvement. Currently, the work has limited novelty and needs more experiments with results to justify the made claims.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2864/Reviewer_d23T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2864/Reviewer_d23T"
        ]
    }
]