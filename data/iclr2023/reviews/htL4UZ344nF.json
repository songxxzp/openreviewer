[
    {
        "id": "0Ao-czB5XAO",
        "original": null,
        "number": 1,
        "cdate": 1666639634186,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639634186,
        "tmdate": 1666639634186,
        "tddate": null,
        "forum": "htL4UZ344nF",
        "replyto": "htL4UZ344nF",
        "invitation": "ICLR.cc/2023/Conference/Paper6429/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work studies the impact of various sub-tokenization methods for neural language models trained on source code. Across a range of tasks and datasets, it demonstrates a number of actionable insights, such as that UnigramLM trained models perform better than BPE ones, and that certain types of tokenization (especially ones that fuse tokens across whitespace and newlines) tend to yield poorer downstream task performance in many settings, while others can be safely used to achieve both roughly comparable results while using fewer tokens to represent the same sized input.",
            "strength_and_weaknesses": "I reviewed a previous version of this paper that was similar to the current submission. As in my review then, I maintain that this is an interesting and novel paper. While it is perhaps more empirical in nature than most work published in ML venues, it provides many actionable insights and highlights the ways in which modeling source code is distinct from natural languages.\n\nGiven that the authors addressed my comments on the prior version, I have no significant weaknesses to note, other than, perhaps, that the model (PLBART) that all the results are based on is relatively small by current standards (two 6-layer Transformers). Given that this already required nearly 10K GPU hours due to the breadth of tasks and settings the model was trained for, I do not fault the authors.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. The results are portrayed quite clearly. For the most part, the confidence intervals are rather wide and the results somewhat variable from one task to another, so that some of the findings do not readily permit any interpretation with statistical significance (e.g., Fig. 5). It may be worth emphasizing this in the discussion.",
            "summary_of_the_review": "This work presents actionable insights on a timely topic. It is rather empirical in nature, but comprehensive and well written. It would be good to publish it.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6429/Reviewer_het6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6429/Reviewer_het6"
        ]
    },
    {
        "id": "hiD2Gj0pdjw",
        "original": null,
        "number": 2,
        "cdate": 1666665773555,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665773555,
        "tmdate": 1666665773555,
        "tddate": null,
        "forum": "htL4UZ344nF",
        "replyto": "htL4UZ344nF",
        "invitation": "ICLR.cc/2023/Conference/Paper6429/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper implements and evaluates several tokenization schemes for pretraining large language models for coding tasks. The paper considers BPE vs UnigramLM algorithms, different vocabulary sizes, and different levels of composition allowed in tokens. Then they pre-trained PLBART-like models with these different tokenizations and compare results on several downstream tasks including code translation, code summary and code generation.  \n",
            "strength_and_weaknesses": "With the recent increase in pretrained large language models for code, investigating the effect of the different tokenization schemes is an important problem. This paper takes a step in that direction by thoroughly experimenting with different combinations of the design space and drawing useful insights. \n\nAlthough the results have a huge variance and does not really show one choice of tokenization is fully superior over another, I commend the authors for the effort it took to do this thorough evaluation and believe, the results will be useful for the people in this community working on similar areas. \n\nOne concern I have is that all the evaluation was done with a single model, PLBART. And so, it is not clear if these insights drawn in this paper would apply to other kinds of models/sizes and other coding tasks. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well-written, but the presentation of the results could be a bit more precise in several places. For example, the abstract says that the proposed approach reduces average length by 17-40% without downstream performance drop. But 40% reduction is achieved with level 4, for which there is a significant performance drop. There are several such imprecise statements through out the paper, which makes the paper hard to read. \n\nIn figure 2, please use the same scale in the x-axis for the all the plots for easy comparison.\n\nAs an additional result, it would be useful to look at how does training and inference times change with the different tokenization schemes. \n\n",
            "summary_of_the_review": "The experiment setup and evaluation is thorough and the paper presents previously understudied insights for the community, although it is not totally clear how the insights extend to other kinds of model architectures. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6429/Reviewer_SjVD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6429/Reviewer_SjVD"
        ]
    },
    {
        "id": "AV6eduMUqx",
        "original": null,
        "number": 3,
        "cdate": 1666754492742,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666754492742,
        "tmdate": 1666754492742,
        "tddate": null,
        "forum": "htL4UZ344nF",
        "replyto": "htL4UZ344nF",
        "invitation": "ICLR.cc/2023/Conference/Paper6429/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Subtokenization is one of the unsung heroes of application of deep learning to code. The paper fills the gap by systematically investigating the results of several subtokenization approaches: BPE, UnigramLM, punctuation combination, native, and systematically reports statistics.",
            "strength_and_weaknesses": "(+) Studies an important under-studied problem.\n\n(+) Experimental results are convincing\n\n(-) It is not clear WordPiece is not included as several code-related model use it.\n\n(-) While the numbers are interesting, the recommendation (using punctuation combination tokenization) appears somewhat obvious with hindsight \u2013 given the punctuation-heavy syntax of programming languages.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear though the presentation can be more insightful and inspiring. The paper has some empirical value.",
            "summary_of_the_review": "Subtokenization is an important aspect of models for code. The paper systematically explores the area and presents numbers which mostly confirm the the implicit assumptions of current models. The recommendation of punctuation combination approach is helpful information. Not including wordpiece is limiting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6429/Reviewer_RLnm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6429/Reviewer_RLnm"
        ]
    },
    {
        "id": "OibP-TVvz4",
        "original": null,
        "number": 4,
        "cdate": 1667367617534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667367617534,
        "tmdate": 1667367940088,
        "tddate": null,
        "forum": "htL4UZ344nF",
        "replyto": "htL4UZ344nF",
        "invitation": "ICLR.cc/2023/Conference/Paper6429/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors of this paper propose various subtokenization strategies affecting input string length in a way so as to improve the efficiency of LLMs when trained on source code and when applied to downstream tasks such as code generation, code summarization and code clone detection.\nAuthors propose a strategy the restricts vocabulary size as well as compresses lengths of inputs without affecting performance via several combination strategies in the UnigramLM and BPE tokenization schemas.\nThe main contribution of this paper is to study the impact of tokenization for source code based applications.",
            "strength_and_weaknesses": "Strengths\n- this paper studies an interesting problem.\n- authors consider code from multiple programming languages.\n\nWeaknesses\n- this paper is a little hard to follow at times, writing could be clear and so could the organization of sections in the paper.\n- the paper primarily compares UnigramLM and BPE, with the authors claiming in the motivations that such tokenizers have demonstrated improvements in NL tasks. However, the authors fail to provide qualitative examples explaining why similar strategies achieve improvements on source code.\n- while authors use the Sentencepiece vocabulary, they do not compare against Sentencepiece. I am not sure why the comparisons were restricted to UnigramLM and BPE alone.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is hard to read and follow in some instances.\nWhile it is known that vocabulary size and combination steps in BPE style algorithms impact the performance of a LM, it is hard to fully understand why it is the case for source code and exactly what gap are these general NL techniques addressing on source code.\n\nExperiments on their own seem easy to reproduce.",
            "summary_of_the_review": "While the problem is interesting, the presented analysis fails to provide insight into how proposed subtokenization strategies exactly help with source code.\nFurthermore, I find it a little puzzling why comparisons against Wordpiece were omitted. Similarly while the point was to demonstrate the strengths of subword tokenizers, commentary on how they compare against other tokenization schema, i.e whitespace etc would be nice to see given that the vocabulary of source code is fairly limited.\nOn the whole while I see the experiments as thorough I am still unsure as to why these results need to be considered as novel.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6429/Reviewer_kR7n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6429/Reviewer_kR7n"
        ]
    }
]