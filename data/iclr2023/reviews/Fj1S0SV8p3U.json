[
    {
        "id": "TI_ff43m4fS",
        "original": null,
        "number": 1,
        "cdate": 1666697348893,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697348893,
        "tmdate": 1666697348893,
        "tddate": null,
        "forum": "Fj1S0SV8p3U",
        "replyto": "Fj1S0SV8p3U",
        "invitation": "ICLR.cc/2023/Conference/Paper5535/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents AugCL, a method for learning to generalize to unseen environments in pixel- based RL using strong visual augmentations. Training is split into two phases of using weak and strong augmentations respectively to leverage a curriculum. The authors use a separate critic network for each phase and demonstrate how this disentangelment induces superior generalization. They further introduce an image augmentation method called Splice, and show how the combination of this with their method achieves state-of-the-art performance on the DMC-GB benchmark.",
            "strength_and_weaknesses": "> Strengths\n\nThe paper is well-written, well-structured and easy to follow.\nThe paper achieves SOTA performance on 13/15 environments of a popular benchmark compared to previous methods. The authors prove the relevance of the key aspects of their proposed methods by running an ablation study.\n\n> Weaknesses\n\nThe authors conclude the value of M (timesteps for the1st augemntation phase) is an open question. I believe that the determining of this value is rather crucial, as the whole notion of curriculum (and hence the core method) relies on this parameter. The authors do report experimenting with 0 and 100,000 as values for M, which yielded poor results. I would like to see more analysis on how to select M and what effect this value beholds in other environments.\nThe paper talks about addressing how to effectively incorporate strong augmentations such as random convolutions and overlays, but then mainly focuses on their own proposed augmentation (Splice) in their experiments. Results using the Overlay augmentation are indeed presented, but 1) originate from a different dataset than which was used in SVEA and SODA, and 2) are not superior in all environments compared to previous works. It can thus be argued, that the proposed method AugCL does not necessarily surpass previous methods, but the new augmentation method (Splice) is the reason for improved performance. It would be interesting to see how much the proposed method outperforms previous methods using the same augmentations.\nThe contribution of the new augmentation method Splice is marginal, as it is not applicable in most other RL benchmarks. Since Splice is tailored for the DMC suite, it cannot be considered as a general augmentation method, which is why I would consider it a contribution of low significancy.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is novel, but its impact is questionable, which reduces the overall quality of the paper.\nThe authors have provided their code with instructions, to be used in combination with the DMC benchmark repository, which significantly contributes to reproducibility.\nThe paper is written in a very clear and concise manner.",
            "summary_of_the_review": "The paper utilises a very trivial method to marginally improve generalization of pixel-based RL on one benchmark. The contribution is novel, however, it is not empirically proved to be beneficial in other benchmarks, due to which I don't find it to be particularly significant. Further, it is dubious to label the simplistic two-phased learning as a curriculum.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5535/Reviewer_QnFd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5535/Reviewer_QnFd"
        ]
    },
    {
        "id": "aH5TEfgFlRg",
        "original": null,
        "number": 2,
        "cdate": 1666708089123,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666708089123,
        "tmdate": 1670269570656,
        "tddate": null,
        "forum": "Fj1S0SV8p3U",
        "replyto": "Fj1S0SV8p3U",
        "invitation": "ICLR.cc/2023/Conference/Paper5535/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThis paper proposes to decouple critics in an actor-critic scenario, where one critic is trained on so-called weak pixel-based augmentations and the other on strong augmentations. The argument is that while weak augmentations help stabilize training within an MDP, strong augmentations help generalize to a larger set of variations on that MDP but destabilize training.\n\n- This induces four parameterized functions, $\\pi_\\psi$, $Q_{\\phi^W}$, $Q_{\\phi^{target}}$, and $Q_{\\phi^S}$ ($W$ and $S$ standing for weak and strong).\n- A new strong augmentation is introduced, called Splice, which pastes in natural image distractors forcing the model to pay attention to the relevant parts of the image.\n- Training is separated into two phases. The method introduces a hyperparameter $M$, the number of steps for which training is in the first phase.\n  - For $M$ steps, $\\pi_\\psi$ is trained on $Q_{\\phi^W}$, $Q_{\\phi^W}$ is bootstrapped to the EMA (of $\\phi^W$) $Q_{\\phi^{target}}$ with weak augmentations for both $\\pi$ and $Q$\n  - Then, $\\pi_\\psi$ is trained on $Q_{\\phi^S}$ with strong augmentations, $Q_{\\phi^W}$ and $Q_{\\phi^S}$ are bootstrapped to the EMA (of $\\phi^W$) $Q_{\\phi^{target}}$ with weak and strong augmentations respectively.\n- This method shows improvements on an array of environments from the DeepMind control benchmarks.\n",
            "strength_and_weaknesses": "The paper is relatively clear and well motivated. It introduces methods that make sense and motivates them well.\n\nI have some issues with the empirical evaluation of the proposed methods:\n- It seems that one of the the main drivers of performance wrt baselines is the augmentation used. Were any baselines trained with Splice?\n- As the authors point out, the role of $M$ is still unclear, but its role is a central part of the method. \n    - Here's a suggestion for a plot: run experiments with varying $M$s and display results with $M$ on the $x$ axis and the AUC or average reward after training on the $y$ axis for two settings, in-distribution and out-of-distribution MDPs.\n    - If what the authors posit is correct, with $M=0$ training should be too hard and not perform well on either MDPs, while with $M\\to \\infty$ (or simply $M>$total number of steps trained on) training should not generalize well to OOD MDPs but work well on in-distribution MDPs; and there should be a sweet spot in the middle where performance is ideal. Not only would this be a nice ablation, it would provide evidence that the _hypotheses_ that underlie this paper are correct.\n\nOther than that, I'm also a bit unsure that invoking curriculum learning is correct here. While you could argue this is _technically_ CL, with two phases we typically only say the the first phase is pretraining. A curriculum on the other hand usually implies a continuously changing \"difficulty level\" and data distribution adjustment, and one that is _adaptive_ to the learner's performance--here $M$ is a hyperparameter.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly clear, but its novelty is somewhat limited. Perhaps I am lacking context here but if I understand correctly training two separate critics for different augmentations is not new, although this particular setup may be an improvement over past work. In-pasting natural image distractors is also not new, although the specifics here may matter for performance.\n\nI believe I could reproduce this paper or at least its broad strokes relatively easily.\n\nNote on formatting: it seems all the non-inline citations are missing parentheses. Are you using the `\\citep{...}` command?\n",
            "summary_of_the_review": "While improvements in performance are always valuable, the scientific contribution here appears relatively minor to me. The proposed methods build on existing ones, but without deeply investigating them. This leaves us with higher numbers but not much new knowledge. \n\nAs is I'm on the fence about accepting this paper, but I think it could be a much stronger contribution with a proper empirical validation _of the hypotheses_.\n\nUpdate: the new results provide more clarity to this paper. In particular they test the hypothesis that $M$ matters, this is interesting in itself, but the other interesting aspect is that in terms of performance, it is the Splice augmentation that seems to offer most of the boost. I'm still leaning towards accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5535/Reviewer_iZtt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5535/Reviewer_iZtt"
        ]
    },
    {
        "id": "FYSKJZ_jN5",
        "original": null,
        "number": 3,
        "cdate": 1666750262639,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666750262639,
        "tmdate": 1666750262639,
        "tddate": null,
        "forum": "Fj1S0SV8p3U",
        "replyto": "Fj1S0SV8p3U",
        "invitation": "ICLR.cc/2023/Conference/Paper5535/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the problem of generalization in reinforcement learning. The authors seek to find algorithms for training a policy such that the policy will perform well on novel test-time environments. The authors propose a two-phase approach. In phase 1, the policy and value function are both trained with \"weak\" data augmentation. In phase 2, a second value function is trained in parallel with \"strong\" data augmentation and is used to update the policy. The method achieves SOTA performance on the Deep Mind Control Generalization Benchmark (DMCGB).",
            "strength_and_weaknesses": "The main strength of this paper is the empirical performance. Achieving the top performance on the DMCGB is certainly important. The method proposed in the paper beats baselines in all but 1 of the DMCGB tasks.\n\nHowever, I do think there are a number of ways the paper needs to improve. Currently, I think the paper feels a bit rushed and a few edits and ablations would increase the impact of the paper substantially. I've included a list of comments and edits below.\n\n- the authors are using the ICLR 2022 template\n\n- Section 3.1 should just be $R$? not $R_t$\n\n- Algorithm 1 - in lines 13 and 16, $L_\\pi$ should take as input a $\\phi$ as well to specify which Q function weights are being used in the equations in section 4.1 (I think in line 13, it should be $\\phi^S$ and in line 16 it should be $\\phi^W$?). I think this detail is important for understanding the algorithm.\n\n- add equation labels for the equations in section 4.1\n\n- at the top of page 5, I think the authors should reference a line different from \"line 1\"?\n\n- I don't think Figure 1 is particularly informative since the arrows don't communicate which parts of the loss each model is being used for. I would personally prefer if the authors wrote out explicitly the loss for the policy and critic(s) in phase 1 and phase 2. For instance, the weak critic and target critic are updated the same way in phase 1 and 2, the only thing that changes in phase 2 is we begin training the strong critic and the policy uses the strong critic in its update (I believe, although currently ambiguous in line 13 of Algorithm 1). Currently, it's not obvious to me looking at Figure 1 that the updates for two of the networks are unchanged in phase 2, but that information would help a lot for internalizing the author's method.\n\n- Table 2 and Table 3 - did the authors evaluate SODA and SVEA with splice? The authors should demonstrate that SODA and SVEA don't get the same performance boost as AugCL when switching from overlay to splice. Otherwise my sense is the main contribution of the paper is the use of splice augmentation, not the particular choice of training weak and strong critics in parallel which is what the authors emphasize in the method section.\n\n- page 9 - ideally the authors would sweep across 3-5 values of M to get a sense of how sensitive the algorithm is to choice of M",
            "clarity,_quality,_novelty_and_reproducibility": "I think given that the method in this paper beats baselines handily on the DMCGB, it's definitely an acceptable quality method. Tables 2 and 3 suggest to me that splice augmentation is the main reason that the method won the benchmark, not the particular choice of losses that the authors use for incorporating splice augmentation. Including SODA and SVEA with splice augmentation in the tables would go a long way in clarifying the importance of each of the authors' contributions. With respect to originality, the method seems very similar to SVEA to me.",
            "summary_of_the_review": "Currently, I feel this paper is somewhat difficult to read and the method has limited impact outside of the DMCGB. However, if the authors address my comments in the \"Strengths and Weaknesses\" section, I will consider increasing my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5535/Reviewer_3K31"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5535/Reviewer_3K31"
        ]
    },
    {
        "id": "2fxDrpCyikW",
        "original": null,
        "number": 4,
        "cdate": 1666972113832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666972113832,
        "tmdate": 1666972113832,
        "tddate": null,
        "forum": "Fj1S0SV8p3U",
        "replyto": "Fj1S0SV8p3U",
        "invitation": "ICLR.cc/2023/Conference/Paper5535/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors tackle the problem of generalization to novel visual environments in Reinforcement Learning.  The authors differentiate between weak augmentations (those that help with learning to solve the task in the same environment) and strong augmentations (those that hurt the training performance on the same environment, but may help generalization across domains).  The authors propose a curriculum where after M steps of only training on the weak augmentations, additional optimization on strong augmentations is included as well.  The authors also propose a data augmentation called Splice, where task-relevant information is cut out from the environment and pasted over a distracting background.",
            "strength_and_weaknesses": "A strength of this paper is in its simple, easy to grasp idea - this would definitely help it be applied out-of-the box to a variety of RL methods.\n\nA main weakness of this paper is in its novelty, which will be elucidated upon in the section below.  Furthermore, I sense that a major weakness of this paper is how restricted it is to the DMC task suite.  Firstly, the selection of weak and strong augmentations were predefined specifically for the DMC tasks (\u2018Known weak augmentations for all DMC tasks are: crop, translate, and shift\u2019).  This limits the application of the proposed model to other domains for which there is no prior knowledge about the weakness or strength of augmentations.  It would be interesting if the authors could automatically uncover such properties of elements in a proposed augmentation set automatically - however, as such, it looks like this approach was specifically designed for the DMC task suite rather than being general.\n\nAlso, the numerical results of the proposed AugCL do not appear to outperform the existing art (in particular, SVEA) on the listed tasks, with the exception of Walker, Walk.  Given how design decisions were specifically selected for this domain, I would have expected more substantial performance improvements.\n\nThe claim that AugCL enables generalization to unseen environments would also be greatly reinforced if it were demonstrated across true unseen visual domains and tasks, rather than remaining in the DeepMind Control suite.  I believe that given the claims of the paper, a more thorough set of experiments would be useful in demonstrating the power of their proposed curriculum.",
            "clarity,_quality,_novelty_and_reproducibility": "I suggest the authors double check that citations are performed correctly according to the ICLR style guidelines - reading the citations throughout the paper was jarring and disorienting due to the lack of differentiation within the sentences.\n\nI can identify no further issue with the clarity, quality, or reproducibility of this work.  In terms of the novelty, I believe the work is slightly novel; I believe that exploring a mix of strong and weak augmentations exists in prior art and the novelty of this work mostly revolves around this two-stage curriculum schedule.  In terms of the Splice data augmentation, I believe similar works have been done before [1, 2], where the agent and task-relevant features are extracted out and placed over a different distracting background.  There were no model or architectural improvements - the authors apply their curriculum on top of an existing SAC model.\n\n[1] Stone et al. \u201cThe Distracting Control Suite \u2013 A Challenging Benchmark for Reinforcement Learning from Pixels\u201d, 2021.\n\n[2] Hansen et al. \u201cStabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation\u201d, 2021.",
            "summary_of_the_review": "The proposed approach in this work is intuitive, and simple to apply to many current RL solutions.  However, I believe it suffers from explicit domain-specific information (distinguishing weak from strong augmentations specifically for DMC), as well as a lack of convincing experiments that fully showcase the strength of the model with respect to the claims.  I also believe the only substantial novel contribution in this work is the curriculum schedule (as the differentiation of weak vs strong augmentations is assumed, not learned/identified ad hoc per environment), which is limited in terms of its intellectual impact.  I therefore recommend that this paper undergo continued revisions before being considered for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5535/Reviewer_wrjJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5535/Reviewer_wrjJ"
        ]
    }
]