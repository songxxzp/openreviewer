[
    {
        "id": "mn6kkXlRxz",
        "original": null,
        "number": 1,
        "cdate": 1666579845818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579845818,
        "tmdate": 1666579845818,
        "tddate": null,
        "forum": "9OEW_t2uO4u",
        "replyto": "9OEW_t2uO4u",
        "invitation": "ICLR.cc/2023/Conference/Paper394/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to achieve disentangled, interpretable and controllable text-guided image manipulation with CLIP-based models. The authors propose CLIP projection-augmentation embedding (PAE) to replace the original multimodal embedding as optimizing target. PAE is simple and can be easily to injected into other CLIP-based text-guided image manipulation models. Experimental results on face datasets shows that it can disentangle face attribute and even control the degree of attribute in the generated results. ",
            "strength_and_weaknesses": "Strength:\n1.\tThis paper is well writing and easy-to-follow. \n2.\tThe analysis of embedding space well explains the motivation of the approach.\n3.\tThe proposed component (PAE) is reasonable and simple enough for re-implementation. It can be combined in other models.\n4.\tExperimental results show the effectiveness of the proposed PAE in text-guided face editing.\n\nWeakness:\n1.\tThe experiments are only conducted on face datasets which makes the title \u201ctext-guided image manipulation\u201d over claimed.\n2.\tBesides Gram-Schmidt and dimension reduction techniques for project, clustering is also a possible solution. While the authors have not mentioned about other alternative methods.\n3.\tThe method seems limited in several scenarios:\na)\tNon-facial images, such as animals, architecture or other objects. \nb)\tTexts that are not attributes. It seems that attribute is better to be mapped into subspace while other texts may be difficult. \nc)\tAttributes with overlaps. There are some attributes that affect multiple parts in the face. For example, \u201cblack hair and wearing glasses\u201d, \u201cAsian woman\u201d.\nd)\tAttributes that are not existed in training data. This also indicates the biasness of this method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and easy to follow. The proposed method is reasonable and novel in limited scenarios. Considering the simplicity of the method, the reproducibility should be high.\n",
            "summary_of_the_review": "Although I could recognize the simplicity and effectiveness of the method in single attribute facial editing, the method has limitation in more broader scenarios of text-guided image manipulation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "As discussed in Sec. 6, the attributes are limited in the training data. In other words, attributes that have not appeared in the dataset cannot be handled. This shows the biasness of the method. Moreover, the methods is limited to only certain scenarios as I showed in the weakness.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper394/Reviewer_62cf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper394/Reviewer_62cf"
        ]
    },
    {
        "id": "bxcBY7vOMKc",
        "original": null,
        "number": 2,
        "cdate": 1666671548468,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671548468,
        "tmdate": 1666671548468,
        "tddate": null,
        "forum": "9OEW_t2uO4u",
        "replyto": "9OEW_t2uO4u",
        "invitation": "ICLR.cc/2023/Conference/Paper394/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present an approach to edit images using text promp by projecting the image embedding to a subspace which is semantically meaningful to the concept being edited. The method integrates seamlessely into most popular CLIP based text driven editing approaches. State of the art quantitative results are shown for text based image editing on a dataset of faces.",
            "strength_and_weaknesses": "## Strengths\n\n1.  The authors propose a simple and elegant solution. The proposed approach is a general augmentation strategy that can be applied to a large class of text based editing approaches.   \n  \n2. The manuscript is well written with attention to detail. All the important components of the approach have been adequately explained\n\n3. Adequate treatment of the related literature has been provided to contextualize the proposed approach in relation to similar methods.\n\n3.  Code is provided to aid in reproducibility.\n\n5. The authors provide important insights regarding the discrete regions of space occupied by the text embedding and image embedding in the joint space. The solution accounts for this gap while influencing image space edits through text.\n\n6. The appendix provides much needed additional details regarding similarity of text and image embeddings and how to structure the projection and augmentation operation.\n\n## Weaknesses\n\n1. There are several simplifying assumptions made regarding (projection+augmentation) operation. Particularly,  \n \n&nbsp; &nbsp;&nbsp; &nbsp; a) Equation (2) and (3) assume that the all other attributes are orthogonal to the attributes captured by the basis of the corpus subspace. It is unclear how valid this assumption is in the general case. For instance, extending upon the examples demonstrated in the paper, mouth open->close and the emotion \"surprised\" may not truly be disentangled.  \n   \n&nbsp; &nbsp;&nbsp; &nbsp; b) The augmentation operation is akin to a linear operation in the subspace, again relying on the linearity assumption on the part of the space spanned by a particular attribute (say emotion). Is it not more likely that the space of emotion edits forms a nonlinear manifold, such that augmentation (or movement) in that subspace is not necessarily linear? There is some discussion about this in the appendix section, but a more detailed explanation of why the linearity assumption holds would be helpful.  \n\n&nbsp; &nbsp;&nbsp; &nbsp; c) The motivation for the augmentation step is unclear. Particularly, if $w$ is a projection of $e_I$ onto a subspace where all other attributes are disentangled why do we need to explicitly \"weaken\" components of $w$. \n\n&nbsp; &nbsp;&nbsp; &nbsp; d) A clearer explanation of what exactly equation (8) and (9) are doing would be insightful . Particularly, it seems like the $l_1$ norm of the $e_1$ in prompt-space is being shrunk and then a scaled version of $P(e_T)$ is being added back. Explanation regarding the function of this scaling term (or reference to works which employ a similar strategy to move in subspaces) would greatly improve readability of that section.  \n \n2. Do we need a different projection operation for every attribute to be controlled?\n4. Experiments are demonstrated on a limited dataset. The narrative would greatly benefit from showing more examples on a wider variety of datasets like (CUB/ Flowers), as it would highlight the generalizability of the approach.  \n\n&nbsp; &nbsp;&nbsp; &nbsp;\ta) Is the limited dataset an issue of annotated data?  \n\n&nbsp; &nbsp;&nbsp; &nbsp;  b) Are the limitation due to lack of pre-trained models for other datasets?\n\n5. A number of references to \"optimization\"  has been made. What exactly is the optimization process involved ? Since all the steps involved in both the projection and augmentation operators are deterministic and the pre-trained models being used are deterministic, what are the learnable/ optimizable parameters in the system.",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity:* The authors present their approach in a detailed manner and the narrative is meaningful. Although some parts of the approach section can be made clearer as indicated in the section above. Some of the design decisions made for the augmentation is unclear. \n\n*Quality:* Although authors demonstrate improvements quantitatively, the qualitative results demonstrated sometimes don't seem to show much change for certain attributes. The quality of writing, however, is informative and engaging.\n\n*Novelty:* The ideas themselves might not be entirely novel since they are leveraging basic ideas from linear algebra, but the insights provided are potentially helpful for textual editing of most latent space based generative models.\n\n*Reproducibility:* The equations, architecture and data are explained clear and aids in reproducibility. The code  is also provided to help reproduce the metrics reported in the manuscript. ",
            "summary_of_the_review": "The authors provide a simple and elegant method for text based editing by projecting CLIP based embeddings  onto \"meaningful\" subspaces. However, the work would greatly benefit from better motivation of the projection and augmentation strategy. Furthermore, the results are only provided on a single class of data. Providing on few other object classes would potentially help aid the narrative for generalizability. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper394/Reviewer_Qecw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper394/Reviewer_Qecw"
        ]
    },
    {
        "id": "R2YTJ-QRZ5",
        "original": null,
        "number": 3,
        "cdate": 1666694591228,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694591228,
        "tmdate": 1666694591228,
        "tddate": null,
        "forum": "9OEW_t2uO4u",
        "replyto": "9OEW_t2uO4u",
        "invitation": "ICLR.cc/2023/Conference/Paper394/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed CLIP-PAE, which projects and augments CLIP image embedding and text embedding in different subspaces, and uses embedding in subspace to optimize the cosine similarity for text-guided image manipulation. It's applied to different baseline methods and outperforms them in both quantitative and qualitative results.",
            "strength_and_weaknesses": "Strengths:\n1. I really appreciate the analysis in Section3. It contains intuitive visualization and a preliminary pilot study which provide an experimental view of why the original CLIP score doesn't work well in guiding image generation. \n2. The methodology, PAE, is novel. It subtly utilizes the relevant semantics in text to build basic vector subspace.\n3. The proposed subspace embedding can be generally used in many previous works that employ CLIP similarity for text-guided image manipulation. Also, the experimental results show that adding the proposed embedding can indeed improve the generation quality. \n\nWeaknesses:\n1. My concern in terms of methodology is that I still cannot get why the disentanglement is guaranteed in the proposed method, neither from theoretical proof nor intuitive understanding. If you have a look at Fig3 b, you can find the similarity score in the hairstyle subspace somehow also somehow increases. From the methodology, it does build the basic vectors by embeddings in the separate semantics but there might be some spurious/bias between selected semantics and other unwanted semantics, which is not specifically avoided.\n2. It's easy to find the relevant corpus of emotion or haircut subspace. But not all subspaces contain a clear definition of the relevant corpus. For example, for `Donald Trump`, as an example in StyleCLIP, it's hard to find a comprehensive and precise relevant corpus. \n3. The quality of the corpus matters for the proposed methods. However, it seems the corpus is manually added. There is no clear indicator for a good set of corpus until it's tried in the experiment and takes back the performance. Such a limitation hurts the generalization ability of the proposed method.\n4. In experiments:\n- From Fig5a, we can find with PAE, the unwanted semantics also change sometimes. For example, the color of the boy's hoodie changed to blue in StyleCLIP+PAE. So echoing what I mentioned before, entanglement seems still exist. It's not guaranteed that the samples are not cherry-picked. I wonder if there is any way we can quantitatively measure the disentanglement degree?\n- In Table 1, it's not in which \\alpha, the numbers are obtained. For the different metrics of same method, is the same \\alpha used?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and different variations of the proposed method are discussed. Quality and originality are good.",
            "summary_of_the_review": "Although some details about methodology and experiments, and limitations are not illustrated, the novelty, intriguing analysis, and strong performance make the paper above the bar from my point of view. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper394/Reviewer_WWDd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper394/Reviewer_WWDd"
        ]
    },
    {
        "id": "C_a7pT0QrU",
        "original": null,
        "number": 4,
        "cdate": 1666704263650,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704263650,
        "tmdate": 1666704263650,
        "tddate": null,
        "forum": "9OEW_t2uO4u",
        "replyto": "9OEW_t2uO4u",
        "invitation": "ICLR.cc/2023/Conference/Paper394/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper explores the limitation of using CLIP to build a joint space for text and image, where the disentanglement, interpretability, and controllability are hard to guarantee in text-guided image manipulation. To mitigate this problem, a PAE is proposed, which defines corpus subspaces spanned by relevant prompts to capture specific image characteristics. Experiments of adopting the proposed PAE in current CLIP-based method improve their performance qualitatively and quantitatively.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is well written and easy to follow.\n2. The proposed method can be adopted in most CLIP-based methods.\n\n\nWeaknesses:\n1. Authors only evaluate the limitation of CLIP on a face dataset, which is limited and might not be applied for a general dataset. \n2. Figure 1 only shows the distance between image features and original text embeddings, and image features and PAE text embeddings, respectively, which might not be enough to support the proposed problem of CLIP, because (1) it is a 2D visualization using PCA, which might not fully show the real distance in a high-dimensional space (2) the distance between them is hard to quantify the differences, without any reference, e.g., comparison with images from another categories.\n3. I am confused about why the proposed method allows a better disentanglement, and how to build the subspace? Do authors need to build multiple subspaces for different attributes, e.g., hair and expression?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper might lack sufficient details about the proposed method, which causes difficulties to reproduce the results.",
            "summary_of_the_review": "See above weaknesses. I am happy to raise my rating based on authors' responses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper394/Reviewer_DXMN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper394/Reviewer_DXMN"
        ]
    }
]