[
    {
        "id": "mJ80a-VM5Yk",
        "original": null,
        "number": 1,
        "cdate": 1666598534459,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598534459,
        "tmdate": 1666598534459,
        "tddate": null,
        "forum": "t7HIN3fUAUu",
        "replyto": "t7HIN3fUAUu",
        "invitation": "ICLR.cc/2023/Conference/Paper3610/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new graph neural architecture search pipeline named Reg-NAS. Specifically, the authors find that regression tasks are more reliable than classification tasks in estimating architecture performance and advocate that a regression self-supervised proxy task be used to estimate the architecture performance.",
            "strength_and_weaknesses": "Pros:  \n1. The study about the stability of using classification and regression tasks to estimate GNN architecture performance is interesting and useful, which can inspire future work in graph NAS.  \n2. The paper is clearly written and easy to follow.  \n  \nCons:   \nThough I find the paper studies an interesting perspective of graph NAS, the current experiments are highly insufficient with the following concerns:   \n1. I\u2019m concerned that only using SSL tasks to estimate the architecture performance may not work well for GNNs. Specifically, it is well known that the performance of architectures heavily relies on the downstream task, e.g., see [1] for an empirical study and [2] for a theoretical justification. For example, what if the SSL task is fundamentally different from the downstream tasks?  \n[1] Design space for graph neural networks, NeurIPS 2020.   \n[2] How neural networks extrapolate: from feedforward to graph neural networks, ICLR 2021.  \n  \n2. The experiments in Section 4 are not convincing enough. The authors only use two molecule datasets, neglecting that graph datasets cover a wide range of domains, e.g., social networks, citation networks, financial networks, etc. Different data sources can cause the graph to behave fundamentally differently. It is doubtful that the same conclusion holds in other areas.  \n\n3. The Spearman correlation coefficient and Kendall Tau shown in Table 3 are not very high, so I wonder whether they can support a good NAS method. Moreover, the authors do not show the quantitative results of the pipeline for the final downstream task, i.e., the real effectiveness of the proposed method is not validated. Besides, I would suggest the authors additionally show the correlation of the top-performing architectures instead of all architectures in the search space, considering that the ultimate goal of NAS is to find good architectures rather than evaluation all possible architectures.  \n\n4. The statement in Section 4.1.2 regarding using random labels seems to be in contradiction with NAS results in other domains, e.g., in computer vision [3]. The authors may need to explore deeper into why random labels do not work for graph tasks.  \n[3] Neural architecture search with random labels, CVPR 2021.   \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper proposes an interesting perspective for graph NAS, but the quality and the clarity of the paper are in a pretty immature state. \n\nThe reproducibility should be good, considering an anonymous link is provided. ",
            "summary_of_the_review": "See above",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3610/Reviewer_bV9M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3610/Reviewer_bV9M"
        ]
    },
    {
        "id": "j6UZFct_8JY",
        "original": null,
        "number": 2,
        "cdate": 1666627637855,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627637855,
        "tmdate": 1666627637855,
        "tddate": null,
        "forum": "t7HIN3fUAUu",
        "replyto": "t7HIN3fUAUu",
        "invitation": "ICLR.cc/2023/Conference/Paper3610/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper first proposes a proxy regression task, which is called REG-NAS, to replace the metric of the ground-truth task and evaluate the GNN architecture performance. The proxy regression task and its metric leading lead to higher ranking stability and faster search.",
            "strength_and_weaknesses": "### Strengths:\n1. REG-NAS only needs one well-trained proxy model, unlike the previous work GenNAS which needs at least 20 models for CNN and RNN. \n\n2. The proposed proxy task requires fewer epochs to converge than the ground-truth task which reduces the search time.\n\n### Weaknesses:\n1. My most concern is the integrity of the paper. From Table 3, we can observe that the Spearman or Kendall Correlation Coefficient is not good enough (e.g., 0.362/0.260 for ogbg-molhiv and 0.421/ 0.303 for ZINC). I have no idea that the metric of the proxy regression task can really replace the downstream metric (AUC, MAE). Thus, in my opinion, the authors should show how the proposed REG-NAS improves the search efficiency of the NAS methods (e.g., RL, EA) on the downstream metric.\n\n2. There is the same question for another contribution: ranking stability. This paper takes a lot of space to talk about the proxy metric is stable and the ground-truth classification metric (AUC) is unstable under different initializations. However, in the practical application, we focus more on ground-truth metrics. If the ground-truth metric is unstable, why do we need a stable proxy metric to replace the ground-truth metric?\n\nIn my opinion, our problem is a GNN NAS problem and I think the proxy task may focus on improving search efficiency for GNN with ground-truth metric. However, it seems that this paper doesn't give sufficient proof for this aspect.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the source code is provided.",
            "summary_of_the_review": "This paper provides a proxy task approach to simplified evaluation metrics for GNN NAS. However, it doesn't provide sufficient proof that the proxy task improves search efficiency for GNN NAS.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3610/Reviewer_i2b5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3610/Reviewer_i2b5"
        ]
    },
    {
        "id": "-rk_aaVDI71",
        "original": null,
        "number": 3,
        "cdate": 1666688278362,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688278362,
        "tmdate": 1666688785956,
        "tddate": null,
        "forum": "t7HIN3fUAUu",
        "replyto": "t7HIN3fUAUu",
        "invitation": "ICLR.cc/2023/Conference/Paper3610/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper designs a proxy task for GNN architecture search. Specifically, a GNN is randomly selected from the search space as the proxy generator. For each candidate architecture in the search space, MSE error between its feature map and that of the proxy generator is computed and used to rank this candidate. This paper then analyses the stability and correlation of such ranking method using two graph-level datasets.",
            "strength_and_weaknesses": "I have several concerns about this paper:\n\n1. Finding a proxy task for GNN architecture search is not appealing, because GNN is typically not deep (especially in your search space) and its training is much cheaper compared with training a deep CNN or a transformer. Only moderate search time is needed to find a good GNN reliably (e.g., with evolutionary search). It is questionable whether reducing search time via a proxy task is worth the extra problems it brings.\n\n2. The description of the proxy pipeline is not clear enough. For example, for each candidate in the search space, is it trained to minimize the MSE error, or is it trained with the original loss function (MSE error is only used as the final evaluation)?\n\n3. The proxy design needs further justification. Feature maps from different message-passing layers are compared to the same F_p of the proxy generator GNN, which is counterintuitive.  Analysis from GNN's perspective should be added.\n\n4. The experimental results are not convincing enough. The correlation in Table 3 is poor and cannot support the proposed proxy task. Small search space, small graph size, only 2 datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see Strength And Weaknesses.\nIt is good to see code is included.",
            "summary_of_the_review": "Please see Strength And Weaknesses",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3610/Reviewer_TB9W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3610/Reviewer_TB9W"
        ]
    },
    {
        "id": "fcE9mlM9D3",
        "original": null,
        "number": 4,
        "cdate": 1666872343311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666872343311,
        "tmdate": 1669571101559,
        "tddate": null,
        "forum": "t7HIN3fUAUu",
        "replyto": "t7HIN3fUAUu",
        "invitation": "ICLR.cc/2023/Conference/Paper3610/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work proposed to search GNN architectures by searching on a proxy regression task. The proxy regression task is generated by regressing some intermediate input features to output features from a pretrained GNN model which is randomly selected from the search space. The results are evaluated based on Ranking Coefficient Spearman\u2019s \u03c1 and Kendall\u2019s \u03c4.",
            "strength_and_weaknesses": "Strength:\n\nThe idea of using regression as a proxy task is interesting.\n\nConcerns:\n\n- Why the ranking stability for classification tasks can be much lower than for regression tasks? Any justification?\n\n- The authors claim to be the first to analyze factors that will affect models\u2019 ranking stability for GNNs. However, to the best of my knowledge, this has been systematically studied in SGAS [1] which is not discussed. I believe a dedicated section for the comparison and discussion with SGAS is needed. \n\n- The authors propose to generate the proxy task by a randomly selected GNN architecture. However, it would cause significant biases toward similar architectures as the selected GNN architecture. For instance, if a GNN with sum aggregations is selected. The search algorithm will favor GNNs with sum aggregations because of the expressiveness of GNNs.\n\n- At the 3rd step of ReG-NAS, a model is selected to be evaluated GNNe from the search space. I wonder how the model is selected. Is it brute force, RL-based, evolution-based, or differentiable-based?\n\n- The final results of obtained architectures on ogbg-molhiv and ZINC are not reported. There is no comparison with SOTA models. \n\n[1] Li, G., Qian, G., Delgadillo, I.C., Muller, M., Thabet, A. and Ghanem, B., 2020. Sgas: Sequential greedy architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1620-1630).\n\n[2] Xu, K., Hu, W., Leskovec, J. and Jegelka, S., 2018. How powerful are graph neural networks?. arXiv preprint arXiv:1810.00826.\n\n\n======== post rebuttal ========\n\nThanks for the authors' reply. However, my main concern is that the method is only evaluated on a small search space with a brute-force search approach. It is nonclear if it can be generalized to a large search space. I keep my rating unchanged.",
            "clarity,_quality,_novelty_and_reproducibility": "Some claims may be inaccurate. The novelty is not significant compared previous method (see Concerns for the details).",
            "summary_of_the_review": "The manuscript can be improved for future submission.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3610/Reviewer_xCbv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3610/Reviewer_xCbv"
        ]
    }
]