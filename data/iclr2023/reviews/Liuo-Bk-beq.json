[
    {
        "id": "c3yJCtFbA-",
        "original": null,
        "number": 1,
        "cdate": 1666513740454,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513740454,
        "tmdate": 1666513740454,
        "tddate": null,
        "forum": "Liuo-Bk-beq",
        "replyto": "Liuo-Bk-beq",
        "invitation": "ICLR.cc/2023/Conference/Paper236/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the impact of homoglyph replacements of the texts on text-to-image models. Three interesting observations are found:\n\n1) Homoglyph replacements can cause image content obfuscation.\n\n2) Homoglyph replacements can induce cultural bias.\n\n3) Homoglyph susceptibility can be observed in other text-to-image models, e.g., Stable Diffusion. However, the effect is weaker in Stable Diffusion than that in DALLE-2.\n\nThe authors also suggest two simple solutions to avoid biases. One is to replace all characters in the texts with standard characters, the other is to train the language encoders with multilingual texts.",
            "strength_and_weaknesses": "Strengths:\n1) The paper studies the text-to-image models from an interesting perspective.\n2) The findings presented in the paper are interesting.\n3) The authors did a thorough analysis to draw these conclusions.\n\nWeakness:\n1) My major concern is that the proposed solutions are too simple and intuitive. In a paper, we hope to see how to alleviate the problem using a more technically interesting solution rather than merely present the problem.\n2) Reasons for the model behaviour are just verbally explained and they are not verified by the experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well-written and easy to follow.\n\nQuality: The analysis of the phenomenon of bias is thorough enough. But the analysis of the reason is weak and potential solutions are too naive.\n\nNovelty: The problem itself is interesting. But samely, the proposed solutions are trivial and intuitive.\n\nReproducibility: Good.",
            "summary_of_the_review": "I tend to give a score of 5 to this paper.\n\nI think the topic this paper explores is interesting and the analysis of the phenomenon is thorough. My concerns are: 1) the paper didn't dive into the reason behind the phenomenon; 2) The proposed solution is too simple.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper236/Reviewer_GVzw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper236/Reviewer_GVzw"
        ]
    },
    {
        "id": "OBSKhacBjni",
        "original": null,
        "number": 2,
        "cdate": 1666652273887,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652273887,
        "tmdate": 1666652273887,
        "tddate": null,
        "forum": "Liuo-Bk-beq",
        "replyto": "Liuo-Bk-beq",
        "invitation": "ICLR.cc/2023/Conference/Paper236/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The shows that current text-to-image models such as DALLE2 and Stable Diffusion produce different images even if only a single character of the text description is changed to a (visually similar) different character (e.g. replacing the latin character \"A\" with the Greek character capital alpha \"\u0391\"). This behavior also shows certain biases of the underlying models, e.g., generating images coming from the cultural or geographic background based on the replace character (e.g., whether a Latin character is replaced by a Greek or a Arabic character). This behavior allows users to change the model output while seemingly keeping the text input constant (the unicode of the character changes but visually this is not obvious when reading the caption).\nThe authors argue that this behavior may allow attackers to reduce the generation quality of a model or to portrait the model behavior differently by imperceptibly changing the input caption.",
            "strength_and_weaknesses": "The paper is very clear and shows several examples for how exchanging characters with similar other characters affects the generate image, either by completely ignoring important parts of the caption (e.g. noun phrases if a single character was changed) or by biasing the output towards different contents based on the origin of the inserted character (e.g. Indian or Vietnamese portrait).\n\nThis is a somewhat interesting finding since it indicates that the models were trained at least with some of these non-Latin characters and learned to associate those with certain contents (e.g. geographic regions).\n\nWhile the finding itself are interesting I don't think they are necessarily surprising. While the characters may look similarly the underlying unicode encoding for those characters is discrete and, therefore, the language model would treat those as independent characters, no matter what they look like visually.\n\nI don't think this can be called an adversial attack though. IMO an adversarial attack is an attack that is a) indistinguishable from a normal \"correct\" input for a human and b) not easy to detect automatically. While a) may be true in this case, the authors themselves state that it is easy to detect these changes simply by checking the unicode encodings of the input string.\n\nAlso, I'm also not sure if calling this behavior \"biased\" is correct. If I ask for a portrait of an \"Indian woman\" getting a portrait of an Indian woman is not biased behavior, but desired behavior. Similarly, replacing a Latin character in the caption with an Indian character can be interpreted as generating an image with Indian influence.\n\nOverall, I feel like this problem (if it really is a problem) is easily addressed by standard preprocessing, similarly to converting all text to lower-case, non-Latin unicode characters can be mapped to their closest unicode characters or, alternatively, the model can be trained on more non-Latin unicodes.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to understand.\nThe findings are somewhat novel and easy to reproduce.",
            "summary_of_the_review": "While the findings are interesting I don't think they contain enough novelty or impact to accept the paper in its current form.\nAlso, I am not sure about the terminology used in the paper, e.g., calling these changes \"adversarial attacks\" or claiming that this behavior is evidence of model bias.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper236/Reviewer_k5Mu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper236/Reviewer_k5Mu"
        ]
    },
    {
        "id": "07r3akpbHVY",
        "original": null,
        "number": 3,
        "cdate": 1666677637777,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677637777,
        "tmdate": 1666677637777,
        "tddate": null,
        "forum": "Liuo-Bk-beq",
        "replyto": "Liuo-Bk-beq",
        "invitation": "ICLR.cc/2023/Conference/Paper236/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper addresses an attack scenario for text guided image generation models like dalle2 or text-image joint model like clip. This paper is entirely qualitative and doesn't provide any quantitative metrics. The paper argues that an attack can be made to these models by replacing one alphabet character in an input text with a unicode non-latin character such as a Korean character, which will affect the output image. ",
            "strength_and_weaknesses": "The strength of this paper is that the situation assumed in the paper is very unique. The paper also shows that by using a character in a language, one can obtain an image that belongs to that cultural group. It may be trivial but I don't think anybody has shown it. \n\nSeveral weaknesses\n1. This is a 100% qualitative paper with no attempt to quantify the extent of the problem being discussed. The problem of bias is essentially finding the right (fair, for example) measure for the problem. We can't just cherry pick a few examples and claim that we found something meaningful. This needs to be thoroughly measured and validated.\n\n2. The attack scenario, while novel, doesn't sound reasonable to me. When will this happen? Let's say there's an attacker, a malicious app. A user enters a text input. The app will then modify the query and inject some signals there. But then why would the app show the modified text back to the user using a non standard latin character? The whole point about homoglyphs is that humans can be confused, but not machines. Why wouldn't the app just modify the text more explicitly by adding more words and not show the modified text to users? ",
            "clarity,_quality,_novelty_and_reproducibility": "There's no quantitative result in the paper. ",
            "summary_of_the_review": "This paper proposes to study an interesting behavior of image-text generation/joint models. Due to the lack of scientific measures to validate the argument, this paper is not ready for publication at a venue like iclr. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper236/Reviewer_wkRc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper236/Reviewer_wkRc"
        ]
    },
    {
        "id": "G629T2KLp-",
        "original": null,
        "number": 4,
        "cdate": 1666726630362,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666726630362,
        "tmdate": 1666726630362,
        "tddate": null,
        "forum": "Liuo-Bk-beq",
        "replyto": "Liuo-Bk-beq",
        "invitation": "ICLR.cc/2023/Conference/Paper236/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper shows that the quality of the generated images from text-to-image models such as DALLE-2 and Stable Diffusion can be greatly impacted by single homoglyph replacements. A malicious user can replace a character with a homoglyph that looks the same to the human eye and the text-to-image system produces images with cultural biases. The authors show that this behavior is (to a certain extent) consistent between different text-to-image models. Finally, the authors propose multilingual training of the text-encoder as a potential solution to this problem.",
            "strength_and_weaknesses": "Strengths:\n\n*  The paper is definitely interesting. The fact that a single letter change can have such a dramatic impact on the generation quality was surprising to me.\n* The authors show that this phenomenon is not specific to one model. There seems to be a more universal issue.\n* The observation that M-CLIP is more robust is very insightful. This suggests that multilingual training could reduce cultural biases present in text-to-image generative models.\n\n\n\nWeaknesses:\n\n* I am not sure about whether this observation poses a new security threat for these models. If a single letter change introduces such cultural biases, I am fairly certain that more obvious changes to the prompt would also trigger culturally biased generations. If the goal of the attacker is to use Stable Diffusion or DALLE-2 to generate disturbing images, I think there are other ways to achieve this as well. So the question becomes, why the homoglyph replacement attack is more dangerous than other types of attacks. I am not sure I have a good answer on that. Since the attacker is the one deciding the prompt in the first place, why is it important to make it look innocent to someone else? How could it be maliciously used? I would love to hear more from the authors on this.\n* I strongly disagree with the potential interpretation of this phenomenon as a feature in Section 5.3. The users of an API should not have their prompt manipulated before entering the system, unless it is explicitly documented or/and there is an option to deactivate it. It is one thing to use filters to catch harmful prompts and another thing to fool the users by presenting results for a modified prompt. But even if the intent is good and the users are informed about how their input was modified, it is still very unclear how the reduction of societal biases would be achieved. What is the right amount of homoglyphs to replace for a \"fair\" generation? How does one ensure that the new generations are not having new biases? The authors rightfully acknowledge that the method works better for specific cultures.\n* The authors propose multilingual training as a potential solution to the observed problem. However, DALLE-2 seems to have been trained on multilingual data. I tried a couple of prompts in Japanese and German and the system successfully generated pictures corresponding to the prompts. So is the claim that the model should have been trained on *more* multilingual data? Or was the text-encoder frozen during the training and we need to train the text-encoder on multilingual data? \n* I am fairly confused about whether the issue is in the Text Encoder, the generative model or both. See also the bullet point above. Would love some experimentation to understand this better.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is overall well-written. The ideas are presented clearly. To the best of my knowledge, the proposed attack is novel and the effectiveness of it is surprising. The authors provide the source code to reproduce the results. I strongly encourage open-sourcing the code upon acceptance.\n\nThere is a point that I don't understand well. Figure 5 seems to be making an even stronger point that the rest of the paper: the sensitivity appears to be in the character level, even independently of *where* the character is in the sentence. My understanding of the way the authors form the embeddings in Figure 5 is the following: i) pass the entire sentence through the text encoder, ii) pass the character to be replaced from the text-encoder, iii) pass the replacement from the text-encoder and finally iv) form the embedding by taking the sum of i) + iii) - ii). Is that the case? If that's true, there is no position information on where the character to be replaced is, e.g. think of what happens when it appears two times.\n\nMinor typo: Athene -> Athens in Section 4.2.\n\n",
            "summary_of_the_review": "This paper presents a surprising observation: single homoglyph replacements change dramatically the quality of the samples and lead to biased generations. The fact that generative models amplify societal biases is not new. This paper shows a new aspect of this problem: the sensitivity to small input perturbations. I believe this is an important observation - even if a system appears to have diverse outputs, a small change to the input can drastically deteriorate its performance. \n\nI am not convinced that the attack itself poses a new security problem. However, it broadens our understanding of the current limitations of these models. Therefore, I (weakly) recommend acceptance for this paper. \n\nI encourage the authors to engage in the rebuttal and address the questions raised above.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper236/Reviewer_4KN7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper236/Reviewer_4KN7"
        ]
    }
]