[
    {
        "id": "s9VH0C8B1m3",
        "original": null,
        "number": 1,
        "cdate": 1666616309349,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616309349,
        "tmdate": 1666616309349,
        "tddate": null,
        "forum": "U2WjB9xxZ9q",
        "replyto": "U2WjB9xxZ9q",
        "invitation": "ICLR.cc/2023/Conference/Paper463/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to learn a generative model of 3D scenes from large unaligned datasets. The paper argues that previous work only produced good results for well-aligned datasets (e.g. human and animal faces), and that directly applying such methods to unaligned data did not work well. In order to address this problem, the idea is to incorporate an off-the-shelf monocular depth estimator (in this case, LeReS) into their model architecture. In their proposed model, a discriminator aims to discriminate between an true RGBD image (the result of applying LeReS to a real unaligned image) and an RGBD image rendered from their triplane image representation. The discriminator also outputs an embedding vector which aims to represent the semantic content of the image, effectively distilling a feature model (in this case, a ResNet-50) into the discriminator. Additionally, the paper uses a more flexible camera parameterization than previous work: the model predicts a field-of-view and look-at position in addition to the position of the camera on a fixed-radius sphere. \n\nThe paper performs experiments on multiple datasets including a variety of LSUN categories and the full ImageNet dataset. It shows strong results compared to previous work using GANs (e.g. EG3D, EpiGRAF, etc.). \n",
            "strength_and_weaknesses": "### Strengths \n* This paper is an empirical work, and the empirical results are relatively strong. \n* There is no reliable quantitative measure of generated geometry, so the paper introduces Non-Flatness Score. NFS is a very simple heuristic, but it does seem to provide at least some insight into the flatness of generated geometry. \n* The NFS values (Table 2) show that for unaligned datasets, previous work (e.g. EG3D) does not learn reasonable depth values, whereas 3DGP does learn reasonable depth values.\n* The proposed camera regularization strategy is intuitive. Figure 7 is an illustrative figure in this regard. \n* The inclusion of Appendix D (Failed Experiments) is very nice to see and much appreciated. The field would benefit substantially if sections like these were included in all papers. \n\n### Weaknesses\n* The novelty of this paper may or may not be viewed as a weakness. On the one hand, there are no groundbreaking ideas; the paper uses components that have been used before and combines them to obtain strong results. On the other hand, the exact method has never been used before; nobody else has combined these components in the precise manner that is being proposed here. \n* For the ImageNet experiments, the method uses more time/compute than competing methods (28.2 days vs 18.7 days or 15.9 days; Table 2). Training for a similar amount of time as prior work would allow for a fairer direct comparison.\n* When discussing GANs with external knowledge (at the top of page 4), the paper claims that \u201cA similar technique is not suitable in our case as pre-training a generic RGB-D network on a large scale RGB-D dataset is problematic due to the lack of data.\u201d However, it should be possible to train a generic (2D) RGB-D network on existing 2D image datasets by converting the 2D RGB images into RGBD images using monocular depth estimation; the paper effectively uses the same. \n\n#### Minor Weaknesses\n* The method appears to be quite sensitive to hyperparameters (e.g. those on page 7). This is somewhat expected, as other GAN-based methods also tend to be quite hyperparameter-sensitive. \n\n### Questions:\n* Existing 3D monocular depth estimation models are far from perfect. Are the failure cases from the monocular depth estimation network learned and reproduced by the generative network? This is mentioned briefly on Page 6 as motivation for the depth adaptor; it would be interesting to provide some more detail on this point. \n* Do symmetries (e.g. those with respect to the position of the camera on the sphere) cause any issues with learning the camera distribution? \n* With regard to the camera distribution, it seems strange to have the same camera distance for diverse ImageNet categories such as a goldfinch and a church. One can of course partially account for this by predicting different focal lengths, but have you tried learning a more flexible distribution of camera such as the inside of a sphere or a 3D annulus? Additionally, how does the distribution of learned focal lengths for different ImageNet classes look like? \n* Given that the model learns a triplane representation, it should be possible to rotate all the way around the scene. However, all of the examples shown are front-facing views with small-to-medium camera movements. Does the model learn reasonable backsides for objects? For example, the backside of the horses shown at the bottom of Figure 5. It is not critical for the paper to reconstruct the backside of objects. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly. Although there is no code provided, the results appear to be reproducible. The novelty of the paper is discussed in the section above. ",
            "summary_of_the_review": "This paper proposes a method for learning 3D generative models from large datasets of unaligned images. The paper tackles an important \ntopic in unsupervised 3D reconstruction. Although the paper does not have any groundbreaking ideas, it combines ideas and models from the existing literature in a different manner from previous work. It tackles an important and often-overlooked problem (that of generalizing existing methods to real-world datasets) and produces relatively compelling results. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper463/Reviewer_K59a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper463/Reviewer_K59a"
        ]
    },
    {
        "id": "vzNmEqXeYn0",
        "original": null,
        "number": 2,
        "cdate": 1666624106874,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624106874,
        "tmdate": 1666624242267,
        "tddate": null,
        "forum": "U2WjB9xxZ9q",
        "replyto": "U2WjB9xxZ9q",
        "invitation": "ICLR.cc/2023/Conference/Paper463/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a new method that learns a 3D-aware generative model on unaligned image datasets like ImageNet. \nThe authors propose to use an off-the-shelf depth estimator to guide 3D generator. The estimated depth is concatenated to the real image and feed to discriminator to guide the generated image and depth from the generator. Additionally, a learnable camera model based on the ball-in-sphere geometry is proposed to support the diverse camera settings in unaligned data, which includes position, fov, and lookat parameters. \nThe results show that the method can generate realistic images with depth on ImageNet dataset. ",
            "strength_and_weaknesses": "** Strength **\n\n- The use of depth priors is an interesting idea to guide 3D-aware generative model. \n\n\n** Weakness ** \n\n1) The use of the off-the-shelf depth estimator could, however, result in unfair comparisons as the depth estimator relies on another datasets for training. To be fair, the depth estimator has to be trained on the same dataset for the generative model, but this might not be possible on ImageNet. \nThe effect of these additional data should be properly discussed. \n\n    a) Geometry prediction of the original depth estimator should be provided. \n  \n    b) Compare a) to the geometry generated by the proposed method. \n  \n    c) Compare the geometry generated by different depth estimators, and depth estimators trained on different datasets. Again, this point is not convincing to me still because the use of the additional data. \n\n\n2) More results on ImageNet should be presented as claimed in the paper title. For example, out of the 1000 classes on ImageNet, how many categories have the authors experimented? \nTo evaluate the geometry quality, the authors might want to refer to real object datasets such as ScanObjectNN, and compare the reconstructed geometry with these objects in point cloud format with FID evaluation. \n\n    [a] Uy et al., Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data, ICCV 2019.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- 3DGP can be a confusing name given EG3D is a previous work. The authors might want to reconsider this. \n\n- The pdf I receive has some lines on page 10 that violate the page format. \n\n",
            "summary_of_the_review": "The paper itself is an interesting read and provides a step toward making 3D-aware generative models more practical and robust. My concern is that the experiments are unfair and could lead to confusion. I am happy to raise my score if the authors could convince me about the actual performance of the model given the off-the-shelf depth estimator. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper463/Reviewer_GoQR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper463/Reviewer_GoQR"
        ]
    },
    {
        "id": "peQ7wbphif",
        "original": null,
        "number": 3,
        "cdate": 1666677871027,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677871027,
        "tmdate": 1668712455693,
        "tddate": null,
        "forum": "U2WjB9xxZ9q",
        "replyto": "U2WjB9xxZ9q",
        "invitation": "ICLR.cc/2023/Conference/Paper463/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends the 3D GAN to more diverse and un-aligned datasets (eg., ImageNet). This problem setting change is significant and very challenging, as we have learned from 2D fields (StyleGAN3--> StyleGAN-XL). This paper introduces several new modules to adapt an existing model EpiGRAF to this new setting including 1) depth-regularized GAN, 2) learnable camera formulation, 3) a knowledge distillation loss. Results on various dataset including ImageNet are promising in both rendering and geometry quality. ",
            "strength_and_weaknesses": "**Strength**\n- The biggest limitation of existing 3D GAN is the applicable data. This paper extends the 3D GAN to a more challenging setting where large-scale un-aligned dataset is used.\n- The introduced components are very interesting and works well. \n   1. The learnable camera formulation is very useful and can be a standard thing for follow-up works in this field. I think this is very important for models object-centric images in the wild. \n   2. The motivation of using depth as regularization makes lots of sense. Even though it's not easy to leverage estimated depth data, the proposed method managed to being beneficial.\n   3. The knowledge distillation loss is particularly useful for ImageNet. \n\n**Weakness**\n\n- My biggest concern is the visual quality. From the provided annoymous link, the generated images suffer from many artifacts. Even though the \"flat\" issue is addressed compared to EG3D, the visual quality is merely improved. Both of them fall far behind the 2D generators. \n- Even though using depth is resonable, it also makes the framework more complicated. It's quite tricky to do the Adversarial Depth Supervision (ADS) and set the p(d) correct. \n- More 3D GAN baselines besides EG3D (tri-plane) could be tested. It's possible that the \"flat\" issues are caused by the tri-plane representation.  Therefore, a volume-based or MPI-based 3D GAN could have very different results.\n- The loss in Eq.1 is somewhat weird. To me, this loss is minimizing and maximizing the same thing simultaneously. Why does this loss make sense? What does it converge to? What's the optimal value for this task? \n\n**Doubts**\n\n- Is \u201cBall-in-Sphere\u201d formulation good enough for any images in-the-wild? What stops us from making it more flexible? For example, why do we need the camera to stay on the hemi-sphere? Why do the loss weights for the three components (pos, fov, lookat) differ so big?\n- How are the camera priors decided? In Fig.7 most priors are relatively uniform, but pitch/look-at are more like gaussian. It's unclear to me how to set these priors. Do we need to choose different priors when dataset changed?  Also, it's also unclear to me why residue based method fails so bad.\n- Why squared loss for distillation? This is not the standard option for knowledge distillation. Does it outperform standard setting, ie., KL-divergence? \n- Why not applying a depth loss on adaptor output? For example, applying LeReS on the rendered image to get the ground-truth depth and use it to supervise the output of adaptor.\n\n**Writing**\n\n- For 2D methods (StyleGAN2 (with KD)) in Tab.1-2, it's better to change their NFS score from 0 to N/A. \n- The qualitative results from the link could be added to the paper (appendix is fine).\n- The paper should be self-contained and thus R1 gradient penalty should be given in Appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThis paper is clear overall. My specific doubts and concerns are mentioned in above section. Appendix is useful and informative. \n\n**Novelty**\n\nThere are mainly 3 introduced modules: 1) depth-regularized GAN, 2) learnable camera formulation, 3) a knowledge distillation loss. All of them are interesting and novel for 3D GAN framework. \n\n**Quality**\n\n- Despite being novel and promising, the introduced novel modules aren't very easy to use and couple of additional modules (eg., adaptor, camera prior, additional hyper-parameters) need to be leveraged. \n- Also, the quality of the generated 2D images far way behind the state-of-the-art 2D generators. (as shown in \n https://u2wjb9xxz9q.github.io./) Even though being view-consistent, there exist many obvious artifacts. Nevertheless, this paper is still an exciting first step in this challenging problem setting. \n\n**Reproducibility**\nThe paper provides fine implementation details in both main paper and the Appendix. However, it would be still quite challenging to reproduce without the source code since the propose framework is relatively complex.",
            "summary_of_the_review": "This paper tries to solve a very challenging task and proposes a legitimate framework consisting several new components. The results are promising and encouraging. I'm supportive overall and my current rating is 6 but not higher because the visual quality of the rendered images are not very satisfying. Also, the introduced framework is a bit complex since it incorporates several sub-modules/networks. \n\nFor the authors: I'm sorry and please point out if I overlooked anything. Happy to chat more from here.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I didn't find any ethics issues even though I didn't check research integrity issues (e.g., plagiarism, dual submission).",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper463/Reviewer_pJ3E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper463/Reviewer_pJ3E"
        ]
    },
    {
        "id": "fKTgur8Tp-f",
        "original": null,
        "number": 4,
        "cdate": 1666713356062,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713356062,
        "tmdate": 1666713356062,
        "tddate": null,
        "forum": "U2WjB9xxZ9q",
        "replyto": "U2WjB9xxZ9q",
        "invitation": "ICLR.cc/2023/Conference/Paper463/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces the first 3D generator that works on 3D datasets without alignment. To this end, conditioning on an off-the-shelf monocular depth estimation approach, the proposed method incorporates generic depth priors to facilitate 3D generation. Moreover, richer camera models are taken into account, ensuring more realistic generation results. Finally, with a distillation mechanism,  a pre-trained image classification network provides further supervision to stabilize and improve training. Experiments on standard datasets validate the effectiveness of the proposed method.\n",
            "strength_and_weaknesses": "**Strength**\n- The paper is well motivated as existing methods do not work well on non-aligned data.\n\n- Using an off-the-shelf depth estimation as a generic prior is intuitive, and experiments show that such prior helps improve the generated quality.\n\n- I find this paper easy and enjoyable to read and follow.\n \n- The visual generation quality is significantly higher than compared methods.\n\n- The learnable Ball-in-Sphere camera model does have a higher than a standard with 2 dof and works for objects that are not center-aligned.  \n\n- The experiments are carefully and extensively conducted. I appreciate that failed attempts are also mentioned in the appendix, which helps the audience and following researchers better understand this work.\n\n** Weaknesses**\n\n- The usage of an off-the-shelf depth estimator may make the comparison with other depth-estimator-free methods unfair. Plus, how do the performance and generalization of the monocular depth estimator affect the performance? Will the performance improve if trained with ground truth depth (for example if it is trained on a depth estimation dataset or a synthetic 3D dataset)?\n\n- The gradient penalty is widely used and studied in GAN-based approaches, the authors simply extend that to the parameters of the camera model. How good is the method if multiple objects are in the scene? The authors imply that their model works well on \"a scene \n consisting of multiple objects\", but I cannot find any experiments verifying this claim.\n\n- The so-called knowledge distillation for discriminator is more like an engineering trick and does not provide much technical insight to me. Moreover, the authors mention that \"it can work with arbitrary architectures of the discriminator,\" but it can only work on a discriminator that produces features with the same dimension as ResNet50. I am also wondering if this L2 KD loss works better than the mostly used KL divergence loss.\n\n- This paper should compare with a 3D photo baseline [a]. Combining a vanilla 2D image generator with such 3D photo generation methods could also produce realistic 3D image synthesis results while it seems to generalize on a larger range of images.  I wonder how the performance differs for these two different generation paradigms. Will the proposed method have a higher multi-view consistency? Will the proposed fail while the 3D photo methods work better in certain situations? Thus, this line of work should also be discussed.\n\n[a] 3D Photography using Context-aware Layered Depth Inpainting. CVPR 2020.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper explains its motivation and ideas well. The technical novelty is good. Some technical components in this work are of fair novelty. Many details are provided to ensure reproducibility. However, I found there are many hyperparameters (e.g., in Eq 5) that may make it hard to reproduce the results.",
            "summary_of_the_review": "Based on the aforementioned pros and cons, I vote for a weak accept of this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper463/Reviewer_yhJP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper463/Reviewer_yhJP"
        ]
    }
]