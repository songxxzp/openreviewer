[
    {
        "id": "AGn2AxXGFf",
        "original": null,
        "number": 1,
        "cdate": 1666584509185,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584509185,
        "tmdate": 1666584509185,
        "tddate": null,
        "forum": "H73xwqPfW2f",
        "replyto": "H73xwqPfW2f",
        "invitation": "ICLR.cc/2023/Conference/Paper2904/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on multitasking ability in reinforcement learning (RL). To do so, a multi-task learning framework, Neural Pathway Framework (NPF), is proposed by simultaneously training multiple specialized pathways where each pathway corresponds to a task. Finally, experiments were conducted on several continuous control tasks under both online and offline settings, including using Mujoco-based and MetaWorld environments, with results demonstrating its effectiveness in terms of performance and parameter size.\n",
            "strength_and_weaknesses": "Strengths:\n- Multitasking is an important ability to achieve artificial general intelligence (AGI). This is an interesting work targets to multitask RL where the ideas build up from the neuro-science perspective on neural pathways in human brains and the machine learning perspective on finding sub-networks with pruning.\n\nWeaknesses:\n- The proposed methods are not presented in a principled way, although the main contribution of this work is primarily on the empirical side. For example, what's the formal definition of the problem? What are the loss functions or objectives for the offline or online settings? What's the mask used in the methods? What are the original networks and sub-networks? How do neural pathways/sub-networks interact with different tasks? How to guarantee or justify that the obtained sub-network is the right solution? How are the learned skills reused/transferred among tasks? What does the mask \"$m$ is fixed\" mean?\n- In Figure 4 (a), why the gradient norm becomes larger? Does that mean there exists a convergence issue? How to define the success rate in (b)?\n- This paper claims that the proposed method \"is especially suitable for real-world applications in which large datasets need to be processed in real-time or which are deployed on low-resource devices\". However, I didn't see any comparisons of time cost or computation cost shown in the experiments.\n\n\nMinors:\n- \"In the online RL setting,.\" -> \"In the online RL setting,\".\n- \"nonstationaritym\" on pp.5 should be \"nonstationarity\".",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper could be improved by clearly showing more details of the proposed method as I mentioned in the main Weaknesses. Besides, there are some questions that could be clarified.\n- The idea of finding neural pathways with pruning is not new. Since the current paper is not organized and written in a good form, especially on the methodology part, it's hard for me to evaluate its originality accurately.",
            "summary_of_the_review": "According to my comments in both the main Weaknesses and the section of \"Clarity, Quality, Novelty\", I feel this is an empirical paper where reasons to reject outweigh reasons to accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2904/Reviewer_rc1X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2904/Reviewer_rc1X"
        ]
    },
    {
        "id": "r1JtXYgPoBj",
        "original": null,
        "number": 2,
        "cdate": 1666589763611,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589763611,
        "tmdate": 1666589763611,
        "tddate": null,
        "forum": "H73xwqPfW2f",
        "replyto": "H73xwqPfW2f",
        "invitation": "ICLR.cc/2023/Conference/Paper2904/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed a multitask RL framework by learning multiple pathways through a single network, called Neural Pathway Framework (NPF), which aims to find a task-specific optimized mask. Started with a method from a network pruning literature, the authors further proposed to use replay buffer and average multiple iteratures to stablize training. Experimental results shows performance improvement with reduced parameter count.",
            "strength_and_weaknesses": "I do have some concerns regarding this paper, especially in terms of the clarity.\n\nThe clarity of the paper needs to be improved. \n1. How is global-model optimized in Algorithm 1? Providing an equation would be very helpful. \n2. The most important equation (1) in paper is not clear to me. By my understanding, m_n is the mask for n_th task, while S(theta; D) is the objective function. Why these two terms are equivalent?\n3. Equation 2 is also unclear to me. It envolves two techniques to stablized the training, keep a temporary replay buffer TD_n and re-confriguring using the average of the previous M iteration scores. In paper, it seems that keeping a temporary replay buffer aims to keep only the most-recent samples from agent to update the mask, but averaging between several iteration is using samples span longer history, which seems contradictory. How would you interpret it?\n4. For results in Table 3, how could variance of BCQ with NPF be 0? Also, since it is normalized, I'm curious of the original performance of BCQ and IQL algorithm.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In terms of originality, Pathway is an interesting idea as an overall framework for AI system to handle discrepency between tasks, domains, etc, which is first proposed by Jeff Dean to my knowledge. That been said, it is still interesting to discuss possible ways to actually implement the idea to solve problems and address difficulties, like how to find the optimal pathways for each task or domain or even sample. \n\nIn terms of concreteness, the proposed model aims at achieving similar performance using sparse sub-network in comparison with full dense network. Thus the experiment in Figure 3 shows inferior results with proposed model in comparison with baseline. It needs to further clarify that how significant it this performance difference.",
            "summary_of_the_review": "The paper studies an interesting problem and gives a potential solution. My major concerns are about clarity. I might will change my recommendation after things are more clearer.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2904/Reviewer_6eEZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2904/Reviewer_6eEZ"
        ]
    },
    {
        "id": "11Mjx31x0Y",
        "original": null,
        "number": 3,
        "cdate": 1666628763498,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628763498,
        "tmdate": 1666628763498,
        "tddate": null,
        "forum": "H73xwqPfW2f",
        "replyto": "H73xwqPfW2f",
        "invitation": "ICLR.cc/2023/Conference/Paper2904/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper aims to learn multiple RL tasks with the same network. SNIP is used to prune all but the 5% most \"relevant\" parameters of the network, yielding a mask for every task. Tasks can share parameters, and therefore all tasks have to be trained simultaneously. The authors show that SNIP depends strongly on the data set, and propose to regularly change the task-masks based on the newest data for online RL scenarios. The new method NPF is evaluated on 3 Mujoco and the Meta-World multi-task benchmarks with the BCQ, IQL and SAC algorithms and compared with other baselines. Results show that NPF is competitive with other multi-task approaches.\n",
            "strength_and_weaknesses": "**Strength**\n\nThe paper presents an innovative approach to learn and represent multiple tasks in one network. The approach is simple and can be adapted into many algorithms. Results look generally good. \n\n**Weaknesses**\n\nThe idea is, to the best of my knowledge, novel, but not very deep. Following the lottery hypothesis, the authors just select parameter-masks and keep them (in the offline setting) during training fixed. In the online setting the authors identify that the change in loss requires them to recompute the mask based on newer data. However, the implementation (averaging the last $M$ scores, stopping criterion) sounds very simplistic and there is no analysis on the dynamics of selected masks, ways to deal with this non-stationarity and whether the mask eventually converges, just as the policy does. More analysis on this would strengthen the paper significantly. Furthermore, there are some questions I would like to have answered:\n1. You emphasize that the training data changes the loss a lot, but in RL the loss is also changed e.g. by bootstrapping: at time $t$ the loss depends on the value function(s) with parameters $\\theta_t$. Have you evaluated this? How is the effect of this on the mask selection? \n2. What is the \"continual pathway configuration\" (referred to later as \"parallel learning\") in Figure 4? Is it NPF without the pretraining stage or training without mask or something else?\n3. How significant are the results? It looks as if the standard deviations overlap a lot. Can you make a statistical test whether your method is significantly better (p <= 0.05) than the next best baseline? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well written and mostly correct (see comments below).\nThe evaluation is mostly solid and the appendix contains enough information to reproduce the experiments.\n\n**Minor comments:**\n- sec.3: \"naivelt\"\n- sec.3: say explicitly whether sharing the same transition is a necessary assumption\n- sec.4.1: $\\frac{\\partial\\mathcal L}{\\partial \\theta_q}$ appears in different equations with different meanings. Clarify this by using $\\frac{\\partial\\mathcal L(\\theta_0)}{\\partial \\theta_q}$ and/or $\\frac{\\partial\\mathcal L(\\theta_0; D_n)}{\\partial \\theta_q}$\n- eq.1: $m_n = S(...)$ appears incorrect. It should be something like $m_n = \\text{argmax}_{q} S(...)$.\n- sec.4.2: shortly explain what \"synchronous parameter updates\" are\n- sec.4.2: \"function 1\" -> \"Equation (1)\"\n- alg.1: clarify where the $n$ in the training loop comes from (e.g. run all tasks $n$ in parallel) and when exactly the optimizer is executed (after all gradients have been summed). Currently the parallelization part is more confusing than clarifying. Adding an inner loop might help.\n- eq.2: it is completely unclear what the sum-variable $m$ refers to here, as it does not appear in the equation and overloads the mask $m$. Better define the sum over variable $k$ and (a) use $TD_n^{t-M:t}$ to denote that you are referring to replay buffer length or (b) define some $s_n^t$ as score at time $t$ and then select the mask based on the average last M scores $\\frac{1}{M} \\sum_{k=0}^{M-1} s_n^{t-k}$ or (c) define an online update $\\bar s_n^t = (1-\\alpha) \\bar s_n^{{t-1}} + \\alpha s_n^t$.\n- alg.2: the algorithm does not reveal how \"the last M iterations\" affect the masks and does not show how $D_n$ and $TD_n$ differ (they both just get all the data, what is the difference?). \n- tab.2: there need to be other baselines (like in tab.3) for comparison of the Mujoco experiments.\n- fig.5: the caption almost touches the main text\n- p. 6: the paragraph \"Online Multitask Training and Addressing Gradient Interference\" is in a weird place. It would make more sense in the beginning of Section 4.  \n- sec.5.1: the text says \"the mean performance with 95% confidence interval\", but the caption fo Table 3 says \"mean and std over 10 seeds\". Which is it?\n- sec.5.1: the MT baseline is ambiguous: do you train one network for all tasks (which should not work without a task indicator) or one network per task?\n- fig.6: figure almost touches the main text",
            "summary_of_the_review": "I really liked this paper. It is not totally clear whether the method will allow something that would otherwise be very hard in multi-task RL, but it is a nice idea and in my opinion well evaluated. The only weak point is a relatively low novelty and a lack of theoretical analysis. However, unless another reviewer finds something really wrong with it, I recommend to accept the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2904/Reviewer_jX9C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2904/Reviewer_jX9C"
        ]
    },
    {
        "id": "9ZNCXJ4kvQ0",
        "original": null,
        "number": 4,
        "cdate": 1666682463925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682463925,
        "tmdate": 1666682463925,
        "tddate": null,
        "forum": "H73xwqPfW2f",
        "replyto": "H73xwqPfW2f",
        "invitation": "ICLR.cc/2023/Conference/Paper2904/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to use masks in multi-task training so that only a part of network parameters are used for a specific task. The framework considers combining both online and offline RL methods. Experiments show that the proposed method significantly reduces the training parameters and achieves competitive performance compared with previous methods.",
            "strength_and_weaknesses": "Strengths:\n\nThe idea of introducing sparse training to multi-task RL is a promising direction.\n\nThe proposed framework is combined with both online and offline RL methods, showing the generality.\n\nWeaknesses:\n\nThe literature review is not extensive, missing the discussion and comparison of related works [1-3].\n\n1) Some technical details are not convincing: not considering Data distributional shift in offline RL setting. Data distributional shift is a common problem in offline RL since the data set is collected by some unknown policy. When the trained policy interacts with the environment, there exist some states/actions out of distribution where the policy may behave badly. \n\n2) The masks are fixed during training. However, it's more reasonable to adjust the mask since the sensitive measure is not static over the whole state space. Also, it is not static during different training stages. On contrary, if the mask is adaptive changing, it may cause unstable training.\n\n3) The experimental results should focus more on the benefits of sparse training regarding the training time/resources saved. For example, the parameter reduction results in offline RL are not provided.\n\nExperiments should consider the full MT50 task set and compare the proposed method with CAgrad [4].\n\n[1] Sparse Multi-Task Reinforcement Learning.\n[2] Multi-task Batch Reinforcement Learning with Metric Learning.\n[3] Hub-Pathway: Transfer Learning from A Hub of Pre-trained Models.\n[4] Conflict-averse gradient descent for multi-task learning.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The Clarity should be further improved. This paper has a lot of grammar issues, such as 'nonstationaritym',  'radient interference', and 'of group the parameters'.\n\nThe idea is composed of several existing techniques, which makes it somewhat not novel. The pruning technique used in this paper does not consider the difference from the single-task setting.\n\nOpen-sourced code is provided.  ",
            "summary_of_the_review": "Overall, this paper points out a promising direction in MTRL and provides relatively extensive experimental results. However, as listed in the pros and cons part, the novelty, technical issues, and missing related works make this paper lean to reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2904/Reviewer_Q8xY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2904/Reviewer_Q8xY"
        ]
    }
]