[
    {
        "id": "6JrCBqVCvRT",
        "original": null,
        "number": 1,
        "cdate": 1666675853102,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675853102,
        "tmdate": 1671412903140,
        "tddate": null,
        "forum": "Rb3RN0maB7F",
        "replyto": "Rb3RN0maB7F",
        "invitation": "ICLR.cc/2023/Conference/Paper4245/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies asynchorous federated learning with communication compression called QUAFL. In this model, The server randomly pick clients whose gradient might be stale, and clients are assumed to have positive expected number of local steps each time it interacts with the server.  A new model aggregation rule is designed in the paper, with convergence analysis. Experiments illustrate the effectiveness of the method.",
            "strength_and_weaknesses": "S1. The FL problem is important and may interest broad audience of ICLR. The asynchronous model seems novel and meaningful in practice.\n\nS2. The delay model considered here more flexible than the classical model with bounded delays, since here the delay can be unbounded in worst case.\n\n---------------------\nW1. In Algorithm 1 line 5, $Y_i$ is used without definition. I understand the global aggregation rule, but for the local update, why is the global model weighed by $1/(s+1)$? In my understanding, if there are very slow clients, then at local synchronization we should put a large weight on the global model to help the slow clients catch up with the global model. Say, if we have million of clients and $s=n$, then all the clients basically only update on their own data and do not synchronize with the server ($1/(s+1)$ is negligibly small). I doubt that the algorithm still works in this case. Could you please provide more intuition on this weight?\n\nW2. There are some overstatements regarding the theoretical results. The $O(1/\\sqrt{T})$ rate in Corollary 4.3 does not match the best convergence rate of FedAvg in Karimireddy et al. (2020). In that paper, the authors showed that the convergence rate of FedAvg can be $1/\\sqrt{Ts}$. \n\nW3. In Corollary 4.3 the learning rate $\\eta$ increases when $s$ decreases. This is counter-intuitive since typically, when there is fewer participating clients, the variance of gradient estimation gets larger and one need to use smaller learning rates. Additionally in this asynchronous case, smaller $s$ means more delays, which implies larger gradient error caused by staleness. Thus, I'm not very sure if this learning rate schedule is a correct choice.\n\nW4. The experimental evaluation is insufficient. Most results are ablation study and the proposed method is only compared with standard FedAvg in one figure. There are more compression methods other than the lattice quantization used in QUAFL which can be tested. There  might also be other aggregation strategies such as the one I mentioned above, directly updating the local model with the global model when interacted. More results should be provided to justify the unique advantage of the new update rule and quantizer, compared with some standard methods.\n\n------------------------------------------------------------------------------------------\nPost-rebuttal:\n\nI would like to thank the authors for the reply and adding more experiments. I appreciate the additional results on QSGD and FedBuff. However, the rebuttal does not fully address my concerns.\n\n1. While there is one figure in the supplemental file that shows the result with $n=300$, the number of active clients $s$ is only 30, which is still quite small. Also, it seems that the experiments are simulated on iid clients as described in Section A.2. Thus, this is not very convincing to resolve my question on the vanishing weight of the global model in the local model update with large $s$ asymptotically. When $s$ is very large, the local models train on their own data without communicating with others, and the global model simply takes the average of the local models. In my understanding, this should not work with non-iid clients. In addition, I think the \"optimal approach\" you mentioned is exactly the standard averaging strategy in which the global weight equals 1 instead of $1/(s+1)$.\n\n2. Thanks for the explanation and please add some discussion in the paper that your rate matches FedAvg under some specific parameter settings and in the wall-clock sense.\n\n3. In the scaffold paper, they consider two learning rates (global $\\eta_g$ and local $\\eta_l$) and the effect of $s$ is \"balanced out\". In your method you only use one learning rate which increases with smaller $s$. Thus, this still looks unnatural and incorrect to me.\n\nI appreciate the efforts in adding more experiments and baselines. Yet I still have some concerns on the correctness of the algorithm and analysis (and the iid experimental setting). So I will keep my score.",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "The paper studies an interesting problem in asynchronous FL. However, I have a few concerns about the algorithm and theoretical analysis. More clarification and experiments might be needed to address this issues. Moreover, the experiments, which mostly consist of ablation studies, are insufficient to show the benefit of the proposed strategies. Thus, I think the paper does not reach the high bar of ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4245/Reviewer_ea7y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4245/Reviewer_ea7y"
        ]
    },
    {
        "id": "_hJUJLOTdpb",
        "original": null,
        "number": 2,
        "cdate": 1666790488612,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666790488612,
        "tmdate": 1666790488612,
        "tddate": null,
        "forum": "Rb3RN0maB7F",
        "replyto": "Rb3RN0maB7F",
        "invitation": "ICLR.cc/2023/Conference/Paper4245/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper incorporates quantization and asynchronous communication to well known Federated Learning algorithm. The compression scheme is based on lattice quantization. Convergence analysis of the proposed algorithm is provided and empirically verified through a set of experiments.\n",
            "strength_and_weaknesses": "Strength:\nI think the paper is quite easy to follow and the majority of related works are cited.\n\nWeakness:\nIt is not clear to me at this point why authors specifically chose Latice quantization and what are the advantages of lattice quantization compared to the rest.\nI think it is very important to compare the tightness of the bound with prior work asyncronous FL algorithms such as [A]. What I would like to see is that if your bound of Theorem 4.2 if I do not use quantization $\\gamma=0$ how would your result compete with prior studies? Unfortunately, It is not obvious to me, and not sure whether you could improve the bounds in prior bounds.\nAnother limitation I see is that for both Theorems 4.2 and corollary 4.3 you need to have $T\\geq \\Omega (n^3)$ and $T\\geq \\Omega (n^4)$. In the convergence of FedAvg we do not have these constraints and how would the author comment on that?\nI think the experiment section is also limited. Authors would want to compare with other Asynchrnous FL algorithms as well. Furthermore, as our ultimate goal is accuracy, Authors should compare the accuracy of their results with FedAvg and other SOTA algorithms.\n\n[A]: Koloskova, Anastasia, Sebastian U. Stich, and Martin Jaggi. \"Sharper convergence guarantees for asynchronous SGD for distributed and federated learning.\" arXiv preprint arXiv:2206.08307 (2022).\n\nMinor comment:\nI think the contributions part should be more concise.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written.\n\nI did not run the experiments myself. \n\n",
            "summary_of_the_review": "Please see the comments above!\nI will update my score based on the authors feedback.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4245/Reviewer_ZjQd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4245/Reviewer_ZjQd"
        ]
    },
    {
        "id": "YTICr1kq_I",
        "original": null,
        "number": 3,
        "cdate": 1667224894229,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667224894229,
        "tmdate": 1667224894229,
        "tddate": null,
        "forum": "Rb3RN0maB7F",
        "replyto": "Rb3RN0maB7F",
        "invitation": "ICLR.cc/2023/Conference/Paper4245/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper examines the federated learning problem when updates are made asynchronously and communication is compressed. The authors present a new variant of the classic federated averaging (FedAvg) algorithm, namely QuAFL, that supports both asynchronous communication with compression. In some parameter regimes, they demonstrate that their proposed algorithm can provide similar convergence to FedAvg. The authors show that the algorithm ensures fast convergence for a few standard federated learning tasks on the experimental side.\n",
            "strength_and_weaknesses": "Strengths:\n-New convergence analysis for asynchronous federated learning\n-Incorporating the idea of compressed communication with asynchronous updates\n\nWeaknesses:\n-The server algorithm has an underlying assumption (lines 4-6 of Algorithm 1) that the server can allocate separate memory to each client's parameters. To this reviewer's understanding, this is not a proper assumption for cross-device federated learning. \n-The update in line 6 of Algorithm 1 requires an additional acknowledgment message from the server and client to ensure that the most updated parameter has arrived at the client. When updates are delayed or communication is lost, this creates an additional burden.\n-The assumptions on the compression operator must be stated formally in the paper. Lemma 4.1 does not clearly state the required setup for compression.\n-Section 3 presents briefly (informally) the assumptions about asynchronous communications. Asynchrony is one of the main contributions of this work, so it should be stated as a separate formal assumption in Section 4.\n-There is a limited comparison with prior works. It is necessary to compare the asynchrony assumption to prior works. As far as this reviewer is concerned, it is unclear how this assumption fits into the literature.\n-The benefit of compression in terms of exchanged bits (or other metrics) in the experiments must be presented and discussed. The experiments should include comparisons with other algorithms.\n",
            "clarity,_quality,_novelty_and_reproducibility": "-Sections 3 and 4 may be revised by the authors. A comprehensive comparison and discussion must be made with prior works in these two sections.\n-Could you please elaborate on the connection between Assumption 4 and the bounded heterogeneity assumption?\n-This work presents a new analysis for asynchronous federated learning with compressed communications building on lattice-based quantization.\n",
            "summary_of_the_review": "This paper presents an algorithm for asynchronous federated learning with compressed communications. The authors propose a feedback-based algorithm that reduces communication overhead in the federated framework based on existing research on asynchronous updates and lattice-based quantization. This paper focuses primarily on theoretical studies and analysis. A major area for improvement of the work is the need for adequate comparisons with prior works, both theoretically and experimentally. According to the analysis and experiments, there is no clear evidence that compression reduces communication volume. Additionally, some work is required to improve the presentation of this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4245/Reviewer_xMTC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4245/Reviewer_xMTC"
        ]
    },
    {
        "id": "76vCn0DsNXG",
        "original": null,
        "number": 4,
        "cdate": 1667282150223,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667282150223,
        "tmdate": 1667282150223,
        "tddate": null,
        "forum": "Rb3RN0maB7F",
        "replyto": "Rb3RN0maB7F",
        "invitation": "ICLR.cc/2023/Conference/Paper4245/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies how to use communication compression and asynchronous training together in federated learning (in particular, FedAvg algorithm). The authors showed that most previous papers used non-standard assumptions when analyzing the convergence of FedAvg with gradient compression. In their new analysis, they are able to use all standard assumptions. Also, they introduce new analysis techniques to analyze asynchronous behaviors. At last, experiments validate the theoretical predictions on how the algorithm changes along with its hyper-parameters.",
            "strength_and_weaknesses": "Strength\n- The algorithm seems to be very complex but the authors are able to provide a very clean theoretical result.\n- I can imagine that analyzing an FL algorithm with both compression and asynchrony must be complicated. The authors successfully manage the complex derivation process.\n- Most previous analyses on gradient compression used non-standard assumptions. But the authors are able to remove/replace them with standard ones.\n\nWeakness\n- The motivation of this paper is a bit weak. In the introduction, the authors start the paper by saying that both compression and asynchrony are needed for practical FL. This is true. However, both techniques have been studied in previous literature. We already get answers to questions like how to apply asynchronous training in FL, and how to use gradient compression in FL. These two techniques are orthogonal to each other and might be easily combined together. It is unclear to me what are the key algorithmic challenges here. If simply combining them does not work, then the authors need to provide some theoretical or empirical evidence. But I didn't find any. If simply combining asynchrony and compression works, then the authors are supposed to change the introduction and motivate the paper mainly from a theoretical perspective.\n- It is unclear to me how the asynchrony works in the proposed algorithm. At each round, the server needs to randomly select $s$ clients. Then, immediately after receiving the server request, the clients need to upload their local models. That means these clients should already start local training at an earlier time. Then, what if some clients never participate in training before? It seems that the authors assume that at any time all clients are performing local training, which is impractical and a waste of resources. Could the author answer the question: how many clients are concurrently performing local training at any given time?\n- The theoretical result seems to be weak. In particular, the dominant term in Theorem 4.2 is $1/\\sqrt{T}$, which does not depend on the number of all clients $n$ nor the number of sampled clients $s$. This rate is slower than vanilla FedAvg, which gets improved rates when selecting more clients.\n- It would be better to provide an update rule for the proposed algorithm so that people can easily get what the algorithm exactly does at each round.",
            "clarity,_quality,_novelty_and_reproducibility": "The theoretical analysis is solid but the conclusion is questionable. The paper is generally well-written.",
            "summary_of_the_review": "While I appreciate the efforts the authors put into the theoretical analysis, the motivation and some technical details of this paper are not satisfactory.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4245/Reviewer_oDyr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4245/Reviewer_oDyr"
        ]
    }
]