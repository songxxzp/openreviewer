[
    {
        "id": "vLsp7Gg59H",
        "original": null,
        "number": 1,
        "cdate": 1666557360172,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666557360172,
        "tmdate": 1668969076412,
        "tddate": null,
        "forum": "ueYYgo2pSSU",
        "replyto": "ueYYgo2pSSU",
        "invitation": "ICLR.cc/2023/Conference/Paper657/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper theoretically derives an in-sample offline RL algorithm, Sparse Q-Learning (SQL), using a behavior regularized MDP for offline reinforcement learning. SQL has interesting connections to existing offline RL algorithms including CQL and the in-sample IQL algorithm. SQL performs similarly to prior methods on MuJoCo and Antmaze tasks and better on Kitchen tasks in D4RL. SQL is shown to be superior on custom smaller datasets than CQL as well as more stable than IQL on mixture datasets containing expert and random data.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is well written, especially the theoretical section.\n- The proposed SQL method is theoretically well-motivated. The theory also points out interesting connections to existing methods including CQL and IQL. Furthermore, SQL only requires one hyperparameter as opposed to two in IQL.\n- The implicit value framework derived in the framework seems like a general framework for instantiating in-sample Q-learning methods based on $\\alpha$-divergences (although most methods might turn out to be intractable).\n\nWeaknesses\n\n- The improvements on the D4RL benchmark do not seem to be statistically significant -- Reported results are likely within 1 standard deviation of baseline results and thus the SOTA claim may not be valid.\n   - Error bars are not reported for any of the baselines in Table 1 and 2 -- however, the standard deviation is known to be large for D4RL datasets. See https://openreview.net/pdf?id=Y4cs1Z3HnqL for a paper that reports standard deviation for some of the baseline methods.\n   -  The proposed method seems more complex to implement than existing methods such as IQL, while resulting only in marginal gains. \n-  Possibly Unfair Comparisons: It seems that per-task hyperparameter tuning is done for SQL while the baseline methods' results seem to be copied from prior papers which used the same hyperparameter for a given domain (Kitchen / Antmaze / MuJoCo).\n- It's not clear whether \"sparsity\" plays an important role in performance of SQL -- this is not explored much in the paper despite the title.\n-  \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clear and easy to read.\n\nQuality: Fair. The paper could do a better job of demonstrating the benefits of SQL over IQL.\n\nOriginality: Fair. The theoretical results are novel and interesting, however the empirical method is highly motivated by the in-sample Q-learning (IQL) method. \n\nReproducibility: There are not enough details in the appendix to easily replicate the results on D4RL -- This possibly can be fixed if the code is open-sourced. ",
            "summary_of_the_review": "Overall, the paper presents theoretically well-motivated method for addressing the ad hoc choices in IQL. However, the method doesn't seem to offer much empirical gains over IQL on the standard benchmarks. However, other than robustness results on mixture datasets (noisy data regime in Section 5.1), it is unclear whether SQL offers any benefits over IQL in terms of practical benefits. \n\nQuestions / Suggestions:\n- Can you provide empirical evidence for whether SQL provide any benefits over IQL in the small-data regime in Section 5.2?\n- In-line with the best practices for evaluation, I'd recommend the use of [rliable](https://github.com/google-research/rliable) library [1] to validate the claim for SOTA results. Furthermore, standard deviation be reported for all methods in Table 1.\n- Does the method scale to high-dimensional image-based datasets such as the Atari datasets in RL Unplugged [2]? \n- Kumar et. al (2022) showed degradation in CQL performance with prolonged training (including on the Antmaze datasets) -- would SQL  provide any benefit compared over CQL / IQL due to being a principled approach for in-sample learning?\n- Are there any guidelines to set the $\\alpha$ parameter -- given that SQL is theoretically motivated, it would be nice if the hyperparameter tuning is easier / more intuitive than existing methods like CQL / IQL.\n\nMinor:\n- Can you clarify how Jensen's inequality is applied in Equation 2?\n- The generality of the IVR framework would be seen if another value of $\\alpha$ was instantiated to derive in-sample version of another existing method or deriving a new method altogether.\n\n\nReferences:\n\n[1] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34, 29304-29320.\n\n[2] Gulcehre, C., Wang, Z., Novikov, A., Paine, T., G\u00f3mez, S., Zolna, K., ... & de Freitas, N. (2020). Rl unplugged: A suite of benchmarks for offline reinforcement learning. Advances in Neural Information Processing Systems, 33, 7248-7259.\n\n[3] DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization (2022). ICLR. https://openreview.net/forum?id=POvMvLi91f\n\n\n-------\n----------  Post rebuttal ---------- \n\nBased on authors's response as well as the updated paper, I am now in favor of acceptance and updated the score to 8 (accept) from 5 (below accept).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper657/Reviewer_HMTv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper657/Reviewer_HMTv"
        ]
    },
    {
        "id": "x4KXQRrJXt",
        "original": null,
        "number": 2,
        "cdate": 1666593051926,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593051926,
        "tmdate": 1670644528588,
        "tddate": null,
        "forum": "ueYYgo2pSSU",
        "replyto": "ueYYgo2pSSU",
        "invitation": "ICLR.cc/2023/Conference/Paper657/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces Implicit Value Regularization (IVR) framework, provides some interesting theoretical results that might help better understand offline methods such as CQL and IQL, and then under this framework, a new offline RL method called Sparse Q-learning (SQL) is proposed. \n\nEmpirical results from D4RL benchmark shows a consistent superior performance over competing algorithms. Authors also provided some other analysis on SQL and also compared to IQL on noisy data regime and CQL on small data regime (mixed datasets made from D4RL datasets). ",
            "strength_and_weaknesses": "Strengths:\n- Overall good writing quality and clarity. \n- Interesting theoretical results that might help better understand offline methods that rely on some form of regularization terms. \n- Empirical results on D4RL and other 2 comparisons seem to show SQL has stronger performance than alternative methods, and some of these methods are recent methods with good performance. \n- Technical details provided for reproducibility. \n- a lot of discussion on related works, easy to understand how it relates to prior works\n\nWeaknesses:\n- Thank you for providing technical details including your hyperparameter search range. However, I am a bit concerned that it seems you performed a hyperparameter search on each dataset individually, and then select the best-performing one. This makes sense, but for the methods you compare to, are they using the same hyperparameter for all tasks, or do they also search hyperparameter on each individual dataset and report the best one? \n- Similarly, in the case when you compare SQL to CQL and IQL on the noisy and small data regime settings, did you also do a thorough hyperparameter search for IQL and CQL? \n- Maybe I missed sth but have you provided empirical evidence and figures to demonstrate that your method has a more \"sparse\" policy than the other competitive methods? \n- What is the final hyperparameter you selected for each individual dataset? (You mentioned the best is selected for each dataset, would be good if you also report the actual numbers)\n- What about computation efficiency? How fast in wall-clock speed does your method compare to others? \n- Hyperparameter sensitivity: for example can you provide some results on how performance changes with different alpha values? This will help other researchers understand how hard it is to tune your method on new tasks. \n\nNot really important: \n- some minor typos\n\nOther Questions: \n- Will your code be open sourced? ",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: Overall writing is clear and presentation is good. \n- Quality: Overall paper quality is good. \n- Novelty: The new method is not that different from existing methods, however, the design of the new method together with the theoretical contributions can be considered novel. \n- Reproducibility: Overall the method is not too complex, technical details provided and some discussion on hyperparameters provided. ",
            "summary_of_the_review": "Overall seems a solid paper, some interesting results and theoretical analysis. Overall writing is good. \n\nMy main concern: \n- Additional technical details and whether other competing methods also had thorough hyperparameter tuning on each task should be provided, also the computation efficiency comparison, and final hyperparameters for each dataset would be good to have in the paper. \n- I'm looking for a bit more analysis on your method, for example on the sparsity thing\n\nI'm willing to increase my score if concerns are fully addressed and if anything is wrong in my comments please point it out. \n\n=============== post rebuttal ===============\nSorry for the late reply, I thank the authors for their effort in addressing the concerns and providing the additional results. The new results seem to be more convincing. I have also checked other reviewers' comments and it seems most of the major concerns are addressed by the rebuttal. Based on that I increase my score to 8.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper657/Reviewer_u1bC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper657/Reviewer_u1bC"
        ]
    },
    {
        "id": "0K1pMYWBvtm",
        "original": null,
        "number": 3,
        "cdate": 1666701716866,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701716866,
        "tmdate": 1666701716866,
        "tddate": null,
        "forum": "ueYYgo2pSSU",
        "replyto": "ueYYgo2pSSU",
        "invitation": "ICLR.cc/2023/Conference/Paper657/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies offline reinforcement learning by proposing the Implicit Value Regularization (IVR) framework. In IVR general regularizers are added to the policy learning objective. Algorithms from prior work are analyzed in this framework showing some of their weaknesses. From these theoretical insights, a practical algorithm is derived that achieves strong results on the D4RL benchmark.",
            "strength_and_weaknesses": "The paper is well-written and does a good job in explaining relevant concepts. The theoretical analysis gives interesting insights and shows also how prior algorithms (IQL and CQL) fit into this framework. Further relevant work is nicely summarized in the context of the paper and put in comparison to the proposed method. \nThe proposed method Sparse Q-learning (SQL) is derived from the theoretical framework under reasonable and well-explained assumptions.\nSQL achieve SOTA performance on the D4RL benchmark and also strong results for the noisy and the small data regime.\n\nSome typos:\np.2 \"implicit value regularization\" -> \"implicit value regularization\"\np.2 \"IQL remains much close to the...\" -> \"IQL remains much closer to the...\"\np.2 \"...similarities to IQL However...\" \"...similarities to IQL. However...\"\np.3 In the last equation defining the policy evaluation operator for V: There should be a plus instead of a minus\np.4 \"... adds a entropy...\" -> \"... adds an entropy...\"\np.5 the sentence \" U\u2217(s) can be uniquely solved from the equation obtained by ...\" appears twice in two consecutive paragraphs.\np.9 \"This simulates the situation where the dataset is fewer and has limited state coverage near the target location because the data generation policies maybe not be satisfied and are more determined when they get closer to the target location\"  what is meant here??",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well and clearly. It gives both new theoretical insights and a practical algorithm that achieve strong performance.",
            "summary_of_the_review": "This is an interesting paper both in terms of theory and practical results. Hence, I vote for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper657/Reviewer_PvW6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper657/Reviewer_PvW6"
        ]
    }
]