[
    {
        "id": "nQ0qcmJF7oA",
        "original": null,
        "number": 1,
        "cdate": 1666579604807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579604807,
        "tmdate": 1666579604807,
        "tddate": null,
        "forum": "87n67AtiHo",
        "replyto": "87n67AtiHo",
        "invitation": "ICLR.cc/2023/Conference/Paper837/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The author studies the batchnorm's negative effect when training with quantized gradient. To address the problem, the author proposes to add an additional loss to control the variance of batchnorm. Empirical results show improvement when quantizing weight, activation and gradient.",
            "strength_and_weaknesses": "Strength:\n1. The observation of the batchnorm's negative effect on gradient explosion seems novel.\n2. The empirical results are comprehensive.\n\nWeaknesses:\n1. Does theorem 1 take weight and activation quantization into consideration? Why is the result not affected by the quantization?\n2. Corollary 1.1: should be not greater -> should not be greater\n3. Please provide a formal definition of D in Theorem 1.\n4. Eq. (4): why should $\\sigma_l$ approaches $\\sigma_*$? Theorem 1 seems to suggest that $\\eta$ should be as large as possible.\n5. From Table 4, the performance is extremely sensitive to the choice of $\\lambda$, considering the gap in Table 1 and 2 is not very obvious compared to $\\lambda$. Therefore, repeated experiments should be conducted for fair comparison in Table 1 and 2.\n6. How does it work for training full-precision networks with quantized gradients?",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation seems novel, the empirical results are comprehensive, and the paper is easy to follow.\nCodes are not provided, so the reproducibility cannot be confirmed.",
            "summary_of_the_review": "The authors make a novel observation that batchnorm may contribute to gradient explosion in FQT. But I think more comprehensive theoretical and empirical results may be needed to make it more convincing. The method proposed seem to be too sensitive to the choice of the new hyper-parameter $\\lambda$, which makes it hard to validate its better performance since the improvements in Table 1 and 2 become negligible compared with the variance in Table 3.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper837/Reviewer_GegD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper837/Reviewer_GegD"
        ]
    },
    {
        "id": "rI3FISVOt-i",
        "original": null,
        "number": 2,
        "cdate": 1666697565856,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697565856,
        "tmdate": 1666697565856,
        "tddate": null,
        "forum": "87n67AtiHo",
        "replyto": "87n67AtiHo",
        "invitation": "ICLR.cc/2023/Conference/Paper837/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors consider fully quantized training (FQT), a subfield of quantization-aware training (QAT), where even the gradients are quantized during backprop. A common issue in FQT is that training can be very unstable. \n\nThe authors study the role of batch normalization (BN) in causing training instability, and they identify BN as a significant culprit. Hence, the authors propose adding a regularization term in the loss function of models using BN, and they empirically demonstrate their approach's effectiveness.",
            "strength_and_weaknesses": "## Strengths\nThe authors propose a solution to a relevant problem, stabilizing FQT, which should be of interest to many people in the ML community. The proposed solution is simple to implement and is intuitively clear: the regularization term they propose doesn't allow the normalizing standard deviation term in BN to become \"too small\" for an appropriate notion of \"small\", which prevents instability during training. Finally, they compare their proposed approach on a reasonable selection of benchmarks and verify their method's effectiveness from different angles.\n\n## Weaknesses\n - Simply by virtue of the extremely sloppy mathematical setup, Thm 1 cannot possibly be correct. The theorem makes a statement about the variance of deterministic quantities. Halfway through the proof in the appendix, the authors suddenly assume that these quantities are random and state assumptions on them. I am not claiming that the statement of Thm 1 is \"morally wrong\" because perhaps it can be shown to be correct with a proper setup. However, as the authors present it, it is uninterpretable.\n - Given how well the authors' solution appears to perform on the benchmarks they consider, I think it would be very useful to show the performance gap between models trained with their method or that are trained without using quantization.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of writing is low, with many typos and hard-to-parse sentences. The general presentation of the paper needs significant improvement. Some of the more notable issues are:\n - \"On the other hand, theoretical calculations on the BitOps computation costs...\" - I am unfamiliar with BitOps; is it some benchmark? In any case, it should be clarified what these are or at least a reference would be needed.\n - \"Empirical data1 shows backward sometimes even costs more in practice.\" - I presume the authors mean backdrop.\n - In Eq 1, the $\\mathbf{g_{x_n}}$ terms are undefined. Are they $g_x = \\nabla_x \\hat{x}$? Also, the second and third terms involving the sums contain mistakes, as the sums use $i$ as an index, but this index is not used in the summands. What is the relevance of the earlier defined $y$ symbol?\n - In Eqs 1 and 2, what is the difference between $g$ and $g_{x}$? Is $g$ the gradient operator, and $g_{x}$ is the gradient evaluated at $x$?\n - In Eq 2, I would avoid using $\\Delta$ as it is usually reserved for the Laplacian operator.\n - \"Gradients play a crucial role in back-propagation based optimization and make a huge impact on training stability and convergence speed.\" - I don't disagree with this statement, but I wonder if the authors might actually be referring to the error in the gradients.\n - \"As the bit-width decreases, the quantization noise injected in error signal at each single layer increases exponentially.\" - This is a very confusing sentence, as it is unclear what the authors mean. In the following sentence, they explain that the error accumulates multiplicatively, and in the sentence after that, they finally explain what they mean by the exponential increase.\n - Figure 2 is not great at demonstrating the claimed exponential increase. I suggest the authors redesign it to plot bit width against gradient error since that is what they are analyzing.\n - The colour scheme in Figure 3 is confusing, especially the colour gradients for the $\\delta$ terms.\n - The \"Prior efforts reducing variances in backprop\" paragraph in Section 5 should be moved (perhaps even verbatim) to the related works section, as it doesn't pertain to the proclaimed content of the section at all.\n - The symbol $D$ is used in the text as the \"statistical variance\" operator, but the authors' whole mathematical setup is deterministic, so it is unclear what they mean.\n - The font sizes of the labels of Figures 3 - 6 should be increased as they are currently difficult to read.\n - X-axis labels are missing in Figure 3\n - Table 3 is not refereced in the main text.",
            "summary_of_the_review": "The authors propose a solution to a relevant problem and demonstrate its effectiveness empirically. However, the paper suffers heavily in terms of clarity of writing and mathematical formalism. Most importantly, it contains mathematical statements that are, at best uninterpretable and flat-out wrong at worst. Hence, the paper is unpublishable in its current form.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper837/Reviewer_FcHZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper837/Reviewer_FcHZ"
        ]
    },
    {
        "id": "GdVSFXPFsp_",
        "original": null,
        "number": 3,
        "cdate": 1666703766529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703766529,
        "tmdate": 1666703924968,
        "tddate": null,
        "forum": "87n67AtiHo",
        "replyto": "87n67AtiHo",
        "invitation": "ICLR.cc/2023/Conference/Paper837/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the problem of quantized training where the gradients are quantized as well in addition to quantization aware training methods. The paper builds up on an interesting observation related to the detrimental effect of batch normalisation in quantized training. It is shown that batch norm amplifies the accumulated gradient quantization noise during the backpropagation. Based on this observation, a rectification method is proposed to reduce the negative effect of accumulated gradient quantization noise.",
            "strength_and_weaknesses": "Strengths:\n- The paper is easy to follow and well-written.\n- There has been a lot of research focused on quantization aware training methods in the literature but this paper aims to solve the challenging task of quantized training. It is an important problem to increase training efficiency and this paper makes a good attempt at it.\nThe proposed method achieves improvements consistently on multiple datasets.\n\nWeaknesses:\n- The computational overhead of the approach has been shown to be not significant but it would be useful if the authors put some empirical comparisons on training time of the proposed method vs the baselines.\n- In table 1, the results for Mobilenet-V2 are considerably worse than UI8 and DAINT8. Can the authors explain the reason why the proposed method is worse in that setup?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is easy to follow and clearly written.\nQuality and Novelty: Technically solid and proposed approach is novel to the best of my knowledge. The empirical results are ok but not too strong.\nReproducibility: I did not find the code with the paper. I would suggest the authors to release the code for the sake of reproducibility during the review process.",
            "summary_of_the_review": "Overall the paper makes a good attempt at the problem of quantized training and the proposed idea seems novel as well as technically solid to me. Though the results are not particularly strong. Still I would recommend weak acceptance at this stage.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper837/Reviewer_dU93"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper837/Reviewer_dU93"
        ]
    }
]