[
    {
        "id": "_J8jduOhap",
        "original": null,
        "number": 1,
        "cdate": 1666548051108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548051108,
        "tmdate": 1666548051108,
        "tddate": null,
        "forum": "lid14UkLPd4",
        "replyto": "lid14UkLPd4",
        "invitation": "ICLR.cc/2023/Conference/Paper3647/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper first discusses out-of-combination scenario, which is a special case of out-of-support scenario. Then the paper propose to use anchor points to convert OOS case to OOC case for improving generalization.",
            "strength_and_weaknesses": "Strength:\n1. Detailed analysis of different o.o.d scenarios.\n2. Under certain constraints, the proposed method of using anchor points works in improving generalization performance.\n\nWeaknesses:\n1. It looks like to me the proposed method only works for a very limited scenario, in which o.o.s can be easily converted to o.o.c. For example, in the tasks chosen by authors, the data is almost stationary with respect to the anchor point, meaning the data distribution now is only on delta x. I wonder if any real-world o.o.d problems are like this. For example, in image classification task, the test images can be o.o.d because of unseen lightning conditions, unseen camera angles, unseen background and etc. I wonder if the proposed method can be extended to any more complex datasets where the stationary property does not hold true.\n2. The discussion of o.o.c condition is a bit loose. For example the theorem on page 5 (what is an informal theorem anyway) requires the condition to be as depicted in figure 3. I think assumptions in theorem should be defined more rigorously than referring to a figure.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is in general clearly written. Discussion in o.o.c can be improved with more rigorous discussion. In general quality is good. Not reproducible as code is not provided.",
            "summary_of_the_review": "The paper has some novelty in recognizing a special case of o.o.s generalization and propose a method to tackle this case. However it is not clear to me how much percentage of real-world o.o.d problems belong to this special case. The proposed method seems to require strong pre-condition ( stationary data distribution w.r.t anchor points) to work",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3647/Reviewer_gXKu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3647/Reviewer_gXKu"
        ]
    },
    {
        "id": "ZSo-jWeNz-c",
        "original": null,
        "number": 2,
        "cdate": 1666690568809,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690568809,
        "tmdate": 1666690568809,
        "tddate": null,
        "forum": "lid14UkLPd4",
        "replyto": "lid14UkLPd4",
        "invitation": "ICLR.cc/2023/Conference/Paper3647/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the issue about the mismatch between $\\mathcal{D}\\_{test}$ and $\\mathcal{D}\\_{train}$ (i.e., out-of-distribution (OOD)). Different from a common OOD situation that the ratio between the density function of $\\mathcal{D}\\_{test}$ and $\\mathcal{D}\\_{train}$ is bounded, the authors focus on out-of-support where the density ratios are not bounded. To solve such a problem, the authors propose a transductive re-parameterization by introducing an anchor point $x'$ for a query point $x$. By leveraging the techniques from low-rank matrix completion, the authors provide a guarantee for OOS extrapolation using such transductive re-parameterization under certain conditions.",
            "strength_and_weaknesses": "Strength: The topic is interesting. The result seems novel and convincing. The presentation is relatively clear.\n\nWeaknesses: Some places may need more explanation and justification. I have some detailed questions as follows.\n\n1. The theorem in Section 3.2 is derived by leveraging the techniques in matrix completion. I wonder what are the key differences and similarities between this paper's result and the matrix completion result.\n\n2. If I understood correctly, the proposed transductive re-parameterization needs $(x-x')$ and $x'$, so the training is on every pair of $x'$ and $x$. Will this cause some complexity issues during training when the number of training samples is large (as the number of pairs is almost the square of the training samples)?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is novel and clearly written. The overall quality of this paper is high.",
            "summary_of_the_review": "I like this paper in general, due to the novelty of the result and the clearness of representation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3647/Reviewer_WsKo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3647/Reviewer_WsKo"
        ]
    },
    {
        "id": "eXuASSzuU5",
        "original": null,
        "number": 3,
        "cdate": 1666895580604,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666895580604,
        "tmdate": 1670806652435,
        "tddate": null,
        "forum": "lid14UkLPd4",
        "replyto": "lid14UkLPd4",
        "invitation": "ICLR.cc/2023/Conference/Paper3647/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Paper tackles the out-of-support extrapolation problem where the input at test is out of the support of the training distribution. A bilinear model is trained to predict the output $h(x)$ using $\\bar{h}(x\u2019, \\Delta x)$ where $x\u2019$ is some chosen anchor point and \\Delta x is the distance of $x$ to the anchor point. Given an unseen test point, the model is able to extrapolate (in certain situations) by using an anchor point seen in training and the distance from it. \n",
            "strength_and_weaknesses": "Out-of-support extrapolation is an interesting but challenging task and the paper provides a novel way of  looking at the problem via a reformulated out-of-combination task. \n\nMy major concerns:\n\n1. While the goal of the work is to explore a \u201cgeneral setting does not assume apriori access to distribution shift or model class\u201d, I believe that their assumption of bilinear transduction is essentially assuming apriori knowledge about the type of distribution shift or model class. While the assumption is weaker than translation invariance/equivariance, it is still strong and is able to extrapolate only in very specific cases. \n\n2. Many of the experiments (periodic functions, push/reach and other robotic manipulations) only require translation invariance/equivariance for out-of-support extrapolation but such models are not compared with. For point-cloud tasks, out-of-support extrapolation requires rotation/translation/scale symmetries but it is hard to judge if these symmetries are learnt automatically from the method as the input to the model is not the point cloud itself, but low dimensional PCA features and the mean of all the points. Adding these symmetries for point-cloud tasks has been explored a lot (e.g., [1]) and should be compared with. \n\n3. I think the approach will have problems with high-dimensional inputs because modeling of each dimension independently is not accurate for most high-dimensional inputs (like images) unless one uses PCA. If PCA or other such techniques is required for the approach, I think this should be discussed along with their approach. \n\n\n\nOther questions/comments:\n\n1. The method is related to translation symmetry due to the $\\Delta x$ term and is able to learn translation invariance/equivariance. But it is not clear to me how much more general it is without using additional data augmentations or PCA. For example, can one expect extrapolation to other symmetries (rotation, scaling) without data augmentations and PCA? \n2. Most of the experiments consist of symmetry due to which extrapolation is possible. What are other types of out-of-support cases that can be solved with the current approach but do not fit into translation/rotation/scale invariance/equivariance?\n3. Please add the difference between train and test distributions for each experiment in the main text. \n\n4. In Meta-world and Adroit benchmarks, please compare with a translation equivariant model. \n5. For point cloud classification, please compare with methods that use the point clouds as inputs directly and not the low dimensional features. \n6. For Slider experiments, it would be interesting to see the ground truth slider torque dynamics for different object masses to see the difference between train and test, and understand how the method extrapolates. \n\n\n\nReferences:\n\n[1] Thomas, Nathaniel, et al. \"Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.\" *arXiv preprint arXiv:1802.08219* (2018).\n\n**Post-rebuttal**\nThe authors have adequately addressed my concerns and I am increasing my score. As in my original review, I think the paper provides a novel way of looking at the challenging out-of-sample problem, is well-formulated and easy to read. After rebuttal, authors have convinced me that the approach empirically extrapolates in many different settings without just using equivariance. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to read. The reformulation of an out-of-support problem to a potentially easier out-of-combination problem is novel and interesting. \n\n",
            "summary_of_the_review": "While the goal of the work is to explore a \u201cgeneral setting does not assume apriori access to distribution shift or model class\u201d, I believe their assumption of bilinear transduction is still strong and is able to extrapolate only in very specific cases. Most of the experiments essentially require symmetries to solve the out-of-support task (for example translation/rotation/scale invariance/equivariance) but such invariant/equivariant methods are not compared with. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3647/Reviewer_dNKc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3647/Reviewer_dNKc"
        ]
    }
]