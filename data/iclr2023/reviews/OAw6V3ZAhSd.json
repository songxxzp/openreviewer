[
    {
        "id": "X54OOcDPbg",
        "original": null,
        "number": 1,
        "cdate": 1666262043621,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666262043621,
        "tmdate": 1669809461371,
        "tddate": null,
        "forum": "OAw6V3ZAhSd",
        "replyto": "OAw6V3ZAhSd",
        "invitation": "ICLR.cc/2023/Conference/Paper4406/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an extension of the deep operator network (DeepONet), originally proposed in (Lu et al., 2019). The DeepONet is a way to learn operators, i.e., mappings between two functions. It takes a set of values from the first function (called sensors) and a query from the second function in input, and returns the value of the second function evaluated on the query point.\n\nThe original DeepONet framed this as learning an inner product between the representation of the sensors (branch) and the representation of the query. This paper instead proposes to parameterize directly a second neural network with the branch network, inspired by hypernetworks. They showcase on two experiments that this allows to reduce the complexity of the network. They also provide a number of theoretical insights on the reason.",
            "strength_and_weaknesses": "I premise this review by saying I am not an expert on the field of operator learning, and I had to read the DeepONet paper to provide some judgment. In particular, I was not able to follow the theoretical derivations in Section 4 completely.\n\nThe setup reminds me of [1], where they show a Transformer network can do Bayesian inference; in both cases, the network at test time is provided with a dataset and a query point and learns to do inference by exploiting a transformer.\n\nFrom what I understand, the connection made here with the hypernetwork is interesting and the paper does provide a strong theoretical / empirical motivation for replacing the original DeepONet with this variant. On the experimental part, I think more details on the actual architectures (apart from saying \"fully-connected networks\") should be provided.\n\nThe paper is not particularly easy to follow without reading (Lu et al., 2019) first. For example, \"sensor values\" are mentioned multiple times but only defined in the middle of the paper. Similarly, \"p-coefficients and p-basis\" is unclear in the introduction. In general, the motivation for using this architecture as compared to, e.g., simply concatenating the sensors values and the query point is not provided.\n\n[1] https://arxiv.org/pdf/2112.10510.pdf ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper requires a strong language editing because many sentences are poorly worded or unclear. I would also suggest to redo Fig. 1 to remove the upper-left component (which is very vague) and only focus on the different variants. The paper also mention related works in different points (including Section 1 and 2), I suggest to unify the discussion of related works and leave it mostly outside of the introduction.",
            "summary_of_the_review": "My suggestion from reading this paper and the original DeepONet paper is that the contribution appears to be valuable, although the paper is not easy to follow and the experimental part appears weak if compared to the DeepONet paper. However, for the reasons mentioned above, this is only a partial evaluation and I cannot trust there are similar papers in the state-of-the-art, or the theoretical derivation has points that I was not able to evaluate carefully.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4406/Reviewer_eaJx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4406/Reviewer_eaJx"
        ]
    },
    {
        "id": "LpbSMLn1tWI",
        "original": null,
        "number": 2,
        "cdate": 1666673162480,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673162480,
        "tmdate": 1669184081749,
        "tddate": null,
        "forum": "OAw6V3ZAhSd",
        "replyto": "OAw6V3ZAhSd",
        "invitation": "ICLR.cc/2023/Conference/Paper4406/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper builds on the deep operator network (DeepONet) for learning operators between function spaces, for examples for PDE solving. This paper proposes HyperDeepONet which replaces the network with a hypernetwork, in other words making the target function input-dependent. This method is shown theoretically to be more parameter efficient and improves on related baselines on synthetic experiments.\n",
            "strength_and_weaknesses": "Strengths:\n- Well-explained problem setup and background\n- Method is well motivated\n- Method comes with theoretical results for both upper and lower bounds (Theorem 1 and 2)\n- Well-rounded empirical study including metrics, visualization of predictions, and several baselines and ablations\n\nWeaknesses:\n- Evaluation is very small-scale, consisting of largely of synthetic data and simple PDEs, and is hard to tell whether the proposed method can be of practical significance on more interesting problems. While results look promising, there are ultimately very few quantitative results and it is hard to evaluate the efficacy of the method.\n- A discussion of the tradeoffs of the proposed method is missing (i.e. potential weaknesses). For example, when the network is more complicated as in HyperDeepONet, how is the computational speed compared to a parameter-matched DeepONet?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite clear, and the background, problem setup, and related methods are well explained. While the proposed methodological improvements are not particularly novel - they ultimately amount to small changes in the neural network parameterization - it is well motivated and comes with theoretical results. Code is provided for reproducing experiments.\n\nThe paper's significance and potential impact could be limited by the niche nature of this field. While the problem setup is clear from a formalism perspective, one suggestion that could make the paper better motivated and easier to read is to provide a simple running example throughout the paper about the problem setup and why these operator learning problems are important.\n\nMinor: sometimes the word \"complex\" is used in a confusing manner where the context lends itself to be interpreted as \"complex as in $\\mathbb{C}$\" when the authors seem to mean \"complex as in complicated\". For example, the second sentence of Section 5: \"To be more specific, we focus on operator learning problems in which the space of output function space is complex.\"\n",
            "summary_of_the_review": "This paper provides an improvement to the deep operator network approach to operator learning, which is well motivated and has promising empirical results.\n\n----------\nPost rebuttal:\nThe authors have provided new content, figures, and experiments addressing my concerns and overall improved the presentation of the paper. I am increasing my score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4406/Reviewer_8Dyr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4406/Reviewer_8Dyr"
        ]
    },
    {
        "id": "U2iM_uZt7R",
        "original": null,
        "number": 3,
        "cdate": 1667223810335,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667223810335,
        "tmdate": 1668929403853,
        "tddate": null,
        "forum": "OAw6V3ZAhSd",
        "replyto": "OAw6V3ZAhSd",
        "invitation": "ICLR.cc/2023/Conference/Paper4406/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper targets a cutting-edge research problem that tries to use deep neural networks to solve partial differential equations (PDEs) through operator learning, with the potential to achieve faster and/or more accurate predictions for complex physical dynamics than traditional numerical methods. Although previously proposed methods (e.g., DeepONet and its variants) achieve some success, due to the limitation of linear approximation investigated by recent work, they typically need a large number of parameters and computational costs to learn the complex operators. In order to solve the problem, the authors propose HyperDeepONet, which learns complex operators with a target network whose parameters are generated by a hypernetwork (conditioned on the input function $u(\\cdot)$). Both theoretical and empirical evidence is provided to demonstrate the lower complexity and effectiveness of the proposed method.",
            "strength_and_weaknesses": "**Pros:**\n\n* The motivation is clear, and the idea of leveraging a hypernetwork to generate parameters of the target network is interesting.\n* The authors analyze DeepONet with its variants from the perspective of: \"how information from the input function $u$ is injected into the target network $\\mathcal{G}_{\\theta}(u)$\" as illustrated in Figure 2. It is quite clear and makes connections among these methods, including HyperDeepONet.\n* Thorough theoretical analysis of the complexity is provided (but I cannot follow this part).\n\n**Cons:**\n\n* The writing of this paper could be further polished. Many typos also exist.\n\n* About the experiment section: I do not see the results of FNO [1], which also belongs to operator learning. Could the authors explain the reason? Moreover, A PDE benchmark [2] has been proposed recently. It would be better to provide some experimental results on it (the second half does not affect my evaluation since it was proposed recently).\n* The scalability of the proposed method seems to be an issue. This paper mainly considers the complexity of the target network. It would be better to consider the complexity of the branch network (also the hypernet) as well in the proposed method (i.e., the complexity of the Hyper will increase as that of the target network increases). \n\n**Question:**\n\n* About the experiment section, \n  * in Figure 7 and 8, could the authors explain why increasing the number of layers in the branch net does not reduce the $L^2$ error of DeepONet. For the HyperDeepONet, why it may even increase the error? What is the intuition behind this phenomenon?\n  * when considering the whole complexity (i.e., the same total Params in table 2), the performance of Hyper seems to be inferior to that of DeepONet. How about the performance of other baselines? \n* The authors mention that real-time prediction on resource-constrained hardware is crucial and challenging, so for each method, how about the training time, and inference time to obtain a desired prediction?\n\n**Minor:**\n\n* Please explicitly define the abbreviation when they first appear, e.g., PDEs on Page 1.\n\n* \"shows\" -> \"show\"; \"small\" -> \"a small\" in point 3 of main contributions.\n* \"solving\" -> \"solve\"; \"nonlinear\" -> \"a nonlinear\" in the second paragraph of related work.\n* \"considered\" -> \"considered as\" in section 3.3.\n\n[1] Fourier neural operator for parametric partial differential equations.\n\n[2] PDEBench: An Extensive Benchmark for Scientific Machine Learning.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Fair. The writing of this paper could definitely be further improved.\n\nQuality: Good. The idea is interesting, along with theoretical and empirical evidence to support it.\n\nNovelty: I'm not an expert in this area, so I cannot fairly evaluate the novelty of this paper.\n\nReproducibility: Good, the authors provide the code of this paper.",
            "summary_of_the_review": "Overall, this paper focuses on a significant research problem and proposes an interesting method to make learning complex operators with lower complexity (e.g., fewer parameters) possible. Well-rounded theory and empirical results are provided. I believe this work could be further improved with better writing and further evaluation. Based on the current version, I recommend a borderline acceptance.\n\n**Update on 04 Nov after reading two other reviews:** I agree with the other two reviewers, and my main concerns about this work are: a) the writing is not good (*eaJx*); b) the evaluation is not strong (*8Dyr*). If the authors could resolve these two concerns and update the draft accordingly before 18 Nov, I would be happy to recommend this work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4406/Reviewer_LzY8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4406/Reviewer_LzY8"
        ]
    }
]