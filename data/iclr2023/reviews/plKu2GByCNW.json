[
    {
        "id": "i4dS3DZT2j",
        "original": null,
        "number": 1,
        "cdate": 1666179726513,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666179726513,
        "tmdate": 1666179726513,
        "tddate": null,
        "forum": "plKu2GByCNW",
        "replyto": "plKu2GByCNW",
        "invitation": "ICLR.cc/2023/Conference/Paper924/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper designed a simple yet effective adapter module that helps plain ViT behave well for dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Benefited by the inductive bias introduced by the spatial prior module and feature interaction module, the plain ViT archives remarkable performance on dense prediction tasks.",
            "strength_and_weaknesses": "#### Strength\n\n1. Promising performance. ViT-Adapter achieves remarkable performance on several dense prediction benchmarks, using the plain ViT model.\n\n2. The paper is clearly written and easy to follow.\n\n3. Extensive experiments. The experiments are solid and comprehensive.\n\n\n\n#### Weakness\n\n1. Only the number of parameters is reported in the table. More metrics such as FLOPs and inference latency are more important.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. It is unclear about ViTDet's performance in Table 1. It seems that the original paper of ViTDet reported higher numbers.\n\n2. The paper described introduced modules clearly. I think it is easy to implement.",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper924/Reviewer_Ycr4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper924/Reviewer_Ycr4"
        ]
    },
    {
        "id": "izzo9aSu2QG",
        "original": null,
        "number": 2,
        "cdate": 1666408346394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666408346394,
        "tmdate": 1666683512962,
        "tddate": null,
        "forum": "plKu2GByCNW",
        "replyto": "plKu2GByCNW",
        "invitation": "ICLR.cc/2023/Conference/Paper924/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed ViT-Adapter for dense prediction task. Essentially, Vit-Adapter utilizes additional modules to inject the image prior into ViT, encouraging ViT to learn transferable representations for the specific dense prediction tasks. The extensive experiments on several dense prediction tasks (e.g., object detection, instanve segmentation, and semantic segmentation) demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n+ The paper is written-well and easy to follow, the idea is intuitive and clear.\n+ The experiments are solid and entensive.\n+ The authors have made good efforts to ablate different components of their approach.\n+ The performances improvement of the proposed method are impressed.\n\nWeaknesses:\n- Adapters in ViT is an interesting research topic, there were some works focused on it  [a-e]. I strongly encourage authors to take them into deep discussion. For example, the differences and advantages between them. The additional fair experiments should be  also used for discussion.\n\n[a] \"Visual prompt tuning.\"\u00a0In ECCV, 2022.\n\n[b] B\"Exploring visual prompts for adapting large-scale models.\" arXiv, 2022.\n\n[c] \"AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition.\" In NeurIPS, 2022.\n\n[d]  \"Neural Prompt Search.\"\u00a0arXiv, 2022.\n\n[e] \"Convolutional bypasses are better vision transformer adapters.\" arXiv, 2022.\n\n- Authors inject spatial priors and multi-scale feautres learning into ViT and achieves significant performances. I doubt that can we add the other priors or learning tricks into ViT to achieve better performances. That is, the module in Vit-Adapter can be replaced by others that is important for dense prediction tasks? Can authors provide additional ablation studies on it.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, novelty and reproducibility of this paper are good.",
            "summary_of_the_review": "This work is interesting and written-well, but some of concerns needed to be improved. I'm willing to raise my rating score if authors carefully tackle my concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper924/Reviewer_hMeh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper924/Reviewer_hMeh"
        ]
    },
    {
        "id": "hlsRZ8ommEb",
        "original": null,
        "number": 3,
        "cdate": 1666680461232,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680461232,
        "tmdate": 1666680461232,
        "tddate": null,
        "forum": "plKu2GByCNW",
        "replyto": "plKu2GByCNW",
        "invitation": "ICLR.cc/2023/Conference/Paper924/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The application of plain ViT on dense prediction tasks suffers from unacceptable complexity. The paper proposes an injection adapter with multi-scale features to use a well-pretrained ViT model and transfer it to downstream task. The experiments are conducted on object detection, instance segmentation, and semantic segmentation. The proposed technique achieves state-of-the-art on COCO dataset, which well demonstrate the high effectiveness.",
            "strength_and_weaknesses": "Strength:\n+ Applying the plain ViT to downstream tasks is an improtant and meanful topic. The motivation is well explained and makes sense. The proposed technique is simple yet effectiveness.\n+ The paper is well-written and well-organized. The figures and visualizations are clear and easy to follow.\n+ The experiments and ablation studies are comprehensive. The proposed adapter shows significant superiority over the previous works.\nAnd it achieves state-of-the-art performance on several tasks.\n\nWeakness:\n- It would be better to give the computational complexity and latency in the experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is clear. The novelty and the reproducibility are good.",
            "summary_of_the_review": "This paper explores an important problem in multimodality and proposes a simple yet effective solution to transfer the ViT models to dense prediction tasks. The experiments give strong evidences to well support.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper924/Reviewer_SVHK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper924/Reviewer_SVHK"
        ]
    },
    {
        "id": "4azeCiM-5hQ",
        "original": null,
        "number": 4,
        "cdate": 1667362025247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667362025247,
        "tmdate": 1667362025247,
        "tddate": null,
        "forum": "plKu2GByCNW",
        "replyto": "plKu2GByCNW",
        "invitation": "ICLR.cc/2023/Conference/Paper924/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work attempts to address the problem of lack of inductive bias in plain ViT architectures which hinders its performance for dense prediction tasks such as semantic segmentation and object detection. Specifically, the authors propose an adapter architecture that can extract and learn representations that are useful for dense prediction tasks. The proposed methodology has been trained and tested on different down-stream tasks and purportedly achieves competitive performance on COCO test-dev dateset in terms of box AP and mask AP. ",
            "strength_and_weaknesses": "########## Strength\n\n1. The paper is well-written and easy to follow. Visualizations properly demonstrate the concept of the proposed architecture. \n\n2. The overall design of the ViT-adapter architecture is quite general and can be used in different applications. Especially, using conv-based prior modules seem to be effective and play an important role. Unlike the previous ViTDet model, the adapter seems to also make use of the input image which can be effectively leveraged along with intermediate extracted features. In addition, the idea of using both conv and transformer-based (i.e. encoders with MLP and cross attention) layers in the adapter seems to be more optimal than using conv-based pyramid-like architectures. \n\n3. The method has been thoroughly tested on many different tasks (both supervised and self-supervised) and achieve SOTA performance on COCO test-dev dateset --although there seems to be other approaches such as FD-SwinV2-G [1], DINO [2], etc. to have better performance, but the performance is still quite competitive. \n\n########## Weakness\n\n1. Despite the effective design of the proposed architecture, the idea of the ViT-adapter is not very new. The first instance of such works was SETR [3] which simply used a CNN-based decoder for feature processing in segmentation. \n\n2. As the name indicates, the architecture is solely designed for ViT architectures, and this may limit its applicability as hierarchical models such as Swin Transformer have gained a lot interest. It is fair to say that with minimal changes, the adapter could be used with Swin and other similar architectures. Hence it could be beneficial to study whether it could be useful when the feature extractor is more powerful. \n\n3. How does the adapter affect the throughput and memory foot-print ? I am concerned that it may not be feasible to use it in practical applications due to it complex architecture. It would be nice to have comparison to SETR and ViTDet for throughput. \n\n\n[1]: Wei, Y., Hu, H., Xie, Z., Zhang, Z., Cao, Y., Bao, J., Chen, D. and Guo, B., 2022. Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. arXiv preprint arXiv:2205.14141.\n\n[2]: Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.M. and Shum, H.Y., 2022. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605.\n\n[3]: Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P.H. and Zhang, L., 2021. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 6881-6890).",
            "clarity,_quality,_novelty_and_reproducibility": "The idea itself is not very novel, but the proposed architecture is interesting and have been optimized  for different applications. The code is supposed to be published on GitHub based on author's claims, hence it should be reproducible. ",
            "summary_of_the_review": "The proposed model addresses the problem of lack of inductive bias in ViT architecture and has been shown to be effective in many different application. The model has been testes rigorously on different dataset and achieves competitive performance on COCO test-dev dateset. As a result, I believe these contributions merit acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper924/Reviewer_BCxb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper924/Reviewer_BCxb"
        ]
    }
]