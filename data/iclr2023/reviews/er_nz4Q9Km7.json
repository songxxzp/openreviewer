[
    {
        "id": "iAjBXAQ30p",
        "original": null,
        "number": 1,
        "cdate": 1666217402157,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666217402157,
        "tmdate": 1666319315880,
        "tddate": null,
        "forum": "er_nz4Q9Km7",
        "replyto": "er_nz4Q9Km7",
        "invitation": "ICLR.cc/2023/Conference/Paper5622/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors study the machine unlearning problem on graphs, where they focus on the edge unlearning problem. The proposed method, EraEdge, is based on subtracting the *influence* of the unlearned edges in a heuristic manner. The experiments demonstrate the efficiency and accuracy of the proposed method. They also show that the output of the unlearned model is close to the one retraining from scratch.",
            "strength_and_weaknesses": "### Strengths\n\n- EraEdge can be applied to non-linear models such as GNNs.\n\n- The methods seem to be efficient in terms of time complexity.\n\n- The problem of machine unlearning on graphs is an important problem.\n\n### Weaknesses\n\n- The proposed unlearning definition is not rigorous and heuristic-based. Also, having approximately close model output does not guarantee privacy.\n\n- The proposed method is an ***approximate unlearning*** method ***without theoretical guarantee*** (i.e. heuristic based). \n\n- The related works about machine unlearning on graphs are not extensive enough. Also, differential privacy based GNNs should also be discussed.\n\n- In the experiment, the authors use random Gaussian vectors as node features instead of the default features. How does the result look like if we use the default features?\n\n- (Minor) The paper focuses on the edge unlearning problem, while the really important problem is the node unlearning problem.\n\n### Detail comments\n\nWhile the problem of machine unlearning on graphs is very important, one should be extremely careful when they claim a method can achieve unlearning. Machine unlearning can be roughly characterized into two categories, exact and approximate unlearning. For exact unlearning methods, we require the unlearned models to be ***identical*** (in distribution) to the one retraining from scratch, examples include sharding-based method as mentioned by the authors. Approximate unlearning requires the unlearned model to be *indistinguishable* from the one retraining from scratch. Note that one should be extremely careful when defining the *indistinguishability*, as an inappropriate definition could lead to zero privacy in some cases (see [1] for a simple example). Apparently, the proposed definition of unlearning belongs to approximate unlearning and the authors should clearly specify it. Otherwise, the paper can be misleading to the readers.\n\nOne rigorous definition of approximate unlearning is via differential privacy type of definition in the parameter space, which was proposed in [1]. The proposed approximate unlearning method therein comes with a differential privacy type of theoretical guarantee. The other line of work such as [2] proposed a heuristic-based measure, which is more similar to the one proposed in this paper. However, the proposed method in [2] involves ***privacy noise*** to *blur out* the potentially leaked information. In contrast, EraEdge does not add any privacy noise to protect the privacy. Most importantly, the authors of [2] conduct extensive experiments and multiple metrics to examine the effectiveness of unlearning their method. On the other hand, the authors of this paper merely examine the closeness of the model output which can be problematic following the same rationale in the counterexample provided in [1] (i.e. the paragraph in the title *Insufficiency of parametric indistinguishability*). Hence, I doubt how private it is for EraEdge on the unlearned data. I would suggest the authors conduct similar extensive experiments to [2], or examine the ability of the proposed method against some membership inference attack methods.\n\nRegarding the related works, the authors miss several recent papers about machine unlearning on graphs. Please check the survey paper [3] for a collection of papers about machine unlearning on graphs, where the node unlearning problem is also studied. It is also worth mentioning the recent development of differential private GNNs, as differential private models automatically achieve (approximate) unlearning *without* any update [1].\n\nIn summary, my main concern about the paper is the privacy of the proposed method. Since EraEdge is not an exact unlearning approach, one has to be extremely careful about the choice of *indistinguishability*. Also, most of the existing approximate machine unlearning methods require adding privacy noise to burr out the information from the approximation error. However, EraEdge does not leverage any privacy noise and thus any approximate error can potentially lead to severe privacy issues (i.e. adversarial attacks). In the empirical evaluation, the authors do not conduct enough experiments to demonstrate the privacy of their EraEdge. This should be done for heuristic-based methods such as those in [2]. Hence, I feel the paper needs a major revision before publishing.\n\n### References\n\n[1] Certified data removal from machine learning models, Guo et al., ICML 2020.\n\n[2] Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks, Golatkar et al., CVPR 2020.\n\n[3] A Survey of Machine Unlearning, Nguyen et al., 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The authors should specify the difference between exact and approximate unlearning. Note that their main baseline methods such as Retrain and GraphEraser are exact unlearning methods while the proposed method is an approximate unlearning method without theoretical guarantees. Hence, merely comparing the test accuracy and time complexity is not satisfied.\n\n- Quality: I have concerns about the privacy of EraEdge method.\n\n- Novelty: Most of the analysis and techniques are from the existing literature. The main novelty comes from applying them to graphs, yet the novelty of this extension used in the papers seems limited. \n\n- Reproducibility: The authors do not provide their experimental code. Details about hyperparameters are also missing.\n",
            "summary_of_the_review": "My major concern focuses on how private the proposed unlearning method is. I also think the authors should specify the difference between exact and approximate unlearning, otherwise the result can be misleading. For example, EraEdge is an approximate unlearning method while retraining from scratch is an exact unlearning method. Once we specify this difference, one can naturally ask how EraEdge *trade-off* privacy for time complexity and test accuracy. I also find the experiment setting a bit weird, where the authors replace the default node features with random Gaussian vectors. This is an artificial setting, and I wonder how the result looks like if the authors use default node features. In summary, I feel the paper needs a major revision before publishing.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5622/Reviewer_UL7T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5622/Reviewer_UL7T"
        ]
    },
    {
        "id": "cSPjEvwlO8X",
        "original": null,
        "number": 2,
        "cdate": 1666413927672,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666413927672,
        "tmdate": 1669227801984,
        "tddate": null,
        "forum": "er_nz4Q9Km7",
        "replyto": "er_nz4Q9Km7",
        "invitation": "ICLR.cc/2023/Conference/Paper5622/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies a notion of unlearning for graph neural networks. Basically, given a set of edges removed from the graph, it tries to address how to fast adjust the model parameters to make the model behave like the model retrained on the graph with edge removal while without retraining. The technical idea is to analyze the influence of the edge removal on the model parameters given the convex and differentiable assumption of the objective. Experiments show some superiority of the proposed method. ",
            "strength_and_weaknesses": "Strengthes:\n1. Graph unlearning is a relatively novel concept. The problem studies here is interesting. Also, the analysis and argument sound reasonable and solid. \n\n2. The paper is written very well. I appreciate the logic flow. The motivation and the exposition of the approach is clear. \n\nWeaknesses:\n1. Here is my biggest concern. Although I overall think the technique in this paper is reasonable and the studied problem is interesting. Unfortunately, recently, I have read a relevant paper on graph unlearning published four months ago [1], which I think has studies a far more extensive setting on graph unlearning than this work. Although that work is just an arxiv paper, I cannot view it as a concurrent work because the content studied in [1], in my opinion, is broader and provides more insights than the setting studied in this work. I know it is tough for the authors but I cannot ignore this. Therefore, this work's statement saying this is the only work that considers unlearning in GNN, which is an over-claiming. \n\nMoreover, [1] studies both node and edge unlearning, while this work only studies edge unlearning. In my opinion, node unlearning is more crucial because a user (typically corresponding to a node), if not wanting to disclose her data, will ask to remove this node from the graph. Moreover, I think [1] also tells more data insights due to their analytic bounds such as the dependence of unlearning performance on node degrees, etc. \n\nI can see some adopted detailed techniques are different, such as [1] using SGC (linear model) while this paper using convex assumptions. My feeling is that if this work may discuss both edge/node unlearning and also provides further insights on how graph structure affects the unlearning performance, I would appreciate the technical difference in this paper and may support an acceptance. However, the current setting of this paper is still kind of narrow compared to [1].\n\n2. This work writes well and is easy to follow. However, I feel there is a little bit misleading in the introduction. For example, after reading the introduction, I thought the paper would like to touch non-convex settings in theory (the fourth paragraph of intro). However, the later analysis is based on convex assumption. Moreover, in intro, the authors say \"empirical studies on tradeoff between unlearning efficiency, accuracy, unlearning efficacy\". However, in the experiments, I can only see the list of these results without tradeoff. My understanding of the tradeoff would be about, e.g., high accuracy/efficacy  requires less efficiency, and the proposed method has a work to balance these aspects. Unfortunately, I do not think the proposed approach has such flexibility. \n\n3. Moreover, I am not clear how the averaged JSD is computed. I can think of there are multiple ways to define averaged JSD. Do you mean averaging over testing samples? or averaging over classes? I think a math equation is needed to show this. \n\n4. Fig.4 the first row has wrong subtitles I think. \n\n5. Regarding experiments, how do you remove edges, randomly and how many times, or? Also, I do not see how to tune the model and how to make sure the comparison between model retraining and the proposed method fair, e.g., same model size? how about learning rate?\n\n6. Since efficiency is one topic of interest in this work, the used graphs are in general too small.  \n\n\n\n[1] Certified Graph Unlearning, Chien et al., 2022. ",
            "clarity,_quality,_novelty_and_reproducibility": "I think I have made these points clear in the above response. \n\nClarity: Generally easy to follow while some explanations on experiment settings are missed\n\nQuality: Okay, but not extensive enough given previous works\n\nNovelty: Topic is interesting while there are a few contributions being over-claimed. \n\nReproducibility: Good while I did not see how to tune hyperparameters and some detailed experiment settings are missed. ",
            "summary_of_the_review": "The studied problem is interesting. The adopted technique is reasonable. However, the novelty and true technical contributions are not sufficient given previous works.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5622/Reviewer_wUNT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5622/Reviewer_wUNT"
        ]
    },
    {
        "id": "2mbgWwbHnV",
        "original": null,
        "number": 3,
        "cdate": 1666673931425,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673931425,
        "tmdate": 1666673931425,
        "tddate": null,
        "forum": "er_nz4Q9Km7",
        "replyto": "er_nz4Q9Km7",
        "invitation": "ICLR.cc/2023/Conference/Paper5622/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "An unlearning algorithm for graph neural networks is proposed in the paper. The paper tries to find the difference between the upweighted model and the original model by utilizing a hessian-based approximation, which can be solved by investigating the corresponding linear system with conjugate gradient methods. The found difference is then added to the original model to obtain the retrained parameters.",
            "strength_and_weaknesses": "Strength\n\n1. The proposed method has a good applicability and can work with most existing variants of GNNs.\n\n2. The paper is a computationally and memory efficient algorithm.\n\n3. The algorithm is evaluated on a number of three real datasets to demonstrate the effectiveness of the proposed approach for graph unlearning.\n\nWeakness\n\n1. Upweighting using the influence function merely approximates the retrained parameters and provides no theoretical error bound on non-convex loss functions. This work then constructs another approximation of this already fuzzy target to obtain the unlearnt model. One can hardly be convinced that such a method will result in a set of parameters that resemble the retrained model. The proposed scheme also provides no means of generating a verification of data removal, which is utterly vital for a data provider.\n\n2. Adding a scaled identity matrix to the hessian to make it positive definite only accounts for the non-invertible problem. It still destroys the basis of Theorem 2, which requires the loss function to be globally convex. This assumption is too strong to make any meaningful sense in real-world scenarios. Also, Theorem 2 seems to be only an adaptation in notations of eq.(3) in [1].\n\n3. In scenarios where the graph is relatively dense and the GCN is deep (e.g., as described in [2]), the affected set of nodes, as defined in the paper, can easily become the whole graph, which sort of destroys the purpose of the paper, which is fast unlearning.\n\n4. Unlearning efficacy is only evaluated for the proposed model without comparison with other baselines, unlike classification accuracy. Also, experiments like the behavior difference between the unlearning model and the retrained model on the unlearned part of the dataset should be added to demonstrate the algorithm's efficacy further.\n\n[1] Pang Wei Koh and Percy Liang. \u201cUnderstanding Black-box Predictions via Influence Functions.\u201d In: Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017. Ed. by Doina Precup and Yee Whye Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, 2017, pp. 1885\u20131894.\n\n[2] Guohao Li et al. \u201cDeepGCNs: Can GCNs Go As Deep As CNNs?\u201d In: 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019. IEEE, 2019, pp. 9266\u20139275.",
            "clarity,_quality,_novelty_and_reproducibility": "It is understandable to a large extent, but parts of the paper need more work.",
            "summary_of_the_review": "In general, the studied problem is interesting and important. In addition, the methodology is principled with three major merits as discussed above. However, the work still has some unaddressed concerns to well justify its technical and empirical contributions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5622/Reviewer_yNN2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5622/Reviewer_yNN2"
        ]
    },
    {
        "id": "5g17Ys3Icu",
        "original": null,
        "number": 4,
        "cdate": 1666684790188,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684790188,
        "tmdate": 1666684790188,
        "tddate": null,
        "forum": "er_nz4Q9Km7",
        "replyto": "er_nz4Q9Km7",
        "invitation": "ICLR.cc/2023/Conference/Paper5622/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies machine unlearning on graph neural networks (GNNs) by analyzing influence function. The authors identify that simply applying influence function on GNNs for edge removal is problematic due to node dependency. As such, the authors propose to estimate the influence function by upweighting the set of all affected nodes. Then the influence is obtained as the reverse of Hessian matrix multiplied by the gradient vector. And conjugate gradient is applied to reduce the computational cost. Experimental results demonstrate the effectiveness of the proposed EraEdge in terms of the indistinguishability betweem model parameters and the efficiency of the proposed EraEdge in terms of running time.",
            "strength_and_weaknesses": "Strengths:\n- Unlearning on GNNs is less studied, and there is no existing works on influence function-based GNN unlearning.\n- The paper is overall easy to follow.\n- The experimental results demonstrate that EraEdge can efficiently unlearn a set of edges via the indistinguishability between the retrained model parameters and the unlearned model parameters.\n\nConcerns/Questions:\n- Many existing influence function-based approximate unlearning techniques (see references below) add Gaussian noises to preserve data privacy. I am wondering why such noise is not needed in EraEdge for data privacy?\n[1] https://arxiv.org/abs/1911.04933\n[2] https://arxiv.org/abs/1911.03030\n[3] https://arxiv.org/abs/2106.04378 \n[4] https://arxiv.org/abs/2006.14755\n[5] https://arxiv.org/abs/2007.02923\n[6] https://arxiv.org/abs/2012.13431\n[7] https://arxiv.org/abs/2110.11891\n[8] https://arxiv.org/abs/2103.03279\n- Existing works often solve Eq. (10)-like equation with Hessian-vector product (HVP). What is the key limitation of using HVP to solve Eq. (10)? If HVP can be used as well, what is the benefit of conjugate gradient over HVP?\n- I doubt that the indistinguishability in the GNN output necessarily mean the data is removed. Similar concern also appear in (Guo et al. 2020) even for linear and convex classifier like logistic regression, not to mention the nonconvexity of GNNs. It is better to study the privacy of this data removal mechanism like how they are studied in differential privacy (e.g., through membership inference attack).\n- How would EraEdge perform when we sequantially delete multiple batches of edges? In many real-world scenarios, it is uncommon that the model will be only unlearned once.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall novel and easy to follow.",
            "summary_of_the_review": "While the paper is novel in terms of its technical aspect, I have a few concerns on the technical details on the privacy of this unlearning approach. In terms of writing, the paper is clear and easy to follow. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5622/Reviewer_TMqC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5622/Reviewer_TMqC"
        ]
    }
]