[
    {
        "id": "cd8B7erTpC",
        "original": null,
        "number": 1,
        "cdate": 1666514243095,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666514243095,
        "tmdate": 1668908970306,
        "tddate": null,
        "forum": "L8iZdgeKmI6",
        "replyto": "L8iZdgeKmI6",
        "invitation": "ICLR.cc/2023/Conference/Paper477/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work is very interesting. It tries to simultaneously address domain generalization, domain adaptation and catastrophic forgetting problem when the learning model needs to tackle continual domain shifts over time, which is called Continual Domain Shift Learning (CDSL). To solve this, this work proposes a framework \u2013 Twofer, which consists of three major modules. Specifically, there is a data augmentation module that can generate more additional data to help enhance domain generalization ability. Then a novel KNN-based pseudo labeling technique is proposed for giving more accurate supervision that can be used in later adaptation. Finally, a modified prototype contrastive loss is used to align features among different domains. The proposed twofer is tested on various dataset and domain orders, with the comparison with extensive state-of-the-art baselines from continual domain adaptation, test-time adaptation, source-free domain adaptation, single domain generalization, multiple domain generalization, and even unified domain adaptation and generalization. According to experiment results, twofer can outperform these baselines on three metrics that are related to domain generalization, adaptation, and forgetting.",
            "strength_and_weaknesses": "Strength:\n\n1.\tThe idea of considering performance before adaptation in domain continual learning setting is novel and practical in the real world.\n\n2.\tThe training-free RandMix method can keep the cost low and achieve high generalization ability, which is very important in this continual domain generalization setting. In many cases, scenarios that require the use of this setting, such as surveillance cameras, have limited computing power.   \n\n3.\tOnly using a subset of features and introducing kNN for final selection in T2PL can filter out features that can cause side effects. I think the effect of this method will be more obvious when the data noisy is relatively high.  \n\n4.\tThe proposed Prototype Contrastive Alignment is novel. Instead of modeling the distribution of previous tasks and then aligning the features in existing works, PCA provides a new and more efficient way to align previous and current features by saving a small part of network parameters and reducing the domain adaptivity gap. Furthermore, this loss function is easy to be adapted to other frameworks.\n\n5.\tThe experiments show high effectiveness compared to other SOTA methods, especially when target domain is much more complicated than source domain, e.g., SVHN in Digital dataset.\n\nWeaknesses:\n\n1. The idea of RandMix looks like a simple combination of DSU and L2D. Not sure my understanding is correct or not.\n\n2. The paper introduces the kNN in T2PL without detailed explanations of why it can work. I think more analysis about it needs to be provided.\n\n3. It seems that the domain order is an important factor that influences the overall performance. In main paper, most of the average performance of three metrics is 5-10% better than the second best. But in appendix, it\u2019s only 1-2% better or even worse than baseline. Can the author(s) explain the reason why?\n\n4. In table 1, the drop in accuracy on SVHN is too large for all source free adaptation methods and makes me doubt the accuracy of the experiments. Can the author(s) explain the reason and I\u2019m also checking the experiments results in related works.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1.\tWriting is clear and easy to follow.\n\n2.\tThe setting and proposed methods are novel.\n\n3.\tSome average accuracy in Table 6 is miscalculated, please check them again.\n\n4.\tReproducibility is good, implementation code is provided.\n",
            "summary_of_the_review": "In summary, the setting is practical and the proposed methods are novel and effective. I thus recommend this paper to appear at ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper477/Reviewer_CHgo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper477/Reviewer_CHgo"
        ]
    },
    {
        "id": "sh-KVl8RUXS",
        "original": null,
        "number": 2,
        "cdate": 1666579133084,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579133084,
        "tmdate": 1666579384470,
        "tddate": null,
        "forum": "L8iZdgeKmI6",
        "replyto": "L8iZdgeKmI6",
        "invitation": "ICLR.cc/2023/Conference/Paper477/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The manuscript proposes to tackle the continual domain shift learning issue, in which the model is trained on a source domain and serveral unlabeled target domains. The authors highlight the pain points of the issue are concluded with three: 1. model generalization on 'before and during' training doamins(TDG); 2. better usage of unlabeled target domains(TDA); 3. a commonplace talk of an old scholar which is Catastrophic Forgetting. To address the issues, a Twofer method is proposed, which is composed by three components: 1. Random Mixup Augmentation; 2. Top^2 pseudo labeling; 3. prototype contrative alignment. Finally, extensive experiments are conducted.\n\n",
            "strength_and_weaknesses": "Strength:\n1. The motivation is new and seems to be practical proposed by the authors;\n2. The experimental results are superior.\n\nWeaknesses:\n1. The writing of the manuscript is hard to follow, some definitions are unclear, i.e., what is distinguishability of data samples in 4th line of P5? Moreover, some formulas have better expression but not the adopted ones, which takes me a lot of time to grab the meaning. The choice of notation is unprofessional. \n2. The motivation of random mixup augmentation is weak. As far as I am concerned, deploying variant noise in training does harm to stability of training. Then, how it improves the generalization is hard to explain. Moreover, there are an ocean of noise generation method, why Eq. 2 is better?\n3. There lacks necessary explaination to Sec2.3 and the caption and content of Fig2 are unclear.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality: Poor\nNovelty: Not sure.\nReproducibility: Good. The code is provided. ",
            "summary_of_the_review": "In summary, the biggest issue to me is the writing. I cannot get the motivation of the proposed modules. To this end, I think the manuscript needs further polish.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper477/Reviewer_sJyy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper477/Reviewer_sJyy"
        ]
    },
    {
        "id": "C3wN-YYp6u",
        "original": null,
        "number": 3,
        "cdate": 1667002133693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667002133693,
        "tmdate": 1671057798230,
        "tddate": null,
        "forum": "L8iZdgeKmI6",
        "replyto": "L8iZdgeKmI6",
        "invitation": "ICLR.cc/2023/Conference/Paper477/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work addresses adaptation to a sequence of discrete domains where accuracy before, during, and after adaptation is desired.\nAccuracy before processing a domain measures domain generalization, accuracy during or immediately following a domain measures adaptation, and accuracy after a domain measures forgetting (or remembering).\nThis setting is named continual domain shift learning (CDSL) in this work, and presented as a new problem statement because of (1) the changing of the domains and (2) the metrics before/during/after the domain is adapted on.\nThe proposed method, TwoFer, incorporates elements of domain adaptation and domain generalization, and its most important components are agressive data augmentation (RandMix), a pseudo-labeling scheme that clusters and filters predictions to keep only a certain percentile, and a prototype representation of classes in the target domains (prototype contrastive alignment or PCA).\nEach component is partly-tuned to its use here, but all of them have a strong relation to prior work that introduced it.\nExperiments cover three benchmarks that are common to domain adaptation and generalization: digits, PACS, and DomainNet.\nHowever, the evaluation protocol differs from the standard protocol, both in the desired measurement of sequences of domains, but also in a different sampling of splits.\nSince the results are sensitive to the order of the domains adapted on, two orders are evaluated in the main paper for each dataset (with one more each in the appendix).\nBaselines cover domain adaptation, in particular source-free and test-time adaptation, as well as domain generalization.\nAblations check the separate contribution of the data augmentation, pseudo-labeling, and prototypical loss components of the method, and analyze alternative choices or applications of each component.\n\n",
            "strength_and_weaknesses": "*Strengths*:\n\n- The setting emphasizes adaptation across multiple domains, in sequence, and the need for more metrics to do so. This setting measures not only accuracy _during adaptation_ to a given domain, but _before_ to measure domain generalization as well as _after_ to measure resilience to forgetting.\n- The baselines are drawn broadly from source-free adaptation and domain generalization (but they are not comprehensive, see weaknesses), and for some baselines such L2D, PDE, and PCL there is an attempt to equip them for the proposed setting by providing the same exemplar memory as the proposed method.\n- The augmentation model, building on domain diversification (Wang et al. 2021) and AdaIN (Karras et al. 2019), proves effective for the proposed method as well as baslines, and significantly impoves accuracy for adaptation across digits datasets (Figure 4a).\n- The ablation study (Section 3.2 and Figure 4) justifies augmentation by randmix, the pseudo-labeling by T2PL, and most of all the PCA loss. However, the ablation is only on Digits, which is the simplest dataset, and it would be more rigorous to repeat these experiments for PACS and DomainNet too in order to check for consistent effects.\n\n*Weaknesses*:\n\n- The proposed setting depends on the order of domains, but only two orders are shown in the main paper, and just one additional order in the appendix. The order in the Appendix for PACS and DomainNet shows _no or little improvement_ for Twofer, while Digits does still improve, which suggests the possibility that order can be quite important. This small sampling of orders is unlikely to give a non-noisy estimate of performance.\n- The framing is neither accurate nor cordial and collegial. This paper does not begin the study of continual shift, counter to the claims of the abstract and introduction. Rather it focuses its metrics on particular phases of adaptation, and emphasizes (1) rapid learning at the beginning of a domain and (2) less forgetting on past domains.\n- The evaluation protocol differs from the compared work, in defining its own splits of the data (see \"Experiment Settings\"), but how exactly the splits are defined is not clear. Alternative evaluations can be necessary to make a point, but such oddball evaluations should be paired with the standard evaluations for fair comparison and sanity checking of the results.\n- There is missing related work on test-time adaptation with more efficient updating and slower forgetting: Efficient Test-Time Model Adaptation Without Forgetting (EATA) at ICML'22. As this prior work addresses key claims of TwoFer, it needs to be compared with in the experimental evaluation. It should likewise be discussed should its contributions and technical details intersect with the claims in the proposed method.\n- There is missing related work on source-free adaptation: AdaContrast (Chen et al. CVPR'22) and SHOT++ (Liang et al. PAMI'21). This requires discussion and comparison, as results are reported for DomainNet and other standard benchmarks.\n- The proposed method heavily relies on data augmentation, which makes it more domain specific than other methods, which perform none or little, like Tent or EATA for example.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity*:\n\n- The introduction's discussion of the \"unfamiliar period\" does reframe the claims in the abstract for the better, by focusing the paper on accuracy pre-adaptation and post-adaptation.\n  However, this framing should come first, and be reflected in the abstract and first part of the introduction, before claiming that the proposed TwoFer \"significantly outperforms\" everything and is \"envisioned as a significant milestone\".\n  These are not precise nor productive claims as written.\n  Furthermore, in talking about the unfamiliar period and the metrics TDG, TDA, and FA, this work could align with the literature on continual learning and discuss \"forward transfer\" and \"backward transfer\" instead of introducing custom terminology.\n- The main results (Tables 1, 2, 3) obscure the compared methods with custom abbreviations (TEN instead of Tent, CoT instead of CoTTA, etc.).\n  While these can be decoded and checked against the text, it would be faster to read if the original names and abbreviations were kept.\n\n*Quality*:\n\n- The main results in Tables 1 & 2 appear to show large gains in average accuracy across domains for the three metrics of accuracy before/during/after a given domain.\n- However, as already noted this evaluation is not entirely standard, and the standard evaluation is not included in the results to first establish performance on a known setting.\n\n*Novelty*:\n\n- Contrary to its claim, Twofer is not the first work to address continual adaptation or \"continual domain shift learning\".\n  [Efficient Test-Time Model Adaptation without Forgetting](https://arxiv.org/abs/2204.02610) at ICML'22 and [Continual Test-time Domain Adaptation](https://arxiv.org/abs/2203.13591) CVPR'22 both address adaptation to a sequence of domains, did so well before the submission deadline for ICLR'23, and report results on larger-scale and more difficult datasets.\n  The earliest instance of learning on non-stationary domains is most likely [Continuous Manifold Based Adaptation for Evolving Visual Domains](https://openaccess.thecvf.com/content_cvpr_2014/html/Hoffman_Continuous_Manifold_Based_2014_CVPR_paper.html), although it works in the unsupervised domain adaptation setting, but that does not make it unrelated.\n- The proposed pseudo-labeling approach is in essence clustering while filtering the predictions by certainty, but such filtering or thresholding is common for pseudo-labeling. See the PAMI edition of SHOT (Liang et al. 2021) called SHOT++ or EATA (cited above in this review) for examples. In particular SHOT++ has an adaptive threshold based on the distribution of confidences, which is related to filtering a fixed percentage as done by T2PL.\n- The use of parameters as prototypes is not novel to this work, and has been done not only by the cited Saito et al. 2019, but goes back to at least Imprinted Weights by Qi et al. CVPR'18.\n\n*Reproducibility*:\n\n- While there are many parts to the method, there is code provided in the supplementary materials. (That is, it is promised in the main text, but this was not verified as part of the review.)",
            "summary_of_the_review": "The dismissal of prior work and the divergent evaluation w.r.t. the established protocol used by the compared methods raises the possibility of serious experimental error.\nIt may be that everything is sound, but the rebuttal needs to clarify exactly how the evaluation was done to be sure. Ideally, the rebuttal would report results in the standard experimental setup for a fair comparison to accompany the main results of the paper in Tables 1 & 2.\nWithout this information, it is hard to gauge the correctness and empirical significance of the submission.\nI am open to counterevidence however, and will reconsider this submission in light of response and discussion.\n\n*For Rebuttal*\n\nMajor\n\n- Please report results in the standard evaluation protocols for PACS and DomainNet. In particular, please use the standard splits, and include results with stationary/non-continual/episodic domains. By comparing in the established setting, the experiments would establish that Twofer is at least as good there, and then the experiments could should improvement in the proposed protocol.\n- Please analyze the sensitivity of TwoFer and the baselines to domain order. If the variance due to order is larger than the reported gains, then the experiments may have inadvertently chased noise.\n- Please relate TwoFer to EATA and AdaContrast and report any comparisons, if any comparable comparisons are possible given the set of experiments in the submission.\n- Please clarify the novelty or lack thereof in the setting, compared to the missing related work raised, and re-articulate the contributions of CDSL vs. other studies of continual shift in existing work like CoTTA and EATA.\n\nMinor\n\n- Was DomainNet filtered according to the evaluation protocol in prior work like Saito et al. 2019 and Chen et al. 2022 (AdaContrast)?\n\n**Update after Response**\n\nThe substantial responses and experiments address concerns about experimental validity concerning data splits, domain ordering, and missing benchmarks. As such I am raising my score to 5 (borderline reject) to acknowledge these improvements while still urging the authors to consider the framing and organization of this work so that it is more comprehensible to an audience that spans adaptation, continual learning, and robustness. I have accordingly raised the marks for correctness and empirical novelty/significance as well.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper477/Reviewer_jYsN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper477/Reviewer_jYsN"
        ]
    },
    {
        "id": "AhkCUeLJNj",
        "original": null,
        "number": 4,
        "cdate": 1667096801851,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667096801851,
        "tmdate": 1667096801851,
        "tddate": null,
        "forum": "L8iZdgeKmI6",
        "replyto": "L8iZdgeKmI6",
        "invitation": "ICLR.cc/2023/Conference/Paper477/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work focuses on a practical challenge that deep learning models cannot remain stable performance when being deployed in real-world environments. For this problem, the author(s) defines three objectives, i.e., target domain generalization, target domain adaptation and forgetting alleviation. To achieve these objectives, a framework called Twofer is proposed to better align domains over time. According to extensive evaluation experiments and comprehensive baseline comparison, the effectiveness of Twofer can be demonstrated.",
            "strength_and_weaknesses": "Strength:\n\n1. The objective of target domain generalization is appealing and significant, and I particularly like the definition of \u2018unfamiliar period\u2019 in the paper. Although a number of domain adaptation studies have been proposed, including continual and source-free, most of them neglect the generalization before adaptation. This paper bridges this gap by allowing the deployed models to evolve themselves and remain stable performance all the time. \n\n2. The proposed Twofer is composed of three major modules, which work together for achieving better domain alignment. These three modules are well-motivated.\n\n3. The author(s) compare twofer with a number of SOTA baselines. The experiments are extensive, and the results can demonstrate the effectiveness of proposed methods.\n\nWeakness:\n\n1. If I understand correctly, twofer needs to store a small number of samples at each stage for later usage, is it possible to relax this requirement? More should be discussed. \n\n2. I notice that there is no section of related work, which is important to this work. Therefore, I believe the author(s) needs a section or a table to present the difference among different DA and DG topics.\n\nQuestions:\n\n1.\tThe author(s) mentions that blindly pushing augmentation data away from the original possibly hurts the generalization performance. Could the author(s) give more explanation here? \n\n2.\tBesides, is it possible to extract useful information from seen domains for guiding the data augmentation in the future? Because the used augmentation module is simple in terms of network structure, the training cost is acceptable if learnable augmentation can bring noticeable performance gain.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized and clearly written, and it focuses on practical issues in deep learning model deployment. This work should be reproducible. The implementation details and code are provided.",
            "summary_of_the_review": "This paper is clear and technically solid. The experiments are thorough with impressive results, and the analysis is extensive. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper477/Reviewer_Nus3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper477/Reviewer_Nus3"
        ]
    }
]