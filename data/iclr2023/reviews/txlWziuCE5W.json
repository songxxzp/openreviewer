[
    {
        "id": "NVS21oZKA_",
        "original": null,
        "number": 1,
        "cdate": 1666582467192,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582467192,
        "tmdate": 1668732858838,
        "tddate": null,
        "forum": "txlWziuCE5W",
        "replyto": "txlWziuCE5W",
        "invitation": "ICLR.cc/2023/Conference/Paper2796/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors extensively surveyed existing literatures to determine how to leverage the large-scale vision language models effectively for understanding medical images. They claim that a well-designed medical prompt containing domain-specific knowledge is the key to bridging the gap between natural image to medical domains. The authors proposed several approaches to generate medical prompts and demonstrated improved performance using fine-tuned models.",
            "strength_and_weaknesses": "Strength:\n- The authors propose several strategies for better elicitation of medical knowledge from vision-language models pretrained on natural images;\n- They focus on the design and automatic generation of medical prompts that can include expert-level knowledge and image-specific information;\n- Through extensive experiments, the authors support their claims in both zero-shot transfer and fine-tuning scenarios.\n\nWeaknesses:\n- There is inadequate justification for explicit requirement of VQA model for deriving attributes/properties;\n- Table 5 shows an inconsistent performance disparity between the *-shot and full data.\n- Limited technical contribution.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written indeed. The smooth transition from one section to the next makes the reader feel at ease with the paper. The motivation of the study is quite straightforward. The authors leveraged existing frameworks and provided implementation details. Hence, I believe the study is reproducible. However, in my opinion, the novelty of the study is critically limited.",
            "summary_of_the_review": "The authors claimed that a well-designed automatic medical prompts generation and leveraging these prompts are the key to elicit knowledge from pre-trained vision language models. However, I am not sure why we need to explicitly derive these attributes using VQA model. While a deep learning model can identify these properties such as shape variance and color difference in an end-to-end setup, I couldn't find a rationale for generating these properties using visual question answering models.\n\nThe performance difference between *-shot and full data (Table 5) is debatable. The performance of the proposed model is not very impressive on full data compared to candidate models. In contrast, the performance of *-shot is noticeably better than that of candidate models. The large performance gap appears inconsistent to me.\n\nOverall, in my opinion, the authors make minor modifications to existing approaches for similar tasks. The technical contribution is somewhat limited considering a high bar to ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2796/Reviewer_DhmB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2796/Reviewer_DhmB"
        ]
    },
    {
        "id": "dPD8uDx8tUo",
        "original": null,
        "number": 2,
        "cdate": 1666673655522,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673655522,
        "tmdate": 1668734358765,
        "tddate": null,
        "forum": "txlWziuCE5W",
        "replyto": "txlWziuCE5W",
        "invitation": "ICLR.cc/2023/Conference/Paper2796/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper ran an empirical study on pretrained vision language models (VLMs) for medical image analysis. They study how to manually design effective medical prompts by using relevant attributes. Results suggest that well-designed prompts can significantly improve the domain transfer capability compared to the default category names.  They evaluated the proposed approach on various existing medical datasets across different modalities including endoscopy, cytology, histopathology and for multiple radiology modality including  X-ray, CT, MRI and Ultrasound. They comprehensively evaluated each modality and dataset for zero-shot, few shot and full fine-tuning  setting. They use the GLIP\u2013T variant as the pretrained VLM and use the PubmedBert-base-uncased variant for language driven prompt generation. ",
            "strength_and_weaknesses": "**Strength:**\n- The paper is one of the pioneers to explore pretrained VLMs for medical image analysis. \n- The application and proposed approach through pretrained transfer learning is relevant and practical.\n- The comprehensive study is well-designed.\n\n**Weakness:**\n- There are some inconsistencies in the results and occasionally there is no significant improvement vs the baseline (Table 5.)\n- Use of some components such as automatic captioning are not well justified.\n- Only a single language model has been studied. It would be very interesting to see at least in a limited study how these results are holding for other VLMs. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written.\n- Novelty is mostly incremental and related to empirical analysis and application, still appreciated and important. \n- Experiments and results and clearly explained and compared to relevant baselines. \n- There is no statistical test or multiple runs to form deviations and marginal improvements. \n- The nature of the work is incremental from a technical standpoint, however the application and design choices in the medical domain are new. \n\n",
            "summary_of_the_review": "This paper addresses an important application of VLM for medical image analysis and comprehensively studies it. The paper includes comprehensive results and reasonable improvements over baseline. \n\n\nAfter rebuttal: as most of my comments has been addressed I increased my score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2796/Reviewer_kq4V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2796/Reviewer_kq4V"
        ]
    },
    {
        "id": "qufPwQT5Pg",
        "original": null,
        "number": 3,
        "cdate": 1666705604149,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666705604149,
        "tmdate": 1666705604149,
        "tddate": null,
        "forum": "txlWziuCE5W",
        "replyto": "txlWziuCE5W",
        "invitation": "ICLR.cc/2023/Conference/Paper2796/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors use two methodologies to generate textual prompts with information related to the analyzed diseases in order to explore whether the injection of expressive attributes can improve the generalization and performance of the GLIP model. They did experiments using a broad range of medical datasets from various modalities. They showed that the proposed automatic prompt generation methodology improves the generalization and performance in zero and few shot scenarios.",
            "strength_and_weaknesses": "*Strengths:\n\n- Extensive experiments using full data, few-shot learning, and zero-shot learning. \n- Experiments in a broad range of datasets, varying from photographs to MRIs. \n- Well-explained methodology related to prompt generation. \n\n*Weaknesses:  \n\n- The authors failed to include images and prompt examples from samples of the radiology datasets.  \n- The authors do not explain how they use MRI and CT data since they are volumetric images (used as 3D? 2D? Axial, coronal, or sagittal view?). ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reproducible since all datasets and models that are employed were made publicly available, while all hyperparameters are fully described in the paper. It is also of good quality, clear enough and exploring a very hot topic (prompt engineering/injection). ",
            "summary_of_the_review": "A well-written paper with suitable methodology and novelty. I do not see any significant weaknesses related to the methodology or experiments, but I will point out some things that can improve the paper in a general way:  \n\n1) Please show some examples of visualizations and prompts for radiology images, as they are different from the ones already in the paper. It will be an excellent addition (maybe as a part of the supplementary material?). \n\n2) The supplementary material can be improved with more visualization examples and prompts, mainly to show the specificities of each imaging modality. \n\n3) Please add additional details on the experiments with the LUNA and ADNI datasets since MRI and CT are volumetric images.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2796/Reviewer_Ko8g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2796/Reviewer_Ko8g"
        ]
    },
    {
        "id": "UTmCLa-caSj",
        "original": null,
        "number": 4,
        "cdate": 1667187499215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667187499215,
        "tmdate": 1668739649496,
        "tddate": null,
        "forum": "txlWziuCE5W",
        "replyto": "txlWziuCE5W",
        "invitation": "ICLR.cc/2023/Conference/Paper2796/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the applicability of pre-trained vision language models to the medical domain. The paper shows how manually designed expressive prompts can bridge the domain gap between natural and medical images. Further, they propose methods for automatic prompt generation and demonstrate across a range of medical datasets for the lesion detection task that these prompts are effective. As a final result, the authors also demonstrate that their fine-tuned models are better than the supervised counterparts.",
            "strength_and_weaknesses": "- Strengths:\n1. The paper proposes an interesting approach to adapt pre-trained VLMs to the medical domain using prompts generated by domain specific LMs and VQA models.\n2. The results across 13 datasets are quite extensive and suggest strong improvements.\n3. The writing is easy to follow for the most part\n\n- Weaknesses.\n1. No error bars in the results so hard to ascertain statistical significance.\n2. No code reproducibility and details on the prompts generation are quite sparse. Similarly some results seem surprisingly strong and could do with additional explanation.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nTypos and nits below\n> moves toward the ear of large-scale pre-trained models\nShould 'ear' be era instead?\n\n> mismatch between domains may compromise the capability of the pre-trained models being transferred from one to another\nConsider adding Raghu et al 2019 citation - https://proceedings.neurips.cc/paper/2019/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf here although it is discussed further in related work.\n\nIn Related work, consider also adding https://arxiv.org/abs/2105.11333\n\n>  However, our preliminary results obtained from the VQA prompts suggest that certain attribute (e.g., location) may not be appropriated\nanswered by the pre-trained VQA models\nappropriated -> appropriately\n\n\nReporting the average performance across datasets in Table 2 could be helpful\n\nQuality\n\nThe paper is well written and easy to follow for the most part.\n\n\nNovelty\nThe idea of using VLMs trained on natural images and augmented with domain specific LMs and VQA models seems interesting and novel to me.\n\nReproducibility\n\n> We freeze the bottom two layers of the image encoder and decay the learning rate by 0.1 when the validation performance plateaus.\n\nWhy was this chosen and what is the result of this?\n\n\nThe backbones differ between the baselines (RN50) and the proposed methods (SwinT). This is an additional confounding factor.\n\n\nI am also a bit surprised by the margin of improvement obtained by finetuned model using a single example in Table 5. Some explanation here would be helpful.\n\nI do not see code details included and the appendix is quite sparse.\n\nMy major question is regarding the results in Table 2. Does this involve any finetuning with the generated prompts? If not, the zero shot results seem surprisingly strong. It may be good to add further explanations as to why this is the case. \n",
            "summary_of_the_review": "The paper proposes an interesting approach to adapting pretrained VLMs to the medical domain using prompts generated by domain specific LMs and VQA models. Results across 13 medical datasets suggest the potential of the method.\n\nOverall the approach is interesting although not particularly novel. The authors should consider adding more explanations and analysis (especially qualitative) to demonstrate why their proposed method is so effective especially Table 2 results in the absence of finetuning.\n\nFurther, error bars are missing in results. Although 13 datasets are considered, they all contain a single task and it might interesting to consider classification or medical VQA tasks too for the paper. \n\nOverall, if the authors can satisfactorily respond to the questions above, willing to reconsider my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2796/Reviewer_r5bj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2796/Reviewer_r5bj"
        ]
    }
]