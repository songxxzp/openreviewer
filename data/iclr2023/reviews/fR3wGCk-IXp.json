[
    {
        "id": "8fVop6CmG_",
        "original": null,
        "number": 1,
        "cdate": 1666580401256,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580401256,
        "tmdate": 1666580401256,
        "tddate": null,
        "forum": "fR3wGCk-IXp",
        "replyto": "fR3wGCk-IXp",
        "invitation": "ICLR.cc/2023/Conference/Paper5184/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a Multilingual Grade School Math (MGSM) which can be used to evaluate the reasoning abilities of large language models by predicting the chain of thought. The authors compare the results of different prompting strategies on two large-scale language models, and further conduct ablation studies to analyze the effect of language frequency, model scale, exemplar amount and exemplar type.",
            "strength_and_weaknesses": "Strength:\n1.\tThe MGSM is the first multilingual dataset that can be used to evaluate the chain of thought in multilingual setting. \n2.\tThe paper is well-writing and easy to read.\n3.\tThe experiments are comprehensive and well organized.\nWeakness:\n1.\tThe contribution of multilingual chain-of-thought is incremental compared to the villa chain-of-though.\n2.\tIt is interesting to see the large gap between Translate-EN, EN-CoT and Native-CoT in MGSM. While the gaps in other benchmarks are not too much. Is it because MGSM benchmark is originated from translation?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. The dataset is novel while the proposed multilingual prompting strategy is incremental to villa chain-of-thought. Considering the heavy resource requirement of experiment, it might be difficult for reproduce given limited resources. ",
            "summary_of_the_review": "The contribution of the new benchmark is clear and the experiments are sufficient, while the prompting strategy of multilingual chain-of-thought is incremental.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_DTUx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_DTUx"
        ]
    },
    {
        "id": "XVqygj6pSr",
        "original": null,
        "number": 2,
        "cdate": 1666628728630,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628728630,
        "tmdate": 1666685031620,
        "tddate": null,
        "forum": "fR3wGCk-IXp",
        "replyto": "fR3wGCk-IXp",
        "invitation": "ICLR.cc/2023/Conference/Paper5184/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This is a very straightforward paper, with impressive and surprising - although to me sad (more on this later) - results. They will certainly motivate future work, and encourage some of the current directions.\n\nThe authors analyze the multi-lingual capabilities of language models who are trained primarily on English data, with a little of other languages as well. They combine this with recent work of chain-of-thought (CoT) prompting, showing that those the combination of large LLM + CoT provide a very decent results on challenging datasets, in a multi-lingual setting.",
            "strength_and_weaknesses": "STRENGTH\n1. Translating a math-problem dataset (GSM8K) from English to 10 other diverse languages, using professional translators; and releasing the full dataset\n2. Benchmarking two LLMs with CoT on that and two other multi-lingual datsets; showing that they provide competitive and sometimes state-of-the-art results compared to a fully supervised setting\n3. Showing that machine-translating the CoT into English is at least as good and often better than prompting with CoT in the target language directly\n\nWEAKNESS\n1. A point which is never addressed is why translated CoT seems to perform always better than human translated CoT. Is this because some of the training data of the LLM might be translatese? Or is the difference too small to be significant? If so, it would be interesting to see the results with a different set of prompts or with a different sampling\n2. Not really a weakness, but this result will not necessarily encourage more research into low-ressource languages. While the large LLM in this paper is anonymized, I picked a random other language model from all those that also have 540B (PALM in this case). PALM contained 24B French tokens, and this paper reports 46.2% when natively prompted and 55.2% when translated. Swahili only had 40M tokens for PALM, but has a comparable performance in this paper when translated (51.2%, compared to 35.2% for native Swahili prompt). Why developping a Swahili (or French?) LLM when I can just use the largest EN one and machine-translate? At the same time, 40M tokens will have much less variety than 578B tokens used for English and some idiosyncrasy of Swahili might not be captured by translation only.\nAnother direction which will be encouraged by this paper is scaling: Swahili had an almost 4x jump between using a 62B model (there are a few of those available these days) and a 540B (not so many available of that size).\n\nNote that Weakness (2) is not really an issue with the paper... just with the consequences that the reported facts might produce.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: very clear, this is a straightforward analysis but delivered in a very clear way\n\nQuality: very good, the experiments are thoroughly done\n\nNovelty: high. I wonder why nobody else did this until now..... </sarcasm>\n\nReproducibility: as soon as the 540B model will be release or an API provided this can be reproduced very easily",
            "summary_of_the_review": "Straightforward idea, very well executed. High impact",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_q52S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_q52S"
        ]
    },
    {
        "id": "gx5rzjdkrZN",
        "original": null,
        "number": 3,
        "cdate": 1667317006173,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667317006173,
        "tmdate": 1670847053340,
        "tddate": null,
        "forum": "fR3wGCk-IXp",
        "replyto": "fR3wGCk-IXp",
        "invitation": "ICLR.cc/2023/Conference/Paper5184/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends the prevalent GSM8K benchmark to a multilignual verison covering ten languages, and accordingly verify a recently proposed technique Chain-of-though upon it, resulting a best performance of 55 with the most competent LLM.\n\nThe authors also verify such capability on established benchmark XCOPA, achieving a new state-of-the-art.\n\nThe paper is well presented and very clear to understand.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is well presented and easy to understand.\n\n2. a multilingual version of the popular benchmark GSM8K is proposed and released, which might be of interest to the community.\n\n3. new state-of-the-art on XCOPA and XLWiC is set.\n\nWeaknesses:\n\n1. The paper does not provide much novelty, insights and inspiration, the technique design simply follows existing works of CoT.\n\n2. The empirical investigation is not sufficient for a solid benchmarking of the new datasets. Specifically, I believe several recently-proposed methods like ZeroShotCoT[1] and SelfConsistencyCoT[2] are also necessary and not difficult to implement.\n\n3. The extension of the multilingual version of an existing benchmark seems providing only limited contribution for a ICLR paper to me. And the empirical verification of CoT on more datasets are also not sufficient for another new paper.\n\n[1] Large Language Models are Zero-Shot Reasoners\n\n[2] Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe claim is clear and backed by experiments.\n\nQuality:\nThe paper is well written. The quality is ok but not extremely solid. Experimental investigations do not cover necessary scope.\n\nNovelty:\nThe novelty is limited. The technique is merely another verification of existing published paper CoT with only minimum adaptation, where a naive translation of examples provides the best performance.\n\nReproducibility:\nThe dataset is released and the code is available. Although I assume most people do not have access to GPT3 or the refered 540B LLM, which are the only keys to obtain such SoTA results.",
            "summary_of_the_review": "The paper is well presented, an multilingual extension of GSM8K benchmark is proposed, CoT is verified accordingly as well as more datasets like XCOPA, which sets a new SoTA.\n\nBut the technique is almost identical to CoT, and the results can be well expected, leaving it only a very incremental contribution. Besides, such empirical verification is also not sufficient as important baselines are missing.\n\nTherefore, I vote for borderline reject, but I'm not absolutely sure and are open for discussion or rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_aQqu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_aQqu"
        ]
    },
    {
        "id": "v7VOY8TTRq",
        "original": null,
        "number": 4,
        "cdate": 1667370665577,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667370665577,
        "tmdate": 1667371185959,
        "tddate": null,
        "forum": "fR3wGCk-IXp",
        "replyto": "fR3wGCk-IXp",
        "invitation": "ICLR.cc/2023/Conference/Paper5184/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduced a multilingual arithmetic reasoning task with a corpus named MGSM, and used the few-shot chain-of-thought (Few-Shot CoT) prompting framework to evaluate the large language models\u2019 few-shot learning capability.  \nAuthors empirically found that the GPT-3 and LLM could provide reasonable performance on their constructed test set, and also provided results on two other multilingual reasoning tasks.\n",
            "strength_and_weaknesses": "Strength:\n1.  The paper shows interesting experimental results on the multilingual arithmetic reasoning task MGSM. Large-scale language models can perform well across languages, and even on underrepresented languages like Swahili.\n2. The paper provides extensive and convincing experiment results on other two multilingual tasks (common sense reasoning and word in-context semantic judgment).\n\nWeaknesses:\n1. The constructed test sample size for the benchmark is a bit small (250 samples).\n2. Some experiments and additional clarification can be considered and added (see Questions).\n\nQuestions:\n1. In Section 4, it would be better if the authors add the experimental results of adopting Zero-Shot CoT (Kojima, Takeshi, et al. 2022) [1], compared with the Few-Shot CoT model.\n2. One major question I have in mind is, is the current sample size of MGSM sufficient as a benchmark corpus? It would be better if the test sample size can be larger than the current 250, as the original GSM8K contains 1,000 test samples.\n3. It would be better if authors can add some insight or feature-based analysis on the emergent ability of cross-lingual reasoning of the tested language models GPT-3 and LLM.\n4. It is a bit surprising that the language models (GPT-3 and LLM) that are not specially pre-trained on multilingual data work well on cross-lingual reasoning with few-shot settings, is it possible to compare them with some multilingual backbones?\n5. I was also wondering whether the performance on MGSM (shown in Table 3) will be affected by adding some text perturbation (here the perturbation should not affect the numeral text), or adding some spans with code-switching.\n6. In Table 7, why are the number of few-shot exemplars in different language settings not the same? For instance, only 1 exemplar is provided for ru, th, te, and bn, which is smaller than that of en, de, fr, and es (6 exemplars).\n\n\nReferences:  \n[1] Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. \"Large Language Models are Zero-Shot Reasoners.\" arXiv preprint arXiv:2205.11916 (2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Very clear, the paper is well-written.  \nQuality: Good.  \nNovelty: This work is a multilingual version extension of the few-shot chain-of-thought reasoner. They empirically evaluated the multilingual reasoning capability of two large scale language models.",
            "summary_of_the_review": "This work is interesting and well-motivated, the technical contribution can be extended by tackling the points of questions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_3ofN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_3ofN"
        ]
    },
    {
        "id": "HRRzzSdJf8",
        "original": null,
        "number": 5,
        "cdate": 1667511132762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667511132762,
        "tmdate": 1667511132762,
        "tddate": null,
        "forum": "fR3wGCk-IXp",
        "replyto": "fR3wGCk-IXp",
        "invitation": "ICLR.cc/2023/Conference/Paper5184/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers chain-of-thought (CoT) in multilingual setup and shows that despite a highly imbalanced pretraining dataset, LLM still learns strong CoT capability in non-English. It extends an existing dataset to multiple languages. It also compares different ways to construct exemplar and CoT.\n",
            "strength_and_weaknesses": "Strength\nIt introduces a new dataset and presents extensive experiments.\n\nWeakness\n- No variance is presented in the few-shot experiment.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and the experiment is well designed. It considers CoT reasoning in a multilingual setup and introduces a new dataset. However, portions of the experiment is impossible to replicate due to the proprietary nature of LLM.\n\n- Is the dataset and CoT explanation publicly available?\n- Caption of table 6 has [CITE]?\n",
            "summary_of_the_review": "This paper considers CoT in multilingual setup and creates a new dataset, despite minor issues of the experiments.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_oVpU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_oVpU"
        ]
    },
    {
        "id": "vQ93cx335j",
        "original": null,
        "number": 6,
        "cdate": 1667542212652,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667542212652,
        "tmdate": 1670830971782,
        "tddate": null,
        "forum": "fR3wGCk-IXp",
        "replyto": "fR3wGCk-IXp",
        "invitation": "ICLR.cc/2023/Conference/Paper5184/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a benchmark containing 250 grade-school math problems translated from English to ten languages and also provides a deep analysis to explore multilingual reasoning abilities of two SotA large language models GPT-3 and LLM-540B in terms of both chain-of-thought and commonsense reasoning. The proposed benchmark was translated by native speakers of the target language with at least two years of professional experience. The experiments are well-designed, and the results are enough to support the claims. They find that on the MGSM benchmark, LLM-540B shows superior multilingual reasoning ability compared to GPT-3 on all languages with different settings. Similarly, for the commonsense reasoning benchmark, LLM-540B also outperforms the SotA models: RoBERTa Large and XLM-R Large on two other benchmarks: XCOPA and XL-WiC, respectively.",
            "strength_and_weaknesses": "**Strengths**\n* The multilingual benchmark is translated by professional translators and verified by an additional one for a random subset.\n* Experiments are well-designed with 4 prompting methods and 3 types of input exemplars.\n* Intensive analysis provides new insights for multilingual reasoning abilities of large language models.\n* The paper is also well-written.\n\n**Weaknesses**\n* The number of examples is not significant, with only 250 examples, compared to GMS8K with 8.5K examples, so the results may not be generalized if more examples are translated and added to the benchmark.\n* The verification process is less discussed, and not convincing to guarantee the quality of the benchmark.\n* The approach of translating a problem to English does not test the multilingual reasoning abilities of LLMs.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarification**\n* In the \"Manual translation process\" passage, who are the popular MT providers?\n\n**Questions to the authors**\n* Did you check the math background of translators?\n* How do you measure the quality of translated examples compared to English examples?\n* How does the pre-training (i.e., datasets, model design) of LLMs affect their multilingual abilities?",
            "summary_of_the_review": "Although the findings are interesting, the contributions are not significant enough for the multilingual research since the benchmark is small and the verification process is not well described (or implemented?), which may flip the current conclusions/claims if more multilingual examples with different types of problems are added. The big plus of this paper is the intensive experiments and detailed analysis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no ethical issue.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_HXqf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5184/Reviewer_HXqf"
        ]
    }
]