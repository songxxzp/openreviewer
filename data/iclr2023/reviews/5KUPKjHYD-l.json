[
    {
        "id": "uimEZzuilUR",
        "original": null,
        "number": 1,
        "cdate": 1666236200018,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666236200018,
        "tmdate": 1666236232129,
        "tddate": null,
        "forum": "5KUPKjHYD-l",
        "replyto": "5KUPKjHYD-l",
        "invitation": "ICLR.cc/2023/Conference/Paper4018/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a method that tend to learn task-independent representations. Specifically, the proposed method uses multiple masks to generate different subspace representations to avoid feature suppression. The proposed method also employs the uncertainty modeling technique against variance from strong augmentations. The experiments demonstrate that the proposed method outperforms SOTA baselines in various downstream tasks.",
            "strength_and_weaknesses": "Strength:\n\n1. How to learn task-independent representations is a key research problem in SSL.\n2. The proposed method shows remarkable improvements in various settings.\n3. The paper is well organized and easy to follow. \n\nWeakness:\n\n1. The proposed method is too complex, with 5 hyper-parameters in the loss function. Although authors claim all hyper-parameters do not need to be careful turn, only results on ImageNet-100/1K are provided. How about on other datasets, e.g., CIFAR-10/100?\n2. The proposed method shows significant improvements on ImageNet-1K compared with MoCo V2, over 3%, but on ImageNet-100, the improvements drop to below 1%. Why does it happen? \n3. The authors do not provide the results of the short-term training, such as 200 epochs. Does the proposed method require long-term training to fully explore the strong augmentations? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, novelty, and reproducibility are all good for this paper.",
            "summary_of_the_review": "Although the proposed method is complex and more experiments should be added, it still shows a new way to learn task-independent representations. I recommend this work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4018/Reviewer_oMUf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4018/Reviewer_oMUf"
        ]
    },
    {
        "id": "JpcehufRAI0",
        "original": null,
        "number": 2,
        "cdate": 1666392531671,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666392531671,
        "tmdate": 1667331135829,
        "tddate": null,
        "forum": "5KUPKjHYD-l",
        "replyto": "5KUPKjHYD-l",
        "invitation": "ICLR.cc/2023/Conference/Paper4018/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose learning embedding in augmentation subspaces via masking, which specifies which dimensions are relevant for which augmentation.  In the loss function, the mask is applied to the embeddings before comparison.  Also, the loss function includes a L1 norm term that encourages  masks to be sparse (with fewer features used).   To model uncertainly in the embeddings, the approach learns the mean (replacing the embedding previously) and covariance matrix.  The loss function is normalized by the covariance.   To stablize the learning of the uncertainty model, they use a regularization term to consistency between the Gaussian embeddings before masking via symmetric KL divergence.\n\nThey compare with 13 algorithms on ImageNet via linear evaluation and semi-supervised learning.  They also evaluate representations learned on ImageNet for downstream tasks on 5 different datasets via linear classification, object detection, and instance segmentation.  Their results indicate that their approach generally compares favorably.  In their ablation study, they look at the effect of disentangled embedding, scheduling, learned mask, and Gaussian uncertainty.  They analyzed the correlation between masks and add more uncorrelated masks than the more correlated ones.   They found \n\n\n\n",
            "strength_and_weaknesses": "Strengths:\n\n1.  Learning embeddings in augmentation subspaces via masking is interesting.\n\n2.  Their results indicate that their approach generally compares favorably. \n\n3.  The paper is generally well written.\n\nWeaknesses:\n\n1.  While performance improves, further discussion/analysis on which subset of the masked embeddings are beneficial to different tasks would be interesting.   ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written.   A minor comment:\n\nEq 1: Would using Z,Z' be more appropriate than V,V' since I think the loss is with respect to the embeddings, not views?\n\n\nThe proposed approach was compared with a number of algorithms on a few tasks.  \n\nReproducibility seems ok.\n",
            "summary_of_the_review": "Learning embeddings in augmentation subspaces via masking is interesting.   Their results indicate that their approach generally compares favorably. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4018/Reviewer_7wWs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4018/Reviewer_7wWs"
        ]
    },
    {
        "id": "HnmDRJxpPk7",
        "original": null,
        "number": 3,
        "cdate": 1666464937814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666464937814,
        "tmdate": 1670492507855,
        "tddate": null,
        "forum": "5KUPKjHYD-l",
        "replyto": "5KUPKjHYD-l",
        "invitation": "ICLR.cc/2023/Conference/Paper4018/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a self-supervised learning method, Masked Augmentation Subspace Training (MAST), which learns more generalizable representations by modeling augmentation-specific invariance properties. This paper shows that MASK outperforms existing SSL methods under various downstream tasks, including semi-supervised learning, detection/segmentation, and fine-grained classification.\n",
            "strength_and_weaknesses": "Strengths\n- This paper aims at learning more generalizable representations, which is an important problem in the SSL literature.\n- The proposed method outperforms previous SSL methods under various downstream tasks.\n\nWeaknesses\n- Method explanation is unclear: how can Eq (2) learn augmentation-specific masks $m_i$? For example, how to know $m_1$ is related to color jittering, and $m_2$ is related to Gaussian blur, as shown in Figure 3a? Eq (2) does not force any relationship between a mask $m_i$ and an augmentation operation $o_j$.\n  - I'm wondering whether MAST uses $K$ forward passes for each training iteration as LooC [1] did. If not, how to learn augmentation-specific embedding spaces? This could be critical because Eq (2) may cause misunderstanding.\n- Recent MAE-based approaches do not learn augmentation-invariant representations. What is the advantage of this paper over them? This paper should be compared with the MAE methods.\n- The hyperparameter choices of \u03bb = 25d/K, \u03bb1 = 600/(dK) and \u03bb2 = 25 seem to be carefully chosen. So, I think ``work well for all experiments without the need of careful tuning.'' is not true. If the authors want to claim that the sentence is true, they should conduct experiments with varying \u03bb, \u03bb1, and \u03bb2 and also should verify that performance is not sensitive to the hyperparameter choices.\n- There exist many fine-grained classification tasks, but Table 4 only provides two benchmarks, CUB and Flowers. More experiments with the tasks should be provided (see SimCLR [2], BYOL [3], or AugSelf [4] papers).\n  - This paper should be compared with [4]: [4] achieves 45% top1 accuracy on CUB-200 and 88% on Flowers-102.\n- There is no analysis of the correlation between each augmentation-specific embedding space and augmentation-related information (e.g., see Table 1 & Figure 3 in LooC [1] or see Table 8 & Figure 5 in AugSelf [4]).\n- There is no analysis of uncertainty. Such an accuracy comparison in Table 5 does not explain how the uncertainty affects learning representations. Also, this paper should provide some empirical evidence verifying that the uncertainty is successfully learned.\n\n[1] Xiao et al., What should not be contrastive in contrastive learning, ICLR 2021 \\\n[2] Chen et al., A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020 \\\n[3] Griall et al., Bootstrap Your Own Latent: A New Approach to Self-supervised Learning, 2020 \\\n[4] Lee et al., Improving Transferability of Representations via Augmentation-Aware Self-Supervision, NeurIPS 2021\n",
            "clarity,_quality,_novelty_and_reproducibility": "The detailed comments are described in the previous section. In summary,\n- Clarity :: The method description is unclear.\n- Quality :: The empirical results seem strong, but some important comparisons are missing.\n- Novelty :: I think the proposed idea is somewhat novel.\n- Reproducibility :: All the hyperparameters are well-described.\n",
            "summary_of_the_review": "Although this paper successfully achieves strong empirical results in various downstream tasks, the method descriptions are unclear, and some important comparisons and analyses are missing. I think they are critical, so I vote for rejection.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4018/Reviewer_JyME"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4018/Reviewer_JyME"
        ]
    },
    {
        "id": "lntcxoCFojq",
        "original": null,
        "number": 4,
        "cdate": 1667428210183,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667428210183,
        "tmdate": 1667428210183,
        "tddate": null,
        "forum": "5KUPKjHYD-l",
        "replyto": "5KUPKjHYD-l",
        "invitation": "ICLR.cc/2023/Conference/Paper4018/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper illustrates method (MAST) on how to transfer data augmentation to a downstream task for Self-Supervised Learning (SSL) methods.\n\nFrom their paper, their conclusion is that their method can show important role of data augmentation in creating useful invariance priors during SSL.\n\nHere are their contribution:\n- introduce MAST to make SSL representations disentangled and uncertainty-aware to\neffectively encode different augmentation invariances for good generalization.\n\u2022show MAST is efficient, is resistant to feature suppression, and achieves state-of-the-art\ndownstream performance on diverse vision tasks without presuming any task information\nduring pre-training.\n\u2022 provide interesting insights about how different augmentations are related in SSL and\nhow uncertainty reflects learning difficulty and impacts learning adaptively.\n",
            "strength_and_weaknesses": "Strength:\n- Their paper has pointed out the problem of current deep learning field is that current model or data augmentation lacks of generalization.\n\n- Weakness:\n\nIn their data augmentation part of 3.2, they set K=5 and K=15. Is there any performance trend of the different number of K?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Their paper is in good clarity and quality.",
            "summary_of_the_review": "This paper mainly contributes a MAST by proposing a unified framework to embed all the distinct priors of invariance into\nfeature representations that can be readily used for different tasks. Generally, it is trying to solve a problem of generalization and downstream task. Generally, their method has shown a good direction on self supervised learning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4018/Reviewer_Psus"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4018/Reviewer_Psus"
        ]
    }
]