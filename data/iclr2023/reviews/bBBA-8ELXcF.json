[
    {
        "id": "aDOJumrALfZ",
        "original": null,
        "number": 1,
        "cdate": 1665706759917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665706759917,
        "tmdate": 1669772982526,
        "tddate": null,
        "forum": "bBBA-8ELXcF",
        "replyto": "bBBA-8ELXcF",
        "invitation": "ICLR.cc/2023/Conference/Paper1918/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a new method for model-based offline RL.\\\nIt uses both a simple uncertainty estimate based on the modeling error instead of an ensemble-based uncertainty estimate of the model, and maximizes the entropy of the model outside the data distribution. \\\nThe method is tested on multiple datasets from two different deterministic MuJoCo benchmarks against various model-based and model-free offline RL methods and achieves competitive results.",
            "strength_and_weaknesses": "**Strengths**\n- The paper addresses an important topic.\n- The method delivers good results on the benchmarks studied.\n\n**Weaknesses**\n\nIn my opinion, there is no major weakness that would justify rejection.\n\n**Suggestions for improvement that should be implemented**\n- The text still needs minor corrections. For example, lowercase is used several times where uppercase is necessary (e.g. \"algorithm 5\", \"appendix A.2\", \"table 3\", \"equation 4\"), while uppercase is used in at least one place where lowercase is required (\"In this Algorithm,\"). There are still broken sentences, like \"that can lead a decrease in performance as well\". In the bibliography, some words are wrongly written in lower case, like \"fisher\" or \"\"q-learning\".\n\n- When discussing uncertainty estimation without ensemble for offline RL, the older paper [1] should be discussed.\n\n- Three repititions of the experiments are very few. The results could be significantly different in the long term mean. A standard deviation calculated using only three values is very unreliable.\n\n\n**Suggestions for improvement in the future**\n- The method should be tested on a stochastic benchmark. \nWhen estimating reward uncertainty based on the modelling error, as in this paper, it is not ensured that only epistemic uncertainty is used and not aleatoric uncertainty. In case of deterministic environments, as used in this paper, this is not a problem because the aleatoric uncertainty is zero. In stochastic environments, however, it could be that the approach is sub-optimal because it also penalizes areatoric uncertainty, which is unnecessary and detrimental to the expected value of the performance.\n\n**Further remark**\n\nA +/- should always be followed by the uncertainty of the measured value. This is usually the standard error rather than the standard deviation in the case where the measured value is a mean. Ultimately, however, it is up to the experimenter (i.e., the authors in this case) to estimate the uncertainty appropriately. Thus, if the authors choose to report the standard deviation (which here is sqrt(3-1) times larger than the standard error) as the uncertainty, then the authors are choosing to make a conservative estimate of the uncertainty, which is perfectly fine, especially given the very small number of only three repititions.\n\n\n\n\n[1] Depeweg et al., Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning, 2018",
            "clarity,_quality,_novelty_and_reproducibility": "All fine.",
            "summary_of_the_review": "A solid paper presenting another approach to avoid misestimation of the return in OOD situations in model-based offline RL.\\\nThe proposed method is simple and effective according to the experiments. \\\nA few small improvements should be made. \\\nFor the future, it would be important to investigate whether the approach is also effective in stochastic environments.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_hpPE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_hpPE"
        ]
    },
    {
        "id": "6OUt41egjU",
        "original": null,
        "number": 2,
        "cdate": 1666567812828,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666567812828,
        "tmdate": 1666567812828,
        "tddate": null,
        "forum": "bBBA-8ELXcF",
        "replyto": "bBBA-8ELXcF",
        "invitation": "ICLR.cc/2023/Conference/Paper1918/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies pessimistic model-based offline reinforcement learning. Motivated by the fact that ensemble methods do not provide accurate uncertainty quantifiers used for constructing penalties subtracted from rewards, the authors present an entropy-regularized algorithm that learns a pessimistic model and provides uncertainty estimation outside the support of offline data. Empirical evaluations show comparable performance to SoTA offline RL algorithms on the D4RL benchmark.",
            "strength_and_weaknesses": "**Strengths**\n- The paper studies an important topic which is uncertainty quantification for pessimistic offline RL. The paper is well-motivated as the ensemble methods do not offer reliable uncertainty estimation. \n- The proposed approach has the benefit of decoupling pessimistic model-learning and policy-learning steps, similar to model-based methods MOReL and MOPO and unlike SoTA model-based offline RL methods COMBO and RAMBO-RL.\n- The proposed algorithm is simple.\n- In addition to evaluations on the D4RL benchmark, the authors also conduct ablation studies investigating the effect of exploration policy as well as regularization and regularization strength on learned policies.\n\n**Weaknesses**\n- The algorithm and technical contributions have limited novelty. The proposed algorithm is an instantiation of pessimistic lower confidence bound methods such as MOPO that subtract a penalty from rewards to account for partial coverage in offline data. \n- It is unclear whether the algorithmic idea of building penalty terms based on determinants of covariance matrices is sound. It appears that the covariance is originally meant to capture the aleatoric uncertainty of the transitions (based on the definition on page 4) and later is used for the penalty terms, which instead require capturing epistemic uncertainty. No theoretical justification for this choice (even in a simple example) is presented.\n- I feel the focus of the paper should be to justify why this particular uncertainty quantifier is sound and useful. So it's important to conduct some theoretical and/or empirical investigations showing that the approach actually gives better uncertainty quantification than other methods such as ensembles or the method in [1] instead of solely relying on the end-to-end result on offline RL benchmarks.\n- The paper does not compare with model-based methods that go beyond using ensembles for uncertainty estimation such as the GELATO algorithm [1]. The paper also does not empirically compare with the ATAC algorithm [2]. Generally, the literature review is very incomplete.\n- The empirical performance compared to existing methods is not very strong.\n- The setting considered is limited. The algorithm focuses on the special case that transitions are Gaussian and uses learned covariance as an uncertainty quantification measure.\n- Discussion in paragraph 2 in the Introduction regarding model-based vs. model-free methods in offline RL is not entirely accurate and the claims appear to be strong and not fully justified. For example, it is unclear whether model-free methods are *inherently* overly conservative or model-based methods are better at generalization. This claim is not supported in theoretical studies of offline RL and is also not entirely supported by empirical observations (for example, CQL (model-free) outperforms MOPO (model-based)).\n\n**References**\n\n[1] Guy Tennenholtz, Nir Baram, and Shie Mannor. Latent geodesics of model dynamics for offline reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021.\n\n[2] Ching-An Cheng,\u00a0Tengyang Xie,\u00a0Nan Jiang,\u00a0Alekh Agarwal\u00a0Proceedings of the 39th International Conference on Machine Learning,\u00a0PMLR 162:3852-3878,\u00a02022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The technical quality and novelty are limited. The authors provide their code in the supplementary material. ",
            "summary_of_the_review": "It is unclear whether the approach presented in this paper is sound and useful. Theoretical and empirical justification is not presented for this choice of uncertainty quantifier. The paper misses empirical comparison with important related work. The approach has limited novelty and empirical performance is not very strong. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_EzoK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_EzoK"
        ]
    },
    {
        "id": "8ZaPEkQ5hu",
        "original": null,
        "number": 3,
        "cdate": 1666744200109,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666744200109,
        "tmdate": 1667145295881,
        "tddate": null,
        "forum": "bBBA-8ELXcF",
        "replyto": "bBBA-8ELXcF",
        "invitation": "ICLR.cc/2023/Conference/Paper1918/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents EPO, a novel model-based offline RL algorithm. The highlight of EPO is that it utilizes only a single model to estimate uncertainty, in contrast with previous works that require an ensemble of models. \n\nThe algorithm has multiple phases. In phase one, the algorithm fits a Gaussian model for the transition and reward. In phase two the algorithm maximizes the differential entropy of the fitted Gaussian model over an exploratory dataset, which is collected by some exploratory policy on the fitted environment from phase one. Finally, EPO optimizes the model over the fitted environment with an uncertainty penalization (based on the entropy of the fitted model) over the reward.\n\nThe authors conducted experiments on the MuJoCo environment and showed that EPO is competitive with some offline RL SOTAs that utilize an ensemble of models for uncertainty quantification.",
            "strength_and_weaknesses": "$\\textbf{Strength:}$\n\nThis work attempt to estimate uncertainty in RL with a single model, which frees practitioners from deploying ensembles in uncertainty quantification. Estimating uncertainty with a single model is an important research direction in both offline RL and online exploration. The paper is well-written and easy to follow. The proposed EPO algorithm has better modularity than previous works, in the sense that it can incorporate various existing model-free RL algorithms (on top of its pessimistic environment estimation).\n\n\n$\\textbf{Weaknesses:}$\n\nI have a few questions after reading the paper. Having some of those questions unresolved could be the potential weakness of the paper. Meanwhile, having some of those questions resolved could increase the potential impact of this paper.\n\n$\\textbf{Questions:}$\n\nThe claimed general approach seems to be limited by various model assumptions. In particular,\n\n$\\textbf{Q1:}$\nHow does EMO differentiate between aleatoric uncertainty and epistemic uncertainty? Gaussian modeling of next-step transition should also incorporate aleatoric uncertainty of $s'$ and $r$ unless the transition and reward are assumed to be deterministic (which is indeed the case for the MuJoCo environment in experiments though). Will such an assumption hinder the generality of the proposed EMO framework?\n\n\n$\\textbf{Q2:}$\nThe adopted (diagonal) Gaussian model requires continuous states, which may further hinder the generality of EMO. \n\n$\\textbf{Q3:}$\n(Minor) In addition, the Gaussian model has only a single peak, which may limit the modeling of uncertainty in the next-step transition (there could only be one high-probability outcome).\n\n\nIn addition, I have a few question regarding uncertainty modeling in EPO. \n\n$\\textbf{Q4:}$\nHow does the explorative policy affect the performance of EPO? Is it trustworthy to consider data sampled from the explorative policy to be OOD data (the trajectory may overlap with the in-distribution dataset)? Why not using the modified EPO described in section 4.3, which seems to penalize more relevant OOD data? As a side remark, previous work (e.g., [1-4]) also sample OOD actions of training policy to enforce more relevant OOD penalization.\n\n$\\textbf{Q5:}$\nThe uncertainty estimate is maximized over OOD data in the training stage (the regularization phase). How does such maximization of OOD uncertainty affect the uncertainty estimate, in particular, the relative scale of OOD uncertainty against the in-distribution uncertainty? Is the estimated uncertainty (after maximization on OOD data) trustworthy?\n\nMinor clarification question:\n\n\n$\\textbf{Q6:}$\nHow does EPO compare against previous ensemble-based methods in terms of the computation resource? I imagine that this could be one of the major advantages of utilizing a single model for uncertainty estimation.\n\n\n[1] A. Kumar et al., Conservative Q-Learning for Offline Reinforcement Learning. (2020)\n\n[2] I. Kostrikov et al., Offline Reinforcement Learning with Fisher Divergence Critic Regularization. (2021)\n\n[3] C. Bai et al., Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning. (2022)\n\n[4] Lyu et al., Mildly Conservative Q-Learning for Offline Reinforcement Learning. (2022)",
            "clarity,_quality,_novelty_and_reproducibility": "See the questions raised above. The reproducibility is justified by the code in supplements.",
            "summary_of_the_review": "The authors present an interesting attempt at uncertainty quantification with a single model. The approach appears to be promising at least in the test environments. Nevertheless, the underlying model assumptions with the Gaussian distribution could be strong, which may limit the generality of EPO as a general workflow.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_bC5e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_bC5e"
        ]
    },
    {
        "id": "T4wGZzR4ru3",
        "original": null,
        "number": 4,
        "cdate": 1667135686806,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667135686806,
        "tmdate": 1667135686806,
        "tddate": null,
        "forum": "bBBA-8ELXcF",
        "replyto": "bBBA-8ELXcF",
        "invitation": "ICLR.cc/2023/Conference/Paper1918/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work analyses model-based approaches to offline RL. Instead of considering model ensemble, it suggests using entropy regularization as a way to explore beyond the data support.  ",
            "strength_and_weaknesses": "The paper is clearly written and easy to follow. However, although I am not familiar with the offline RL literature, I found the proposed method quite standard, ie, regularizing the loss for better generalization. \nAlso, I am a bit skeptical regarding the experimental results. I explain why below. \n",
            "clarity,_quality,_novelty_and_reproducibility": "To me, the novelty seems light. The Gaussian structure of the model parameters makes me expect some theoretical guarantees, and perhaps also, a theoretical insight on how to choose the regularization coefficient depending on the size of the dataset and/or its support. \n\nQuestions to the authors:\n1. In sec. 4.2 when reporting the overall performance, the authors state that \u00ab\u00a0the values for RAMBO-RL, etc., are reported from their respective papers\u00a0\u00bb: does this yield a fair comparison? I would expect re-running those baselines and comparing results on the same machine+seeds. \n2. The authors state that avoiding ensemble model using regularization instead increases sample complexity. I do not find any empirical evidence of this claim in the experiments. Therefore, although intuitive, such a claim is still arguable (no theoretical guarantees plus no empirical evidence)\n\nMinor comments:\n- p.2, \u00a72: \u2018utlizing\u2019 -> \u2018utilizing\u2019\n- p.2, \u00a72: \u2018the best of two worlds\u2019 -> \u2018the best of both worlds\u2019\n- p.7, \u00a71: \u2018adheres to the prior work\u2019 -> \u2018adheres to prior work\u2019\n- p. 7 \u00a71: \u2018to train an RL algorithm to obtain $\\pi^*$\u2019 \u2014> redundant \u2018to\u2019\n- section 3.4.: I would merge this section to the previous one (too short)\n- p.7, sec. 4,2: \u2018to the state-of-the-art\u2019 \u2014> \u2018to state-of-the-art\u2019",
            "summary_of_the_review": "The lack of technical novelty and my concerns regarding experiments lead me to a weak accept. \n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_Y21Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_Y21Y"
        ]
    },
    {
        "id": "eSax2KhmXl",
        "original": null,
        "number": 5,
        "cdate": 1667251493252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667251493252,
        "tmdate": 1667251493252,
        "tddate": null,
        "forum": "bBBA-8ELXcF",
        "replyto": "bBBA-8ELXcF",
        "invitation": "ICLR.cc/2023/Conference/Paper1918/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents Entropy-regularized Model-based Offline RL (EMO), a model-based offline RL algorithm that learns a pessimistic MDP, where the uncertainty quantification is performed without using ensemble models. The dynamics model is trained in two phases. In the first warm-up phase, the model is learned by maximizing the log-likelihood. In the second regularization phase, an additional dataset is generated by an exploration policy on the learned model, and the model is optimized by maximizing the log-likelihood for the offline dataset while maximizing the entropy for the generated dataset by the exploration policy. In the experiments, EMO performs competitively with the baseline algorithms.\n",
            "strength_and_weaknesses": "[Strengths]\n1. This paper presents a method that provides uncertainty quantification using a single model. This is favorable in terms of computational efficiency, compared with the existing model-based offline RL methods that use ensembles.\n\n\n[Weaknesses]\n1. It seems that the method cannot be applied to stochastic environments. In the proposed framework, it is unclear how to differentiate between aleatoric uncertainty and epistemic uncertainty when using a learned covariance matrix for uncertainty quantification. I think we may need to penalize epistemic uncertainty but not penalize aleatoric uncertainty.\n2. Mixed experimental results: EMO outperforms baselines in some domains but underperforms in other domains.\n3. Modified EMO shares a similar spirit with RAMBO-RL, in terms of adversarial model update, but RAMBO-RL still outperforms Modified EMO in Table 3.\n4. In Figure 2, it seems the performance of EMO is highly sensitive to the hyperparameter $\\alpha$.\n\n[Questions]\n1. Do $\\mu_\\theta$ and $\\Sigma_\\phi$ share the lower-layer network parameters? or are they completely separate networks?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The method itself is easy to understand and looks clearly written. Still, I am not convinced of why we should maximize the entropy: Due to its nature of directly using entropy as a reward penalty, it seems that the method is hard to be applied to stochastic environments.",
            "summary_of_the_review": "Although the proposed method is appealing in that it learns only a single model for uncertainty estimation, the experimental results are not convincing enough by mixed results. Also, I have a concern that the method would be difficult to deal with stochastic environments since aleatoric uncertainty and epistemic uncertainty cannot be distinguished in the proposed framework.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_8Cwi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1918/Reviewer_8Cwi"
        ]
    }
]