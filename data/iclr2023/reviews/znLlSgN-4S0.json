[
    {
        "id": "8DLXPvmq-7",
        "original": null,
        "number": 1,
        "cdate": 1666310554593,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666310554593,
        "tmdate": 1669264817704,
        "tddate": null,
        "forum": "znLlSgN-4S0",
        "replyto": "znLlSgN-4S0",
        "invitation": "ICLR.cc/2023/Conference/Paper3709/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In order to better utilize the training paradigm of CTDE, the authors introduce dependency policy correction and dependency critic correction based on AC architecture to consider the action dependencies among the agents, and also design the corresponding network structure so that more accurate joint policy can be learned and thus more effective and better training results can be obtained.",
            "strength_and_weaknesses": "Strength.\n* The paper is clearly written and well understood\n* Looks interesting by introducing corrections in the central training\n* The experimental section shows that the method proposed by the author works very well\n\nWeaknesses.\n* Equation (10) looks wrong because I don't see other additional constraints that make it mathematically reasonable since there is no way to ensure that the sum of \\pi_i^{dep} is 1. Also Equation (10) is not consistent with the implementation in Fig 3.\n* Equation (13) looks like it has many combinations possible, such as (Q - 10) + (c + 10), which means that its optimization space will become larger\n* Equations (16) and (17) are not clear on what policy the choice of next action is based on\n* Currently I am confused about the learning process of both ind and dep polices. In fig4, we can find that ind is slower than dep, but in fig7, the two polices are similar. Does this mean that there is no need to actually consider other agent's actions in smac? So why MACPF can outperform other strategies? Also is there a need for other experiments that consider agent?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:Writing is clear\n\nQuality:Looks good\n\nNovelty:More interesting\n\nReproducibility:Not sure, need the author to open the source code\n",
            "summary_of_the_review": "The work is interesting, but I currently have some doubts and I will adjust my scores further after the rebuttal",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3709/Reviewer_9QY5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3709/Reviewer_9QY5"
        ]
    },
    {
        "id": "3p0T6wFwc0",
        "original": null,
        "number": 2,
        "cdate": 1666671245385,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671245385,
        "tmdate": 1669566609963,
        "tddate": null,
        "forum": "znLlSgN-4S0",
        "replyto": "znLlSgN-4S0",
        "invitation": "ICLR.cc/2023/Conference/Paper3709/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the optimality issue of cooperative multi-agent reinforcement learning and claims that the independent policy factorization may lead to learning suboptimal policies. The authors propose a conditional policy factorization method, where an agent\u2019s policy conditions on other agents (e.g., with lower indices). This paper claims that learning with such factorization theoretically leads to the optimal joint policy. It also shows that there exists an independent policy factorization that has the same value as conditional policy factorization, which enables decentralized execution. This paper empirically demonstrates the outperformance of the proposed method over baselines in a set of cooperative MARL tasks.",
            "strength_and_weaknesses": "**Strengths**\n1. This paper studies the optimality of cooperative multi-agent reinforcement learning (MARL), which is an important problem. \n2. Although the idea of conditional policy factorization is not novel, it is interesting to see this idea to be implemented with the deep neural network.\n3. Empirical results on both a simple matrix game and more complex SMAC tasks show the effectiveness of the proposed method, compared to the baseline methods.\n\n**Weaknesses**\n1. The motivating example in Section 2.3 is confusing. From Section 2.2, FOP seems to have very limited expressiveness, the same capacity as a linear value factorization, e.g., VDN. This is because IGO factorization with the Boltzmann policy is equivalent to the linear value factorization. Therefore, FOP may not be a good candidate to show the limitations of independent policy factorization.\n2. The idea of conditional policy factorization has been proposed in [1], which shall be cited and discussed.\n3. Given the motivating example and the didactic matrix game, this paper seems to study multi-modality problems in cooperative MARL. This problem has been studied in [2], which should also be discussed or compared. \n4. The reviewer tried QTRAN on the matrix game in Figure 4 and found it could quickly learn the optimal policy, which has different results from the paper and may invalidate the claim of this paper. \n5.  Does the proposed method have a larger number of parameters than other methods, like FOP? If so, does this contribute to its outperformance?\n6. How does the proposed method perform against baselines on all benchmark tasks in SMAC?\n7. The paper is well-organized, but the presentation can be improved. The reviewer strongly suggests the authors use multi-agent MDPs to rewrite theorems and their proofs because Dec-POMDP with the infinite horizon is an undecidable problem. In addition, the approximation in Theorem 1 is quite informal.\n\n[1] Dimitri Bertsekas. Multiagent rollout algorithms and reinforcement learning, 2019. https: //arxiv.org/abs/1910.00120\n[2] Wei Fu, Chao Yu, Zelai Xu, Jiaqi Yang, Yi Wu. Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning. ICML 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method looks novel by simultaneously learning both independent and dependent policies. The presentation is generally well-structured but can be improved with rigorousness. ",
            "summary_of_the_review": "The proposed method is interesting and shows outperformance over baselines. However, it is still not quite intuitive to understand why the proposed method outperforms baselines (see weaknesses above). In addition, the motivation of this paper shall be strengthened. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3709/Reviewer_Eh9U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3709/Reviewer_Eh9U"
        ]
    },
    {
        "id": "WbD237KF0wP",
        "original": null,
        "number": 3,
        "cdate": 1666699076581,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699076581,
        "tmdate": 1666699076581,
        "tddate": null,
        "forum": "znLlSgN-4S0",
        "replyto": "znLlSgN-4S0",
        "invitation": "ICLR.cc/2023/Conference/Paper3709/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper addresses the problem of centralized training and decentralized execution (CTDE) in multiagent reinforcement learning. The paper contributes the MACPF algorithm (multi-agent conditional policy factorization), which relies on the idea that the joint policy given the history can be factorized conditioned on the actions of previous agents. In other words, in a $N$-agent Dec-POMDP, \n\n$$\\pi_{joint}(\\boldsymbol a\\mid\\boldsymbol \\tau)=\\prod_{n=1}^N\\pi_n(a_n\\mid\\tau_n,a_{<n}),$$\n\nwhere $\\tau_n$ is the history of agent $n$. Additionally, the paper observes that a dependent policy $\\pi_n(a_n\\mid\\tau_n,a_{<n})$ can be written as the sum of an independent policy plus a correction term, i.e., \n\n$$\\pi_n(a_n\\mid\\tau_n,a_{<n})=\\pi_n(a_n\\mid \\tau_n)+b_n(a_n\\mid \\tau_n,a_{<n}),$$\n\nwhere $b_n$ is the correction term. During the centralized training, the agents thus learn the dependent policy by learning the two components (the independent policy and the correction term). The former can then be used for decision-making during decentralized execution. The results portrayed in the paper indicate that the proposed approach indeed leads to improved performance over other competing approaches.",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper is very clearly written, and the ideas proposed are interesting and - to the extent of my knowledge - novel. The results also suggest that the proposed approach indeed pushes the performance of existing CTDE approaches.\n\n**Weaknesses**\n\nThere are some steps in the proofs which I didn't fully grasp and which could be better discussed. Specifically,\n1. I may have missed/misunderstood something, but it is not clear to me that the optimal policy factories as shown in (5), particularly in terms of the dependency on history. In light of this, I have some difficulty in making sense of Theorem 1, which seems to state that there is an optimal policy that verifies such factorization. Or is it stating that the algorithm converges to an optimal policy _over all policies that factorize as in (5)_?\n2. Still on the point above (and, again, I may have missed/misunderstood something) in the proof of Theorem 1 it is not clear to me how the conclusion that the sequence of policies $\\{\\pi^k\\}$ generated by the algorithm is improving (or, better said, not worsening) leads to the conclusion that the resulting policy is optimal overall joint policies. It seems to me that - in order for such a conclusion to hold - the limit must be unique from any initial policy, but it's not clear to me how such a conclusion follows from the proof...",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is clearly written.\n\n**Quality**\n\nIn general, the paper strikes me as technically sound, aside from the two points I mentioned before.\n\n**Novelty**\n\nAs far as I know, the contributions of the paper are original, and improve over the state of the art.\n\n**Reproducibility**\n\nThe results use a number of benchmark domains from the literature, and the paper provides in the appendix details on implementation. I did not check these carefully, but my impression is that the work is reproducible.",
            "summary_of_the_review": "The paper proposes a novel approach for centralized training and decentralized execution in multiagent reinforcement learning. The proposed approach is novel and provides improvements over the state of the art. There are, however, a couple of technical issues that I did not fully understand and which could be better explained.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3709/Reviewer_krAZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3709/Reviewer_krAZ"
        ]
    },
    {
        "id": "BTDJU-CE22",
        "original": null,
        "number": 4,
        "cdate": 1667661517504,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667661517504,
        "tmdate": 1667661566577,
        "tddate": null,
        "forum": "znLlSgN-4S0",
        "replyto": "znLlSgN-4S0",
        "invitation": "ICLR.cc/2023/Conference/Paper3709/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper deals with the problem of decentralized policy in MARL when using the factorizing value method, especially FOP. To solve this problem, they propose **multi-agent conditined policy factorization** (MACPF) that incorporate dependency between the agents. \nDue to the fact that the proposed method learns the joint policy, it can guarantee to the optimal joint policy. They \nThey give the suitable network structure for the condition policy, also show the proper experiment results both in toy-example and complex task, SMAC. \n",
            "strength_and_weaknesses": "- Streagth   \nActually, learn the joint-policy, it can gaurantee the optimal policy in joint-aciton space.   \nOuput the decentrazlied individual policies distilled from the dependent policies   \n\n- Weakness   \n  Complex network structure, do exist dependent and individual networks ( $Q^{dep}_i, Q^{int}_i$ )    \n  To learn the individual policies from joint dependency policies, they propose the naive method\n   ",
            "clarity,_quality,_novelty_and_reproducibility": "They describe clearly the proposed method, and I think that the paper has the good quality. If the authors will give the hyperparameter in their experiments, it is helfpul for reproducibility.\n",
            "summary_of_the_review": "Learning the coordinated strategies in multi-agent is challenging because of partial-observability.\nThe author gives the intuition about what the problem is in the previous value-factorized method. To learn the opitmal policy, the depedency between agents should be considered, but it can break the decentralied execution in the test time. To keep the decentralized execution, having the factorized policies, they build the two seperate networks, dependent and individual networks. The proposed network structure is not simple, but they show the good performance by using the valued-based method when comparing strong baselines.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3709/Reviewer_FBdE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3709/Reviewer_FBdE"
        ]
    }
]