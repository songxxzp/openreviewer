[
    {
        "id": "mvoPF_ctDe",
        "original": null,
        "number": 1,
        "cdate": 1666527214932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666527214932,
        "tmdate": 1668672146703,
        "tddate": null,
        "forum": "gyTuMfkOney",
        "replyto": "gyTuMfkOney",
        "invitation": "ICLR.cc/2023/Conference/Paper1704/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies out-of-distribution detection, aiming at making classification models excel at discerning ID and OOD data. It is an important problem for safety-critical applications, and has attracted increasing attention recently. The authors claim that they adopt an original approach based on the functional view of network, which exploits the trajectories through the layers and their statistical dependencies. Specifically, those data whose trajectories differ from ID data are characterized as OOD data. The authors conduct experiments on ImageNet dataset, and the authors claim their superiority over the state-of-the-art methods. ",
            "strength_and_weaknesses": "Strength: \n\n- *The novelty of this paper is satisfactory*. The key observation of this paper is that existing OOD detection methods largely overlook the sequential nature of the underlying problem and thus limit the discriminative power of those methods. It motivates the authors to explore the trajectories through the layers and their statistical dependencies in OOD scoring. To me, it is a relatively new and reasonable idea, which may contribute to the community. \n\nWeakness:\n\n- *What does it mean for the term \"functional point of view\"*. It seems that the authors want to study the sequential behavior of the model outputs, discerning ID and OOD data by their different trajectory through the network. So, to me, I am not sure why the authors exploit the OOD detection problem in the \"functional point of view\u201c instead of the \"sequential point of view\". Maybe some references or explanations can be helpful. \n\n- *The adopted methodology in modeling the sequential behavior may require further discussion*. To model the sequential behavior of data, I think it might be a direct choice in using the time-series models such as LSTM. I do not fully understand why the adopted method can model the trajectory of model outputs, and why it can be superior over LSTM.  \n\n- *Are the mean vectors sufficiently informative for the embedding features?*. It seems that the proposed method distinguishes ID and OOD data by measuring the distances of features to the average features regarding each layer. Then, a nature question is that: is the mean vector sufficient to cover the diversity of ID data in the embedding space. Some justifications or illustration may require here. \n\n- *More experimental results may require*. The authors conduct experiments on ImageNet benchmark, which is a challenging OOD detection setting with large semantic space and complex data features. However, to fully justify the superiority of the proposal, I think the authors should conduct more experiments (e.g., CIFAR benchmarks, Hard OOD detection [1]). Also, more advanced methods should be included in comparison [1,2]. \n\n[1] Yiyou Sun, et al. Out-of-distribution Detection with Deep Nearest Neighbors. ICML'22. \n\n[2] Xuefeng Du, et al. VOS: Learning What You Don't Know by Virtual Outlier Synthesis. ICLR'22. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and has its novelty to some extend. Overall, I think the quality of this paper is sufficient, while several issues may require to be solved in the revision. Further, I did not check the reproducibility of the paper. ",
            "summary_of_the_review": "The proposed method discerns ID and OOD data by analyzing the trajectory of model outputs, which is uncovered by much of the previous works. However, there are several issues that may require to be solved, which I think it can improve the clarity and the quality of the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1704/Reviewer_5MWe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1704/Reviewer_5MWe"
        ]
    },
    {
        "id": "nFQN_o2rQe",
        "original": null,
        "number": 2,
        "cdate": 1666613853987,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613853987,
        "tmdate": 1666613853987,
        "tddate": null,
        "forum": "gyTuMfkOney",
        "replyto": "gyTuMfkOney",
        "invitation": "ICLR.cc/2023/Conference/Paper1704/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work considers the multi-layer structure of a pre-trained classifier and formulates the information forward from a functional view. Then each test input is transformed into a sequence of layer-wise activations. The authors use the projected distance on the in-class typical direction as the measurement of similarity and propose an OOD detection score by summarizing layer-wise distance. Empirical evaluation shows that the proposed method performs well on four ImageNet benchmarks.",
            "strength_and_weaknesses": "Strength:\n- This work proposes a novel OOD detection method by using a multi-layer pre-trained classifier. \n- The paper is well-written and the technical details are easy to follow.\n- The empirical evidence is significant compared with the baseline methods.\n\nWeakness: \n- The method is derived from a functional point of view. The authors do not discuss the influence of the depth of the pre-trained classifier. If the pre-trained model only has five layers, is it proper to use five-length vectors to approximate functional data?\n- Mood is also a multi-layer OOD detection method and discusses that the image with different data complexity should use feature extractors with different depth. In this work, the authors do not discuss the influence of data complexity and do not compare the proposed method with Mood. Does the summation operation in (8) weaken the sensitivity of the OOD detector?\n- Your results in Table 1 outperform the baseline methods. Comparing with recent progress like KNN with ResNet50 pre-training, the improvement is not significant. Furthermore, if you combine your proposed detector with enhancement methods like ReAct, is it possible to further improve the current performance?\n\nLin, Ziqian, Sreya Dutta Roy, and Yixuan Li. \"Mood: Multi-level out-of-distribution detection.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality, and Reproducibility are good. The paper is well-organized. The experimental evaluation is weak. The key resources and sufficient details are given. I did not run the source code. I think the experimental results are reproducible. Novelty is fair. The proposed detector is derived from a new perspective.",
            "summary_of_the_review": "I recommend marginal acceptance. My reasons and problems are listed in the Weaknesses section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1704/Reviewer_CjT9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1704/Reviewer_CjT9"
        ]
    },
    {
        "id": "ugZ91iUBXy",
        "original": null,
        "number": 3,
        "cdate": 1666676694119,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676694119,
        "tmdate": 1666676694119,
        "tddate": null,
        "forum": "gyTuMfkOney",
        "replyto": "gyTuMfkOney",
        "invitation": "ICLR.cc/2023/Conference/Paper1704/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a feature-based method to get OOD scores that measures the trajectory similarities between the test data and the training data. The method first gets the per-class feature prototype \\mu_{l,y} at each layer based on training data, and then figures out the reference trajectory by calculating the cosine similarity between the training samples and the prototypes. During the inference time, the authors evaluate the trajectory consistencies of test samples. The experimental results show that the proposed method outperforms the baseline and some recently proposed methods.",
            "strength_and_weaknesses": "Pros:\n\nThe author examines the OOD samples during the forward pass through a novel and interesting perspective that takes feature trajectory into account. It might be a more discriminative way to see how the network handles ID and OOD inputs.\n\nCons:\n\n1. In this paper, the proposed method uses the class prototype \\mu_{l,\\hat{y}} corresponding to the network predicted label as the reference. But what if the labels predicted by the network are wrong (we know the top-1 accuracy on ILSVRC 2012 is around 80%)? For a misclassified ID sample, should it be considered as OOD data on the wrong class? The authors use hard labels (i.e., argmax(f(x_0))) to obtain reference trajectories for the corresponding category. What about using soft labels to perform weighted sum (\\sum_i {score}_{i}\\cdot d(z_{l,0}, \\mu_{l,i}))? The authors may explore this in their experiments.\n\n2. The authors need to conduct more detailed ablation study. For example, the authors only take the outputs of five stages (body.block1, body.block2, body.block3, body.block4, head.flatten) as latent features. What if the output after each convolutional layer is tracked? Furthermore, I notice that u_0 in Algorithm 2 is already a trajectory similarity between a test sample and the training features. Whether it is necessary to recalculate the similarity between u_{0} and u needs to be explored in the ablation experiment. And it is best for the author to further analyse the reasons for such a design through theory.\n\n3. The paper is unclear that what shape features are used to calculate cosine similarity. If they are (B, C_l, H_l, W_l), does that introduce a lot of computation (numclass\\times(L+1)\\times B\\times C_l\\times H_l\\times W_l)? I think the authors need to report in detail the additional computational burden introduced by the proposed method, both FLOPs and time-consuming. Kindly remind the author that when implementing the code, you can first select the feature prototype of a specific category by torch.gather, and then calculate the cosine similarity through matrix multiplication or einsum to avoid repeated calculation between input features and all categories.\n\n4. In Table 1, the authors do not compare with the SOTA method, and the performance of the method proposed in this paper has not yet reached the level of SOTA (e.g., KNN[1], DICE[2]). I encourage the authors to further optimize the proposed method during rebuttal for more competitive results. I will consider raising the rating of this paper after the results are further improved.\n\n5. The authors say \"State-of-the-art methods treat step (ii) as a supervised learning problem\" in Section 3.2, paragraph 1 which needs some citations to determine which methods do so.\n\nReference\n\n[1] Sun, Y., Ming, Y., Zhu, X., & Li, Y. Out-of-distribution Detection with Deep Nearest Neighbors. In ICML 2022.\n\n[2] Sun, Y., & Li, Y. Dice: Leveraging sparsification for out-of-distribution detection. In ECCV 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper does not have good clarity. Thus, the quality and novelty are hard to evaluate.",
            "summary_of_the_review": "The paper proposed a feature-based method to get OOD scores that measures the trajectory similarities between the test data and the training data. The method first gets the per-class feature prototype \\mu_{l,y} at each layer based on training data, and then figures out the reference trajectory by calculating the cosine similarity between the training samples and the prototypes. During the inference time, the authors evaluate the trajectory consistencies of test samples. The experimental results show that the proposed method outperforms the baseline and some recently proposed methods. However, this paper does not clearly demonstrate the motivation, making the justification difficult.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1704/Reviewer_8gLg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1704/Reviewer_8gLg"
        ]
    },
    {
        "id": "baFPSqhUzz",
        "original": null,
        "number": 4,
        "cdate": 1666763410659,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666763410659,
        "tmdate": 1666763410659,
        "tddate": null,
        "forum": "gyTuMfkOney",
        "replyto": "gyTuMfkOney",
        "invitation": "ICLR.cc/2023/Conference/Paper1704/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a functional perspective for out-of-distribution (OOD) detection.  The layer-wise features from a deep neural network are considered a trajectory. The class-wise centroids of the trajectories are computed from the training samples. Given a test trajectory, the similarity with respect to the centroids is estimated as a measure for OOD-ness, i.e., lower similarity indicates an OOD sample. Experiments are conducted by considering Imagenet to be in-distribution and various other datasets, such as iNaturalist, SUN, Places, and Textures, as OOD sets.",
            "strength_and_weaknesses": "Strengths:\n\n1. Proposed\u00a0functions perspective for OOD detection is interesting.\u00a0\n\n2. During the test, the similarity is measured with respect to cluster means ignoring the covariance matrices. This significantly improves the complexity, especially\u00a0for the Imagenet with 1000 classes.\u00a0\n\n3. A series of ablation studies are conducted to\u00a0analyze various aspects of the approach.\n\n4. Authors provide the code to facilitate reproducibility.\n\nWeaknesses:\n\n1. Is considering the centroids alone, without the covariance, reliable to detect OOD samples? Have\u00a0the authors experimented with a variant where the similarity is computed considering the covariance matrix as in the Mahalabois\u00a0approach?\n\n2. Authors claim that the state-of-the-art approaches\u00a0use supervision to train the OOD classifier. This statement is not completely correct as many recent approaches do not consider supervision (OOD samples or pseudo-OODs) during training.\n\n3. In Fig 2 b, it appears that the trajectories across ID and OOD samples are similar at various feature layers. How does this affect the robustness of the approach?\n\n4. Some of the recent state-of-the-art approaches are not compared against, such as\n\ni. Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. ICML, 2022.\nii. Vikash Sehwag, Mung Chiang, and Prateek Mittal. SSD: A unified framework for self-supervised outlier detection. ICLR, 2021.\n\n5. Authors are encouraged to consider a commonly used experimental\u00a0setup where CIFAR is used in distribution and SVHN,\u00a0TinyImageNet, and LSUNs are used as OOD. This will help to compare against other state-of-the-art approaches and further justify the efficacy of the approach.\u00a0\n\n6. The clarity and writeup can be improved\n\ni. Fig 1 is hard to follow given the description at that stage.\u00a0\nii. In Fig 2 c, the conclusion is not clear. Only the self-correlation\u00a0is evident from the plot\u00a0the correlations between the features from other layers are somewhat non-uniform.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity can be improved, the proposed approach is somewhat novel. Code is provided. ",
            "summary_of_the_review": "While the proposed idea of considering feature trajectories is interesting, it requires additional experimental validations to justify the efficacy of the approach. Please address the comments in the weaknesses section. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1704/Reviewer_aTaH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1704/Reviewer_aTaH"
        ]
    }
]