[
    {
        "id": "skNrlE22y3",
        "original": null,
        "number": 1,
        "cdate": 1666584813026,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584813026,
        "tmdate": 1666584813026,
        "tddate": null,
        "forum": "ygYXtbb3og3",
        "replyto": "ygYXtbb3og3",
        "invitation": "ICLR.cc/2023/Conference/Paper1028/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to:\n\n1. Estimate the mutual information between state and action\n\n2. Use the estimate as regularization, by adding it to the policy and value function objective.",
            "strength_and_weaknesses": "Strength \n\nThe idea of estimating the mutual information between state and action pair and using the estimates to improve policy evaluation and improvement are intuitive.\n\nWeaknesses\n\nThe writing is not very clear. I list a few examples below: \n\n- In the abstract, \"Intuitively, mutual information \u2026 reflects how a behavior agent reacts to certain environment states during data collection\". While this is factually true, I fail to see how this is different from simply estimating the behavior policy. \n\n- The abstract also writes \"equivalent to maximizing the likeli- hood of a one-step improved policy on the offline dataset. In this way, we constrain the policy improvement direction to lie in the data manifold.\" I do not see how the former leads to the latter, or even what the latter means at all. AFAIK, the data manifold is not defined in the paper at all.\n\n- The introduction writes \"Though these methods are effective at alleviating the distributional shift problem of the learning policy, the im- proved policy is unconstrained and might still deviate from the data distribution.\" This sentence seems self-contradictory. If these methods are effective at alleviating the issue of distribution shift, then how can the learned policy still deviate from the data distribution.\n\nIn addition to writing, the paper also has two other weaknesses:\n\n- The paper claims TD3+BC and CQL are special cases, but it is unclear why their method outperforms these methods. The paper claims that their method leads to a better mutual information estimation. But what does \"better\" mean, and why does that lead to better performance compared to CQL? This is not explained at all.\n\n-  The method does not perform well when learning from uniform data. How about data where the behavior is uniform in some state, but much more constrained in others, which is the more realistic settings? The limitations of the methods seem to be an afterthought that does not receive adequate analysis.",
            "clarity,_quality,_novelty_and_reproducibility": "The method is novel as far as I know. However, the writing is not clear and too generic. The paper does not release code, so I also do not have high rating as far as reproducibility is concerned.",
            "summary_of_the_review": "The proposed idea is novel and intuitive. But the paper writing can be significantly improved, and why the method outperforms baselines deserve more analysis to be more convincing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1028/Reviewer_SVYL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1028/Reviewer_SVYL"
        ]
    },
    {
        "id": "mxxWmcHBMvK",
        "original": null,
        "number": 2,
        "cdate": 1666624993575,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624993575,
        "tmdate": 1666624993575,
        "tddate": null,
        "forum": "ygYXtbb3og3",
        "replyto": "ygYXtbb3og3",
        "invitation": "ICLR.cc/2023/Conference/Paper1028/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new framework for handling distribution shift in offline RL, based on a mutual information based regularizer. The mutual information term gives a measure of the data collecting policy's actions on certain states. The regularizer computes a lower bound to the overall policy improvement step and the paper shows how this lower bound can still be useful for leading to overall improvements in offline RL. ",
            "strength_and_weaknesses": "\t- The paper proposes a novel regularizer for offline RL and provides intuitions for why such a regularizer might be useful. Existing works have proposed conservative updates to offline RL, but this paper argues the need for such a regularizer. \n\t- Mutual information based regularizers have been well studied in online RL literature; so it is interesting to see a use case for it in offline RL; the lower bound is computed using existing approaches in the literature however (lemma 3.1 and 3.2)\n\t- The main claim of the paper is that this framework can unify two popular offline RL algorithms, namely CQL and TD3+BC. Overall, it leads to the existing algorithms being modified based on equations 5 and 6 and the work shows that this regularizer can lead to a lower bound to the policy improvement step (theorem 4.1)\n\t- The theoretical justification of this work, along with the derived algorithm, is well explained. However, such techniques are not completely novel and adapts from existing approaches. For example, equation 12 for integrating the regularizer into an offline RL framework is somewhat expected where the additonal term is required based on the approximation of the MI term. \n\t- Section 4.4 provides interesting insights for the justification of how the regularizer unifies existing algorithms. \n\t- Experiments are primarily done on the D4RL benchmark, and it seems there are marginal improvements due to the addition of the regularizer. However, I am surprised that even in appendix the full result plots are not shown, and it is difficult to judge the actual empirical significance of this work. \n\t- Overall, I think the paper is useful in terms of adapting a well known regularizer from online RL into offline RL. However, the resulting approximations for the MI term, and the way it is adapted into offline RL is not completely novel. Even if the algorithm is well derived from existing literature, the empirical significance of the work is not fully clear either. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The novelty of this work is somewhat limited, given the approximations for the regularizer have been extensively studied in the past. The paper is however well written and easy to follow, with the theoretical derivation for the resulting updates properly derived. I do have concerns about the reproducibility of the work, since the results are only presented in the form of a table, without actual return plots, and it is not clear to me whether there is a bias variance trade-off due to the addition of this regularizer. Full results not being presented is a major concern for the empirical significance of this work. ",
            "summary_of_the_review": "The work proposes a novel regularizer based on mutual information. This form of regularizer has been popular in online RL literature, and past works studied approximations to such MI terms. Therefore, the contributions and novelty of this work seems somewhat limited to me. Additionally, the empirical significance of the work is not clear enough. I am willing to re-evaluate my score if the paper clarity can be improved, and experimental significance made clearer. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1028/Reviewer_aonx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1028/Reviewer_aonx"
        ]
    },
    {
        "id": "OQU6HolZ7R",
        "original": null,
        "number": 3,
        "cdate": 1666668789489,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668789489,
        "tmdate": 1668876196665,
        "tddate": null,
        "forum": "ygYXtbb3og3",
        "replyto": "ygYXtbb3og3",
        "invitation": "ICLR.cc/2023/Conference/Paper1028/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a regularization method for constraining policy improvements in offline RL. The proposed regularization is based on a lower bound to the mutual information between states and actions, and attempts to mitigate the issue of distribution shift that arises when the policy is queried on out-of-distribution actions.",
            "strength_and_weaknesses": "Strengths\n\n1. The proposed regularization is simple to implement and is well-motivated. It can be easily integrated with most offline RL framework as an additional term in the objective. \n\n2. The practical algorithm is discussed in detail and implementation details are clearly mentioned. \n\n3. The ablations and embedding visualizations provide nice intuitions about what MISA is learning empirically. \n\nWeaknesses\n\n1. Conceptually it is unclear how exactly is MUSA better than prior methods like CQL in mitigating distribution shift. There isn't enough intuition provided regarding this, beyond reference to experimental results and \"better mutual information estimation\" (in section 4.4) Why exactly should the MI estimation be better?\n\n2. The empirical results do not show strong benefit of MISA over baselines, partly because the hopper, cheetah, walker tasks are almost saturated. For the other tasks in Kitchen, Adroit, and Maze, not all baselines are evaluated, so empirical benefits are unclear.\n\n3. The lower bound result in Theorem 4.1 does not discuss anything about how tight the bound is, and whether the bound still holds in practice (i.e. in the experiments) with several approximations, including function approximations.",
            "clarity,_quality,_novelty_and_reproducibility": "The contribution is novel in my understanding, and sufficient details for experiments are provided. The writing is clear and easy to follow.",
            "summary_of_the_review": "Although there are some weaknesss especially with expeirments that I have pointed out, and which should be addressed, I am leaning towards acceptance because the key insight is simple, neat, and seems to be promising for reducing distribution shift in offline RL. However, the additional experiments/comaprisons are needed to confirm the beenfits. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1028/Reviewer_mLZD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1028/Reviewer_mLZD"
        ]
    },
    {
        "id": "-eH_0KaYuq",
        "original": null,
        "number": 4,
        "cdate": 1667110173593,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667110173593,
        "tmdate": 1670275238015,
        "tddate": null,
        "forum": "ygYXtbb3og3",
        "replyto": "ygYXtbb3og3",
        "invitation": "ICLR.cc/2023/Conference/Paper1028/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present MISA, a framework for offline RL based on mutual information based regularization over states and actions. The authors motivate their approach step by step and demonstrate how TD3+Behavior cloning and CQL are related to their proposed approach. Finally, the authors provide empirical results relative to key baselines on the D4RL benchmark while also providing ablation studies and analysis. ",
            "strength_and_weaknesses": "Strengths: \n- The authors present a clear buildup to their approach in math and justify each step.\n- The authors contextualize their approach pretty well with respect to prior art on a theoretical level.\n- The approach gets pretty good performance relative to baselines on the D4RL benchmark. \n\nWeaknesses: \n- The novelty is not terribly high in comparison to past work on offline RL and mutual information estimation. \n- The introduction of the mutual information regularization in equations 5 and 6 struck me as quite intuitive in nature and not really originating from first principles.\n\nBoth: \n- I do really appreciate that the authors clearly highlight limitations of their approach when doing learning from data that is using far from an expert policy. However, on the other hand, it is quite a significant limitation relative to baselines. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality is pretty good overall and I really appreciate the theoretical discourse of this paper. The novelty is not very high, but I do appreciate that the authors also clearly present their contribution relative to prior art. The authors have provided their code in the supplemental material. ",
            "summary_of_the_review": "My biggest concerns about this paper are the relatively low amount of novelty and the fact that the overall motivation is a bit intuitive in nature. However, I lean towards acceptance because I do really appreciate the theoretical discourse and experiments. I feel that in light of this context the authors have provided, the paper is fleshed out enough to make a contribution to the conference, particularly because of its focus on such an important area. \n\nUpdate After Author Response: \n\nI have read through the other reviews and responses to each review. I have kept my score unchanged, but I can't really serve as a strong advocate for this paper either. It is quite outside my area of expertise, so I feel that my confidence is quite low about what the right thing to do here is as I sympathize with all of the points made.\n\nI agree that the novelty is not huge here, but also hear the authors that it is a significant contribution beyond the past literature. However, the results not being too significant could indeed be a potential issue for adoption of this method. Moreover, I think the authors are too quick to discount the importance of not performing well on data from a uniform policy. It is certainly not common for offline RL benchmarks, but if the data compiled by our behavior policy is already \"expert\" level then offline RL is only improving things at the margins. In typical off-policy RL we start with a uniform policy and while I acknowledge that these are certainly different settings, I think offline RL should be considered harder than off-policy RL (due to lack of access to the underlying behavior policy and lack of access to a simulator during training) and not a strictly easier setting (akin to some slightly more ambitious form of imitation learning).\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1028/Reviewer_ZgDk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1028/Reviewer_ZgDk"
        ]
    }
]