[
    {
        "id": "x2-WqMX20Uo",
        "original": null,
        "number": 1,
        "cdate": 1666649314303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649314303,
        "tmdate": 1666649882268,
        "tddate": null,
        "forum": "VA1YpcNr7ul",
        "replyto": "VA1YpcNr7ul",
        "invitation": "ICLR.cc/2023/Conference/Paper886/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel propose novel methods improving the theoretical oracle and communication complexity of the previous state-of-the-art method. New methods can compress vectors, which makes them more useful in federated learning",
            "strength_and_weaknesses": "Strength: This paper proposes a novel propose novel methods improving the theoretical oracle and communication complexity of the previous state-of-the-art method. This paper is theoretically sound in general\n\nWeaknesses: Some experiments on federated learning should be done to verify the performance.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is presented clearly,  only minor flaws.",
            "summary_of_the_review": "This paper proposes a novel propose novel methods improving the theoretical oracle and communication complexity of the previous state-of-the-art method. New methods can compress vectors, which makes them more useful in federated learning\nThe paper is theoretically sound, and the current version can be significantly improved in the following aspects:\n1.\tPage 21, 1/n^2  \u2211_(i=1)^nE_C[...] should be  1/n  \u2211_(i=1)^nE_C[...].\n2.   Page 29,   E_h [\u2016h^(t+1)-\u2207f(x^(t+1) )\u2016^2 ]\u2264  ((1-p))/(n^2 B^2 )..... should be E_h [\u2016h^(t+1)-\u2207f(x^(t+1) )\u2016^2 ]\u2264  ((1-p))/(n B^2 )..... \n3.  Finally, some experiments on federated learning should be done to verify the performance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper886/Reviewer_cCdR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper886/Reviewer_cCdR"
        ]
    },
    {
        "id": "64XQHfkHQRK",
        "original": null,
        "number": 2,
        "cdate": 1666653342151,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653342151,
        "tmdate": 1666653342151,
        "tddate": null,
        "forum": "VA1YpcNr7ul",
        "replyto": "VA1YpcNr7ul",
        "invitation": "ICLR.cc/2023/Conference/Paper886/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The focus of this paper is on distributed nonconvex smooth optimization in a setting where there is synchronization among the $n$ workers and there is a central server that is coordinating communications among the workers. The authors are interested in obtaining an $\\varepsilon$-approximate first-order stationary point as a solution to three variants of the finite-sum distributed optimization problem in which the objective function $f$ is respectively:\n\n- $f(x) = \\frac{1}{n} \\sum_{i=1}^n f_i(x)$ (Gradient Setting)\n- Each local function $f_i(x)$ has a further finite-sum form (Finite-Sum Setting)\n- Each local function is expectation of a stochastic function (Stochastic Setting)\n\nThe authors aim to reduce the number of communications rounds needed in these three settings to achieve optimal oracle complexity, and this is accomplished through the use of independent, unbiased compressors at each worker. The resulting algorithms are respectively termed DASHA, DASHA-PAGE, DASHA-MVR, and DASHA-SYNC-MVR, where DASHA-SYNC-MVR is a variant of DASHA-MVR in which workers synchronize with each other every once in a while using uncompressed messages.",
            "strength_and_weaknesses": "**Strengths**\n- The authors provide rigorous theoretical guarantees for the four different variants of DASHA for distributed nonconvex optimization. The results show that DASHA and DASHA-PAGE improve on the SOTA communications complexity without requiring any uncompressed synchronization steps. The variant DASHA-SYNC-MVR also improves on the SOTA communications complexity while requiring communications of uncompressed messages.\n- Results of a substantial number of numerical experiments are provided in the paper to showcase the superiority of DASHA over the SOTA algorithm MARINA.\n\n**Weaknesses**\n- DASHA is a mash-up between MARINA and existing distributed nonconvex optimization methods. Other than the fact that three variants of DASHA get rid of the uncompressed synchronization in MARINA, this reviewer could not pinpoint a difference between MARINA and DASHA. As such, the main novelty of this work seems to be in terms of theoretical analysis of MARINA when the uncompressed synchronization step is removed. The authors could have done a better job of clarifying where does this novelty lie in the analysis (e.g., pinpointing the key analytical approaches in the lemma that helped improve the analysis).\n- The experimental results are not described clearly in my opinion. It is not clear from reading the description how did the authors count the number of bits. Additionally, how exactly was the fine tuning done for the step sizes in the experiments? The results for stochastic setting don't seem as convincing for the case of $K = 2000$ and the choice of $B=1$ also seems strange.\n\n**Some Additional Feedback**\n- The mathematical expressions in the abstract have symbols that are not defined in the abstract and one has to guess about them till one gets to the later part of the paper.\n- Section 1.1: Might be useful to clarify what is the expectation going to be over.\n- Equation (2): We have $\\frac{1}{n}$ in both this equation and in (1). Is this right?\n- Definition 1.1: Does $\\omega$ really belong to $\\mathbb{R}$? Can it be negative?\n- Section 4, Third paragraph: Why does it say that it provides a \"suboptimal oracle complexity w.r.t. $\\omega$\"? Are not the additional factors not a function of $\\omega$?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality**\n- The paper is very well written and it was a pleasure to read it.\n\n**Novelty**\n- As noted above under weaknesses, the novelty of this paper in terms of algorithmic development is minimal. One might even argue if the class of algorithms in this paper should be given a new name or should the name be a variant of MARINA. The main novelty of this work is in terms of theoretical developments, but the authors need to do a better job of clarifying the novelty of that part of the work.\n\n**Reproducibility**\n- The paper follows the narrow definition of reproducibility in the sense that a GitHub repo is present for this paper. However, the ReadMe for the repo is extremely minimal and does not satisfy the requirements for full reproducibility in my opinion.",
            "summary_of_the_review": "Overall, it is a very nicely written paper and was a real pleasure to read. The algorithmic novelty of this work seems to be overstated. The theoretical novelty of the work needs to clarified better so that the reader can appreciate it. The experiment results also need to be described in a better fashion, rather than simply being presented.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper886/Reviewer_f95g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper886/Reviewer_f95g"
        ]
    },
    {
        "id": "rNHtvf6mo4",
        "original": null,
        "number": 3,
        "cdate": 1667100202950,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667100202950,
        "tmdate": 1667100202950,
        "tddate": null,
        "forum": "VA1YpcNr7ul",
        "replyto": "VA1YpcNr7ul",
        "invitation": "ICLR.cc/2023/Conference/Paper886/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The goal of the paper is to solve, to $\\epsilon$-additive accuracy, finite-sum non-convex optimization under the coordinator model of distributed computation, with as small a communication cost as possible. Each server (there are $n$ of them) has access to an oracle: the paper studies three natural variants, the gradient, the mini-batch gradient, and stochastic gradient.",
            "strength_and_weaknesses": "STRENGTHS - I think the paper studies an important problem, is very clearly written, and seems to have done a reasonable job of comparing against prior work. \n\nWEAKNESSES - This isn't really a weakness, rather a question I have (perhaps the authors can address this in their rebuttal) --- Is the proposed algorithm a \"distributed version\" of SVRG (or some algorithm in that family)? Again, even if it is, I don't think that's a problem, I just think (if it is), it would help contextualize the paper better by discussing this connection.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is very nicely written! ",
            "summary_of_the_review": "I recommend acceptance of this paper: the problem is interesting in its own right and of great practical importance to modern machine learning where functions are potentially nonconvex and dimensions large, and the algorithms make intuitive sense. I also appreciate the effort put into keeping the paper clear and easy-to-understand. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper886/Reviewer_xs5t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper886/Reviewer_xs5t"
        ]
    },
    {
        "id": "8uUegkYvBs",
        "original": null,
        "number": 4,
        "cdate": 1667220772697,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667220772697,
        "tmdate": 1667220772697,
        "tddate": null,
        "forum": "VA1YpcNr7ul",
        "replyto": "VA1YpcNr7ul",
        "invitation": "ICLR.cc/2023/Conference/Paper886/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies convergence of compressed SGD to a first-order stationary point. The paper considers unbiased compressors and improves state-of-the-art convergence rate and communication complexity for both finite-sum and stochastic settings. The algorithms presented in the paper differ in update rules and their requirement for periodical synchronization. The paper uses unbiased RandK compressor with the proper K to avoid synchronization.\n",
            "strength_and_weaknesses": "I have the following suggestions about the paper:\n-- I think the paper would benefit from outlining the proofs\n-- The proofs look similar to that of MARINA. Can you please elaborate on the differences between the techniques in these two papers?\n",
            "clarity,_quality,_novelty_and_reproducibility": "I would suggest the following changes:\n-- I believe it would be better to extract Line 8 of Algorithm 1 into a separate equation.\n-- I think it\u2019s better to just follow footnote 2, and  replace all smoothness constants with L.\n",
            "summary_of_the_review": "Solid paper, accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper886/Reviewer_hqd4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper886/Reviewer_hqd4"
        ]
    }
]