[
    {
        "id": "5MBpkue_QbO",
        "original": null,
        "number": 1,
        "cdate": 1665839777810,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665839777810,
        "tmdate": 1669621382852,
        "tddate": null,
        "forum": "yIxtevizEA",
        "replyto": "yIxtevizEA",
        "invitation": "ICLR.cc/2023/Conference/Paper3516/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the computational bottleneck problem of Transformer Neural Processes (TNPs) that results from the quadratic complexity of transformers with respect to the input sequence length. The paper aims to combine the best of the worlds of state-of-the-art expressive power at the expense of high compute cost (TNPs) and scalable computation at the expense of suboptimal performance (other NP variants). The paper proposes a modification to the TNP pipeline to trade these competing goals using a fixed-sized attentive memory. The main idea is to embed the input into a fixed-dimensional latent space and model high-order interactions among data points as cross-attention to this embedding as opposed to self attention in the native observation space.",
            "strength_and_weaknesses": "**Strengths:**\n\n * The paper reports a comprehensive set of experimental results, where the proposed method appears to indeed approach the TNP performance in reduced computational cost.\n* The proposed idea is rather simple, easy to grasp and straightforward to implement. It is an original, though trivial, combination of a number of known architectural attention structures.\n\n\n**Weaknesses:**\n\n * It is not clear to me why quadratic complexity on the context set is such a severe limitation. Is it not the common assumption of NPs that the context set is rather small? Similarly, I assume the target queries are iid when conditioned on the context. Then how critical is it in reality for a target batch to be big? Can one simply not process small query batches even in parallel?\n* The proposed novelty is very incremental and straightforward. It does indeed improve performance, but I would categorize the presented method rather as a useful implementation trick than as a solid scientific finding.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a clear and precise scientific language. I only find Figures 1 and 2 not sufficiently descriptive. They were not useful for me to understand the core idea, as they contain only a complex block diagram with lots of abbreviations. I would prefer the description of the *original idea* instead of architectural details.\n\nThe paper contains sufficient details to reproduce the reported results.\n\nThe proposed method is novel only in the sense that some well-known architectural elements such as latent embeddings and cross-attention are used in the very context probably for the first time.\n\n\n--- After rebuttal ---\n\nWhile I appreciate very much the authors' effort, I'm afraid my concerns are not addressed. The author response echoes the statements already mentioned in the paper, which I already read carefully and understood before writing my review. And I do not find these very arguments convincing. NPs are designed for small context sets as they are typical building blocks for few-shot learning setups. In the cases where one needs to extract context from massive data, one can always employ representative point selection (or learning) methods or simply backpropagate the gradients up till the context entries (or their low-dimensional embeddings) akin to the learnable inducing points for Gaussian Processes. While the solution presented in the paper has some degree of originality, I'm afraid I still do not see its scientific value. Furthermore, the experiments do not appear to be designed to prime realistic applications where a big context sets could indeed be a problem.",
            "summary_of_the_review": "While being a decent piece of work with some degree of originality and some promising results, I find the scientific contribution of the paper too slim for a main-track ICLR paper. I am also not yet convinced about the significance of the studied problem (see first bullet item under Weaknesses).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3516/Reviewer_peD2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3516/Reviewer_peD2"
        ]
    },
    {
        "id": "los-4aq8a8R",
        "original": null,
        "number": 2,
        "cdate": 1666485940758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666485940758,
        "tmdate": 1670371486147,
        "tddate": null,
        "forum": "yIxtevizEA",
        "replyto": "yIxtevizEA",
        "invitation": "ICLR.cc/2023/Conference/Paper3516/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new member of neural processes. The aim is to improve the attention efficiency of transformer neural process where the attention strategy requires quadratic computation with respect to the number of context data points.  The idea is from iterative attention (Jaegle et al., 2021b) which changes the self-attentions between context data points to a cross-attention with latent variable and a self-attention between latent variable. When the size of the latent variable is significantly smaller than the one of context size, such an exchange could greatly reduce the total computational cost. ",
            "strength_and_weaknesses": "Strength\n\n1. The motivation is clear and the proposed idea looks reasonable.\n2. There are extensive empirical evaluations and the performance looks good and comparable with transformer NP\n\nWeaknesses\n\n1. Considering the main contribution of this work is to reduce the computational cost, there should be some empirical evaluation on the time complexity in addition to table 6 so let the readers know how much time we can save from transformer NP when achieves similar results. \n\n2. The table 6 is a little unfair, because there is L in the new iterative attention. As shown in Figure 4, the size of L apparently heavily affects on the final performance. It would be better to include it in the time complexity. \n\n3. It is expected to see more discussions or analysis on the effect of this new attention comparing with previous one. For example, will it affect on the convergence rate? Will it break the process property of the neural processes because the proposed method introduce more `constraints' on the latent variables which are assumed to be independent before, like NP and CNP? like permutation invariant and marginal consistency? \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and written,  and figures and tables are clear and easily to understand. It is easy to follow the motivation and idea. \n\nHowever, the novelty is a little weak. The only contribution is to introduce the iterative attention to replace the previous cross-attention in transformer NP without much analysis on the possible effects. ",
            "summary_of_the_review": "The paper is well written and easy to follow. The motivation is clear and method is reasonable and effective. The main weakness is the in-depth analysis on the new iterative attention to the base neural process. please see the `Strength And Weaknesses' for more details. \n\n\n---- after rebuttal \n\nThanks to the authors to clarify my concerns in the response. I do believe the proposed method could save a significant amount of computational time and memory according to the new results. Some other issues are resolved as well. I appreciate that. So I am happy to change my score from 5 to 6. The reason why I did not give 8 is the novelty reason, although I believe the proposed technique could improve the efficiency of TNP a lot and I recognize its value to the area, the technique itself is mainly from the existing work - iterative attention. I read the authors' response (to all reviewers) carefully but there is no explanation of the 'new' design and technique proposed compared with iterative attention. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3516/Reviewer_vZAK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3516/Reviewer_vZAK"
        ]
    },
    {
        "id": "FJ6pxYlKYl",
        "original": null,
        "number": 3,
        "cdate": 1666629053017,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629053017,
        "tmdate": 1670278818855,
        "tddate": null,
        "forum": "yIxtevizEA",
        "replyto": "yIxtevizEA",
        "invitation": "ICLR.cc/2023/Conference/Paper3516/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "TNP uses GPT layers and shows good performance. But its memory complexity $O((N+M)^2)$ could be large if $N$ is large. The paper proposes a latent-bottleneck of $L$ vectors that mediate between the $N$ context points and the $M$ target points. By doing this, the authors are able to reduce the complexity to $O(NL + ML)$ where $L$ is the number of latent vectors and a human-specified hyperparameter.",
            "strength_and_weaknesses": "### Pros\n\n1. The use of latent-bottleneck in the context of TNPs is novel and interesting.\n2. Shows better performance than pre-TNP baselines. Claims to show better memory complexity than TNP through a big-O analysis. Naturally, the performance is expected to be worse than TNP and this is empirically confirmed. But this performance drop can be traded-off with gains in memory saving shown via the big-O complexity.\n3. The comparison with EQTNP is useful as a natural variant of TNP. The fact that EQTNP performs a bit worse than TNP-D is new knowledge.\n\n### Weaknesses/Questions\n\n1. $L$ is treated as a pre-specified constant. However, if the tasks are indeed too complex/difficult such that they typically require a large $N$, then users are likely to choose a proportionately large  $L$ also to handle the underlying difficulty of the tasks. In other words, the task difficulty is a confounder that affects both $N$ and $L$ simultaneously. So, it might be tricky to say that the complexity is simply $O(N+M)$ while completely omitting $L$. It might be more justified to say $O((N+M)L)$?\n2. In experiments, the choice of $L$ should also be better justified given what distribution of $N$ values was used (perhaps reporting $L$ as a fraction of the mean value of $N$?). For instance, in the 1-D regression problem, $N$ takes values in $[3,47)$, and in this case, showing $L=128$ (which seems much larger than any $N$) seems to defeat the purpose of having latents? \n3. A way to address the above concerns might be to show the actual GPU memory consumption at test time as a function of $N$ and $L$ and compare LBANP\u2019s memory savings relative to TNP. I am unsure what would be the best way though and I leave it up to the authors to decide the best way. Also, what was the memory cap that prevented running some experiments in Table 3?\n4. The interleaving seems to make the proposed model deviate from the standard GPT layers \u2014 something that, to me, was a big appeal of TNPs. Why is interleaving important in EQTNP and also in LBANP? Why not first do encoding of the context points with a standard transformer and then later do the prediction of target points with another standard transformer decoder? An ablation could be helpful. \n\n### Minor Comments/Questions\n\n1. Would be good to show a row for the computational complexity of EQTNP in Table 1 rather than only in Table 6.\n2. In Section 4.3, the Normal distribution may be shown with a calligraphic $\\mathcal{N}$.",
            "clarity,_quality,_novelty_and_reproducibility": "The closest works seem to me to be TNP and ANP. With respect to these, to my knowledge, the model is novel. The quality and clarity is good. The authors have also promised to release the code.",
            "summary_of_the_review": "I believe in the promise and value of introducing the latent bottleneck and I tend to believe in the reduced big-O complexity. But in experiments, reporting the actual saving in memory should go hand-in-hand with the reporting of the performance. I am currently rating the paper as 6 in the hope that the authors will be able to add more justification. I am happy to revise my rating in the light of new information.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3516/Reviewer_RFme"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3516/Reviewer_RFme"
        ]
    }
]