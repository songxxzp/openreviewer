[
    {
        "id": "Z2VvKIEaEo",
        "original": null,
        "number": 1,
        "cdate": 1666507447675,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666507447675,
        "tmdate": 1666507447675,
        "tddate": null,
        "forum": "dfDv0WU853R",
        "replyto": "dfDv0WU853R",
        "invitation": "ICLR.cc/2023/Conference/Paper2306/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the offline Reinforcement Learning setting, and more precisely the problem of computing Q-Values from transitions in the dataset, where bootstrapping from out-of-distribution actions (actions not in the dataset) may lead to over-estimation and poor learning. The proposed solution builds on sampling importance-resampling, and changes how the transitions used for learning are sampled from the dataset, to sample \"good\" (in-distribution) samples more often. These transitions are used to train a critic (using a somewhat conventional TD loss, with the main contribution being the sampling and not a new loss), and an actor, in the loss of which the estimated behavior policy of the dataset appears.\n\nExperimental results are thorough and show that the proposed method outperforms several baselines on challenging continuous-action tasks.",
            "strength_and_weaknesses": "Strengths:\n\n- The problem being considered is important and an active area of research. The proposed solution is elegant and outperforms the baselines. It also seems relatively easy to implement and computationally cheap (except that a SumTree needs to be added to the experience buffer)\n- The paper is clear and well-written, and theoretical details are given to motivate the approach, and justify the results.\n- Section 4.4, the practical implementation, is very well-written and concisely gives all the details needed for implementing the algorithm\n\nWeaknesses:\n\n- I did not find any big weakness of this work, only minor comments.\n- Starting from Equation 4, the notation mixes $\\Delta$ and $\\nabla$, without a (one-line) explanation of what these symbols mean. It is a bit weird to compute a TD-error from a gradient, so the equation looks like the $\\Delta$ is also a gradient (the gradient of the Q update, not the update itself). I find this quite confusing and explanations of the notations would be welcome.\n\nQuestion:\n\n- For offline RL papers, I'm always curious of what happens if the proposed algorithm is used in an online setting. In the case of this paper, is it possible to, every so often, let $\\pi_{\\theta}$ execute an episode in the environment, add the resulting transitions to the experience buffer, and continue learning from that?",
            "clarity,_quality,_novelty_and_reproducibility": "- Quality: high. The proposed method is well-presented. It is theoretically analyzed in a sound way, and the empirical evaluation is thorough and well-designed (with strong baselines and challenging environments)\n- Clarity: medium-high. The paper is easy to follow for someone very familiar with offline RL algorithms, but I have the feeling that a quick reader, or someone less familiar with offline RL, may find the paper less clear. For instance, neither the abstract nor the introduction mention the simple summary of the paper: \"we modify how samples are sampled from the experience buffer to correct for out-of-distribution actions\". Sentences like \"utilizes sampling-importance resampling to execute in-sample policy evaluation\" are only understandable by people already familiar with sampling-importance resampling.\n- Originality: average. The proposed method seems novel, and sampling-importance resampling is not used often, but it is quite close to Prioritized ER (mentioned by the authors in the paper). This does not lower the value of the paper, though.",
            "summary_of_the_review": "The paper considers an important problem, proposes an elegant solution with a theoretical analysis, and empirically shows that the proposed method outperforms many baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2306/Reviewer_nmzD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2306/Reviewer_nmzD"
        ]
    },
    {
        "id": "E2GZvzfkPyg",
        "original": null,
        "number": 2,
        "cdate": 1666833055542,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666833055542,
        "tmdate": 1666833055542,
        "tddate": null,
        "forum": "dfDv0WU853R",
        "replyto": "dfDv0WU853R",
        "invitation": "ICLR.cc/2023/Conference/Paper2306/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents In-sample Actor-Critic (IAC), an algorithm for offline RL. The policy evaluation of IAC is done only using in-distribution samples of the dataset, where each sample's (normalized) importance ratio is used for resampling probabilities. Then, TD updates are performed using the resampled samples. For policy improvement, advantage-weighted regression is used to ensure that the learning policy samples actions within dataset support. The SIR is a consistent estimator and has a lower variance compared to the standard importance sampling. Experimental results show that IAC performs competitively with the state-of-the-art methods on Gym-Mujoco locomotion tasks and AntMaze tasks.\n",
            "strength_and_weaknesses": "[Strengths]\n1. This paper presents an offline RL algorithm exploiting in-sample learning based on importance resampling, which enables stable learning yet achieves good empirical performance.\n\n\n[Weaknesses]\n1. It seems technical novelty is limited, given that IAC can be understood of a combination of Importance Resampling [1] for policy evaluation and AWAC[2]/CRR[3] for the policy improvement part.\n2. The algorithm requires behavior density estimation, which introduces additional computational costs and hyperparameters.\n3. In the experiments, comparison with some baselines is missing, e.g. AWAC and CRR[3], which are using similar approaches. Also, it would be great to compare with the method using self-normalized importance sampling instead of SIR.\n\n\n[Questions and comments]\n1. I am wondering if using state-distribution correction $d^\\pi(s) / d^\\mu(s)$ can further improve the performance.\n2. In addition to the OneStepRL and IQL, OptiDICE [4] only requires in-distribution samples $(s,a,r,s')$ of the offline dataset too.\n3. I am curious to see the result when using different $\\eta$ in Eq (12), e.g. $\\{0.1, 0.3, 0.5, 0.7, 0.9\\}$. Does $\\eta=1$ always perform the best?\n\n\n[1] Schlegel et al., Importance Resampling for Off-policy Prediction, 2019\n\n[2] Peng et al., Advantage-weighted regression: Simple and scalable off-policy reinforcement learning, 2019\n\n[3] Weng et al., Critic Regularized Regression, 2020\n\n[4] Lee et al., OptiDICE: Offline Policy Optimization via Stationary Distribution Correction Estimation, 2021\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written and easy to follow. It seems that the technical novelty is limited given that IAC is a combination of existing methods for policy evaluation and policy improvement. It seems Theorem 3 of the paper is very similar to Corollary 3.1.1 in [1], but it was not properly mentioned.\n\n[1] Schlegel et al., Importance Resampling for Off-policy Prediction, 2019\n",
            "summary_of_the_review": "I think this paper presents a sound offline RL algorithm of in-sample learning. My main concerns are technical novelty and some missing baselines in the experiments. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2306/Reviewer_uBTY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2306/Reviewer_uBTY"
        ]
    },
    {
        "id": "9kBrBs3N2q",
        "original": null,
        "number": 3,
        "cdate": 1666872780468,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666872780468,
        "tmdate": 1666872780468,
        "tddate": null,
        "forum": "dfDv0WU853R",
        "replyto": "dfDv0WU853R",
        "invitation": "ICLR.cc/2023/Conference/Paper2306/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper applied the sampling-importance resampling methods to offline learning to conduct in-sample learning. The methods in this paper fit well into the line of in-sample offline RL learning.\n\n",
            "strength_and_weaknesses": "\n\nStrength:\n\nThe method is very clear and makes sense for in-sample offline learning.\n\nThe paper contains several solid theoretical analyses of the method.\n\nThe experiments on Gym locomotion tasks and ANtMaze tasks are sufficient to show the effectiveness of the method.\n\nWeakness:\n\nOne limitation of the method is that it needs to know the probability of the in-sample action in the dataset. Although the paper also presented the result by removing the density estimator, this method does not perform well on AntMaze problem. Can you give the reason why the method does not perform well on the AntMaze-UMaze and AntMaze-Large datasets? The paper also didn't report the training time for using a density estimator.\n\nThe density estimator only uses Gaussian distribution to model the distribution. This often could limit the capacity as in practice the distribution of the dataset could hardly be a unimodal distribution.\n\n\n\nSeveral Questions:\n\nWhat's the training time for using a density estimator?\n\nI cannot see why the proposed method outperforms other in-sample offline RL methods. Could the author provide some insights on this? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n\nThe presentation of the paper is clear.\n\nThe methods in this paper fit well into the line of in-sample offline RL learning.\n\nThe implementation of the paper is not open-sourced. Open-source code is not compulsory, however, it could help readers understand some details of the methods.",
            "summary_of_the_review": "\n\nThe presentation of the paper is very clear and makes sense for in-sample offline learning. The theoretical analyses of the method are solid and well support the main idea of the paper. The experiments on various tasks are also sufficient to prove the effectiveness of the method.\n\nAlthough the method proposed in this paper is very interesting and contributes to the community, I cannot see why the superiority of the proposed method compared to existing other in-sample methods. \n\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": " ",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2306/Reviewer_Y1J6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2306/Reviewer_Y1J6"
        ]
    },
    {
        "id": "EIxMwTr7gD4",
        "original": null,
        "number": 4,
        "cdate": 1667025707799,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667025707799,
        "tmdate": 1667025707799,
        "tddate": null,
        "forum": "dfDv0WU853R",
        "replyto": "dfDv0WU853R",
        "invitation": "ICLR.cc/2023/Conference/Paper2306/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes In-sample Actor Critic (IAC), a new algorithm for offline reinforcement learning. The main idea is to avoid extrapolation error by explicitly performing in-sample policy evaluation to learn the critic. To implement this, IAC applies sampling-importance resampling which has lower variance than a naive importance sampling implementation. The paper provides empirical studies in Mujoco locomotion control and AntMaze showing that IAC contains competitive performance compared to the state-of-the-art algorithms. ",
            "strength_and_weaknesses": "Strength\n\nAs discussed in the paper, offline RL algorithms suffer from extrapolation error by bootstrapping from out-of-distribution (OOD) actions. Previous methods mostly rely on using regularization to penalize OOD actions. But such methods are very sensitive to the regularization level. More importantly, when the data collection policy is very bad, the regularization actually prevents the algorithm from finding a good policy. What I really like about this paper is to rethink the problem from the first principle: is it possible to do bootstrapping from the in-sample actions? From this perspective I think this paper makes contribution to a very important problem in offline RL. The paper is also well organized and well written. Due to the high variance of importance sampling (IS), the paper proposes to use sampling-importance resampling (RIS). The authors then compare the variance of RIS and IS, and discuss how RIS is applied to the critic learning objective in offline RL algorithms. Most implementation details are clearly discussed in the paper. \n\n\n================================================\n\n\nWeakness: \n\nI have the following questions and comments. Please correct me if I missed anything important. I am happy to adjust my score based on how well the authors answer the questions in the rebuttal. \n\n1. My major concern is on the computation cost of the algorithm. It seems that after updating the current policy, IAC needs to adjust the weights (is ratio) for all (s,a) in the dataset.\n\n2. IAC needs to learn the behavior policy before training. How does it affect the performance if we train $\\beta$ as the other networks? Is there an ablation study for this? \n\n3. The paper suggests that one can simply treat $\\beta$ as a constant such that it\u2019s canceled in the weights. In fact, from Table 1, we can see that w/o $\\beta$ even produce better results than original IAC. Is there an explanation on this? It\u2019s always interesting to understand why something you try to approximate actually works better than the original algorithm. \n\n4. Following the previous question, I am wondering if it is because the domains are continuous control problems, where we can only observe one action in one state. Thus the log probability of $\\beta$ will be very close to zero. Then I am wondering how does IAC perform in discrete problems? \n\n5. IAC works very well empirically. But I am wondering why the algorithm performs so well, especially compared to IQL, which also performs in-sample bootstrapping. One limits of IQL in my opinion is that it\u2019s hard to approximate the in-sample maximum with expectile regression when the behavior policy is very sub-optimal. But it seems to me that RIS also has the problem. I think it\u2019s better to provide some evidence, even simple proof-of-concept experiments, to show the advantage of IAC over IQL. \n\n6. In Section 5.1, the authors suggest that IAC outperforms one-step RL as it has lower bias. But isn\u2019t that the main advantage of IAC compared to one-step RL is to use multi-step DP?  (This doesn\u2019t effect my score, I am just wondering if this discussion is important. )\n\n7. Finally, I am wondering what does OOD action exactly mean? The paper seems to suggest that OOD actions are those actions that are not in the dataset. Despite this is very reasonable, another view is to think of actions that are out of the support of the data collection policy. The difference is that for example an action can be covered by the behavior policy, but due to finite samples we don\u2019t observe it in the dataset. It seems to be the second perspective is more interesting, as it allows generalization. Any comments on this? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the previous section for more details.",
            "summary_of_the_review": "I recommend not accepting as the proposed algorithm might have large computational cost in practice and some claims are not well support by the experiment results. However, I am willing to adjust my score if the authors answer my questions in the rebuttal. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2306/Reviewer_Ga1X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2306/Reviewer_Ga1X"
        ]
    }
]