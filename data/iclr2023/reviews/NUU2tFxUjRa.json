[
    {
        "id": "JJdYf-ywK4",
        "original": null,
        "number": 1,
        "cdate": 1666559379518,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559379518,
        "tmdate": 1666559379518,
        "tddate": null,
        "forum": "NUU2tFxUjRa",
        "replyto": "NUU2tFxUjRa",
        "invitation": "ICLR.cc/2023/Conference/Paper5790/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method for consistent distribution sampling in latent two-tower models for recommender system. The method adds weights sampling in the loss calculation (on the mix of subsampled random negatives and negatives from a hard negative mining) to better approximate the overall sampling distribution which seem to help on few open-sourced datasets and in a/b tests in production.",
            "strength_and_weaknesses": "Strengths:\nThe paper shows a positive effect of the solution in the experiments.\nWeaknesses:\n1) Paper quality is has big issues. It needs to be proofread, the plots are hard to understand, terminology is not clear.  I think it is enough for rejection by itself.\n2) I am not sure if the uniform weight sampling is actually a desired outcome. E.g. in simclr the quality of the results dropped with the batch size after certain batch size.\n3) it is not clear what is the baseline in the a/b test. ",
            "clarity,_quality,_novelty_and_reproducibility": "I had a hard time understanding the paper. I think it needs to have a large overhaul before it can be accepted to any venue/journal. \n",
            "summary_of_the_review": "I think the paper has big problems with presentation, so it should be rejected on this basis. I also feel like this paper would be a better fit for a recsys conference (or a workshop), rather than ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5790/Reviewer_WYde"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5790/Reviewer_WYde"
        ]
    },
    {
        "id": "AVjnW3dq_q",
        "original": null,
        "number": 2,
        "cdate": 1667100327473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667100327473,
        "tmdate": 1667100327473,
        "tddate": null,
        "forum": "NUU2tFxUjRa",
        "replyto": "NUU2tFxUjRa",
        "invitation": "ICLR.cc/2023/Conference/Paper5790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Online ML systems usually have explicit positive samples (E.g., ads conversion, click, buy, like, etc.) to train on. However, negative samples are often implicit and have a much higher volume concerning positives. A common technique in constructing a training data set is to sample from the negatives to having more reasonably balanced data sets. However, the sampling process introduces problems concerning selection bias, not considering the long tail, etc. The paper presents a new negative sampling strategy called consistent data distribution sampling to improve this process. It focuses on the long tail and hot items specifically and provides mechanisms to improve convergence.\n",
            "strength_and_weaknesses": "Strengths:\n\n-It provides a good overview of the problem analytically and why we need to be mindful in sampling negatives. It also provides a rigorous analysis of the convergence.\n\n-The results are decent. Particularly close to the ANCE. It is not surprising since it also provides a similarity/distance-dependent process. \n\nWeakness:\n\n-It is not clear the complication and analysis of what similarity check brings concerning complexity (as an incremental unit) and how it grows with the dimensionality of data and representation size. For example, how much does it worsen the training time? What is the trade-off with respect to increased training time (so in an online system, maybe you will be able to retrain 4 times instead of 10 times a day).",
            "clarity,_quality,_novelty_and_reproducibility": "-The paper is mainly written well and easy to follow. \n\n-I am not impressed by either the novelty of the technique or the results. It seems like a fairly incremental paper studying a critical problem.",
            "summary_of_the_review": "I am borderline with this paper. The novelty and results are not impressive. It is an important problem and has strength in analytical analysis. But in such a practical problem, we either need a more thorough practical evaluation or a stronger algorithm with stronger results to pass the bar. so my rating is a marginally below acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5790/Reviewer_RorQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5790/Reviewer_RorQ"
        ]
    },
    {
        "id": "kEG2-NGALiI",
        "original": null,
        "number": 3,
        "cdate": 1667235442034,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667235442034,
        "tmdate": 1667235442034,
        "tddate": null,
        "forum": "NUU2tFxUjRa",
        "replyto": "NUU2tFxUjRa",
        "invitation": "ICLR.cc/2023/Conference/Paper5790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Long-tail distribution of items is an important factor which affects the user\u2019s experience in industrial search systems, recommendation systems, and so on. This submission tries to address the training-inference inconsistency due to the long-tail distribution. Concretely, the authors comprise a relatively large-scale of uniform negative sampling and batch negative sampling to make sure that long-tail items and hot items can be trained adequately. To faster the convergence of training, the proposed model also searches some hard negative items based the ANN index for the training queries. The main contribution of this submission is to propose a new loss function which considers the weight of each positive and negative training items. Both the offline and online experiments validate the effectiveness of the proposed model.",
            "strength_and_weaknesses": "Strength:\n1. The topic is interesting and important to improve the user\u2019s experiences in industrial system.\n2. The submission has an online experiment to validate the effectiveness of the model in real industrial system.\n\nWeakness:\n\n1. The novelty and contribution are incremental. The main contribution is to propose the loss function based on the classical binary-cross entropy by weighting each training items.\n2. The experiments are not adequate. Please conduct more experiments on other datasets and make up some ablation studies.\n3. Please give more explanations for the auxiliary loss (i.e. e.q. (10) and e.q. (15),). What\u2019s the intuitive meaning of the auxiliary loss?  How about the training efficiency as the auxiliary loss and the weight (i.e. e.q.(16)) consider all the items?\n4. In the last paragraph on page 5, the sentence \u2018Along with the training, the difference between $\\theta$ and $\\theta^{\u2018}$ becomes larger. Intuitively, the difference between $\\theta$ and $\\theta^{\u2018}$ should be smaller as the model converges. Please elaborate on how to set the checkpoints and how to update the checkpoint in your experiments. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and I believe the experiments can be reproduced. However, the quality and novelty are incremental. To highlight the original contribution, authors should figure out the difference between this submission and the previous work, like Xiong et al. (2020), Yang et al. (2020).",
            "summary_of_the_review": "The submission focuses on the training of long-tail distribution of items which is important to improve the user\u2019s experience in real search systems and recommendation systems. However, the novelty and the original contribution are incremental. Additionally, the experiments are not adequate.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5790/Reviewer_sTd5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5790/Reviewer_sTd5"
        ]
    },
    {
        "id": "TfK4eKYdHSj",
        "original": null,
        "number": 4,
        "cdate": 1667267549181,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667267549181,
        "tmdate": 1667267549181,
        "tddate": null,
        "forum": "NUU2tFxUjRa",
        "replyto": "NUU2tFxUjRa",
        "invitation": "ICLR.cc/2023/Conference/Paper5790/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work tries to tackle negative sampling in embedding model training for retrieval. Consistent Data Distribution Sampling is proposed by combining large-scale uniform training negatives with batch negatives. High divergence negatives are employed to improve the learning convergence. The sampled negatives are fused in a proposed auxiliary loss based on the asynchronized embedding matrix over the entire index set. The experiments show SOTA performance.",
            "strength_and_weaknesses": "*Strength*\nOffline experiments compared with related approaches and peer work show SOTA\nCombining multiple sampling strategy and the auxiliary loss is somewhat novel\n\n*Weaknesses*\n- The experimental results are thorough, but lack of some ablations, see clarity section, and it is also necessary to support the claim that \"high divergence improves the learning convergence\".\n- The results seems marginal, authors can clarify what is the typical gain in published papers for such datasets as an auxiliary support.\n- The difference and novelty of the proposed approach is not highlighted, and it is confusing that what makes a difference. Please include concrete technical contribution in the introduction section, current presentation is a bit high level. e.g. \"We proposes CDDS which .... by tech 1, tech 2, tech 3......\"\n- The writing of the paper needs improvement. Especially the incorrect clause usage/unnecessary inverted order sentence improve the difficulty of reading. Simple and short SVO sentences are fine. And please use accurate words to express.\n",
            "clarity,_quality,_novelty_and_reproducibility": "*clarity*\n- How many steps that the ANN index table is behind the training? It would need enough time to allow the inference for index.\nIf the step gap is large. This may yield suboptimal performance since the embedding may change a lot from the lookup table. The step gap matters. it would be nice to have one experiment around this discussion\n- Wonder to see the ablation of sampled negatives, like CDDS vs CDDS - high divergence negative vs CDDS - uniform negative vs etc\n\n*writting*\n\n- Figure 1 is never cross-referred in the main text, it would be nice to describe in detail in section 4.\n- Section 5.6 seems not necessary as it does not compare with any other approaches.\nSome of my guesses of what authors want to express,\n- procession -> processing, procession mostly means parade\n- Totally - > In general\n- \u201dAs a result of retrieving a possible...., the training negative set ........\u201c I don't see a causation between two sub parts, please simply say V^- = V \\ V^+. That is good enough",
            "summary_of_the_review": "The technical part of the paper is somewhat sound, but the presentation of the work needs improvement (not only grammatically). The skeleton of the work lies on prior works, unless authors clarify the difference from prior works, I think the novelty is limited.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5790/Reviewer_ExGn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5790/Reviewer_ExGn"
        ]
    }
]