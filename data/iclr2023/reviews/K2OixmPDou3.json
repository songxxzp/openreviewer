[
    {
        "id": "Zb72inDiKc",
        "original": null,
        "number": 1,
        "cdate": 1665626777472,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665626777472,
        "tmdate": 1665626777472,
        "tddate": null,
        "forum": "K2OixmPDou3",
        "replyto": "K2OixmPDou3",
        "invitation": "ICLR.cc/2023/Conference/Paper1069/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In summary, this paper first observes that there exists a historical training stage where the model has a higher OOD detection performance than the final well-trained one. After that, this paper aims to backtrack the model to that stage. To achieve this, this paper proposes a method called UM to forget those \"atypical samples\" that are sensitive to the changes in model parameters.",
            "strength_and_weaknesses": "Strength:\n1. I find the observation that there exists a historical training stage where the model has a higher OOD detection performance than the final well-trained one very interesting and intuitive. I am pretty supervised that no one ever made such an observation.\n2. I believe that throughout the main paper and the appendix, a quite comprehensive set of experiments has been conducted.\n\nWeaknesses (or confusions that may need further clarification):\n1. I prefer if the authors could discuss the relationship between this paper and the concept of overfitting. Specifically, from my perspective, the main inspiration of this paper from its own observation is that after a certain number of epochs of training, the model starts to memorize rather than learn knowledge. However, this concept is quite similar to the idea of overfitting. Hence, my two questions will be: (1) Does this mean that the large number of methods that are designed to alleviate overfitting can also be used to facilitate OOD detection, and has the authors tried this? (2) If (1) is not the case, could the authors possibly explain more on what is the difference between the memorization of \"atypical samples\" here and the memorization in overfitting?\n\n2. I may also prefer if the authors could discuss more on the relationship between their observation and their proposed method. Specifically, when the authors simply say \"We further attribute it to the memorization behavior on atypical samples.\", I feel confused about this. In other words, why the reason behind the observation that a historical training stage can have a better OOD detection result than the final stage is necessarily the memorization behavior on atypical samples? I hope to see a little bit more discussion on this.\n\n3. Personally, I think the description of UMAP may be too brief in the main paper. I understand that there is a page limit there for the main paper, but I still have the feeling that UMAP may need to be discussed slightly more since a large number of experiments are based on UMAP, which leaves me a feeling that it is quite important but less explained.\n\n4. Two minor comments are: (1) Is it a typo in equation 3 that the loss aims to minimize over the mask instead of the model parameter? (2) I think that especially in Table 1, the author should not only bold their \"94.01 \u00b1 0.08\" and \"74.86 \u00b1 0.21\", but also the same results of the previous methods. ",
            "clarity,_quality,_novelty_and_reproducibility": "I think while there may lack certain important discussion, the overall clarity and quality of this paper are above the passing line, and enough details have been provided in the paper for reproducibility.",
            "summary_of_the_review": "Overall, I think this paper gives an exciting observation and the proposed method sounds reasonable. However, with certain important discussions lacking in the current version, while I tend to accept this paper, I still expect the authors to answer my questions well in the rebuttal phase.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1069/Reviewer_3ovB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1069/Reviewer_3ovB"
        ]
    },
    {
        "id": "XEgs-O0GkeU",
        "original": null,
        "number": 2,
        "cdate": 1665764069872,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665764069872,
        "tmdate": 1665764069872,
        "tddate": null,
        "forum": "K2OixmPDou3",
        "replyto": "K2OixmPDou3",
        "invitation": "ICLR.cc/2023/Conference/Paper1069/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is motivated by the observation that a fully-converged model does not necessarily give the best OOD detection performance. The overfitting is likely due to the memorization of the atypical samples in the training data. The author proposes the Unleashing Mask used in the training stage to identify these atypical samples and let the network forgets them. The experimental results show that the proposed methods are effective in both post hoc methods and OE-based methods. \n",
            "strength_and_weaknesses": "\nStrength:\n\n1. The paper is well-written and easy to follow in most sections.\n\n2. The motivation is clear and the proposed methods make sense in addressing the related problems. \n\n3. The related works are extensive and well-organized\n\n4. The experimental section is comprehensive and covers the majority of the necessary empirical study.  \n\n\nWeakness (not necessary, but questions/suggestions): \n\n1. Why can the atypical samples be mined by the layer-wise mask? I was lost in reading section 3.2. More explanations and details can be elaborated on there. \n\n2. Are the masks applied at all layers or just some typical layers? \n\n3. How does Equation 3 stabilize around the stage that can forget those atypical samples? Seems like the absolute value term makes sure the masked loss is close to the original loss. The second term is minimizing the masked loss. Where is the \"forgetting\" mechanism happening? \n\n4. Why use the layer-wised mask? Is it the optimal choice for identifying the atypical samples? Can we use a unit/weight mask or use high CE error to identify atypical samples? \n\n5. Can we use other strategies like the early stopping criterion to prevent overfitting? The comparison would be interesting to see. \n\n6. Another interesting experiment to do is to separate the training dataset into two types (typical samples / atypical samples) and train two models respectively on them. It would be good to show that atypical samples are indeed the cause of OOD performance getting worse. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "See Strength And Weaknesses.",
            "summary_of_the_review": "This paper reveals an important observation that well-converged networks do not necessarily lead to a good OOD detection performance. The cause is likely the over-memorization of the atypical samples. The paper proposes an algorithm to identify and forget them, which makes sense. Overall this paper is a good study of the OOD detection problem. So I vote for acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1069/Reviewer_miMq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1069/Reviewer_miMq"
        ]
    },
    {
        "id": "V4v9UJrFyG",
        "original": null,
        "number": 3,
        "cdate": 1666662681572,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662681572,
        "tmdate": 1666676286223,
        "tddate": null,
        "forum": "K2OixmPDou3",
        "replyto": "K2OixmPDou3",
        "invitation": "ICLR.cc/2023/Conference/Paper1069/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes Unleashing Mask (UM) with the goal of maintaining a model's capability to distinguish between out-of-distribution (OOD) and in-distribution (ID) training examples. \n\nThe paper posits that as classification models train and the training loss decreases, there is a point where simple measures such as ODIN effectively distinguish between ID and OOD, but then as training loss trends to zero, the model is forced to learn atypical and difficult, in-domain class examples which degrades. While this atypical example memorization does improve model accuracy at the primary task, it also harms the model's performance for OOD. \n\nTo address this, the UM method specifies a binary layer-wise mask that serves to force forgetting of atypical examples.  ",
            "strength_and_weaknesses": "The paper presents experiments training on CIFAR10 and CIFAR100, then measuring classification accuracy as well as OOD false positive rates. The proposed method appears to work well across many OOD sets. However, I do have a few questions. \n\nWhile I can understand the basic idea linking the memorization of atypical ID examples to a decrease in OOD discrimination ability as a motivation for UM, I am not sure that this is solidly supported by the evidence of the paper. \n\nFurther, it isn't clear to me exactly how the parameters of the key equation (3) are selected. According to the paper: \"For\nCIFAR-10 as ID datasets, the value of mask rate is 97.5% and the estimated loss constraint for\nforgetting is 0.1 for our tuning until the convergence; For CIFAR-100, the value of mask rate is\n97% and the estimated loss constraint for forgetting is 1.2 for our tuning until the convergence.\" \nWould it be possible to run a baseline with parameters selected to effectively turn off masking (i.e., mask rate 100%)?\n\nWould it be possible to run CIFAR100 for OOD and add to Table 2? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written about average -- better than some papers but overall there is a lack of clarity and a need for editorial work. \n\nIt's not clear what is meant in many places.\n\nWhile it's helpful to add 10 pages of Appendix, it also felt a bit slapped together. The paper should be able to stand as-is within the paper limits. \n\nSome example edits and specific comments: \n\nAbstract: \"to reveal the once-covered detection capability\" --> unclear what is meant by \"covered\". Perhaps \"that restores the OOD discriminative capabilities of the model\"?\n\np. 1: \"how to find\" --> \"how can one find\" or \"how can we find\"\n\np. 1: \"Especially, it is a general existence across various setups,\" --> unclear. Maybe \"This is generally true across different models\"?\n\nFig. 1: \"FPR95\" should be defined here rather than page 7 or  forcing the reader to refer to another citation. \"false positive rate (FPR95) of OOD examples when true positive rate of in-distribution examples is at 95%\"\n\nFig. 1:  \"for multiple times\" --> \" multiple times\"\n\nFig. 1: \"More exploration can refer to Figure 2.\" --> \"Figure 2 contains further exploration.\"\n\np. 2: \"expected stage.\" --> unclear what this refers to. \n\np. 2: \"results accordingly demonstrate their rationality.\", perhaps consider:  \"results accordingly demonstrate their effectiveness\" because experimental results cannot show rationality.\n\nFigure 3: \"samples that sensitive\" --> \"samples that are sensitive\"\n\nFigure 3: \"Then fine-tune\" --> \"Then we fine-tune\"? \n\np. 6 \"CE loss\" --> at least the first time, please use \"cross-entropy (CE) loss\"\n\np. 7 \"existential\" --> \"existing\"\n\nFigure 2: The caption should indicate the primary training task and dataset. \n",
            "summary_of_the_review": "The paper presents the idea of using forgetting of atypical examples via UM to preserve a models' ability to distinguish OOD.\nWhile experiments support the overall proposal, it seems there are some additional probing or analysis that could be performed to more deeply try to understand whether the improvement is coming from the model or from some other factors. The paper has some issues with clarity and would benefit from a heavy edit.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1069/Reviewer_PAPe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1069/Reviewer_PAPe"
        ]
    },
    {
        "id": "DY27CwtEqn",
        "original": null,
        "number": 4,
        "cdate": 1667094278604,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667094278604,
        "tmdate": 1667094278604,
        "tddate": null,
        "forum": "K2OixmPDou3",
        "replyto": "K2OixmPDou3",
        "invitation": "ICLR.cc/2023/Conference/Paper1069/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is to restore the out-of-distribution sample detection ability given a well-trained model by fine-tuning with the proposed method Unleashing Mask (UM). Based on empirical observations, authors show that a model is trained to learn how to classify in-distribution samples while distinguishing out-of-distribution samples in early phase. Then, at epoch 60 in the authors' example, the model keeps learning in-distribution samples while losing its ability to distinguish out-of-distribution samples. To recover the model's ability to identify out-of-distribution samples, authors suggest UM that identifies atypical samples and helps the model forget them.",
            "strength_and_weaknesses": "Strength: Adaptability to previous robust loss functions.\n\nDetailed information on baselines in appendix A is helpful for readers who needs baseline details in a page.\n\nWeaknesses\n1. Expensive: Because UM denoises a pretrained model via fine-tuning about 100 (I believe so based on several empirical results in the table although not specified for fine-tuning; authors need to clarify it) epochs, I don't think it worths such expensive fine-tuning process.\n2. Weak logical support: In addition to Weaknesses 1, if we can access to already deployed model and its training data, it worths applying UM or UMAP to a model when it's at best condition (at epoch 60 according to the authors) and help it learn only in-distribution data. Why should we start as fine-tuning?\n3. Labor-intensive: mask rate should be empirically found by UM users. Besides, we need to choose first the OOD baseline, second UM or UMAP to obtain the best result because no case consistently performs the best. \n4. Lack of empirical support: We need to evaluate the impact of UM or UMAP, but results such as Table 1 and 2 lacks their results (I understand the authors' intention to emphasize the extendability of UM and UMAP.) \n5. No theoretical analysis: To support the authors' claim on UM, even a typical theoretical analysis is required which is absent.\n  \nQuestions\n1. Why UM model itself doesn't have (i.e., FC + UM or FC + UMAP)?\n2. What is fine tuning epochs $k$ and loss constraint $zeta$?\n3. Why only Fig 4(c) has 200 epochs? Besides, is 200 fine-tuning stage as a whole?\n4. What are OOD recall before/after fine-tuning by UM/UMAP?\n5. Some acronym words such as FPR95 needs explanation for readers without sufficient expertise or knowledge in this field.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Parts of the paper lacks explanation and thus needs clarification (see questions).\n\nQuality: The paper lacks appropriate theoretical analysis or detailed/thorough empirical support.\n\nNovelty: Fine-tuning a pretrained noisy model for another hundreds of epochs, rather than robustly training a model in scratch not to include noise while achieving performance, seems unreasonable.\n\nReproducibility: Because authors noted that they will upload their codes and datasets during discussion phase for reviewing purposes in APPENDIX, so the reproducibility cannot be checked at the moment.\n\n[1] KDD 2021, Robust Learning by Self-Transition for Handling Noisy Labels.",
            "summary_of_the_review": "Based on the weaknesses and overall quality assessment, I would recommend reject of this paper. I welcome the enlightenment of the authors on my ignorance and misunderstanding if there is any. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1069/Reviewer_Ga94"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1069/Reviewer_Ga94"
        ]
    }
]