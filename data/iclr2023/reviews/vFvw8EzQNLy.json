[
    {
        "id": "PcE5Fr6iZp",
        "original": null,
        "number": 1,
        "cdate": 1666504392911,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666504392911,
        "tmdate": 1666504392911,
        "tddate": null,
        "forum": "vFvw8EzQNLy",
        "replyto": "vFvw8EzQNLy",
        "invitation": "ICLR.cc/2023/Conference/Paper4890/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Motivated by the data inefficiency problem of off-policy algorithms, like PPO, this paper proposed a parallel version of the Q-learning algorithm. Specifically, this approach uses three parallel processes, namely actor, V-learner and P-learner, to perform a DDPG-like learning process. Empirical evaluations on six Isaac Gym tasks has demonstrated that the proposed approach has better learning efficiency than baselines like PPO, SAC, etc.",
            "strength_and_weaknesses": "Strength:\n1. The evaluation of the proposed method is extensive and involves many different tasks.\n2. The parallel simulation environments considered here are interesting and they might be important to greatly reduce the training costs for RL. \n\nWeaknesses:\n1. The introduction of the three parallel processes is quite abrupt without adequate discussions about the motivation of such considerations. It's unclear why these design choices are inspired from existing methods like DDPG or other variants. Furthermore, what's the challenges to design a Q-learning algorithm for parallel environments? Why a direct extension of DDPG can not work?\n2. It's not clear what's the key difference between parallel environments on a single workstation and parallel environments on a computer cluster. Does such difference really matters in the design of the algorithm? Why?\n3. The discussion of the main techniques is too brief and can not convince readers the design rationales behind the proposed algorithm.\n4. Another suggestion is to write the proposed method as an algorithm so that it would be more clear the algorithmic improvements made over existing approaches. Stressing the design considerations that make the proposed approach more efficient than others.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper has a considerable space to be improved. The novelty of the proposed approach over existing methods requires further elaborations. The reproducibility is good.",
            "summary_of_the_review": "Despite an extensive amount experiments have been conducted, the writing of this paper requires a considerable revisions to convince the contributions of this paper and illustrate how each design consideration leads to performance improvement over existing methods.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There are no ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4890/Reviewer_iU6x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4890/Reviewer_iU6x"
        ]
    },
    {
        "id": "HDr3-3aAY5",
        "original": null,
        "number": 2,
        "cdate": 1666509308511,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666509308511,
        "tmdate": 1666509308511,
        "tddate": null,
        "forum": "vFvw8EzQNLy",
        "replyto": "vFvw8EzQNLy",
        "invitation": "ICLR.cc/2023/Conference/Paper4890/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes paralleled Q-learning, a three-component framework to accelerate Q-learning. The design of the components enables concurrent execution, and experiments on Isaac Gym and vision-based benchmarks against other state-of-the-art algorithms such as PPO, DDPG and SAC show significant improvement as measured by cumulative return. ",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is written very clearly, with all the sections accurately describing what the algorithm is about. The relationship between the different components in the framework has been clearly demonstrated by graphs, and the intuition behind the components as well as choices of returns/RL structures is easy to follow.\n\n2. The experiments are comprehensive, and provide sufficient details for reproducibility as well as ablation studies. In particular, the discussion on the ratio of beta values(indicating update frequency comparisons) is very informative.\n\nWeaknesses:\n\n1. The algorithm is named Parallel Q-learning. However, the key of the algorithm lies in decomposing DDPGs/SACs and making use of techniques of improving Q-learning to better estimate Q-functions in the learning process. It would be helpful to emphasize in the abstract and intro where the novelty lies since the title may be a bit misleading.\n\n2. While the results of the experiments are largely self-contained and informative, it would be helpful to include the performances of PQL with SAC as the underlying algorithm. Since it is indicated in the paper that the PQL framework can be naturally extended to SACs, the inclusion of such ablation studies would answer the question how much the effectiveness of the PQL framework would depend on the choice of off-policy algorithms.    \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly, and the research question is interesting with high relevance. The novelty appears to be non-trivial, and the results are detailed enough for reproduction.",
            "summary_of_the_review": "I would rate this paper a 8, given its scope of contributions and good quality. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4890/Reviewer_BMNj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4890/Reviewer_BMNj"
        ]
    },
    {
        "id": "FCMU7k2rGV",
        "original": null,
        "number": 3,
        "cdate": 1666675929643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675929643,
        "tmdate": 1666675929643,
        "tddate": null,
        "forum": "vFvw8EzQNLy",
        "replyto": "vFvw8EzQNLy",
        "invitation": "ICLR.cc/2023/Conference/Paper4890/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to run thousands of simulations of a RL environment to speedup Q learning. Unlike previous efforts, that leverage distributed computing to run agents on large numbers of machine, they propose to leverage a single workstation running numerous tasks in parallel.\nThe paper contributes a novel framework, called PQL, that can train RL policies faster than previous efforts without requiring a large amount of compute resources.",
            "strength_and_weaknesses": "Scale is a major force behind the success of reinforcement learning. However, unlike numerous previous approaches that achieve scale by distributing the computation over large amounts of servers, PQL proposes to leverage a single workstation by multiplexing the simulation of thousands of environments along with a value function and a policy on a single GPU.\n\nThe approach appears to be similar to that of DDPG, and the implementation is based on an existing framework targeted for distributed training that was repurposed to run multiple simulations of the Isaac Gym environment on a single workstation, which is the main contribution of the paper. However, several points are unclear:\n * Where do the wall-time gains come from ? Is Isaac Gym more efficient when running multiple simulations in parallel, or is there something else driving the performance improvement ? Ideally, I'd like to be able to answer the question: what aspect(s) of PQL approach is inherently better and drives most of the performance improvements.\n * How applicable is this beyond Isaac Gym ? Would it help with other simulation based gyms ?\n * How would the PQL approach compare against other distributed RL implementations such as IMPALA/seed-RL if these implementations were also repurposed to target the Isaac Gym ?",
            "clarity,_quality,_novelty_and_reproducibility": "The empirical evaluation of PQL shows that it's able to learn faster than other approaches in 6 simulated environments. However, the presentation of the paper doesn't help understand how this result is achieved. Figure B.2 indicates that PQL is roughly as sample efficient as DDPG which is expected since they use similar off-policy approaches, but more sample efficient than PPO (on-policy), which is also expected. Figure 3 shows that PQL is faster than PPO and DDPG on the same hardware, so the implementation of PQL has to be able to do more work per second. How this is achieved is not explained. Is the GPU utilization increased and if so, how ? Does the framework better balance the workload between the CPU and the GPU ? Is one of the code bases that PQL leverages more efficient than the ones leveraged in the PPO and DDPG implementations used as baselines, thus reducing the training time ?\n\nIt's not clear what figure 5&6 demonstrate. If the relative ratios of total rollouts, updates to the Q function, and policy updates had been kept constant while the number of environments changes, and if the noise levels had been modified to avoid increasing or decreasing how much exploration is performed, we could have discovered the presence of absence of a communication bottleneck in the implementation. As done in the paper, there are too many unaccounted factors to be able to draw any conclusions.\n\nThe batch size experiment needs to be better explained. How is the batch size applied ? Is it applied solely on the Q and policy networks, or is it also applied to the actors ? It would have been better to show how the efficiency of the 3 components of the approach changes as batch size varies.\n\nSome aspects of the paper, such as the beta factors in section 3.2, are described in more detail than necessary: it's enough to define them, any reader can infer what they do from there. On the other hand, some mechanisms need more explanations. For example, why is f_a defined as the number of rollouts per environment per unit of time instead of the total number of rollouts per unit of time ? The later could make the setting more robust to changes in the number of environments operating in parallel, and could have decreases the number of hyper-parameters. Similarly, in section 3.2, how are the values of a_u and a_l determined and how would the noise injection process change if the actions were discrete instead of continuous ? \n\nPlease increase the font on the figures. The legends are very tiny, and I suspect impossible to read on a printed version of the paper.\n\n\n",
            "summary_of_the_review": "PQL is an interesting paper that demonstrates that a better implementation of the Q learning strategy can make it competitive against PPO from the point of view of training time. However, the author need to provide an intuition as to why their approach is inherently more performant. Furthermore, the evaluation needs more work to clearly demonstrate how the authors achieve superior performance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4890/Reviewer_bLp3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4890/Reviewer_bLp3"
        ]
    },
    {
        "id": "bFxkeDvfl0b",
        "original": null,
        "number": 4,
        "cdate": 1666769721368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666769721368,
        "tmdate": 1670548272132,
        "tddate": null,
        "forum": "vFvw8EzQNLy",
        "replyto": "vFvw8EzQNLy",
        "invitation": "ICLR.cc/2023/Conference/Paper4890/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors investigate the interplay between the training speed/performance of an offline RL method and the total number of environments used during training. A large number of RL papers default to online RL methods, specifically PPO, because of the relative ease of these methods, however, offline methods should be more sample efficient by virtue of reusing samples from the replay buffer. The authors implement an offline RL training scheme, PQL, to leverage recent advances in GPU simulation environments to accelerate the training process while achieving comparable results with online methods. PQL breaks up the data collection and training pipeline into 3 processes, an Actor, P-learner, and V-learner. Since the simulation environment is located on the GPU all 3 processes may be co-resident on a single GPU or distributed across multiple GPUs on a single node. The placement of processes on different GPUs and the ratio of data collection vs training speeds are studied empirically and optimal settings tend to vary, as expected, across environments. The authors also study empirically how the batch-size and replay-buffer sizes influence the training process as the number of processes changes.",
            "strength_and_weaknesses": "Strengths\n--------------------\n- The fundamental question being addressed by the paper is interesting. The training dynamics of RL algorithms using GPU-simulated environments may be dramatically different from those of CPU-simulated environments. The abundance of environments producing data at a rate much higher than previously studied coupled with lower communication overheads, because data is generated directly on the device, provide ample motivation to rethink training algorithms and hyperparameters typically used by members of the community. In short, is PPO the best off-the-shelf algorithm in light of the differences between training environments?\n- The authors demonstrate faster convergence for their offline training method compared to PPO on several GPU-simulated environments.\n- The influence of several hyperparameters, such as batch-size, replay-buffer size, the number of training environments, and the number of GPUs were studied as part of the empirical results.\n\nWeaknesses\n-------------------\n- The paper is empirically heavy but theoretically lite. There's no motivation underlying the choices for exploration of the design space of the parallel training environment hyperparameters.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is easy to follow and clearly conveys the intended motivation and goals of the authors. The quality of the empirical data is good (though I found it a bit difficult to read some of the legends in Figures 3, 4, 7, 8, 9, 10). The novelty lies in the focus on training an off-policy algorithm with a large number of environments vs the typical setup with an on-policy algorithm and a relatively small number of environments (unless you have access to a large distributed system). I believe the results should be reproducible given access to the implementation.",
            "summary_of_the_review": "For researchers that do not have access to a large pool of computational resources the ability to take maximum advantage of the resources they do have on hand is imperative. As such, on-policy methods typically underutilize GPU resources because they cannot generate a sufficient amount of data to keep the GPU saturated. Though many works have tried to address this issue by increasing the number of environments this typically requires an increase in the number of CPUs as well. I think it is quite interesting to try taking full advantage of the GPU-centric training environments to better utilize the hardware and investigate the interplay between the number of environments and the training dynamics. This type of investigation seems to be inline with the spirit and direction of the Issac Gym GPU simulator project.\n\nThe improved training time to accuracy is visible in a number of the plots. In Section 2 the authors reference an improvement in regard to both the sample efficiency and training times of off-policy algorithms but when I think of sample efficiency I would like to see plots of the return vs the number of environment interactions. The authors noted in section 4.6 that the volume of data generated by the simulators is so high that data in the replay buffers are being replaced rapidly causing the method to become more on-policy. I assume this means the sample efficiency also resembles that of an online policy in some cases, is that correct? Does this training scenario detract from one of the selling points of offline algorithms to reduce the volume of required training data?\n\nIn the discussion section the authors mention not using many modifications that are typical of modern offline algorithms. Was it too difficult to add these features on top of the current implementation? I feel that omitting these features could be a mistake since top-performing offline algorithms use them so without those reference points it's more challenging to compare PQL performance against prior work.\n\nA concern is that the primary contribution is the framework that implements PQL, however, this artifact mostly builds on existing work (Issac Gym, PyTorch, etc). This diminishes the novelty of the work and makes the results appear more informative rather than a novel contribution that rises to the level required for publication at the current venue.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4890/Reviewer_xfGC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4890/Reviewer_xfGC"
        ]
    }
]