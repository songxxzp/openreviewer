[
    {
        "id": "FMTkj4oDnx",
        "original": null,
        "number": 1,
        "cdate": 1666583455040,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583455040,
        "tmdate": 1666583455040,
        "tddate": null,
        "forum": "2PI2EKASh_Z",
        "replyto": "2PI2EKASh_Z",
        "invitation": "ICLR.cc/2023/Conference/Paper5283/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes adding a normalization term to the existing MI-based clustering objective.",
            "strength_and_weaknesses": "- Weakness:\n    1. The organization of the paper is poor. For example, the authors spend about two pages discussing previously well-known methods and the corresponding properties. It is not clear what problem the authors want to tackle.\n    2. The current \u201ccontributions\u201d (especially the second) are not exactly contributions. They are more like details in the proposed method.\n    3. Fig. 2 indicates that the weight parameter needs to be carefully tuned to make the normalization term work.\n    4. Experiments on MNIST show almost the same performance for the proposed method and previous MI-based ones. Moreover, it is unfair to conduct K-means on the raw data while using a 1-layer classifier for the proposed method.\n    5. The experimental results on deep clustering benchmarks are not correct. For example, the results for IIC do not match those reported in its original paper. Thus I doubt the correctness of the conducted experiments.\n    6. The authors claim the normalization term is helpful for those pseudo label based clustering methods. But no corresponding experiments are conducted to support the claim.\n    7. Parameter analysis on the weight gamma is missing.\n    8. Minor: It is really strange to name the loss L_our.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing and organization of this paper are poor. It is hard to read and follow the authors\u2019 idea. The proposed normalization term is not novel. The reproducibility might be good since the proposed method is quite trivial.",
            "summary_of_the_review": "As mentioned above, this paper is of bad quality in organization and writing. The proposed method is neither novel or effective. The experiments are also not conducted correctly and sufficiently.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5283/Reviewer_oWJS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5283/Reviewer_oWJS"
        ]
    },
    {
        "id": "nz3OABhksz",
        "original": null,
        "number": 2,
        "cdate": 1666599182389,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599182389,
        "tmdate": 1666599182389,
        "tddate": null,
        "forum": "2PI2EKASh_Z",
        "replyto": "2PI2EKASh_Z",
        "invitation": "ICLR.cc/2023/Conference/Paper5283/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work observed a connection between mutual information-based clustering loss and margin-maximization, and proposed two changes to the process: (i) explicit $\\ell_2$ regularization to promote the margin maximization behavior, and (ii) replacing the entropy term in MI, which can be viewed as a reverse KL divergence, with a forward KL divergence.  The modifications lead to improved performance on weakly supervised classification tasks.",
            "strength_and_weaknesses": "The authors provided intuitions on the proposed modifications, which are based on past interpretations for feature-space clustering.  The proposed algorithm appears to have competitive performance, but I am not familiar with this literature and cannot judge on the significance of the empirical results.\n\nOn the downside, the novelty is a bit lacking: the use of $\\ell_2$ regularization is ubiquitous, and its margin-promoting behavior is known in different but related settings (e.g., softmax classification); the replacement of reverse KL with forward KL is straightforward given the fairness interpretation of the former.  The interpretation of MI as enforcing \"fairness\" and \"decisiveness\" is also somewhat arbitrary and simplistic, as they appear to provide no further concrete insights, than the proposed algorithm.  Thus, it appears that the significance of the work hinges on the empirical side.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is well-written, although I think there could be more focus on the justification of the proposed algorithm, as opposed to the background.",
            "summary_of_the_review": "+ The authors made some observations on the MI loss for clustering, which leads to an algorithm that appears to perform well\n\n- The motivating discussions are somewhat simplistic, and not fully convincing",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5283/Reviewer_7XGP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5283/Reviewer_7XGP"
        ]
    },
    {
        "id": "bOGlj-uxl7Y",
        "original": null,
        "number": 3,
        "cdate": 1666639493529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639493529,
        "tmdate": 1666639493529,
        "tddate": null,
        "forum": "2PI2EKASh_Z",
        "replyto": "2PI2EKASh_Z",
        "invitation": "ICLR.cc/2023/Conference/Paper5283/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper discusses information theory-based clustering and self-supervised learning.\nThe main contribution lies in the use of \"reversed\" KL divergence and cross-entropy.\n\n\nThe paper is a bit confusing, so I present here my understanding of the theory.\n\nClustering can be done by maximizing the Mutual Information over the class prediction, $\\sigma$:\n$$\\min_\\sigma -MI(\\sigma, X)$$\nAccording to Bridle et al. (1991) the latter can be approximated as follows, where the $avg$ means average:\n$$\\min_\\sigma avg[H(\\sigma)] - H(avg({\\sigma}))$$\nThe last term can be rewritten as a KL divergence wrt to a uniform distribution plus a constant that we omit:\n$$\\min_\\sigma avg[H(\\sigma)] + KL(avg(\\sigma)|| Unif)$$\nThe clustering is done in a feature space learned by a network with weight w, and the clustering is controlled by one/two vectors v.\nKrause et al. 2010  suggested an l2-regularization of all the weights involved that can be weighted.\n$$\\min_\\sigma avg[H(\\sigma)] + KL(avg(\\sigma)|| Unif) + \\gamma ||v,w||^2$$\nThe idea is that controlling the norm of v (classifier) also controls the margin. exactly like an SVM.\nControlling the norm of w helps the training.\n\n*First contribution*: the authors reverse the KL (Eq10) and weigh it (the authors omit w).\n$$\\min_\\sigma avg[H(\\sigma)] + \\lambda KL(Unif||avg(\\sigma)) + \\gamma ||v,w||^2$$\nThe argument is to prevent $avg(\\sigma)$ from being a one-hot vector in a more strict manner than if the KL is computed the other way around. $avg(\\sigma)$ being a one-hot vector ( or at least that some of its dimensions go to zero) means that some clusters collapse aka are empty.\n\nNow let's assume we have some (soft) pseudo-labels $y$. If we step back and state what we try to achieve:\n1/ Good class separation with a margin: $\\min_{v,w,\\sigma} avg[H(\\sigma)] + \\gamma||v,w||^2$\n2/ All the clusters should be used (fairness): $KL(Unif||avg(y))$\n3/ Consistency: the pseudo labels match the class assignments: $y = \\sigma$\n\n*Second contribution*: Using a KL to match $\\sigma$ and $y$.\nWe invoke Lagrange to merge them all:\n$$ L = avg[H(\\sigma)] + \\beta \\cdot avg[KL(\\sigma||y)] + \\lambda KL(Unif||avg(y)) + \\gamma||v,w||^2$$If we set $\\beta=1$, it simplifies to the final loss:\n$$ L = avg[H(\\sigma,y)] + \\lambda KL(Unif||avg(y)) + \\gamma||v,w||^2$$\nwhere the cross entropy between class prediction $\\sigma$ and pseudo-label $y$ is reverse compared to usual.\nFor $y$ fixed, $H(\\sigma,y)$ is a line that is horizontal if the model is not confident on the pseudo-label $y$. This means the gradient becomes close to zero. So no need to prune the pseudo-labels!",
            "strength_and_weaknesses": "+ Reversing the cross-entropy for the pseudo-labels is indeed a good idea. In my understanding, it can replace pruning or selecting the pseudo-labels since the low-confident ones will \"naturally\"  have little influence on the training.\n+ Inverting the KL is also interesting but I don't really see a huge benefit. Fair enough.\n\n- The writing is confusing. Arguments cross each other, the construction is not linear and some variables are not clearly defined. \n- The experimental section is poor. Several Ablation analyses are missing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The text is a bit confusing which makes it a bit difficult to realize the contributions.\n\nNovelty: reversing the Cross-Entropy or the KL are interesting ideas, that are worth talking about.\n\nReproducibility: I wouldn't be able to implement the model. The theory is fine but it does help with the implementation. What is y? is it just an ad-hoc variable? is it part of the network? what is the difference between $\\sigma$ and y in the implementation?",
            "summary_of_the_review": "The paper brings interesting ideas.\nAfter reading it all, I am still unsure what is $y$. how does it differ from $\\sigma$ and how do we compute all of these?\n\nThe authors try to build a discussion about clustering with information theory, but it turns confusing. There are too many arguments not very well organized. They compare themselves too much to Jabi et al. to the point that the main contributions are a bit hidden. I think the whole presentation could be greatly simplified.\n\nI do find the ideas interesting, but the paper/presentation is too confusing.\n\nIf I understood well, you're doing a centroid based clustering (hence the v) where you want the separation to be controlled with information theory tools. It is a fair endeavor. But then you mix geometry (l2 regularization) and information theory (entropy).\nCan't you control the margin using only pure entropy-based tools?\n\nRegarding clustering with information theory, I would suggest the authors have a look at: \nhttps://arxiv.org/abs/1406.1222\nIn this paper, they use pure information theory tools and achieve pretty impressive results.\n\n\n\n\n\nFigure 1 is a bit unfair. depending on the initialization, k-means or GMM can split this dataset correctly.\nAlso for a softer boundary, then just use GMM which would be fairer than k-means, since the latter uses hard assignments while the proposed method relies on soft assignments, like GMM.\n\nEq 5: I think you want to say that y arises from a uniform distribution. The correct notation is $\\bar{y] \\tilde u$.\n\npage4 The topic moves from clustering to self-supervised learning without real warning.\n\nFig3: The caption is confusing. So are the plots. Regarding Fig 3.a: why allowing $\\sigma$ to have zero coordinates not acceptable? You might also want to let the network decide and close some clusters.\n\nPage 6 optimization problem. Why is $\\sigma$ not under the first min? I put it in my summary. Maybe I am wrong. Maybe sigma is a function of v and w. it's not clear.\nWhy do you use y in the second minimization, while it was not in the previous equations? I am not sure this helps to understand how the equations connect.\n\nSection 3.1: Why Equation 4 and not 10? I thought Eq10 was your final model. \nAlso, k-means can cluster raw MNIST with more than 53% accuracy, using sklearn implementation.\n\nEquation 10 relies on pseudo labels. It is thus also dependent on the initialization. So I would be interested to see how it works when not supported by a network.\n\nAppendix G: Why Equation 9 and not 10? There are no gamma and lambda in equation 4. Figure 5, which one is Eq4 which one is Eq.9?\n\nTable 2: For IIC, did you use over-clustering heads as it is recommended in the paper?\nThe discussion in Appendix H should be in the main text.\n\n\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5283/Reviewer_3fzk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5283/Reviewer_3fzk"
        ]
    }
]