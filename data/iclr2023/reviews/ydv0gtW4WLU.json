[
    {
        "id": "cvAQ6pAgSx",
        "original": null,
        "number": 1,
        "cdate": 1666623888088,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623888088,
        "tmdate": 1666623888088,
        "tddate": null,
        "forum": "ydv0gtW4WLU",
        "replyto": "ydv0gtW4WLU",
        "invitation": "ICLR.cc/2023/Conference/Paper3755/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Learn temporally abstracted skills from expert demonstrations. This work uses a two-level hierarchy, where the subgoals given to the lower level of the hierarchy are always achievable, and progressively increase in difficulty. This is achieved by choosing subgoals of increasing length for the agent until the agent is unable to achieve the subgoal.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper is relatively easy to understand and the concept is an interesting application of hindsight in learning from demonstration. \n\nTHis paper represents a fairly sizable combination of techniques to produce results.\n\nWeaknesses:\n\nThis method is limited by the requirement of being able to reset the environment to a state mid-trajectory, which either requires a carefully designed environment or access to a simulator. This can impact the application of this method to real-world tasks.\n\nThe paper combines ideas from a wide range of areas which obfuscates where the improvements come from or if a single core idea is in particular more compelling. In particular, the primitive informed parsing could be performed without an expert and have a similar effect of curriculum, still requiring the agent to move to this information. Including the expert and thus IRL/behavior cloning, this makes it less clear why these components are inherently necessary to the core idea (or resetting hindsight labels). In addition, adding on a hierarchical meta-policy seems to only add an extra layer of complexity.\n\nIn the movement domains, there is only marginal improvement, and these domains are simple enough to be toy domains. The majority of the improvements are in the pick and place domain, where it appears that both the hierarchical and RPL methods completely fail. In this case it seems like this is because pick-and-place tasks require expert information, and a basic application of that data to policy learning would probably result in a significant improvement in performance since pick and place tasks have certainly been performed by hierarchical, RL and IRL methods in the past. ",
            "clarity,_quality,_novelty_and_reproducibility": "The work is high quality, providing a somewhat novel hindsight-based hierarchical algorithm in the context of supervised learning, and deriving some limited analysis on the divergence between the learned policy and the expert policy. It also provides insight in a sizable number of add-ons and hacks that allow this method to work in simulated robotics tasks, which can be useful for HRL researchers trying to use similar methods. It is formatted clearly and the main ideas are expressed well, although as is typical for HRL, where the benefit comes from can be somewhat obfuscated.",
            "summary_of_the_review": "I propose to accept this paper because it has sufficient novelty, is well written and provides interesting insights into getting HRL to work in some simulated robotics domains.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3755/Reviewer_oc7V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3755/Reviewer_oc7V"
        ]
    },
    {
        "id": "2WwBDSA_RIs",
        "original": null,
        "number": 2,
        "cdate": 1666812119080,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666812119080,
        "tmdate": 1666812119080,
        "tddate": null,
        "forum": "ydv0gtW4WLU",
        "replyto": "ydv0gtW4WLU",
        "invitation": "ICLR.cc/2023/Conference/Paper3755/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the problem setting of off-policy hierarchical reinforcement learning, focusing on automatic-curriculum generation (via the proposed PIP) and bootstrapping from  a handful of expert demonstrations using imitation-based regularization. The paper\u2019s contributions include an adaptive parsing method that segments state-only expert demonstrations into skills that are at the fringe of the current capabilities of the lower-level policy. The proposed adaptive method is shown to outperform its fixed counterpart on two simulated tasks involving a robotic manipulator moving through a maze, and moving a block to a desired goal position. Furthermore, visualizations in the paper confirm that subgoals of increasing difficulties are set by the higher-level policy as training progresses (see Figure 2, Page 3 of manuscript).\n",
            "strength_and_weaknesses": "Strengths Of The Paper:\n\n1) One way the proposed approach is appealing is due to the elimination of one form of expert annotation: using the lower-level policy to adaptively parse expert state-demonstrations into skills, rather than a fixed horizon-based parse.\n\n2) Suboptimality bounds for the higher-level policy and lower-level policy are given, which is implied to become tighter as the lower-level skills improve as training progresses.\n\n3) Implementation details for the algorithm on both tasks is discussed at length, and specific hyperparameter values are given which will help to facilitate reproducibility.\n\nWeaknesses Of The Paper:\n\n1) Experimental evaluation of the approach, while showing the proposed method in a positive light, is not thorough, and can be improved in several ways. Firstly, the method is only evaluated on two reinforcement learning tasks, both of which are based on the same position-based robotic manipulator. This environment functions similarly to a 3D pointmass, where actions are directly added to the current 3D position of the gripper. The environment is made more challenging by using a sparse reward, but the complexity of the underlying task is simple in comparison to other tasks that were studied in previous HRL papers, including locomotion-based maze navigation (from HIRO, Nachum et al. 2018), and torque-based manipulation (from HAC, Levy et al, 2018).\n\n2) While the proposed adaptive parsing method is effective in practice, some details are unclear, which I will discuss next. First, the paper states that on the maze task the replay buffer is flushed and re-populated every 200 steps, and 100 steps for the block task. How many new environment steps are taken or required during this flushing and re-population operation, and are these included in the budget reported (2.93e6 timesteps for the maze task and 6.75e6 timesteps for the block task). For a fair comparison, baselines would ideally be compared to the proposed method under the same conditions (ie, the same number of environment timesteps, including those used by PIP). It remains possible the evaluation already takes this detail into account, but is unclear from the text.\n\n3) On Page 6 of the manuscript, the authors state \u201cour method deals with non-stationarity by regularizing the higher policy with imitation learning\u201d and this is not validated. I agree with the author\u2019s that it is likely the proposed approach does mitigate non-stationarity by design, but this claim is not experimentally verified. Such an experimental verification should be performed that compares to other HRL methods that mitigate non-stationarity. Building on Weakness (2), the authors should ensure all methods operate in the same conditions in regards to any additional samples that PIP requires to relabel the buffer.\n\n4) Performance is reported in a somewhat uncommon format. It would be helpful to report success rate versus the cumulative number of environment samples collected (including any additional samples that may be required during the PIP parsing step, if any).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this paper presents an original idea for an important problem in hierarchical reinforcement learning: learning effective lower-level skills. The idea is high-quality, and the inclusion of an analysis of the suboptimality of the learned skills helps to improve its quality. However, the paper is not experimentally thorough, and the quality of the paper is brought down by the overall simplicity of the tasks used for evaluation, see Weakness (1). The paper\u2019s experimental limitations outweigh the high-quality idea, and the included suboptimality analysis.\n\nThe paper is generally clear, except for a minor issue detailed in Weakness (4) regarding how performance is reported, and in Weakness (2) regarding the data requirements of PIP. After these are resolved, I would consider the paper generally clear, with no major clarity flaws. Experiments appear to be algorithmically reproducible, as hyperparameters of the proposed method (including the introduced \\lambda and u and specified), though some details from the environment setup are missing. For example, how are the doors of the maze randomized, and how is the train-test split of the evaluations tasks constructed for each task.\n",
            "summary_of_the_review": "Overall, the paper presents an original and promising idea, but falls short in terms of its experimental evaluation of the idea. In addition, certain flaws related to clarity hinder the interpretability of the results (see \u2018Strength And Weaknesses\u2019). I am willing to reconsider my evaluation if weaknesses are addressed, either with new results, or clarifications to questions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3755/Reviewer_xPx6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3755/Reviewer_xPx6"
        ]
    },
    {
        "id": "XAivYkBov9C",
        "original": null,
        "number": 3,
        "cdate": 1666858633648,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666858633648,
        "tmdate": 1669635942216,
        "tddate": null,
        "forum": "ydv0gtW4WLU",
        "replyto": "ydv0gtW4WLU",
        "invitation": "ICLR.cc/2023/Conference/Paper3755/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The manuscript presents CRISP, an approach for hierarchical reinforcement learning from expert trajectories. Their approach relabels states from expert demonstration as subgoals to train a higher level. States are labelled as subgoals when they are the first in a subsequence that can\u2019t be reached by the lower level from the start of this subsequence. Thus, subgoals are chosen that are \u201cjust out of reach\u201d for the lower level, developing a learning curriculum. The two levels are trained to optimize both offline reinforcement learning and imitation learning within one objective. The paper provides a bound for the suboptimality of the method compared to an optimal policy. The method is evaluated on two simulated tasks and outperforms two hierarchical reinforcement learning baselines.",
            "strength_and_weaknesses": "Strengths:\n+ The relabelling of the expert demonstration states based on failures to reach them is a very interesting idea.  The emergence of a learning curriculum from this simple procedure seems very promising and is nicely visualized in Figure 2.\n+ The results compared to the baselines on the two investigated tasks look impressive and highlight the strength of using a hierarchy combined with the subgoal learning curriculum.\n\nWeaknesses:\n- The method part is at different parts very hard to follow, and I found various parts confusing. Below is a list of points that I think need to be addressed:\n    \u2022 p. 3, section 3.1: The lower primitive policy $\\pi^j_L$ is introduced with superscript $j$, where $j$ denotes \"an indicator of the current subgoal reaching capability of the lower primitive\". In my opinion this sentence does not sufficiently explain $j$. The indicator $j$ is not further clarified, does not change in the pseudocode, and is dropped later (section 3.2). So why is the superscript $j$ needed?\n    \u2022 p. 4, Definition 1: What is $D_{TV}$? I'm assuming total variation, but it is not introduced.\n    \u2022 p. 4, The proof for the suboptimality of the lower primitive is missing. It can probably be derived from the proof for the upper policy, however, seeing that they differ with respect to lambda it is necessary to explain where this difference comes from.\n    \u2022 p. 5, Eq. 10: What is $\\epsilon$? This needs to be introduced.\n    \u2022 p. 5, section 3.4: Why does Eq. 10 incentivize the higher policy to make \"reasonable progress towards achieving the final goal\"? Does it not just incentivize generating subgoals that are hard to distinguish from the subgoals of dataset $D_g$?\n    \u2022 p. 5, section 3.5: It reads like the off-policy RL objectives can be referred to as $J^H_D$ and $J^L_D$. Shouldn't the off-policy objective be $J^H_{\\theta_H}$ and J^L_{\\theta_L} of Eq. 11 and Eq. 12? Because $J^H_D$ is defined above as the IRL objective (Eq. 10). $J_D^L$ is not defined before. The authors need to be more precise about the different objectives.\n    \u2022 p. 5, Eq. 11 & 12: $\\lambda$ is here introduced as a hyperparameter whereas in section 3.2, $\\lambda$ was used as a factor for the upper bound of suboptimality. To avoid confusion different names should be used.\n- From the experiments it is not clear how much the approach relies on the expert trajectories to learn to solve the tasks. How many expert trajectories were used for each of the two tasks? How were the expert trajectories generated? How does the number of expert demonstrations influence the system's performance? For example, how do fewer or more demonstrations affect the performance? Could the system deal with \"bad expert demonstrations\" that are not useful in solving the task? For example, what would happen when a couple of random rollouts are included in the demonstration dataset? I think these questions need clarification in the description of the experiments, further discussion, and maybe additional experiments.\n- What is the effect of resetting the subgoal buffer, i.e., the hyperparameter $u$? How does $u$ affect the sample efficiency of the approach? An ablative study evaluating different values of $u$ would help understand how resetting the subgoal dataset affects the method.  \n- How is CRISP-BC different to CRISP-IRL? Why does it perform better than CRISP-IRL in the Pick and Place task? Highlighting the difference between the two versions would also help understanding the approach.\n- The method requires that the simulator can be reset to an arbitrary state. This is a major restriction for applying the method. The authors acknowledge this limitation but a more detailed discussion on how this limitation could be weakened would improve the paper. For example, it prohibits applying the system on real robots. Thus, the claim of the abstract that the method \u201c[\u2026] is suitable for most real world robotic control tasks\u201d is too strong.\n- Please fix the citation style. Use citet only of the citation is part sentence, grammatically (as a subject or object). Otherwise ,use \\citep. Here an example: \u201cLearning effective hierarchies of policies has garnered substantial research interest in RL Barto & Mahadevan (2003)\u201d should be \u201cLearning effective hierarchies of policies has garnered substantial research interest in RL (Barto & Mahadevan, 2003)\u201d.\nMinor corrections and suggestions:\n- typo on p. 1: \"balnced\"\n- bottom of p. 5: it should be Algorithm 2 instead of Algorithm 0\n- typo on p. 9: \"tn maze navigation\"\n- Figure 1 should be referenced in the text.\n- introduce the abbreviation IRL on p. 3 when first mentioning inverse reinforcement learning",
            "clarity,_quality,_novelty_and_reproducibility": "While motivation and related work are written clearly, the method section needs clarifications and improvements. The experiments are mostly clear, but some information (e.g., on the expert trajectories) are missing to fully understand the experiments and be able to reproduce them. There are too many design details (optimizer, learning rate, \u2026) missing, to guarantee reproducibility. However, this could be fixed by providing code to run the experiments. The idea seems novel.",
            "summary_of_the_review": "In sum, I think the idea is very interesting, but the clarity and details of the method and experiments still need improvement to be accepted. The idea seems interesting, but it\u2019s potential and limitations could be evaluated much more thoroughly in experiments. \n\n-- post rebuttal update --\nSee the answers below. I have raised my score to 5 (and updated correctness to 3). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3755/Reviewer_WG9D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3755/Reviewer_WG9D"
        ]
    }
]