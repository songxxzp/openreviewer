[
    {
        "id": "mfCvwhFcd9j",
        "original": null,
        "number": 2,
        "cdate": 1666536251201,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666536251201,
        "tmdate": 1669865851622,
        "tddate": null,
        "forum": "KLrGlNoxzb4",
        "replyto": "KLrGlNoxzb4",
        "invitation": "ICLR.cc/2023/Conference/Paper2503/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper deals with the video scene graph generation problem. 1) this paper proposes a new task named SF-VidSGG, which requires to train the VidSGG models with only weak supervision. 2) this paper proposes a novel pseudo-label assignment (PLA) method for the SF-VidSGG task. In particular, PLA decouples the problem into two steps: 1) assigning a pseudo-localized scene graph to each frame in the video, and 2) training a VidSGG model in a fully supervised manner using the pseudo-localized scene graph. Extensive experiments show the effectiveness.",
            "strength_and_weaknesses": "Strengths\n1)The proposed SF-VidSGG task is interesting and meaningful. The weakly supervised scene that this task focuses on are more in line with real-world cases and thus help bridge the gap between video scene graph generation research and practical applications.\n2)The proposed PLA method is novel and model-agnostic. In particular, it generates different \u201cpseudo labels\u201d using multiple-teachers, and dynamically assigns weights for these teachers by considering the temporal dependencies in videos.\n\nWeaknesses\nThe paper does not give an exact reason for choosing the \u201csingle-frame weak supervision\u201d. It is required to provide a comprehensive discussion on different choices of weak supervision and explain why not using other types of weak supervision.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and is easy to follow\nThis paper proposes a new task for VidSGG. It is meaningful for real-world application. The proposed method seems work and interesting.",
            "summary_of_the_review": "This paper proposes a weakly-supervised VidSGG task. It utilizes the unlocalized scene graph of intermediate frames and proposes a model-agnostic pseudo label assignment method PLA. The experiments showthe effectiveness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2503/Reviewer_Ca8D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2503/Reviewer_Ca8D"
        ]
    },
    {
        "id": "C8EB8UaSAcp",
        "original": null,
        "number": 3,
        "cdate": 1667041404085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667041404085,
        "tmdate": 1669601800541,
        "tddate": null,
        "forum": "KLrGlNoxzb4",
        "replyto": "KLrGlNoxzb4",
        "invitation": "ICLR.cc/2023/Conference/Paper2503/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a weakly supervised setting where single-frame ground truth annotation is provided among many video frames; and the single ground truth annotation only provides an unlocalized scene graph. To tackle this problem, a new Psuedo Label Assignment (PLA) method is proposed. ",
            "strength_and_weaknesses": "[advantages]\n1. The paper is well-written and easy to follow.\n2. The proposed setting is important in practice.\n3. The method of using model-based and model-free methods to generate pseudo labels and fusion of the updated weight is interesting. Experiments demonstrate the effectiveness of the proposed PLA.\n\n[weaknesses]\nThe experiments are somewhat insufficient to justify the claims made by the authors. See suggestions below. \n\nThe paper can be improved in the following aspects:\n\n1. The problem setting is claimed to be weakly supervised. However, the proposed method requires a pre-trained object detector. If my understanding is correct, this object detector is trained in a fully supervised setting on ground truth bounding boxes and class labels of a given image. To some extent, the proposed method still requires bounding boxes. It does not fit the weakly supervised setting as described.\n\n2. Follow-up on 1, what if the authors are NOT using pre-trained object detectors or using an imperfect object detector with noisy outputs, would PLA perform efficiently? Can the authors quantify the performance of PLA when there are e.g. 10%, 20%, 40%, and 80% errors made by the pre-trained object detectors?\n\n3. Can the authors show how would the size of the weakly supervised labels influences PLA performance? e.g. the performance of PLA against 10%, 20%, 40%, and 80% annotated video frames out of the entire number of video frames? How would all the baselines perform in these titrated cases?\n\n4. So far, the authors only demonstrate the effectiveness of the method on Action Genome. There is around 1% improvement over various testing scenarios (Table 1). To make the claim convincing, please include the same table but test on another video graph generation dataset to benchmark with SOTA.\n\n5. Data availability (see below)\n\n[minor] Explain what is R (recall) in R@10, R@20 and so on. ",
            "clarity,_quality,_novelty_and_reproducibility": "5. This is a nice engineering work consisting of many concepts and components explored by the community already. Given the complicated designs of various components and hyperparameters, would the source code become available for reproducibility purposes? Please include such statements in the paper.",
            "summary_of_the_review": "So far, my ranting is weakly rejected due to insufficient experiments; however, I am happy to change the rating depending on the feedback provided by the authors in the rebuttal.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "ok to me",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2503/Reviewer_hgye"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2503/Reviewer_hgye"
        ]
    },
    {
        "id": "mos-2KRNgUF",
        "original": null,
        "number": 4,
        "cdate": 1667211462552,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667211462552,
        "tmdate": 1669862507474,
        "tddate": null,
        "forum": "KLrGlNoxzb4",
        "replyto": "KLrGlNoxzb4",
        "invitation": "ICLR.cc/2023/Conference/Paper2503/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper propose a new task that only leverage a single frame with unlocalized annoation for video-level scene graph generation. To do this, the author propose an approach called PLA to create pseudo labels for unannotated frames, and train the fully-supervised VidSGG model with these pseudo labels.  The results on Action Genome benchmark demonstrates the effectiveness of PLA. ",
            "strength_and_weaknesses": "Strength:\n1. The paper propose to do vidSGG with weak supervision, which is a good contribution. \n2. The proposed approach of pseudo label is reasonable and easy to follow. \n3. The ablation study shows the effectiveness of proposed modules. \n\nWeakness:\n1. Approach\n\na. The approach seems only supports short video without drastic changes. If the scene changes, the proposed approach may not work since the pseudo label assigiment will not be correct. \n\nb. The approach is not very novel. The pipeline is a simple combination of three parts, and all of them are not very new, but just a simple pseudo label->training pipeline. \n\n2. Experiments\n\na. How to choose the annotation frame? Is it chosen randomly, or mid-one? How about choose the first frame? An experiment of different annotation frame choosing strategy should be added. \n\nb. Should add experiments to compare with fully-supervised approach to show how much is behind this upper-bound. \n\nc. Should compare with existing VidSGG methods metioned in Related Work. \n\nd. Please show more qualitative illustrations. Maybe in supplementary file. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: I think the approach is not novel and experiments are not sufficient. \nOrginality, Clarity: Good. Easy to follow. \nOriginality: The main contribution is to use weak supervision for VidSGG task, but the proposed method is not very novel. ",
            "summary_of_the_review": "Due to the lack of experiments and the lack of novelty, I think the paper is below the acceptance bar. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2503/Reviewer_XWwS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2503/Reviewer_XWwS"
        ]
    },
    {
        "id": "ricCwItVZU",
        "original": null,
        "number": 5,
        "cdate": 1667352353838,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667352353838,
        "tmdate": 1670894623166,
        "tddate": null,
        "forum": "KLrGlNoxzb4",
        "replyto": "KLrGlNoxzb4",
        "invitation": "ICLR.cc/2023/Conference/Paper2503/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper extended weakly supervised image scene graph generation to videos, and presented a new task of weakly supervised video scene graph generation, where a model must learn from unlocalized scene graphs on a sparse set of video frames. In tandem with the new task, the paper also proposed an interesting solution. The key to the solution is to generate pseudo scene graph labels by leveraging existing object detectors, distilling from image scene graph models, and modeling temporal continuity in videos. The proposed method is evaluated on the Action Genome dataset with some promising results.  \n",
            "strength_and_weaknesses": "**Strength**\n\nIn spite of an extension from images to videos, the proposed task of weakly supervised video scene graph generation is new.\n\nThe proposed method harnesses multiple cues (object detector, image scene graph model, transition probability between predicates) to generate pseudo scene graph labels for training. The method is interesting and could provide a baseline for future research. \n\n**Weakness**\n\nWhile the task and the method is interesting, the technical components of the proposed method are less exciting. Many of the ideas can be found in prior works, including (1) using object detector to ground unlocalized scene graphs (e.g., Zhong et al., 2021), (2) distilling from another model (see works on knowledge distillation), (3) using visual tracking to augment training samples (described as model-free teacher), and (4) modeling of relation transitions (Mi et al., 2021).  The proposed method seems like a reasonable combination of multiple existing ideas from video understanding and image scene graph generation. It is thus difficult to gauge the key innovation here. \n\nThe gap between a strawman baseline (learning a model from the same set of sparse frames with unlocalized scene graphs) and the proposed method is rather small (1% on SGDet). In fact, a stronger and perhaps more realistic baseline is listed as model C in Table 1 (within a gap of 0.7% on SGDet), where a model is trained using sparse scene graphs plus augmented annotations from simple tracking. It is not totally clear if this gap is sufficient to justify the proposed method, which is arguably more sophisticated in its training scheme.\n\n**Other Minor Comments**\n\nA key argument in the introduction to motivate this work is that relations that involve motion (e.g., walking vs. running) can not be readily distinguished in static images. This unfortunately remains an issue for the proposed method, as it relies on image features from an object detector (L285). I wonder if the authors have considered using video features (e.g., from 3D convolutional networks or video transformers) for the proposed task. \n\nI thought the baseline listed Table 2 is referring to model A in Table 1, yet the numbers do not quite match. It will be great if the authors can provide some clarification here. \n\nL322, I think the text is referring to Table 2 instead of Table 1.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is globally well written with some minor rough edges in the text. The proposed task is interesting yet the solution is less innovative. For reproducibility, some implementation details are provided in the paper and the authors promised to release the code in the future. ",
            "summary_of_the_review": "The paper presented an interesting new task of weakly supervised video scene graph generation, and proposed a solution to the task. While the solution is quite reasonable and combines several existing ideas, it lacks technical innovation and the results are less satisfactory. Overall, I am not enthusiastic about this paper. \n\nThe author response has addressed most of concerns, and I have raised my rating accordingly. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2503/Reviewer_spBp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2503/Reviewer_spBp"
        ]
    }
]