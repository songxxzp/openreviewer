[
    {
        "id": "cB_wy99zfzL",
        "original": null,
        "number": 1,
        "cdate": 1666617684107,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617684107,
        "tmdate": 1669935662445,
        "tddate": null,
        "forum": "YAxV_Krcdjm",
        "replyto": "YAxV_Krcdjm",
        "invitation": "ICLR.cc/2023/Conference/Paper4043/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a multimodal model that does not require any training on image-text pairs. The proposed non-parametric model, ASIF, leverages independent pretrained unimodal models to extract embeddings of data points in ground-truth image-text pairs. At inference, ASIF first computes the relative representation of a given input from one modality as the vector containing similarities between the given input and each data point from the dataset. It then treats this vector as the relative representation of the input's ideal correspondence from the other modality, and retrieves the candidate that has the most similar relative representation to the vector. With some sparsification treatments, ASIF achieves quite competitive zero-shot results on several image classification benchmarks, including ImageNet, ImageNet v2, CIFAR100 and Pets, using a relatively small paired dataset.",
            "strength_and_weaknesses": "Strength\n- The paper is well-written and throughly addresses related work and implications of the proposed method.\n- The proposed method is simple yet quite effective for multimodal zero-shot tasks using a relatively small paired dataset.\n\nWeaknesses\n- However, it seems that the model performance heavily depends on pretrained unimodal models, and the results reported in the paper come from the unimodal models that are trained on large-scale unimodal data: ImageNet21K (about 14M images) and 1B sentences scraped from the internet. I don't think that it is appropriate to say that ASIF is data efficient using such models. This seems to be more than the LiT training data. Do you have additional results using unimodal models that are trained on less data, such as ImageNet1K?\n- Since the model performance also heavily depends on the ground-truth image-text pairs, you need to provide quantitative results on out-of-distribution tasks. I am not sure which dataset among ImageNet (or ImageNet v2), CIFAR100 and Pets is far distant in distribution to the subset of CC12M that you use for computing relative representations.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read, and the proposed method can be easily implemented just by following the paper's instructions. However, one of the key contributions of the prosed method, data-efficiency, is not well-supported. The method has some technical novelty.",
            "summary_of_the_review": "The proposed method is simple yet quite effective for building a multimodal non-parametric model, but one of the key contributions, data efficiency, should be validated with unimodal models that are trained on small-scale datasets.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable: The paper does not have any ethical considerations to address.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4043/Reviewer_MKVK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4043/Reviewer_MKVK"
        ]
    },
    {
        "id": "VRTfoSULOsm",
        "original": null,
        "number": 2,
        "cdate": 1666758702622,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666758702622,
        "tmdate": 1666758702622,
        "tddate": null,
        "forum": "YAxV_Krcdjm",
        "replyto": "YAxV_Krcdjm",
        "invitation": "ICLR.cc/2023/Conference/Paper4043/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an interesting idea of turning two unimodal pre-trained models into a multi-modal model without training. Specifically, each sample is encoded into a relative representation with respect to paired multi-modal data in its own modality. The relative representation serves as the bridge between two modalities. ASIF achieves a non-trivial performance but still lags behind related works such as LIT and CLIP.",
            "strength_and_weaknesses": "## Strength\n1. interesting idea for turning two unimodal pre-trained model into a multi-modal model w/o training.\n1. the paper is well-written and easy to follow\n\n## Weakness\n\n1. The major assumption of the proposed method is that \"captions of images that are close in the visual space to be themselves close in the language space.\" Please quantitatively prove this claim.\n\n1. How does ASIF scale with respect to the size of multimodal dataset? In Figure 4, it looks like ASIF still has room for performance growth if more data are given. The reviewer is interested in seeing when the performance saturates. As the multimodal dataset scales up to 400M, what's the performance gap between ASIF and CLIP/LiT?\n\n1. What's the relationship between the \"quality\" of the visual encoder and ASIF's performance? For example, do we need to take a larger model (in terms of #params) or a model trained with more data for ASIF to achieve better performance? Or maybe the pre-trained visual encoder's performance on eg ImageNet better correlates with ASIF's performance.\n\n1. sensitivity of hyper-params p and k.\nIn Figure 4, it looks like the performance is quite sensitive to p and k. It would be good to show the grid search results with respect to these two parameters. Further, how sensitive are p and k to different target datasets? For example, the best (p, k) pair for ImageNet vs for Pets. \n\n1. Even though the idea is interesting, what's the future of this method? According to the performance, it doesn't seem like a good alternative to existing multi-modal models such as CLIP or LiT.\n\n1. In Table 1, it would be good to add a column of #params for each model.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well-written. However, some important experiments mentioned in the Weakness section are expected to further validate the claims and support their conclusions.",
            "summary_of_the_review": "This paper proposed an interesting idea to bridge two uni-modal pre-trained models w/o training. However, some experiments are missing as detailed in the Weakness section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4043/Reviewer_nLjp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4043/Reviewer_nLjp"
        ]
    },
    {
        "id": "q8MtdOd31t",
        "original": null,
        "number": 3,
        "cdate": 1666818348655,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666818348655,
        "tmdate": 1666818567684,
        "tddate": null,
        "forum": "YAxV_Krcdjm",
        "replyto": "YAxV_Krcdjm",
        "invitation": "ICLR.cc/2023/Conference/Paper4043/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes ASIF, which can retrieve relevant captions given an image **without training**, utilizing 1) two pre-trained unimodal encoders and 2) a relatively small multimodal datasets. The key intuitive is that \u201ccaptions of similar images should be themselves similar\u201d; thus the retrieval is done through the relative distances given by the two pre-trained unimodal encoders. \n\nIn experiments, the authors show decent performance on several image classification datasets, using conceptual captions as the multi-modal dataset.",
            "strength_and_weaknesses": "Pro: It it interesting to see such an effort on tuning unimodal encoders into multimodal models using non-parametric methods. \n\n---\n\nCon: clarification needed on relationship with respect to K Nearest Neighbor.\n\nAs noted by the authors, ASIF, in essence, is \u201cmaking predictions on new samples by exploiting the similarity with a dictionary of previous data points\u201d. This makes ASIF extremely similar to KNN. The first half of ASIP, i.e., finding the k most-similar images, looks the same as KNN. What differs is how to find the best caption given the k \u201cneighbor\u201d image-caption pairs. I would imagine a naive approach of finding the caption from the training set that is mostly similar to k captions (using Language Model similarity) would do pretty well (similar to the majority voting in KNN). The author also provides a nice discussion in Sec 4 \u201clearning or retrieval\u201d. However, if we link ASIF to KNN, then it would be clear that ASIF, like KNN, counts as a non-parametric supervised learning algorithm. \n\nIn fact, KNN has been used for evaluating self-supervised visual representation models (e.g., Table 1 of Zhou et al.) and the performance is already high (78.0). But I can see that the performance is not directly comparable as Zhou et al. used ImageNet as the KNN training set while this paper uses Conceptual Captions (CC). The impressive results in the paper might look less \u201csurprising\u201d, if we were to believe that ImageNet and CC have a lot of overlap. Then the current results would be somewhat a domain-shifted version of the KNN experiments in Zhou et al.\n\nI would thus appreciate a discussion on:\n\n 1) what makes ASIF different from and potentially better than KNN (and similar methods);\n\n 2) experiments on a naive KNN baseline on using CC data for ImageNet. \n\nZhou et al., iBOT: Image BERT Pre-Training with Online Tokenizer\n\n---\n\nMinor questions:\n\nFor the image classification experiments, ASIF will select a caption from the 1.6M captions. How should we map the retrieved caption to a label? Is it a simple rule-based ngram match?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow. The results are novel.",
            "summary_of_the_review": "I very much like the intuition of ASIF. It works decently in practice and has several nice properties nicely argued by the authors.\n\nHowever, my main concern is that the method is very similar to KNN and once we consider it as some kind of KNN, the analysis and results look less \u201csurprising\u201d.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4043/Reviewer_hNdq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4043/Reviewer_hNdq"
        ]
    }
]