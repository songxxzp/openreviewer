[
    {
        "id": "_Nm5EA6WC5j",
        "original": null,
        "number": 1,
        "cdate": 1666687276869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687276869,
        "tmdate": 1669199463000,
        "tddate": null,
        "forum": "GBU1mm8_WkV",
        "replyto": "GBU1mm8_WkV",
        "invitation": "ICLR.cc/2023/Conference/Paper5821/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the author's establish a novel method to train normalizing flows without the need to compute the likelihood. Thereby, the computation of the Jacobian determinant of the flow is avoided, which can be expensive or requires a restricted architecture to be cheap to compute. They propose objectives inspired by two-sample tests, such as the energy loss, and show how they can be estimated and used for training. This objective can be used to train a variety of flows, and the authors focus on applying it to flows having Jacobian determinants that are expensive to compute. Moreover, they introduce a novel flow architecture, called semi-autoregressive flow, that can only be trained with their energy loss.\nIn their experiments, the authors demonstrate that their approach is competitive with other training procedures on UCI and MNIST datasets, while being often faster to train and evaluate.",
            "strength_and_weaknesses": "### Strengths\n\nComputing the Jacobian determinant of normalizing flows is a challenge, but is required for most existing training procedures. Often, the architecture is designed such that the computation becomes simple or approximations are used, but this can limit expressivity and accuracy. The authors address this challenge in a creative way by proposing a novel training procedure based on two-sample test objectives.\nFurthermore, they demonstrate that such a training procedure unlocks new opportunities in terms of flow design by introducing a novel flow model that can only be trained with their method.\nThey apply their method to flows of various architectures and, thereby, demonstrate its effectiveness. Important is the runtime comparison, since the superior runtime is one of the main arguments for using their method.\n\n### Weaknesses\n\nMy main criticism concerns the experimental evaluation. The authors apply their method to various normalizing flows for which alternative training procedures exist, but the baseline models trained via maximum likelihood have a different architecture. There should also be experiments with the same flow architecture but different training methods being used. Moreover, the use of the CRPS and the NLL as evaluation objectives make it difficult to compare the work with other articles in the field, where the likelihood (UCI) or bits per dim (MNIST) are the main evaluation metrics being used. Computing them might be expensive for some of the models, but this is important information to evaluate the method.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and well organized. The method is novel and the argumentation correct. My main concern regarding the quality of the paper is that the experimental evaluation of the method has flaws that I mentioned above.",
            "summary_of_the_review": "Due to the importance and the novelty of this method, I tend towards accepting this paper. However, I ask the author to address the concerns I voiced regarding the experimental evaluation\n\nEdit after rebuttal:\n\nI thank the reviewers for their response and additional experiments. I am still inclined to accept the article.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5821/Reviewer_Ljxv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5821/Reviewer_Ljxv"
        ]
    },
    {
        "id": "9XLQ8-UIY8",
        "original": null,
        "number": 2,
        "cdate": 1666820186153,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666820186153,
        "tmdate": 1666820793532,
        "tddate": null,
        "forum": "GBU1mm8_WkV",
        "replyto": "GBU1mm8_WkV",
        "invitation": "ICLR.cc/2023/Conference/Paper5821/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes training normalizing flows using sample-based objectives. ",
            "strength_and_weaknesses": "\nWeakness:\n  - The use of IPM are standard in the GAN literarture. From what I understand, the main thing separating this work from those training invertible generative models with a discriminator (e.g. [1]), is that there are no parameters in this objective. In fact, the CPRS objective (Eq 3) has the same gradient as MMD, so the only reason this isn't adversarial is because unlike the MMD-GAN objective, this work does not make use of a neural network-defined kernel function.\n  - Related to the above, the only experiments are in tabular and MNIST, where I imagine simple kernels and distance metrics suffice. But for training on more complex distributions like colored images, a neural network discriminator is crucial. \n  - Furthermore, this work does not convincingly show that training without a discriminator is sufficient, since Flow-GAN [1] is not compared against.\n  - In terms of evaluation metrics, it would be better to use an evaluation metric that is different from the training objective for SAEFs, which heavily biases towards SAEF. Have the authors considered metrics such as MMD, NLL, or a kernel-based density estimator [2]?\n\n[1] \"Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models.\" Grover et al. (2018)\n\n[2] \"On the Quantitative Analysis of Decoder-Based Generative Models.\" Wu et al. (2016)",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty:\n  - Using IPMS to train normalizing flow has been explored before [1]. \n  - The use of slicing seems interesting, but it isn't clear if this was used in the experiments. Furthermore, this seems similar in spirit to using random features for kernel estimation [3]. \n\nClarity:\n  - This paper discusses multiple objectives for training. Which objective(s) were used for training the models in the experiments? Were the baselines trained with the same objective (CPRS) or maximum likelihood?\n  - I feel like NLL values should be reported in Table 5, since all of these are invertible and can compute log-likelihood values. \n  - The NLL values should be negated in Table 4.\n\nComments:\n  - Can the inverse of SAEF be computed? Perhaps not analytically, but an iterative procedure [4] should work.\n  - SAEF's log probability can be computed by computing the determinants of each block (which I assume is how the NLL estimates are computed in Table 4). Have the authors tried training with maximum likelihood and comparing it to the sample-based objectives? \n\n[3] \"Random Features for Large-Scale Kernel Machines.\" Ali Rahimi and Ben Recht. (2007)\n\n[4] \"Mintnet: Building invertible neural networks with masked convolutions\" Song et al. (2019)",
            "summary_of_the_review": "Since the paper focuses on sample quality and sample-based metrics, I feel the experiments on only tabular and MNIST are not sufficient. Furthermore, I'm not convinced this is better than the existing variety of GAN objectives such as MMD-GAN. The main difference seems to be the lack of a neural network for defining a kernel function; however, human-designed metrics tend to scale poorly to more complex data sets.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5821/Reviewer_B3hX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5821/Reviewer_B3hX"
        ]
    },
    {
        "id": "YaOP9e3v3SR",
        "original": null,
        "number": 3,
        "cdate": 1667571483020,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667571483020,
        "tmdate": 1670614595828,
        "tddate": null,
        "forum": "GBU1mm8_WkV",
        "replyto": "GBU1mm8_WkV",
        "invitation": "ICLR.cc/2023/Conference/Paper5821/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a novel strategy to train invertible neural networks that model a distribution, a.k.a. normalizing flows. In particular, the authors propose to use proper metrics as the objective function to train flows. This approach relaxes the requirement of computing the determinant of the Jacobian of the flow and unlocks architecture for which computing this determinant would be too computationally demanding. \n\nAuthors strongly motivate their approach with theoretical arguments for the soundness of proper metrics. They also validate their approach experimentally, showing improvements in the sample quality compared to likelihood-based training. ",
            "strength_and_weaknesses": "# Strengths\n- The idea presented in this paper is exciting and somehow novel. It efficiently trains invertible architectures while keeping the potential advantage of their invertibility at test time. The generalisation of the ideas from Si et al. (2022) to non-autoregressive architectures is well-exposed and has a practical interest.\n- The empirical results demonstrate that using the energy metric to train the flow is a good objective function if one is interested in good sample qualities (although it is not always clear how we define good samples) or by the marginal densities (as well represented by the CRPS score).\n\n# Weaknesses\nAlthough I think the ideas presented in the paper make sense and could have a real interest for some practical use cases, I find the paper's organisation and way of presenting ideas confusing. For instance, the tables' order does not follow the structure of the paper and the captions are uninformative, which makes it hard for the reader to follow the results at first glance. In addition, the discussions in section 3 go in many different directions and are missing a link between them. It is only at a second read that we can more or less grasp what each paragraph brings to the paper.\n\nI am also a bit confused by the argument from the authors that autoregressive architectures lead to \"good sample quality\". They can be good indeed. Still, I do not see why they are better than other generative models in terms of sample quality. \n\nThe authors say that MAF leads to gaussian distribution, which is only valid for a 1-step flow but is immediately relaxed if there are dependencies between variables and one uses more than one step of the flow. Indeed, MAF allows non-linear interaction between the variables. Thus the corresponding joint and marginal distribution can be non-Gaussian.\n\nI also think the author should mention that CRPS only reflects the quality of the marginal distribution and cannot say whether the model captures dependencies well.\n\nAt some point, the authors say that using the sliced objective (which I find a neat idea) reduces the computational complexity. Of course, this is true. However, it is unclear to which extent this is necessary, I would have liked to see numbers for that.\n\nI recommend that the authors create a better structure and story for this paper. I do think the ideas are excellent and (probably) well-motivated theoretically. However, the article in its current form is not pleasant to read, which I find sad for the quality of the ideas presented.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "See my comments above.",
            "summary_of_the_review": "Although the idea and experiments presented in this paper are interesting. I find the presentation too poor to argue for acceptance. Hence I argue for a weak reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5821/Reviewer_gK13"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5821/Reviewer_gK13"
        ]
    },
    {
        "id": "7RkuzKaRP9",
        "original": null,
        "number": 4,
        "cdate": 1667589908471,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667589908471,
        "tmdate": 1667590551176,
        "tddate": null,
        "forum": "GBU1mm8_WkV",
        "replyto": "GBU1mm8_WkV",
        "invitation": "ICLR.cc/2023/Conference/Paper5821/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper uses the energy loss for training normalizing flows. This produces a consistent estimator that doesn't require computing determinants of the Jacobian of the transformation, making it more flexible in terms of the choice of architecture. The paper also introduces a few tricks to further improve scaling and/or performance: slicing the data using 1D projections, and a semi-autoregressive flow model that interpolates between a full autoregressive model and a completely parallel model (i.e., a typical normalizing flow). Experiments on UCI and handwritten digit datasets show favorable results compared to baseline flows, VAEs, and autoregressive models, while allowing for fast sampling and exact posterior inference.",
            "strength_and_weaknesses": "Strengths\n- The energy loss is straightforward, faster to compute, and produces good samples.\n- Slicing provides an effective way of scaling the training.\n- Semi-autoregressive flows elegantly trade-off computation and expressivity.\n- Exact posterior calculation is possible, allowing for things like interpolation that is not easy to do in autoregressive models.\n\nWeaknesses\n- The experiments are only done on UCI and handwritten digit datasets. At the very least, CIFAR-10 would have been good.\n- Comparisons to larger-scale flow models like GLOW would be more convincing.\n- There are key differences between the behavior of the energy loss and NLL (they are not strictly correlated). More exploration of where they differ in terms of allocation of probability mass would be nice.",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is reasonably easy to follow, at least on a high level. The paper is very heavy on the use of acronyms, which makes it a little difficult to follow for one who is not intimately familiar with all of these acronyms and their variations. A section on each acronym used in the tables and a brief description of them would be helpful. Also, in the case of CRPS vs U-CRPS, a mathematical definition would be useful as opposed to just a textual description.\n\nMinor nitpicks:\n- Table 5 should be lower down, the current draft has it above Table 4.\n- Appendix typo: \"the above **clam** also holds\"",
            "summary_of_the_review": "The use of the energy loss is compelling, and the background behind it is nicely written. This approach frees normalizing flows from their previous architectural constraints, and this paper explores this idea through the introduction of a semi-autoregressive flow model that interpolates between faster sampling and better sample quality.\n\nI think that the ideas certainly have merit, however it feels like there is a bit of a gap between the promise of the paper and the actual demonstration of the approach. Specifically, the experiments are very small-scale for 2022, only operating on UCI datasets and MNIST. I'm surprised that CIFAR-10 wasn't included, as it is commonly used as a baseline for generative models. For example, one method referenced and compared with (Papamakarios et al., 2017) uses CIFAR-10, and it is now 5 years old. Normalizing flows (e.g., GLOW),  PixelCNN, and VAEs have all been scaled to more challenging datasets many years ago.\n\nI certainly appreciate a careful and detailed study of the estimator and its effects on small-scale data, however the conspicuous lack of larger-scale experiments tell me that either the authors have not yet run these experiments, or there is a severe limitation that is keeping the model from working on those datasets.\n\nI personally appreciate new and elegant ideas, and so all I'm asking for is an attempt on something a more challenging baseline. If the results are negative, then a discussion about reasons for this, and avenues for improvement would be necessary. The intention is to ensure that this paper is carried forward and does not languish because other researchers find it cannot scale well.\n\nIf there is a strong reason why the existing experiments are sufficient, then I would be happy to discuss this with the authors.\n\nMinor comments:\nI think you should add additional references for normalizing flows.\nLaurent Dinh, David Krueger, Yoshua Bengio, \"NICE: Non-linear Independent Components Estimation\", ICLR 2015\nLaurent Dinh, Jascha Sohl-Dickstein, Samy Bengio, \"Density estimation using Real NVP\", ICLR 2017\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5821/Reviewer_WiBH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5821/Reviewer_WiBH"
        ]
    }
]