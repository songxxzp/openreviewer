[
    {
        "id": "X6Oss5ht8O_",
        "original": null,
        "number": 1,
        "cdate": 1666672113252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672113252,
        "tmdate": 1666672113252,
        "tddate": null,
        "forum": "Q-UHqMorzil",
        "replyto": "Q-UHqMorzil",
        "invitation": "ICLR.cc/2023/Conference/Paper1915/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work investigates the function invariance w.r.t. eigenvectors of graph Laplacian to reduce the learning complexity, including sign invariance and basis invariance. Then it proposes corresponding SignNet and BasisNet to incorporate these two invariances where many popular models can be directly plugged in as basic models. Moreover, it provides theoretical guarantees that the proposed models are expressive enough to conduct tasks beyond eigenvectors. Finally, the proposed methods are evaluated by synthetic and real-world experiments.",
            "strength_and_weaknesses": "Pros:\n1. The motivation of introducing invariances into the model naturally reduces the learning complexity, which is verified sufficiently by excellent performance on real tasks against strong baselines.\n2. It is very easy to directly plug many popular GNNs into SignNet to achieve performance boost, as shown in Table 1.\n3. The theoretical arguments are comprehensive, including constructing SignNet and expressiveness of SignNet/BasisNet.\n\nCons:\n1. Is BasisNet included in any experiment other than Appendix J.3? I am wondering whether BasisNet suffers high complexity because there might be many distinct multiplicities $d_i$ so that too many IGNs are included. In that sense, is there any way to implement BasisNet efficiently? Also, how is the runtime of BasisNet in Table 9?",
            "clarity,_quality,_novelty_and_reproducibility": "The details are presented clearly. \n\nThe quality of theoretical arguments is good, and it enjoys great numerical performance. \n\nThe novelty and reproducibility are also good.",
            "summary_of_the_review": "I would like to suggest an acceptance.\n\n=== Acknowledgement ===\n\nDue to time limit, I did not check proof in Appendix. The theorems seem to be good intuitively.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1915/Reviewer_cG9j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1915/Reviewer_cG9j"
        ]
    },
    {
        "id": "ZZCwLRwje1N",
        "original": null,
        "number": 2,
        "cdate": 1666789598926,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666789598926,
        "tmdate": 1666789598926,
        "tddate": null,
        "forum": "Q-UHqMorzil",
        "replyto": "Q-UHqMorzil",
        "invitation": "ICLR.cc/2023/Conference/Paper1915/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces two architectures SignNet and BasisNet that respect some basic symmetries inherited by spectral graph methods i) sign flips: if v is an eigenvector then so is \u2212v; and (ii) more general basis symmetries: in higher dimensional eigenspaces there might be infinitely many choices of basis eigenvectors. The authors show that under certain conditions the proposed architectures are universal and can approximate any continuous function of eigenvectors with the proper invariances. Moreover, the networks are theoretically powerful for graph representation learning\u2014they can provably approximate and go beyond both spectral graph convolutions and powerful spectral invariants. Experimental results are shown on molecular graph regression tasks, learning expressive graph representations, and texture reconstruction on triangle meshes. ",
            "strength_and_weaknesses": "Strength: Designing proper architectures that preserve symmetries and are more expressive than current ones is a very interesting and open topic in the graph machine learning literature. The proposed architectures in the paper are theoretically sound, and lead to nice integrations of symmetry-preserving positional encodings into a GNN framework. The results are adequately analysed and sufficiently supported.\n\nWeaknesses: Some of the notions are not easy to understand from the text. Theorem 1 which is one of the main results of the paper should be discussed and explained more. ",
            "clarity,_quality,_novelty_and_reproducibility": "This is a relatively well-written paper, of high quality. The results are novel, and the authors provide enough material for reproducing them. ",
            "summary_of_the_review": "In summary, this is a strong paper with nice theoretical results. Some aspects that I think should be clarified are the following:\n\n1) While Theorem 1 is interesting and insightful, could you please provide an explanation of what it implies in practice? Does it mean that when the group is a product, we could do separate sums over the elements of the group to achieve invariance? \n\n2) How does the computational complexity of IGN affect the overall complexity of the algorithm?\n\n3) Is there an inductive bias towards locality in SignNet (as in other graph convolutional methods)? \n\n4) Figure 2 needs more explanation. Since the proposed positional encoding schemes can take as input node features, why do we need another GNN layer i.e., GNN(A,X,sigNet or basisNet(V,X)) on the top of that? Is that related to localisation aspects? ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No specific concerns. ",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1915/Reviewer_a8ck"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1915/Reviewer_a8ck"
        ]
    },
    {
        "id": "fXpgDiAZOd",
        "original": null,
        "number": 3,
        "cdate": 1667021107414,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667021107414,
        "tmdate": 1667021107414,
        "tddate": null,
        "forum": "Q-UHqMorzil",
        "replyto": "Q-UHqMorzil",
        "invitation": "ICLR.cc/2023/Conference/Paper1915/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose SignNet and BasisNet which are invariant to the sign and rotation symmetries that exist in eigenspaces, particularly in the Laplacian context for graphs. They demonstrate a universal approximation theorem with the approach under some conditions. They evaluated SignNet and BasisNet with various experimental tasks/datasets. ",
            "strength_and_weaknesses": "Strengths:\n\n- The task that the authors are tackling is of general interest and importance, especially given the prevalence of Laplacian eigendecompositions in the literature, and the method/architecture that they propose are to the best of my knowledge novel and original. \n\n- The paper is well written and clearly motivated. \n\n- Their theorem on universal approximation for spectral graph convolutions is significant. \n\n- Their experimental evaluation is comprehensive and demonstrates the efficacy of the proposed approach well. \n\nWeaknesses: \n\n- Proposition 2, Theorem 2 and Proposition 3 are stated somewhat verbally/informally in the main paper. It would be ideal to include a mathematically rigorous statement in the main paper. In particular, given how commonly the term \"universally approximates\" is used in the paper, it was not rigorously defined in the main paper.  Given that there can be different types of universal approximation, it would be best to rigorously define it in the main paper to avoid any ambiguity/confusion. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clear paper, well-written, novel approach that has strong experimental evaluation. ",
            "summary_of_the_review": "Overall a strong paper. Would be stronger if they make their theoretical statements more clear/specifically stated. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1915/Reviewer_aMn3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1915/Reviewer_aMn3"
        ]
    },
    {
        "id": "z1bJO0H5uos",
        "original": null,
        "number": 4,
        "cdate": 1667485521869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667485521869,
        "tmdate": 1667485711831,
        "tddate": null,
        "forum": "Q-UHqMorzil",
        "replyto": "Q-UHqMorzil",
        "invitation": "ICLR.cc/2023/Conference/Paper1915/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, SignNet and BasisNet new neural structures are proposed, which are the same as the two key SY invariants displayed by eigenvectors. It is also proved that under certain conditions, they can approximate any continuous function of invariants with the expected eigenvectors. The experimental results show that the proposed method is superior to the forward baseline in molecular graph regression, learning expression graph representation and learning neural field on triangular grid.",
            "strength_and_weaknesses": "In this paper, SignNet and BasisNet new neural structures are proposed, which is interesting",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is of fair quality and novelty.",
            "summary_of_the_review": "My research field is not related to this manuscript and I am not very familiar with the research content. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1915/Reviewer_BB19"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1915/Reviewer_BB19"
        ]
    }
]