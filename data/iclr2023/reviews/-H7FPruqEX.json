[
    {
        "id": "PhQwsuAt0K_",
        "original": null,
        "number": 1,
        "cdate": 1666446809042,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666446809042,
        "tmdate": 1668766468970,
        "tddate": null,
        "forum": "-H7FPruqEX",
        "replyto": "-H7FPruqEX",
        "invitation": "ICLR.cc/2023/Conference/Paper4476/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel approach to generalized policy iteration (GPI) exploiting the inherent connection between policy evaluation and policy improvement. It firstly shows that the policy evaluation and policy improvement steps are connected in the case of parameter sharing between the value function and the policy, it describes the benefit of this connection and proposes the method CASA to exploit this connection. CASA is tested on the Atari 200M benchmark where it achieves remarkable state-of-the-art results.",
            "strength_and_weaknesses": "Strenghts\n-----------\n- The paper studies the interesting problem of generalized policy iteration from the original point of view of bridging the gap between policy evaluation and policy improvement;\n- Empirical results on Atari are impressive.\n\nWeaknesses\n--------------\n- The quality of the writing is not satisfactory and makes the paper hardly readable and understandable;\n- The proposed method CASA seems to rely on the fact that ALL parameters should be shared between value functions and policy. However, in the paper it is stated that only \"most parameters\" are shared. Doesn't this critically go against the assumptions of CASA? Please clarify;\n- The figures are too small and hardly readable.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is quite suboptimal. Many sentences are not well formulated and make the paper not easily understandable. For example:\n- \"Since $Q^\\pi \u2212 Q_\\theta$ and $Q^\u03c0 \u2212 V_\u03b8$ are two scalars, when $\\nabla_\\theta Q_\\theta \\varpropto \\nabla_\\theta \\log \\pi_\\theta$, the two sides of $\\beta$ meet\": instead of saying that the sides meet, considering rephrasing saying that the gradients of policy improvement and policy evaluation overlap;\n- \"As I and E are perpendicular to each side of \u03b2, they become parallel arrows on opposite directions.\": how can I and E become arrows? This should be rephrased;\n- \"By equation 6, the directions of the gradients g are in common. We call it a path. The equation 6 shows that $\\nabla_\\theta LQ$ and $\\nabla_\\theta J$ walk along the same path g.\": these sentences are redundant and should be merged in one.\n\nThe novelty of the proposed method is satisfactory.\n\nThe list of used hyperparameters is thoroughly reported for reproducibility, but the source code is missing.",
            "summary_of_the_review": "This paper studies the important problem of generalized policy iteration in reinforcement learning, and the proposed method is intriguing and able to achieve remarkable results in the challenging Atari 200M benchmark. However, I'm not convinced about the theoretical correctness of the method. It seems to me that conflict between policy evaluation and policy iteration is avoided only in the case where the parameters of value functions and policy are exactly the same. This is never the case in practice. As also stated in the paper, the parameters are mostly shared, except for individual heads. It is not clear to me whether the fact that MOST parameters are shared, is considered sufficiently good to meet the assumption of CASA; in that case, it would be nice to have a better discussion about it. Unfortunately, the low quality of the writing makes everything slightly harder to grasp.\n\nIt would be really nice if the authors could solve these doubts in the rebuttal, and I'd gladly increase my score in case the answer will be convincing enough.\n\nPost-rebuttal feedback\n------------------------------\nI have read the revised paper and appreciate some improvements on the previous version. I am still doubtful about the approach, sharing similar concerns to those expressed by Reviewer aLPu, in particular\n> In that sense, this is really a value-based method as there is no \"independent\" learning of a parameterized policy.\n\nThus, I raise my score for the improved version but I still consider this work under the threshold of acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4476/Reviewer_hwhf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4476/Reviewer_hwhf"
        ]
    },
    {
        "id": "m9mpPrfMBO",
        "original": null,
        "number": 2,
        "cdate": 1666552518179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666552518179,
        "tmdate": 1666552518179,
        "tddate": null,
        "forum": "-H7FPruqEX",
        "replyto": "-H7FPruqEX",
        "invitation": "ICLR.cc/2023/Conference/Paper4476/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This works studies the relation between policy evaluation and policy improvement in the context where the value-function(s) and the policy share the same parameterization, and suggests a way to consolidates the two type of updates. ",
            "strength_and_weaknesses": "The basic ideas underlying the paper are interesting and could potentially be a valid contribution. However there are several key issues that make the claims either difficult to understand (in the good case) or wrong/inaccurate (in the worse case). The evaluation is also not satisfying, relying on complex architectures and environments (see more below) to try and support what are fundamentally abstract theoretical claims.\n\nI would recommend the authors to reconsider the evaluation experiments that are used to support the theoretical claims. Ultimately, even a much simpler tabular settings where everything is much more controlled can be a much better starting point for this kind of paper, even if not all claims or properties can be rigorously proven theoretically.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is hard to follow. There are technical details which are not explained well, or are misleading, throughout. Some can be easily fixed, but some are more fundamental.\n\n**Major issues**\n- In general, the paper claim to \"CASA, Critic AS  an Actor, which theoretically guarantees that the evaluation gradient of the state-action values and the policy  gradient to be parallel\". But the way this is achieved is a triviality, because the architecture forces the log-policy to be linear in the state-action values. In that sense, this is really a value-based method as there is no \"independent\" learning of a parameterized policy. Everything goes through the value (more precisely, advantage) function. And the main idea is to use a Softmax instead of a Max (greedy) for the action-selection rule.\n  \n- The definition of $\\beta$  and its explanation (Section 3.1) are misleading (or at least unclear). The definitions of $\\mathbf{I}$ and $\\mathbf{E}$ included (and rightly so) an expectation over state-action pairs as induced by $\\pi$. This expectation for some reason is missing from the definition of $\\beta$. Note that with this expectation, it is **not** the case that \"$Q^\\pi-Q_\\theta$  and $Q^\\pi-V_\\theta$  are two scalars\" that can be pulled out of the expectation because in general they will be correlated with the gradients. In fact the entire PG method relies on this correlation, otherwise the expectation value of the score function by itself $\\mathbb{E}\\left[\\nabla\\log\\pi\\right]$ is simply $0$. Since this is not explained it's also unclear how $\\beta$ is calculated empirically later, from full trajectories.\n- The explanation that follows is also confusing, or at least I couldn't understand what is being claimed (The math says that $\\mathbf{I}$ and $\\mathbf{E}$ are parallel, but the text says they are perpendicular?)\n-  Related to that, the question on \"under what conditions $\\nabla_\\theta Q_\\theta \\propto \\nabla_\\theta\\log\\pi$  is misleading because $\\nabla_\\theta Q_\\theta$ does not reflect policy evaluation at all, if anything it is more related to policy improvement: since $\\theta$ (implicitly) parameterizes the policy, $\\nabla_\\theta Q_\\theta$  (implicitly) means a change in the policy that will make the action values higher, i.e., improve the policy.\n\n- The fact that all experiments are done with a recurrent network (a detail that is somewhat \"hidden\" in Section 4.1) is a very serious shortcoming for the evaluation. The entire theoretical attempt is built on top of memoryless/reactive value-function and policy estimations (as is standard for MDPs), but then the experiments is done with an RNN. This might be a source of correlations in the gradients, estimations, etc. throughout the trajectories which is completely out of scope of the more \"theoretical\" explanation.\n\n**Minor issues**\n*(or, really, not minor but more easily fixable)*\n\n*Preliminary section:*\n- The presented objective wrongly mixes the \"trajectory\" and \"state-action occupancy measure\" formulations (e.g. Sutton et al. 1999, Ghosh et al. 2020) for the RL objective. If $s\\sim d^\\pi$  then this already encapsulates the $\\sum_t \\gamma^t \\mathbb{P}^\\left(t\\right)\\left[s\\right]$ averaging. Since the discounted occupancy measure is not really used later in the paper it seems preferable to simply write the objective in the \"trajectory formulation\" i.e., $J=\\sum_t\\left[\\gamma^tr\\left(s_t,a_T\\right)\\right]$ and explain how the sequence $s_t,a_t$ is being sampled by the policy-environment interaction. Moreover:\n\t- it is currently not defined that $a_t$ is sampled via $\\pi\\left(s_t,\\cdot\\right)$ .\n\t- The measure $d^\\pi$ as defined is **NOT** the \"stationary distribution induced by $\\pi$\" but rather the normalized discounted occupancy measure / visitation distribution. It becomes the stationary distribution only in the limit $\\gamma=1$ (yielding the time-average criterion instead of the discounted criterion of performance).\n\n- In the next paragraph, the definition of $V\\left(s\\right)$ and $Q\\left(s,a\\right)$ should be conditioned on $s_0=s$ and $s_0=s,a_0=a$ respectively, rather than on $s$ and $s,a$ which can be confusing to interpret.\n\n- The same issue of mixing the \"trajectory\" and \"state-action occupancy measure\" repeats in the paragraph on Policy-Gradient methods (namely $\\Psi$ has an explicit time-index inside it which is not really defined. Contrast this to how it is defined in Schulam et al. 2015, Eq.1).\n- It is not clear why the concept of $\\gamma$-justness, which is never used again in the paper, is introduced here, instead of a more simple explanation. This also confuses between the exact PG and an estimate of it based on sampled trajectories (the concept of  \"\"$\\gamma$-justness\" being relevant to the latter rather than the former case, while here the paper describes the exact gradient).\n  \n- The Equations defining the objective/optimization problems, gradients, and (in the next Section) $\\beta$ should all be numbered. \n\n*Methodology section*:\n- Why is the PG theorem cited *again*, only a few sentences after it was already presented, but using a different form than the one presented in the Preliminary section? This will only confuse readers.\n  \n- What does $\\mathbb{E}_p$  represents (in another un-numbered equation, discussing the relation to dueling DQN)? what is $p$? \n- What is $\\mu$ in Table 1?\n\n- The text in Figure 3 (axes labels, labels) is **tiny** and becomes readable only in 350% zoom. \n  \n- Figures 4 and 3: Several panels report $\\cos$ values side by side but have (very) different y-axis scale. This is very misleading. All Figures depicting $\\cos$ values should have a normalized scale between 0 to 1 (or -1 to 1 if relevant).\n \n*Related work*:\n- In general, the concept of $\\beta$ might be related to the concept of a \"compatible value function\" (Sutton et al. 1999, see also Kakade 2001). It would be interesting if the proposed concept here can (maybe after some corrections/modifications) serves as a generalization of the original compatibility notion (which was more of a binary concept, yes or no). I would encourage the authors to relate and explain how and if their ideas relate to that, or at the very least mention it in the related work.",
            "summary_of_the_review": "The paper deals with an interesting question and some parts or aspects of it could be a valid contribution. However in its current form it has too many open gaps and issues that should be fixed before it is ready for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4476/Reviewer_aLPu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4476/Reviewer_aLPu"
        ]
    },
    {
        "id": "J2_DdxvHJAZ",
        "original": null,
        "number": 3,
        "cdate": 1666559301517,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559301517,
        "tmdate": 1666559301517,
        "tddate": null,
        "forum": "-H7FPruqEX",
        "replyto": "-H7FPruqEX",
        "invitation": "ICLR.cc/2023/Conference/Paper4476/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the problem of solving the alleged inconsistencies between the policy evaluation and policy improvement steps of the \u201cpolicy iteration\u201d algorithm. The paper proposes a method called CASA as a solution which suggests some modifications on the gradient calculations of standard algorithms such as PPO.",
            "strength_and_weaknesses": "*Strengths:*\n\nThe formulation of the beta variable is interesting.\n\n*Weaknesses:*\n  \n * I cannot make sense of the studied problem at all. What does it mean to have a gradient conflict between two operational steps (policy evaluation and policy improvement) which have complementary contributions to the solution?\n * The paper states its aim as to \u201celiminate the inconsistency between the policy evaluation and policy improvement steps\u201d. However, I do not understand how the proposed CASA method solves this problem. It only sets up a tunable continuum between these two steps. But one would also get the same using the \\lambda-policy iteration formula, which is introductory textbook stuff.\n * I do not find the reported ablation study informative. The benefit of CASA can be more directly quantified if the paper compares PPO vs PPO+CASA and R2D2 vs R2D2+CASA directly in the Atari game use case. \n * Overall the experiments section is rather slim. The paper does not compare against some strong candidates such as SAC and MBPO.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has a rather scattered presentation style. I found it hard to follow the storyline, disambiguate the studied problem, and a main hypothesis drawn as a solution to it. This also makes it hard to view the experiment results as empirical evidence to a scientific claim.",
            "summary_of_the_review": "The studied problem of the paper is not stated in a technically rigorous and unambiguous and intuitively clearly motivated way. The solution attempt also does not appear to follow a consistent deductive process. The reported experiment results have weak relevance to the main hypothesis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4476/Reviewer_QNNj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4476/Reviewer_QNNj"
        ]
    },
    {
        "id": "7Mc8-BXRTgn",
        "original": null,
        "number": 4,
        "cdate": 1667480190052,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667480190052,
        "tmdate": 1667480190052,
        "tddate": null,
        "forum": "-H7FPruqEX",
        "replyto": "-H7FPruqEX",
        "invitation": "ICLR.cc/2023/Conference/Paper4476/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a method for reducing gradient interference between different components (Q, V, and policy) in deep RL with discrete actions. By carefully placing stop-gradient (sg) operators in the objective, the authors show (both theoretically and empirically) that the gradients between the Q function and the policy can be made parallel, so that there is no interference. The method, CASA, can be applied to value-based or policy-based algorithms using the same computations, and attains strong performance on Atari.",
            "strength_and_weaknesses": "Strengths:\n* The paper addresses an issue that many ignore in deep RL \u2013 the relationship between the policy evaluation and policy improvement steps in policy iteration.\n* The clever usage of sg in the objective leads to the desired result without any approximation or extra computationally intensive steps.\n* CASA substantially improves the performance of the base algorithm (IMPALA) on the Atari benchmark.\n\nWeaknesses:\n* The method as presented only applies to discrete-action problems, owing to the softmax parameterization of the policy and the summation over actions when computing the advantage.\n* The paper shows plots for many different gradient angles, but it\u2019s not clear which we should care the most about. It would be helpful to have a better understanding of how the angles between the gradients correlate with performance.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is mostly clear, but could be improved in places.\n\nIn the Preliminary section, the objective $J$ is written incorrectly. If you\u2019re taking the expectation over the discounted stationary distribution, the random variable inside the expectation should just be $r(s,a)$, not the discounted sum of rewards.\n\nThe CASA approach is novel, to my knowledge.\n\nThe appendix includes the relevant Python package versions and an exhaustive list of hyperparameter settings, so I believe it would be reproducible.",
            "summary_of_the_review": "I think the paper is a useful contribution, as it tackles a relatively underexplored problem with a clever and effective solution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4476/Reviewer_o19T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4476/Reviewer_o19T"
        ]
    }
]