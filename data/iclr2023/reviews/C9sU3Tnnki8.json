[
    {
        "id": "zExKWZBas3",
        "original": null,
        "number": 1,
        "cdate": 1666577355694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577355694,
        "tmdate": 1666577355694,
        "tddate": null,
        "forum": "C9sU3Tnnki8",
        "replyto": "C9sU3Tnnki8",
        "invitation": "ICLR.cc/2023/Conference/Paper2191/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper carefully designs a transformer as the backbone for treatment effect estimation, which is applicable to various tasks. They conduct many experiments to demonstrate the effectiveness of the proposed backbone and analyze the properties of the proposed transformer based on these results.",
            "strength_and_weaknesses": "Strength.\n1.\tThis paper verifies the effectiveness of the transformer for treatment effect estimation tasks, which is important for causality. And they conducted many experiments to demonstrate the effectiveness of the proposed transformer.\n2.\tThey analyze in detail the essential properties of the proposed method in causal: compatibility with propensity score modeling, parameter efficiency, robustness to continuous treatment value distribution shifts, and explainable in covariate adjustment.\n\nWeaknesses.\nThe proposed transformer lacks novelty in the methodology.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and high-quality, but slightly lacks novelty.\nThis is not the first transformer-based paper I've seen in the causal field.\n",
            "summary_of_the_review": "This paper conducts extensive experiments to verify the effectiveness of transformer. This paper demonstrates that transformer has many good properties in the causal domain. From the paper's perspective, the approach proposed in this paper lacks a certain degree of novelty.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2191/Reviewer_NSTZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2191/Reviewer_NSTZ"
        ]
    },
    {
        "id": "b9QyWIo0ono",
        "original": null,
        "number": 2,
        "cdate": 1666666097451,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666097451,
        "tmdate": 1666666685020,
        "tddate": null,
        "forum": "C9sU3Tnnki8",
        "replyto": "C9sU3Tnnki8",
        "invitation": "ICLR.cc/2023/Conference/Paper2191/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose to adopt Transformers as model backbones for treatment effect estimation. Comprehensive experiments are conducted to show the effectiveness of the proposed Transformer-based TEE methods over the existing MLP-based TEE methods. ",
            "strength_and_weaknesses": "Strength:\nThe studied problem is very important in practice and the paper is well-written. The paper is a comprehensive study that explores the effectiveness and efficiency of Transformers in TEE problems.\n\nWeaknesses:\n1.  The technical contribution of this paper is not very solid. \n\ta. The Transformers are well-studied and show superior performance over other neural models (MLPs, CNN or RNN) in many domains including the TEE (e.g., CETransformer and ANU). It is not very surprising that the proposed Transformer-based method can also outperform the baselines in TEE problems.\n\tb. The treatment prediction head is trained in an adversarial pattern to alleviate the treatment bias which is already been used in many existing works (e.g., Bica et al., 2020; Kallus 2020). \n\n2. For the covariate adjustment via cross-attention module, the learned attention weights tend to be very unstable and thus unreliable to be used for covariate selection and interpretability. As shown in Table 4, different variations of the proposed model yields different and inconsistent results in learned attention weights. The results will become more unstable in real-world scenarios with high dimensional covariate space and complex relationships among the covariates. \n\n3. The motivation behind using separated linear layers for treatment and dosage is to alleviate the issue of treatment distribution shift in the test (i.e., some treatment values in the test are not seen during training). However, whether this motivation and experiment design will violate the positivity assumption that the probability of receiving any treatment is non-zero? For example, in extrapolation (h=5), t\\in[0.25, 5.0] in training and t\\in[0, 5] in testing. Thus the individuals in the training set will have zero probability to receive t\\in[0, 0.25).  Then the problem becomes whether it is meaningful to study the treatment distribution shift problem in TEE under the positivity assumption.\n\n4. It would be great to incorporate the comparison (from conceptual-level and empirical analysis) with existing Transformer-based TEE methods in the main paper. These methods should be the most related baselines to compare with.\n\nBica, Ioana, et al. \"Estimating counterfactual treatment outcomes over time through adversarially balanced representations.\" ICLR 2020.\nKallus, Nathan. \"Deepmatch: Balancing deep covariate representations for causal inference using adversarial training.\" ICML 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is well-written and easy to follow.\n2. The paper quality is good with very comprehensive experiments.\n3. The technical novelty is not enough. Both Transformers and adversarial learning techniques have been used in the existing works.\n4. The code is provided in the supplementary material.",
            "summary_of_the_review": "The studied problem is very important in practice and the paper is well-written. It is a comprehensive paper, but not a technically novel one. There are also some technical questions in the experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2191/Reviewer_u5ZN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2191/Reviewer_u5ZN"
        ]
    },
    {
        "id": "gyfEwA9Fjw",
        "original": null,
        "number": 3,
        "cdate": 1666783883090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666783883090,
        "tmdate": 1666783883090,
        "tddate": null,
        "forum": "C9sU3Tnnki8",
        "replyto": "C9sU3Tnnki8",
        "invitation": "ICLR.cc/2023/Conference/Paper2191/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is interested in estimating the heterogeneous treatment effect (TEE). The authors argued that TEE is not used in practice because current approaches to tackling it make solid parametric assumptions.\n\nAccording to the authors, the limitations of previously proposed approaches mainly relate to their poor generalizability. Hence, they tend to focus on a narrower context and ignore other scenarios. \n\nThe method present in this paper is based on transformers and their backbone modules. Specifically, its main module is the attention layer. \n\nMy main concern in this paper is the assumption about unconfoundeness, which states that there are no hidden confounders. Since the authors are arguing against the applicability of previous works, this assumption impedes using their approach in real applications as well.",
            "strength_and_weaknesses": "Strengths:\n\n- The authors used recent advances in NN architectures (e.g., transformers) to design a new method to tackle the Treatment Effect Estimation problem. For instance, they propose different mappings for covariates and treatments, which avoid missing information about treatments given their lower dimensional representation.\n- The authors considered different modalities of treatments. Namely, treatment with binary/continuous dosages, structured treatments, and language data. The broad of distinct treatment types helps assess their method's applicability.\n\nWeaknesses:\n\n- The assumption related to unconfoundeness is a very limiting factor in the usage of the proposed method in real-world applications.\n- The authors provide extensive analysis using different datasets. However, the reproducibility of their results is difficult, given that they did not make the source code available.\n- The authors should clarify the T- and S-learner approaches to causal inference. Most readers are not familiar with these terms.\n\nMinor suggestions:\n\n- It seems to have a typo in Assumption 2: \" such that, i.e.\"  -> \"i.e.,\"",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThe paper is clear in its assumptions and does a good job of verifying its hypothesis.\n\nNovelty\n\nThe paper presents a novel approach to the problem of interest and highlights the differences compared to previous works.\n\nReproducibility\n\nOne may find it challenging to reproduce the results, given the authors did not publish their source code.",
            "summary_of_the_review": "This paper proposes to advance the literature by leveraging recent developments on NNs to tackle the Treatment Effect Estimation problem. They provide extensive experiments and detailed information about their proposed method. However, they argued that previous approaches have issues with real-world applicability. When they ignore the presence of confounders, this paper seems to suffer from the same applicability problem.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2191/Reviewer_mZnE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2191/Reviewer_mZnE"
        ]
    }
]