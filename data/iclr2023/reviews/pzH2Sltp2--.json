[
    {
        "id": "jZxzBZq2FE",
        "original": null,
        "number": 1,
        "cdate": 1666348048135,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666348048135,
        "tmdate": 1668775810273,
        "tddate": null,
        "forum": "pzH2Sltp2--",
        "replyto": "pzH2Sltp2--",
        "invitation": "ICLR.cc/2023/Conference/Paper3442/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new framework for molecular property prediction. The proposed framework consists of (1) a new edge attribute fusion-based graph neural network architecture and (2) three pretraining objectives. The newly proposed GNN architecture additionally uses line graph-based view of a molecule as an additional source of information. Furthermore, the contrastive learning objectives consist of (1) contrastive learning between graph-based and line graph-based molecular representations, (2) intra-contrastive learning between edge representations, and (3) inter-contrastive learning between edge representations. After combining all the components, the proposed method outperforms the existing pretraining frameworks for molecular property prediction.",
            "strength_and_weaknesses": "Strength 1: The idea of using line graph as an additional view for a molecule is novel for pretraining. \n\nStrength 2: The proposed method shows strong empirical performance.\n\nWeakness 1: Most importantly, the proposed method is combination of many ideas, but the current experiments are insufficient to demonstrate significance of such ideas. \n\nWeakness 1.1: In the ablation studies, simply combining line graph and contrastive learning achieves average performance of 71.16, which is worse than six of the considered baselines. \n\nWeakness 1.2: After combining edge attribute fusion, the proposed method generates property prediction using different neural network architecture compared to the baselines. If this is the case, the authors should consider the same architecture for the baselines too.\n\nWeakness 1.3: The proposed method is based on combination of three loss functions, while most of the baselines rely on one loss function. To measure the significance of these loss functions, the experiments should be constructed more carefully. For example, one could fix the budget for tuning the hyperparameters and try to combine the previously proposed pretraining objectives.\n\nWeakness 2: Table 2 does not have any \u201coverall\u201d statistics like ranking and average, so it is hard to interpret the table and make conclusion.  \n\nWeakness 3: I suggest comparing with 3D-infomax [1] and MGSSL [2] as a pretraining baseline. I also suggest comparing with works that simultaneously propose graph neural network (or Transformer) architectures with pretraining objective (GROVER [3], MolR [4]). Comparing with other architecture is important since the work also proposes a new architecture for property prediction. \n\nWeakness 4: I appreciate how the authors showed the candidate hyperparameters for the finetuning experiments. However, there is no detail on which metric is used to choose the hyperparameters. Furthermore, it is not clear whether if the budget used for tuning the pretraining hyperaparameter is the comparable across the baselines. \n\n[1] 3D Infomax improves GNNs for Molecular Property Prediction, ICML 2022\n[2] Motif-based Graph Self-Supervised Learning for Molecular Property Prediction, ICML 2022\n[3] Self-Supervised Graph Transformer on Large-Scale Molecular Data, NeurIPS 2020\n[4] Chemical-Reaction-Aware Molecule Representation Learning, ICLR 2022",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear to read and the proposed idea is novel with respect to the existing baselines. Reproducibility of this work will be high if the authors publish their code given acceptance (along with the protocols for selecting the hyperparameters). ",
            "summary_of_the_review": "This paper proposes an interesting idea for pretraining of molecular representations. However, I am concerned by the empirical evaluation which is not thorough enough to validate the significance of the proposed work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_3j8n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_3j8n"
        ]
    },
    {
        "id": "XY4Nz1QIr-",
        "original": null,
        "number": 2,
        "cdate": 1666371161348,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666371161348,
        "tmdate": 1667247195634,
        "tddate": null,
        "forum": "pzH2Sltp2--",
        "replyto": "pzH2Sltp2--",
        "invitation": "ICLR.cc/2023/Conference/Paper3442/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "**(Motivation:)** This paper points out the limitations of existing graph contrastive learning, which not only alters the semantic of original graphs during perturbation, but also leverages the domain-specific knowledge that might not be generalizable across different graph datasets. \n\n**(Method:)** To tackle those two problems, the authors propose to use the line graph of the original graph, wherein the edge of the original graph is transformed to the node of the line graph. In particular, the authors maximize the representation similarity of original and its line graph, while minimizing other negative original-line graph pairs, under the existing contrastive learning objective. Also, the authors propose to minimize the inconsistency between node/edge features of the original graph and edge/node features of the line graph, by interchanging their information. Furthermore, the authors propose two additional contrastive losses, which capture edge-level similarities within and between graphs. \n\n**(Experiment:)** The authors validate the proposed LGCL on the graph classification tasks, showing the propose model outperforms relevant baselines.",
            "strength_and_weaknesses": "### Strengths\n* The idea of using line graph transformation for graph contrastive learning, where the positive pair consists of the original and its line graph, is interesting and novel. \n* The problem of information inconsistency between original and line graphs is well justified, which is nicely solved with the proposed edge attribute fusion method. \n* The proposed method empirically outperforms other competitive baselines.\n* This paper is well-structured, and easy to follow.\n\n### Weaknesses\n* There is a recent work [1] that points out the limitation of contrastive learning: altering the semantics of graphs during perturbation, and this problem-level idea is the same as the authors suggest. Thus, this relevant work [1] should be discussed, and might be compared. \n* The tackled over-smoothing issue incurred by the line graph is unclear. Why the line graph additionally introduces the over-smoothing issue? And how this over-smoothing issue is tackled by the proposed edge-level contrastive losses in Section 4.3 and Section 4.4?\n* The experimental setup of Figure 3 is unclear. Does \"No Pre-Train w/LG\" denote the fine-tuning model that uses representations of both original and line graphs? Then, for the proposed method, is the READOUT function in equation (3) changed to include the line graph representation?\n\n---\n\n[1] Graph Self-supervised Learning with Accurate Discrepancy Learning, NeurIPS 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n* The tackled problems (i.e., existing contrastive learning methods alter the semantic structures of graphs, and also sometimes require the domain-specific knowledge) and proposed methods, based on the line graph-based contrastive learning with edge attribute fusion and local contrastive losses, are clearly described.\n* However, the clarity of the over-smoothing issue and the experimental setup of Figure 3 should be further improved (See Weaknesses for details). \n\n### Quality\n* The technical quality of this work is high. The authors define the under-explored challenges of the existing contrastive learning, and tackle them with the novel line graph-based contrastive learning.\n* The experimental results show that the proposed LGCL outperforms baselines, which further supports the technical quality of the proposed idea.\n\n### Novelty\n* To my knowledge, the main contrastive learning objective based on the similarity learning between original and line graphs is novel. \n* In addition to the main objective, the proposed additional components, such as edge attribute fusion scheme and local contrastive losses, are novel. \n\n### Reproducibility\n* The authors do not provide the source code that lowers the reproducibility of this paper; however, the authors plan to release the source code after the acceptance. Therefore, the reproducibility will be probably high.",
            "summary_of_the_review": "While there are some revision points: discussing more work and improving clarify of some sections, I believe they require minor revisions. Thus, given the high quality and novelty of this work with strong empirical results, I recommend the acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_xTh3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_xTh3"
        ]
    },
    {
        "id": "k8MSBBJHWb",
        "original": null,
        "number": 3,
        "cdate": 1666705813679,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666705813679,
        "tmdate": 1666705813679,
        "tddate": null,
        "forum": "pzH2Sltp2--",
        "replyto": "pzH2Sltp2--",
        "invitation": "ICLR.cc/2023/Conference/Paper3442/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a pre-training strategy for graph neural networks based on line graphs. The method is then benchmarks on several molecular property prediction tasks.",
            "strength_and_weaknesses": "Strengths:\n- The work approaches a relevant problem, how to pre-train molecular encoders\n- Clearly written and structured paper\n\nWeaknesses:\n- The lacks embedding into related works outside of this small community of graph pre-training\n- The claimed improvement is hardly relevant since even the most simplistic molecular descriptors (Morgan, ECFP) produce better representations\n- Only a single experiment has been performed\n- There are technical errors, such as the lack of error bars and confidence intervals, and unclear model and hyperparameter selection \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity, Quality, Novelty and Reproducibility\n\n### Clarity\nThe paper is clearly written and structured and easy to follow.\n\n\n### Quality\n\nA) The work is not well embedded into related works. It only looks into very closely related works on graph pre-training but not even a bit ahead to other methods for molecular property prediction or to contrastive learning. The authors should embed their work better into related works, and, e.g., check how simple methods and representations usually perform on Tox21[1] or the MoleculeNet benchmarks. \n\nB) The experimental part is limited and all methods operate at performances that are far below usual approaches to molecular property prediction. The authors only perform experiments for molecular property prediction on the so-called MoleculeNet benchmark. Since the method should generally work for graph pre-training, the authors should also perform experiments on graph that arise in a different domain (e.g. documents or social networks). Furthermore, the usual encoding or representation of molecular graphs are extended-connectivity fingerprints [2] that just encode subgraphs of different size. Using this simple and efficient representation, one can easily outperform the presented method using linear probing (see my table below). Thus, one can save all the computational costs that is used for pre-training the suggested encoder. \nThe authors should include baselines into their table (such as linear probing on frequently used molecular descriptors), reference usual performances on these datasets (e.g. in MoleculeNet), and suggest a pre-training method which at least matches the performance of molecular descriptors. \n\n|          type              |   method                                 | split        | BACE           | BBBP           | ClinTox        | HIV            | MUV            | SIDER          | Tox21          | ToxCast        |\n|----------------|--------------------------------|----------|------------|------------|------------|------------|------------|------------|------------|------------|\n| Linear probing | ECFP encoder-4096 (reviewer)                 | random   | 91.52\u00b10.0  | 91.25\u00b10.0  | 71.14\u00b10.0  | 80.36\u00b10.0  | 77.05\u00b10.0  | 64.61\u00b10.0  | 76.87\u00b10.0  | 69.43\u00b10.0  |\n\nC) Lack of error bars, confidence intervals and statistical test for many performance metrics. \nThe performance metrics in Figure 3 and Table 2 are provided withouth error bars or confidence intervals, such that differences could just arise by chance. The authors should perform repeated training runs, or perform cross-validation to obtain error bars. \n\nD) Unclear balancing strategies for the loss function and missing hyperparameter selection procedure.\nThe authors use a combination of three loss functions to pre-train their GNN. However, such combination is usually hard to balance since the losses can be at completely different scales or move to different scales during learning -- a major problem, e.g., in deep reinforcment learning. Suprisingly, in this work the balancing parameters $\\alpha$ and $\\beta$ could just be set to 1. Generally, it is unclear how the many hyperparameters that come with this architecture (scale parameters, learning rate schedules, GNN architecture, regularization, ...) have been selected. The authors should describe their hyperparameter selection strategy and their metric how to select good hyperparameters. \n\n\n### Novelty\nThe proposed strategy to pre-train graph neural networks appears novel; differences and similarities to other pre-training strategies, e.g. [3], should be described clearer. \n\n\n### Reproducibilty\nThe code has not been provided, such that this work is hardly reproducible. \n\n\n\n### References\n[1] Papers with Code, Tox21 (Drug Discovery), https://paperswithcode.com/sota/drug-discovery-on-tox21\n[2] Rogers, D., & Hahn, M. (2010). Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5), 742-754.\n[3] Fang, R., Wen, L., Kang, Z., & Liu, J. (2022). Structure-Preserving Graph Representation Learning. arXiv preprint arXiv:2209.00793.",
            "summary_of_the_review": "The proposed pre-training strategy is reasonably novel, altough not well embedded into related work. The results and conclusions are hardly relevant since the predictive quality is even below classical molecular descriptors. There are several technical errors in the experimental part. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_KhWt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_KhWt"
        ]
    },
    {
        "id": "5cSq0Ty63iv",
        "original": null,
        "number": 4,
        "cdate": 1666781316122,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666781316122,
        "tmdate": 1668736910049,
        "tddate": null,
        "forum": "pzH2Sltp2--",
        "replyto": "pzH2Sltp2--",
        "invitation": "ICLR.cc/2023/Conference/Paper3442/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper fouse on learning good representation for molecuars. The authors introudce line graph to graph contrastive learning and propose two contrastive loss (i.e., inter contrastive loss and intra contrastive loss) to retain molecular semantics. Experiment shows proposed LGCL ahieve SOTA performance on several datasets.",
            "strength_and_weaknesses": "**Strength**\nPaper is well writen and easy to follow the idea\n\n**Weakness**\nOverall, the idea of this paper is not novel and the method propsed by the authors is incremental. \n1. The idea of introducing the line graph of the molecuars to graph representation learning is trivial. It has been explorer in the various GNN papers. Although this paper might be the first one to apply the line graph to the contastive learning, the effort has been made is minor and not significant. It just apply line graph as a contrastive veiws.\n\n2. There is no evidence to show that the proposed intra-local contrastive loss can allivate the oversmooth problem, however, the author claim for that. Need either thertical analysis or empericial result to show this.\n\n3. Some reuslt is much worse than the baseline, can author give some explaination on that.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good, Easy to follow the paper.\nQuality: Poor.\nNovelty: not novel.\nReproducibility: Not sure.",
            "summary_of_the_review": "The author give a trival solution to the learning the represeatation for molecuars. The novelty of this paper is not enough for ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_TRA8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_TRA8"
        ]
    },
    {
        "id": "OM-tBqPSfd9",
        "original": null,
        "number": 5,
        "cdate": 1666804956554,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666804956554,
        "tmdate": 1666804956554,
        "tddate": null,
        "forum": "pzH2Sltp2--",
        "replyto": "pzH2Sltp2--",
        "invitation": "ICLR.cc/2023/Conference/Paper3442/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper claims that random or learnable graph augmentation methods may inevitably vary molecular semantics and make them inappropriate applied to molecular structure. The authors propose a novel approach to augment the original graph, i.e., converting it to a line graph. The edge attribute fusion tackles the inconsistent representation issue. Two following contrastive losses enhance representation learning.",
            "strength_and_weaknesses": "Strength:\n1. The paper is well-written and easy to follow. The storyline is clear, and the figures are intuitive.\n\n2. The authors are willing to share the code link after acceptance. The reproducibility of this paper is satisfactory.\n\n3. The experimental setting is very exhaustive. The authors describe almost all the interesting experimental details.\n\nWeakness:\n1. The experimental settings are kind of solid but not enough. The experimental section only includes overall performance comparison and ablation study in two different settings. The hyper-parameter sensitivity experiments are missing. And the authors report that the pretraining experiments of their model require 20 hours to converge. I am curious about the efficiency of their method compared with other methods.\n\n2. Some model designs are too simple to come up with. The idea behind the model is kind of interesting. But some model designs are too simple to come up with. For example, the GNN architecture and contrastive loss are similar to other works.\n\n3. The authors claim that edge attribute fusion is essential to fix the inconsistency problem between the representations from two views. But the experimental results shown in figure 3 demonstrate that the model performs worse on half of all the datasets with this attribute fusion.",
            "clarity,_quality,_novelty_and_reproducibility": "The reproducibility is good because of the promise of releasing original code and detailed experimental results.\nThe figures and writing are clear and easy to follow.\nThe quality of the paper is roughly satisfactory and exceeds the minimum threshold for top conferences. \nThe technical novelty is limited.",
            "summary_of_the_review": "The paper would benefit from several revisions before publication, including adding some experiments, improving the model design, and checking the necessity of the components carefully.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_awP4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3442/Reviewer_awP4"
        ]
    }
]