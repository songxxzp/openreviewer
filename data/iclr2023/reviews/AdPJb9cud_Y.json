[
    {
        "id": "1rRqq7Jw5a",
        "original": null,
        "number": 1,
        "cdate": 1666488040608,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666488040608,
        "tmdate": 1666488040608,
        "tddate": null,
        "forum": "AdPJb9cud_Y",
        "replyto": "AdPJb9cud_Y",
        "invitation": "ICLR.cc/2023/Conference/Paper2051/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a method for differentiable volume rendering using a set of Gaussian ellipsoid primitives. The key idea is to use a large set of Gaussian ellipsoids to parameterize the density and color within a volume and then to render using ray marching to solve the volume rendering equation. Conveniently, integrals along rays through the ellipsoids can be computed in closed form, leading to computationally efficient rendering. The method is demonstrated on a pose estimation task, where a ground truth mesh is aligned to a model, as well as for rendering textures, point clouds, and for shape fitting from multiview imagery. Overall, the method appears to outperform other techniques for the pose estimation task, and qualitative results suggest good performance for other tasks.\n",
            "strength_and_weaknesses": "The idea of using Gaussian ellipsoids in the context of volumetric rendering, while not new, is nevertheless interesting in the context of differentiable rendering because one can compute the rendering integrals analytically. The paper does a good job of conveying the efficiency of the approach, as even megapixel images can be rendered in real time for a scene with thousands of Gaussian ellipsoids.\n\nThe quantitative results also shows that the method performs well for the pose estimation task (which involves aligning ground truth geometry to an input image).\n\nStill, the paper has a few weaknesses: there are a large number of typos and grammatical errors that hinder the clarity of the paper, and there are several details lacking from the technical description and evaluation of the method that seem important.\n- There are no quantitative results that evaluate the texture extraction and rerendering, occlusion reasoning, and shape fitting via inverse rendering tasks.\n- It's not always clear how the method is initialized, which is one potential drawback. How many Gaussian ellipsoids are required in the experiments and how does the rendering quality depend on this? This seems like a critical parameter to evaluate.\n- Given the connection to volumetric rendering methods and the interest in multiview reconstruction, I was hoping to see more reconstruction results and how this compares to methods that use a neural network to parameterize the volume density and color. Is there an advantage here? Or does the requirement of initializing a constant number of ellipsoids a priori limit the performance?",
            "clarity,_quality,_novelty_and_reproducibility": "With respect to clarity, there are a large number of typos and grammatical errors that would need to be corrected. I list a few below:\n- Intro: \"Liu et al. Liu et al. (2019)\"\n- Related works: \"Blinn Blinn\" and other typos for the references. There are similar typos with the references throughout the paper that should be corrected.\n- Volume density aggregation: This sentence has many grammatical errors: \"However, computing the integral using brute force is so [computationally] inefficient that [it is] infeasible for [current devices]\"\n- Section 4.4 \"official tutorial pyt\"\n\nThere are a number of other technical questions I had, which I did not see addressed:\n- For the texture extraction and rerendering task is the CAD model already aligned to the image? If so, how does this compare to other simple baselines? For example, couldn't one simply map each vertex to a color and render using, e.g., rasterization and barycentric interpolation of the colors on the mesh faces? \n- When ray tracing how does one know which ellipsoids contribute to the ray? Does this require a hit test against all ellipsoids?\n- How are the ellipsoids initialized when doing shape reconstruction?\n\nAs for novelty, there is a long history of using \"Gaussian transfer functions\" for volume visualizations. Some of the references in this line of work appear in the paper. So, the main novelty here is the incorporation of Gaussian primitives into the differentiable rendering framework.   \n\nWith respect to reproducibility, I think there is no problem as the authors also promise to release the code.\n\nAdditional comment:\n- Fig. 2: indicate the row of the image for which the plot is given (e.g., with a dashed line across the image).",
            "summary_of_the_review": "Overall, while I think the idea of using Gaussian ellipsoids as a primitive for differentiable rendering is interesting and potentially very reasonable (as it builds on a long legacy of using Gaussian primitives for volume rendering), I have some reservations about the evaluation and lack of quantitative comparisons for 3/4 of the presented tasks. I find it difficult to get a sense of how well the method performs. There are also some technical details which appear to be missing from the paper. As it stands, I lean slightly negative on the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2051/Reviewer_qZYG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2051/Reviewer_qZYG"
        ]
    },
    {
        "id": "srV5kSIHQf",
        "original": null,
        "number": 2,
        "cdate": 1666657080305,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657080305,
        "tmdate": 1666657080305,
        "tddate": null,
        "forum": "AdPJb9cud_Y",
        "replyto": "AdPJb9cud_Y",
        "invitation": "ICLR.cc/2023/Conference/Paper2051/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel differentiable renderer. It represents the scene \nusing a set of Gaussian ellipsoids that determines the density of each point. \nDuring rendering, it traces each ray and accumulates the contribution of \neach ellipsoid using volume rendering. The paper demonstrates the effectiveness \nof the renderer on tasks such as pose estimation and shape fitting, and shows\nthat it achieves better performance that rasterization-based differentiable\nrenderer such as Pytorch3D.",
            "strength_and_weaknesses": "Strengths\n1. Compared to rasterization-based differentiable renderers, the proposed method \nutilizes volume density and can better reason about occlusions and\noverlapped components. \n\n2. Compared to implicit volumetric representation, the usage of Gaussian\nellipsoid is more intuitive and can be easily converted from other\nrepresentations such as meshes and point clouds.\n\n3. The paper derives an efficient approximation for the aggregation function  \nthat is differentiable with respect to both visible and invisible components.\n\n4. The paper shows that the proposed renderer achieves better performance than\nbaseline methods on multiple downstream tasks. \n\nWeakness\n1. When talking about implicit representations such as Mildenhall2020, the paper\nsays that it lacks modifiability and interpretability. It's not clear to me why \nthe proposed Gaussian ellipsoids representation can be better in these aspects,\nand it's worth adding more discussions here. Also recently there are new\nvolumetric representations such as Plenoxels and Direct Voxel Grid Optimization\nthat apply feature grids + MLPs. It will also be better to discuss these related works, \nand how the proposed method will be better. \n\n2. It's also worth discussing relations to physically-based differentiable\nrendering methods, such as \n* Differentiable Monte Carlo Ray Tracing through Edge Sampling\n* Differentiable Signed Distance Function Rendering\n* A Differential Theory of Radiative Transfer\n\n3. In the inverse rendering tasks, the produced results are blurry. What are the\nlimiting factors for the proposed method to generate sharper renderings? Can it\nbe improved by increasing the number of Gaussian ellipsoids?\nAlso, the paper says a silhouette is used. I am wondering whether it's really\nnecessary, considering that volume-based methods such as Mildenhall2020 do not\nrequire mask supervision. Similarly, it's also not clear to me why additional\nconstraints are needed.  \n\n4. In Figures 14 and 16, it seems that the method produced normal maps with\nobvious artifacts, especially on the Armadillo and the dragon scene. What are\nthe reasons for those artifacts?\n\n5. I think one advantage of the proposed method over rasterization-based\nmethod is that it's using volume rendering and therefore can handle translucent\nobjects. It will be interesting to add a comparison on tasks like fitting a\ntranslucent object.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is math-intensive, especially in the derivation of the aggregation \nfunction. I would suggest the authors add an illustration to the paper with \neach symbol, and also it will be better to start with a high-level idea of the \napproximation. It's difficult to follow in the current format.\n\n2. The quality and novelty of the paper look good to me. The combination of\n3D Gaussians and volume rendering is interesting and technically sound.\n\n3. The reproducibility remains a concern, considering that there are many\ncomponents in the paper with subtle details. It will be great if the authors can\nrelease the code.\n\n",
            "summary_of_the_review": "Overall, I believe that the combination of 3D Gaussian and volume renderings is \nnovel and technically sound. The paper shows the effectiveness of the proposed\nmethod on multiple tasks. Therefore, I hold a positive attitude toward the\npaper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2051/Reviewer_LqFK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2051/Reviewer_LqFK"
        ]
    },
    {
        "id": "F-fer8jqAL",
        "original": null,
        "number": 3,
        "cdate": 1666670354384,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670354384,
        "tmdate": 1666670354384,
        "tddate": null,
        "forum": "AdPJb9cud_Y",
        "replyto": "AdPJb9cud_Y",
        "invitation": "ICLR.cc/2023/Conference/Paper2051/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a differentiable rendering algoithm that converts 3D meshes into a collection of 3D gaussians and ray traces the resulting scene. The conversions to volumetric gaussians smoothes the visibility function of the scene, similar to the smooth visibility and blending functions of soft differentiable rasterizers (SoftRasterizer, PyTorch3D).\n\nThe paper describes how to project a set of 3D gaussians along a 1D camera ray, and how to solve the resulting volumetric rendering integral in closed form. \n\nTwo approximations are mentioned: One sub-samples the geometry by only rendering 10% of all gaussians in a coarse-to-fine approach. The other optimization clusters nearby gaussians, which further reduces the number of 3D density samples.\n\nThe paper describes how arbitrary attributes can be associated with the 3D gaussians, which allows for the rendering of RGB textures or neural features.\n\nThe paper presents several experiments, including pose estimation of CAD shapes, texturing, shape fitting, and a synthetic occlusion experiment.\n",
            "strength_and_weaknesses": "With 3D gaussians, the paper introduces an interesting representation for differentiable volumetric rendering. The algorithm for rendering a set of gaussians is sound and the experiments demonstrate strong results on the pose estimation task and plausible behavior on the inverse rendering experiments.\n\n---\n\nThe paper is unfortunately quite difficult to follow. The formatting suggests that not much time was spent on proof-reading the paper, as the citation format expected by the authors is inconsistent with the ICLR format. Some sections are tiring to read from the frequent duplication of author names in the citations (see Section 2, first paragraph).\n\nThe authors use non-standard terminology for common concepts, which does not help the readability. For example, in Table 1, it would be better to use conventional graphics terms like these:\n\n - \"Component\" -> \"Geometric primitive\"\n - \"Component Tracking\" -> \"Visibility algorithm\"\n - \"Aggregation\" -> \"Blending\"\n\nCertain statements are confusing and likely incorrect, for example the statement that \"Graphics renderers use explicit object representations, which represent objects as a set of isotropic components.\"  (Sec. 2) (What components are referred to? What is \"isotropic\" about conventional graphics rendering?)\n\n---\n\nWhile the paper is quite clear in how a set of gaussians is rendered, the way in which shapes are converted to sets of gaussians is not sufficiently discussed. There is one paragraph on the bottom of page 4 that mentions how to pick the covariance matrix, but it is not mentioned in the main text how the number or position of the gaussians is chosen. From the appendix, it becomes clear that there is a 1-to-1 mapping with variance based on the neighboring edge length. \n\nThere are potential issues with this approach, in that the softness of the rendering (both spatially and along the ray) will depend on the mesh tesselation. Coarse meshes will be very soft, while finely tesselated meshes will converge to point-based splatting. The paper should include an analysis of this effect with an ablation and/or failure cases.\n\n---\n\nThe mentioned coarse-to-fine approximations are not well motivated and their impact not thoroughly evaluated.\n\n---\n\nSection 4.2 / Figure 7:\n\nIf the CAD model and pose is provided, this result is expected of any renderer that supports RGB textures. This result does not require a differentiable renderer, as each vertex (or gaussian) can simply be projected into the image, where its RGB value can be looked up without back propagation or optimization. It is not clear why training or symmetry information (Figure caption) would help.\n\n---\n\nFigure 8: How many gaussians are rendered for each cuboid? It appears that there are more than 8 gaussians, but the appendix mentions a 1-to-1 vertex to gaussian mapping. Can each gaussian vary its position independently, or are the gaussian positions constrained by the cuboid shape?\n",
            "clarity,_quality,_novelty_and_reproducibility": "There are issues with the clarity of the writing, see my comments in the previous section. \n\nA few additional references should be added,\n\n\"Learning Deformable Tetrahedral Meshes for 3D Reconstruction\"\nNeurIPS 2020\n\n\"Differentiable Monte Carlo ray tracing through edge sampling\"\nACM Transactions on Graphics 2 TOG 2018\n\n\"Mitsuba 2: A Retargetable Forward and Inverse Renderer\"\nSIGGRAPH Asia 2019\n\n\"Modular Primitives for High-Performance Differentiable Rendering\"\nACM Transactions on Graphics 2020\n",
            "summary_of_the_review": "The closed form solution for rendering a set of gaussians is a worthwhile contribution, but overall the exposition in the paper is quite difficult to follow. Experiment 4.1 appears to be a strong result, but Experiment 4.2 is in my opinion not very relevant. Experiments 4.3 and 4.4 are quite basic and would benefit from a more thorough discussion of how mesh conversion and coarse-to-fine rendering is handled.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2051/Reviewer_cWiy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2051/Reviewer_cWiy"
        ]
    },
    {
        "id": "WLt-jeWKdeh",
        "original": null,
        "number": 4,
        "cdate": 1667287746791,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667287746791,
        "tmdate": 1667287746791,
        "tddate": null,
        "forum": "AdPJb9cud_Y",
        "replyto": "AdPJb9cud_Y",
        "invitation": "ICLR.cc/2023/Conference/Paper2051/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a new differentiable volumetric rendering method based on ray casting through a discrete number of Gaussian shaped occlusion density functions. In practice, assumptions are made to simplify the volumetric rendering equation in terms of peak/max densities in order to reduce the rendering equation to something that can be computed efficiently in closed form. The authors show competitive speed to PyTorch3D, with applications to pose estimation, novel view synthesis, and inverse rendering.",
            "strength_and_weaknesses": "Strengths: The idea is good and the background is well established and covered. As a differentiable rendering method, the idea to decompose the scene into \"ellipsoidal blobs\" allows much better occlusion/de-occlusion reasoning for the applications covered in the paper, and therefore, better performance than image space differentiable rendering techniques.\n\nWeaknesses: The paper is fairly confusing, and the exposition needs to be reworked. I had trouble understanding it during the first read-through, in part due to Equation references that do not appear in the main text (is the main paper referencing equations from Appendices?). There is also a lot of typographical errors, including simple citation errors that could have been easily spotted and fixed with a single read-through. This tells me that, despite its length and the richness of the idea, this paper was surely rushed. \n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: I think this is good work, and does contribute a nice idea to the field of differentiable/inverse rendering. I do take small issues with some of the results and figures. For example, I don't find Figure 5 very illuminating without also comparing rendering fidelity. That is, I don't care if one method is faster than the other if has worse fidelity and vice versa. Also, I am a little curious about the novel view synthesis results of Figure 7-- without training, the network cannot predict any new information-- so is this result simply due to the bilateral symmetry of a car and ellipsoidal coloring that can be rotated geometrically? In general, I would want more explanation given for this surprising result.\n\nClarity: The clarity, at least for me (and I do research in this are), was very bad. Yet, I believe there is a much more straightforward exposition possible, I just don't think the authors took the time to structure their exposition for comprehension. I wish the paper was better written because I really do like the idea and execution, but I don't think it is publication-ready at the current time. For example, I would put the method for turning a mesh into Gaussians in the main text, and I would also explain how the K nearest ellipsoids are selected for ray tracing (as well as compare compute times & fidelity with and without this technique). Other time I think a simple proofread could have helped clarity-- for example, in Section 3.4, the word \"donates\" is used several times, but I think the word should have been \"denotes\"? \n\nOriginality: I think the idea is original and I think the author's well-covered earlier work that has similar ideas. I also appreciate Table 1 for contextualizing the method.",
            "summary_of_the_review": "I like this work but do not like this paper. I think the work should be published and would be of considerate interest to the community. However, I think the paper is not well-organized and the ideas are not well-presented. Part of this problem is due to simple typographical issues, but part of this problem is due to hard to understand and haphazard exposition.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2051/Reviewer_ppJY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2051/Reviewer_ppJY"
        ]
    }
]