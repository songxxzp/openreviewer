[
    {
        "id": "LkLrORxkD6F",
        "original": null,
        "number": 1,
        "cdate": 1666370852765,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666370852765,
        "tmdate": 1666370852765,
        "tddate": null,
        "forum": "QHevLM-OnA",
        "replyto": "QHevLM-OnA",
        "invitation": "ICLR.cc/2023/Conference/Paper4488/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims at unifying Bayesian inference, cooperative communication, and discriminative learning in a general learning framework, called Generalized Belief Transport (GBT). The cornerstone of this unifying framework lies in the paradigm of entropy-regularized unbalanced optimal transport defined using three scaling parameters, from which classical learning models can be derived using special points. The paper examines both the batch (Section 3) and the sequential (Section 4) versions of GBT, where the latter opens the door to several questions related to the convergence of online GBT for several scaling points.\n",
            "strength_and_weaknesses": "Overall, this general framework is conveying some interesting results for the study of learning agents, but it is quite difficult to assess their significance due to clarity issues and the absence of a detailed related work section (see comment below). \n",
            "clarity,_quality,_novelty_and_reproducibility": "First of all, a detailed related work section should be included just after the introduction. For the moment, we just have a short paragraph (Section 5) that briefly mentions existing studies without relating them to the present framework. Notably, several results have already been found in the balanced case (Wang et. al., 2020). \n\nFurthermore, the (entropy-regularized) Unbalanced Optimal Transport problem was studied in several papers, and Pham et. al. (2020) have established a quadratic bound on the runtime complexity of the Sinkhorn algorithm. But in their transport plan (Eq. 2 in Pham et. al., 2020), the last two parameters are equal (i.e. $\\epsilon_n = \\epsilon_{\\theta}$). Contrastingly, in the present study, the transport plan (1) is more general. So, the results in (Pham et. al., 2020) cannot be directly applied here, and hence, some generalized convergence analysis of the Sinkhorn algorithm should be provided here.\n\nBeyond those positioning issues, the paper has some clarity issues. In Section 2, the probability distributions $\\eta$ and $\\theta$ are sometimes fixed (Section 2.1), but other times they can vary in a sequential way. To this point, I would suggest first examining the GBT framework in the batch case, where  $\\eta$ and $\\theta$ are fixed and Eq (1) is analyzed for several scaling points. Then, this framework should be generalized to the sequential case by presenting the GBT algorithm (Algorithm 1) in Section 4, by clarifying the updates for $\\theta_k$ and $\\eta_k$ (note, that the update for $\\eta_k$ is missing in Algorithm 1).\n",
            "summary_of_the_review": "Arguably, unifying Bayesian inference, communication inference, and discriminative learning is very challenging.\nThis paper aims at paving the way in this direction but, as indicated above, it is quite difficult to assess the significance of the results, \ndue to clarity issues and the lack of a detailed related work section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4488/Reviewer_KwJp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4488/Reviewer_KwJp"
        ]
    },
    {
        "id": "qc9okOnr3U",
        "original": null,
        "number": 2,
        "cdate": 1666474824285,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666474824285,
        "tmdate": 1666474824285,
        "tddate": null,
        "forum": "QHevLM-OnA",
        "replyto": "QHevLM-OnA",
        "invitation": "ICLR.cc/2023/Conference/Paper4488/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed a framework that can align a set of hypotheses models with a set of datasets. Specifically, the hypotheses and datasets are viewed as empirical distributions and certain loss functions can serve as the transport cost of moving one hypothesis to a dataset. Thus, this alignment can be reviewed as an unbalanced optimal transport problem. In the proposed formulation, the coefficients that control the KL for regularizing the marginals have a new interpretation. The authors showed that the whole learning framework is interpolating between several learning paradigms, including Bayesian inference, frequentist estimation, cooperative communication and discriminative learning. The authors also provided a detailed theoretical analysis to justify the consistency properties.",
            "strength_and_weaknesses": "Pro:\n- First of all, this paper is well-written. The relationship between different learning perspectives is introduced comprehensively. \n- Overall the idea of formulating such a learning problem as unbalanced optimal transport is quite novel based on some existing works connecting OT with cooperative communication.\n- The authors introduced their theoretical results quite rigorously. \n\nCons and questions:\n- In the algorithm 1, where is the $\\eta$ updated? What is the data sampler $\\tau$? In addition, in each iteration there will be new data samples obtained from the sampler, will that lead to a different cost matrix $C$ in each iteration?  \n- Maybe one limitation of this work is that, I can see its potential in unifying several ML perspectives in an elegant way however it could be great if the authors could specify some applications of the belief. It seems to me that this framework maybe is related to autoML or model selection and the authors might consider some tasks on real-world datasets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The claims and results in this paper are sound. The theoretical analysis is pretty comprehensive. ",
            "summary_of_the_review": "From my perspective, this draft is well presented and the proposed idea shares a new perspective on connecting several classical ML perspectives. Thus, I lean toward the acceptance of this work. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethic concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4488/Reviewer_TGqt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4488/Reviewer_TGqt"
        ]
    },
    {
        "id": "t5IVS2bUW8",
        "original": null,
        "number": 3,
        "cdate": 1666639436085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639436085,
        "tmdate": 1669409712415,
        "tddate": null,
        "forum": "QHevLM-OnA",
        "replyto": "QHevLM-OnA",
        "invitation": "ICLR.cc/2023/Conference/Paper4488/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a unified theoretical framework, called generalized belief transport (GBT), that generalizes subclasses of machine learning algorithms including Bayesian inference, cooperative communication, and classification. The authors visualize the parameter space of learning models encoded by GBT as a cube, with the aforementioned subclasses of classic learning algorithms as particular points. Finally, the authors investigate online learning in GBT framework, where the learner\u2019s marginal on the data is not fixed a priori, but accumulates evidence based on experience. Asymptotic convergence properties are provided.",
            "strength_and_weaknesses": "This paper provides a unified theoretical framework that generalizes several classic machine learning algorithms. This framework, called GBT, provides a novel perspective of these classic learning algorithms using the theory of optimal transport. The cubic view of the parameter space of GBT is intuitive, and how classic algorithms lie on this cubic space as particular points is well explained.\n\nOverall, I found this new framework intriguing. However, it is unclear how existing methods could benefit from such an alternative perspective. Does GBT provide improvement in the classic learning settings? Does GBT permit us to learn in settings that were challenging before? It would be appreciated if the authors could provide examples to show how this novel framework permits us to improve over current baselines.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and well organized. I have not checked the details of the proofs, but the derivations seem reasonable. The conclusion that classic learning algorithms as special points on the cubic space of GBT models seem intuitive. As far as I am aware, the GBT formulation of machine learning methods is novel. Detailed proofs are provided in the appendix.",
            "summary_of_the_review": "This paper proposes a unified theoretical framework, called generalized belief transport (GBT), that generalizes subclasses of machine learning algorithms including Bayesian inference, cooperative communication, and classification. This allows one to visualize the parameter space of learning models encoded by GBT as a cube, with the subclasses of classic learning algorithms as special points. It would be appreciated if the authors could demonstrate how the GBT framework leads to new algorithms that improve over current baselines. However, I find this theoretical framework novel and could have an impact across fields of machine learning.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4488/Reviewer_pJWT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4488/Reviewer_pJWT"
        ]
    },
    {
        "id": "QRcA9ILsZK",
        "original": null,
        "number": 4,
        "cdate": 1666873459875,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666873459875,
        "tmdate": 1670653307521,
        "tddate": null,
        "forum": "QHevLM-OnA",
        "replyto": "QHevLM-OnA",
        "invitation": "ICLR.cc/2023/Conference/Paper4488/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": " This paper introduces a new unified formalism where Bayesian\n inference, Discriminate learning, Cooperative learning, and Maximum\n Likelihood Estimation are all expressible using a single algorithm\n inspired by the Sinkhorn algorithm from the optimal transport literature.\n\n The algorithm's generality is expressed with a cube where one could in\n principle express not just any of the above algorithms but linear combinations\n of them.",
            "strength_and_weaknesses": " The formalism introduced in the paper is novel, but I'm not particularly sure\n that it is all that useful. While I can see how exploring a Bayesian inference where\n the prior is made progressively more or less informative has some value, I don't\n see what benefits come from some of the other mixtures.\n\n I found the paper very difficult to follow. Large amounts of time is spent on\n introducing the work with fairly basic ideas being defined that likely reader\n would already be familiar with. I also am unsure some of the terminology is being\n used precisely enough. I don't know what it means to do Frequentist inference. From\n context it seems to be maximum likelihood estimation. Bayesian inference seems\n to be restricted to discrete space of parameters. None of these limitations\n are explicitly mentioned, I just don't see how the algorithm presented could\n generalize otherwise. It would be very helpful if the authors clarified precisely\n which methods their formalism can represent instead of just saying \"Bayesian inference\"\n and \"Discriminate Learning\".\n\nThe author's response did help clarify some of what these interpolations could mean, but I\nremain unsure all pairs of interpolations within the cube yields an explainable learning algorithm. \n",
            "clarity,_quality,_novelty_and_reproducibility": "While the contribution is novel, I'm not sure it's particularly significant. As mentioned in\nthe strengths and weaknesses section, the clarity of the work could be greatly improved.\nSince the work has stated it's mostly theoretical and the experiment in the paper seems\nstraightforward, I don't have reproducibility concerns. \n",
            "summary_of_the_review": " This is an original idea that would greater benefit from additional clarity and better\n exposition of the value of the presented formalism.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4488/Reviewer_wkW3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4488/Reviewer_wkW3"
        ]
    }
]