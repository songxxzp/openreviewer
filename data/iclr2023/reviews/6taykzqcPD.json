[
    {
        "id": "Fhn7K_gcJh6",
        "original": null,
        "number": 1,
        "cdate": 1666594386211,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594386211,
        "tmdate": 1666594386211,
        "tddate": null,
        "forum": "6taykzqcPD",
        "replyto": "6taykzqcPD",
        "invitation": "ICLR.cc/2023/Conference/Paper3258/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies layer-wise training by SGD for two-layer neural networks under teacher-student setups. Specifically, for the multiple-index models as teachers, the paper shows the convergence to the principal subspace spanned by the true parameters of the teacher. Moreover, several applications of this result are provided: (i) the learnability of the single-index model, (ii) an efficient generalization error bound, and (iii) compressibility guarantees.",
            "strength_and_weaknesses": "**Strengths**:\n\nThe feature learning property of neural networks trained with SGD has become a challenging research interest. Recently, many studies have been devoted to this line of research, but there are still several limitations. In this sense, the paper certainly contributes to the context by studying the SGD with multiple steps.\n\n- This paper provides an efficient sample complexity linear in the input dimension for learning the single-index model, which improves the existing results in related studies which treat one-step gradient descent. This improvement would be nice.\n\n- Moreover, the connection with an efficient generalization bound (Theorem 5) and the compressibility guarantee (Theorem 6) are also interesting, although these results are straightforward applications of Theorem 3.\n\n**Weaknesses**:\n\n- For the case $k>1$, the theory (Theorem 3) does not guarantee the convergence to the true parameter. In other words, the optimization accuracy is unclear in general, and thus the theory cannot estimate how small the expected risk can be.\n- A learnable class (i.e., $k=1$ and $f$ is monotone) is still rather limited compared to [Abbe et al. (2022)] which shows the learnability of staircase functions.\n\nHowever, I also acknowledge the difficulty of overcoming these limitations.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-organized and easy to read, and there is a certain improvement over the existing results.\n\nMinor comment:\n\n- The definitions of notations $R(W), R_\\lambda(W)$ omitting $a, b$ are missing, although we can guess them.\n\nQuestion:\n\n- The right-hand side of Eq. (3.2), (3.3) can be arbitrarily small as $\\gamma \\to \\infty$. Is there any condition on $\\gamma$ missed in this statement?",
            "summary_of_the_review": "Although there are still several limitations in the theory, this paper certainly contributes to the context by providing an improvement over the existing studies.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3258/Reviewer_qmRw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3258/Reviewer_qmRw"
        ]
    },
    {
        "id": "kIEeMDh9q_",
        "original": null,
        "number": 2,
        "cdate": 1666667002353,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667002353,
        "tmdate": 1666667002353,
        "tddate": null,
        "forum": "6taykzqcPD",
        "replyto": "6taykzqcPD",
        "invitation": "ICLR.cc/2023/Conference/Paper3258/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper  proves  that SGD on neural networks can learn low-dimensional features in certain settings, and uses this to derive novel generalization and excess risk bounds.",
            "strength_and_weaknesses": "\nThis paper study the convergence of SGD (applied  to two layer networks)  to the principle subspaces of the teacher model.\nStrength:\nStudying the convergence of SGD sequence to the estimation target (not only to the minimizer at the optimization level) is an important perspective to understand SGD in deep learning.\nSince the the target model is a multi-index model which is easier to due to the low dimensional intrinsics structure. So it is not surprise the final   results   reducing  the curse of dimensionality.\n\nWeakness:  In the perspective of nonparametric regression, we also need bound the estimation error, i.e., the estimated network function and \ntarget function g.  But the authors never mentioned  about this.   See for example  \n1:  I. Kuzborskij and Cs. Szepesv\u00e1ri. Nonparametric regression with shallow overparameterized neural networks trained by gd with early stopping. COLT, 2021.\n2:  T. Hu, W. Wang, C. Lin, and G. Cheng. Regularization matters: A nonparametric perspective\non overparametrized neural network. AISTATS, 2021.\n3: S Frei, NS Chatterji, PL Bartlett\uff0c Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data\uff0c COLT,  2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well orgnized.",
            "summary_of_the_review": "A nice work. And it can be better if  the author will study the nonparametric estimation error. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3258/Reviewer_vyfX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3258/Reviewer_vyfX"
        ]
    },
    {
        "id": "8KC7TvKk3n",
        "original": null,
        "number": 3,
        "cdate": 1667663305779,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667663305779,
        "tmdate": 1670345570369,
        "tddate": null,
        "forum": "6taykzqcPD",
        "replyto": "6taykzqcPD",
        "invitation": "ICLR.cc/2023/Conference/Paper3258/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the representation learning ability of two layer neural\nnetworks trained with gradient descent to approximate the outputs of a function\nin a nonparametric class, applied to a few projections of a high-dimensional\ninput vector -- in this setting, kernel methods and other rotation-invariant\npredictors generally suffer from a curse of dimensionality, but methods such as\nneural networks can potentially avoid the curse of dimensionality by learning\nthe relevant directions and then fitting the target in this lower-dimensional\nspace. They consider the setting of gaussian inputs, standard (mean-field-type)\nrandom initialization, and training with variants of stochastic gradient\ndescent on the square loss and smooth activations, and on convex Lipschitz losses\nand the ReLU, with large weight decay regularization. Their main technical result\nconsists of showing that online SGD (samples from the population loss) on the\nfirst layer weights leads to fairly rapid convergence to weights that are\nsuitably close to the span of the relevant $k$ directions of the teacher model\n-- the rates for the norm of the irrelevant components of the trained weights\nare on the order $\\sqrt{dm/T}$, where $d$ is the ambient dimension, $m$ is the\nnumber of neurons, and $T$ is the number of iterations of SGD, which is\nsufficient to bound the risk given that the losses are locally Lipschitz. The\nauthors apply this result to obtain a guarantee for learning noisy single-index\nteacher models (single neuron) up to the noise level with a two-stage procedure\n(after SGD on the first-layer weights to learn the right representation for the\ninput, fit the readout layer weights with essentially ridge regression), and\ngeneralization gap control (no empirical risk control) in more general settings\ndue to the low-dimensional representation learned. \n\n",
            "strength_and_weaknesses": "## Strengths\n\n- The paper is very well written. The coverage of background material and\n  related work seems essentially exhaustive. The notation and technical\n  discussions are precise and easy to parse at multiple levels of detail. The\n  authors provide interesting intuitions and connections to classical work in\n  statistics (e.g.  model misspecification) that enhances the level of insight\n  of the presentation.\n\n- The results concern the important problem of representation learning in\n  nonlinear neural networks. The authors seem to have taken some care to\n  present their results in some generality (different losses and activations,\n  step size regimes for SGD, etc.). The conclusions for learning the first\n  layer weights are qualitatively interesting.\n\n## Weaknesses\n\n- The algorithmic results for neural network training + generalization are\n  relatively weaker than the results for recovery of the first layer weights --\n  just concerning single index models or the generalization gap.\n\n- It would be helpful for the sake of comparison if the theorems did not hide\n  properties of the link $g$ -- especially to compare with the results of\n  Damian et al., which expose specific rates when the target is a polynomial\n  with a certain neural-net-like structure.\n\n## Minor / Questions / Etc.\n\n- The role of the noise $\\epsilon$ in the link function does not seem clear in\n  the theorems in the main body, prior to section 4 -- what dependence does it\n  play in the results?  Can it be zero? In section 4, why is this noise\n  necessary? (It would be helpful in understanding this if Theorem 4 exposed\n  the dependence on the subgaussian rate of the noise $\\sigma^2$, say.)\n\n- Some discussion of why the risk needs to be truncated at level $\\tau$ in\n  section 4 would be helpful.\n\n- With regards to the similarity to previous works, some discussion in the\n  rebuttal might be helpful to understand the distinct insights in this work\n  (especially, on the technical side, the differences that the authors allude\n  to in the related work section between analysis of the SGD trajectory, as\n  here, with the one-step gradient trajectory, given that both are unnatural\n  two-stage (\"layerwise training\") algorithms).\n\n- The claim at the start of section 4.1, suggesting that a key aspect of the\n  neural network training/generalization problem revolves around coping with\n  model misspecification (positing the existence of such an underlying model),\n  is interesting and thought-provoking. Another line of theoretical work on\n  neural nets focuses on the ability of neural networks to learn to adapt to\n  specific low-dimensional structures in data itself -- e.g. in terms of\n  representation capacity [1], or algorithmic aspects [2-5]. Adapting to this\n  type of structure, and the analysis it entails, seems somewhat different from\n  what the authors study here, and I would be curious to hear the thoughts of\n  the authors on whether there are similarities, whether they are in tension,\n  etc.\n\n[1] http://dx.doi.org/10.1093/imaiai/iaac001\n\n[2] https://proceedings.neurips.cc/paper/2020/file/a9df2255ad642b923d95503b9a7958d8-Paper.pdf\n\n[3] https://openreview.net/forum?id=O-6Pm_d_Q-\n\n[4] https://papers.nips.cc/paper/2021/hash/f26df67e8110ee2b44923db775e3e47f-Abstract.html\n\n[5] http://arxiv.org/abs/2206.12314\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is excellently written. The paper has a similar motivation, and\napplies similar technical tools, to other concurrent works in this area --\nthere are slight differences in the setting considered here that the authors\nmay unpack in more detail in the rebuttal (the differences mentioned in the\npaper, that of SGD vs one-step gradient, seem like they might be superficial).\n\n",
            "summary_of_the_review": "Similar to other quasi-concurrent works, this work presents an interesting\nmathematical study of a single level of representation learning in nonlinear\nneural networks trained in function approximation-type tasks with imperfectly\nspecified models (beyond the typical teacher-student). The results here are\nsimilar to those in concurrent works, but the excellent presentation makes this\nwork a valuable entry point to this evolving topic in the literature.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3258/Reviewer_k8J3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3258/Reviewer_k8J3"
        ]
    }
]