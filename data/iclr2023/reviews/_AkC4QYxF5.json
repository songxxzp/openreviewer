[
    {
        "id": "4H84aqCXQK",
        "original": null,
        "number": 1,
        "cdate": 1665870784563,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665870784563,
        "tmdate": 1665870784563,
        "tddate": null,
        "forum": "_AkC4QYxF5",
        "replyto": "_AkC4QYxF5",
        "invitation": "ICLR.cc/2023/Conference/Paper604/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops a reanalysis of TD with SVRG variance reduction and establishes a tight bound that matches the complexity result of SVRG in convex optimization. The developed analysis is simpler than all previous research and leads to better convergence bounds. Moreover, numerical experiments show the advantage of this algorithm over other existing variance-reduced TD algorithms.",
            "strength_and_weaknesses": "Strength:\n\n-The presentation is clear and easy to follow\n\n-This paper is the first to prove that TD with SVRG can match the complexity result of SVRG in convex optimization.\n\nWeakness:\n\n-Paper writing is poor. There are many grammar issues, incomplete sentences, etc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors developed the key lemma 1, which allows them to establish an analysis analogous to the original analysis of SVRG in the convex setting. This simpler analysis is based on introducing the functions $f,\\omega$, and provides a better understanding of TD under variance reduction. I suggest the authors clarify the novelty of this lemma, i.e., how does this lemma differ from the existing works that derive similar type of bounds? ",
            "summary_of_the_review": "See above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper604/Reviewer_kaRJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper604/Reviewer_kaRJ"
        ]
    },
    {
        "id": "0LJ76Op0VhG",
        "original": null,
        "number": 2,
        "cdate": 1666290526140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666290526140,
        "tmdate": 1666290526140,
        "tddate": null,
        "forum": "_AkC4QYxF5",
        "replyto": "_AkC4QYxF5",
        "invitation": "ICLR.cc/2023/Conference/Paper604/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a theoretical convergence proof for TD-learning algorithm with variance reduction structure. The sample complexity can match the complexity of SGD for convex optimization in iid finite-sample and iid online sampling senario. The empirical study furthor verifies the performance improvement from the variance reduction technique.",
            "strength_and_weaknesses": "***Strength***\nThis paper improves the sample complexity of an existing convergence result of the VRTD algorithm.\n\n***Weaknesses***\n1. In RL, online learning cannot have iid sampling. It can be a good starting point but the author should consider the Markovian sampling structure as [Xu2019].\n\n[Xu2019] Xu T, Wang Z, Zhou Y, Liang Y. Reanalysis of Variance Reduced Temporal Difference Learning. In International Conference on Learning Representations 2019 Sep 25.\n\n2. The theoretical result only obtains a constant-level complexity improvement. I don't think it fills any gap unless the theoretical lower bound is also provided; TD-learning is not equivalent to SGD in convex optimization. Also, there exists theoretical results showing that variance-reduction technique can make a variant of TD-learning algorithm achieve better complexity than $\\mathcal{O}(\\epsilon^{-1}\\log \\epsilon^{-1})$, see [Ma2020].\n\n[Ma2020] Ma S, Zhou Y, Zou S. Variance-reduced off-policy TDC learning: Non-asymptotic convergence analysis. Advances in Neural Information Processing Systems. 2020;33:14796-806.\n\n3. The gradient splitting method seems unrelated to this paper, even if it is put in the title. To my understanding, it doesn't bring any high-level perspectives to make the reader have a new understanding on TD-learning algorithm; it is just a trick used to derive the bound. Every optimization paper can have a lot of such tricks.   \n ",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of writting should be further improved. \n\nThe clarity is poor. Many necessary concepts are not introduced in this paper. How are the finite-sample and iid online sampling senario related to MDP? How do you split the gradient and what is the gradient for TD-learning algorithm? \n\nNo originality. Applying the variance reduction technique to TD-learning is not new. The proof technique is not new neither. The sample complexity improvement is not satisfactory (unless the theoretical lower bound is provided).  ",
            "summary_of_the_review": "In summary, I will reject this paper because: (1) both algorithm and proof technique are not new; (2) the problem setting is not possible in RL.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper604/Reviewer_Tp1f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper604/Reviewer_Tp1f"
        ]
    },
    {
        "id": "PDMAp1AjJK",
        "original": null,
        "number": 3,
        "cdate": 1666690244333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690244333,
        "tmdate": 1670217827259,
        "tddate": null,
        "forum": "_AkC4QYxF5",
        "replyto": "_AkC4QYxF5",
        "invitation": "ICLR.cc/2023/Conference/Paper604/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the Temporal difference (TD) learning method for policy evaluation in reinforcement learning. The proposed approach TD-SVRG method is a variant of the TD method by introducing the well-known SVRG technique. Theoretically, their analysis can lead to better convergence bounds for previous methods. Numerical results validate the improved performance of the proposed method over the existing methods.",
            "strength_and_weaknesses": "Strengths:\n\n1: This paper proposes a variant of the TD method with a faster convergence by introducing the well-known SVRG technique.\n\n2: Extensive experiments are conducted to verify the effectiveness and efficiency of the proposed algorithm.\n\n\nWeaknesses:\n\n1: It is unclear whether the proposed method significantly improves the existing complexity in VRDT. For the overall complexity, which term will dominate the maximization function in theory or in practice? Since the superiority of the proposed method to the VRDT method depends on this, the authors would do well to provide more analysis. ",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: This paper applied the well-known SVRG technique to the TD method to achieve faster convergence. It is new but seems not very interesting.\n\nQuality: The theoretical analysis is technically sound with excellent analysis. Experimental validation is provided. \n\nClarity: Some concepts in the paper is unclear. What is 'Gradient Splitting' in the title? \n\nReproducibility: The code needed to reproduce the experimental results is not provided.\n\n",
            "summary_of_the_review": "This paper proposes a variant of the TD method with a faster convergence by introducing the well-known SVRG technique. The novelty and the improvements are limited.\n\n------Update------\n\nI have read the author's response and would like to keep the score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper604/Reviewer_pncF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper604/Reviewer_pncF"
        ]
    },
    {
        "id": "2qs26TNMc3",
        "original": null,
        "number": 4,
        "cdate": 1666731598252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731598252,
        "tmdate": 1669829968263,
        "tddate": null,
        "forum": "_AkC4QYxF5",
        "replyto": "_AkC4QYxF5",
        "invitation": "ICLR.cc/2023/Conference/Paper604/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper uses SVRG to reduce variance in TD learning. The resulting algorithm is analyzed using the gradient splitting perspective on TD learning to prove finite sample convergence rates that match those of SVRG in the setting of convex optimization. The analysis is done for several settings, and the findings are illustrated with experiments on several environments.",
            "strength_and_weaknesses": "**Strengths:**\n1. The analysis uses the gradient splitting approach, which yields a significantly simpler algorithm, analysis, learning rate/step size, and a better convergence rate.\n1. The paper considers several different settings, which increases the number of readers who would be interested.\n1. The paper provides recommendations for setting the learning rate, and actually uses this recommended learning rate in the experiments.\n\n**Potential Weaknesses/Questions:**\n1. Many grammatical errors, including typographic errors (\"TD-leaning\", \"apprxoimate\") and missing words, especially definite and indefinite articles. For example, \"determining *the* expected reward *an* agent will achieve if it chooses *actions* according to *a* stationary policy\". Some grammatical issues are listed below, but there were too many for me to write down all of them.\n1. ~~The paper doesn't comment on the assumptions made other than stating that they are standard. It would be nice to include a sentence or two explaining why the assumptions are reasonable or whether they are just for convenience (like Assumption 2; the feature vectors can always be normalized to make it true).~~\n1. Jumping between the notation used in other papers and the notation used in this paper is difficult. It would be better to convert the results of other papers to the notation used in this paper.\n1. ~~Font size of figures is too small to read without zooming in a lot.~~\n1. I have some concerns about the experiments:\n    1. ~~Why weren't the parameters for each algorithm set using a grid search the way they were for PD-SVRG? Without doing this, it's not clear that the chosen parameters are representative of the performance of each algorithm, and hence no conclusion about the performance of each algorithm can be drawn.~~\n    1. ~~Why was the learning rate for TD set to $1/\\sqrt{t}$? It seems like the algorithm stops learning very quickly due to the learning rate shrinking so quickly. How does the algorithm perform with a learning rate of $1/t$ instead?~~ I realized I was confused about this.\n    1. ~~What were the parameter values checked during the grid search? This would make the experiments more reproducible.~~\n    1. ~~Either confidence intervals or standard error (itself a form of confidence interval, I guess) should be included in the plots to communicate statistical significance to the reader.~~\n    1. Consider using either colourblind-friendly colours in the plots, or replacing the legend with labels for each line to remove the dependence on colour to determine which algorithm is which. Using differently-shaped points for each algorithm is a good start, but when the points are too close together it becomes difficult to tell which algorithm is which.\n    1. ~~What is the threshold for removing highly correlated features? This would help reproducibility.~~\n1. ~~The paper states that TD-SVRG and PD-SVRG converge linearly, but the y-axis of the plots appears to be in log space, which is confusing. Could this be clarified?~~\n1. ~~It would be good to have a concluding section that summarizes the main takeaways of the paper.~~\n\n**Grammatical/typographic issues:**\n1. \"Robbins & Munro (1951)\" should have the authors names inside the parentheses, because they are not being referred to in the sentence. Also, I think the second author's name is spelled \"Monro\".\n1. The \"Korda & LA (2014)\" citation is wrong: the authors are Nathaniel Korda and L.A. Prashanth, and the paper was published at ICML 2015.\n1. \"These methods are collectively known as variance reduction.\" Should this be, \"These methods are collectively known as variance-reduced gradient methods.\"?\n1. \"We analyze this case in 4 and 5\": It's better to write \"Sections 4 and 5\", because otherwise \"4 and 5\" could refer to an equation, appendix, theorem, etc.\n1. In Algorithm 1, should N be M? If not, it would be good to define N explicitly.\n1. \"unbalacedness\"\n1. \"is the the size\"\n1. \"on practice\" should be \"in practice\"\n1. \"envtironments\" in Figure 1.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\nThe paper is very clear conceptually, but there are quite a few grammatical mistakes that make it harder to read, including missing words and typographic errors. The discussion of results is very clear, and helpful explanations are provided throughout.\n\n**Quality:**\nI have some concerns about the experiments, detailed in the Strengths/Weaknesses section of this review.\n\n**Novelty:**\nTo the best of my knowledge, the analysis is novel.\n\n**Reproducibility:**\nThe paper is missing a few details that would be helpful for reproducing its experiments.",
            "summary_of_the_review": "~~Despite really liking this paper, I must recommend the current version be rejected due to concerns about clarity and the experiments detailed above. If the authors address these concerns satisfactorily and no critical issues are found by the other reviewers, I would recommend acceptance of a revised version.~~ The authors have addressed most of my concerns in their response, except some of my concerns about clarity (making these clarity changes would drastically reduce the work required of each reader to understand the paper). I have increased my score to reflect this.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper604/Reviewer_158U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper604/Reviewer_158U"
        ]
    }
]