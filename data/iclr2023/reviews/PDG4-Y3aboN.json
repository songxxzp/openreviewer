[
    {
        "id": "6bAo4xnF1B",
        "original": null,
        "number": 1,
        "cdate": 1666659113304,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659113304,
        "tmdate": 1666659113304,
        "tddate": null,
        "forum": "PDG4-Y3aboN",
        "replyto": "PDG4-Y3aboN",
        "invitation": "ICLR.cc/2023/Conference/Paper4842/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Model compression via quantization of weights and activations helps improve the efficiency and reduce memory requirements of DNNs.\nHowever, quantifying the impact of quantization across the different layers in a DNN is challenging. For this purpose, the proposed work establishes FIT as a measure of DNN performance, when using multi-precision quantization (MPQ), without the need to retrain the network.\nFIT is a metric that combines the impact of quantization on weights as well as activations in a unified manner such that the computation of the metric itself is quick and readily usable. \nThus, FIT can be used to understand and readily gauge the impact of MPQ and avoid the pitfalls of bad performance.",
            "strength_and_weaknesses": "Strengths\n- The proposed work provides a clear and concise explanation of the problem domain being tackled. This guides the reader from the general impact of the proposed work towards the specifics of the problem being handled.\n- The evaluation schemes provided for the estimator as well as the application on various networks provide a good foundation.\n\nWeaknesses\n- Could the authors provide citations for the statement: \"different layers, within different architectures, respond differently to quantization\". (Pg. 1, Introduction, Paragraph 4, Line 1)\n- The proposal of rank-correlation as a novel evaluation criterion is mentioned as a core contribution towards the end of the introduction. Could the authors describe possible evaluation schemes to compare against prior works, as baselines to judge the proposed scheme. Currently, rank-correlation exists as a readily available tool that has been repurposed for evaluating the task at hand. Multiple baselines could help highlight the contribution.\n- Could the authors provide citations for the statement: \" which is insufficient for prediction, and more challenging for practitioners to implement. In addition, trace computation can become very expensive for large networks\". (Pg. 3, Paragraph 1, Lines 9-11).\n- While the evaluation of the EF estimator in Table 1 shows a relatively model-agnostic behavior, it would be instructive to include evaluations with alternative DNN architectures that branch away from bottleneck-like structures (more densely packed connections) and alternative datasets, to help firmly establish the agnostic behavior observed.\n- Could the authors normalize the X and Y scales across both plots in Figure 2? The visual comparison would provide a more compelling evidence in that regard.\n- Could the authors discuss the type of information provided by FIT_A and FIT_W and try to provide more justification as to how they provide complementary information?",
            "clarity,_quality,_novelty_and_reproducibility": "Quality\nThe work provides strong intuition of the importance of evaluating the quality of a network post quantization without having to retrain the network. In terms of general concepts and building new ideas from foundational ones, the quality of the work is very good.\n\nClarity\nThe well-laid out context as well as thorough explanation of the method itself lend a lot of clarity to the reader.\n\nOriginality\nWhile the general ideas are firmly established in existing work, the novelty of the proposed work comes from combining them and simplifying them for the task at hand.",
            "summary_of_the_review": "The proposed work establishes the need for a metric to analyze the sensitivity of networks to quantization without having to retrain the network. For this purpose Fisher Information Trace is selected as the measure of choice and its estimation process is defined in the proposed work. \nOverall, the evaluation of FIT against the hessian, proving its equivalence for the task at hand, and subsequent evaluation across various application domains helps validate the claims made about FIT.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_cnF4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_cnF4"
        ]
    },
    {
        "id": "4Mh4x9zfSf",
        "original": null,
        "number": 2,
        "cdate": 1666673034598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673034598,
        "tmdate": 1666673034598,
        "tddate": null,
        "forum": "PDG4-Y3aboN",
        "replyto": "PDG4-Y3aboN",
        "invitation": "ICLR.cc/2023/Conference/Paper4842/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The presented work proposes a method for estimating model performance and mixed precision configuration without network retraining. The introduced method is based on the Fisher information, what allows for faster computations compared to other existing methods based on e.g., Hessians. ",
            "strength_and_weaknesses": "The work is clear and well structured. It also flows logically, with a detailed background, good comparison with the previous work, clearly explained FIM metric and experiments that include in-depth comparison with Hessian and analysis of model sensitivity based on the proposed criterion. The method is evaluated on a broad set of models and datasets to evaluate its performance and usability for different tasks. \n\nIn general, I don't see any major omissions. Some sections that could be improved to further improve the quality of submissions are:\n1) Could you elaborate more on the ideas of determining MPQ configurations from initialization instead of the trained models?\n2) Could you provide more details. about the applied quantization scheme? Symmetric, asymmetric, per tensor, per channel? Have you performed experiments with other quantization schemes?\n3) The abstract mentioned that hundreds of quantization configurations were used, but there was no clear summary of used configurations in the description, could you please explain it in more detail?",
            "clarity,_quality,_novelty_and_reproducibility": "The work is interesting. It proposes to use Fisher information for guiding mixed precision quantization, what to the best of my knowledge hasn't been used for such purpose before. Quality of the submission is good, the work flows logically. Additional details provided in appendix clearly explain all claims and can be useful for reproducing results. ",
            "summary_of_the_review": "This is a good quality interesting paper with detailed experiments and clear conclusions. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_uWSp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_uWSp"
        ]
    },
    {
        "id": "XIdNHzhpMB",
        "original": null,
        "number": 3,
        "cdate": 1666812667807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666812667807,
        "tmdate": 1666812859972,
        "tddate": null,
        "forum": "PDG4-Y3aboN",
        "replyto": "PDG4-Y3aboN",
        "invitation": "ICLR.cc/2023/Conference/Paper4842/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper finds a more efficient alternative than the Hessian Trace metric used in the mixed-precision HAWQ paper, named FIT, which is a further simplification of said metric based on the empirical fisher. They show the metric to be equivalent in many scenarios compared to the Hessian trace metric. ",
            "strength_and_weaknesses": "Strengths:\n* It is very clear the authors know what they are talking about, as they mentioned all the relevant literature, and understand the underlying methods/ideas very well. \n* An excellent overview of the literature, and the derivations leading to the final FIM objective. This is a great overview for anyone to read that wants to dive into this topic. \n\nWeaknesses\n* The metric itself has been introduced before, for both compression and quantization. I have seen it several times in the past. The first mention of it I know in the setting of quantization and compression is from this paper: https://arxiv.org/pdf/1810.06401.pdf - where it is derived from an information theoretical context. There is no novelty in the paper, since it is a small tweak to HAWQ, and the tweak itself already existed.\n* I think the paper is lacking in terms of experimental results. It is good the authors included the accuracy comparison w.r.t. the target loss for this method and 'competitive' methods. However, these numbers don't mean much for the performance in the mixed-precision setting. For the correlations in Table 2, I wonder if those are high enough to do a proper mixed-precision setting that is non-trivial. For these networks, how well does the mixed precision method rank the networks, and how well can it find a non-trivial solution? Why is this analysis that was done for semantic segmentation also not done for these other models? I don't like hammering on more results, but since the metric itself is not novel, the onus is on the authors to really show that in this setting this metric works well and is the best we can do. The paper turns into a survey paper, rather than a novel method paper. \n* I also wonder about the sense of these metrics in general, and if there is no easier solution. I think this is related to a key underlying assumption at the start of the paper: \"The exact (per parameter) perturbations \u03b4\u03b8 associated with quantization are often unknown.\"\nThis is a weird statement. If you 8-bit quantize a layer\u2019s weights, with a given min-max parameter setting, you know exactly what \u03b4\u03b8 is. There is nothing inherently stochastic about it. Instead of making this stochastic assumption, you could just quantize each layer to the intended potential bit-widths, and measure this on your validation set. Here is your loss function approximation, no surrogate measure needed. This also holds for the activations, where there is arguable a distribution over your data (but not over noise on your model, as the formulation now has it). Please compare what your metric would improve over a simple eval step on a batch of data for per-layer/per-quantizer bit-width settings. In practice, one can generally not set intermediate bit-widths anyway, and generally you'd pick 4, 8, 16 or something, which will likely be almost as efficient as this method.",
            "clarity,_quality,_novelty_and_reproducibility": "* Some things are quite unclear on the implementation details. How exactly is the quantization done? Do we fold batch-norm? Do we do asymmetric/symmetric quantization? Per-channel or not? What bit-widths are included for the mixed precision results? I would like to see more details on this in the paper\n* The quality of the paper is high, very well polished\n* The novelty is not there, as I mentioned in the strenghts/weaknesses above, since literally this metric has been used before in the same context of quantization\n* The authors noted they included sample code, so these specific experiments can be reproduced. ",
            "summary_of_the_review": "The paper is extremely well-written, and great in terms of clarity and exhaustiveness on the literature regarding Hessian-like estimates for deep learning. \nThe method of the paper has been published before, in the context of quantization. The results do convince me that this metric works just as well as the HAWQ metric; They also convince me that they are better than the other metrics the authors found in the literature. However, the authors have not convinced me that these metrics are useful in mixed-precision, that the resulting algorithm actually does something that is non-trivial, and that this is a great way to do mixed precision in general.\n\nI would like to see the following things:\n* A proper set of mixed precision results on more than 1 network, not only the semantic segmentation one. The authors included a few more networks for their comparative analysis, but then do not show actual mixed precision results on these nets. This should include showing that the loss approximation is good enough to find non-trivial mixed-precision results\n* A comparison to a very simple baseline - Just quantize each layer individually with your intended bit-width, and measure the output loss on a bit of data (few batches e.g.); Your metric is gauranteed to be a less accurate metric for mixed-precision by construction. How well does it perform in practice, and why would your metric be expected to perform better? \n* A more thorough experimental setup description. There's a lot of choices to be made in quantization, and these choices matter. \n\nIf these are addressed in a satisfactory way, I am very willing to increase my score. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_QyLp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_QyLp"
        ]
    },
    {
        "id": "xfTRXdcIMT",
        "original": null,
        "number": 4,
        "cdate": 1666911409614,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666911409614,
        "tmdate": 1666911409614,
        "tddate": null,
        "forum": "PDG4-Y3aboN",
        "replyto": "PDG4-Y3aboN",
        "invitation": "ICLR.cc/2023/Conference/Paper4842/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to use Fisher Information Trace (FIT) to perform mixed quantization of deep neural networks. The author of the paper use FIT to measure sensitivity of parameters and activations regarding quantization, and shows improvement of the performance.",
            "strength_and_weaknesses": "Strengths\n\n1. This paper demonstrate clear advantage of FIT to Hessian-based method.\n\n2. The writing of the paper is good.\n\n\nWeaknesses\n\n1. There are no ImageNet results of the FIT quantization, which is the major weakness of this paper. The author mentioned ImageNet at the beginning of section 4.1, which uses ImageNet to analyze the proposed method, however, no final accuracy results are give,. Only CIFAR-10 and MNIST.\n\n2. The experimental setting and demonstration is unclear. What is the bit width setting for each of the accuracy result? \n\n3. Many baselines regarding mixed precision quantization are not compared. For example: \u201cMix and match: A novel fpga-centric deep neural network quantization framework\u201d (HPCA 2021), \u201cRMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions\u201d (CVPR 2021), and more.\n  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to strength and weaknesses.",
            "summary_of_the_review": "To sum up, the clarity and quality of this paper need to be improved. The author of the paper did some interesting works on model quantization but fails to demonstrate them with thorough experiments. Please refer to strengths and weaknesses for more information.\n\nI think this paper needs to be revised, both on the technical contribution and experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_orGP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_orGP"
        ]
    },
    {
        "id": "mXPpbDO0jmW",
        "original": null,
        "number": 5,
        "cdate": 1667082731391,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667082731391,
        "tmdate": 1669601490588,
        "tddate": null,
        "forum": "PDG4-Y3aboN",
        "replyto": "PDG4-Y3aboN",
        "invitation": "ICLR.cc/2023/Conference/Paper4842/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new analytical metric to quantify the impact of quantization for deep neural networks. The proposed metric provides an estimation of quantization impact as accurately as hessian methods yet it is more computationally efficient and its variance is more favorable. The authors compared the proposed method with the other metrics and demonstrate its superior performance.",
            "strength_and_weaknesses": "(Strength)\n- A nice overview and derivation of the impact of quantization on the loss. \n\n- The nice properties of the proposed metric are that it is more robust (since it shows a smaller variance) and more efficient (in terms of computation)\n\n- The comparison of correlations that show superior predictability of the proposed methods for the impact of quantization loss.\n\n(Weaknesses)\n- The evaluation in Table 2 seems to be very limited. By exploiting the computational efficiency of the proposed algorithm, the authors should also try challenging tasks, such as a large CNN for ImageNet classification or a large language model like BERT.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Some of the important analysis seems to be missed: 1) derivation of FIT for activation quantization. 2) detail comparison with the Fisher information matrix  (Li et al., 2021)",
            "summary_of_the_review": "This paper proposed a practical metric, FIT, for quantitatively analyzing the impact of quantization errors in deep neural networks. FIT's robustness (due to small variance) and computational efficiency would be very useful for the practitioners in this study. However, currently, the evaluation results are very weak; it would be highly appreciated if the authors could include more experimental results, such as ResNet50 on ImageNet.\n\n\n====== Post rebuttal comments ======\nThe reviewer thanks the authors for their careful rebuttal. Demonstrating a solid accuracy boost on BERT models seems to make the paper much stronger, and I believe that such a demonstration confirms the applicability of the proposed method for researchers in this field to broader applications. Thus, I increase my score to Accept. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_qJTT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4842/Reviewer_qJTT"
        ]
    }
]