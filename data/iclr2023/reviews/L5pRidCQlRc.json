[
    {
        "id": "Jo3SDS20Mr",
        "original": null,
        "number": 1,
        "cdate": 1666614184006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614184006,
        "tmdate": 1666614184006,
        "tddate": null,
        "forum": "L5pRidCQlRc",
        "replyto": "L5pRidCQlRc",
        "invitation": "ICLR.cc/2023/Conference/Paper3733/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the effect of label noise on generalization when training\noverparameterized models.  Prior work has given theoretical and empirical\nevidence that overparameterization improves generalization.  This work shows\nthat, when the labels of the training data are very noisy, overparameterization\nmay actually harm generalization and provides a theoretical analysis of the\nphenomenon for 2-layer linear networks.  They provide empirical evidence for\nthis effect using both synthetic and real noise for classification tasks.  They\nalso consider using recent robust training methods (DivideMix and ELR) and find\nthat in some cases using these methods may actually exacerbate the phenomenon\nin the sense that it appears for lower noise rates.  They conclude that using\nlarger models may not always be the right choice in the presence of label noise\nand using robust training methods does not seem to make things better.\n",
            "strength_and_weaknesses": "Strengths\n\n1. Studying the effect of overparameterization in noisy settings is\na well-motivated and natural question. This paper provides some theoretical and\nempirical results and also some observations about overparameterization in\nconjunction with robust training techniques in noisy settings that fit in this\nline of research.\n\nWeaknesses\n\n\n1. The bias-variance decomposition closely follows the proofs of the prior work\nof Yang et al. [1].  The theoretical results on the 2-layer linear network\n(Theorem 1) seems to follow by using the same techniques as in [1]. Overall, I\nfound the theoretical results presented in this work not particularly\ntechnically challenging or novel as they follow closely the model and proofs of\n[1].\n\n2. There is a discrepancy between the noise models in the theoretical and\nexperimental results.  The theoretical results consider a regression setting\nwith additive gaussian noise and the experimental results consider\nclassification settings with symmetric or asymmetric random classification\nnoise.  I agree with the authors that studying label noise in classification\nsettings is more relevant so I think it would be better if the theoretical\nresults also considered a similar setting.\n\n3. In the experimental results the noise rates required so that the ascent in\ntest loss happens seem to be very large (in most cases above 50%).  For\nexample, in MNIST the noise rates are 80% and 95%.  For such large noise rates\nin the training data, it does not seem very surprising that the test loss is\neventually increasing as the noisy examples are the majority.  I think it would\nbe interesting to also consider adding random noise to the test data and see\nwhether the \"last ascent\" would then turn into a \"descent\".\n\n[1] Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-off for generalization of neural networks. In International Conference on Machine Learning, pp. 10767\u201310777. PMLR, 2020.\n\n\nTypos and Minor Issues\n\nAbstract, \"particularly when trained on smaller dataset\" -> datasets\n\nTheorem 1, \"$n/d = \\infty$ should be $n/d \\to \\infty$ (The same for Theorem 2).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well-written and the results are clear.  I found the\ntheoretical results to be a bit lacking in novelty and originality.\n",
            "summary_of_the_review": "\nThis paper studies an interesting and well-motivated setting and presents some\ntheoretical and empirical results.  I believe that there is some value in the\nextensions of the theoretical results of [1] in the noisy settings and\nempirical observations on using robust training methods are interesting.\nHowever, the theoretical results presented in this work seem to heavily rely on\nprior work and do not seem particularly surprising and novel.  I think it would\nbe be more interesting if the noise models in the theoretical framework were\nmore aligned with the experimental setting (random classification noise).\nMoreover, I am not sure on the practical significance of the empirical results\nas considering that the majority of the training data are corrupted seems to be\na bit unrealistic.  Therefore, overall, I am not sure whether this work is\nabove the threshold for ICLR.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3733/Reviewer_RtYb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3733/Reviewer_RtYb"
        ]
    },
    {
        "id": "ulgIINEjTY",
        "original": null,
        "number": 2,
        "cdate": 1666637388673,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637388673,
        "tmdate": 1666637388673,
        "tddate": null,
        "forum": "L5pRidCQlRc",
        "replyto": "L5pRidCQlRc",
        "invitation": "ICLR.cc/2023/Conference/Paper3733/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the effect of label noise on the test error of overparameterized models. In particular, it shows that when the labels are sufficiently noisy, the test error increases again with model size (after the double descent peak). The authors show this theoretically in a simple linear random features model, and provide empirical evidence for it in real neural networks.",
            "strength_and_weaknesses": "**Strengths**\n- The paper provides closed-form solutions for the bias and variance. \n- There are experiments on real neural network architectures.\n\n**Weaknesses**\n- There is a lack of novelty. The main theoretical results are a minor extension of Yang et al., and it's unclear whether they could just be obtained by taking a limit of previously published results.\n- The analysis is restricted to linear random features. Why is this restriction necessary?",
            "clarity,_quality,_novelty_and_reproducibility": "- How is the variance decomposition (Eq. 1) related to the decomposition of Adlam and Pennington (2022a) in Example 2? \n- Can Theorem 1 be obtained from previous work by taking the limit \\psi \\to \\infty?\n- Taking psi \\to \\infty while keeping kappa finite means \\sigma^2 goes to infinity. This is a very odd limit. Doesn't this mean the noise is much larger than the signal?\n- Page 6: There are bias-variance decompositions for losses other than MSE see \"A Generalized Bias-Variance Decomposition for Bregman\nDivergences\" Pfau (2013).",
            "summary_of_the_review": "The main findings of the paper lack novelty, which could hurt the paper's interest to the ICLR community. In addition, the relationship of the theoretical results to previous work is not fully explained or explored.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3733/Reviewer_fFmQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3733/Reviewer_fFmQ"
        ]
    },
    {
        "id": "_YPRDcAtjuI",
        "original": null,
        "number": 3,
        "cdate": 1666756773898,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756773898,
        "tmdate": 1666760428279,
        "tddate": null,
        "forum": "L5pRidCQlRc",
        "replyto": "L5pRidCQlRc",
        "invitation": "ICLR.cc/2023/Conference/Paper3733/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper demonstrates the existence of a phenomenon called \u201cfinal ascent\u201d where noisy labels can hurt the generalization models with larger capacity, which contrasts with \u201cdouble descent\u201d where increasing the model capacity first hurts the model performance and but improves the performance again. First, the paper theoretically proves the existence of the phenomenon in the recently popular random feature regression framework. Roughly speaking, the result shows that in the presence of label noise, the risk of the analytical solution has an extra term governed by the noise-to-sample size ratio. With similar technique, the paper also proves a theorem that shows sparsity can alleviate final ascent. In the second part, the paper conducts a series of experiments on MNIST, CIFAR10/100 and MLP, LeNet and ResNet34 and show that realistic models and data also exhibit final ascent.",
            "strength_and_weaknesses": "### Strength\n1. The proposed phenomenon is timely for the discussion of scaling and the message that larger model can hurt potentially generalization performance is very important\n2. The theoretical component seems technically solid\n3. The experimental results on deep neural network are thorough and match the prediction of the proposed random feature regression model\n4. The observation that robust methods exacerbate final ascent is extremely interesting and could lead to fruitful future research\n### Weakness\n1. The phenomenon only becomes relevant with extremely large label noise (> 60%) which makes it less interesting for most practical cases where we can expect a reasonable signal-to-noise ratio.\n2. The plots for 5c seem to contain fewer settings than the other ones. Why? I personally would be interested to see more\n3. There is a gap between the regression setting of the analytical model and classification setting of the experiments.\n4. It also seems like architectures can have a non-trivial effect on the phenomenon.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity** The paper is very clear\n\n**Quality** The paper is of good quality\n\n**Novelty** The paper builds on existing framework but has new contribution\n\n**Reproducibility** Seems good\n",
            "summary_of_the_review": "The paper makes a solid theoretical contribution to the line of work on random feature models and conducts thorough experiments. I do not have major complaints about the paper and thus recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3733/Reviewer_gtPU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3733/Reviewer_gtPU"
        ]
    },
    {
        "id": "zo2G0ZTivF",
        "original": null,
        "number": 4,
        "cdate": 1666858651707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666858651707,
        "tmdate": 1666858997710,
        "tddate": null,
        "forum": "L5pRidCQlRc",
        "replyto": "L5pRidCQlRc",
        "invitation": "ICLR.cc/2023/Conference/Paper3733/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the role of overparameterization for generalization when training with noisy labels. Specifically, the authors focus on the noisy label training data setting and investigate how test loss of model changes with respect to different levels of overparameterization. This paper provides two interesting empirical findings: (1). 'the variance of the generalization loss experiences a second ascent when the noise-to-sample size ratio is large'; (2). when applying robust training methods (robust towards label noise), the robust training methods make the last ascent more pronounced. The paper also provide theoretical analysis for the first finding in a simplified setting. ",
            "strength_and_weaknesses": "**Strength**:\n\n1. This paper identifies two interesting findings on the effect of \u201coverparameterization under label noise: \n \n>(1). the variance of the generalization loss has a second ascent when the noise is large; and \n\n>(2). the robust training methods (robust towards label noise) make the last ascent more pronounced. \n\nThese two findings could be practically useful for handling label noise, especially when we apply robust training methods for training highly overparameterized models.\n\n\n**Weaknesses**:\n\n1. [minor] There seems to be only one point for demonstrating the 'last descent' phenomenon on CIFAR10 (as shown in Figure 2 and Figure 5(b)). It would be interesting to study an even wider network to further examine the trend (if computation allows).",
            "clarity,_quality,_novelty_and_reproducibility": "(Clarity) This work is well presented.\n\n(Quality) High quality work, makes important contributions.\n\n(Novelty) Novel.\n\n(Reproducibility) Good.",
            "summary_of_the_review": "This paper provide interesting theoretical and empirical findings on overparameterization under label noise, I would recommend acceptance.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3733/Reviewer_romc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3733/Reviewer_romc"
        ]
    }
]