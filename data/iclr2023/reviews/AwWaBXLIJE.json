[
    {
        "id": "LNfwn1hXY5",
        "original": null,
        "number": 1,
        "cdate": 1666569858156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666569858156,
        "tmdate": 1669718781882,
        "tddate": null,
        "forum": "AwWaBXLIJE",
        "replyto": "AwWaBXLIJE",
        "invitation": "ICLR.cc/2023/Conference/Paper2315/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method, dubbed Q-Pensieve, for reusing critic knowledge across training iterations in a multi-objective reinforcement learning (MORL) setting. Specifically it proposes a modification of Soft Actor Critic (SAC) in the multi-objective setting that uses snapshots of previous Q functions in the learning update for which a convergence proof is provided. Finally the proposed algorithm is evaluated on a number of continuous control domains against other benchmarks and across 3 metrics and some ablations are considered. ",
            "strength_and_weaknesses": "The ideas presented in this work are generally well presented and the various sections do a good job of introducing the various background concepts. The method itself is quite easy to understand although I have not looked into the proof in the Appendix closely. \n\nWhile I generally think the paper is well written, there are some questions that I had regarding the implementation and results. One point in particular that could be improved is that the authors should mention earlier in the text the expected size of the replay buffer being used for the Q snapshots. It was not until the experiments that I saw the replay size of 4 was mentioned. On first reading it was not clear how big the replay size was and since this can have a large memory footprint the size matters quite a bit.\n\nIn the experimental section episode dominance is defined as a metric that directly compares a pair of algorithms. In Table 1 this metric is presented for all the baseline methods - how is it computed here? My understanding is that this metric defines on average for how many preferences does algorithm 1 out-perform algorithm 2. But it isn\u2019t clear what the two algorithms chosen in practice are.\n\nThe idea of using different Q networks for learning has also been studied in single objective RL. For example, TD-3 often outperforms SAC in single-objective RL using 2 Q networks in the update rule. There are differences in the two approaches but I am curious how well a TD-3 like update (extended to the multi-objective setting) compares to the method being proposed in the paper? Even with a Q buffer size of 4, the memory footprint of the proposed approach is quite large so it would be compelling if a simpler method worked as well.\n\nAdditionally the mechanism of information sharing through the Q replay is not clear to me. How is the replay buffer implemented? With such a small size is it effectively a queue containing the last N snapshots of learning? What happens when the replay is updated every N/2nd iteration and only stores N/2 snapshots instead say? As things stand I think the idea has merit but I think more can be done to understand exactly where the benefits are coming from which could perhaps simplify the method.\n\nEDIT (Post author discussion period):\n\nI would like to thank the authors for responding to my feedback and updating the paper with my comments. After looking through the discussion between the authors and the other reviewers and the latest draft of the paper, I think this is in good shape to be accepted. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is generally clear and well written. One important sentence that needs to be fixed is that in the \u2018Implicit search\u2019 part of the Related Work- \u2018While there is implcit\u2019. This sentence is clearly incomplete.\n \nThere were also a few minor typos:\nEquations 1 and 5 should have V(s\u2019) instead of V(s).\nPage 8 has \u2018Q reply buffer\u2019 instead of \u2018Q replay buffer\u2019\n",
            "summary_of_the_review": "Overall I think this paper is generally well presented and meets the acceptance bar. I am happy to increase my score once my clarification questions and results are presented. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2315/Reviewer_VKcR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2315/Reviewer_VKcR"
        ]
    },
    {
        "id": "KR73njOqz32",
        "original": null,
        "number": 2,
        "cdate": 1666603736469,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603736469,
        "tmdate": 1666603736469,
        "tddate": null,
        "forum": "AwWaBXLIJE",
        "replyto": "AwWaBXLIJE",
        "invitation": "ICLR.cc/2023/Conference/Paper2315/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper study the problem of  multiple objective reinforcement learning. As previous methods that search for Pareto front are not sample efficient. The paper propose the Q-Pensieve method, which updates the policy with learned Q-networks from past iterations. The paper provide some theoretical analysis. Finally, experiments on several multi-objectives environments validate its effectiveness over stoa baselines.",
            "strength_and_weaknesses": "Strength:\n\n1.The paper study an important problem and the paper is well written.\n\n2.The performance improvement is significant.\n\n3.Theoretical analysis is interesting.\n\nWeakness:\n\n1.The idea that uses replay buffer is not quite novel. It is common for rl methods trained with replay buffer. I think it is novel to use the learned critic in previous rounds, can this technique helps for single RL algorithms?\n\n2.The performance curve of each algorithm during the training is missing.\n\n3.For the experimental details, how is the preference vector decided in practice?\n\n4.The critic function is related with the preference vector, can does the model generalize well on this variable?\n\n5.How the performance change when the number of objectives increases?",
            "clarity,_quality,_novelty_and_reproducibility": "1.The paper is well written.\n\n2.The performance is significant and the model is novel\n\n3.The code for the implementation is provided.",
            "summary_of_the_review": "Overall, the paper is an interesting. A new MORL method is proposed to exploit the learned critic in previous rounds. The performance improvement is significant. But some detailed analysis and the discussion of the limitation are missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2315/Reviewer_g5HG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2315/Reviewer_g5HG"
        ]
    },
    {
        "id": "ODH92tZ4Up",
        "original": null,
        "number": 3,
        "cdate": 1666613740740,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613740740,
        "tmdate": 1666613740740,
        "tddate": null,
        "forum": "AwWaBXLIJE",
        "replyto": "AwWaBXLIJE",
        "invitation": "ICLR.cc/2023/Conference/Paper2315/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for improving sample efficiency in multi-objective reinforcement learning (MORL). Like universal value function approximators, a state-action value function represents a large number of value functions for any preferences. The most important contribution is the policy improvement step utilizing a set of previously learned Q-functions. The proposed method is evaluated on the MuJoCo benchmark tasks, and the experimental results show that the proposed method performs better in several metrics. Additionally, the proposed method outperforms a single-objective SAC because the proposed method can escape from sub-optimal policies. ",
            "strength_and_weaknesses": "Strength\n1. The authors propose a novel multi-objective actor-critic algorithm inspired by Yang et al. (2019). \n2. To improve sample efficiency, a Q-replay buffer is proposed, and it is efficiently integrated with the proposed algorithm. \n3. The experimental results show that the proposed method performs better than the baselines with respect to several metrics. \n\nWeakness\n1. The sampling distribution is not discussed explicitly. If the preference is uniformly sample, applying the proposed method to problems with many objectives is difficult. \n2. The scalability of the proposed method is not discussed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The update rule of $W_k(\\lambda)$ is unclear. Policy improvement (8) and policy evaluation (9) implicitly assume that $\\lambda$ is fixed during learning. However, Algorithm 1 shows that $\\lambda$ is sampled from $\\Lambda$ according to $\\mathcal{P}_\\lambda$. Even if the current $\\lambda$ is included in $W_k$, the learning performance depends on the sampling distribution $\\mathcal{P}_\\lambda$. Please discuss the sampling distribution in detail. \n\n2. The proposed method adopts the entropy-regularized framework, but the existing works (Abels et al., 2019; Yang et al., 2019) are based on the standard framework. Although I understand that policy improvement (8) is tightly linked to the entropy regularization of policy, I do not fully understand whether the proposed method requires the entropy regularization term. \n\n3. I am interested in the scalability of the proposed method, especially the ability to deal with many objectives. However, since the proposed method was only evaluated on two-objective MuJoCo tasks, it would be better to discuss the scalability. For example, envelop Q-learning (Yang et al., 2019) is evaluated on the extended SuperMario task, which has five objective functions. \n\n4. Equations (1) and (5): V(s) should be V(s\u2019).\n",
            "summary_of_the_review": "Overall, this paper is well-written, and the experimental results are promising. However, the proposed method becomes more appealing if it is evaluated on problems with many objectives. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2315/Reviewer_ecaS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2315/Reviewer_ecaS"
        ]
    },
    {
        "id": "-bQeZhZHh6",
        "original": null,
        "number": 4,
        "cdate": 1667281479158,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667281479158,
        "tmdate": 1670436747182,
        "tddate": null,
        "forum": "AwWaBXLIJE",
        "replyto": "AwWaBXLIJE",
        "invitation": "ICLR.cc/2023/Conference/Paper2315/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents Q-Pensieve, a method using versions of past Q functions to update a new Q function. This method enables information sharing across policies. The authors instantiate the idea in a soft actor-critic algorithm. Experiment results on DST, LunarLander, and several MuJoCo environments are provided where Q-Pensieve outperforms other baseline methods. ",
            "strength_and_weaknesses": "Strength:\nThe Q-Pensieve idea is well-motivated as one can interpret this as an extension of the envelope Q-learning algorithm by Yang et. al. It is good that the Q-Pensieve retains the same convergence policy iteration results. \n\nThe empirical results are good as well, with Q-Pensieve outperforming other baselines in three different performance metrics. My understanding is that this is a fairly straightforward method to plug in so I appreciate its effectiveness.\n\nWeakness:\nThe major weakness is the absence of the envelope Q-learning algorithm by Yang et. al. As the Q-Pensieve algorithm is closely related, the authors should provide a comparison.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear. I have some clarification questions:\n1. Do the actor and the critic network depend on the preference vector? If so, how is the preference vector handled as an input feature?\n\n2. Can the authors provide more details on maintaining the Q replay buffer? How often did you push a new Q function into the buffer? \n\n3. The practical implementation uses a relatively small Q replay buffer of size 4. Is the algorithm sensitive to this parameter?\n",
            "summary_of_the_review": "The proposed Q-Pensieve algorithm appears to be simple and effective. However, an important baseline is missing.\n\n==========================\nI want to thank the authors for writing a detailed response. My main concern, the comparison with envelope Q-learning, is adequately addressed. I would encourage the authors to expand the comparisons to include all the environments in a later revision.\n\nOn the Q replay buffer size study, it seems the performance is monotonically increasing with the buffer size. It would be good to add a discussion on the trade-off of the performance with memory overhead.\n\nOverall, I decided to raise my score from 5 to 6.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2315/Reviewer_jdUh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2315/Reviewer_jdUh"
        ]
    }
]