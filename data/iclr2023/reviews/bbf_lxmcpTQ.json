[
    {
        "id": "KFcSV3RfJ6",
        "original": null,
        "number": 1,
        "cdate": 1666110974632,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666110974632,
        "tmdate": 1666110974632,
        "tddate": null,
        "forum": "bbf_lxmcpTQ",
        "replyto": "bbf_lxmcpTQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5493/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes a new database for training a multimodal agent in a simple grounding task.  The user is faced with a screen and wishes to select a radio button or checkbox by voice.  If the agent gets it wrong, the user can attempt to correct the agent by a followup command such as \"click the checkbox below\".  The data was collected using a wizard of oz paradigm in which the human user sees a screen display and the randomly sampled UI object to be selected.  The user then issues an appropriate written (?) command.  The human agent sees the same screen with all clickable objects marked and the user's input.  The agent then selects the most likely target and the process repeats.  All data is derived from an existing data set called RICO, first published in 2017. \n\nIn addition to the data, the authors provide benchmark models and experimental results.  The benchmark model consists of a resnet encoder for the screen which is then plugged into a transformer along with the view hierarchy features. The resulting embedding is then input to an decoder along with the previous turns of the \"conversation\".  The agent is tested using both a heuristic and neural trained user model.\n",
            "strength_and_weaknesses": "The authors will make both the dataset and the code available.  This dataset will be a useful addition to those working on this type of interaction and more generally for those interested in multi-modal processing systems.   However, the outputs appear to be limited to clicking buttons and check boxes.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-presented although important information is glossed over.  In particular, I wasn't sure how the view hierarchy features were encoded and what role the screenshot played.   It also seemed to me that a simple hand-crafted baseline could have done very well on this task.",
            "summary_of_the_review": "There is little that is original in this paper and I found the motivation for the work unconvincing.  If the goal is to empower a UI agent to respond to natural language commands as well as direct clicks, then why would you complicate matters by conditioning with a resnet encoded screen shot?  If the agent rendered the display, it knows exactly where everything is so why not simply create a list of the available UI objects and their spatial relationship.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5493/Reviewer_tBFu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5493/Reviewer_tBFu"
        ]
    },
    {
        "id": "pIhRYs2AFmf",
        "original": null,
        "number": 2,
        "cdate": 1666661792848,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661792848,
        "tmdate": 1666661792848,
        "tddate": null,
        "forum": "bbf_lxmcpTQ",
        "replyto": "bbf_lxmcpTQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5493/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In  this paper,  it proposed a novel interactive task for multimodal grounding where a user and an agent work collaboratively on an interface screen.  The authors collected a new large dataset for interactions between human and agent, in which 20% are more than 1 round. The paper also experimented with different modeling variants and evaluation strategies to build different benchmarks on different statistics. \n",
            "strength_and_weaknesses": "The main contribution of this paper is to create a rich dataset which contains the human agent interactions for language grounding on a GUI screen. The paper also built some benchmarks based on different model variants and evaluation strategies to show the interaction  significantly improves grounding accuracy.\nSome questions:\n1. Besides the Mobile UI grounding task, there are also some tasks in interactive grounding, for example in the HRI field, what\u2019s the difference between GUI Grounding and other fields\u2019 interactive grounding task?\n2. What\u2019s the difference between online results and offline results?\n3. It\u2019s not clear how you evaluate the human-in-the-loop setting?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposed to create a large dataset for the interactive GUI Grounding task. It clearly described how the dataset is collected, and also built different model variants and metrics to investigate the relative statistics of the task.\n",
            "summary_of_the_review": "This paper clearly presents and verifies the contributions including the dataset created and the experimental benchmarks.  As it mainly focuses on the interactions and grounding. It might be better to submit to nlp related conferences or dialog related conferences.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5493/Reviewer_t7tT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5493/Reviewer_t7tT"
        ]
    },
    {
        "id": "gKSdMJi_Ij",
        "original": null,
        "number": 3,
        "cdate": 1666673816644,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673816644,
        "tmdate": 1666674739759,
        "tddate": null,
        "forum": "bbf_lxmcpTQ",
        "replyto": "bbf_lxmcpTQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5493/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents present MUG, a novel interactive task for multimodal grounding where a user and an agent work collaboratively on an interface screen, essentially bridging multimodal applications with user interfaces in HCI research. Their main novelty is an interactive task as opposed to existing static interaction tasks in user interfaces, and multiple rounds of interactions enables the user to give further commands for the agent to refine or correct its actions. This paper presents a dataset of 77,820 sequences of human user-agent interaction on mobile interfaces to study this problem and experiment several models and evaluation strategies, finding that iterative interaction improves task completion accuracy but also presents directions for future work.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is well motivated and described. The ideas are clear and experiments are on several large multimodal datasets.\n2. The paper is largely clear with clear figures and exposition. It was a joy reading this paper.\n3. The paper makes important contributions to the data collection and model evaluation, with important insights for future work.\n\nWeaknesses:\n1. There can be more comparisons and evaluation of actual multimodal methods - right now all the tested methods are largely on interactive learning/online and offline RL, but the paper is motivated as a multimodal paper. See https://arxiv.org/abs/2107.07502 for an example of a recent multimodal benchmark and a suite of tested multimodal fusion models with key takeaways for future work, and see https://arxiv.org/abs/2209.03430 for more references in the field of multimodal grounding and the key methods there.\n2. There can also be a better motivation and definition of multimodal grounding in this task - is grounding simply finding the right region of interest as a bounding box? How does grounding change across interaction? How important is grounding to the specific object versus whole UI-level representation? What are the effects of the amount of data required for grounding? Many questions should be formalized or motivated here. Given that this seems to be the main selling point of the dataset there needs much more analysis on how current grounding (or more generally alignment) methods work on this task and what future innovations are needed.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is largely clear with clear figures and exposition. It was a joy reading this paper. The paper makes important contributions to the data collection and model evaluation, with important insights for future work.",
            "summary_of_the_review": "There can be more comparisons and evaluation of actual multimodal methods, including better motivation and definition of multimodal grounding in this task.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5493/Reviewer_YoBY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5493/Reviewer_YoBY"
        ]
    },
    {
        "id": "_2jXvRJG9HM",
        "original": null,
        "number": 4,
        "cdate": 1667269522333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667269522333,
        "tmdate": 1667269522333,
        "tddate": null,
        "forum": "bbf_lxmcpTQ",
        "replyto": "bbf_lxmcpTQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5493/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a novel benchmark dataset for language grounding on user interfaces, notably exploring the ability to handle *sequences* or multiple turns of instructions/actions between a human user and virtual assistant. This dataset is of remarkable size, consisting of 77K unique language utterances, grounded in 31K individual mobile app \u201cscreens\u201d belonging to a set of 7K unique mobile applications. In addition to the dataset itself, this work proposes a straightforward baseline with a set of meaningful ablations that characterize the difficulty and open challenges of working on this task.\n\nThe dataset is collected by pairing two human annotators together, with one human acting as the \u201cspeaker\u201d and the other as the \u201cfollower\u201d \u2014 similar to many other multi-turn data collection procedures in the NLP literature. In addition to assembling this dataset, the work further splits the data into different splits based on the \u201cchallenge\u201d level of the interactions in question \u2014 where \u201cchallenge\u201d is a function of taking more than 1 turn to get to the correct \u201canswer\u201d (agent interaction on the screen). That means that ~80% of the dataset falls into the single-turn instruction following paradigm.\n\nThe proposed approach pairs an object-oriented visual tokenizer with a Transformer decoder that eats the history of state/actions and outputs the a \u201ccontext\u201d vector that is used to score each object to interact with (e.g., as a ranking problem). The ablations of the approach show that single-turn instruction following is not enough, and that modeling the entire context, and ability to make decisions in sequence (e.g., via imitation learning) is critical to performing well on this dataset. \n",
            "strength_and_weaknesses": "I believe this to be a well constructed dataset, with applications for assistive technology, and a nice playground for studying deeper questions of multimodal language grounding. \n\nHowever, I do believe this work has several weaknesses; while the paper focuses on the multi-turn aspect of their data collection, only 20% of the collected interactions actually require \u201crevisions\u201d or \u201ccorrections\u201d from the human \u2014 this seems rather low for a dataset that hopes to spawn future work in multi-turn interactions. Furthermore, the type and diversity of language that human users give is rather limited; as noted in the work, most utterances are only ~4 words long; it\u2019s not clear the data is rich enough to enable multi-turn grounding research, or if the difficulty is just in mapping \u201ckey phrases\u201d to UI components \u2014 it would have been nice to see this ablation.\n\nFinally, the choice of Offline RL as an ablation seems a bit unnatural; given that we know the \u201cright\u201d UI component to select in each interaction, treating this as a multi-step MDP doesn\u2019t feel appropriate; instead, it\u2019s almost as if something like DAgger would be the better baseline \u2014> this would also be strictly better than the imitation learning baseline \u2014 I would love to see how much this improves on the existing results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general this paper was a joy to read, with very digestible takeaways. The novelty of this dataset with respect to prior work is clear (though perhaps a bit understated), and I\u2019m excited to see this dataset and baselines released as mentioned in the work!\n",
            "summary_of_the_review": "This is a decent dataset paper, proposing an interesting resource for language grounding on user interfaces. However, I don\u2019t believe the paper as written delivers on its promises; I think the language diversity is lacking, and the multi-turn nature of the dataset is a bit oversold.\n\nI would really love to see an ablation experiment that just maps key phrases (bigrams, trigrams) to corresponding UI components to see if the \u201cdifficulty\u201d of this dataset is actually multi-turn grounding, or just having the right mapping of words to components on the interface \u2014> if it\u2019s the latter, then it feels like any pretrained model with some amount of knowledge of UI/web components (e.g., WebGPT) would make this dataset trivial.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5493/Reviewer_FLkv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5493/Reviewer_FLkv"
        ]
    }
]