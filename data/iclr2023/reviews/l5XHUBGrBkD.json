[
    {
        "id": "zC_txVAW48",
        "original": null,
        "number": 1,
        "cdate": 1666605556856,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605556856,
        "tmdate": 1666605556856,
        "tddate": null,
        "forum": "l5XHUBGrBkD",
        "replyto": "l5XHUBGrBkD",
        "invitation": "ICLR.cc/2023/Conference/Paper1951/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to obtain a robust and efficient neural architecture by searching the best teacher layers and the number of filters for student network. Experiments are conducted on CIFAR and ImageNet-100, which show good performance over prior methods.",
            "strength_and_weaknesses": "Strength:\n1. It make senses to automatically search for the best teacher layer to supervise each student layer, and ablation studies justify the effectiveness of this strategy.\n2. The paper is well written and easy to understand.\n\nWeaknesses:\n1. The knowledge distillation method is quite standard except for the optimal teacher layer searching. For example, there are not much new things in Section 3.1 and distillation loss in Section 3.4.\n2. The architecture search method seems to have limited novelty over FBNetv2.\n3. The proposed method use knowledge distillation to improve the performance, while the compared methods not. It is not quite fair and no ablation studies to show the performance of the proposed method without knowledge distillation.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to understand. The novelty of the proposed method seems to be limited.",
            "summary_of_the_review": "My main concerns lie on the limited novelty of the proposed method and the unfair comparisons with prior work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1951/Reviewer_4K2f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1951/Reviewer_4K2f"
        ]
    },
    {
        "id": "6XvKN0m5Qn9",
        "original": null,
        "number": 2,
        "cdate": 1666694478014,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694478014,
        "tmdate": 1666694478014,
        "tddate": null,
        "forum": "l5XHUBGrBkD",
        "replyto": "l5XHUBGrBkD",
        "invitation": "ICLR.cc/2023/Conference/Paper1951/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a NAS method to find robust neural network that can defend adversarial attracks. It combine knowledge distillation with NAS to pursue better performacne. The authors conduct experiments on multiple tiny datasets, such as CIFAR-10, ImageNet-100.",
            "strength_and_weaknesses": "Strength:\n\n-It is interesing to find robust architecture with automatic search. \n\n-The proposed method use a teacher to enhance the model's performance and robustness, which is reasonable. \n\n-This paper is well-written and easy to follow.\n\nWeakness:\n\n-Directly combining NAS and knowledge distillation can usually improve performance. The authors should compare the proposed method with other knowledge distillation methods.\n\n- Only tiny datasets are used to evaluate the effectiveness of the proposed method. These datasets have a large gap with the practical scense. Comparesion on large dataset (such as ImageNet) is required to validate the method's effectiveness.",
            "clarity,_quality,_novelty_and_reproducibility": "See Strength And Weaknesses",
            "summary_of_the_review": "See Strength And Weaknesses",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1951/Reviewer_k7LD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1951/Reviewer_k7LD"
        ]
    },
    {
        "id": "neaTpNo1Jz",
        "original": null,
        "number": 3,
        "cdate": 1667280552678,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667280552678,
        "tmdate": 1669108529018,
        "tddate": null,
        "forum": "l5XHUBGrBkD",
        "replyto": "l5XHUBGrBkD",
        "invitation": "ICLR.cc/2023/Conference/Paper1951/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper utilizes neural architecture search to discover neural network architectures that are adversarially robust and with low latency at the same time. In particular, they introduce an adversarially robust teacher model for cross-layer distillation in the training and search process to improve the adversarial robustness of the model, and specifically search which layers of the teacher model are used for feature distillation.",
            "strength_and_weaknesses": "==== Strength ====\n- The first work that applies knowledge distillation-based NAS for adversarial robustness.\n- Intuitively, the proposed method is reasonable.\n- The paper writing is clear, the method description is easy to understand.\n\n==== Weakness and Questions ====\n\n1. Unsupported statements and missing discussions.\n\na) This paper claims in the introduction that RNAS-CL is ``the first NAS method that jointly optimizes accuracy, latency, and robustness against adversarial attacks''. However, quite a few studies have already explored this problem, for example, [1][2][3]. Should remove this statement, and discuss / compare with them.\n\nb) The proposed method adopt ideas from existing methods, should discuss more about these existing work, for example, [4].\n\n2. Weakness and questions about the experiments.\n\na) Missing comparisons with robustness NAS studies like [1][2][3].\n\nb) Necessary baseline results are missing. Table 1 shows the performance of the architectures discovered by different methods on CIFAR-10. The RNAS-CL-* models are searched with the Wide-ResNet teacher model, but the performance of the teacher model is not mentioned. The performance of Wide-ResNet should be given as baseline results. In particular, I wonder if the searched architecture outperforms the Wide-ResNet model, which is very important to verify the effectiveness of the proposed method;\n\nc) Why are only the results of using WRT as the teacher model on CIFAR-10 reported in Table 1? For ImageNet-100, the results with different models as teacher models are provided in the appendix. How will the teacher model's architecture or performance influence the performance of discovered architecture.\n\nd) Some essential experiment settings are missing. For example, what\u2019s the step number for PGD attack in the paper?\n\n[1] Xie, Guoyang, et al. \"Tiny adversarial mulit-objective oneshot neural architecture search.\" arXiv preprint arXiv:2103.00363 (2021).\n\n[2] Ning, Xuefei, et al. \"Discovering Robust Convolutional Architecture at Targeted Capacity: A Multi-Shot Approach.\" arXiv preprint arXiv:2012.11835 (2020).\n\n[3] Yue, Zhixiong, et al. \"Effective, efficient and robust neural architecture search.\" 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 2022.\n\n[4] Li, Changlin, et al. \"Block-wisely supervised neural architecture search with knowledge distillation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\nAnother minor suggestion: You should use ``'' for quotation marks in LaTex.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good. The novelty is on the borderline. No code, so reproducibility is doubtful.",
            "summary_of_the_review": "My concerns about this paper mainly come from the insufficient experimental verification. I may consider improving the score if the authors could provide the missing experiments and results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1951/Reviewer_xKFw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1951/Reviewer_xKFw"
        ]
    }
]