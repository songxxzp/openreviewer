[
    {
        "id": "Rawpv6hLAd",
        "original": null,
        "number": 1,
        "cdate": 1666562285463,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666562285463,
        "tmdate": 1666592113729,
        "tddate": null,
        "forum": "Z-aIURmBbBk",
        "replyto": "Z-aIURmBbBk",
        "invitation": "ICLR.cc/2023/Conference/Paper3895/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an extension of Masked Autoencoders (MAE) to the vision-language domain called Multi-Modal Masked Autoencoder (M3AE). Image-caption pairs are partially corrupted by masking some image patches and some language tokens. The non-masked image patches and language tokens are passed through an autoencoder based on transformers, which is trained to reconstruct the full image-caption, including masked parts. The embeddings of image patches that are learned by the transformer encoder are transferred to downstream image tasks, such as classification and out-of-distribution detection. ",
            "strength_and_weaknesses": "Strengths: The paper is easy to understand, with the methodology clearly presented. The results section includes some nice qualitative analysis and visualizations to help understand the representations that are learned by M3AE.\n\nWeaknesses: \n\n1) The empirical support for the proposed approach is weak.\n\nOne of the main takeaways from the experiments (Figure 3-5) is that M3AE outperforms MAE in the downstream ImageNet classification task. This is a good sanity check, but it is not significant nor surprising. The M3AE framework involves predicting text token logits from image patches, which is similar to a supervised classification task where the text is providing labels for training the image encoder. On the other hand, MAE is completely unsupervised. \n\nIt would be better to compare M3AE against other vision-language pretraining methods. However, no vision-language pretraining baselines are provided except for CLIP which outperforms M3AE by a large margin in Figure 3.\n\n2) In the introduction, the authors make the strong claim that M3AE offers two benefits over contrastive pretraining methods, but neither benefit is demonstrated in the experiments.\n\nFirst, the authors emphasize that contrastive learning \"requires paired image-and-text data and therefore cannot leverage widely available unpaired data.\" This suggests that leveraging unpaired data will make M3AE perform better than CLIP. But as shown in Figure 3, CLIP does much better than M3AE. To support this claim, the authors should either (i) decrease the paired data for CLIP to e.g., 10-30% and show that M3AE does better than CLIP for the same amount of paired data in Figure 3, or (ii) add some unpaired image or text data for M3AE and show that it can do better than CLIP by leveraging this unpaired data.\n\nSecond, the authors claim that the \"separation of image and text encoders\" in contrastive learning \"hinders the joint understanding of image and text\", but they provide no proof of this. To support this claim, they should show that M3AE performs better than contrastive learning on a multimodal downstream task. Also, note that separation of image and text encoders is not necessary in contrastive learning (e.g., GLIPv2 enables fusion between image and text features before the contrastive objective), so this is not an inherent limitation of contrastive pretraining methods.\n\n3) The authors write: \"Surprisingly, we find that M3AE benefits from a higher text mask ratio (50-90%), in contrast to BERT whose standard masking ratio is 15%.\" This statement is misleading because it suggests that M3AE benefits from a higher text mask ratio compared to BERT on the same downstream task. But actually, BERT embeddings are evaluated on language tasks while M3AE embeddings are evaluated on image tasks. It does not make sense to compare BERT's text masking ratio with M3AE's, since M3AE's text representation is never evaluated in a downstream task.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - good; the paper is clear and well-written.\n\nQuality - the experiments needs work. The main experimental result (that M3AE outperforms MAE) is not significant enough (weakness 1), and the other main claims in the introduction are also flawed (weaknesses 2-3)\n\nOriginality - ok. The concept of masked auto-encoding is not novel, although extension to the multimodal setting merits some consideration.",
            "summary_of_the_review": "See weaknesses 1-3 above. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3895/Reviewer_GbHq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3895/Reviewer_GbHq"
        ]
    },
    {
        "id": "yUs8vu6LrR",
        "original": null,
        "number": 2,
        "cdate": 1666626053194,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626053194,
        "tmdate": 1666652068470,
        "tddate": null,
        "forum": "Z-aIURmBbBk",
        "replyto": "Z-aIURmBbBk",
        "invitation": "ICLR.cc/2023/Conference/Paper3895/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an empirical study of M3AE trained on a large-scale image-text dataset, and find that multimodal masked autoencoder is able to learn generalizable representations that transfer well to downstream tasks. ",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is well written and this reviewer enjoyed reading it.\n\n2. The proposed approach is simple and intuitive.\n\n3. It shows promising results on image classification and out-of-distribution detection tasks.\n\nWeakness:\n1. The idea lacks of novelty. \n\n2. No experiments for vision-language tasks have been performed (e.g., text-to-image retrieval, VQA, visual reasoning, etc). The authors only fine-tune their pre-trained models for visual tasks, such as out-of-distribution detection and image classification. As the paper targets at multi-modal pre-training, it's essential to provide the experiments for vision-language down-stream tasks!\n\n3. Figure 2 is confusing. For example, in the lower left corner, the text \"Trees in a winter storm\" even appears in the space of its right image. \n\n4. Ablation study experiments aren't sufficient, it's important to conduct the experiments, e.g., different mask ratios of MAE.\n  \n5. Why not use two separate encoders for visual and textual feature extraction? The proposed M3AE only use one encoder for these two kinds of modalities together, but this may make the models confused. it's better to perform ablation study to use two different encoders, so that we can figure out the necessity of such design.\n\n6. The comparison is unfair, the proposed method uses more vision-language data (e.g. CC12M) for pre-training, and compare the models with MAE, which is only pre-trained on ImageNet. The performance gap between M3AE and CLIP is still very large. ",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity - good; the paper is well-written.\n\n2. Quality - the experiments aren't sufficient. Please see the weakness 2,4 and 5.\n\n3. Originality - ok. The proposed M3AE lacks of novelty.\n\n4. Reproducibility - unclear. ",
            "summary_of_the_review": "The paper is about the new approach for pre-training vision-language models by the proposed multimodal masked autoencoder. Although the approach is interesting, it is incremental and not novel. Some of the experiments demonstrating the superiority of the proposed approach are also unclear.\n\nThe proposed M3AE haven't performed experiments on vision-language down-stream tasks, it's not clear that the approach can learn good multimodal representation. Although the model can achieve better performances compared with MAE, M3AE uses larger dataset (e.g. CC12M). The performance gap between M3AE and CLIP is large.\n\nSome important ablation study experiments, such as different mask ratios of MAE and different encoders for visual/textual inputs, aren't conducted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3895/Reviewer_1MQ5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3895/Reviewer_1MQ5"
        ]
    },
    {
        "id": "kTJPdOHa6aK",
        "original": null,
        "number": 3,
        "cdate": 1666723481245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723481245,
        "tmdate": 1670567517165,
        "tddate": null,
        "forum": "Z-aIURmBbBk",
        "replyto": "Z-aIURmBbBk",
        "invitation": "ICLR.cc/2023/Conference/Paper3895/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the multimodal representation learning in the masked auto-encoder way. The work is motivated by the fact that many existing multimodal learning approaches require a large amount of paired text-image data. This work builds upon MAE, extending it to text modality as well. The proposed method can handle both the unpaired image data and paired image-text data. The experiments show a clear improvement over MAE in several tasks, and show the effectiveness of the learned transferrable representation. \n\n--- post-rebuttal ---\\\nThanks for the authors' response. I have carefully read the response and other reviewers' comments. I believe the current form of this paper  is not ready for ICLR publication. I keep my score. ",
            "strength_and_weaknesses": "Strength \n\n-The idea of applying masked autoencoder strategy to multimodality data is rational. The finding on higher mask ratio on text performs better than the low mask ratio on pure text is interesting. The explanation makes sense to me. \n\n-The experiments show the strength over MAE in different scenarios and applications.  \n\n \n\nWeakness \n\n-There is other work also working on the similar motivation, i.e., leveraging unpaired data and paired text-image data for multimodal representation learning. BEIT-V3 [Wang et al.,], where they demonstrate an exceptional performance over many existing solutions including CLIP on a number of downstream tasks. It seems this paper misses the discussion on this work. And I\u2019d also like to see the experimental comparison with BEIT-3 to show the advantages. \n\n[Wang et al.,] Wang et al.,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks, https://arxiv.org/abs/2208.10442 \n\n-I have a little concern on the effectiveness on the large-scale data, such as several hundreds of millions of data points or even larger. The current training is on CC-12M, and shows a good improvement over MAE. However, when the number of training samples gets larger, I am not sure if this improvement margin can be preserved.  \n\n-Following the above point, thanks to the effort on the large scale data curation LAION, there are billions of image \u2013text noisy pairs available. Please explain the application scenario of this method. \n\n-Figure 9, the reconstruction result from MAE is missing. I\u2019d like to see a comparison to show the strength. \n\n-Other minors: In page 3, \u201cFor patches and tokens, we sample s random\u201d, not sure what \u201cs\u201d means here. \n\nIn page 7, there is a space in \u201cM3AEoutperforms\u201d  ",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear to me. and I'm satisfied with the details of the technical points. I think this work can be easily reproduced. Regarding novelty, I'm ok with the proposed method to extend MAE to multimodal representation learning. While I like to see the clear comparison with other existing efforts like BEIT-3 with a similar motivation.",
            "summary_of_the_review": "Overall, the proposed solution is rational and mostly well presented. While my major concerns are lack of comparison with the existing method and no clear motivation given billions of paired text image data available. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3895/Reviewer_qu2E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3895/Reviewer_qu2E"
        ]
    },
    {
        "id": "unb23F9yyM3",
        "original": null,
        "number": 4,
        "cdate": 1666870199667,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666870199667,
        "tmdate": 1666870199667,
        "tddate": null,
        "forum": "Z-aIURmBbBk",
        "replyto": "Z-aIURmBbBk",
        "invitation": "ICLR.cc/2023/Conference/Paper3895/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper scales masked autoencoders (MAE) to image-text multimodal data, it investigates the use of modality-agnostic unified encoders, which learns generalizable representations across a number of downstream tasks.",
            "strength_and_weaknesses": "Strengths:\n(a) The paper is overall well-written, the problem well formulated and novel to me. Generalizing masked autoencoders to work on more modalities is definitely interesting to the community.\nWeaknesses:\n(a) There seems to be a lack of comparison with other baselines. The paper only compare with CLIP and MAE, while not other baselines, such as MultiMAE[1], using the same modality-agnostic setting, were not included.\n(b) The author should also include a discussion on how is M3AE different from MultiMAE.\n[1] MultiMAE: Multi-modal Multi-task Masked Autoencoders, ECCV 2022",
            "clarity,_quality,_novelty_and_reproducibility": "Well-motivated paper yet lacks discussion and comparison with other baselines.",
            "summary_of_the_review": "The paper is overall well-motivated and well-written, yet the experiment part seems to fall short with a lack of discussion and comparison with other baselines.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3895/Reviewer_ZSrM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3895/Reviewer_ZSrM"
        ]
    }
]