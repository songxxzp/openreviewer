[
    {
        "id": "X8odGuWTDQ9",
        "original": null,
        "number": 1,
        "cdate": 1666377758282,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666377758282,
        "tmdate": 1668622564927,
        "tddate": null,
        "forum": "hhvkdRdWt1F",
        "replyto": "hhvkdRdWt1F",
        "invitation": "ICLR.cc/2023/Conference/Paper4112/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel method for learning algorithms with neural networks. Specifically, the authors design a new architecture for processing graph data and a new training routine that considers both primal and dual formulations of optimization problems. Using min cut max flow problems, they compare their method to existing neural algorithmic reasoning tools. They show that their method beats the existing methods.\n",
            "strength_and_weaknesses": "Strengths: \n  - Interesting and novel approach to leaning algorithmic reasoning using primal/dual.\n  - Timely: the algorithmic reasoning community is growing.\n  - Clear writing and good exposition.\n\nWeaknesses: \n  - There is only one algorithmic problem considered. In related work, more than one algorithm is used to show performance. This is a major weakness of the paper since the main claims are not entirely supported. The abstract states, \"We demonstrate that simultaneously learning the dual definition of these optimisation problems in algorithmic learning allows for better learning and qualitatively better solutions.\" and In the conclusion, the authors write:  \"We showed that learning together the primal-dual problem can substantially\nimprove the quality of the predictions, as testified by the quantitative and qualitative evaluations\nof the models. Furthermore, dual algorithmic reasoners have demonstrated to generalise better,\nshowing positive knowledge transfer across different families of graph distributions and extracting\ninformative representations for large-scale graphs while only being trained on toy-synthetic graphs.\" But I find the claims much more general than the empirical findings which relate to only one problem on graph data and only two different families of graphs. \n\nMinor issues not affecting score:\n    - The citations and figure references are not linked. \n\n___\n\n\nAfter reading the author response, I have changed my score. The authors have addressed my concerns.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and the quality is good. The paper presents a novel approach. No code is provide, but the details in the manuscript seem thorough.",
            "summary_of_the_review": "The technique is interesting and novel, but the empirical results are lacking. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4112/Reviewer_4SGE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4112/Reviewer_4SGE"
        ]
    },
    {
        "id": "29dRBzUEb0b",
        "original": null,
        "number": 2,
        "cdate": 1666627659251,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627659251,
        "tmdate": 1666627659251,
        "tddate": null,
        "forum": "hhvkdRdWt1F",
        "replyto": "hhvkdRdWt1F",
        "invitation": "ICLR.cc/2023/Conference/Paper4112/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper follows a line of work on learning to mimic algorithms with neural networks to allow for better out-of-distribution generalization. Following recent advancement, the authors aim to train a GNN to simulate the intermediate steps of graph algorithms, using intermediate states of the true algorithm as supervision targets.\n\nThe authors expand on recent findings that training such an architecture benefits from multi-task learning, i.e. simultaneously learning to execute multiple related algorithms. They propose to leverage duality by simultaneously learning the primal and the dual version of an optimization algorithm. This gives the benefits of multi-task learning, reduces the chance for error propagation, and allows to leverage the learnt dual problem as a subroutine of the primal algorithm.\nSpecifically, this paper considers learning to execute the Ford-Fulkerson algorithm for the max-flow problem along with its dual min-cut problem.\n\nThe method follows the encode-process-decode framework, and consists of two iteratively executed GNN processors operating on the latent space. The first processor retrieves augmenting paths, while the second one learns to perform flow-update operations and predicts min-s-t-cuts. Some of the constraints on the max-flow are enforced by appropriate transformations, others are corrected for after termination of the neural algorithm.\n\nThe method is empirically tested on both synthetic and real-world data. \nThe neural algorithm is trained on small synthetic 2-community graphs.\nIn the synthetic experiment,  it is then evaluated on synthetic graphs that are out-of-distribution and out-of-family. The results show a clear benefit of incorporating the dual information both in terms of final performance and in terms of algorithmic alignment. They also demonstrate the benefit of using the dual solution as a subroutine of the primal neural algorithm, thereby extending beyond the setting of simple multi-task learning.\n\nIn the real-world experiment, the task is to classify the edges of a large-scale brain vessel graph from edge-level features. The goal is to improve existing classification pipelines by extracting additional features that capture the flow-capacity, using the neural algorithm that was trained on synthetic data. After retraining the encoder to adapt for the new inputs, the neural algorithm is executed for a single step on the large-scale graph, and the latent representation of the neural algorithm is aggregated to extract the edge features. Running existing classification pipelines with the additional features shows a clear performance improvement, demonstrating that the neural algorithm captures useful information even when executed on graphs much larger than the training data.",
            "strength_and_weaknesses": "Overall, I liked this paper a lot. It is very well written and the idea of using duality information for learning to execute optimization algorithms with neural networks is exciting. \nThe empirical evaluation is also very strong in my opinion, clearly showing the benefit of the proposed method even when going beyond an evaluation on purely synthetic data. \n\nOne critique point is that the paper only covers one specific primal/dual pair of algorithms (max-flow/min-cut), and the architecture is heavily engineered towards this problem. It would be great to see how the presented ideas of using dual information as a subroutine of the primal algorithm translate to other primal/dual pairs of algorithms, a short exposition of potential candidates would be very useful.\nAdditionally I have some remaining questions on the details of the method and the experiments. \n- What is the architecture used for the two decoders? \n- Is there an interpretation for the big differences in performance from max and mean pooling? Could they be used in conjunction to improve performance or at least to remove the need to optimize over it?\n- Regarding re-training for the real-world experiment: What exactly is the training setup for training the new encoder? Is the true radius of the vessel used to produce ground truth capacities that the target algorithm is executed on? Have the authors tried to directly predict the capacity from the three available features as a baseline?\n\n\nSidenote: In Figure 1 the second processor also accesses the inputs of the algorithm, but the arrows are not drawn.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has very high clarity, everything easily understandable and except for some remaining details (see weaknesses) well explained.\nThe quality of the paper is high, the method appears to be well thought-through, and the conducted experiments are well-designed and demonstrate the effectiveness of the method.\nTo the best of my knowledge the idea of using duality to enhance GNNs for algorithmic reasoning is novel.\nAlmost all parts of the method and the experiments are explained in sufficient detail. Code for reproducing the experimental results is not provided.",
            "summary_of_the_review": "This paper proposes a novel idea, translates it into a well-designed method and empirically validates the effectiveness on both synthetic and real-world data. This is a strong paper in my opinion that is highly relevant for the ICLR community, and I therefore recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4112/Reviewer_vSCg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4112/Reviewer_vSCg"
        ]
    },
    {
        "id": "UTdQ2kPy2b",
        "original": null,
        "number": 3,
        "cdate": 1666663115391,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663115391,
        "tmdate": 1666663115391,
        "tddate": null,
        "forum": "hhvkdRdWt1F",
        "replyto": "hhvkdRdWt1F",
        "invitation": "ICLR.cc/2023/Conference/Paper4112/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes DAR, an approach of Neural Algorithmic Reasoning following the FordFulkerson algorithm for calculating maximum flow. The key idea is to simultaneously learn the primal and the dual problem which is max-flow and min-cut in this case. This should produce a similar advantage to multi-task training. Experiments on synthetic graphs demonstrate that the proposed method performs better than learning to predict max-flow only. Experiments on real-world Brain Vessel Graphs demonstrate that DAR retains useful knowledge for extracting edge features that advance the performance of edge classification.",
            "strength_and_weaknesses": "Strength\n1 The idea of using duality for neural algorithmic reasoning is intuitive and interesting. \n2 The implementation of DAR for max-flow and min-cut is technically sound.\n3 Experiments on both synthetic data and real-world data are solid and demonstrate the advantage of neural algorithmic reasoning and dual algorithmic reasoning.\n\nWeakness\nThe main concern is that this paper proposes only the idea of DAR and one demonstration of how to implement this idea for max-flow but lacks a general implementation method. It is unclear how difficult it is to set up a DAR model for other algorithms and tasks. Maybe proposing a general implementation is too difficult, more experiments on how to use duality for other algorithms are also welcome and helpful.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written except for one section: In Sec. 4.2, I didn't understand what Algorithm reconstruction does here. What is the expected output from the network and what is the training target and loss function of \"learning steps of algorithm reconstruction\"?\nThe idea of duality algorithmic reasoning is novel to my knowledge.\nThe paper presents sufficient details for reproducing results. ",
            "summary_of_the_review": "This is an interesting paper on neural algorithmic reasoning. The proposed approach is reasonable and achieves decent improvements compared with baselines. I lean toward accepting. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4112/Reviewer_vEcN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4112/Reviewer_vEcN"
        ]
    }
]