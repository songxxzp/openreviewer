[
    {
        "id": "7PAziPZnCv",
        "original": null,
        "number": 1,
        "cdate": 1666410075628,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666410075628,
        "tmdate": 1666410075628,
        "tddate": null,
        "forum": "PXibCVxXdT",
        "replyto": "PXibCVxXdT",
        "invitation": "ICLR.cc/2023/Conference/Paper843/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper extends the Wasserstein autoencoder framework for the unconditional generative model to the setting of having structural constraints e.g., conditional independence on the latent variables (representation, or factors). In greater detail, the authors consider three examples of having additional information to the generator. The first example is the \"M2\" model (Kingma et al., 2014). In this setting, there exists an observed nuisance variable $S$, and the latent variable $Z$ wants to encode the representation invariant to the unwanted variation in $S$. The second example is the extension of the variational fair autoencoder (an extension of the \u201cM1 + M2\u201d semi-supervised model) that has partially observed label variable $Y$ in addition to the nuisance variable  $S$. The final example is the extension of the first example with two independent nuisance variables that can be missing. The main modeling techniques in the paper are factorizing the couplings between the joint observed data variables distribution and the joint model variables distribution by the chain rule and matching the aggregated posterior to the prior (which is used in WAE and is equivalent to Kantorovich optimal transport via the gluing lemma). The ground metric on the joint space is the square root of the square of distances on marginal variables. The authors compare the proposed framework with the HSICconstrained VFAE (HCV) and the FairDisCo in learning fair representation on Adult Income and Health datasets, learning invariant representation on the Extended Yale B dataset, and conditional generation on MNIST and VGGFace2 datasets.",
            "strength_and_weaknesses": "## Strength\n\n* The paper is the first work that extends WAE to the setting of the \"M1\" and \"M2\" models. \n* The experimental results indicate that the proposed framework is better than the previous approach such as variational fair AE and FairDisCo.\n\n## Weaknesses\n\n* The writing is hard to follow. The author can simply replace complicated notations by showing the graphical model of the inference/recognition/encoding model.\n* The chosen ground metric e.g., $\\tilde{d}$ in Example 1 is not a valid metric on the joint space of supports of random variables for generalized choices of  $d$ and $d'$.  Therefore. the final objective is not the \"Wasserstein\" distance. The specific settings of those distances are missing in experiments.\n* The duality gap could be large for the Lagrange duality. The tuning of various Lagrange multipliers is a negative point.  Moreover, the choices of the divergences e.g., $D$ and $H$ in Example 1 are not discussed.\n* The baselines of experiments are not strong. There are more recent works on the same problem e.g., [1],[2]. Also, the performance of the proposed framework is not clearly superior compared to the chosen baselines e.g., Figure 2, Table. 2.\n\n[1] Hierarchical VampPrior Variational Fair Auto-Encoder, Philip Botros and Jakub M. Tomczak\n[2] Flexibly Fair Representation Learning by Disentanglement, Elliot Creager et al.",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: The paper is not easy to follow since it uses a lot of notations.\n* Quality: The paper does not compare the proposed framework with recent frameworks. \n* Novelty: The paper uses directly the techniques from the Wasserstein autoencoder ( WAE) paper.\n* Reproducibility: The code is submitted, and neural network architectures are reported carefully. However, some settings such as the choice of ground metric and divergences seem to be missing.",
            "summary_of_the_review": "The paper should include a graphical model for the inference model (encoder) and compare the proposed framework with recent papers (mentioned in the weaknesses part).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper843/Reviewer_Zcji"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper843/Reviewer_Zcji"
        ]
    },
    {
        "id": "DAFzrL8MQ7",
        "original": null,
        "number": 2,
        "cdate": 1667322447406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667322447406,
        "tmdate": 1667322447406,
        "tddate": null,
        "forum": "PXibCVxXdT",
        "replyto": "PXibCVxXdT",
        "invitation": "ICLR.cc/2023/Conference/Paper843/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors argue that there is a principled way to handle structural constraints on the latent variables in Wasserstein AutoEncoders. They argue that this is in contrast to the VAE where numerous tricks have been employed on the objective function in order to impose similar structural constraints, e.g., the semi-supervised work and beta-VAE.",
            "strength_and_weaknesses": "Thank you for a very interesting read. The research is of high interest to the research community and shows a clear strength of using WAEs. To my knowledge and review, the theory is sound. The experiments are meaningful and provide good empirical evidence for the claims in the paper.\n\nThe weakness from the initial review regards the strong advocation that this is in contrast to VAEs. I agree with the fact that many research papers have employed tricks on the objective function that are far from principled in the VAEs.\n\nRegarding the comparison to VAEs:\n1) Have you, in fact, shown that WFAEs are preferred over the VAE framework? If so, a response in the rebuttal would be much appreciated.\n2) Is the comparison to the VAE papers fair? For example, some of the very deep VAE papers, e.g., \"Very Deep VAEs ...\" by Child et al. or \"BIVA ...\" by Maaloe et al. shows principled approaches to the VAE in cohesion to image generation examples that could suggest that they learn structural constraints on the latent variables. What would happen if you employ a structural constraint on these models only by altering the graphical model (no changes on the objective function) and how would that compare to the WFAE? \n\nRegarding the general empirical findings:\n- It would be useful to see many more examples of conditional generation examples, e.g., figure 5 in \"Autoencoding beyond pixels using a learned similarity metric\" by Larsen et al.\n- What about semi-supervised classification evaluations? Wouldn't it be possible to report on these and compare to state-of-the-art?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read and the quality is high. The novelty is not that high, but the reviewer find the findings of importance and interest to the research community.",
            "summary_of_the_review": "Interesting paper. Concerns can be alleviated through a rebuttal of the comparison to VAEs and by expanding on the empirical finding.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper843/Reviewer_ernN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper843/Reviewer_ernN"
        ]
    },
    {
        "id": "M3gltbDxsV",
        "original": null,
        "number": 3,
        "cdate": 1667520961427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667520961427,
        "tmdate": 1667520961427,
        "tddate": null,
        "forum": "PXibCVxXdT",
        "replyto": "PXibCVxXdT",
        "invitation": "ICLR.cc/2023/Conference/Paper843/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new analysis of Wasserstein Autoencoders and claims the new analysis reveals a learning objective form that naturally can be optimized without the addition of ``ad-hoc\" penalties. The paper describes this anaylsis and presents a full derivation of their construction in three prototypical generative model examples with side or nuisance information. They evaluate their method in a few settings where conditional generative models are deployed.",
            "strength_and_weaknesses": "Strengths\n1) The paper is detailed in its derivation for the numerous examples it presents.\n\n2) The paper does a good job of setting up the problem and describing necessary background in a way that flows nicely with the story of the paper.\n\n3) The notation, analysis, and arguments are typically very clear, and a careful reader would be able to follow most of the discussion with mild background in Wasserstein method and VAEs.\n\nWeaknesses\n1) The paper does not provide a solution for the problem and claim suggested in the abstract or introduction. The authors claim that penalties and constraints can be derived directly from, or induced from, the conditional independence structure of the WAE. The introduction describes a number of prior literature that use a variety of metrics to enforce or push independence, and claim that none of these are a ``principled way ofo imposing the encoder structure\". The end of the preliminaries describes a funtion $\\delta$ that already has a new, arbitrary divergence $\\mathcal{D}$. The core of the paper, the three examples presented, all have additional regularization/constraint terms arbitrarily appended to their objectives, explicitly suggesting they can take the form of an MMD loss, GAN loss, HSIC loss, etc.\n\n2) There is no general algorithmic or analytic procedure presented that might generalize past the three problems described here. The experimental setups shown follow the examples shown, but it's not clear what the examples are providing over the existing methods when the arbitrary regularizers are not added.\n\n3) The experimental evaluation in general is not very convincing. The most important piece in experimental setups similar to this is choosing the regularization weights. This can drastically change the results of all models tested, and no discussion is provided that clarifies how or which weights were chosen. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\nThe writing in general is clear and the analysis is easy to follow. All terms, notations, and steps are well described.\n\nThe main thesis of the paper is hard to follow, especially throughout the main body of the paper in the examples.\n\nQuality\nThe quality of the paper is not up to the typical standard at ICLR. The main claim is not supported by the text, and it's not clear what else is being provided.\n\nNovelty\nThe authors correct a previous theorem in prior work, but otherwise it is not clear what the novel observation and value proposition is.\n\nReproducibility\nCode is provided and with the appendix, seems to be sufficient to replicate the experiments but I have not gone through in detail. ",
            "summary_of_the_review": "It is not clear what the paper is proposing and the main claim is not justified at all. The paper seems to have derived some formulations and then tacked on arbitrary regularizers, completely antithetical to the claims in the abstract and introduction.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper843/Reviewer_4HcR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper843/Reviewer_4HcR"
        ]
    }
]