[
    {
        "id": "CYujY3Y8cdG",
        "original": null,
        "number": 1,
        "cdate": 1666589393505,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589393505,
        "tmdate": 1667211502339,
        "tddate": null,
        "forum": "UaAD-Nu86WX",
        "replyto": "UaAD-Nu86WX",
        "invitation": "ICLR.cc/2023/Conference/Paper2829/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a discrete diffusion model for graph generation by combining discrete diffusion [Austin 2021] and graph transformer [Dwivedi and Bresson 2021]. Its main novelties are:\n - The diffusion model is discrete: sampling of node and edge features is performed after adding noise.\n - The denoising network is a graph transformer. It is permutation equivariant and generalizes to graphs of different sizes (sec 3.3). \n - Since \"having no edge\" is encoded as a categorical edge feature, choosing a uniform noise model does not preserve the sparsity of the graph. Instead, the paper proposes a noise model that preserves the marginal distribution of node and edge types during.\n - Conditional generation is made possible through regressor guidance.",
            "strength_and_weaknesses": "=== Strength ===\n\nThis paper fills a known gap in the literature, between discrete diffusion [Austin 2021] and diffusion for graph data [Niu 2020]. As a result, the approach is well-motivated, and contributes greatly to the literature on deep graph generation. The model is analyzed thoroughly, especially on permutation equivariance and invariance. The ablation studies are thorough: the benefit of discrete (over continuous) diffusion, marginal (over Gaussian) noise, and conditional (over unconditional) generation are well-established.\n\n=== Minor weaknesses ===\n\nWhat is z in the algorithms?\n\nI understand the omission of graph transformer network, but the discussion about sampling after adding noise should not be omitted. After all, it is the main reason why the diffusion model is \"discrete.\"\n\nI would like to see an evaluation on training on small graphs and testing on large graphs, similar to those in \"Learning the Travelling Salesperson Problem Requires Rethinking Generalization\".",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality: I was left confused about how the graph transformer network maintains a discrete graph before checking the code. Other than that, the paper is generally well-written; the literature review is thorough and complete.\nNovelty: Using discrete diffusion in graph generation is technically novel. Conditioning a one-shot generation model on a subgraph is, to the best of my knowledge, new to the literature.\nReproducibility: I briefly checked the code. It appears to be well written.",
            "summary_of_the_review": "The paper is well-written, the contribution is novel, and should be of interest to a broad graph generation and diffusion audience. The method proposed is well-motivated and the empirical evaluation is thorough. I recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2829/Reviewer_7EF5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2829/Reviewer_7EF5"
        ]
    },
    {
        "id": "xd3xF01c8y",
        "original": null,
        "number": 2,
        "cdate": 1666633199165,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633199165,
        "tmdate": 1666633199165,
        "tddate": null,
        "forum": "UaAD-Nu86WX",
        "replyto": "UaAD-Nu86WX",
        "invitation": "ICLR.cc/2023/Conference/Paper2829/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents DiGress, a diffusion model for graph generation. Apart from using graph-based architectures, the key idea is to start with a noise model that preserves the marginal distribution of node and edge types rather than using uniform noise. Then the denoising network is augmented with structural features. Finally, conditional generation is enabled by guidance procedures. Empirical results shows that the performance of DiGress matches the performance of autoregressive models on some molecular datasets.",
            "strength_and_weaknesses": "Strength:\n- The paper is written clearly and the idea behind non-uniform diffusion noise addition makes sense, and is empirically confirmed.\n- The proposed architecture handles node-level, edge-level and graph level features and has some nice properties, such as equivariance, and exchangeability.\n- The diffusion model framework allows for easy use of classifier-based / classifier-free guidance for conditional generation.\n\nWeakness:\n- The method does not deal with growing graphs like in autoregressive models. \n- The sampling procedure is not entirely clear (in algorithm 3). While the distribution $p(G^{t-1} | G^t) p(y | G^{t-1})$ can be sample from Langevin dynamics, it is not sure if that is actually used -- more likely it is similar to the sampling procedure of diffusion models, which would not guarantee that $G^{t-1}$ is sampled from that distribution described above. \n- Some of the experimental results seem odd. In Table 1. the V.U.N. for ConGress is 0% meaning that it generates nothing that is valid, unique and novel graphs. In Table 4, the validity of Congress is 0% and FCD is also 0, which seems quite odd because FCD is better if it is lower (that is also a typo? also KL div is pointed upwards as well).\n- The performance between ConGress and DiGress differ by a lot, which seems odd as usually the discrete case converges to continuous case as number of timesteps goes to infinity.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity (High): the paper is written clearly in general. \n\nQuality (High): The paper has extensive method and experiment sections that are generally well written. \n\nOriginality (High): while training diffusions on categorical data is not exactly new, it is interesting to choose a different forward distribution. This is also well motivated in the context of graph generation. \n\nReproducibility (High): code is provided.",
            "summary_of_the_review": "The method section of the paper is written nicely, but there are some confusions surrounding the experimental sections. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2829/Reviewer_br8Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2829/Reviewer_br8Y"
        ]
    },
    {
        "id": "BpuIpVbc2pL",
        "original": null,
        "number": 3,
        "cdate": 1666975710644,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666975710644,
        "tmdate": 1666975710644,
        "tddate": null,
        "forum": "UaAD-Nu86WX",
        "replyto": "UaAD-Nu86WX",
        "invitation": "ICLR.cc/2023/Conference/Paper2829/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces DiGress, a discrete diffusion-based generative model for graphs. The noise model sequentially corrupts the graph structure by modifying the categories of edges and nodes. The denoising model is a Transformer that leverages additional features. Experiments on graph and (conditional) molecule generation demonstrate the efficacy of DiGress.",
            "strength_and_weaknesses": "Strengths\n- The paper also introduces a continuous version (ConGress) and uses it for ablation purposes, showing that the discrete version produces better results.\n- The proposed method yields good results for general graph generation, outperforming SPECTRE (ICML, 2022).\n\nWeaknesses\n- The proposed method does not advance the current art --- widely used molecule generation methods (e.g., JT-VAE) outperform the proposed approaches.\n- Experiments on conditional generation are somewhat weak, with no extensive comparison with other baselines.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper reads well and tackles a relevant problem (one-shot molecule generation). Nonetheless, the technical novelty is limited as the work follows the framework by Austin et al., (2021), with improvements mainly coming from adopting marginal probabilities as prior and structural features. Regarding reproducibility, the authors have made their code available, and the paper provides sufficient details about the method and its training methodology.\n\nA few questions/comments:\n- It requires some effort to understand some parts of the formulation. For instance, the idea of predicting the clean graph at each step $t$ isn't adequately motivated and doesn't fit into the formulation introduced in Section 2. Also, the paper uses $u$ and $v$ to refer to node and edge distributions, which I found unusual. \n- ZINC represents one of the main benchmarks to evaluate molecule generation. Any specific reason for not considering it? \n- What are the numbers for VUN (valid, unique & novel) for the experiments in Tables 3 and 4?\n- Tables 3 and 4 miss some baselines --- e.g., GraphAF (https://arxiv.org/pdf/2001.09382.pdf).\n- While the paper shows the idea is promising, the results are not strong. For instance, in Table 3, JT-VAE (a contribution from 2018) outperforms DiGress and ConGress. Also, LSTM on SMILES representation (used in the REINVENT tool, for instance) beats the proposal. - - Thus, the paper doesn't seem to advance the current art.\n- Could the same additional features be used to boost the performance of the baselines?\n- Experiments on conditional generation are weak. What are the current SOTA methods for these tasks? How do the proposed methods perform on different properties?\n- Typo: $G^t$ instead of $G_t$ in Algorithm 1.",
            "summary_of_the_review": "Although I have concerns regarding clarity and performance gains over current art, this is an interesting contribution to one-shot molecular generation. Thus, I am inclined to accept the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2829/Reviewer_SeWu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2829/Reviewer_SeWu"
        ]
    }
]