[
    {
        "id": "AnFSos09Xk",
        "original": null,
        "number": 1,
        "cdate": 1666067577523,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666067577523,
        "tmdate": 1666067577523,
        "tddate": null,
        "forum": "WK22pk7bSFR",
        "replyto": "WK22pk7bSFR",
        "invitation": "ICLR.cc/2023/Conference/Paper2679/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method based on minimax risk classifiers, termed LMRCs, for lifelong learning with time-dependent classification tasks. The LMRCs exploits forward and backward knowledge transfer based on underlying Gaussian assumptions, with perhaps a frozen pre-trained feature extractor. Theoretical analyses and empirical experimental results are presented.",
            "strength_and_weaknesses": "Strength.\n\n(1) The paper is well-written.\n\n(2) The presented theorems and derivations seem to be solid.\n\nWeaknesses.\n\n(1) The related work and the baseline methods in the experiments are kind of outdated.\n\n(2) It seems the presented method assumes one can access all $\\tau_i, i \\in [1,j+d]$ when dealing with the $j$-th task.\n\n(3) It seems the presented method is presented based on a frozen pre-trained feature extractor and simple Gaussian assumptions (\\ie Eqs (5-6) and (10-11) from the Kalman filtering).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written in general. However, the underlying logic may not be clear to some audiences; it's not easy to figure out the big picture. Because of that clarity issue and the lacking of comparisons with existing methods, it's hard to evaluate the novelty. The reproducibility is satisfactory.",
            "summary_of_the_review": "There are so many lifelong learning papers in recent years; please make comparisons and discuss the advantages of the proposed method.  \n\nHow to train the feature mapping $\\Phi(x,y)$ with the presented LMRCs? Is $\\Phi(x,y)$ assumed known/frozen?\n\nIn the paragraph before Eq (3), what's the 0-1-loss? Does that mean you assume 2-category classification tasks throughout the paper?\n\nIn Eqs (4-6), the derivations are based on Gaussian assumptions, right? How do those assumptions affect the forward/backward knowledge transfer in lifelong learning?\n\nIn the experiments, the baseline methods are kind of outdated.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2679/Reviewer_vTda"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2679/Reviewer_vTda"
        ]
    },
    {
        "id": "hjnkcQ9PGj",
        "original": null,
        "number": 2,
        "cdate": 1666754039267,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666754039267,
        "tmdate": 1666754039267,
        "tddate": null,
        "forum": "WK22pk7bSFR",
        "replyto": "WK22pk7bSFR",
        "invitation": "ICLR.cc/2023/Conference/Paper2679/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the author(s) used minimax risk classifiers to propose a lifelong method that utilized forward and backward learning for time-dependent tasks. Through Empirical experiments, the author(s) tried to show the advantage of the proposed algorithm.\n\n---",
            "strength_and_weaknesses": "### Strength:\n\n1. Trying to address the issues of catastrophic forgetting and i.i.d. constraint in current lifelong learning.\n\n2. Providing theoretical guarantees for the proposed algorithm.\n\n3. Performing empirical experiments to validate the performance of the proposed algorithm.\n\n---\n\n### Weaknesses:\n\n1. The theoretical analysis of the proposed algorithm, such as generalization bounds, may require further empirical experiments.\n\n2. Some descriptions in this paper are not so clear.\n\nFor more details, please see the section of \"Clarity, Quality, Novelty And Reproducibility\".\n\n---",
            "clarity,_quality,_novelty_and_reproducibility": "I have the following comments/questions. I look forward to the response/clarification from the author(s). Thanks.\n\n1. For a better understanding of the theoretical analyses, such as Theorems 1 and 2, some empirical experiments may be needed for further verification. In addition, are these bounds tight?\n\n2. The theoretical analyses in this paper only hold when the loss function is 0-1 loss, right? Or can it be applied to more general or arbitrary loss functions? \n\n\nIn addition, some tiny issues/typos:\n\n(1) The first line below Eq. (1), \"$\\ell$(h,p) denotes the expected loss of classification...\". To be consistent with the above, here it should be \"error probability\", right?\n\n(2) Some abbreviations and full names appear many times throughout the main text. This is not necessary.\n\n(3) The format of the references is inconsistent. Please check carefully and correct it.\n\n---\n\n\n",
            "summary_of_the_review": "I think the topic in this paper is important and this paper is generally well-written; However, there are some unclear aspects in the paper. Maybe it needs the author(s) to clarify them. Thanks.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2679/Reviewer_KxHd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2679/Reviewer_KxHd"
        ]
    },
    {
        "id": "a4SjEnLseS",
        "original": null,
        "number": 3,
        "cdate": 1667199633102,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667199633102,
        "tmdate": 1667199748416,
        "tddate": null,
        "forum": "WK22pk7bSFR",
        "replyto": "WK22pk7bSFR",
        "invitation": "ICLR.cc/2023/Conference/Paper2679/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles lifelong learning by improve effective sample size using both forward and backward tasks. This paper assumes that tasks arrive consecutively non-iid but satisfying a martingale assumption. This paper adopts the framework minimax risk classification into the lifelong learning setting. After showing the improvement in effective sample size theoretically, the paper presents numerical experiments to demonstrate the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "Strength: This paper presents an interesting framework for lifelong learning, i.e., the lifelong minimax risk classifiers (LMRCs), and the proposed method is novel to me. The theoretical analysis seems solid. \n\nWeakness: I'm concerned about a few settings and assumptions where more justification is needed. \n1. The assumption of $p_{i+1}-p_i$ being independent and zero-mean is only justified by a sentence. However, the validness of such assumption should be discussed more in my opinion, since it is an important assumption to the proposed method. For example, are there any real-life settings where this assumption may hold true (or even approximately being true).\n2. The proposed method relies on the feature map $\\\\Phi$ which is not learnable. However, one would argue that without learning the feature map it is not really doing a proper lifelong learning. \n3. Instead of learning the feature map, the proposed method learns the uncertainty set. How would one know if the leaned uncertainty set is a good one? For example, will the uncertainty set be too big such that the maximization part overpowers the minimization part, and the resulting classifier is not really meaningful? Alternatively, does the method provide a bound on the regular loss function for each task?",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and clarity of the paper are good but could be improved. For example,\n1. I would suggest writing the assumption of $p_{i+1}-p_i$ being independent and zero-mean with formal statement. For example, is the zero-mean of it the measure that maps all sigma-algebra to zero? \n2. At the beginning of section 3.2, it says \"we denote by $R^\\\\infty_j$ the smallest minimax risk \". How is the \"smallest minimax risk\" defined, e.g., under which uncertainty set?\n\nThe proposed method and its analysis are novel in the sense that the authors adopt minimax risk classifiers (MRC) [Mazuelas et al.], ",
            "summary_of_the_review": "Although the paper is interesting and somewhat novel, I am concerned about a few settings used/assumed in the paper, as detailed in the weakness section. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2679/Reviewer_Q8gE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2679/Reviewer_Q8gE"
        ]
    }
]