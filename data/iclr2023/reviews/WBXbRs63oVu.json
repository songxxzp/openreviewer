[
    {
        "id": "SI4_w9a1st",
        "original": null,
        "number": 1,
        "cdate": 1666247102986,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666247102986,
        "tmdate": 1666247102986,
        "tddate": null,
        "forum": "WBXbRs63oVu",
        "replyto": "WBXbRs63oVu",
        "invitation": "ICLR.cc/2023/Conference/Paper5818/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an algorithm to leverage LLM's rational through prompt, to improve final downstream models' performance by taking the computation into account. Specifically, to save human efforts, the authors propose to use LLM to generate rationals. It then train a QA model based by giving the generated rationals. To alleviate the shortcut, they further propose a counterfactual regularization to encourage the model to learn to use the rational instead of learning the shortcut. ",
            "strength_and_weaknesses": "* The main strength of the paper is paying attention to the computational budget, which is one motivation behind the algorithm. I believe it will become an important direction as the pretrained models are getting larger and larger. \n\n* The algorithm design is simple. On the other hand, the non-trivial part is the potential shortcut issue, and the authors propose a counterfactual regularization to resolve it. \n\n### Questions\n\n* Although I understand one argument of the paper is computation efficiency, I'm still curious about what is the performance of the fine-tuned LLM on the benchmarks. The used LLMs are still \"relatively\" small compared with GPT-3 and PaLM. It would be nice to have these numbers to serve an upper bound.\n\n* For the OOD results, the proposed algorithm leverage the rational from the LLM. However, it has been demonstrated that LLM has strong zero-shot performance as well as great generalization capability.  Therefore, regardless the training data it used, when testing on the different dataset, as long as the rational provided by LLM is faithful, isn't it expected that any model built on top of it can generalize well?  In this case, is it fair to argue the proposed model has good generalization capability? or it's  actually  benefitted by LLM's good generalization capability?\n\n* Question about robustness of the perturbed data.  When the rational is wrong, the model can still make right decision. Although the authors try to argue it's not a contradictory results, it does sound contradictory to me that, if the model is encouraged to take the rational into account when making decision, how can it not be misled if we feed a wrong rational? if it can be robust, isn't it not leveraging rationals when making decisions?  Could the author comment more on it? \n\n* The rational qualities are also known sensitive to the prompt design. How does it affect the final algorithm performance?",
            "clarity,_quality,_novelty_and_reproducibility": "* Novelty: see bullet 1 above\n* Quality: the paper is well written and the the experiments are thorough.\n* Reproducibility: the code is provided",
            "summary_of_the_review": "The paper focus on a computational efficient algorithm by using LLM, which seems important to me. The overall quality is good as aforementioned. There are some questions not clear to me, but overall I think it's a good paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5818/Reviewer_WxQk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5818/Reviewer_WxQk"
        ]
    },
    {
        "id": "VNv7kyx8Ylf",
        "original": null,
        "number": 2,
        "cdate": 1666586772989,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586772989,
        "tmdate": 1670702634331,
        "tddate": null,
        "forum": "WBXbRs63oVu",
        "replyto": "WBXbRs63oVu",
        "invitation": "ICLR.cc/2023/Conference/Paper5818/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors, presents PINTO a two stage pipeline to improve rationale-based language reasoning. The first step is to prompt a language model (20B in this case) to generate rationale given a question, while the second step is to fine tune a smaller (<1B) LM with the pair (Question, generated rationale) -> Answer.\n\nThe authors experimented PINTO on the CSR benchmarks (CommonsenseQA, StrategyQA, OpenBookQA, QASC). The results show that PINTO works better across the four dataset when compared to other baseline settings. The authors also reports interesting ablations and case studies. ",
            "strength_and_weaknesses": "Weaknesses\nIt's hard to grasp the novelty and the contribution of this paper. For instance, the main message, or at least if I understood this paper correctly, is that mid size LM (20B) can be use to generate rationale (which per se is not novel) and the by fine tuning on it a smaller model the performance are \"close\" to simple baselines (which are not SOTA nor this is novel per se). Some detailed points:\n- Prompted Self-Rationalization uses larger model to solve the task directly, however the model is very large. Then why not use those generation to train a smaller model as in [1,2] directly? \n- Based on the provided analysis that  more faithful rationales can improve the performance, what is the quality of the generated rationales? is there a small scale human eval to quantify this. \n\n\n[1] https://arxiv.org/abs/2110.07178\n[2] https://arxiv.org/pdf/2104.08826.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is not very clear on the contribution and something misses baseline in the main table (e.g. Table 2, some results are in the caption of the table). More over the name of the baselines (e.g., Standard or Dropout) are very hard to follow. \nQuality: the paper has some merit. \nNovelty: not very novel (see weakness). \nReproducibility: with some effort, but it's reproducible.",
            "summary_of_the_review": "The paper lack of a major contribution and major focus. After the discussion between the reviewers, I decide to keep the increase my score (post-rebuttal), to 5.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5818/Reviewer_gqDU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5818/Reviewer_gqDU"
        ]
    },
    {
        "id": "FKYV7Q8aaI",
        "original": null,
        "number": 3,
        "cdate": 1667453402246,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667453402246,
        "tmdate": 1669848571802,
        "tddate": null,
        "forum": "WBXbRs63oVu",
        "replyto": "WBXbRs63oVu",
        "invitation": "ICLR.cc/2023/Conference/Paper5818/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an LM pipeline that first generate free-text rationales and use the generated rationales to fine-tune a reasoning module such that it makes the prediction that relies on the rationale as much as possible. A regularization scheme is proposed to mitigate issue where rationales might be ignored.\n\nThe method is evaluated on in-domain and out-of-domain setups using accuracy and faithfulness as metrics.",
            "strength_and_weaknesses": "Strengths:\n\n- The method is simple, sound, and makes intuitive sense. To my knowledge, this is the first work to\n- The general evaluation setup is largely appropriate such as choice of datasets, ID vs OOD evaluation and measuring faithulness\n\nWeaknesses:\n\n- The current formulation only allows the method to be applied to multi-choice classification problem or where the classification target space is not too large\n- The baselines are not on the same parameter scale especially the w/o Rationalization baseline. It is unclear whether the performance gain is coming from using a strong rationale LM or that this framework in-general is better than the other options. (This might explain why ID even increases.)\n- One way control for the issue above might be using rationale modules of different sizes, and use a small LM (<1B) to fine-tune on the rationales generated by these different modules, and eventually stack it with the proposed PINTO training procedure. At the very least it might be a better comparison to the Fine-tuned self-rationalization baseline.\n- It might be a stretch to draw any conclusion from Figure 4 as mentioned in section 5.2 due to the high variance and lack of trend\n- The issue of answer leakage is mentioned in section 3, I am not sure how the proposed framework circumvent such an issue. With PINTO, the rationale can still contain answer information right? (Please correct me if I misunderstood.)\n- Would be nice to have variance statistics",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well written, appropriately organized, and easy to follow\n- The method is novel as far as I am aware\n- The method is straightforward and practical\n- The general quality is good, but some limitations remain (will increase the scores if the authors can address them)",
            "summary_of_the_review": "The paper presents a practical method to solve multi-choice classification problem using rationales. The paper is well-written and provides a new way of incorporating rationales to improve OOD performance. Despite obvious limitations and some issues in how the baselines are constructed, there is no major flaws.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5818/Reviewer_mr1Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5818/Reviewer_mr1Y"
        ]
    },
    {
        "id": "RcuqlSkK5H",
        "original": null,
        "number": 4,
        "cdate": 1667795918328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667795918328,
        "tmdate": 1669953268683,
        "tddate": null,
        "forum": "WBXbRs63oVu",
        "replyto": "WBXbRs63oVu",
        "invitation": "ICLR.cc/2023/Conference/Paper5818/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a pipelined method for reasoning tasks with language models. The proposed pipeline has two parts: a rational generation module and a reasoning module. The proposed method has two major novelties: 1) the rationale generation uses a frozen LM with few-shot prompts. 2) The reasoning module uses regularizations to improve the faithfulness of utilizing the rationales.\n\nThe paper compares the proposed method with several baseline methods and shows that the proposed model achieves better performance on 4 QA tasks which require external knowledge. They also test the out-of-distribution performance and robustness of noisy rationales of their proposed method.",
            "strength_and_weaknesses": "Strength:\n- The paper proposes a novel LM pipeline that rationalizes with prompt-based learning and reasons via counterfactual regularization.\n- The empirical results show the advantage of the design choice: 1) pipeline-based model over unified model and 2) regularization method for reasoning module\n- The paper is well-written\n\nWeakness:\n- The evaluation is only about the proposed method and its own variants (baselines). But the performance is much lower than the state-of-the-art results. For instance, the best performing CSQA system has accuracy around 80% (https://www.tau-nlp.sites.tau.ac.il/csqa-leaderboard2), and the proposed method only achieves 60%. Of course, the best systems use additional knowledge sources with careful task-specific tuning. But the gap makes me wonder if the proposed method could help upon the state-of-the-art system.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is well-written, and I can understand technical details.\n- Quality: The paper is solid with clear motivation and detailed empirical results.\n- Novelty: The paper is novel for its regularization on reasoning module.\n- Reproducibility: Some hyperparameters are missing but one should be able to reproduce with some effort.",
            "summary_of_the_review": "The paper proposes an interesting method combining LMs with different capacities for open-domain QA tasks. The first LM generates answer-specific rationales for the second LM to reason about. Two regularization objectives are proposed to improve faithfulness. The ablation study on 4 widely used benchmark shows promising performance of the proposed method. However, the performance is far away from the state-of-the-art system for the corresponding tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5818/Reviewer_8SKc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5818/Reviewer_8SKc"
        ]
    }
]