[
    {
        "id": "Opxh-o3m5CP",
        "original": null,
        "number": 1,
        "cdate": 1666652248002,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652248002,
        "tmdate": 1666652248002,
        "tddate": null,
        "forum": "brk7Ct4Tb1M",
        "replyto": "brk7Ct4Tb1M",
        "invitation": "ICLR.cc/2023/Conference/Paper4672/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on inverse protein folding, a fundamental and challenging problem in protein science. The authors propose a new regularization term by including the trained AlphaFold model and its predicted structural confidence metric. To improve the efficiency of the model optimization loop, they distill the AlphaFold into a smaller model. They perform extensive experiments with different protein design models to show generality of the regularization trick. Experimental results show that the model with regularization achieves better sequence recovery rate and large improvement in protein diversity, while preserving the structural consistency.",
            "strength_and_weaknesses": "Strength:\n1. Address an important problem with a well-motivated method.\n2. Simple techniques with impressive results, especially in protein diversity.\n\nWeakness:\n1. Lack of technical novelty. The models and distillation methods are adapted from existing works without non-trivial modifications.\n2. Lack of related work on knowledge distillation. \n3. The presentation is not friendly to readers that are unfamiliar with inverse protein design.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is kind of hard to follow for general machine learning researchers. Considering this is machine learning conference, I suggest authors to significantly improve the presentation to make it more friendly to general ML audience. See details below.\n1. The main paper starts by discussing how to distill AlphaFold model. The audience of the ICLR conference is mostly machine learning researchers, who may not be familiar with proteins. I\u2019d like to suggest that the authors add a preliminary section to introduce the basic concepts of proteins, the problem definition, and high-level ideas of related methods.\n2. The authors mix the method section with the experimental setup and results. This makes the paper more like a technical report that requires readers\u2019 knowledge of the task and datasets.\n3. There are 10 figures and 2 tables in the main paper. The readers need to switch between different pages for finding results with similar meanings. This issue can be solved by better illustration. For example, the contents of Fig. 7,9(,10) can be organized in one table.\n\nQuality is pretty high with extensive experiments and detailed methodology.\n\nFor novelty, I feel like the paper's idea is not surprising to me. The idea of distilling Alphafold into inverse folding is somewhat new, but the ML components like the distillation method and network implementation are all established methods.\n\nThe author promised to provide the source code upon acceptance. I suppose reproducibility should not be a problem.",
            "summary_of_the_review": "A method distilling Alphafold for inverse folding, which is an important topic for computational protein design. The idea of distilling the advanced protein structure prediction model for inverse folding might be interesting to the specific \"ML for protein\" audience, while generally the ML models and methods are all just existing works. Experiments are sound and comprehensive.\n\nQuestions:\n1. It\u2019s interesting to see that such a regularization term can increase sequence diversity. Could the authors provide some explanations and in-depth analysis of this phenomenon? \n\nMinor points:\n1. In the caption of Fig.3. Line 5: TM/LDTT -> TM/LDDT\n2. In Fig. 7,9,10. The cyan bar is overlapped with the purple bar, which makes the color similar to the blue bar. Since the baseline value is the same for all methods in one figure, I suggest using a dashed red line rather than a cyan bar.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4672/Reviewer_Y6Az"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4672/Reviewer_Y6Az"
        ]
    },
    {
        "id": "Y_AgqjjiS1",
        "original": null,
        "number": 2,
        "cdate": 1666806834542,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666806834542,
        "tmdate": 1666807020717,
        "tddate": null,
        "forum": "brk7Ct4Tb1M",
        "replyto": "brk7Ct4Tb1M",
        "invitation": "ICLR.cc/2023/Conference/Paper4672/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposed a knowledge distillation module for protein inverse folding, i.e., generating a protein sequence by its 3D structure. The proposed system recovers more diverse amino acid chains with a lower perplexity and higher recovery rate efficiently.\n \n\n \n",
            "strength_and_weaknesses": "**Strengths:**\n\n1. The paper overall is easy to follow, and the model construction is clearly introduced.\n\n2. According to the experimental results reported in the main paper, the main claimed contributions of \u201cefficiently interpreting the protein sequence\u201d and \u201cappliable to other tasks (protein infilling)\u201d are supported with evidence (although this evidence is flawed, see weaknesses).\n \n**Weaknesses:**\n\n1. The system design can be better motivated. In particular, why it is essential to infer protein structure from the predicted sequence for an inverse folding problem? Does it follow some convention or some specific empirical meaning?\n\n2. It is questionable whether the comparison of 'the efficiency' of the proposed method against baselines is fundamentally fair. It seems the only (in)direct comparison is in the left plot of figure 2, where the inference time is compared against different folding models. However, if the reviewer understands correctly, the distillation model itself requires running AlphaFold2 to obtain the predicted labels (which are later used for training). In other words, Figure 2 actually compares a fraction of the proposed model with the entire competitor model.\n\n3. The comparison of computational efficiency is incomplete. While the authors report the inference speed with a chain of 500 amino acids, it is not clear if the proposed method also performs swift inference speed in large-scale protein sequences. As the chain length can go beyond thousands of amino acids, the practical importance of the proposed model is questionable. While the authors claimed \u201ctruncated proteins up to length 500 to reduce computational complexity\u201d (above Figure 4), other language models (such as ESM-IF1) work well on amino acid chains of up to 1024 tokens.\n\n4. More experimental results should be included (in the main text) to support the merit of the proposed method. For instance, as the designed module mainly solves the inverse folding problem, at least some specific baselines should be compared, such as ESM-1v and ESM-IF1. Meanwhile, the designed computational blocks should be investigated with detailed ablation studies. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Minor Questions/Problems:**\n\nSection 2: Jumper et al\u2019s work should be named AlphaFold2, instead of AlphaFold.\n\nSection 3: \u201cTraditionally, the distillation would be done using soft labels\u2026\u201d. reference(s) is missing to support \u2018traditionally\u2019.\n\nSection 3: \u201cwe do not use the probabilities as they are harder to collect\u2026\u201d. it is unclear that the probabilities are harder than what to collect?\n\nSection 3: \u201chard labels\u2026computed based on AlphaFold\u2019s predicted 3D structure\u201d. It is very confusing: in the previous line a hard label refers to **ground truth** classes, and it is not clear why AlphaFold\u2019s **predicted** 3D structures are considered \u2018hard labels\u2019.\n\nFigure 2: what is the meaning of \u2018Logits\u2019 in the upper purple box? Does it refer to a softmax activation function? Or a Logistic regression?\n\nSection 3.3: \u201cvocabulary size to 50, corresponding to discretizing\u2026\u201d. What is the relationship between the vocabulary size and pTM/pLDDT range? (as the authors used the word \u2018corresponding to\u2019)",
            "summary_of_the_review": "The submission is not sufficient to be published in ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4672/Reviewer_jZLG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4672/Reviewer_jZLG"
        ]
    },
    {
        "id": "SJlPjn30CO",
        "original": null,
        "number": 3,
        "cdate": 1667249565116,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667249565116,
        "tmdate": 1670358447651,
        "tddate": null,
        "forum": "brk7Ct4Tb1M",
        "replyto": "brk7Ct4Tb1M",
        "invitation": "ICLR.cc/2023/Conference/Paper4672/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper mainly tackles the problem of protein design through training of an inverse folding model. The procedure involves using a protein folding model such as AlphaFold as a consistency regularizer along with the sequence generation task. To make such learning feasible in terms of computational efficiency and training time, it distills AlphaFold folding model into a smaller, more efficient, but similarly-accurate BERT-like transformer model.",
            "strength_and_weaknesses": "+ The main idea is a simplistic distillation of the AlphaFold model to be used as a consistency regularization for other protein modeling tasks.\n+ The papers shows that such as simple idea works in drastically improving efficiency of the folding model and the cosistency regulariztion is clearly effective for multiple tasks.\n+ The correlation of the distilled model's scores seem to be acceptably high with respect to the original AF's scores.\n+ The general performance on protein design seems to significantly improve over the recent baselines in terms of ground-truth sequence likelihood and amino-acids recovery.\n+ Different types of inverse folding backbones is used in conjunction with the consistency loss to demonstrate the universality of its usefulness.\n+ Interestingly, the diversity of the obtained well-structured sequences is improved over the baselines.\n+ Results on (structure-preserving) sequence infilling is also encouraging.\n\n\nSome questions:\n- how does the distilled model perform if trained from scratch and not from a pre-trained language model? \n- can the fact that the model is pre-trained as a self-supervised language model suggest that it might have memorized some sequences that it has to generate on the validation set?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the experimental setup is well described.",
            "summary_of_the_review": "The paper proposes a simple consistency regularization method for protein modeling, with the most focus on protein design. Such a simple setup achieves encouraging results which significantly improve over recent baselines on important tasks and aspects. Conditioned on a satisfying discussion around my questions above I find the paper a clearly well-performed application paper on an important topic of relatively-wide interest.\n\n-------- post rebuttal ---------\nI read the other reviews and the authors' response. I believe the paper has clear merits for publication as the approach is reasonable and novel, the diversity of baselines are increased with the new SC mechanism while mostly maintaining the recovery rate, and the efficiency of distilling model for AF2 scores.   ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4672/Reviewer_LQja"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4672/Reviewer_LQja"
        ]
    },
    {
        "id": "SIaVIrcRVSl",
        "original": null,
        "number": 4,
        "cdate": 1667281287399,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667281287399,
        "tmdate": 1667282233821,
        "tddate": null,
        "forum": "brk7Ct4Tb1M",
        "replyto": "brk7Ct4Tb1M",
        "invitation": "ICLR.cc/2023/Conference/Paper4672/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries to incorporate forward folding/structure prediction models, i.g., AlphaFold 2 in particular, to provide informative feedback for inverse folding models during training. One of the challenges here is forward folding models are usually too large and computationally expensive to be integrated into the learning framework. To tackle this, this paper proposes to distill AF2 by a ProtBert, a Transformer based language model, in terms of AF2\u2019s confidence metrics, i.e., pLDDT and pTM scores. After distillation, the distilled score model is used to provide signals regarding structural consistency for learning inverse folding models. Experiments on fixed backbone design (CATH 4.2) and antibody CDR infilling (SabDab) show the effectiveness of the proposed approach. \n\n",
            "strength_and_weaknesses": "Overall, the motivation for incorporating folding models to improve the structural fidelity of inverse folding models is straightforward, and resorting to knowledge distillation makes sense. However, my major concerns lie in both the model and evaluation parts. Please refer to the questions for more details.\n\nQuestions:\n\n1. The baseline models, GVP, are weak. It remains unclear whether the proposed idea could bring consistent improvements when strong inverse folding models are considered, such as protein mpnn?\n2.  Since additional 907k AF2 predicted proteins are used to build the distilled model, a simple baseline in which all of these predicted data are directly used for inverse folding training should be carefully considered and compared. What\u2019s more, does the proposed approach, which requires significant additional overhead in terms of both distillation and regularized training, complement with a simple data augmentation approach? This important question remains unclear. \n3. Comparisons to the state of the arts methods are missing, e.g., ESM-IF (Hsu+ ICML 2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "n/a",
            "summary_of_the_review": "-",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4672/Reviewer_QCkw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4672/Reviewer_QCkw"
        ]
    }
]