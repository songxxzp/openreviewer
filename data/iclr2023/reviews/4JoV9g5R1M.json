[
    {
        "id": "w51CVKRp9dJ",
        "original": null,
        "number": 1,
        "cdate": 1666620332501,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620332501,
        "tmdate": 1666620332501,
        "tddate": null,
        "forum": "4JoV9g5R1M",
        "replyto": "4JoV9g5R1M",
        "invitation": "ICLR.cc/2023/Conference/Paper2443/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is concerned with emergent communication: How pairs of agents (neural networks in this case), develop joint symbols for communicating information, and is in particular concerned with the compositionality of those symbols. \n\nThe hypothesis of this paper is that the mechanism of attention, similar to the one popularised in transformers, lead to more compositionality in the emergent language. \n\nThe experimental setup in this paper is based on the \"referential game\", where one agent(\"speaker\") sees an object and need to convey this to the second (\"listener\"). The listener must then pick the correct object from a set of distractors. The speaker is implemented as a pre-trained CNN, the listener as a Transformer decoder. Experiments are run in a simple one-hot game and on inputs based on pairs of garments taken from the fashion MNIST dataset. \n\nThe experimental results are: \n- the use of attention lead to better performance in the game\n- the use of attention lead to more compositionality in the emergent language",
            "strength_and_weaknesses": "### Strengths\n- The paper is generally clear and pleasant to read. \n- The question of language emergence and compositionality of codes in particular is an interesting and relevant one. \n- The experimental setup is well described and clear. \n\n### Weaknesses\n- It remain unclear to me whether the chosen experimental design is well suited to the research question. \n\t- First, the FashionMNIST scenario is only based on identifying pairs of items, there are no properties attached to the items, so it is not clear what type of compositionality we should expect.\n\t- Second, the message length is 2, therefore it is unclear how compositionality really comes into play. The argument is made on the pre-message dimensions of the attention model, but is that really the language in this experiment? This could be made clearer. \n- The attention mechanism used in transformers, if very successful in neural networks, are a very specific form of attention, there are other. This is not conveyed in the related work section. I would have liked some more discussion on the specific properties of this type of attention and why it is particularly suited for the emergence of compositionality. \n- The discussion of the results could be clearer: The results seem to show that attention increases compositionality (according to the chosen metrics), but also that attention generally improves performance at the task across the board. Is it the case that increased attention and therefore the benefit of attention is due to increased compositionality, or does compositionality increases due to better performance? I am not sure this question is answered by the experimental results. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear, although some of the arguments could have been more developed in my view. For example, it is not made clear why compositionality is a relevant property *in this case* (I agree that this is an important property of language in general). Why should it emerge at all in this experimental setup: As I understand only identity is to be conveyed, but no attribute. \n\nThe novelty of the paper resides in its scientific question, namely that transformer-like attention mechanism induces more compositionality in emergent language. The technical content of the paper is well known and studied. I cannot comment on the novelty of this question, as I do not have an in-depth knowledge of compositionality in language. \n\nThe authors make clear that the source code will be made available on acceptance. Although all data is drawn from the publically-available Fashion-MNIST it would be important that the code to generate the exact same compositions of garments would be provided. \n\np.4 positioanl should be positional",
            "summary_of_the_review": "In summary, this is a well written paper, tackling an interesting and important question, namely compositionality in emergent language. \n\nThe paper tries to establish the effect/importance of transformer-like attention in compositionality, but it is unclear to me that the results really answer this question, beyond showing the expected result that attention improves performance. \n\nI have some concerns that the task may be too simple to effectively study the emergence of compositionality, as it may indeed not be needed at all. \n\nThe paper would also gain from a more in-depth discussion of the different mechanisms and approaches to attention and rationales why they would affect the emergence of compositionality. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2443/Reviewer_kyNy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2443/Reviewer_kyNy"
        ]
    },
    {
        "id": "sXdUXE0_yL-",
        "original": null,
        "number": 2,
        "cdate": 1666627199864,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627199864,
        "tmdate": 1666627199864,
        "tddate": null,
        "forum": "4JoV9g5R1M",
        "replyto": "4JoV9g5R1M",
        "invitation": "ICLR.cc/2023/Conference/Paper2443/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper explores how attention may impact language emergence in two instances of the Lewis Game setting.\nThe authors observe that transformers seem to introduce an inductive bias toward compositionality.\nThey then look at the alignment between concepts and attention weights.\n",
            "strength_and_weaknesses": "First of all, the authors point out a relevant fact: the language emergent models mostly remain stuck to RNN, while transformers were a game-changer in the Language modeling community. Therefore, updating the EmeCom modeling with transformers is worth exploring.\nHowever, I think that the paper only scratches the surface of the problem:\n - The transformer is 1-attention layers, which are merely basic memory networks. In this case, I would also add an RNN with attention.\n - there is no actual discussions about the difference transformer impact on language (depth, encoder/decoder, decoder-only). This would be by far the most exciting part for me\n - the task is extremely toyish, while transformers shine when complexity increase. For instance, a message length of 2 prevents advanced compositionality,\n - there is no clear disentanglement between the vision-transformer and language transformer in the paper, while compositionality may come from different input\n\nAs a result, my main takeaway from the paper is that the transformer (single-attention layers) outperforms RNN-based system. Yet, even this point must be taken with caution due to the lack of extensive architecture search. \n\nFor instance, a simple idea to increase paper generability: Given different X tasks of increasing difficulty (few attributes with many values or vice-versa) or with different settings (small/long message length), I would explore different combinations of neural architecture (RNN, RNN+attention, different transformers) where network capacity is fixed. Then, if an attention-based network consistently outperform RNN based model (acc/topsim), then it is an interesting outcome.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well-written. Actually, I would encourage the authors to make it more concise. It would allow more space to perform more experiments and ablation to nail the research questions.\nThe experiments are interesting but tend to be too narrow. In Emecom, a simple change of hyperparameters may drastically change results, and there were no attempts to try to refute the outcome. Besides, \nMore importantly, the paper focuses on model architecture... but never fully describes the model itself (only mentioning  Vaswani2017 is not enough when the core topic of the paper is transformer). Therefore, it greatly lowers paper reproducibility.\nSimilarly, a few details are missing on the training (number of images, batch size, etc.)\n\nSmall remarks:\n- The fashion task description is not very clear. I would also refer to [1] to illustrate the task too\n- Figure 5 is nice! This was really an excellent idea. To ease readability, I would recommend plotting cumulative normalized bars instead of overlapping bars\n\nFinally, I would recommend the authors also include some work on transformer compositionality in natural language or images. Quick search provides many references [2-4] \n \n[1] https://proceedings.neurips.cc/paper/2021/file/9597353e41e6957b5e7aa79214fcb256-Paper.pdf\n[2] https://aclanthology.org/2022.acl-long.251.pdf\n[3] https://vigilworkshop.github.io/static/papers-2019/43.pdf\n[4] https://arxiv.org/abs/2111.08960\n ",
            "summary_of_the_review": "While I value the initial research direction and some effort in analyzing the results (e.g., Fig5), the paper does not fully explore how transformers may impact the emergent language. I would recommend iterating over diverse tasks and architectures to discover potential correlations. Note that this does not require extensive computation if the network remains below a few million parameters on a simple one-hot or visual task. \nFurthermore, I am missing reproducibility points.\nTherefore, I cannot recommend paper acceptance in its current state. Yet, I recommend the authors to continue their research work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2443/Reviewer_gZp6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2443/Reviewer_gZp6"
        ]
    },
    {
        "id": "88Z5QM2TqNg",
        "original": null,
        "number": 3,
        "cdate": 1666695615553,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695615553,
        "tmdate": 1666695615553,
        "tddate": null,
        "forum": "4JoV9g5R1M",
        "replyto": "4JoV9g5R1M",
        "invitation": "ICLR.cc/2023/Conference/Paper2443/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies languages that emerge when two agents (Speaker and Listener) communicate to solve a referential game. The authors propose to endow the agents with attention mechanisms to create a pressure for learning more compositional emergent languages. The experimental study shows that indeed changing the architectures in this way results into increases in the compositionality metrics (topographic similarity, pos-dis, and bos-dis). The authors also provide interesting insights in the behaviour of the agents by analysing the emerging attention patterns.",
            "strength_and_weaknesses": "Strengths:\n* Generally the paper is well-written and has a good literature review. \n* The results (Table 1) show a significant improvement over the baseline and the interpretability section (section 4.3) is interesting, even if the difference between the failure and success settings is not significant.\n\nWeaknesses:\n* The main novelty of this paper is agents\u2019 architecture (section 2.2). However, this attention mechanism is not sufficiently explained. \nThe authors state that the query is the symbol of the message and the keys/values are the inputs\u2019 representation. This is still a vague description of the mechanism. \n* The baseline architecture simply averages input embeddings. Is this the most fair baseline we can come up with? For instance, can we have an architecture where the set of vectors are projected to a smaller dimension (/2) and concatenated?\n* Relatedly, the goal of the work is to show that the attention-enabled agents come up with more compositional languages. For now a possible interpretation of Table1 is that such agents are simply more effective at solving the task (have higher acc) and, consequently, they attain better compositionality. Is there a way to disentangle success rate vs compositionality, ie showing that for a fixed success rate the languages would be more compositional? Maybe the baseline architecture is simply too weak due to the averaging?\n* Stepping back, I think it is not surprising that a hand-crafted architecture, tailored for this specific game setup, can be created to promote compositionality in the emergent languages. What makes this particular architecture interesting? One specific way this architecture can be interesting is if it is very general and as powerful as standard Transformer yet provides additional benefit of compositionality. However, I believe the current experiments do not allow this conclusion, primarily because they only use the single-head attention. Would the compositionality bias persist with multiple heads?\n* In Figure 3, we only have examples where Speaker and Listener have access to the same example. It would be interesting to look at the attention map when the examples are different but from the same label (is the position important?).\n* For the Fashion-MNIST game: What is the size of the training dataset (it is only specified that there are 45 labels)? \n\n\n* Typo: S2.4, \"Positioanl\"\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: the text is generally clear;\n* Novelty: I believe the findings are novel;\n* Quality: the experiments seem to support the claims;\n* Reproducibility: the authors state that the code would be open sourced upon acceptance.\n",
            "summary_of_the_review": "My main comments are:\n* It is not clear that the baselines are not overly simplistic;\n*  I think it is not surprising that a hand-crafted architecture, tailored for this specific game setup, can be created to promote compositionality in the emergent language. And it seems to me that this particular architecture is hand-crafted as it has a single-head attention. What makes this particular architecture interesting? \n* Ideally, to support the claim of the paper, one might want to disentangle the task success rate from how compositional language is;\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2443/Reviewer_4G68"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2443/Reviewer_4G68"
        ]
    },
    {
        "id": "eoBwc-sqIcQ",
        "original": null,
        "number": 4,
        "cdate": 1667419173163,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667419173163,
        "tmdate": 1667419173163,
        "tddate": null,
        "forum": "4JoV9g5R1M",
        "replyto": "4JoV9g5R1M",
        "invitation": "ICLR.cc/2023/Conference/Paper2443/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyze the effect of attention on emergent communication. The paper hypothesizes that the attention mechanism will bias agents towards more compositional encodings. Experimental results on a small-scale symbolic game and image reference game derived from Fashion-MNIST demonstrate that attentive speaker and/or listeners result in improvements in task success on unseen objects and various proxy measures of compositionality such as topographic similarity and positional disentanglement. Further analysis builds on the intepretability of attention, and shows that attention can be used to analyze attribute and message alignment, as well as predict coordination between speakers and listeners.",
            "strength_and_weaknesses": "*Strengths*\n\nThe experimental results in this paper support the hypothesis that attention helps in the particular architecture considered. The improvements in all metrics over the ablated non-attention model are significant, despite large error bars. \n\n*Weaknesses*\n\nMy primary concern is that this paper has two main contributions: 1) weaker models result in worse performance and 2) attention is interpretable. These contributions are relatively limited, as a correlation between model flexibility and performance is common and the interpretability of attention has been analyzed previously [1].\n\nI am hesitant to ask for more experiments, but will suggest one. To increase the scope of the first contribution, one could test if more powerful models actually do result in task metric improvements. For example, one could run a multi-headed attention speaker and/or listener. It's possible that a single head attention model might perform better, since a compositional encoding may want to focus on one aspect of the referent at a time. This would contradict the \"weaker models result in worse performance\", and be an interesting additional contribution. \n\n*Questions*\n\n* The error bars in Table 1 are much larger than the ones reported in [2], which are run with a larger message space. I assume this is due to variance in the REINFORCE gradient estimator, which is one of the reasons for introducing the KL regularization. Is there an alternative explanation for this, and/or a way to reduce the size of the error bars? Perhaps other forms of variance reduction, such as multi-sample estimators and/or control variates.\n\n[1] Wiegreffe, Sarah and Yuval Pinter. \u201cAttention is not not Explanation.\u201d EMNLP (2019).\n[2] Chaabouni, Rahma, et al. \"Emergent communication at scale.\" International Conference on Learning Representations. 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was clear and reproducible. I do have some concerns about the novelty of the claims, which are explained in the *Weaknesses* section.",
            "summary_of_the_review": "I recommend a borderline reject, since I am not confident that the contributions are novel.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2443/Reviewer_4TD5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2443/Reviewer_4TD5"
        ]
    }
]