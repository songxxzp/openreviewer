[
    {
        "id": "HQEjv20DeSI",
        "original": null,
        "number": 1,
        "cdate": 1665811322499,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665811322499,
        "tmdate": 1665811322499,
        "tddate": null,
        "forum": "M95oDwJXayG",
        "replyto": "M95oDwJXayG",
        "invitation": "ICLR.cc/2023/Conference/Paper2559/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper handles the model selection problem in the context of unsupervised domain adaptation. In classical domain adaptation literature, model selection is a hard problem because of the lack of labeled target data. In recent years, several linear aggregation methods have been proposed yet they lack theoretical guarantee on the target error. Under the covariate shift assumption, the authors propose a simple yet effective linear aggregation approach based on importance weighting and provide a target error guarantee with respect to the target error incurred by the best linear aggregation. The proposed adaptation algorithm outperforms not only heuristic aggregation methods but also existing single-model selection approaches with theoretical guarantees.",
            "strength_and_weaknesses": "### Strengths\n\n- **Simple method**: The proposed method is based on importance weighting, which is a well-understood yet pretty simple method. In spite of its popularity, importance weighting has not been well studied in the context of linear aggregation for unsupervised domain adaptation. Hence, it does not undermine the novelty or contribution of this paper.\n- **Effective empirical performance**: The empirical performances through massive experiments demonstrate the practical effectiveness of the proposed method. It is remarkable that the proposed aggregation approach outperforms the existing methods to select a single best model.\n\n### Weaknesses\n\n- **Theory does not elucidate the effect of the model size.** Although the experiments demonstrate that the aggregation of multiple model candidates outperforms a single best model, it is not evident from the claim of Theorem 1. I tried to figure out from its proof but failed because the model size $\\\\ell$ is hidden in absolute constants of concentration bounds. Is it possible to comment on what goes on with the target error when we increase $\\\\ell$?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very clearly and easy to follow. Below I give a few suggestions though they are minor.\n\n- In references, Sugiyamai et al. (2007) should be corrected to Sugiyama et al. (2007).\n- When showing the main theoretical claim Theorem 1, I would emphasize that this error analysis is independent of the estimation error of density ratio.\n- In Section 5.1, the meaning of Figure 2 is a little bit ambiguous. For example, \"the approach proposed in You et al. (2019) and a heuristics baseline\" (12nd line of Section 5.1) are compared in Figure 2, but I could not find which parts in Figure 2 indicate them. It is not clear which part in Figure 2 corresponds to \"the given sequence of models\" (14th line). In addition, I do not understand why three models (IWA, SOR, and DEV) are aggregated as shown at the bottom of Figure 2, given my understanding that IWA is already an aggregation of multiple models.",
            "summary_of_the_review": "The authors provide a transparent algorithm with a clear presentation, hence I would suggest accepting this paper. Minor issues should be easily fixed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2559/Reviewer_AZPJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2559/Reviewer_AZPJ"
        ]
    },
    {
        "id": "PF8pEqQXHF",
        "original": null,
        "number": 2,
        "cdate": 1666589974975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589974975,
        "tmdate": 1668530318506,
        "tddate": null,
        "forum": "M95oDwJXayG",
        "replyto": "M95oDwJXayG",
        "invitation": "ICLR.cc/2023/Conference/Paper2559/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies how to address the issue of hyper-parameter selection in unsupervised domain adaptation. Specifically, the authors propose a method that subsequently computes a linear aggregation of the models. In addition, theories for bounding the target error are added to make the method convincing. Experiments show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n1. The targeted problem is important and valuable for domain adaptation (without labeled data).\n2. This paper is well-written, it is enjoyable to read.\n3. Experiments on language, image, text and time-series data show the effectiveness of the proposed method.\n\nWeaknesses:\n1. My main concern is the theories for bounding the target error. The conclusion is the target error of the computed aggregation is asymptotically at most twice the target error of the optimal aggregation. However, this bound is loose and cannot give a guarantee of superior performance.\n2. Another concern is the selection of datasets. For example, the performance gains achieved by DA methods on Amazon Reviews and WISDM are marginal (The performance of So is close to TB). \n3. The authors should conduct experiments on the datasets in the original paper of baselines. For example, experiments on digit, office and visda should be considered.\n4. More insightful analyses should be provided. For example, the visualization of the importance weight of each model should be shown. It is interesting if the importance weight can tell us what is the best model in the sequence. Now the paper only tell us what the method is and if the proposed method can achieve better results without further discussions.\n5. The scope of the selection model should also be discussed. The ensemble method works well given many effective models. However, if the candidate set of the model contains a bad model, the ensemble method may not work well.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. This paper is well-written.\n\nQuality: Needs improvements. More insightful discussions, more experiments on existing benchmarks should be provided.\n\nNovelty: Good. The proposed method is somewhat novel.",
            "summary_of_the_review": "Overall, this paper is well-written and contributions are not very marginal. However, more improvements should be added to the paper.\n\nStrengths:\n1. The targeted problem is important and valuable for domain adaptation (without labeled data).\n2. This paper is well-written, it is enjoyable to read.\n3. Experiments on language, image, text and time-series data show the effectiveness of the proposed method.\n\nWeaknesses:\n1. My main concern is the theories for bounding the target error. The conclusion is the target error of the computed aggregation is asymptotically at most twice the target error of the optimal aggregation. However, this bound is loose and cannot give a guarantee of superior performance.\n2. Another concern is the selection of datasets. For example, the performance gains achieved by DA methods on Amazon Reviews and WISDM are marginal (The performance of So is close to TB). \n3. The authors should conduct experiments on the datasets in the original paper of baselines. For example, experiments on digit, office and visda should be considered.\n4. More insightful analyses should be provided. For example, the visualization of the importance weight of each model should be shown. It is interesting if the importance weight can tell us what is the best model in the sequence. Now the paper only tell us what the method is and if the proposed method can achieve better results without further discussions.\n5. The scope of the selection model should also be discussed. The ensemble method works well given many effective models. However, if the candidate set of the model contains a bad model, the ensemble method may not work well.\n\n\n------Update------\n\nI carefully read the author's response and most of my concerns are addressed. I'm happy to improve my rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2559/Reviewer_WzwU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2559/Reviewer_WzwU"
        ]
    },
    {
        "id": "Vlr79ko0CYN",
        "original": null,
        "number": 3,
        "cdate": 1666597086017,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597086017,
        "tmdate": 1670405050643,
        "tddate": null,
        "forum": "M95oDwJXayG",
        "replyto": "M95oDwJXayG",
        "invitation": "ICLR.cc/2023/Conference/Paper2559/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The work proposes a theoretical approach for choosing hyper-parameters in unsupervised domain adaptation. The main strategy is to compute an aggregation of models with target error bound, which theoretically relies on the extension of importance weighted least squares to linear aggregation of vector-valued functions. The paper conducts large scale comparative experiments on language, images, time-series classification tasks.The proposed method outperforms IWV and DEV and sets a new state-of-the-art performance.",
            "strength_and_weaknesses": "Strenth:\n1. The proposed aggregation approch is general without strong assumptions.\n2. The paper gives detailed theoretical analysis.\n\nWeaknesses:\n1. For experiments, though the paper\u2019s benchmarks are large and diverse, but still some popular UDA benchmarks, such as VisDA-2017, DomainNet etc are missing. Besides, the paper mainly compares with conventional methods, while certain more recent works are missing.\n2. The novelty of the paper is limited besides the combination of IWV with linear aggregation. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow.",
            "summary_of_the_review": "The paper proposes a new aggregation models based method in unsupervised domain adaptation. Experiments on language, images, time-series classification tasks show effectiveness of the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2559/Reviewer_V758"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2559/Reviewer_V758"
        ]
    },
    {
        "id": "r9NPt2P30D",
        "original": null,
        "number": 4,
        "cdate": 1666692151731,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692151731,
        "tmdate": 1670217750408,
        "tddate": null,
        "forum": "M95oDwJXayG",
        "replyto": "M95oDwJXayG",
        "invitation": "ICLR.cc/2023/Conference/Paper2559/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a method that extends weighted least squares to deep neural networks for hyperparameter selection in unsupervised domain adaptation. In theory, the authors show that the target error of the proposed algorithm is asymptotically not worse than twice the error of the unknown optimal aggregation. Numerical validate the superiority of the proposed method over the DEV and the IWV method. ",
            "strength_and_weaknesses": "Strengths:\n\n1: This paper proposes a simple but effective method that extends weighted least squares to deep neural networks for hyperparameter selection in unsupervised domain adaptation.\n\n2: Experiments on different tasks are conducted to verify the superiority of the proposed algorithm.\n\n3: This paper is well-written and easy to follow. The background knowledge is presented well. The readers can easily understand the paper.\n\n\nWeaknesses:\n\n1. The baselines are few. Do the baselines include important and all the SOTA UDA methods?\n\n2. In theory, the authors show that the target error of the proposed algorithm is asymptotically not worse than twice the error of the unknown optimal aggregation. Such a bound cannot provide guarantees of better performance of the proposed method over others.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: This proposed method is somewhat novel.\n\nQuality: The experimental validation is extensively evaluated for different tasks.\n\nClarity: This paper is well written, and the theoretical analysis is easy to follow from the outline provided.\n\nReproducibility: The code needed to reproduce the experimental results is provided.",
            "summary_of_the_review": "This paper proposes the first method that extends weighted least squares to deep neural networks for hyperparameter selection in unsupervised domain adaptation. The method is technically sound. \n\n------Update------\n\nI have read the author's response and most of my concerns are addressed. I will keep the score for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2559/Reviewer_2X6X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2559/Reviewer_2X6X"
        ]
    }
]