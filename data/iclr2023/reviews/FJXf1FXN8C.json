[
    {
        "id": "veX4x_xx5G",
        "original": null,
        "number": 1,
        "cdate": 1666007526304,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666007526304,
        "tmdate": 1666256182844,
        "tddate": null,
        "forum": "FJXf1FXN8C",
        "replyto": "FJXf1FXN8C",
        "invitation": "ICLR.cc/2023/Conference/Paper5208/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents theoretical insight to explain GD with hard and conjugate labels for a binary classification problem.This paper shows that for square loss, GD with conjugate labels converges to a solution that minimizes the testing 0-1 loss under a Gaussian model, while GD with hard pseudo-labels breaks down. Besides\uff0cthe authors discuss them under different loss functions.",
            "strength_and_weaknesses": "Strength\uff1a\n- The paper is well-organized and clearly written, which is easy to follow.\n- The theoretical work of this paper is sufficient, which improves the value of the paper.\n- The paper provides insights into when and why GD with hard labels or conjugate labels works in test-time adaptation.\n\nWeakness\uff1a\n- The original conjugate pseudo-labels[A] perform results across different domain adaptation. The author neglect the explaination of this scenarios. Is the convergence analysis general for diverse domain with large gap?\n- My concern is the assumption in equation (8). The authors have not provided the value range of $\\epsilon$. Although the authors mention,\u201d... as it means that the\nsource model is better than the random guessing in the new domain.\u201d, how to illustrate the special case of right angle when $\\epsilon=1$\uff1f\n- The convergence of GD was analyzed in section 5. What is the main obstacle to investigating convergence according to the idea proposed in this paper\uff1f Moreover, in section 5.2, does $log(t)$-rate convergence boost compared to the convergence rate of related pseudo labels\uff1f\n\n[A] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and Zico Kolter. Test-Time Adaptation via Conjugate Pseudo-labels. NeurIPS, 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well-organized and clearly written.\n\nQuality: Technically solid paper.\n\nNovelty: The paper makes non-trivial advances over the current state-of-the-art.\n\nReproducibility: Key details are sufficiently well-described for competent researchers to confidently reproduce the main results.",
            "summary_of_the_review": "Please see Q1 and Q2.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5208/Reviewer_w2JR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5208/Reviewer_w2JR"
        ]
    },
    {
        "id": "FU8dxGpuIH",
        "original": null,
        "number": 2,
        "cdate": 1666611465531,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611465531,
        "tmdate": 1666611465531,
        "tddate": null,
        "forum": "FJXf1FXN8C",
        "replyto": "FJXf1FXN8C",
        "invitation": "ICLR.cc/2023/Conference/Paper5208/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is a theoretical analysis of a recent work [a]. They use the Gaussian model to explain and understand the gradient descent with hard-label and conjugate labels. For convenience analysis, they only consider the binary classification problem. The theoretical results and analysis are interesting.\u00a0\n\n[a] Test-Time Adaptation via Conjugate Pseudo-labels. NeurIPS, 2022.",
            "strength_and_weaknesses": "1. This paper focuses on binary classification problem, and the authors analyze three classical loss functions: square loss, logistic loss, and exponential loss. But binary cross-entropy loss has been widely studied in community, especially for training deep models. So, I am interested in the results of this loss function.\n2. The assumptions adopted by the author seem to be somewhat strict, such as that the conjugate function must be an even function. We know that not all functions satisfy this constraint. If it is an odd function, does the conclusion in the paper still hold?\n3. We find that the Gaussian models used in Section 3 and Section 5 are different. How much does this different choice affect the conclusion? If we still use the Gaussian model in Section 3, will the conclusions of Section 5 not hold?\n4. In section 5, we observe that the normalized operation of the parameter w is ignored. Whether or not to use normalized has a large impact on the results, and the authors need to clarify this modification.\n5. How to derive Equation 24 is not very clear.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The theoretical analysis and results are novel enough and the presentation is clear. However, the assumptions of the paper are a bit too strong, and the conclusions of the paper have limitations.",
            "summary_of_the_review": "This paper gives an interesting understanding of the GD with hard and conjugate labels for test-time training. I think this will bring new insights to the community.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5208/Reviewer_4HLp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5208/Reviewer_4HLp"
        ]
    },
    {
        "id": "jzM1qq3xV-",
        "original": null,
        "number": 3,
        "cdate": 1666683565439,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683565439,
        "tmdate": 1666683565439,
        "tddate": null,
        "forum": "FJXf1FXN8C",
        "replyto": "FJXf1FXN8C",
        "invitation": "ICLR.cc/2023/Conference/Paper5208/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper tries to theoretically understand why GD with conjugate labels outperform other pseudo labels. Further results focus on the role of the loss function and provide certain guarantees on the convergence.",
            "strength_and_weaknesses": "There has been much work on test-time adaption in recent years. Although certain behaviours are empirically understood, we often do not understand the underlying principles yet.\nAlthough the authors make certain assumptions (e.g., binary labels, population dynamics), these assumptions are probably necessary for coming up with theoretical guarantees and are not too limiting.\nThe main results are well-explained and technically sound. As such, they do not only provide as with a better understanding, but might spur future research for more general settings.\n\nThe empirical evaluation is restricted to a small synthetic example. These experiments corroborate the theoretical findings and as such are sufficient for the scope of this paper. Yet, one could argue that coming up with constructive insights from the theoretical work and showing some improvements on some more realistic test-time adaptation problems would have been nice.\n\nFrom Algorithm 1 I further take that the samples are all drawn from the same new domain. It can make a drastic difference whether this is the case or if the domain changes more frequently. Although this assumption makes sense in many settings, I miss a brief discussion on it.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-motivated and well written. As far as I can tell, the theoretical work is of good quality. I particularly like that the authors are very honest about the scope and the limitations of the work, i.e., the paper actually delivers what is promised. Moreover, I really enjoyed that the main arguments of the proofs (which are arguable the main contribution of the paper) can be found in the main text and are not hidden in the appendix.",
            "summary_of_the_review": "The main contribution of the paper is that it provides a novel understanding on the role of the labels (conjugate vs. hard) and the loss function on the potential for test-time adaption. As such, I think it is an interesting theoretical piece of work that might spur further research in this direction.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5208/Reviewer_VqqL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5208/Reviewer_VqqL"
        ]
    },
    {
        "id": "BjnGZGwFCqy",
        "original": null,
        "number": 4,
        "cdate": 1666958831501,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666958831501,
        "tmdate": 1670002485756,
        "tddate": null,
        "forum": "FJXf1FXN8C",
        "replyto": "FJXf1FXN8C",
        "invitation": "ICLR.cc/2023/Conference/Paper5208/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "**Summary**\nThis paper investigates why conjugated pseudo label in test-time adaptation (TTA), which is recently proposed approach, perform better than hard pseudo-labeling. Specifically, they show that under Gaussian model, GD with hard pseudo-labels fails to find optimal solution while GD with conjugated pseudo label reaches optimal solution. \n",
            "strength_and_weaknesses": "**Strength**\n\n**[S1]** This paper is generally well written and easy to follow. \n\n**[S2]** Conjugated pseudo label is timely topic in test-time adaptation. Besides, pseudo-label itself is general techniques and worth investigation. \n\n---\n**Weakness**\n\n**[W1] Assumption of the analysis is not tailored for TTA. ** While this paper mainly focus on the test-time adaptation setup (as clearly shown in the title), the current analysis seems to be hold for general pseudo-label setup. In other words, it is unclear what is the uniqueness of the TTA setup, and how the analysis explain the nature of conjugated pseudo-label. \n\nOf course, the generality of the analysis itself is not a bad thing, however, it is unclear for me how the analysis is significant if it is not tailored for TTA. The paper should clearly discuss whether the current analysis is specialized for TTA setup, or more general one. Besides, if the later case, the paper should discuss more on what was already well investigated in pseudo-labeling and how the analysis in this paper is significant compared to existing findings. \n\n\n**[W2]Practical implication is limited. ** Since the analysis conducted only for the simple case on binary classification with Gaussian model, practical implication is limited. They also provide only limited toy empirical investigations. ",
            "clarity,_quality,_novelty_and_reproducibility": "**[Clarity and Quality]**\nThis paper is generally well written. \n\n---\n**[Novelty]**\nSee weakness section. \n\n---\n**[Reproducibility]**\nNot applicable. \n",
            "summary_of_the_review": "Overall, I agree that the conjugated pseudo label is timely and worth to investigate topic, but the current analysis is limited to very limited setup and therefore the practical impact is limited. Since I am not the theory person, I might underestimate the significance of the theoretical advancement. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5208/Reviewer_3i1a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5208/Reviewer_3i1a"
        ]
    }
]