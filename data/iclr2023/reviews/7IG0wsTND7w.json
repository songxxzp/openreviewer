[
    {
        "id": "MhCJqru8ii",
        "original": null,
        "number": 1,
        "cdate": 1666588885645,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588885645,
        "tmdate": 1666588885645,
        "tddate": null,
        "forum": "7IG0wsTND7w",
        "replyto": "7IG0wsTND7w",
        "invitation": "ICLR.cc/2023/Conference/Paper4108/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper tackles domain generalisation by synthesizing the hardest domain, and then using domain adaptation techniques. Domains are parametrized in fourier space using the amplitudes which the phases are taken from the original images. The hardest target domains synthesized by adversarial training. The classifier is learned via MCD, which is modified here as the target labels are known. Results seem better than mixup in fourier space.",
            "strength_and_weaknesses": "Strengths:\n* The paper is clearly written and a pleasure to read.\n* The main ideas are intuitive\n* Augmenting MCD with the true labels is probably novel (as is an aspect of this setting)\n* Results indeed outperform many previous methods\n\nWeakness\n* Synthesizing domains in fourier space clearly does not generate realistic images - and so the idea diesn't really work in that sense - even though it did generate improvements on the baseline. The method resolves some of the issue by using mixup, but this is obviously not the main idea the paper was selling. It makes the reviewer ponder if this is the fourier parametrization is the correct one for synthesising the target domain. \n\n* The method is only slightly novel compared to the previous baseline of fourier domain mixup. Most of the advantage comes form SWAD, which is orthogonal. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well written and clear\nQuality: the experiments seem extensive to me, however the poor visual quality of the target domains makes me question whether there is more to this story.\nNovelty: Novelty is limited, to replacing amplitude mixup by adversarial training and the supervised MCD loss.\nReproducibly: The method is not too complicated, perhaps it can be reproduced but no code was released.",
            "summary_of_the_review": "This is a well written paper, which tells a nice story and presents strong results. The novelty is a little limited and the domain synthesis is raises some doubts. I am borderline on this paper and waiting for the rebuttal and other reviews.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4108/Reviewer_pXLL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4108/Reviewer_pXLL"
        ]
    },
    {
        "id": "Lp_nb5ajb82",
        "original": null,
        "number": 2,
        "cdate": 1666626923659,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626923659,
        "tmdate": 1666626923659,
        "tddate": null,
        "forum": "7IG0wsTND7w",
        "replyto": "7IG0wsTND7w",
        "invitation": "ICLR.cc/2023/Conference/Paper4108/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper targets at solving domain generalization. The authors tackle the DG problem as DA task where the worst-case `target' domain is adversarially synthesized. Specifically, they generate Fourier amplitude images and combine them with source domain phase samples. MCD is further exploited to relate the target domain performance to the discrepancy of classifiers in the model hypothesis space. Bayesian hypothesis modeling is adopted to make adversarial MCD minimization feasible. Experiments show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n+ This paper studies an important problem\n+ This paper is well-written, it is enjoyable to read.\n+ Experiments are comprehensive and show the effectiveness of the proposed method.\n\nWeaknesses:\n+ The contributions are a little limited considering that MCD is an existing approach.\n+ This paper should concentrate more on true solutions in DG. More discussions about the true solution in DG should be provided.\n+ Why choosing Bayesian hypothesis modeling? It should be clearly discussed.\n+ More insightful discussions should be provided. For example, how to improve the generalization ability of deep models using other adversarial DA approaches (e.g. DANN and MDD)?\n[1] Domain-adversarial training of neural networks\n[2] Bridging theory and algorithm for domain adaptation.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. This paper is well-written, it is enjoyable to read.\n\nQuality: Good. Experiments are comprehensive and show the effectiveness of the proposed method.\n\nNovelty: A little marginal. MCD is an existing approach. And why choosing Bayesian hypothesis modeling is not clear.\n",
            "summary_of_the_review": "Strengths:\n+ This paper studies an important problem\n+ This paper is well-written, it is enjoyable to read.\n+ Experiments are comprehensive and show the effectiveness of the proposed method.\n\nWeaknesses:\n+ The contributions are a little limited considering that MCD is an existing approach.\n+ This paper should concentrate more on true solutions in DG. More discussions about the true solution in DG should be provided.\n+ Why choosing Bayesian hypothesis modeling? It should be clearly discussed.\n+ More insightful discussions should be provided. For example, how to improve the generalization ability of deep models using other adversarial DA approaches (e.g. DANN and MDD)?\n[1] Domain-adversarial training of neural networks\n[2] Bridging theory and algorithm for domain adaptation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4108/Reviewer_71uu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4108/Reviewer_71uu"
        ]
    },
    {
        "id": "jrXyq4ofmfy",
        "original": null,
        "number": 3,
        "cdate": 1666629042925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629042925,
        "tmdate": 1666629042925,
        "tddate": null,
        "forum": "7IG0wsTND7w",
        "replyto": "7IG0wsTND7w",
        "invitation": "ICLR.cc/2023/Conference/Paper4108/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces a new domain generalization (DG) method by synthesizing the virtual target domain samples in Fourier domain and exploiting the maximum classifier discrepancy (MCD) principle in source domain and generated target domain. It also gives a modified MCD loss, which is suitable for DG task. The experiments show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength\n(1)\tThe proposed method is novel and well-motived. \n(2)\tThis paper gives a new perspective on DG by finding the worst-case ib an adversarial way and posing it as a domain adaptation (DA) task. \n(3)\tEmpirical results indicate that the proposed method greatly outperforms the other methods.\nWeakness:\n(1)\tEven the generated target domain is the \"worst case \", it is not as same as the true target domains. It would be better to discuss this in theoretical analysis.\n(2)\tWhy do not directly minimize e^*(\\mathcal{H} ; S, T) by one supervisor loss if you have known the label of generated target domain.\n(3)\tThe results of CMNIST and RMNIST in the DomainBad benchmark are not provided in this paper. \n(4)\tIn this paper, there is no comparison with other methods [3,4] in different backbone networks, such as resnet18 in PACS.\n(5)\tOnly the result of the leave-one-domain-out setting is provided. Can you give the result in single source generalization setting as EFDMix[4]?\n[1] Minyoung Kim, Pritish Sahu, Behnam Gholami, and Vladimir Pavlovic. Unsupervised Visual Domain Adaptation: A Deep Max-Margin Gaussian Process Approach. CVPR2019.\n[2] Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, and Tao Xiang. Stochastic classifiers for unsupervised domain adaptation. CVPR2020.\n[3] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. ICLR2021.\n[4] Yabin Zhang, Minghan Li, Ruihuang Li, Kui Jia and Lei Zhang. Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization. CVPR2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and novelty are pretty good. Not sure if the paper results can be reproduced.",
            "summary_of_the_review": "The paper proposes a novel and well-motived method for domain generalization. However, there are lacks of some experiments and more discussion about the theory and the relation between SMCD and other losses. If you solve the problems, I am willing to improve my rating.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4108/Reviewer_5QDe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4108/Reviewer_5QDe"
        ]
    },
    {
        "id": "CV1ckTVRtn",
        "original": null,
        "number": 4,
        "cdate": 1666805917884,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666805917884,
        "tmdate": 1666805917884,
        "tddate": null,
        "forum": "7IG0wsTND7w",
        "replyto": "7IG0wsTND7w",
        "invitation": "ICLR.cc/2023/Conference/Paper4108/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approach to domain generalisation whereby they use Fourier to generate synthetic images which are then used as a worst-case target domain to improve the model's robustness and therefore its generalisation. This is reminiscent of a domain adaptation approach. The amplitude-based generator that creates the synthetic images is trained in an adversarial manner and is combined with the phase source domain data. Surprisingly, the method performs competitively compared to other methods proposed.\n ",
            "strength_and_weaknesses": "Strengths:\n\na) generating target images for modelling worst-case scenario\nb) repurposing mean classifier discrepancy to propose a supervised variant that leverages the labelled generated target domain data\nc) competitive performance\nd) relative simple implementation\n\nWeakness:\n\na) not a weakness per se, but I am trying to get my head round why fourier amplitude for adversarially generating images.... is that to create bad but not too bad images deliberately to force a better generalisation performance? It sounds rather arbitrary to me (not in a negative way) so I would like to know what the thought process was to arrive to that approach vs trying other ways,",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well articulated and provides the extra context in the appendix.\nThe empirical part is good and the method is decent and relatively simple, albeit working surprisingly well across several datasets. \nI would say that pseudocode is just enough to reproduce it, but that assumes that the hyperparameters described in the main text are what one needs to reproduce the results rather than trying to magically grid search parameters. In addition, it would be nice to have a response from the authors as to whether they plan to release the full code.\n\n****Authors please be consistent in the use of American vs British English - choose whichever you prefer but stick with it across the paper.****",
            "summary_of_the_review": "This is a good submission that seems to perform well. Authors have run several experiments some appearing in the main text and some others in the appendix. There is a good empirical component and the novelty is fair. I think this is a decent submission to be considered for acceptance.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4108/Reviewer_M2ax"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4108/Reviewer_M2ax"
        ]
    }
]