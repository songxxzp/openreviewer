[
    {
        "id": "RBzDtpR61UQ",
        "original": null,
        "number": 1,
        "cdate": 1666689455148,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689455148,
        "tmdate": 1666689455148,
        "tddate": null,
        "forum": "9EAQVEINuum",
        "replyto": "9EAQVEINuum",
        "invitation": "ICLR.cc/2023/Conference/Paper4875/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a contrastive learning framework for NER tasks. The proposed model is flexible in predicting nested entities.  The idea is novel and experiments demonstrate the effectiveness of the proposed model. ",
            "strength_and_weaknesses": "Some important details are missed or not cleared. For example, how to set the maximum length L. What is the relation between L and the model performance (accuracy and speed). Experiments didn't include the most common dataset CoNLL 03. This framework setting is similar to the template-based NER (using prompt,\"Template-based named entity recognition using BART\") but this work didn't compare with this work. ",
            "clarity,_quality,_novelty_and_reproducibility": "Some important details are missed or not cleared. For example, how to set the maximum length L. What is the relation between L and the model performance (accuracy and speed).  I didn't see the open source statement in this paper, as the experiments require many details, without the source code, it is hard to reproduce the results. ",
            "summary_of_the_review": "Novel and strong results, but the details are not complete and the experiments/comparison are not complete. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4875/Reviewer_QTaJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4875/Reviewer_QTaJ"
        ]
    },
    {
        "id": "zGwG6QRTS0E",
        "original": null,
        "number": 2,
        "cdate": 1666714131313,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714131313,
        "tmdate": 1666714131313,
        "tddate": null,
        "forum": "9EAQVEINuum",
        "replyto": "9EAQVEINuum",
        "invitation": "ICLR.cc/2023/Conference/Paper4875/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new framework that performs the Named Entity Recognition task from the semantic matching perspective (bi-encoder here). The idea is to map candidate text spans and entity types into the same vector representation space and perform the NER task using distance metric accordingly. One important issue is separating non-entity spans from desired entity mentions. To alleviate these problems, the paper proposes a position-based objective and a dynamic thresholding approach. Experiments on both supervised and distantly supervised NER tasks demonstrate the effectiveness of the proposed approaches.",
            "strength_and_weaknesses": "\nStrength:\n- Utilizing Bi-encoder has never been considered, while this idea is very similar to the span-based ner. (see Weaknesses)\n- The empirical results are extensive.\n\nWeaknesses:\n- The proposed approach is very similar to span-based NER. In span-based NER, the label type embeddings are randomly initialized (the final layer before softmax). However, the proposed approach utilizes a neural network to obtain that matrix. In the end, the novelty seems limited.\n- Besides nested NER, I would also prefer adding some flat NER results (CoNLL, WNUT, and so on).",
            "clarity,_quality,_novelty_and_reproducibility": "\n- The overall writing is clear. \n- The quality meets the standard of ICLR. \n- The novelty is relatively limited. \n- As the paper has some detailed designs on special losses, the code is suggested to be public to reproduce empirical reesults.",
            "summary_of_the_review": "This paper proposes a new framework that performs the Named Entity Recognition task with the bi-encoder model. The paper shows strong results on nested NER and distant supervised NER tasks. While the results are promising, the proposed approach is similar to the span-based NER approach, which might have limited techniqual novelty. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4875/Reviewer_FXLT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4875/Reviewer_FXLT"
        ]
    },
    {
        "id": "29DRYUriPT3",
        "original": null,
        "number": 3,
        "cdate": 1666944868941,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666944868941,
        "tmdate": 1666944868941,
        "tddate": null,
        "forum": "9EAQVEINuum",
        "replyto": "9EAQVEINuum",
        "invitation": "ICLR.cc/2023/Conference/Paper4875/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Most of existing works approach NER as sequence labeling, span classification, or sequence-to-sequence problems. These conventional methods rely on CRF to extract entity values which performs subpar on nested NER tasks. To address these limitations, this work formulate NER as a representation learning problem. Intuitively, their method associate entity type embeddings with entity name embeddings in a common subspace. To achieve the goal in a principle manner, they cast the problem into contrastive learning framework. Moreover, they proposed two contrastive objectives for model training and dynamic thresholding for adaptive model inference. Extensive experiment demonstrated the effectiveness of their approach and ablations conveyed insightful findings. ",
            "strength_and_weaknesses": "Strength\n- This work is well-motivated. The idea of framing NER as representation learning is interesting. Although there are some works combine contrastive learning with NER, this paper proposed novel strategies to handle challenges specifically.\n- The introduced position-based objective to treat entity spans differently. Specifically, they penalize more entity span that have completely no overlap with the gold span while penalize less partially correct span. Their experiment results (as shown in Table 8) implies this objective contribute performance gains consistently and substantially. \n- The authors conducted extensive experiment in various settings and benchmarks. Their methods beats SOTA baselines almost across the board. \n- Various ablation studies have been conducted to share insightful findings.\n\nMajor Concerns\n- There are serval other works [1-3] which are quite relevant/similar with this paper as they solve NER under contrastive learning framework as well. I would appreciate the authors if they can compare their method with these works from methodology perspective.\n- I am wondering how would the model handle entity name that could be a valid value in different entity types. For instance, Willson could be a person name, while it also could be a ORG(company) name. Could you share some thoughts on this case?\n- Currently the method enumerates all candidate entity spans as negative samples for calculating contrastive losses. In the paper, the maximum token length is 256. However if we have even larger sequence length and/or large $L$, do we expect the computational cost would increase significantly? If so, is there any alternative and more efficient way to create negative samples?\n- For span-based objective, what is the dimension of $h_i^T$? I guess it should be $\\mathbb{R}^m$? Could you elaborate more about $D$? Is it learnable? and how to learn it? Is $D(j-i)$ refers to the token representations between $h_i^T$ and $h_j^T$? It seems the answer is not. So why do you using the token embeddings of $h_i^T$ and $h_j^T$ only instead of taking token embeddings between them into consideration?\n- In eq(8), is $u_{[CLS]}=Linear_B^T(h_{[CLS]}^T)$? Do you think $u_{[CLS]}$ would collapse to $e_k^B$? why or why not?\n- In eq(10), what is $\\ell_{\\text{joint}}^+$? Are you referring to $\\ell_{span}^+$?\n\n[1]. Fu, Yingwen, Nankai Lin, Ziyu Yang and Shengyi Jiang. \u201cA Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity Recognition.\u201d ArXiv abs/2204.00796 (2022): n. pag.\n[2]. Ye, Hongbin, Ningyu Zhang, Shumin Deng, Mosha Chen, Chuanqi Tan, Fei Huang and Huajun Chen. \u201cContrastive Triple Extraction with Generative Transformer.\u201d AAAI (2021).\n[3]. Lin, Bill Yuchen, Dong-Ho Lee, Minghan Shen, Ryan Rene Moreno, Xiao Huang, Prashant Shiralkar and Xiang Ren. \u201cTriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition.\u201d ACL (2020).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good as the paper is easy to follow. The paper still can be further polished with proofreading. \nQuality: Good. The technique part is sound and solid. Extensive experiments and ablations are diverse and insightful.\nNovelty: Good. The authors formulate the problem under contrastive learning with new methodology, though these exist some works are relevant to this work closely.\nReproducibility: No data point as the code is not shared.",
            "summary_of_the_review": "Overall the proposed method for NER is moderately novel. It can benefit the community in this filed. The experiments and ablations demonstrated the effectiveness of their method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4875/Reviewer_wC16"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4875/Reviewer_wC16"
        ]
    },
    {
        "id": "euS93-D46R7",
        "original": null,
        "number": 4,
        "cdate": 1667380259984,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667380259984,
        "tmdate": 1669058909266,
        "tddate": null,
        "forum": "9EAQVEINuum",
        "replyto": "9EAQVEINuum",
        "invitation": "ICLR.cc/2023/Conference/Paper4875/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Paper presents a bi-encoder framework for NER, which applies contrastive learning to map candidate text spans and entity types into the same vector representation space and make the representation of entity mentions be similar with corresponding entity type. It proposes span-based objectives that compare span and entity type, and position-based objectives that compare start and end tokens separately with entity type. It shows its effectiveness in nested NER with fast inference speed.",
            "strength_and_weaknesses": "Paper presents bi-encoder style NER framework with novel objectives suitable to NER and shows its effectiveness in nested NER problems. However, I have following major concerns:\n\n(1) Lack of baseline. Span-based NER, which predicts over all the candidate spans in the sentence, has been widely explored in prompt-based learning (Ding et al., 2021, Cui et al., 2021, Ma et al., 2022\u2026 etc.) and . It should have baselines from other research lines for making this paper be more convincing.\n\n(2) No details about candidate generation. Based on window-size, I think the number of span candidates will vary a lot. This ambiguity makes Table 7 be not convincing since the inference speed might be affected by the number of span candidates.\n\nPrompt-Learning for Fine-grained Entity Typing,, Ding et al., 2021\nTemplate-based Named Entity Recognition using BART, Cui et al., 2021\nTemplate-free Prompt Tuning for Few-shot NER., Ma et al., 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "Writing is unclear.\n- What is D(j-I). Span width embedding matrix?\n- What is l_joint?\n\nNot clear about the reproducibility.\n",
            "summary_of_the_review": "Paper presents bi-encoder style NER framework with novel objectives suitable to NER and shows its effectiveness in nested NER problems. However, I think paper needs to be improved a lot in terms of writing and experiments.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4875/Reviewer_mFSQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4875/Reviewer_mFSQ"
        ]
    }
]