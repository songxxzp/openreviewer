[
    {
        "id": "l-EAbfDInzg",
        "original": null,
        "number": 1,
        "cdate": 1666245903166,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666245903166,
        "tmdate": 1666245903166,
        "tddate": null,
        "forum": "fCbTxKYJovs",
        "replyto": "fCbTxKYJovs",
        "invitation": "ICLR.cc/2023/Conference/Paper2692/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates applying knowledge distillation to federated learning. It summarizes many related works to give a set of common building blocks and experiments with different choices for each block on ResNet20/56 + Cifar10/100 to see their impact on the accuracy.",
            "strength_and_weaknesses": "Strength:\n- There is a good effort to summarize the common building blocks across many knowledge-distillation-based FL algorithms. This is helpful for readers to get a high-level overview of algorithms in this class.\n- This paper also provides a few popular methods that can be used for each block and performs experiments to compare these methods. The results may be useful when readers try to adapt an existing method to their applications or systems.\n\nWeakness:\n- The main issue of this paper is that it is difficult to read and understand. The paper is flooded with undefined terms, symbols, and confusing statements. Also, there is no figure to serve illustration. For example, the paragraph in Sec. 3 states that \"In every training round, each of the local trainers will distribute one of the global models to a subset of clients\". For FL, the server is typically responsible for distributing models. Does this sentence mean one client will choose a model from the global models and distribute it to other clients, involving something like client-to-client communication? Why does a client store global models in addition to their local models? Another example is that the terms like \"global model\", \"ensemble model\", \"local trainer\", \"ensemble trainer\", and \"global trainer\" are not defined, and the symbols in the pseudo code are not defined either.\n- There are many unsupported or unexplained claims. For example, Sec. 4.1 has the sentence \"Existing distillation-based aggregation methods require all client models to build the ensemble, causing privacy concerns.\". Why does this approach cause privacy concerns and why doesn't the proposed approach are not explained nor supported by references. Another example is also from Sec. 4.1. \"We noticed the idea of Fed-ensemble, which builds the ensemble from multiple global models for inference purposes. This solves the privacy and scalability issues.\". Again, there is no explanation about why this solves the privacy and scalability issues.\n- Most of the experiments are on the small CIFAR10 dataset plus the small ResNet20 model except for Table 4, which contains ResNet56 and Cifar100. Because the authors never mention what the metric for the accuracy is, I assume it is top-5 accuracy. ResNet56 on CIFAR100 can easily get a top-5 accuracy above 90%, but the numbers in Table 4 are at the level of 60%. Therefore, I am skeptical about whether the conclusions drawn from these experiments can be generalized to practical settings.\n- In Sec. 6.2, there is only one sentence \"Ablation study will be provided in the appendix.\". The ablation study is an important part of the main paper. Personally, I don't think moving it to the appendix to save space is a good practice. I leave it to the chair to make the judgment.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality:\n- As stated in the weakness section, I don't think this paper is clear and easy to understand.\n\n================================================================\n\nNovelty:\n- The technical novelty is limited. Although this paper summarizes existing works and experiments with several existing methods, it does not present any novel methods or novel ways of applying existing methods.\n\n================================================================\n\nReproducibility:\nYes if the authors can improve the wording.",
            "summary_of_the_review": "Although it is a good effort to summarize related works and experiment with multiple design choices, the technical novelty is somewhat limited. Moreover, this paper needs to be revised to make it easier to understand and better explain and support the claims.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2692/Reviewer_Cg3f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2692/Reviewer_Cg3f"
        ]
    },
    {
        "id": "FMFL8Ltx4KX",
        "original": null,
        "number": 2,
        "cdate": 1666273441272,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666273441272,
        "tmdate": 1666317690098,
        "tddate": null,
        "forum": "fCbTxKYJovs",
        "replyto": "fCbTxKYJovs",
        "invitation": "ICLR.cc/2023/Conference/Paper2692/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper first propose a generalized distillation framework. Based on the study of each component in the generalized framework, this paper proposes Federated Efficient Ensemble Distillation(FedEED), which is efficient, scalable and privacy.",
            "strength_and_weaknesses": "Strengths:\n\n1. This paper attempts to achieve generalizability of federated distillation, which is a very innovative study.\n\n2. The application of knowledge distillation in the field of federated learning is well researched and summarized.\n\n3. A number of experiments are done using several federated distillation algorithms.\n\nWeaknesses:\n\n1. In abstract\n\t- This paper first proposes a generalized distillation framework which divides the federated distillation process into three key stages. However, there are various types of federated distillation, and the three-stage framework is only one of them, so it should not be called a generalized distillation framework. \n\t- FedEED also achieves a higher level of privacy protection, because the access to client models is no longer required. However, The client model still needs to be transferred between local trainer and  ensemble trainer, which means the client model can still be attacked.\n\n2. In framework\n\t- This paper describes in detail the three-stage process of the generalized federated distillation framework, wouldn't it be better to add a framework figure to illustrate it more visually? \n\t- This framework includes K local trainers, an ensemble trainer, and a global trainer, but traditional federated learning includes K clients and a server, what is the correspondence between them, this paper does not explain.\n\n3. In experiments\n\t- This paper mentions that the number of clients is 20, with 40% of activated clients, which means that there are very few clients, which is contrary to this paper's emphasis on the fact that FedEED achieves improved scalability in large-scale systems. Therefore, experiments on more clients should be conducted to demonstrate the scalability of FedEED.\n\t- The correspondence between Figure 1 and Table 1 is not clear, and the introduction part is also a bit messy, which is easy to cause confusion. If you follow the order in Figure 1, the order in Table 1 would be \u201cGlobal model(K=1)\u3001Ensemble(K=1,Clients)\u3001Ensemble(K=1,Clients,Weighted)\u3001Ensemble(K=1,Bayesian,Gaussian)\u3001Ensemble(K=1,Bayesian,Dirichlet)\u3001Global model(K=4)\u3001Ensemble(K=4,Clients)\u3001Ensemble(K=4,Aggregated)\u3001Ensemble(K=4,R=2,Aggregated)\u3001Ensemble(K=4,R=4,Aggregated)\u201d.\n\t- The dataset CIFAR100 is included in the description of Table 2, but it is not available in the experimental results. In addition, only Table 4 conducted experiments on CIFAR100, why not include this dataset in other experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "Limited clarity, Limited quality, Limited novelty, Highly reproducibility.",
            "summary_of_the_review": "This paper presents an innovative research of ederated distillation, But I have doubts about the scalability and privacy of the proposed framework FedEED.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2692/Reviewer_KBtv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2692/Reviewer_KBtv"
        ]
    },
    {
        "id": "cEr10A0zC61",
        "original": null,
        "number": 3,
        "cdate": 1666545927519,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666545927519,
        "tmdate": 1666546194708,
        "tddate": null,
        "forum": "fCbTxKYJovs",
        "replyto": "fCbTxKYJovs",
        "invitation": "ICLR.cc/2023/Conference/Paper2692/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors studied the key components of distillation-based model aggregation in FL. Different from existing approaches, the ensemble teacher of FedEED is constructed by aggregated models, instead of client models, to achieve improved scalability in large-scale systems.",
            "strength_and_weaknesses": "Strength:\nDue to the use of aggregated models, FedEED also achieves a higher level of privacy protection. In addition, the knowledge distillation in FedEED only happens from the ensemble teacher to a designated model such that the diversity among different aggregated models is maintained.\n\nWeaknesses:\nThe authors evaluated the performance of the proposed method only on the task of Non-IID labels and did not validate the performance of the proposed method on the Non-IID feature task, which is a more realistic scenario because there is often a large domain gap problem in the distribution of data features of different clients as the different data acquisition devices, environments, and qualities. Therefore it does not provide a comprehensive assessment of the generality of the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "The problem this paper address is clear. The logic of the article is clear. Somewhat novelty. Repeatable in a limited simulation scenario.",
            "summary_of_the_review": "1.  The authors should add experiments to further validate the performance of the proposed method on Non-IID feature tasks to further evaluate the generality of the proposed method.\n2. The authors' presentation is not clear and the architecture of the algorithm cannot be well understood. Perhaps the authors can give a clear architecture diagram to assist the readers to understand the essence of the proposed method clearly.\n3. Several general FL methods have been proposed and achieved excellent performance, such as FedBN, FedRep, and FedDC, and the authors should analyze the performance of more SOTA FL methods to further evaluate the proposed approach.\n\na) Li X, Jiang M, Zhang X, et al. Fedbn: Federated learning on non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021.\nb) Collins L, Hassani H, Mokhtari A, et al. Exploiting shared representations for personalized federated learning. International Conference on Machine Learning. PMLR, 2021: 2089-2099.\nc) Gao L, Fu H, Li L, et al. FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 10112-10121.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2692/Reviewer_4iWR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2692/Reviewer_4iWR"
        ]
    },
    {
        "id": "mI57PwnEvkt",
        "original": null,
        "number": 4,
        "cdate": 1666578752652,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578752652,
        "tmdate": 1666656990430,
        "tddate": null,
        "forum": "fCbTxKYJovs",
        "replyto": "fCbTxKYJovs",
        "invitation": "ICLR.cc/2023/Conference/Paper2692/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a generalized federated distillation framework which divides local nodes into groups: the aggregation is first inside each group and then from groups into the final global model.  The advantage is groups can do each aggregation in parallel in the first stage.\nThe experiments exhaust several popular federated distillation method for the aggregation. The results show the proposed group-division ensemble strategy does improve over the ensemble directly from all locals.   ",
            "strength_and_weaknesses": "Strength: \n1. The proposed group-division ensemble framework can be generalized to different existing FL methods. First, aggregation inside individual group helps to improve the level of parallel.  \n2. Experiments show the proposed methods achieves better performance than directly aggregating all locals.\n\nWeakness:\n1. Limited novelty: the authors only replace the typical aggregation in FL with two stages: group aggregation and then global aggregation. The detailed ensemble strategies (e.g. bayesian ensemble) are borrowed from existing works. The novelty and contribution is limited considering the method is just simple combination.  \n2. Overclaim about scalability: I cannot see any improvement on scalability from the proposed method and experiments design.\n3. Overclaim about privacy: In abstract it says \"FedEED also achieves higher level of privacy protection, because the access to client models is no longer required.\" But the method still iteratively upload local model parameters besides the group global model. [1][2] demonstrated that local private data could be fully recovered from publicly-shared gradients (iteratively shared local model parameters). \n3. Overclaim about efficiency: No quantitative analysis of efficiency metrics such as latency, communication bandwidth. Considering the \"ensemble trainer\" also involves the participation of local models, I doubt if the latency and communication bandwidth could be even higher than the existing FL methods for comparison. It is meaningless to show #rounds in the table, as the latency and bandwidth of each round in your method is obviously higher than other FL methods. The communication cost in Table3 is very ambiguous without any analysis or calculation.\n4. Experiments: only have experiments on cifar10/cifar100. Should do more through experiments under typical FL setting: such as EMNIST in SCAFFOLD or NLP tasks (AG News, SST2 in FedDF). \n\n[1] Deep leakage from gradients.\n[2] Inverting gradients\u2013how easy is it to break privacy in federated learning?",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: generally good.\nClarity: clear written except some claims about scalability, privacy, efficiency. (see weakness)\nOriginality: limited novelty, even the group-division ensemble is not new (Fed-ensemble proposes similar group division method). Could you explain the difference with Fed-ensemble?",
            "summary_of_the_review": "Although the experiments show better performance over the baseline methods, the novelty is very limited. And the efficiency metrics are not clear. I also doubt the claims regarding privacy and scalability.   Since the \"efficiency\" is the main point of the paper, I hope the authors could provide more efficiency analysis with quantitative results on latency, communication bandwidth. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2692/Reviewer_Ybra"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2692/Reviewer_Ybra"
        ]
    }
]