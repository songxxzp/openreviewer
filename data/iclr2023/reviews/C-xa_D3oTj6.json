[
    {
        "id": "pnUgGNQ8VUl",
        "original": null,
        "number": 1,
        "cdate": 1666287237217,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666287237217,
        "tmdate": 1666287237217,
        "tddate": null,
        "forum": "C-xa_D3oTj6",
        "replyto": "C-xa_D3oTj6",
        "invitation": "ICLR.cc/2023/Conference/Paper2595/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper combines differential extrinsic plasticity (DEP) and reinforcement learning for more efficient learning of how to control overactuated systems. Demonstrations are done on simple hand design systems as well as complicated muscle-driven locomotion control. ",
            "strength_and_weaknesses": "Pro:\n\n1. A novel combination of RL and DEP for effective learning for muscle-driven systems. Experiments demonstrate the proposed approach outperforms existing baselines.\n\n2. The method is well motivated by issues surrounding the difficulty to explore with overactuated system. I especially appreciate the simple example that is introduced to provide intuition.\n\nCon:\n\nNot much. To nitpick a little, the motion quality for the ostrich control is still not natural. ",
            "clarity,_quality,_novelty_and_reproducibility": "The method is sound, novel and well-motivated. \n\nDetails are provided for reproducibility and the authors also mention the code will be released.",
            "summary_of_the_review": "This provides a new and intuitive explanation of why it is hard to learn controllers for overactuated systems with standard RL approaches using Gaussian noise. DEP is introduced to effectively resolve this issue. A wide range of examples is used to demonstrate the effectiveness of the proposed method,  including state coverage of a simple overactuated system and challenging locomotion control with bipedal systems with many muscles. I believe this paper makes a big step forward in solving muscle-driven control problems.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2595/Reviewer_Bv9J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2595/Reviewer_Bv9J"
        ]
    },
    {
        "id": "V7wjBRLwTL",
        "original": null,
        "number": 2,
        "cdate": 1666559429126,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559429126,
        "tmdate": 1666559429126,
        "tddate": null,
        "forum": "C-xa_D3oTj6",
        "replyto": "C-xa_D3oTj6",
        "invitation": "ICLR.cc/2023/Conference/Paper2595/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Conventional exploration strategies for RL scale poorly to high-dimensional action spaces. \nDifferential Extrinsic Plasticity (DEP), integrated with RL, is proposed as a mechanism for coping\nwith highly-redundant actuation spaces, such as that of multiple musculotendon units that span joints\nin biomechanical models. The key idea of DEP is to learn a state-to-action mapping that is largely defined\nby a linear state-to-action matrix, C, which is updated by the velocity correlation matrix of the current\nand previous state.  This induces its own dynamics, without a goal task, and has been shown in prior\nwork on self-organizing behavior, to produce converge to useful stationary behaviors.  In this work,\nit is proposed to integrate this with RL, via alternating DEP and RL motion phases during learning.\nThe results are tested on a wide variety of scenarios and over-actuated systems. Various baselines\nand ablations are presented, including Uhlenbeck-Ornstein noise, colored noise, MPO and TD4 results, \nintegration with HER, and more.\n",
            "strength_and_weaknesses": "Strengths:\n\n- important and difficult problem:  high-D action spaces, redundant actuation, musculoskeletal control\n- impressive results and progress on the above problem\n- progress towards understanding how overactuation might be resolved in natural systems\n\nWeaknesses:\n- missing possible intuition about how the redundant actuation is ultimately resolved \n- writing: The underlying assumptions needed to apply DEP could be articulated more clearly,\n  when coming from the common RL mindset, where there is generally great freedom in the state specification.\n- mixed motion quality of some of the results, subjectively speaking, e.g., Ostrich running, Ostrich head-and-neck during gaits\n- unclear how more classical biomechanics criteria, i.e., metaboloic energy consumption, might come into play,\n  or if these results are predictive in any way of what can be observed in natural systems\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally very clearly written, within the available space constraints.\n\nIt builds on the previously introduced Differential Extrinsic Plasticity (DEP).  In this paper it really becomes a lot more useful via the integration with RL.  Thus I see it as being highly original.",
            "summary_of_the_review": "The paper makes significant progress on tackling an important and difficult problem, \nnamely that of high-dimensional action spaces that arise in over-actuated systems\nsuch as musculoskeletal models. It makes a very valuable contribution in this regard.\n\nThe underlying assumptions needed to apply this framework could be articulated a bit more clearly,\nwhen coming from the common RL mindset, where there is generally great freedom in the state specification.\nThe paper is in a good position to also further state caveats and differences in terms of bridging from RL to\nthe biomechanics community, which may have different evaluation criteria, related to motion quality,\nexperimental comparisons to ground-truth data, and predictive capabilities.\n\n-------------------------------------------------------------\nDetailed comments follow below.\n\nGiven the importance of matrix C, perhaps give it a name that connects with its implied role?\nConceptually, I now see that it represents the weights of a one-layer feedf-foward network.\n\nThe original DEP paper discusses the role of updating the threshold terms, \"to\ndrive the neurons away from their saturation regions.\"  Is this still important in this work, to stay\naway from converging to static poses, or does the integration with RL obviate this need?\n\nPerhaps some of the intuition provided in the original DER paper could be reiterated in this paper, to help\nthe reader, i.e., \u201cchaining together what changes together.\u201d \n\nIs the value of Delta t kept fixed for the bulk of the results?\n\nThe coverage of the related work is already excellent in the current paper, although\nthere is additional early work on muscle-based locomotion that also comes to mind, e.g.,\n\"Optimizing Locomotion Controllers Using Biologically-Based Actuators and Objectives\" (2012)\n\"Flexible Muscle-Based Locomotion for Bipedal Creatures\" (2013),\nwhich are both objective-based optimizations, albeit with strong priors on the control architecture.\n\nTo what extent can the final structure of the matrix C be interpreted?\nThis would be useful to here about. Is there an emergent low-dimensional structure?\nIs this structure consistent across multiple runs?  \n\nFor muscles spanning multiple joints, the inverse mapping, f, presumably becomes much less clear,\nas it now also depends on the motion of more than one joint.  Does this have any impact on the\nability to achieve good results?  Relatedly, I'm also curious to better understand how critical\nit is to have knowledge of the 1:1 correspondence between sensors and actuators.\n\n4.1 DEP introduction\n\nIt is not clear in the description at this point if the state s can be very general, as in RL,\nor if it refers much more specifically to the length of the muscle actuators. \nI believe that it is the latter, so it is worthwhile clarifying. \n\nText after eqn (2): \"f is an inverse model\"\nThis begs the question \"an inverse model of what?\".\n\nUpdate rule for C dot, with \"-C\" being a damping term. Perhaps call this a first-order decay term,\nwhich presumably acts as a regularizer?\n\nWhat forms of perception are available for the last three ostrich tasks, i.e., foraging, stepdown, slopetrotter?\nPerhaps it is in the appendix, but providing basic knowledge of \"is the their knowledge fo the upcoming terrain\"\nwould be useful.\n\nAppendix A \"... chemical muscle activiation dynamics\"\nIt's great to see this discussed, are these actuation dynamics being modeled or not?\nPresumably not (which is fine as a simplification), but state this explicitly.\n\nDEP-MPO for the human hopping is, subjectively speaking for this reader, the highest quality result. Very nice.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2595/Reviewer_sbak"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2595/Reviewer_sbak"
        ]
    },
    {
        "id": "RqSYFPipfZ",
        "original": null,
        "number": 3,
        "cdate": 1666727866580,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727866580,
        "tmdate": 1666727866580,
        "tddate": null,
        "forum": "C-xa_D3oTj6",
        "replyto": "C-xa_D3oTj6",
        "invitation": "ICLR.cc/2023/Conference/Paper2595/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an exploration strategy for reinforcement learning. This exploration strategy is based on the differential extrinsic plasticity rule in neuroscience which outputs temporal correlated and state-dependent exploration noise. The exploration strategy can also be parameterized as a neural network and this paper update it by using a gradient-free method. During training, it is used as an intra-episode exploration and the overall method achieves large improvement over baselines on overactuated and musculoskeletal systems in simulation.",
            "strength_and_weaknesses": "Strength:\n1) This paper is well-motivated and the writing is clear. \n2) The idea of using the differential extrinsic plasticity strategy is interesting and it\u2019s novel to adapt it in the reinforcement learning tasks.\n3) The authors promised to release the code, which could be a benefit to the whole community.\n\nWeakness:\n1) Is there a specific reason of using MPO as the baseline, instead of some other methods like TD3, SAC, PPO? \n2) Can the authors provide some qualitivative visualization about what is the learned exploration strategy?",
            "clarity,_quality,_novelty_and_reproducibility": "I believe this paper is well-written and novel. It would be great to release the code to the community, as the authors promised.",
            "summary_of_the_review": "In summary, I think this paper is a good paper and worth reading for the community. The exploration can be helpful for varies applications such as RL from image observations or RL on more complex systems.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not Applicable.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2595/Reviewer_x7yt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2595/Reviewer_x7yt"
        ]
    },
    {
        "id": "e-PGQEyxdUD",
        "original": null,
        "number": 4,
        "cdate": 1666937086644,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666937086644,
        "tmdate": 1666937086644,
        "tddate": null,
        "forum": "C-xa_D3oTj6",
        "replyto": "C-xa_D3oTj6",
        "invitation": "ICLR.cc/2023/Conference/Paper2595/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies a novel and interesting problem: muscular control with overactuated action spaces. It first demonstrates that the main issue with overactuated action spaces is how to effectively and efficiently perform exploration using a clear torquearm example with 2 and 600 DoF theoretically and empirically. It further proposes a framework that integrates differential extrinsic plasticity, a method from the domain of self-organization, into the exploration stage of reinforcement learning. The experimental results across torquearm, arm26, human reacher, ostrich-foraging, ostrich-run, human-run, and human-hop show that the proposed framework can learn a better policy with overactuated action spaces compared to various baselines. Moreover, this work also shows that the learned policy is robust to some forms of environmental changes.",
            "strength_and_weaknesses": "+) The idea to integrate differential extrinsic plasticity (DSP), a method from self-organization, into the traditional RL framework is interesting and self-motivated. Although the way of integration (alternating exploration policy between DSP and learned policy) is simple, it tackles the core problem causing ineffective policy learning with overactuated action spaces. Moreover, the studied problem in this work is also essential and appealing.\n\n+) The paper is well-written and easy to follow. It clearly demonstrates its task, challenges (theoretically and empirically), and the proposed idea (with necessary background reviews). The figures clearly illustrate the results qualitatively and quantitatively. The supplementary videos also show the effectiveness of the learned policy with many qualitative examples.\n\n+) The proposed method works well across different agents (torquearm, arm26, ostrich, human arm, human walk) on different tasks (reaching, foraging, walking, hopping). The robustness experiments further show that the learned policy is able to maintain its performance better than baselines.\n\no) As mentioned by the authors in the conclusion section, the integration of DEP and RL in this work is firmly straightforward, and it might not directly provide much impact on other applications. I am looking forward to seeing more general or sophisticated frameworks, including ideas inspired by neuroscience or biology.\n\n-) I am curious about the sensitivity of the policy to the switch probability p_switch in the Intra-episode exploration stage. In the supplementary (table 4), the p_switch seems hand-picked for different agents for different tasks. Do authors conduct an ablation study regarding it?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper's studied issue, idea, and experiments are clearly presented. The studied problem is interesting, and the proposed method is novel. In addition, the implementation details are all in the supplementary. Therefore, I have no problem with reproducibility.",
            "summary_of_the_review": "This work proposes integrating DSP into the traditional RL framework to tackle the inefficient exploration issue with overactuated action spaces. The idea of utilizing DSP for a more effective exploration is self-motivated. In addition, the studied problem, proposed idea, and the final results in the paper are well-presented. The quantitative results are impressive, and the qualitative results show that the proposed framework can successfully learn a robust policy with overactuated action spaces.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2595/Reviewer_PfGN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2595/Reviewer_PfGN"
        ]
    }
]