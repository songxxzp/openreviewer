[
    {
        "id": "6WPY-uA_Oij",
        "original": null,
        "number": 1,
        "cdate": 1666667469624,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667469624,
        "tmdate": 1669100534101,
        "tddate": null,
        "forum": "G7E_K3WaLpK",
        "replyto": "G7E_K3WaLpK",
        "invitation": "ICLR.cc/2023/Conference/Paper5601/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes LAtFormer, which incorporates lattice geometry and topology priors in attention masks. The proposed architecture is a modification of the standard attention mechanism where attention weights are scaled using soft attention masks generated by a convolution neural net. Experiments on ARC and synthetic visual reasoning tasks show that LatFormer requires 2 orders of magnitude fewer data compared to standard attention and transformers.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is evaluated on a number of tasks.\n2. Results show that Latformer can learn geometric transformations with fewer training data.\n\nWeaknesses:\n1. Table 3 only compares the proposed method to alpha-AMD and SIFT which are old methods. Since this is an image registration task, the proposed method should be compared to more recent methods, e.g., COTR: Correspondence transformer for matching across images. ICCV 2021.\n2. The paper should also discuss failure modes, limitations of the proposed method and future work.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper reads well. The proposed method makes sense. Code is not submitted and hence reproducibility is not guaranteed.",
            "summary_of_the_review": "Please see the comments in the boxes above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_9teR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_9teR"
        ]
    },
    {
        "id": "B-ztCTPk8U",
        "original": null,
        "number": 2,
        "cdate": 1666694934814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694934814,
        "tmdate": 1666694934814,
        "tddate": null,
        "forum": "G7E_K3WaLpK",
        "replyto": "G7E_K3WaLpK",
        "invitation": "ICLR.cc/2023/Conference/Paper5601/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on helping deep learning models learn fundamental geometric transformations efficiently. Specifically, LatFormer is proposed to incorporate lattice symmetry biases into attention mechanisms by modulating the attention weights using learned soft masks. Experiments are conducted on both synthetic tasks and image registration tasks to show that the proposed model can generalize better than the same attention modules without masking and Transformers. Besides, LatFormer also shows potential on ARC tasks that incorporates geometric priors. ",
            "strength_and_weaknesses": "Strength:\n\n+ The proposed method presents an interesting way to infuse inductive biases and knowledge priors in deep neural networks, which is an important open problem for next step's developments of AI and neural networks. \n\n+ The idea of using a convolutional neural network to generate soft attention masks to scaled attention weights is reasonable. \n\n+ Experimental results on synthetic tasks and ARC tasks show that the proposed LatFormer can generalize better and from fewer\nexamples than transformers and unmasked attention modules. Experiments on image registration task further validates the applicability of LatFormer on natural images.\n\n\nConcerns:\n\n- LatFormer shows potential of inductive biases and knowledge priors. I'm wondering if LatFormer also performs better than learning transformations invariant representations by applying augmentations? Do we have such studies on the current experimental settings/tasks?\n\n-  For image registration experiments in Section 5.3, how about reuslts of using the baseline Transformer?\n\n- How about the application of the proposed LatFormer to other reasoning tasks, e.g. Physical Reasoning in [1] \n\n[1] Physical Reasoning Using Dynamics-Aware Models, ICML 2021",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. The code is not provided. The experiments on ARC tasks require extra annotations which is not public available. ",
            "summary_of_the_review": "This is an interesting paper about learning geometric transformations with deep neural networks, with experimental evidences from both synthetic tasks, visual reasoning tasks as well as applications on natural images.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_YP2T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_YP2T"
        ]
    },
    {
        "id": "6wUljBhIWg6",
        "original": null,
        "number": 3,
        "cdate": 1666725124768,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725124768,
        "tmdate": 1669775985600,
        "tddate": null,
        "forum": "G7E_K3WaLpK",
        "replyto": "G7E_K3WaLpK",
        "invitation": "ICLR.cc/2023/Conference/Paper5601/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an approach to incorporate lattice priors into attention mask for deep learning. The theoretical results show this prior can be formulated as convolutions on an identity matrix. The proposed approach improves the sample efficiency of the model to solve the ARC tasks (few-shot geometric transformation learning) compared to vanilla transformers. \n",
            "strength_and_weaknesses": "Strength\n* The idea of introducing structural priors in the form of attention masks for geometric transformation tasks is intuitive and well-motivated\n* The theoretical results show that such priors can expressed as a convolution on an identity matrix, which motivates the design of LatFormer\n* Empirical results show remarkable sample efficiency gains over vanilla transformers in ARC tasks and outperform baselines on real image registration\n\nWeakness\n* It\u2019d be valuable to show more experiments on real data. How about showing the results on standard image registration/correspondence learning benchmarks?\n* In Table 3, are there other baselines to compare with? I\u2019d imagine a vanilla transformer would be a reasonable one.\n* In Figure 5, it seems that the transformer is barely learning the task. I\u2019m curious how many samples would it take for a transformer to learn this task perfectly (how far should we extend the x-axis).\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and the ideas appear to be novel. It seems reasonably easy to reproduce. Many experimental details are provided in the appendix.",
            "summary_of_the_review": "The idea of the paper is reasonable. Theoretical results and experiments on synthetic data show the proposed approach is superior to vanilla transformer. However, the experiments on real data seem relatively weak and it\u2019d be valuable to see more evidences of LatFormer outperforming transformers and more existing approaches than SIFT/alpha-AMD on image registration benchmarks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_ch2n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_ch2n"
        ]
    },
    {
        "id": "gBOTL_NftFn",
        "original": null,
        "number": 4,
        "cdate": 1666866256747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666866256747,
        "tmdate": 1666866256747,
        "tddate": null,
        "forum": "G7E_K3WaLpK",
        "replyto": "G7E_K3WaLpK",
        "invitation": "ICLR.cc/2023/Conference/Paper5601/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, a LATFORMER model is introduced for learning effective attention. It incorporates both lattice geometry and topology priors for learning attention masks. The paper is demonstrated on ARC and on synthetic visual reasoning tasks, showing improved performances compared to standard attention and transformer on these tasks. ",
            "strength_and_weaknesses": "Pros:\n+ The idea of introducing lattice geometry and topology priors for attention design is interesting\n+ The paper is clearly written.\n\nCons:\n- The experiments on ARC and synthetic datasets and tasks are not sufficient to demonstrate the effectiveness and generalization of the design. It would be much more interesting if the method can be demonstrated on main-stream vision and NLP tasks to show the effectiveness of the designed attention mechanism.\n- The ablation study should be extended to show the proposed attention mechanism with the lattice geometry and topology priors. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The novelty of the paper is not significant. ",
            "summary_of_the_review": "The experiments could be enhanced. More challenging tasks and datasets could be considered to better validate the proposed attention mechanism. The novelty is moderate. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_vL2a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_vL2a"
        ]
    },
    {
        "id": "HHduiI0lka",
        "original": null,
        "number": 5,
        "cdate": 1667020302099,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667020302099,
        "tmdate": 1667020302099,
        "tddate": null,
        "forum": "G7E_K3WaLpK",
        "replyto": "G7E_K3WaLpK",
        "invitation": "ICLR.cc/2023/Conference/Paper5601/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper addresses problems that involve learning a geometric transformation (e.g., rotation) on the input data. Examples of the problem are Abstraction and Reasoning Corpus. The core idea of the proposed method is to inject lattice symmetry prior as soft masks to modulate the attention weights in a transformer. The paper shows that the various lattice symmetry actions can be realized with attention masks (such as translation, rotation, and reflection). The results on ARC tasks show that using the proposed method with infused lattice symmetry prior can perform well (Table 2 and Figure 5) and is sample-efficient. ",
            "strength_and_weaknesses": "Strength: \n+ Interesting idea of infusing lattice symmetry prior to a problem into an end-to-end trainable transformer model. \n+ The progress on ARC datasets using deep learning models is promising. \n+ The method is technically sound. The CNN-based mask prediction (Lattice Mask Expert) is interesting and supports compositions of multiple actions (e.g., reflection and rotation).\n\nWeakness:\n- From the examples in the paper and supplementary material, it seems that rotation is only 90 degrees. Not sure if this makes the problem trivial to solve (if we know the symmetric group of testing input data).\n- No visual examples and results on the cross-modal image registration tasks. I am not familiar with the tasks so it's a bit hard for me to judge the significance of the results and how the results validate the proposed method. \n- I wasn't able to follow some of the details in Section 3.3 and Section 4. But it may be because I don't have the background on related problems. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- I can get the general idea of the paper but I was not able to follow the details without a relevant background in these problems.\n\nQuality:\n- I think the evaluation quality is somewhat weak since there are no real baseline competitors (e.g., neural-symbolic visual reasoning work). The paper only compares with end-to-end trainable models such as CNN and transformers.\n\nNovelty:\n- I find infusing these priors using soft attention masks novel and interesting.\n\nReproducibility:\n- The supplementary material provides more implementation details of the architectures and theorem proof. ",
            "summary_of_the_review": "I think this is an interesting paper that shows end-to-end trained models can perform well on abstract reasoning tasks. I don't know the practical implication of solving these lattice symmetry problems and how the ideas in the paper could be used to solve some other tasks. The paper did show an image registration task. But it does not provide sufficient details (e.g., are these only translations to rotation and translation? what's the task? what metrics were used to evaluate the success in Table 3?) for me to understand the significance. For registering images from different modalities, one should also try stronger descriptor-matching methods beyond SIFT, e.g., SuperGlue.\nGiven these concerns, I am thus on the fence about this paper but leaning slightly positive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't find ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_TgJP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5601/Reviewer_TgJP"
        ]
    }
]