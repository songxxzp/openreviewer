[
    {
        "id": "No-v0PimtY",
        "original": null,
        "number": 1,
        "cdate": 1666403911376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666403911376,
        "tmdate": 1666636746106,
        "tddate": null,
        "forum": "Hq16Jk2bVlp",
        "replyto": "Hq16Jk2bVlp",
        "invitation": "ICLR.cc/2023/Conference/Paper2172/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an automatic search based approach to design latency efficient CNN models for FHE. In particular, the paper presents a three stage approach of the search and fine tune: i) a multi-objective co-evolutionary (MOCoEv) search algorithm to maximize validation accuracy and minimize the number of bootstrapping operations, ii) a gradient-free search algorithm, R-CCDE, to optimize EvoReLU\ncoefficients, and iii) polynomial-aware training (PAT) to fine-tune polynomial-only CNNs for one epoch to adapt trainable weights to EvoReLUs. Demonstration on CIFAR-10 and CIFAR-100 shows the efficacy of the proposed method.",
            "strength_and_weaknesses": "### Strengths\n\n1. In the current era of data and model IP protection, the problem statement of minimizing latency for secure and private inference is gullible and the paper is decently written.\n2. The issue with FHE-MP-CNN is well identified, particularly, the point of same high order approx. for all ReLUs.\n3. The multi objective problem formulation is well inspired.\n\n### Weakness\n\n1. Abstract introduced too much technicality as compared to its size with many technical terms. This definitely raises more question as opposed to providing a short overview of the paper. Some terms are not even clear, e.g. what's the full form of RNS-CKSS (in context of abstract).\n\n2. There has been few research in the domain of ReLU reduction in the context of private inference, including selective network linearization (SNL) [1], DeepReDuce [2] etc. These line of research definitely aligns with the current problem statement, hence, it would be interesting to see how efficient approximation of ReLU and removal of ReLUs go hand in hand in the context of FHE.\n\n3. If EvoReLU does not help in reducing latency, then is it replaced (from AppReLU) to improve accuracy? Or there is something I am missing? Does EvoReLU introduces polynomial error based on layer sensitivity? Please clarify, as the authors had mentione dApprox ReLU uses similar approx error for all ReLU layers.\n\n4. If you replace the EvoReLU with ReLU during backward pass, wont it introduce significant error? How that error is compensated in the framework? Why EvoReLU introduce exploding gradient issue, and why it can't be solved by traditional mean of solving exploding gradients? STE was introduced to primarily deal with non differentiable functions, not to handle exploding gradients. So, the citation of STE in this context is confusing!\n\n5. MoCoEv and Sec. 3.3 are written in a hurried way without properly mentioning the inspiration of the proposed solution. I find no discussion on possible alternatives and why we should buy this \"REGULARIZED COOPERATIVE DIFFERENTIABLE CO-EVOLUTION\" as the best potential solution.\n\n6. There is no pareto frontier of the search space, either for bootstrap or the EvoReLU polynomial choices. There is no latency vs accuracy plot either. \n\n7. Details of latency improvement computation is missing (the one mentioned as 17% and 22%). \n\n8. Additional results on higher resolution datasets are missing. Both CIFAR-10 and 100 are of size 32x32x3.\n\n9. The authors should also compare with more advanced search methods, instead of only comparing with NSGA II, which is not necessarily the state-of-the-art.\n\n### Other small issues\n\nPlease clarify (in the preliminaries) why bootstrapping operation is costly in a bit more details.\n\n\n[1] selective network linearization, ICML 2022.\n\n[1]  DeepReDuce, ICML 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: Average, as the intro misses another domain of research to solve similar problem. However, the preliminaries are well written.\n\n**Quality**: Good in terms of problem identification, average in terms of providing intriguing solution. \n\n**Novelty**: Average.\n\n**Reproducibility**: Code not shared. Algorithm is provided in Appendix.",
            "summary_of_the_review": "The paper targets to solve the latency bottleneck issue of bootstrap and ReLU units forFHE based private inference. The problem is well motivated. However, the paper lacks ease of flow and clarity in writing. Details of error analysis for approx EvoReLU, latency evaluations, additional results, reason behind choice of search optimization are missing. Thus, despite good promise, the paper falls significantly below acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2172/Reviewer_b7YQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2172/Reviewer_b7YQ"
        ]
    },
    {
        "id": "S5bR5mbmbF",
        "original": null,
        "number": 2,
        "cdate": 1666585987186,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585987186,
        "tmdate": 1670435814893,
        "tddate": null,
        "forum": "Hq16Jk2bVlp",
        "replyto": "Hq16Jk2bVlp",
        "invitation": "ICLR.cc/2023/Conference/Paper2172/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a novel and comprehensive framework for convolutional neural network inference over a fully homomorphic encryption environment. Based on previous work FHE-MP-CNN, they made significant improvements by\n1) using a new approximation for the non-linear activation function ReLU\n2) an automatic procedure that helps choose your approximation function which simultaneously maximizes the accuracy and minimizes the computation cost. \n3) a new training procedure that better suits the proposed inference framework.",
            "strength_and_weaknesses": "Strength: this paper provides a framework for secure CNN inference that you could solely rely on it. The proposed techniques are novel and sound. And the experiments and evaluations are complete. This paper is also clearly written and provides all the necessary details.\n\nMy one question is about the choice of approximation functions. AFAIK Chebyshev approximations are quite generic. Is there any room for improvement given that we are particularly interested in the sign function? (Similar problem is considered here https://ia.cr/2021/572 but for sine function). Or maybe it will bring more complexity in designing the later search algorithm.",
            "clarity,_quality,_novelty_and_reproducibility": "Great quality, very clear, self-contained, and original work with novel techniques.",
            "summary_of_the_review": "I think it's a great paper that matches this conference. Shall definitely be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2172/Reviewer_Lx4T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2172/Reviewer_Lx4T"
        ]
    },
    {
        "id": "MAIUEXrq2EZ",
        "original": null,
        "number": 3,
        "cdate": 1666657948412,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657948412,
        "tmdate": 1666657948412,
        "tddate": null,
        "forum": "Hq16Jk2bVlp",
        "replyto": "Hq16Jk2bVlp",
        "invitation": "ICLR.cc/2023/Conference/Paper2172/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose Automated adaption of CNNs for evaluation over fully homomorphic encryption (FHE) called AutoFHE to accelerate CNNs on FHE and automatically design a homomorphic evaluation architecture. AutoFHE claims to exploit the varying sensitivity of approximate activations across different layers in a network and jointly evolves polynomial activations (EvoReLUs) and searches for placement of bootstrapping operations for evaluation under RNS-CKKS.",
            "strength_and_weaknesses": "Strengths::\nThe paper proposes an interesting concept.\n\nWeaknesses::\nThe paper can be improved based on the following points.\n1) It might be better to provide a motivation case study to show the shortcomings of the design choices of the state-of-the-art approach FHE-MP-CNN in section 1. This would motivate the readers to understand the importance of the proposed mechanism.\n2) In section 3, it is mentioned, \"We directly optimize the end-to-end objective to facilitate finding the optimal combination of layerwise polynomials.\" However, from the text it is not evident how the design choices were made nor how layer wise polynomials were optimised. Please specify.\n3) In section 3, please specify what each variables such as x, p, d, y means such that readers are able to understand the concept without domain expertise on the particular topic. This is to improve readability.\n4) Please briefly specify why CIFAR datasets were only utilised for evaluation in section 4. Why not other datasets? How will the proposed mechanism perform with other datasets (if such an evaluation is done or at least discuss the limitations of the proposed).\n5) Experimental section (section 4) lacks comparative study with state-of-the-art methods to show the efficacy of the proposed. Please provide such comparative studies or discuss the evaluations.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity of the paper is lacking is many cases. Please note the weaknesses section in the review. Though an interesting solution is proposed efficacy of the novelty could not be evaluated from the provided experimental evaluation. Reproducibility could be improved.",
            "summary_of_the_review": "1) It might be better to provide a motivation case study to show the shortcomings of the design choices of the state-of-the-art approach FHE-MP-CNN in section 1. This would motivate the readers to understand the importance of the proposed mechanism.\n2) In section 3, it is mentioned, \"We directly optimize the end-to-end objective to facilitate finding the optimal combination of layerwise polynomials.\" However, from the text it is not evident how the design choices were made nor how layer wise polynomials were optimised. Please specify.\n3) In section 3, please specify what each variables such as x, p, d, y means such that readers are able to understand the concept without domain expertise on the particular topic. This is to improve readability.\n4) Please briefly specify why CIFAR datasets were only utilised for evaluation in section 4. Why not other datasets? How will the proposed mechanism perform with other datasets (if such an evaluation is done or at least discuss the limitations of the proposed).\n5) Experimental section (section 4) lacks comparative study with state-of-the-art methods to show the efficacy of the proposed. Please provide such comparative studies or discuss the evaluations.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns as of yet.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2172/Reviewer_Btk1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2172/Reviewer_Btk1"
        ]
    }
]