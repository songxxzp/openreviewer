[
    {
        "id": "M8q1ogEPcf",
        "original": null,
        "number": 1,
        "cdate": 1666709230906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709230906,
        "tmdate": 1666709230906,
        "tddate": null,
        "forum": "bPiHuNUNv_R",
        "replyto": "bPiHuNUNv_R",
        "invitation": "ICLR.cc/2023/Conference/Paper804/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the problem of learning a NE in two-players zero-sum games via no-regret algorithms. While most of the state-of-the-art algorithms guarantee convergence in terms of average strategies, existent methods that guarantee last-iterate convergence require strong assumptions like the uniqueness of the NE. In the present work, the authors provide regularization methods that allow to achieve last-iterate convergence without such assumptions. The first algorithm they propose is a regularized version of Online Mirror Descent that optimizes a strongly convex objective directly on the whole strategy polytope of the player. In the second part of the paper, the authors rely on the laminar regret decomposition to decompose the regret in additive terms at each information set to be minimized separately, claiming convergence of such method to a perfect equilibrium. The authors provide proofs of convergence with associated convergence rates. Finally, an experimental evaluation to support the theoretical claims is presented.",
            "strength_and_weaknesses": "STRENGTHS\n\n1) The problem of designing last-iterate convergent no-regret algorithms is interesting and it is receiving a lot of attention from the literature.\n\n2) The paper is well written.\n\nWEAKNESSES\n\n1) The paper is hard to follow as it comes as a list of results, which are indeed related, but require the adoption of different perspectives and techniques, making the reader uncomfortable. \n\n2) Theorem 3.1 is rather straightforward to derive, since it directly follows from the results in [Cen et al. 2021b]. Am I right? The other results require a straightforward application of some well-known techniques, such as the laminar-regret decomposition by Farina et al. 2019.\n\n3) I have some concerns about the claim that the Reg-CFR algorithm converges to an EFPE. In particular, as far as I am concerned, the key technical result to prove this is Lemma F.3, but this only predicates on the duality gap. This should be related to the distance to a Nash equilibrium of the game, but it does not say anything about convergence to EFPE (since in zero-sum games all the Nash equilibria, including EFPEs, have the same value that is the value of the game). Am I right or am I missing something?",
            "clarity,_quality,_novelty_and_reproducibility": "CLARITY: The paper is well written, even though is quite hard to follow.\n\nQUALITY: As far as I am concerned, most of the results are correct, I only have some concerns on the convergence to EFPE.\n\nNOVELTY: Most of the results are novel, but the adopted techniques are not.\n\nREPRODUCIBILITY: Proofs and experiments are fairly reproducible.",
            "summary_of_the_review": "Overall, the paper is well written, even though is quite hard to follow. The technical results are interesting, but not exceptional. I also have some concerns on a claim in the paper, namely that related to the coverage to EFPE.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper804/Reviewer_r8KK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper804/Reviewer_r8KK"
        ]
    },
    {
        "id": "NnRd3ry9EJ3",
        "original": null,
        "number": 2,
        "cdate": 1666799216723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799216723,
        "tmdate": 1666799254594,
        "tddate": null,
        "forum": "bPiHuNUNv_R",
        "replyto": "bPiHuNUNv_R",
        "invitation": "ICLR.cc/2023/Conference/Paper804/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the impact of regularization in solving extensive-form games (EFGs). It shows that after adding the regularization terms to different pay-off functions, several convergence  results could be achieved with either improved convergence rate or weaker assumptions. More specifically, for the dilated optimistic mirror descent, the regularized version can achieve a fast O(1/T ) last-iterate convergence in terms of duality gap without the uniqueness assumption of the Nash equilibrium (NE). For the counterfactual regret minimization, adding regularization term helps get the first last-iterate convergence results for CFR type algorithms.",
            "strength_and_weaknesses": "Strength:\n  The paper shows the impact of adding specific regularization terms to different algorithms in solving extensive-form games, which either improves the convergence rate or has weaker assumptions than before.\n\nWeakness:\n   There has been some previous work like Cen et al. (Reg-OMWU) in exploring the usage of regularization term. Since it's in the same line of work (trying to get benefits by adding regularization), the paper did not explain if there is any connection between the results in this paper and the previous work results. More specifically, the most importance step to achieve all the results presented in the paper is to connect the regularized update back to the original un-regularized problem with iterate convergence. Although the work in Cen et al. is for NFGs, I am not quite sure if the extension to EFGs is incremental or challenging. ",
            "clarity,_quality,_novelty_and_reproducibility": "Please see my comments above for the clarity, quality and novelty of the work.",
            "summary_of_the_review": "Overall I think the paper result is interesting and meaningful in further improving the state of the art works by add specific regularization terms. But it would be better to clearly build the connection between this work and the previous work in using the regularization term to really show that the effort is not trivial or incremental.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper804/Reviewer_D4YS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper804/Reviewer_D4YS"
        ]
    },
    {
        "id": "w11aRUE4UW",
        "original": null,
        "number": 3,
        "cdate": 1666992571494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666992571494,
        "tmdate": 1670382465132,
        "tddate": null,
        "forum": "bPiHuNUNv_R",
        "replyto": "bPiHuNUNv_R",
        "invitation": "ICLR.cc/2023/Conference/Paper804/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper analyzes EFG when a regularization term is inserted in the game setting and it comes up with the first last-iterate convergence results for CFR type algorithms, in contrast to existing results on the average iterate convergence. It provides new algorithms with considerably low complexity and proves rigorously their guarantees.",
            "strength_and_weaknesses": "S: The problem is well motivated, the paper is well written and the proofs seem ok from a first read.\n\nW: The impact of the paper as a 'Learning Representations' result needs to be more investigated. The impact on EFG seems to be there, but it is not sufficiently and explicitly linked to a Learning task. There are some experiments in the supplement, but they remain in the setting of a game, rather than Learning/Inference/Prediction. \nTherefore the results seemingly lie mainly in the area of Algorithmic GT, and the insertion of the regularizer is not shifting the study necessarily to Learning Theory (given the scope of ICLR).  \n\nW2: A 'discussion' section would be more illuminating at the end of the main paper. \nThe ending of the paper seems a little 'abrupt' in terms of summing up the achieved complexity.\n\nW3: In general, the goal of the main submission document is to discuss the theorems, like the authors are doing in page 7, and not emphasize the proofs if space is limited. For example Lemma 1 and Corollary 4.4. could be placed in the Supplement allowing for more discussion in its place. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Paper is clear, well written and mathematically founded.\n\nQuality: In terms of expressing the ideas in writing, the presentation seems as a sequence of theorems without a discussion at the end. This is mainly associated with the scope and goal of the paper, which here seems to be more about the limiting behavior of a game setting, and less about a converging Learning method. In general, there should be a connection/instantiation to a learning procedure.\n \nNovelty: From a first read, it does not overlap with previous papers.\n\nReproducibility: Seems ok, because the focus here is the modelling, not the experiment. The experiments are there to 'validate' the regularizer effect. Although there is not any code link provided, this is not an issue, because someone can use the pseudocode (ADAPTIVE WEIGHT-SHRINKING ALGORITHM) if necessary, for the reproduction of the last iterate performance. ",
            "summary_of_the_review": "Overall it seems a rigorous paper, but the focus is on Algorithmic Game Theory, without complete instantiation to Learning Theory (to my opinion) in order to make a clear impact there. A better fit might be SAGT or EC conference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper804/Reviewer_M1He"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper804/Reviewer_M1He"
        ]
    },
    {
        "id": "pJPqh5ciy9S",
        "original": null,
        "number": 4,
        "cdate": 1667490512668,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667490512668,
        "tmdate": 1668959879674,
        "tddate": null,
        "forum": "bPiHuNUNv_R",
        "replyto": "bPiHuNUNv_R",
        "invitation": "ICLR.cc/2023/Conference/Paper804/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors study regularized extensive-form games (EFGs). They show that various algorithms achieve last-iterate convergence guarantees on such games. Then, the authors use the fact that for small amounts of regularization the solution to the regularized problem is also an approximate solution to the original problem. ",
            "strength_and_weaknesses": "Strengths:\n\n* The authors provide a useful set of results around the performance of CFR/DOMWU/DOGDA on strongly-convex strongly-concave saddle EFGs. This is indeed useful work, and I think it deserves publishing.\n* The results on CFR seem particularly nice, and less similar to existing results by e.g. Cen et al.\n* The $1/t$ iterate convergence in Theorem 3.2 was surprising to me. It seems like a nice result. Unfortunately I was brought in as a last-minute reviewer and did not have time to verify this claim.\n\nWeaknesses:\n\n* The authors mischaracterize the existing literature and open problem on last-iterate convergence guarantees. It is not that surprising that one can get last-iterate guarantees without uniqueness when solving a regularized EFG. This is in some sense already known since the same thing occurs in NFGs, and strongly-convex strongly-concave saddle-point problems more generally. \n* The related work discussion is extremely limited in the body of the paper. Many ideas existed already, but those papers are either not cited or only discussed vaguely in the appendix. Nesterov smoothing and the excessive gap technique in particular stand out in this regard. Also, Cen et al. also showed quite similar results for NFGs, which somewhat makes the present paper overselling. I also feel strongly that there should be existing literature on last-iterate convergence in strongly-convex strongly-concave saddle-point problems more broadly, in the first-order methods literature.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is ok.",
            "summary_of_the_review": "Fundamentally, I think that this paper mistakes what the research community was trying to achieve by looking for last-iterate convergence guarantees. All the previous literature that the authors cite tries to show last-iterate convergence of OMWU, DOGD, DOMWU, etc, for various cases of bilinear saddle-point problems (matrix games, EFGs, etc). Indeed, there, it is an open problem how to deal with the issue of requiring uniqueness in order to get convergence when using entropy regularization. The authors do not provide a resolution to this problem though: the authors provide a *different* algorithm which yields last-iterate convergence by solving a regularized problem. This approach was already known to yield last-iterate guarantees (e.g. in Cen et al. 2021). OMWU and Reg-OMWU are not the same algorithm, yet the authors repeated conflate these algorithms in the abstract and introduction, thereby giving the misleading impression that they resolve an open problem.\n\nHere is an example of an incorrect claim:\n\n> we first show that dilated optimistic mirror\ndescent (DOMD), an efficient variant of OMD for solving EFGs, with adaptive\nregularization can achieve a fast Oe(1/T) last-iterate convergence in terms of duality gap without the uniqueness assumption of the Nash equilibrium (NE).\n\nThe authors show a quite different result: they show that last-iterate convergence occurs in a strong-convex-strongly-concave game, due to the fixed amount of regularization. It was known that such last-iterate guarantees are possible; the uniqueness assumption specifically is trying to deal with last-iterate convergence of the *standard* DOMD algorithm. We already knew from Nesterov smoothing (which should be cited! The authors cite the excessive gap technique paper, but not the original Nesterov smoothing paper) that you can add strong-convex regularization to a problem and get better guarantees. \n\nA minor note here, by the way, is that Wei et al already show last-iterate linear-rate convergence of OGDA in EFGs: their result is for any bilinear saddle-point problem with polyhedral decision sets. It's just that their result requires $\\ell_2$ as the regularizer, and thus does not allow dilated regularizers.\n\nGiven the present way that the paper is presented, I am leaning towards rejecting the paper, especially because the authors are already aware of these issues and chose not to address them in their revised paper (see ethics concern below). I will say that I actually think the results are pretty nice when viewed simply as results about a different algorithm, and they could arguably be accepted from that perspective. In that case, I would expect the discussion of the obtained results to be more in line with how Cen et al. present their results: they readily acknowledge that their results are on regularized problems, and thus are not directly comparable to the goal of showing last-iterate results for DOMWU/OMWU/etc.\n\nI do have a few questions for the authors:\n\n* Am I correct that in Theorem 3.2, the algorithm completely resets for every $\\epsilon$? In particular, if I wanted to get a particular level of precision, I would not actually run this restarting algorithm at all, but instead pick my desired $\\epsilon$ and simply start the algorithm at that $\\epsilon$?\n* For the second statement in Theorem 3.2, is this a property of regularizing with the entropy regularizer, or is it a specific property of Reg-DOMWU? In particular, suppose I solve the entropy-regularized problem with some other algorithm and obtain an iterate guarantee. Can your result be used to directly bound the iterate distance in the original problem? I don't really see why such a property should be specific to running DOMWU on the regularized problem.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper804/Reviewer_KrJP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper804/Reviewer_KrJP"
        ]
    }
]