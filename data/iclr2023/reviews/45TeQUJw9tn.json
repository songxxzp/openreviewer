[
    {
        "id": "oWKyFG0R8q2",
        "original": null,
        "number": 1,
        "cdate": 1666389852669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666389852669,
        "tmdate": 1666389852669,
        "tddate": null,
        "forum": "45TeQUJw9tn",
        "replyto": "45TeQUJw9tn",
        "invitation": "ICLR.cc/2023/Conference/Paper1314/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The goal of the paper is to extend molecule generation using diffusion models to include a way to control how \u201cout-of-distribution\u201d generated molecules should be. In order to do this, the authors propose adding a term to the reverse-diffusion score (using an SDE formulation) along with a manually tuned hyperparameter $\\lambda$ which controls \u201cOOD-ness\u201d. The authors demonstrate that their method is capable to generating molecules which are unique from the training set, yet still likely have realistic bioactive properties using some computed proxy metrics (i.e. docking score, drug likelihood score, and synthetic accessibility score).",
            "strength_and_weaknesses": "Overall, I like the idea of controlling OOD-ness seems somewhat novel, and I think the authors do a good job of demonstrating that by controlling $\\lambda$, the OOD-ness can be adjusted. Their analysis of OOD-ness using FCD and MMD (with associated dimensionality-reduction plots) is convincing to me. I also think the combination of OOD-ness with conditional optimization (i.e. the \u201cproperty gradient\u201d) is a natural next step which the authors address and show results for.\nBelow are some things I believe could be stronger:\n### Property prediction is limited to a single, manually crafted objective\nIn order to show property prediction, the analysis focuses entirely on a single objective, which combines DS, QED, and SA scores into a single scalar (using the product). It is not clear why this is the chosen objective. The degree of manual tuning in this objective might suggest a lack of robustness in the results. For example, why not use an objective which is the sum of these three scores (with adjustable weightings)?\nIt would be more convincing if we could see three more distinct property models, each one trained to predict DS, QED, or SA separately. Since the authors are using classifier guidance, the score model would not need to be retrained. Showing that MOOD can then optimize for OOD molecules which show better DS, QED, or SA scores separately would greatly increase the confidence that the method is working and is not limited to a specific, manually crafted objective.\n### OOD molecules may generate large numbers of \u201cgarbage\u201d molecules\nIn order to evaluate how well the model generates molecules which satisfy the property objective, the analysis seems to focus entirely on \u201cnovel hit ratio\u201d and  \u201cnovel top 5% docking score\u201d, which are defined to measure how well the \u201cmost OOD\u201d molecules satisfy the property objective. However, these metrics are limited to comparing only the generated molecules which already have a good property score (i.e. the analysis is limited already to molecules which pass certain thresholds). Thus, it is very possible (if not likely) that many molecules generated by this method (not just OOD molecules, but also when $\\lambda$ is set to 0) are \u201cgarbage\u201d molecules which do not actually correspond to any bioactivity.\nIt would be much more convincing that this method works, if the authors could compare these property values (i.e. DS, QED, and SA) of all generated molecules (not just those that pass DS/QED/SA thresholds) between their methods (MOOD/MOOD-ID/MOOD-w/o-P) and a couple of benchmarks.\n### Effect of $\\lambda$ on OOD-ness\nIt would be nice to also see how the magnitude of $\\lambda$ can affect OOD-ness; for example, if lambda is increased further, does it also increase OOD-ness?\n### Smaller details\n- How is the UMAP in Fig. 4 generated?\n- In Fig. 6, it is not clear what the relationship is between the ZINC250k molecules shown and the MOOD-generated molecules: are they the closest ZINC250k molecules based on MMD? Or something else?\n- It is not clear how QED and SA scores were computed\n- The property-prediction conditional score in Eq. 9 depends on $\\lambda$, but it seems (under the last section of Section 3) that the authors don\u2019t actually use lambda in the property-prediction score model. Although this makes some empirical sense, it conflicts with the derivations in Eq. 7 \u2013 9. It would be more clear to have Eq. 7 \u2013 9 assume conditional independence between OOD-ness and property labels to begin with.\n- In the sentence following Eq. 13, it should explain what SA is (in structural biology, SA could also refer to solvent accessibility)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and the analyses performed are supportive of good science. The main novelty is to add a term to conditional generation in reverse diffusion in order to increase the likelihood of generating OOD samples, and showing that it can be used in conjunction with property gradients to generate conditional samples that are somewhat OOD. The authors have included their codebase and it looks comprehensive.",
            "summary_of_the_review": "Overall, the paper demonstrates a novel method to control the OOD-ness of generating samples from a generative diffusion model. Their analyses demonstrate convincingly that their method can be used to generate samples that are different from the training set. They also show that within these OOD molecules, there can be found molecules which are expected to have certain bioactive properties.\n\nI find the main weaknesses to be that the paper only focuses on a single computational proxy for bioactivity (which arbitrarily combines a few metrics into a single optimization goal). Thus, the method may not be very robust: it might not work if we want to optimize for a different set of molecular properties. Additionally, there are no analyses that show how many of the generated molecules (OOD or otherwise) are reasonable. It is very possible that the vast majority of molecules generated are garbage.\n\nTogether, I think that the contribution of OOD control is interesting and this paper is a good proof of concept, despite its missing analyses that could really show its usefulness in more applications.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1314/Reviewer_xSWJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1314/Reviewer_xSWJ"
        ]
    },
    {
        "id": "j4o-54rAmY",
        "original": null,
        "number": 2,
        "cdate": 1666462194344,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666462194344,
        "tmdate": 1669993991474,
        "tddate": null,
        "forum": "45TeQUJw9tn",
        "replyto": "45TeQUJw9tn",
        "invitation": "ICLR.cc/2023/Conference/Paper1314/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes MOOD (Molecular out of distribution diffusion), a method using diffusion models to generate novel molecules which differ frrom training molecules and potentially have desirable properties. The method essentially modifies the reverse-time SDE equation to reduce the coefficient of the $\\nabla \\log{p}$ (score) term and add a term involving a property predictor. Experimentally they demonstrate that MOOD produces molecules which are:\n1. Different than the training set\n2. Possess desirable properties using an oracle based on molecular docking",
            "strength_and_weaknesses": "**Strengths**:\n\n- The core idea of using a diffusion process to generate novel molecules is interesting and could potentially be useful\n- The experimental results on 5 non-trivial oracles are interesting and encouraging. I appreciate that they didn't use logP/QED/other trivial oracles\n- Writing is very clear\n\n**Weaknesses**\n\n- _Problem is not well-motivated_: the authors state in many places that MOOD is designed to overcome the inability of existing methods to explore areas of chemical space away from the training data. While some methods do have this limitation, a huge number of methods do not, for example methods using genetic algorithms [1, 2] or graph enumeration [3]. Put simply, producing molecules which are \"different\" is **not** an open problem which would benefit from the development of new methods. My guess would be that the authors are not aware of a lot of earlier literature on this topic (most citations are ML papers from the past 5 years). I would like to see the authors correct this, and perhaps have a more nuanced discussion about the pros/cons of existing methods. Roughly, this would probably be that exploratory methods can propose new molecules but many of them are of low quality, while generative models can propose higher-quality molecules but they are generally more similar to the training data. MOOD could then be described in the context of this trade-off.\n- _Mathematical motivation for MOOD is not convincing_. My first concern is that the definition of $y_0$ used to arrive at equation 6 is unclear. The description somewhat implied that it was a binary random variable indicating whether or not a given graph $G$ is OOD, but this would imply a distribution on the set {0, 1} not $[0, 1)$. However, it is also described as a hyperparameter, which means it is _not_ a random variable and therefore writing $p(G|y_0=\\lambda)$ and $p(y_0=\\lambda|G)$ does not make sense. Equation 5 only adds to this confusion, since it is written as a distribution over $y_0$, but the \"derivation\" in appendix A.1 treats it like a distribution over $G$ (which I think is a mistake). It also seems to not be a valid probability distribution if $p(G) < p_*$, since equation 5 suggests $p(y_0)=0$ (i.e. it would not normalize to 1 and therefore is not a distribution). Overall this path to arrive at equation 6 makes very little sense to me. I think the authors would need to more clearly define the interpretation and role of $y_0$, and fix any errors in the reasoning that follow (for example if $y_0$ is a random variable then the proof in A.1 is not correct). My second concern is that construction of equation 9 is not really explained at all. My guess is that the authors intended to substitute their model for $p(G_t, y_0, y_p)$ into equation 2, but this is not correct, because it neglects the gradient $\\nabla_G\\log{Z(G)}$, i.e. the gradient of the normalizing constant in equation 8. The authors should clarify this. Finally, in equation 11, setting the coefficients $\\alpha$ to values which depend on the score seems like it would have implications for the correctness of the reverse-time SDE, but this is not discussed (I could be wrong about this though). **Overall**: the key equations of MOOD do not seem to have a principled justification.\n- _Missing important baselines_: the baseline methods used by the authors are all methods which tend not to produce OOD molecules. While the inclusion of these baselines is good, the authors should of course include baselines which do not suffer from these problems (see discussion above). The open-source methods from [4] could be a good starting point, for example Graph GA. These methods will almost certainly explore as well as MOOD or better, and could plausibly also produce higher-scoring molecules in section 4.2\n- _Issues with metrics in section 4.2_: the novel hit ratio and novel top 5% docking score are both normalized by the number of unique (hit) molecules, which can be problematic (assuming I have understood them correctly; if not then the authors should explain their metrics more clearly). For example, out of 3000 molecules, method A produces just 2 unique molecules (one novel hit and one poor molecule), it's novel hit ratio would be 100%, while method B producing 500 novel hits, 500 similar hits, and 2000 poor molecules would have a novel hit ratio of 50%. I think method B would be superior in this case, while the author's metrics would favour method A. Novel top 5% docking score suffers from similar problems. I think the authors should either report standard metrics, or continue to sample from all methods until a certain number of unique hit molecules are produced, which would fix the normalization problem.\n- _Lack of experiments on standard benchmarks_: while I support the authors creation of a new, reasonable molecular optimization benchmark, the problem with just evaluating MOOD on a novel task is that the general difficulty and idiosyncrocies of this task are not understood, so it is unclear how much of the method's success is \"general\" and how much is due to being particularly well-suited for these specific tasks. I think that evaluating MOOD on at least one standard benchmark would be useful. One option would be to use the benchmark from PMO [4], which includes a variety of semi-toy functions such as GuacaMol. These are not as realistic as docking, but have also been tried with many more baselines, which would help contextualize the performance of MOOD. A second option using docking would be dockstring [5] which contains several standardized docking benchmarks.\n\n[1] Nigam, AkshatKumar, et al. \"Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES.\" Chemical science 12.20 (2021): 7079-7090.\n\n[2] Jensen, Jan H. \"A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space.\" Chemical science 10.12 (2019): 3567-3572.\n\n[3] Ruddigkeit, Lars, et al. \"Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17.\" Journal of chemical information and modeling 52.11 (2012): 2864-2875.\n\n[4] Gao, Wenhao, et al. \"Sample efficiency matters: a benchmark for practical molecular optimization.\" arXiv preprint arXiv:2206.12411 (2022).\n\n[5] Garc\u00eda-Orteg\u00f3n, Miguel, et al. \"DOCKSTRING: easy molecular docking yields better benchmarks for ligand design.\" Journal of chemical information and modeling 62.15 (2022): 3486-3502.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: In general I think the clarity of the paper is good, aside from a few details (e.g. the defintion of $y_0$ in section 3). Some of these details are important however.\n\n**Novelty/Originality**: I don't think I know enough about diffusion models to comment on the novelty of this method compared to other diffusion-model methods. It appears that the core equations of MOOD are novel. From the viewpoint of drug-discovery, I don't think this work is particularly original: it is effectively using a generative model to produce molecules which are predicted to have high properties using a property-prediction model, which describes a huge amount of existing work. To me, it is unclear whether using a diffusion model instead of any other method of exploring chemical space (e.g. a genetic algorithm) addresses any shortcomings of existing methods. Therefore my impression is that the overall contribution to drug discovery is not super large here.\n\n**Quality**: to me, the quality of the motivation for the problem and the method were not very high, and the experiments could be improved by reporting different metrics, additional baseline methods, and trying at least one established benchmark. Therefore my overall assessment of the quality is medium-low.\n\n**Reproducibility**: I'm not an expert on diffusion models, but it seems like the key equations for the method are present. The authors also included code.",
            "summary_of_the_review": "This paper proposes a potentially interesting idea to a problem which I don't think is actually a problem; instead it is just a shortocming of several popular models in ML over the past 5 years. The derivation of the method itself is unclear, which makes the method seem not very principled (EDIT: further discussions with the authors make me think that several key equations are incorrect due to a mis-use of Bayes rule, so I lowered my score). I do not find the experimental results convincing, particularly because of the omission of a large class of important baseline methods. Together, these shortcomings imply that the paper should not be accepted.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1314/Reviewer_ZWEt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1314/Reviewer_ZWEt"
        ]
    },
    {
        "id": "3b75w3Zgcr",
        "original": null,
        "number": 3,
        "cdate": 1666626720008,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626720008,
        "tmdate": 1666626720008,
        "tddate": null,
        "forum": "45TeQUJw9tn",
        "replyto": "45TeQUJw9tn",
        "invitation": "ICLR.cc/2023/Conference/Paper1314/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Summary of the paper\nThe paper extends the score-based generative models by Song & Ermon (2019) to control the OOD level of generated molecules. To constrain the generation process such that it targets generating only OOD examples having desirable properties, they utilize the gradient of a property prediction network to guide the sampling process.\n\nThe authors demonstrated that the given approach outperformed the existing works on generating new drugs both in terms of novelty and having higher binding affinity confirmed by docking methods.\n\n",
            "strength_and_weaknesses": "Strengths\nThe paper was well-written, it addresses a practical issue of AI-based approaches. \n\nWeaknesses\n\nRegarding the proposal of a controlled parameter for OOD sampling from a score-based model,  it is worth comparing the given approach to the VAE, where OOD sampling in VAE can be controlled by the deviation of the sampled latent variable to the predicted Gaussian distribution given by the encoders. I see a comparison to LIMO (Eckmann et al., 2022) but in the experiments with LIMO for novel drug generation, did you sample the latent variable using a constant factor deviating from the predicted standard deviation to control the novelty? \n\n\nDrugs (Antimicrobial peptides) generation conditioned on a few desirable properties is not a new idea, please refer to Payes Das et al. (https://www.nature.com/articles/s41551-021-00689-x). The difference between the prior art is that it trains a conditional VAE to control the desirable property constraint while the (MOOD) framework is based on a score-based model and utilizes the gradient of a property prediction network to guide the sampling process to domains that are highly likely to satisfy the given constraints. I see a comparison to LIMO (Eckmann et al., 2022)  but the given method was not trained to generate molecules that have the constraint on desirable properties. This is not a fair comparison.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty is limited as it is a minor extension of the score-based generative model framework. Existing works for generating de novo drugs and satisfying desirable property constraints exist but rely on VAE rather than score-based models. Comparison with VAE-based approaches should be considered to support the reason why we need new solutions other than VAE-based solutions.\n",
            "summary_of_the_review": "This is a minor extension of existing score-based generative models. Comparison with VAE models by Payes Das et al. (https://www.nature.com/articles/s41551-021-00689-x) on conditional generation and adjust the VAE to control the novelty in the latent space could be a potential simple baseline to compare to.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1314/Reviewer_gMda"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1314/Reviewer_gMda"
        ]
    },
    {
        "id": "9o9JWfAIcg",
        "original": null,
        "number": 4,
        "cdate": 1666708106504,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666708106504,
        "tmdate": 1666708106504,
        "tddate": null,
        "forum": "45TeQUJw9tn",
        "replyto": "45TeQUJw9tn",
        "invitation": "ICLR.cc/2023/Conference/Paper1314/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The present paper is concerned about a generative model of molecules. The authors consider that the existing generative models are limited in that i) the generated molecules are similar to those in the training dataset, while what we want are very novel molecules, and ii) the target properties used in common benchmark tasks are not very helpful for drug discovery and existing methods are not likely to fully optimize more realistic multi-objective optimization tasks due to their limited exploration capabilities. Thus, the authors propose a molecular out-of-distribution diffusion (MOOD) framework, where the score-based model is guided by the property prediction network to generate out-of-distribution molecules with desirable properties. \n\nIn particular, the authors define the ODD-ness of a graph in Eq. (5) and use it to guide the generative process towards increased ODD-ness. The effectiveness of the ODD controller is showcased in Figure 2. In addition, the authors utilize property prediction networks to guide the generative process towards maximizing the properties. These two guidances are valanced by the magnitudes of the two related score functions.\n\nIn the experiments, the authors first confirm that their ODD controller can actually control the ODD-ness of the generated molecules. The authors then compare the proposed method with baseline methods in terms of novel hit ratio, which asks the method not only to optimize the objective function but also to be dissimilar to the molecules in the training dataset. The results show that the proposed method achieves the highest novel hit ratios. The authors also provide ablation studies.",
            "strength_and_weaknesses": "## Strengths\n- The proposed method is a solid improvement over existing score-based graph generative models, whose effectiveness has been shown empirically.\n- The evaluation metrics in the experiments are well designed to be aligned to the research objective, to find out-of-distribution molecules.\n- Reproducibility is maintained as the supplementary material contains the code.\n\n## Weaknesses\nI am still not sure whether the generated molecules have to be out-of-distribution. A molecule similar to those in the dataset will be acceptable if its properties are sufficiently optimized; in other words, if we have two molecules that have similar properties but one of them is close to the training dataset while the other is far from the training dataset, then is the out-of-distribution molecule preferred? If not, then the novelty is not an end but a mean, and it is not natural to incorporate the novelty into the evaluation metric.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nThis paper is clear enough and is easy to read.\n\n## Quality\nThe method proposed is well aligned to the research objective of this paper, and is of high quality.\n\n## Novelty\nAs far as I am aware of, the method disclosed in the paper is novel.\n\n## Reproducibility\nThe source code is included in the supplementary material and is reproducible.",
            "summary_of_the_review": "I consider this paper is a border-line paper. While the method proposed is well designed to achieve the research objective, I am not fully convinced of the need of OOD-ness. It could be a mean to further optimize the objective function, but I am still not sure whether the OOD-ness itself is to be pursued or not. In addition, the property prediction or simulation-based property calculation could be inaccurate for OOD molecules, which de-motivate me to pursue OOD-ness. Therefore, to better assess the usefulness of this paper, I would like to request the authors to clarify i) whether the OOD-ness itself is pursued or it is just a mean to optimize the score, and ii) risks of pursuing OOD molecules and ways to avoid them. I would also appreciate it if the authors could point out my misunderstandings if any.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1314/Reviewer_3YM5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1314/Reviewer_3YM5"
        ]
    }
]