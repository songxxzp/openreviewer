[
    {
        "id": "nrAndG3iRK",
        "original": null,
        "number": 1,
        "cdate": 1666520259781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666520259781,
        "tmdate": 1666520259781,
        "tddate": null,
        "forum": "9y0HFvaAYD6",
        "replyto": "9y0HFvaAYD6",
        "invitation": "ICLR.cc/2023/Conference/Paper1335/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new model called Hidden Markov Transformer (HMT) to tackle the problem of simultaneous machine translation (simt), which tries to translate source to target live as the source is being receive. The model has to decide when to read the source buffer and when to generate or write target token that achieves both low latency and high accuracy.\nThe method is based on hidden markov chain, which the moments of starting translating are treated as hidden events and it frames the translation results as the corresponding observed events. The model is trained by maximizing the marginal likelihood of the\nobserved target sequence over multiple possible moments of starting translating\nDuring training, for each target token, is model is trained to output a number of possible states and maximize over which state as the starting state for the current target token.\n\nThe results show that the method outperform previous baselines and achieve better latency and accuracy trade-off curve compared to the baselines.",
            "strength_and_weaknesses": "Strength:\n\n* the method is quite novel and experimental results seem strong\n\nWeakness:\n\n* The explanation and presentation of the methodology is complicated and can be reduced.\n* More experiments of other language pairs, like french, and low-resource languages (like FLoRes) are needed to confirm the method extendability and generalization.",
            "clarity,_quality,_novelty_and_reproducibility": "* the method is novel and seem to have good quality\n* However, as no codes are provided, it's hard to determine if it is reproducible",
            "summary_of_the_review": "Overall, the method is quite novel and produce good results, even though I suggests to add more language pairs to the experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1335/Reviewer_WoTh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1335/Reviewer_WoTh"
        ]
    },
    {
        "id": "7VRplrynPR1",
        "original": null,
        "number": 2,
        "cdate": 1666543385329,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666543385329,
        "tmdate": 1669526026438,
        "tddate": null,
        "forum": "9y0HFvaAYD6",
        "replyto": "9y0HFvaAYD6",
        "invitation": "ICLR.cc/2023/Conference/Paper1335/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a hidden Markov Transformer (HMT) for simultaneous machine translation (SiMT). The proposed HTM is able to simulate when to start translating and to generate the target token. The results of the proposed method are reported on two offline simultaneous translation datasets.\n\n\n",
            "strength_and_weaknesses": "Strength:\n\n1. This method explicitly models when to start translating and generating target words;\n2. Ablation experiments verified the key operations of the proposed HMT;\n3. This study reported that the proposed HMT improves the translation performance on two offline SiMT datasets.\n\nWeaknesses\uff1a\n1. The training/inference speeds are lower than those of the Wait-k method, which is a disadvantage for SiMT;\n2. The interaction and relationship between the proposed HMT and the read/write policy were not clearly introduced;\n3. Too many hyperparameters (i.e., L\u3001K\u3001Lamda1, and Lamda2) are not conducive to the optimization of this method on other language pairs;\n4. There lack of necessary experiment to verify what the proposed HMT indeed capture compared to the baseline wait-k.",
            "clarity,_quality,_novelty_and_reproducibility": "1. What about BLEU scores in Table 1?\n2. The attention on the states adopts multiple modes. This results in some redundant information. However, the author's experiment showed that the multiple modes are the best in terms of results in Table 6, which makes me very confused.\n3. Why does the parameter amount not increase?\n4. There did not provide source code in the submission.",
            "summary_of_the_review": "This is an extension of the existing wait-k and causes high latency.\n\n\nThe author's response addressed my concerns, and thank you very much. I have raised the score to 8",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1335/Reviewer_4GZM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1335/Reviewer_4GZM"
        ]
    },
    {
        "id": "FuSMhK4WaUe",
        "original": null,
        "number": 3,
        "cdate": 1666574331335,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574331335,
        "tmdate": 1669554713188,
        "tddate": null,
        "forum": "9y0HFvaAYD6",
        "replyto": "9y0HFvaAYD6",
        "invitation": "ICLR.cc/2023/Conference/Paper1335/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper used a Hidden Markov Transformer(HMT) to improve the READ/WRITE actions in simultaneous neural machine translation. Specifically,\u00a0this model treats the moments of starting translating as hidden events and the target sequence as the corresponding observed events. The HMT model judges whether to select each state from low to high latency. The authors hoped that\u00a0the HMT model effectively learns when to start translating under the supervision of the observed target sequence. The proposed model was evaluated on two language pairs and gained improvement over the baseline systems in terms of BLEU scores and latency.",
            "strength_and_weaknesses": "Strength:\n1. The proposed HMT approach gained improvement over the baseline systems in terms of BLEU scores and latency.\n2. Experiments have proved the effectiveness of the method, and there are very reasonable expanded analyses.\n\nWeaknesses:\nThe average pooling result on the hidden states of the received source is only a summary of the past state. And this model didn't use an explicit alignment method. Therefore, the proposed approach is more like a pruning\u00a0method to cut low probability branches of the READ/WRITE path.  This doesn't quite match what the paper claims that explicitly models multiple possible moments of starting translating in both training and inference.\n\nQuestions:\n1. How to evaluate the proposed method to make better read/write decisions than the other SNMT?\n2. If there is higher confidence in the later state, will the performance of the model be affected by generating too early?\u00a0\n3. Can you provide an evaluation of the quality, clarity, and originality of the work?",
            "clarity,_quality,_novelty_and_reproducibility": "This article did not release the code but used open-source deep-learning frameworks and public datasets. Therefore, I believe this work can be reproduced with some effort.\u00a0",
            "summary_of_the_review": "This paper provides a new perspective on simultaneous neural machine translation, and sufficient experiments prove the effectiveness of the method. Compared with several strong baseline systems, the proposed method achieves state-of-the-art performance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1335/Reviewer_NSCn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1335/Reviewer_NSCn"
        ]
    },
    {
        "id": "hJQZ9weK6Bd",
        "original": null,
        "number": 4,
        "cdate": 1666684346678,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684346678,
        "tmdate": 1666684346678,
        "tddate": null,
        "forum": "9y0HFvaAYD6",
        "replyto": "9y0HFvaAYD6",
        "invitation": "ICLR.cc/2023/Conference/Paper1335/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the core issue of simultaneous machine translation: time to start translation. The paper proposes HMT inspired by HMM, where the next token generation is regarded as the hidden variables, and target tokens as the observable variables. At each time step, K indicates available length of source for translation, and each state predicts the next token to be yield. During training, the marginal probability of generating the correct token is maximized. During inference, the first state with over 0.5 confidence is selected as the generated token.\n\nExperiments conducted on two SiMT benchmarks validate the effectiveness of HMT. Extensive experiments are also conducted to promote better understanding of how HMT works.",
            "strength_and_weaknesses": "Reasons to accept:\n- The methods produces latent inputs for the translation to boost latency, which is interesting and promising.\n- The conducted experiments are extensive, providing in-depth analysis on how the proposed method works.\u2028\n\n\nQuestions:\n1. Is the encoder in Full-Sentence MT bi-directional or uni-directional? If uni-directional, an alternative paper should be cited instead the original Transformer paper.\n2. Why HMT even outperforms Full-Sentence MT in De->En? Also in Section 5.3, I don't understand why Multiple is better than Max, where the latter always provides more information than the former when generating. Could authors please provide more explanations?\n3. What is the system overhead (e.g., memory) of HMT compared to baselines? Since HMT produces K states for each timestep, and self attentions are conducted on these multiple states, the memory requirements for SA would be K^2 compared to vanilla models. This could be a concern for edge devices.\u2028\nMissing reference:\nLiu D, Du M, Li X, et al. Cross attention augmented transducer networks for simultaneous translation[C]//Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 39-55.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is basically clear, except for several previously mentioned questions. The method is interesting and intuitive for the proposed problem. The experiments are extensive and solid.",
            "summary_of_the_review": "This paper addresses the core problem of simultaneous machine translation: when to start translate. The paper propose HMT as a solution. The experiments demonstrates the effectiveness of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1335/Reviewer_C1gX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1335/Reviewer_C1gX"
        ]
    }
]