[
    {
        "id": "nZ90qNWcYoA",
        "original": null,
        "number": 1,
        "cdate": 1666198950997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666198950997,
        "tmdate": 1666198950997,
        "tddate": null,
        "forum": "KyoVpYvWWnK",
        "replyto": "KyoVpYvWWnK",
        "invitation": "ICLR.cc/2023/Conference/Paper6148/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper deals with the verification of Neural Ordinary Differential Equations (NODEs), which some papers have claimed to have more inherent robustness than standard neural networks.\nEvaluating a NODE is based on solving the ODE defined by an initial condition (the input to the model) and a dynamics model, given by a trained neural network. Different types of ODE solvers might be used for the solution. This goal of this paper is to be able to obtain bounds on the outputs of the ODE, such that their robustness can be evaluated rigorously.\n\nThis work propose: \n- to restrict the possible values that the step size of the solver can take. This allows to get some of the benefits of adaptive solvers, while maintaining the property that the solver can only go through a finite number of states (a state being determined by a time $t$ and a step size $h$). The fact that only a finite number of states can be reached makes it feasible to evaluate bounds on the network output (it just becomes dependent on computing bounds for each possible state)\n- an algorithm to compute bounds on such a computational graph. Usually, bound computation is done on fixed computational graphs, where you can define exactly the function of each node (broadly, a node in the computational graph to bound is an activation of a network and can only have been produced in a given way). With the proposed formalism, each node (a given state) may have been reached by doing a variable number of steps of the solver, so to obtain a bound, you need to pick the worst case bound across all the possible paths that may have been taken. The author explain how to do this both for simple IBP bounds, as well as for the more complex linear bounds.\n\nEvaluation is performed on image classification, using MNIST and FashionMNIST, as well as on time series regression. ",
            "strength_and_weaknesses": "# Strengths\nThe paper proposes a clearly novel and interesting solution to be able to perform formal verification and bound propagation on a model for which existing methods would not work. The proposed solution is interesting, builds on existing verification research (DeepPoly) while clearly delineating their own contribution, and makes intuitive sense. \nRelevant context (explanation about adversarial robustness and neural ODE) is given to make the paper accessible.\n\nThe author have provided their codebase as supplementary material, and have thoroughly detalied their experiments in the appendix, which makes me think that the work should be quite reproducible.\n\nThe result seems to be what you would expect with more traditional models: models trained without consideration for later verification are hard (or here apparently impossible) to verify, but incorporating the bounding procedure during the training makes it much easier to perform verification, at the cost of some nominal accuracy.\n\n\n# Weakness\nThe part where I am actually a bit hesitant is on the applicability of this research. I'm mostly familiar with research on neural network verification, and less so on Neural ODE. Is there area where they particularly shine? Observing the experiments, what I see is that if I compare the adversarial and certified accuracy obtained on MNIST (83.9% for eps=0.1), they are significantly worse than simple convnets trained with IBP (99.77% for the same eps), in addition to being much more complex.\nAs a consequence, even if the results are technically interesting, I wonder if they are actually important. \n\nI also note that the author make a claim that restricting step sizes to an exponential grid has \"minimal impact on solver efficiency\". I think that this should be evaluated beyond the results of Figure 4., which seems to be only for a single ODE. Would it be possible to perform this evaluation on more complex ODE, such as the one that might be defined by a Neural ODE? \n\n## Question about training.\nIn Section 5., it is described that IBP bounds are used for training, and that trajectory needs to be sampled. Given the description of section 5., it seemed to me that it would be possible to actually compute a true upper bound. Is the decision to sample trajectories based on making the training iterations faster / use less memory, or is there another reason why it's not possible to use the true upper bound?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above.\n\nSome minor notes / typos:\n- The plots of Step size vs. Time (in Figure 1. or Figure 3.) allow the step size to go negative, is that possible / normal?\n- The caption of Figure 7 is kind of overlapping with the legend of the figures.\n- At the end of section 5, the authors discuss the fact that \"eps-annealing alone is insufficient to stabilize the training\". I did not see any discussion of annealing prior to that in the paper, but it is discussed in the appendix. Either some more of the discussion on training should be moved to the main paper, or this bit should be added to the appendix, because as it is, the context for it is not present.\n- Second line of section 6.2 -> \"We rescaling most features\"\n- Caption of Figure 9 -> \"Comparison of th \"\n\n",
            "summary_of_the_review": "The paper is rigorous and provide a good solution to the question \"how can you propagate bounds through the solving of a Neural ODE\". The method discussed is presented well. The main doubt is whether that question is worth solving in the first place.\n\n\n[Note]\nI trust myself to review the elements of the paper that relate to verification but don't have significant expertise in Neural ODE.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6148/Reviewer_CQh5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6148/Reviewer_CQh5"
        ]
    },
    {
        "id": "7B9H2cqRNi",
        "original": null,
        "number": 2,
        "cdate": 1666645049025,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645049025,
        "tmdate": 1666645247583,
        "tddate": null,
        "forum": "KyoVpYvWWnK",
        "replyto": "KyoVpYvWWnK",
        "invitation": "ICLR.cc/2023/Conference/Paper6148/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a framework based on linear bound propagation that takes advantage of a new class of ODE solvers to enable training and verification of Neural Ordinary Differential Equations. Specifically, the proposed new class of adaptive ODE solvers, CAS, is based on variable but discrete time steps such that the solver trajectories can be captured in a graph representation. This reduces the runtime of NODES from intractable exponential computation time to polynomial time. ",
            "strength_and_weaknesses": "One of the main contributions of this paper is the reduction of the computational complexity of verification of NODES by GAINS enabled by controlled adaptive ODE solvers. The theoretical impact of the use of a CAS over an AS solver has been briefly and clearly described in 'Comparison to Adaptive Solvers' on page 5. However, the included empirical comparison is difficult to grasp. Moreover, from Figure 4, it appears that for a small error threshold, the difference in performance between CAS and AS is relatively large. It would be good if the authors could include for some of the experiments on the (F)MNIST and PHYSIO-Net datasets the difference in performance and (standard) adversarial robustness of the AS and CAS solvers. Furthermore, the authors should clarify the impact of the update factor on the computational complexity. \n\nThe authors perform an extensive empirical evaluation of the performance of the framework. These experiments clearly show the strength of the proposed framework to assess and improve adversarial robustness. It would be interesting to compare the performance of the networks trained by GAINS and TisODE or other approaches as cited in the related work that improve empirical robustness. \n\nFurthermore, it should be emphasised in the related works that for an ODE solver with fixed time step, the problem considered in this work is similar to that of verifying neural networks dynamic models, which are  discrete time dynamical models driven by a neural network for which formal robustness verification and control against have been recently studied [1,2]. \n\n[1] : Adams, Steven, et al \"Formal control synthesis for stochastic neural network dynamic models.\" IEEE Control Systems Letters (2022). \n\n[2] :  Wei, Tianhao, and Changliu Liu. \"Safe Control with Neural Network Dynamic Models.\" Learning for Dynamics and Control Conference. PMLR, 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors are the first to propose an analysis framework for verification of high dimensional NODEs. The paper is well written, the context and problem formulation are introduced clearly and the most important aspect of contribution are explained well. The theoretical results given seem to be theoretically sound. The theoretical and experimental details are clearly documented, and the code for implementation of the work is shared.",
            "summary_of_the_review": "To the best of the my knowledge, this work is the first to present a scalable verification framework for NODEs. The paper is well written, and the theoretical contribution seems sound. The authors included an extensive empirical evaluation of the performance of the framework. The authors should explain in more detail the restrictiveness of the proposed controlled adaptive ODE solver. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6148/Reviewer_LBBD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6148/Reviewer_LBBD"
        ]
    },
    {
        "id": "LbH--HkyXKk",
        "original": null,
        "number": 3,
        "cdate": 1666970468148,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666970468148,
        "tmdate": 1668700960529,
        "tddate": null,
        "forum": "KyoVpYvWWnK",
        "replyto": "KyoVpYvWWnK",
        "invitation": "ICLR.cc/2023/Conference/Paper6148/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "### Problem\nThe paper tackled robustness certification and certified training of Neural ODEs.\n\n### Proposed Method\nAuthors claim that directly verifying adaptive solvers is difficult. So authors propose a modified solver called controlled adaptive ODE solver. This essentially restricts the step-sizes to a discrete set, which in turn restricts states that the solver can reach compared to the usual adaptive solvers.\n\nThe authors then propose a novel verification method for this solver, which is built on top of propagation-based verification methods like DeepPoly or CROWN.\n\nAuthors also show how this method can be used for certified training.\n\n### Experiments\nAuthors conduct two sets of experiments, on classification on MNIST and FMNIST, and time-series on Physio-net. ",
            "strength_and_weaknesses": "### STRENGTHS\n1. Novel Problem:\nThe verification of neural ODEs is an under-explored area.\n2. Novel solution:\nI like that the solution is built on top of a propagation based method, which makes it versatile and easier to develop in the future. Furthermore, it is clear that the authors had to do sufficient work to first make CAS solvers, then \n3. Good writing:\nThe paper is very well written. The figures are also very helpful for understanding the method.\n\n### WEAKNESSES\n1. Validate CAS vs AS on more problems\n- Could you please add experiments on more common problems to compare the two solvers. It is important to make sure that CAS is not much inferior to usual AS solvers.\n2. Certified accuracy of even adversarial trained methods is 0 everywhere\n- This is quite surprising. It would be important to understand why this is happening.  Does this mean that the bounds produced by the method are very weak? I would recommend the following experiments\n- Could you plot a figure similar to Fig.2 from Beta-CROWN 9 (https://arxiv.org/pdf/2103.06624.pdf) as this will help us understand the tightness of the bounds.\n- Can you reduce the epsilon further to see if we can verify adv trained networks at all? \n3. Medium scale experiments missing\n- It would be good to have experiments on CIFAR-10. It is common to use even Imagenet-32 in Neural ODE papers, so running on CIFAR-10 should not be a trouble. Point 2 above scares me that your method might not scale well scale well.\n- Baseline: Would it be possible to add a baseline into the tables? Maybe an inferior version of your own method by removing some component? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation and clarity are very good.\n\nThe novelty and quality of technical contribution is also good.\n\nThe code is provided for reproducibility.",
            "summary_of_the_review": "The problem and proposed method are novel. But I am concerned about the scalability of the technique. The bounds don't seem to be very tight on MNIST. Experiments missing on a medium scale dataset like CIFAR-10 make it difficult to validate this. I am on the borderline (5/6) for this reason at the moment and willing to change my rating depending on responses to my concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6148/Reviewer_yJLc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6148/Reviewer_yJLc"
        ]
    },
    {
        "id": "oEnl3-XVzn",
        "original": null,
        "number": 4,
        "cdate": 1667363643389,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667363643389,
        "tmdate": 1667363643389,
        "tddate": null,
        "forum": "KyoVpYvWWnK",
        "replyto": "KyoVpYvWWnK",
        "invitation": "ICLR.cc/2023/Conference/Paper6148/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method called GAINS to address the robustness certification of neural ODEs via combining ODE solvers with variable steps and an efficient graph representation of solver trajectories. The authors provide some arguments showing the proposed approach significantly reduce the run time.  Numerical study is also performed on MNIST, FMNIST, and Physio-Net to demonstrate the certified robustness of the proposed method. ",
            "strength_and_weaknesses": "Strength:\n1. The combination of ODE solvers with variable steps and the graph representation of solver trajectories seem to be interesting. \n\n2. The improvement of running time is significant.\n\nWeaknesses:\n\n1. MNIST is a very simple task. Is it possible to run the proposed method for CIFAR10?\n\n2. The theoretical novelty of this paper is unclear. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and includes some solid results on certified robustness of the neural ODEs. Codes have been provided to reproduce the results. I think the idea of combining ODE solvers with variable steps and the graph representation of solver trajectories is interesting. I am a little bit concerned with the scalability of the proposed approach, since MNIST is really considered as a very simple task. ",
            "summary_of_the_review": "For now I give a \"6.\" Depending on how the authors address my concerns (on scalability and theoretical novelty), I may either increase or decrease my score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6148/Reviewer_bH4g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6148/Reviewer_bH4g"
        ]
    }
]