[
    {
        "id": "bisUCH78VnA",
        "original": null,
        "number": 1,
        "cdate": 1666583947314,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583947314,
        "tmdate": 1666666872690,
        "tddate": null,
        "forum": "f9eHl5mKx5i",
        "replyto": "f9eHl5mKx5i",
        "invitation": "ICLR.cc/2023/Conference/Paper5190/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a simple scheme for machine unlearning: learning to minimizing DL divergence with original model on the retain set (can also minimize test error), while maximizing DL divergence with original model on the forget set. It is shown to be effective across many task setups versus many baselines.",
            "strength_and_weaknesses": "The method seems simple yet consistently effective, and there are many analyses and insights discussed in the experiment parts. It also points out problems of previous theoretically-inspired methods.\n\nI'm not familiar with this domain, but seems the scales of experiments are modest, and even \"large scale\" setup is more about portion of forgetting instead of scale of model/data (as mentioned in intro)? The title, abstract, and intro might need to better characterize what is \"scalable\" in the method. ",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: the paper is mostly clear. Figure 1 and 2 are a bit hard to read, as colors overlap. \n- Novelty: the method seems intuitive, but I cannot judge the novelty as I'm not familiar with related work.\n- Reproducibility: I see no code, but the appendix about implementation details seems comprehensive.",
            "summary_of_the_review": "The method is simple and effective, but authors might want to better quantify the \"scalable\" part. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5190/Reviewer_eMqy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5190/Reviewer_eMqy"
        ]
    },
    {
        "id": "eql0YEV4h_8",
        "original": null,
        "number": 2,
        "cdate": 1666598035551,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598035551,
        "tmdate": 1666638029543,
        "tddate": null,
        "forum": "f9eHl5mKx5i",
        "replyto": "f9eHl5mKx5i",
        "invitation": "ICLR.cc/2023/Conference/Paper5190/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the problem of machine unlearning. The paper proposes a method that trains a student model starting from the original model, and updates the student model based on the loss so that the student's loss on the unlearned samples is maximized while the loss on the retained samples is minimized. Different from previous machine unlearning work, the paper focuses on the efficiency and performance of deep neural networks, and shows improved results in terms of forgetting errors, retaining accuracy, and test accuracy.",
            "strength_and_weaknesses": "Strengths:\n1. The proposed method is relatively efficient and can apply to a larger set of unlearning samples compared to previous methods.\n\n2. The proposed method is simple and easy to implement.\n\n3. The experimental result shows that the proposed method has better consistency in giving the best performance among various tasks in terms of forgetting errors, retaining accuracy, and test accuracy.\n\n\nWeaknesses:\n1. The proposed objective is a minimax objective, and training for such an objective could be potentially very tricky and it may require a lot of tuning. The paper mentions that it follows certain techniques from training GAN models, but it still seems careful tuning is required to optimize the objective effectively. Also, it is hard to know whether the optimization result is good or bad without any theoretical understanding of the proposed objective.\n\n2. The novelty of the proposed method is also limited. The proposed objective is similar to the consistency and contrastive loss used in self-supervised learning and the optimization procedure follows techniques of training GAN models.\n\n3. The proposed method has better consistency over a variety of tasks. However, in some tasks, the proposed methods do not give the best performance and more investigation seems required to understand what kind of tasks the proposed method does better and why.\n\n4. The proposed method does not have theoretical guarantees of any form. I.e., it is hard to know whether the unlearning set of samples has been truly forgotten. Showing theoretical results on deep models is very hard, and showing theoretical properties for simpler models could still be insightful.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe writing and the presentation of the paper are very clear.\n\nQuality:\nThe proposed method is sound and the given experiment results are solid. However, I have a few comments/questions:\n1. As stated in the paper, the reported results are points on the Pareto front. I wonder how such a point is selected. Is it possible to compare various methods across the entire Pareto front to make the results more convincing? I.e., can we tune the parameters of the proposed methods as well as some baselines to get different trade-offs among the three evaluation metrics?\n2. There are multiple ways to combine the three terms in the objective. E.g., we can use the ratio between the retaining loss and the forgetting loss, similar to a contrastive loss. I wonder if the authors have tried different forms of combinations and why the current linear form is chosen.\n3. Sensitivity analysis over the hyperparameters and ablation studies could make the experimental results more convincing.\n\nNovelty:\nAs discussed in the strengths v.s. weaknesses part, the novelty of the method is limited.\n\nReproducibility:\nThe method is simple to implement and many details of the experiments are given. I believe the reproducibility is decent.",
            "summary_of_the_review": "Given the limited novelty and some questions about the current method and experiments, I think modifications and clarification need to be made to accept the current paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5190/Reviewer_8kmh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5190/Reviewer_8kmh"
        ]
    },
    {
        "id": "9VxObSf3vZO",
        "original": null,
        "number": 3,
        "cdate": 1666803775351,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666803775351,
        "tmdate": 1666803800116,
        "tddate": null,
        "forum": "f9eHl5mKx5i",
        "replyto": "f9eHl5mKx5i",
        "invitation": "ICLR.cc/2023/Conference/Paper5190/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a machine unlearning approach, Brainy student, to forget a subset of training data even at larger scales. The solution is inspired by a recent popular student-teacher architecture where a student network disobeys the teacher network to avoid inheriting information about the forget set while obeying the teacher network for the retain set. The experimental investigation reveals that the proposed method significantly improves upon previous methods in achieving unlearning in a wide range of scenarios.",
            "strength_and_weaknesses": "Strengths:\n* The idea of machine unlearning for larger scales, by simultaneously encouraging the student to \u2018stay close to the teacher on retaining examples, while encouraging it to \u2018move away from the teacher on forgetting examples, is an interesting contribution to the different domains of AI. \n* The problem formulation and its components are well explained with mathematical equations.\n\nWeaknesses:\n* This proposed approach of using student-teacher architecture for machine unlearning is not entirely new. There is a plethora of papers in machine unlearning that uses a student-teacher approach to train the models. \n* The proposed framework is similar to the recent work \u201cCan Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher\u201c, Chundawat et al. 2022. However, the authors have not cited this work, and experimental analysis is limited to previous works.\n* Figure 3 requires standard error bars for better comparison across all the methods. Also, the authors could perform a statistical significance test to compare the results. \n",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is not well written\n* The paper is rather hard to read (it contains a lot of grammatical errors) and difficult to connect sentences. \n* This proposed approach of using student-teacher architecture for machine unlearning is not entirely new.\n* Typos:\nAbstract: motivated to (motivated by)\nPage 8: state-the-art (state-of-the-art)\n\n",
            "summary_of_the_review": "Overall, the authors propose a new framework, brainy student, a student-teacher network-based model for the purpose of machine unlearning at larger scales. However, the paper lacks model training details, lacks a discussion on why the proposed model is superior to the previously existing methods, and the robustness of the model is still to be validated.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5190/Reviewer_GpWt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5190/Reviewer_GpWt"
        ]
    },
    {
        "id": "JEubHD580wr",
        "original": null,
        "number": 4,
        "cdate": 1666872874982,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666872874982,
        "tmdate": 1666872874982,
        "tddate": null,
        "forum": "f9eHl5mKx5i",
        "replyto": "f9eHl5mKx5i",
        "invitation": "ICLR.cc/2023/Conference/Paper5190/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The focus of this paper is deep machine unlearning. This problem refers to removing the influence of a subset of data from the weights of a trained deep model. For instance, an application of this kind of method is to remove user data to guarantee their \"right to be forgotten.\" Other applications include removing noisy labels, outliers, harmful biases, and out-of-date instances. Given the widespread use of DNNs, this is a significant problem.\n\nSince previous approaches to deep machine unlearning make unrealistic assumptions related to DNNs, such as assuming the convexity of loss functions, there is a need for scalable solutions. The student-teacher methodology inspires the proposed solution. Here, the student ignores to-forget instances and considers all others.\n\nA hypothesis of this paper is that by leveraging the student-teacher-inspired methodology are able to scale and simultaneously provide minimal performance loss. The authors designed a new method based on that methodology and conducted empirical experiments to validate their hypothesis. All research questions are clearly stated.\n\nSpecific comments:\n\n- I have difficulty understanding why, in practice, Eq. 4 does not work. I suggest the authors include some intuition on that.\n- In Sec. 3, Parag. 5, \"performing this maximization step\" ->  \"performing this minimization step\" (according to Eq. 4).",
            "strength_and_weaknesses": "Strengths\n\n- This paper is easy to follow and well-motivated.\n- In the context of new legislation and other relevant use cases, this paper has a clear motivation.\n- The proposed method seems to be practical.\n\nWeaknesses\n\n- It is a relevant point to include the source code and enable others to reproduce the experiments.\n- The current method is valid to other architectures, such as Vision Transformers? Or a new deep unlearning method needs to be envisioned? This paper deserves this discussion.\n- Given the current metrics used, I did not have a clear understanding of issues related to, for instance, class imbalance.\n- The evaluation metrics should be clearly presented, such as using equations.\n- Besides the points highlighted as future work, the authors should discuss the limitations of their method. Given the space restriction, some parts of related work could go to the Appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is very clear and properly discusses previous literature. The authors correctly model the problem of interest and present a new method for it.\n\nUnfortunately, the authors do not include material to reproduce the results.\n",
            "summary_of_the_review": "This paper presents a scalable solution to the deep unlearning method by exploiting the student-teacher methodology. The results validate their hypothesis. Additionally, there are missing important discussions and missing original code implementation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5190/Reviewer_VJBS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5190/Reviewer_VJBS"
        ]
    }
]