[
    {
        "id": "F3xuWMPVLc",
        "original": null,
        "number": 1,
        "cdate": 1666509914852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666509914852,
        "tmdate": 1666509914852,
        "tddate": null,
        "forum": "u-RuvyDYqCM",
        "replyto": "u-RuvyDYqCM",
        "invitation": "ICLR.cc/2023/Conference/Paper5763/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an Actor-Critic RL algorithm able to learn from an offline dataset, while addressing the problem of out-of-distribution actions. The core contribution of the paper is the identification of how nicely soft policies and Q-updates (based on the softmax over actions) are amenable to updates robust to out-of-distribution actions. The resulting algorithm looks like the Soft Actor-Critic, but with the actor and critic losses slightly modified to introduce a term that depends on the estimated behavior policy used to collect samples in the dataset. Experimental results show that the resulting algorithm outperforms several baselines on standard benchmarks, and a theoretical analysis explains why the algorithm makes sense.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is very interesting to read and the idea is exciting! Some parts of the paper, that may appear simple (such as Equation 5, the trick to reduce the important of $\\pi_D$), are eye-opening.\n- The proposed algorithm is a relatively simple modification of the standard soft actor-critic losses. Nothing extra to compute, no resampling of the dataset to do. The algorithm seems easy to implement, and at almost no computational cost compared to vanilla SAC.\n- The paper is very clear and well-presented. Just the right amount of maths is given to understand how the new losses are designed, why they make sense, and how they can be implemented. Equations 14 to 16 are nicely grouped together to give a direct overview of the full (modified) SAC algorithm.\n- The sanity check is very nice to have, more papers should have a small and simple sanity check like this.\n\nWeaknesses:\n\n- The empirical evaluation is not as precise and complete as in other papers. Even in the appendix, I did not find absolute scores obtained by the agents in the environments (to be easy able to verify that the baseline scores match what other papers report, and to easily compare the scores obtained by IAC with baselines not present in the paper). Normalized scores are not that useful and usually hide more information than they reveal. Evaluation on AntMaze could also have been interesting: some people motivate that AntMaze is more difficult than the Mujoco tasks due to its sparse reward function, that better stresses the ability of an RL agent to bootstrap (instead of just following $r_t$).",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: high. The paper is easy to read and to follow, takes time to explain everything needed to understand the algorithm, and does not have any distracting part.\n- Quality: high. The proposed method is sound, easy to implement, cheap to compute, and compatible with state-of-the-art RL algorithms of the family of Soft Actor-Critic.\n- Originality: high. The proposed algorithm is, to my knowledge, novel. Its derivation is interesting and novel too.",
            "summary_of_the_review": "Very well-written paper with sufficient (but can be improved) empirical results. I would strongly advocate for the authors to include raw (un-normalized) scores in the final version of the paper, for ease of comparison against algorithms not present in this particular paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5763/Reviewer_xpgy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5763/Reviewer_xpgy"
        ]
    },
    {
        "id": "emtE4zrvQ7",
        "original": null,
        "number": 2,
        "cdate": 1666673881455,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673881455,
        "tmdate": 1666673881455,
        "tddate": null,
        "forum": "u-RuvyDYqCM",
        "replyto": "u-RuvyDYqCM",
        "invitation": "ICLR.cc/2023/Conference/Paper5763/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In maximum entropy RL, we use a soft Bellman optimality equations where the standard max operation is replaced by the log-sum-exp operation. Just as researchers have proposed to use in-sample max instead of max over the entire action space, this paper propose to use in-sample softmax to improve the softmax operation, such that we only uses actions well-covered by the dataset. Some theoretical study are provided to show the convergence property similar to softmax. Experimental results demonstrate the effectiveness of the proposed in-sample softmax operation for a SAC-type algorithm on various offline RL benchmarks.",
            "strength_and_weaknesses": "Strength:\n- The paper is well-written and easy to follow.\n- Nice theoretical characterization.\n- Experiments results seem promising.\n\nWeaknesses/Questions:\n- The idea is a bit straightforward (though not necessarily a weakness), as we have in-sample max to improve max, it seems not surprising to use in-sample softmax to replace softmax.\n- The explanation/derivation in page 4 before section 4 may not be necessary as the math seems trivial. Also some argument could be more accurate (e.g. when $\\pi_D(a|s) = 0$, equation (7) is undefined since $\\log(\\pi_D(a|s) )$ does not exist).\n- The in-sample softmax optimal policy approaches the in-sample optimal policy as the temperature approaches zero. In practice, how do you choose the temperature. Have you considered annealing the temperature for better performance?",
            "clarity,_quality,_novelty_and_reproducibility": "See above for more detailed comments.",
            "summary_of_the_review": "The paper propose to use in-sample softmax instead of softmax for maximum entropy offline RL, which leads to in-sample SAC that shows promising results on some offline RL benchmarks. The idea is interesting, though being a straightforward extension of existing ideas.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5763/Reviewer_Dopc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5763/Reviewer_Dopc"
        ]
    },
    {
        "id": "EMeC-PPCMGS",
        "original": null,
        "number": 3,
        "cdate": 1667455745953,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667455745953,
        "tmdate": 1667455745953,
        "tddate": null,
        "forum": "u-RuvyDYqCM",
        "replyto": "u-RuvyDYqCM",
        "invitation": "ICLR.cc/2023/Conference/Paper5763/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an offline RL algorithm based on the principles of in-sample maximization (when fitting the Q function) and maximum-entropy RL. It introduces the in-sample softmax, which is a straightforward modification of the usual softmax used in MaxEnt RL to only include actions in the support of the behavior policy, similarly to Batch-Constrained Q-Learning (BCQ). The convergence of in-sample soft policy iteration to an optimal policy (for an appropriate definition of optimal) is proved.\n\nThe main novelty is in showing that this in-sample softmax can be written as an expectation over samples from the dataset, reducing dependence on the behavior policy. Using this identity, the authors develop an algorithm similar to Soft Actor-Critic (SAC), which they dub In-sample Actor-Critic (InAC). Experiments demonstrate that InAC is effective in avoiding out-of-distribution (OOD) actions and attains high performance in both purely offline and offline-to-online settings.",
            "strength_and_weaknesses": "Strengths:\n* The paper presents a clean motivating theory that is not too far removed from the practical implementation.\n* Unlike some other offline RL algorithms, InAC does not enforce constraints between the learned policy and the estimated behavior policy. This is good because imperfections in the estimated behavior policy can bias learning.\n* Experiments show that the algorithm can effectively avoid OOD actions.\n* The performance of the algorithm is similar to or better than existing algorithms, both for offline and offline-to-online learning.\n\nWeaknesses:\n* Judging by Figure 3, it seems that some of the algorithms have not converged within the 0.8M iterations. Obviously it is good that InAC learns faster, but it\u2019s possible that the other algorithms could reduce the gap if given more iterations. Since training time is typically not the constraining factor in offline RL, I think all algorithms should be run to convergence.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear and well-written.\n\nThe in-sample softmax, while a combination of existing ideas, is novel to my knowledge. More importantly, the authors show identity (5), which is the core algorithmic innovation and addresses a key issue in offline RL.\n\nThe authors include detailed descriptions of the experimental setup and hyperparameters for reproducibility purposes.\n\nA small note: in equation (47), I think the final equality is the wrong way. (The logic would be correct and match the statement of the lemma if the sign were flipped.)",
            "summary_of_the_review": "I think the paper presents a useful idea and algorithm with compelling results, and therefore should be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5763/Reviewer_LBrh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5763/Reviewer_LBrh"
        ]
    }
]