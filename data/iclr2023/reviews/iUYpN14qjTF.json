[
    {
        "id": "r1w3BbcTor",
        "original": null,
        "number": 1,
        "cdate": 1666418846743,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666418846743,
        "tmdate": 1666418846743,
        "tddate": null,
        "forum": "iUYpN14qjTF",
        "replyto": "iUYpN14qjTF",
        "invitation": "ICLR.cc/2023/Conference/Paper4057/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proves under some particular situation, e.g. the input vector is the concatenation of a 1-sparse feature vector and a noise vector, under certain assumptions, the two layer CNN (whose second layer is assumed to be fixed as all 1\u2019s), converges to \u201cbad\u201d local optima with high probability when trained using ADAM. However under the same setting, when trained using gradient descent, with high probability the final convergence is a relatively \u201cgood\u201d local optimum. The authors also show for the convex settings such as linear models + logistic loss, both ADAM and gradient descent converge to \u201cgood\u201d local optima with high probability.  \n",
            "strength_and_weaknesses": "Strength: It looks like the first work to show the finite sample generalization guarantee on the local optima of a CNN trained using ADAM as compared to GD. I guess one reason we haven\u2019t seen such results is that in general it is not easy to give rigorous finite sample guarantees on ADAM\u2019s generalization errors for the non-convex neural networks. In that sense, this paper has its unique value. \n\nWeakness: Even though I understand without the assumptions it is hard to provide rigorous proof, I would still say the assumptions and simplifications made in the paper are too much. Those include\n1) two layer CNN but the second layer is fixed to all 1s\n2) the simplifications on the input vectors seem weird. 1-sparse looks over simplified to me, and in general the noise comes together with the input signal instead of coming from a totally different dimension.  \n3) the width of the network is assumed to be polylogarithmic in the training sample size. Even though over-parameterization is used a lot, usually people do not assume the model\u2019s width grows with the sample size in applications\n4) The work only gives results on full batch ADAM and full batch GD. However in real applications full batch optimization is rarely used. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the paper is pretty clean to me. The overall quality of the draft is great.\n",
            "summary_of_the_review": "I have mixed feelings about this paper. On one hand it is a solid move towards understanding the generalization capability of ADAM and GD. On the other hand, the assumptions and simplifications made in the paper have made me less excited. The models and data discussed in the draft are too far away from the real models and data used in applications, so that I am hesitating to use the insights from the paper in any real applications.  \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4057/Reviewer_52yW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4057/Reviewer_52yW"
        ]
    },
    {
        "id": "CEz0fIURB-",
        "original": null,
        "number": 2,
        "cdate": 1666613783115,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613783115,
        "tmdate": 1669777003339,
        "tddate": null,
        "forum": "iUYpN14qjTF",
        "replyto": "iUYpN14qjTF",
        "invitation": "ICLR.cc/2023/Conference/Paper4057/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work mainly focuses on the generalization comparison between Adam and GD. Specifically,   for a well-designed two-layer network,  Adam often converges to a bad minimum while GD can converge to a good solution in terms of generalization error, even with weight decay regularization. For convex problem, with the weight decay, Adam and GD converge to the same solution. ",
            "strength_and_weaknesses": "Strengths: \nThere are main three contributions in this work. \n1) On a well-designed two-layer network,  Adam often converges to a bad minimum while GD can converge to a good solution in terms of generalization error.   \n\n\n2) For convex problem, with the weight decay, Adam and GD converge to the same solution.  \n\n3)The above two results show the different behaviors of Adam and GD and may provide some insights for their deep learning application. \n\nWeaknesses:\n1)The analysis in this work heavily depends on the well-designed network and data assumptions. For network, this work designs a special two-layered network which is shallow and also uses a rarely used  polynomial RELU. This network has a big gap with the real network. For data, it assumes the feature is 1 sparse while the noise is also s-sparse. For feature, it is too sparse to hold in practice. Though the authors claim their results still hold when feature is k-sparse, as long as the sparsity gap between feature and noises exit. However, they did not explain more how sparsity gap is required and whether it is reasonable in practice setting.  So it is a big concern whether this analysis with these well-designed network and data can really reflect the performance for network training in practical applications.    \n\n\n2)In deep learning, one often SGD and stochastic Adam to train network. But this work analyzes GD and  deterministic Adam which yields a gap. Though the authors also say their results can be extended to stochastic setting but did not give many details. It is better to directly put the results under the stochastic setting in the paper, if stochastic setting can be well handled, since one often cares more about the real settings. \n",
            "clarity,_quality,_novelty_and_reproducibility": "For writing, most parts of this work are well written and clear.  \n\nFor the contribution parts,  since there are gaps on both network architectures, data assumptions, and algorithms, the analysis of this work actually does not match the practical setting and may not well reflect the truth of practical applications. \n",
            "summary_of_the_review": "Overall, this work provides some good initial results. But its assumptions may not be consistent with the real settings.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4057/Reviewer_oAui"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4057/Reviewer_oAui"
        ]
    },
    {
        "id": "3oo_UsOzzK",
        "original": null,
        "number": 3,
        "cdate": 1666660779042,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660779042,
        "tmdate": 1666660779042,
        "tddate": null,
        "forum": "iUYpN14qjTF",
        "replyto": "iUYpN14qjTF",
        "invitation": "ICLR.cc/2023/Conference/Paper4057/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the generalisation properties of Adam compared to gradient descent (GD). In order to simplify the analysis and isolate some distinguishing hypothesised differences between the two optimisation variants, the authors make use of a data model consisting of simple, sparse feature vectors and noise. The paper then provides a theoretical analysis in terms of convergence, generalisation, feature learning and noise memorisation, using a 2-layer neural network trained with weight decay regularisation. The main results are that both Adam and SGD behave similar in the convex case, but in the convex case they converge to different global solutions, Adam tending to fit the noise in the data.",
            "strength_and_weaknesses": "### Strengths\n\nAs main strength, I highlight that this paper is technically solid, mostly well-written and easy to follow. I value that introduction is clearly written, it includes motivations for the work and a concise review of the relevant related work. Further, both the methodology and the main results are discussed with sufficient detail, and I value that the significance of the mathematical results are unpacked nicely in the text, for example in Section 5. Finally, I also appreciate that, in the last section of the paper, some of the limitations of the paper are transparently mentioned as future work, for instance the limitation of the analysis to a two-layer neural network.\n\n### Weaknesses\n\nIn my opinion, the main weakness of this paper is that the limitations adopted to enable a solid and conclusive analysis may impact the relevance and generalisation of the conclusions for practical applications. In particular, I identify two main limitations: \n\n* The use of a highly synthetic data model (almost as simple as it can get), crafted to highlight the disadvantages of Adam with respect to gradient descent.\n* The dependence of the results on a two-layer neural network.\n\nThe first limitation is directly connected with the data. Data sets used in practical applications vastly differ from the data used to derive the theoretical results in this paper, which consists of the combination of a 1-sparse feature vector and noise vector. This choice highly simplifies the analysis, but it leaves doubts to whether the conclusions can be extended to more complex data. As a matter of fact, the authors point out that their \"theoretical analysis can lead to an opposite conclusion on the generalization comparison between Adam and GD if the noise is sparse and feature is denser\"\n\nThe second limitation is also important, as it restricts the relevance of the conclusions to shallow models (2-layer neural networks), whereas Adam was born as an optimisation algorithm for deep neural networks and it is in this realm where it has been found to be superior--- according to certain aspects such as convergence rate, lower dependence on hyper-parameter optimisation, as well as generalisation in some cases---to gradient descent.\n\nWhile I value the contribution of an analysis with a set of limitations to enable detailed conclusions, I think it is fair to highlight that the significance of the results are limited by such assumptions.\n\nSome more minor weaknesses I would like to point out are the following:\n\n* One of the main conclusions highlighted by the authors in the paper is that Adam and SGD converge to the same solution in the convex case, but differ in the non-convex case. That both algorithms are able to find the global optimum in convex optimisation (with weight decay) is intuitive (the differences lie in the convergence rate), and that the two algorithms are most different with a non-convex objective is also expected. Therefore, it is unclear how much technical novelty this conclusion is able to offer.\n* Figure 1 shows visualisations of the features learnt by the first layer of AlexNet trained with Adam and SGD on CIFAR-10. However, no further details are provided beyond those mentioned in the capture of the figure. This visualisation, in and on itself without further details and analysis, seems to me like a rather superficial result, disconnected from the rest of the paper which is focused on theoretical results.\n* In Section 3, the authors claim that the data model used in the paper is \"more practical\" than that in Wilson et al. (2017) and Reddi  et al. (2018). In this regard, I missed a more detailed discussion of the justification of the data model used in this paper, as well as the differences with models used in previous works analysing Adam. Furthermore, it is unclear to me how the data model in this paper can be \"more practical\", while it is an extreme case where the feature feature is a 1-sparse vector.\n* The last item in the review of related work (Feature learning by neural networks) lacks diversity, since only works from the same group of authors is cited. As a matter of fact, citations of Allen-Zhu & Li are over-represented in the list of references.\n* While the paper is well-written in general, some aspects could be improved in my opinion:\n    * Some sentences contain long list of citations, which makes them unreadable. For instance, see the paragraph \"Optimization and generalization guarantees in deep learning\"\n    * Typos or writing issues:\n        * \"In this section, we discuss the works that are mostly related to our paper\": the word \"mostly\" sounds strange to me in this sentence.\n        * The word \"data\" is used several times in singular, where I would instead use \"data point\". For instance: Definition 3.1, after Equation 3.2, second paragraph in Section 5.1\n        * patter -> pattern (in last paragraph (Proof outline) of the introduction of Section 5)",
            "clarity,_quality,_novelty_and_reproducibility": "I have commented on these aspects in the previous section about strengths and weakness, but here is a summary:\n\n* Clarity: the paper is well-written in general, and I especially appreciate that the text nicely supports the mathematical derivations.\n* Quality: without having reviewed in detail the whole appendix, the theoretical analysis seemed sound and solid to me, and the conclusions reflect the results.\n* Novelty: I am not an expert in the theoretical analysis of Adam, but I suspect that some of the conclusions might have been reached and discussed before, perhaps by different means, since they seem intuitive to me. However, I am not providing a large weight to novelty in my review.\n* Reproducibility: the paper contains only a small set of experimental results. Some of them contain a sufficient level of detail (as in Appendix A), but some lack sufficient details (Figure 1). I am not aware of any code accompanying this submission.",
            "summary_of_the_review": "My overall impression of this paper is positive, since it is a clearly written, solid analysis of relevant optimisation algorithms such as gradient descent and Adam. While I think the set of assumptions to carry out the analysis (shallow network and simple data model) limit the relevance of the results, I still think that the contributions in this paper advance our understanding of Adam and gradient descent.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4057/Reviewer_1Tav"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4057/Reviewer_1Tav"
        ]
    },
    {
        "id": "wuzC5h_q1-",
        "original": null,
        "number": 4,
        "cdate": 1666758469709,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666758469709,
        "tmdate": 1666758521847,
        "tddate": null,
        "forum": "iUYpN14qjTF",
        "replyto": "iUYpN14qjTF",
        "invitation": "ICLR.cc/2023/Conference/Paper4057/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "On the specified data distribution, this submission shows that Adam and GD, starting from the same initialization, can converge to different solutions with significantly various generalization errors, even with proper regularization. ",
            "strength_and_weaknesses": "####  Strength\n- This submission is well-written and the key motivation is well-discussed.\n- It reveals the difference w.r.t. generalization and convergence speed between Adam and GD on a non-convex problem under the non-asymptotic setting, which was an open problem before.\n\n\n\n#### Weaknesses\n- The submission does not consider the de-bias term in Adam; hence, it should specify the initialize condition for $m_{j,r}$ and $v_{j,r}$ in Eq. (3.3) and Eq. (3.4), respectively.\n- The noise vector's norm is much smaller than that of the feature part.\n- The stochastic gradient complexity to find an \u000f$\\epsilon$-approximate first-order stationary point on the general non-convex problem (non-stochastic) for GD is $\\epsilon^{-2}$. Namely, $T = O(\\epsilon^{-2})$ in Thm.4.1 for GD, which is right. Not the complexity's lower bound for the first-order optimation methods on the non-stochastic non-convex problem is $\\epsilon^{-1.75}$. The complexity for Adam in Thm.4.1 is $\\epsilon^{-1}$, which obviously violates the known lower bound. I know some assumptions exist on the data and model while the low bound is proved in the worst case, but the author should also discuss the part against a common guarantee.\n- It is strange that the results for Adam have no connection to the moving average coefficients $\\beta_1$ and $\\beta_2$. \n- As the author also claimed, Adam is only similar to signGD when  $\\beta_1$ and $\\beta_2$ are small. However, in practice and as pointed out by the author, we usually set  $\\beta_1=0.9$ and $\\beta_2=0.99$, which distinguish signGD and Adam. Moreover, small  $\\beta$'s may make Adam diverge on some examples, but large $\\beta$'s will not. Hence I quite doubt the proof routine that extends the results on signGD to Adam.\n- For Lemma 5.2, $\\eta s \\sigma_p$ seems much smaller than $\\eta$ due to $s = O(1)$ and $\\sigma_p = O((poly log (n))^{-1})$, why to claim that $\\eta s \\sigma_p$ increases. much faster?",
            "clarity,_quality,_novelty_and_reproducibility": "Although there are assumptions about the data, this paper considers the problem in a new setting (i.e. non-asymptotic and non-convex), so the novelty seems sufficient.",
            "summary_of_the_review": "Although there are some limitations and issues in the theoretical part, this submission attempts to explain an interesting experimental observation from the theoretical perspective, so I have a positive score on it.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4057/Reviewer_zveC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4057/Reviewer_zveC"
        ]
    }
]