[
    {
        "id": "JnURWkP6tl7",
        "original": null,
        "number": 1,
        "cdate": 1666591238176,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591238176,
        "tmdate": 1666591238176,
        "tddate": null,
        "forum": "5g4FC-SHkaV",
        "replyto": "5g4FC-SHkaV",
        "invitation": "ICLR.cc/2023/Conference/Paper2016/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the data heterogeneity issue in Federated Learning (FL) by letting local clients leverage model parameters from nodes with similar data distributions. Each client approximates the data similarity of its neighbors through a weighing parameter $w_{ij}$ which is learned by EM optimization. In order to achieve this learning, each client needs to broadcast its learned model gradients to (selected) neighbors. The weighted ensemble of neighboring models is used for inference as the final model once upon training convergence.\n",
            "strength_and_weaknesses": "\nPros:\n+ Technical sound approach towards addressing data heterogeneity.\n+ Well-designed experiments that validate each component in the proposed approach.\n+ Comprehensive related work.\n\nCons:\n\nLowe efficiency in 1) communication during learning and 2) model inference after learning convergence. Even with the modifications that relax the all-to-all communication pre-requisite, each client still needs to download gradients from multiple neighbors. While FedAvg style algorithm is more communication efficient. The final model inference still requires a forward pass through multiple models. In general, I feel that this design is not practical when the model size is scaled up or when the number of users is high.\n\nMinor:\n\nAn algorithm box should be provided to summarize the proposed approach.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper proposed a technically sound approach to tackle data heterogeneity. It is clearly written with moderate novelty.",
            "summary_of_the_review": "This paper proposed an FL approach that ensemble local models with similar data distributions to tackle data heterogeneity. This approach is technically sound but could raise potential concerns regarding communication efficiency and scalability.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2016/Reviewer_r1RL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2016/Reviewer_r1RL"
        ]
    },
    {
        "id": "YNJMMn9QdI",
        "original": null,
        "number": 2,
        "cdate": 1666756204798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756204798,
        "tmdate": 1666756204798,
        "tddate": null,
        "forum": "5g4FC-SHkaV",
        "replyto": "5g4FC-SHkaV",
        "invitation": "ICLR.cc/2023/Conference/Paper2016/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper focuses on personalized server free federated learning. A decentralized algorithm is proposed based on EM algorithm in which each device learns from the devices which has similar data distributions. Theoretical results are provided along with empirical justifications.",
            "strength_and_weaknesses": "*Strengths*\n- The paper is well written and easy to follow. \n- The problem is interesting and proposed solution is discussed theoretically as well as experimentally. \n\n*Weakness*\n\n- I am not sure if decentralized setting is well motivated for federated learning. This is specially be because the FL is specifically for larger number of devices, and implementing a decentralized algorithm is almost impossible in those settings. The authors should provide some specific use cases for the motivation.\n\n- The proposed framework seems to solve the issue of FedAvg, but it brings is new issue of requirement of agents communicating with each other which can be impractical to consider. Please provide some examples. \n\n- How to select the neighbors, what kind of communication graph is required for the algorithm to work? \n\n- What are the additional challenges in the theoretical analysis for the decentralized settings? For instance, what are the additional challenges need to be addressed as compared to decentralized EM algorithm in the following paper, and it's related papers. \n\nP. A. Forero, A. Cano and G. B. Giannakis, \"Consensus-based distributed expectation-maximization algorithm for density estimation and classification using wireless sensor networks,\" 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, 2008, pp. 1989-1992, doi: 10.1109/ICASSP.2008.4518028.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and novelty is good. ",
            "summary_of_the_review": "Overall the paper is written well, and the idea is interesting, but it is not clear how much is the contribution in terms of algorithmic development as compared to existing decentralized EM algorithms in the literature. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2016/Reviewer_Knpq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2016/Reviewer_Knpq"
        ]
    },
    {
        "id": "SeUzmqazDZU",
        "original": null,
        "number": 3,
        "cdate": 1666764673747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666764673747,
        "tmdate": 1666764673747,
        "tddate": null,
        "forum": "5g4FC-SHkaV",
        "replyto": "5g4FC-SHkaV",
        "invitation": "ICLR.cc/2023/Conference/Paper2016/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem that the global model may perform poorly for most clients, e.g. the data at different clients is very diverse. It addresses this problem by learning how data distributions of different clients mix together as the distribution at client i. It utilizes EM algorithm to keep updating the mixture weights and in the end each client makes predictions by a mixture of all clients' local models. Further, it proposes a version with efficient computation. In the experiments, the efficacy of proposed algorithms is justified.",
            "strength_and_weaknesses": "**Strength**\n1. The problem is well motivated.\n2. This paper proposed a novel algorithm (to my best knowledge, though I am not very familiar with the literature) with the guarantee of convergence.\n\n**Weaknesses**\n1. According the description of dataset, is it possible that when clients are in the sample group, some data will simultaneously appear in the training data (of some client) and test data (of other client)?\n2. The paper seems to miss the experiment that different distributions have some overlap instead of disjoint support, e.g. client 1 with 80% label 0 and 20% label 1 and client 2 with 20% label 0 and 80% label 1? When this happens, for sanity check, would the mixtures of distributions by learned weights are the distributions of each client?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good. The algorithm is novel to my best knowledge.",
            "summary_of_the_review": "The problem and the proposed algorithm are well motivated. However, experiments at one reasonable setting are missed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2016/Reviewer_Tffe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2016/Reviewer_Tffe"
        ]
    },
    {
        "id": "QMFisNwqI5",
        "original": null,
        "number": 4,
        "cdate": 1667084899646,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667084899646,
        "tmdate": 1667084899646,
        "tddate": null,
        "forum": "5g4FC-SHkaV",
        "replyto": "5g4FC-SHkaV",
        "invitation": "ICLR.cc/2023/Conference/Paper2016/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes FedeRiCo, a decentralized federated learning framework that allows clients to collaborate much or little with other participating clients to enhance the performance of the federated learned models. FedeRiCo leverages an EM-style optimization procedure to solve the global model weights and clients' collaboration weights concurrently. Both theoretical analysis and empirical results are shown to justify the effectiveness of the proposed FedeRiCo method.",
            "strength_and_weaknesses": "Strength:\n- the paper is well-written and the targetted problem is well motivated.  \n- both theoretical and empirical results are shown to demonstrate the effectiveness of the proposed method.  \n\nWeaknesses:\n- the proposed EM algorithm seems to be somewhat known, which limits the novelty of the proposed FedeRiCo method.  \n- the privacy guarantees for the decentralized federated learning framework are not clear. It seems the individual client's gradient can be seen by the other clients (in the small gradient steps in lieu of full optimization in each round). The authors are expected to discuss the privacy guarantees more explicitly.\n- the scale of the experiment is small, how does FedeRiCo perform on the ImageNet scale datasets?",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: the presentation of the paper is clear, and the proposed idea is easy to follow.   \n- Quality: the writing quality of the paper is good. Both theoretical and experimental results are shown to justify the effectiveness of the proposed method.\n- Novelty: the proposed method seems to be somewhat known, and thus the novelty is limited.\n- Reproducibility: enough details are shown for the implementation and experimental setup. I thus believe the experiments can be reproduced.  ",
            "summary_of_the_review": "The paper seeks to tackle an interesting and important problem in federated learning to enhance collaboration among participating clients. The proposed method makes sense but is somewhat known. I am thus concerned about the novelty of the paper. Moreover, the privacy guarantees of the proposed method are not clear, which is a clear weakness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2016/Reviewer_xqTG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2016/Reviewer_xqTG"
        ]
    }
]