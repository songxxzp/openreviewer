[
    {
        "id": "eEpTawr0IDv",
        "original": null,
        "number": 1,
        "cdate": 1666716881611,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666716881611,
        "tmdate": 1666716881611,
        "tddate": null,
        "forum": "43nOUI4VHUw",
        "replyto": "43nOUI4VHUw",
        "invitation": "ICLR.cc/2023/Conference/Paper5675/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new method to improve watermarks for generative models, particularly investigating the trade-off between attribution accuracy and generation quality. The proposed method explores latent semantic space as watermark, by mean of principal components and eigenvectors. The paper also explores how the design parameters of the method can be used to establish a practical and computationally inexpensive trade-off. Experiments show that the method outperforms the SOTA that uses shallow watermarking.",
            "strength_and_weaknesses": "\nStrengths:\n\n1) the paper is well written and well organised, presenting convincing evidence that it outperforms the SOTA\n2) the theoretical derivations are thorough and seem also convincing\n\nWeaknesses:\n1) this authors only compare their method with the method of Kim et al. (2020). The method should be compared against more methods that explore different properties of watermarking. For instance:\n\nMatthew Tancik, Ben Mildenhall, and Ren Ng. Stegastamp: Invisible hyperlinks in physical photographs.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 2117\u20132126, 2020. [exploring printer-proof watermarking]\n\nShadmand, Farhad, Iurii Medvedev, and Nuno Gonc\u0317alves. \"CodeFace: A Deep Learning Printer-Proof Steganography for Face Portraits.\" IEEE Access 9 (2021): 167282-167291. [exploring printer-proof watermarking]\n\nRuowei Wang, Chenguo Lin, Qijun Zhao, and Feiyu Zhu. Watermark faker: towards forgery of\ndigital image watermarking. In 2021 IEEE International Conference on Multimedia and Expo\n(ICME), pp. 1\u20136. IEEE, 2021. [exploring resistance to watermark attack]\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and show goos quality.\nIt present a novel method.\nAs the code is not available, it is difficult to reproduce.\n",
            "summary_of_the_review": "The paper is interesting and presents a convincing method for watermarking. However, the paper would benefit of comparing its results against more difficult scenarios (printer-camera) and alternative methods.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5675/Reviewer_X6TA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5675/Reviewer_X6TA"
        ]
    },
    {
        "id": "VCN2mV3UMA7",
        "original": null,
        "number": 2,
        "cdate": 1666863750046,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666863750046,
        "tmdate": 1669011512044,
        "tddate": null,
        "forum": "43nOUI4VHUw",
        "replyto": "43nOUI4VHUw",
        "invitation": "ICLR.cc/2023/Conference/Paper5675/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel approach for watermarking generative models (GANs + latent diffusion models). While previous approaches focused on adding watermarks in the image space, the paper's approach adds watermarks in the latent space to induce subtle semantic changes to the generated images. The latent directions of change are selected as the principal components but with small variances to avoid strong semantic changes. By using these latent directions to watermark the images, the procedure promises to be computationally efficient since it needs to only compute the directions once. The paper further provides a theoretical analysis of the accuracy-quality tradeoff and empirically evaluates the approach on StyleGAN2 and Latent Diffusion models.",
            "strength_and_weaknesses": "Strengths:\n+ Tackles an important problem of generative models: watermarking the generative models (and their generated images) to allow attribution of contents to their source models.\n+ Most parts of the paper are clearly written and easy to follow. Particularly the introduction section states a good entry into the problem.\n+ The proposed solution of adding watermarks in the latent space instead of the image space offers an interesting avenue.\n+ The empirical results promise that the model indeed beats previous approaches in terms of accuracy-quality tradeoff and induces less noticeable changes to the images.\n+ The formal analysis of the accuracy-quality tradeoff provides interesting insights into the problem.\n\nWeaknesses:\n- Parts of section 3 are partially imprecise and difficult to read. Rewriting some parts of it and adding some intuition would make following it easier. Also, a list of symbols in the appendix might be worth considering.\n- I expect the approach to only work for generative models with disentangled latent spaces. So for other types of GANs without using, e.g., a mapping network to entangle the dimensions, the approach is not applicable. Similarly, I am not sure if the approach works (in principle) with other diffusion models, e.g., imagen or DALL-E 2.\n- I also miss some information on the computation needed to perform the attribution step. Since it needs to solve an optimization that involves forward passes through the generator, it might take some time. So I think approaches with watermarking in the image space need to train separate attribution networks, which is costly at training but fast at inference. And the proposed approach is fast at watermarking (does not need to train a separate attribution model) but the attribution process might be slow / computation costly. If this is true, the trade-off should at least be stated in the paper.\n- Unfortunately, no source code is provided to reproduce the results",
            "clarity,_quality,_novelty_and_reproducibility": "Whereas some aspects and the intuition of the approach are clear, some other aspects, particularly Section 3, are hard to follow. For example, the sentence \"Let l : Rdx \u00d7 Rdx \u2192 R be a distance metric between two images, (\u03b1\u0302, \u03d5\u0302) the estimates.\" in 3.2 does not make clear, how the estimates parameters are related to the distance metrics. Readability can be improved by providing more details in the whole section and improving sentence structures.\n\nBesides, the quality of the writing and experiments is good. However, some abbreviations, such as SG2, should be introduced. \n\nOverall, the approach is novel and adds an interesting avenue to the problem of watermarking. I am not aware of previous research proposing a similar approach but I am also not deep into the watermarking literature.\n\nI expect most parts of the approach could be implemented following the description in the paper. However, since no hyperparameters and seed are stated and no source code is provided, I expect an exact reproduction of the results to be impossible.\n\nSmaller remarks:\n- The figures, particularly in the appendix, should be added as pdf files to enable loss-less zooming. \n- The statement at the end of section 4, \"Attacks with large pixel-wise perturbations can be perceptually insignificant.\" should be supported by some reference.\n- The URL in the reference of Kelly is not fitting the page margin.",
            "summary_of_the_review": "In summary, I like the proposed idea and direction of the paper, as well as the empirical evaluation, and think that it will benefit the community if the paper would get accepted. It tackles an important problem of generative models that becomes even more important with increasing model quality and recent advances in text-to-image synthesis. However, the quality of the writing should be improved, and details for reproduction should be added, in the best case together with the source code.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not have any ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5675/Reviewer_Dm6b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5675/Reviewer_Dm6b"
        ]
    },
    {
        "id": "rhNu0NRzKc",
        "original": null,
        "number": 3,
        "cdate": 1666997114400,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666997114400,
        "tmdate": 1666997114400,
        "tddate": null,
        "forum": "43nOUI4VHUw",
        "replyto": "43nOUI4VHUw",
        "invitation": "ICLR.cc/2023/Conference/Paper5675/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to produce watermarks on the content of generative models for the purpose of source tracking. The watermarks are added to the latent representations to produce outputs that are identifiable while preserving the quality of the distribution when compared to the original generative model. \n\n",
            "strength_and_weaknesses": "Strengths: \n* The problem of source tracking the generated content of various generative models is very important.\n* The proposed method is novel and conceptually interesting. \n\nWeakness: \n\nMy main concern of this method is in the paper's evaluation setup and goals. In the paper, the authors only considered the following scenarios: (1) no post-process, (2) simple post-processing such as JPEG, noise blur etc.  Both (1) and (2) can be achieved easily by appending a blind image watermarking algorithm on top of the generated content. E.g. output = BlindWatermark(G(z), identifier). I believe the proposed baseline would actually outperform the suggested method on the simple distortions measured in this paper (See works such as [1] and [2]). \n\nI do believe there to be advantages in encoding in the latent semantic space that the baselines [1] and [2] cannot solve. Namely, if the \"distortion\" itself is a \"deep learning based\" attack. E.g., some conditional generative model which distorts the image far in the pixel space, but retains the semantic meaning.  But this is not measured at all in the paper and defeats the purpose of encoding information in the latent space. \n\nOther minor suggestions:\n* More visual samples, and human rating results on the generated samples would help. FID alone isn't enough to determine visual sample quality. \n* The structure of the paper could be improved. Proposition statements should be moved to the appendix and Section 3 should contain clearly the step-by-step explanation of the watermark encoding and decoding algorithms. This would help the less experienced reader quickly understand the proposed method.\nE.g. Encoding: Find d_phi principle component vectors of matrix H ....\nDecoding: Solve Equation (Sec 4.3) to estimate the watermark signal phi. \n\n\n[1] Zhu, Jiren, et al. \"Hidden: Hiding data with deep networks.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n[2] Luo, Xiyang, et al. \"Distortion agnostic deep watermarking.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper could be structured better. It took me a while to sift through the actual method scattered throughout the proposition and experiment sections. \n\nQuality: See weakness.\n\nOriginality: Good.",
            "summary_of_the_review": "Overall I think the paper proposes an interesting idea, and experiment results show some promise of the proposed approach. But given the major concerns over the evaluation, I do not think the paper is ready for ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5675/Reviewer_tdgU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5675/Reviewer_tdgU"
        ]
    },
    {
        "id": "8PJ6NejZ2it",
        "original": null,
        "number": 4,
        "cdate": 1667763605859,
        "mdate": 1667763605859,
        "ddate": null,
        "tcdate": 1667763605859,
        "tmdate": 1667763605859,
        "tddate": null,
        "forum": "43nOUI4VHUw",
        "replyto": "43nOUI4VHUw",
        "invitation": "ICLR.cc/2023/Conference/Paper5675/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to attribute generative models using digital watermarking. The paper explores the use of latent semantic dimensions as watermarks. Experiments are presented using StyleGAN2 and Latent diffusion models, which show that the approach is promising.The paper further showed that there is a tradeoff between generation quality and attribution accuracy. ",
            "strength_and_weaknesses": "The paper showed that there is a tradeoff between generation quality and attribution accuracy. \n\nThough some aspects of the paper are interesting, many parts are still not clear. Practical use case is missing. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Mostly clear. Not fully known how reproducible the paper is. ",
            "summary_of_the_review": "Though some aspects of the paper are interesting, many parts are still not clear. \n\nIt is not clear how this approach will be useful in a practical scenario. Since watermarking is applicable in practical use cases, the authors should explain how the approach can be used practically. For example, are the authors suggesting to use a StyleGAN algorithm which also includes the deep watermarking method, and use this algorithm for generation?\n\nIt is not clear what the watermark actually is. Usually a watermark has some message or pattern embedded. \n\nOnly StyleGAN2 and Latent diffusion models have been attributed. To be practically useful, more GANs or generative methods need to be experimented. \n\nIn the post processing experiments, the post processing parameters are done for a fixed setting (example Guassian noise of -.1 standard deviation). It is also not clear what JPEG quality factor was used. Usually for post processing experiments, a range of values are considered and the analysis is then performed. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5675/Reviewer_h68J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5675/Reviewer_h68J"
        ]
    }
]