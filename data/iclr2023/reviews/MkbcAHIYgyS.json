[
    {
        "id": "qizHXdetC8",
        "original": null,
        "number": 1,
        "cdate": 1666463354519,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666463354519,
        "tmdate": 1666463354519,
        "tddate": null,
        "forum": "MkbcAHIYgyS",
        "replyto": "MkbcAHIYgyS",
        "invitation": "ICLR.cc/2023/Conference/Paper1606/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper develops a batch update method for editing memory in transformers called MEMIT. Experiments are done on incorporating true/fake information (zsRE/CounterFact) using GPT-J and GPT-NeoX. On these tasks, MEMIT dramatically outperforms small update methods (MEND, ROME) when the number of edits is large.",
            "strength_and_weaknesses": "STRENGTHS\n- A new problem proposal (\"mass-editing\") on the relatively new topic of memory editing in transformers\n- On the proposed new setting, MEMIT is the only memory editing method that works. \n\nWEAKNESSES \n- I find the paper quite opaque. I find it difficult to learn much insight, besides the high-level impression that MEMIT extends ROME to make multiple edits and modify multiple layers at once. Why this is so necessary to achieve effective editing is not very clear, and consequently much of the work feels like engineering.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed research problem is novel and seems useful. \n\nThe paper itself is opaque, stitching a sequence of steps together each of which is described without much insight. As an empirical work this is perhaps okay. ",
            "summary_of_the_review": "The paper proposes MEMIT, a method for mass-editing memory in transformers, which works very well in the proposed setting. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1606/Reviewer_6w9b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1606/Reviewer_6w9b"
        ]
    },
    {
        "id": "CGUI19tILM",
        "original": null,
        "number": 2,
        "cdate": 1666658029526,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658029526,
        "tmdate": 1666658029526,
        "tddate": null,
        "forum": "MkbcAHIYgyS",
        "replyto": "MkbcAHIYgyS",
        "invitation": "ICLR.cc/2023/Conference/Paper1606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a methodology to edit memory in a transformer model which can be used to modify or add the model's knowledge. ",
            "strength_and_weaknesses": "Pros:\n- Knowledge editing seems quite intriguing especially for transformer models which seems like a black box. This seems to be a step in the right direction of model interpretability. \n\n\nCons:\n- However, the knowledge that can be edited is based on associations related to subjects. I wonder if this can be extended to cover more general knowledge such as adding abilities for model to know a broad set of facts in a topic, etc. \n- This methodology seems to rely on critical layers \u2014 do they always exist? what if all layers are critical? would the method scale to such behavior?\n- The method also seems heavily based on 'last subject token' which seems quite specific.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is quite well written.\n\nNovelty: Moderate novelty. This paper iterates on previous work to perform editing on thousands of associations at once.\n\nQuality: The technical proposal and the experiments are quite nice -- the knowledge editing seems quite limited in scope to be used for practical purposes.\n\nReproducibility: The promised code and data release should make it reproducible. ",
            "summary_of_the_review": "The paper probes an interesting aspect of transformers language model, that is, how to modify knowledge encapsulated in the weights. This paper demonstrates that it is possible to do so in the factual association setting, which shows premise and shed some light on the interpretability. The scale of the edits is impressive compared to previous work. The experiments are quite well executed. However, the scope of this work for real-world applicability is still in question, as the knowledge editing pertains to explicit relations, or what the user calls associations. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1606/Reviewer_J5Nr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1606/Reviewer_J5Nr"
        ]
    },
    {
        "id": "72BgfSVctk",
        "original": null,
        "number": 3,
        "cdate": 1666671776697,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671776697,
        "tmdate": 1669079187986,
        "tddate": null,
        "forum": "MkbcAHIYgyS",
        "replyto": "MkbcAHIYgyS",
        "invitation": "ICLR.cc/2023/Conference/Paper1606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the problem of editing factual knowledge in transformer-based language models. Authors note that existing model editing techniques fail when introducing a large batch of edits. They present a fairly model-specific algorithm called MEMIT, directly modifies model weights of FFN (MLP) transformer layers that matter most for a specific input. On GPT-J-6B and GPT-NEO-20B, MEMIT can perform a large batch (e.g. $10^4$) edits with reasonable efficacy and specificity.",
            "strength_and_weaknesses": "\n\n### Strengths\n\n- to the best of my knowledge, this work is technically sound: the MEMIT algorithm appears valid and agrees with the current understanding of large language models\n- the main evaluations deal with multi-billion-parameter language models - the types of models that are (arguably) most likely to need editing, since retraining them from scratch is extremely expensive\n- authors take a considerable effort to make their experiments reproducible. Two rounds of applause for (1) writing a decent README and (2) specifying versions of all dependencies (in a conda yml).\n- authors clearly state the main limitations of their work (with one major exception, see weaknesses)\n- the paper is generally well-written and easy to follow\n\n### Weakness\n\n- Motivation: the paper focuses on *how* to to make $10^4$ edits, but talks very little about *why* would one need to do that. I can come up with some obvious use cases, e.g. running public machine translation systems that deal with bulk user feedback -- or patching GPT-J (2021) with all major news from 2022. However, I'd argue that it would be best if authors discuss these potential applications somewhere within the first 2 pages.\n\n- Hiding runtime numbers: to the best of my udnerstanding, running MEMIT requires at least 0.9 hours per experiment (Appendix B.4), compared to <=2 minutes for MEND. While it does not invalidate the main contributions (and it is still faster than, say, ROME), it would be best to clearly state that the method is slower in the limitations section and in the main paper.\n\n- Lacking ablation: MEMIT is a fairly complex algorithm with many design decisions. It would be best to validate that these decisions actually matter. How significant is it that MEMIT edits MLP (and not attention) layers? Is MEMIT sensitive to \\lambda - and how sensitive? How does the efficacy / consistency change with the number of layers (i.e. if we restrict MEMIT to edit a smaller number of layers)?\n\n\n\n### Missing related work\n\n- https://openreview.net/forum?id=HJedXaEtvS - to the best of my knowledge, this is the first published work on editing transformers, though it is closer to MEND and likely not scalable to LLMs\n\n### Questions\n\n- in Table 1, does MEND perform all 10^4 edits sequentially, in parallel, or some combination of the two (e.g. $10$ times $10^3$)? (S5.1 correctly states that MEND *can* apply edits in parallel, but does not state if it does\n\n- How many parameters, on average, are updated in MEMIT? (depending on the number of samples and the model) Is it a negligible fraction, or a significant potion of the entire model? Why asking: is it feasible to maintain multiple MEMIT updates in parallel like adapters / soft prompts?\n\n- In appendix B.4, you state \"If all such vectors are already computed, MEMIT takes 3226.35 sec\". How long does it take to compute these vectors?\n\n\n> GPT-NeoX runs require at least two: one 48GB GPU for running the model in float16, and another slightly smaller GPU for executing the editing method. \n\nAre there any obvious ways to run MEMIT in a smaller memory footprint? E.g. does MEMIT workload allow for efficient RAM offloading?",
            "clarity,_quality,_novelty_and_reproducibility": "\n**Clarity and quality:** overall, the paper is well-written and easy to follow, with a fairly standard overall structure. Subjectively, I would argue that it lacks practical motivation / justification, explaining when / why does one need to perform many edits in parallel.\n\n**Novelty:** to the best of my understanding, the proposed algorithm is (1) novel and (2) first to tackle the specific problem (mass edits) better than FT-W. However, unless authors convince me otherwise, I believe that this is a fairly niche problem, limiting the potential impact.\n\n**Reproducibility:** as i stated earlier, authors made considerable effort in making this paper reproducible, providing both configuration, instrutions, and the specific versions of all the libraries. If there was a score for reproducibility, I would rank it as high, definitely above average among the last year's accepted papers.",
            "summary_of_the_review": "To the best of my understanding, this paper offers a technically sound and practically effective solution to a relatively niche problem. While it has a few areas for improvement (e.g. explicitly compare algorithm runtime in the main paper, add clearer motivation in the first 2 pages). However, these changes are relatively minor, i.e. possible to implement within the author response period.\n\nMy overall recommendation is \"Weak Accept\" since this paper brings meaningful contribution to a niche audience. I can be convinced to raise my score if authors convince me that their work has (significantly) higher impact on researchers / practitioners than i currently assume.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1606/Reviewer_NCki"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1606/Reviewer_NCki"
        ]
    },
    {
        "id": "whPF4v1eONH",
        "original": null,
        "number": 4,
        "cdate": 1666723860880,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723860880,
        "tmdate": 1666724706125,
        "tddate": null,
        "forum": "MkbcAHIYgyS",
        "replyto": "MkbcAHIYgyS",
        "invitation": "ICLR.cc/2023/Conference/Paper1606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work addresses the problem of updating world knowledge in large language models (LLMs), which learn many common facts through training on large-scale datasets. This problem is an important one to the machine learning community, as many applications rely on knowledge acquired by LLMs for different applications. The authors narrow down the problem to (subject, relation, object) triplets, or ($s_i, r_i, o_i$). This is assessed by giving the model a natural language prompt $p_i = p(s_i, r_i)$ and having the LLM predict $o_i$. The edits keep subject $s_i$ and relation $r_i$ the same, but modify the associated object $o_i$. This is accomplished by analyzing and modifying the feedforward MLP layers of a transformer, which have been shown to contain key-value memories.\n\nFirst, the authors identify which layers are critical to storing a given fact, using a causal tracing method. They then use the insight that the transformer MLP stores key-value memories to estimate a change $\\delta$ that updates a given layer's MLP to store the updated memory (in the form of an ($s_i, r_i, o_i$) triplet). Each layer in the set of critical layers is altered to contribute an approximately equal portion of the overall change needed for the model to output the updated ($s_i, r_i, o_i$). The estimated updates are found by using gradient descent to move the hidden state representations toward responding with the updated object $o_i$ in response to the prompt $p_i$. The overall method is called MEMIT.\n\nThe authors evaluate their method by measuring several metrics to test how well the model has learned the new edited facts, measuring efficacy, generalization, specificity, fluency, and consistency of its responses. This is done by evaluating how well MEMIT preforms on adding correct information, as well as adding counterfactual information, which is a false fact that has a low score from the original unedited LLM. These are datasets used in a recent work (Meng et al. 2022). Their results show that MEMIT performs especially well at scaling the number of edits compared to other recent editing methods.",
            "strength_and_weaknesses": "Strengths\n- Well-motivated by previous literature and views on memory in transformers/NNs\n- Thorough and clear section to explain the editing algorithm (although see question below)\n- Experimental results show a clear advance over previous methods for large-scale fact editing in transformer architectures\n    - Experiments include a comparison to a fine-tuning baseline\n- Clearly notes limitations of their method (i.e. doesn't cover spatial or temporal reasoning)\n- Authors plan to open-source the method on publication\n\nWeaknesses\n- Needs more clarity on the evaluation metrics used in the experiments\n    - The metrics in Table 1 are presented in 5.2.1, and several more metrics in 5.2.2. Are these actually the same metrics with slightly different names? Some specificity on this would be appreciated.\n\nFurther questions\n- In Eq. 17 and in Algorithm 1, line 6, the layers are $l \\in \\mathcal{S}$. However, in section 4.1, Figure 2, and the beginning of 4.3, the critical layers are denoted $l \\in \\mathcal{R}$. Is this difference a mistake, or is there a difference between $\\mathcal{S}$ and $\\mathcal{R}$? If there is, please define $\\mathcal{S}$ in 4.3, or somewhere near Eq. 17, to make this clear.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the work was clearly written and presented.\n\nQuality: the paper is well-written, the model choices well-motivated and explained, and the experiments appear to be sound.\n\nNovelty: while drawing on some previous work on causal intervention, this is a clearly novel proposal that has notable scaling results that have not been seen in previous work in this area. This is an important contribution in a newly developing area of research.\n\nReproducibility: while I did not thoroughly review the code provided in the supplementary material, the authors appear to have shared the details needed to reproduce their method and results. The authors also plan to open-source the method after publication.\n",
            "summary_of_the_review": "Overall, I enjoyed reading this paper and found the contributions clear and the results convincing. While I am not intimately familiar with the recent literature on model editing, this is clearly an important problem that seems to be well-addressed by this method. In particular, the success of their approach in scaling to thousands of edits makes it a promising proposal for modifying LLMs over time. I also appreciate that the authors discuss the limitations of this approach as well -- it is restricted to working with $(s, r, o)$ triplets, and does not cover many other types of knowledge that LLMs seem to acquire. However, the method may provide a template for ways to address other types of declarative knowledge.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1606/Reviewer_55ot"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1606/Reviewer_55ot"
        ]
    }
]