[
    {
        "id": "BE_T--a2poK",
        "original": null,
        "number": 1,
        "cdate": 1666494620917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666494620917,
        "tmdate": 1666494620917,
        "tddate": null,
        "forum": "ConT6H7MWL",
        "replyto": "ConT6H7MWL",
        "invitation": "ICLR.cc/2023/Conference/Paper216/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on a very interesting issue: how to learn from black-box models using label-only data? For this purpose, they propose a novel method called IDEAL, which aims to query-efficiently learn from black-box model APIs in order to train a good student model without any real data. In detail, the proposed method trains the student model in two stages: data generation and model distillation. Their proposed IDEAL is query-efficient, as it does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Results can well support that the proposed method consistently outperforms all the baselines.",
            "strength_and_weaknesses": "Strengths\uff1a\n\n- This paper focuses on learning from label-only black-box models, an interesting but less explored topic. From my understanding, the proposed method is by far the most practical one that considers data-free, label-only, black-box, and query-efficient learning. In Table 1, it is clear that query-efficiently training a good student model from black-box models with hard labels is very practical, but challenging. Generally, I think the investigated problem is sound and interesting. I think this can be an extremely strong paper in black-box KD.\n\n- The idea is novel and straightforward. The approach is technically sound. The experiment showcases solid performance improvement over baselines. Particularly in Table 2, the proposed  method has significantly better results than other baselines. In Fig. 3, this paper provides convincing and solid comparisons between baselines with small and large query budgets.\n\n-The paper also conducted very detailed and convincing ablation studies. The ablation studies are quite thorough in my view. It\u2019s easy to understand why this method works.\n\nOverall, this paper is very interesting and to my knowledge novel. The presentation is clear and easy to follow. Technical details are clearly described. It seems like a pioneering contribution towards black-box KD. \n\nWeaknesses:\nI didn't see major weakness in this paper actually. I am just curious about below questions:\n\n1. How different generator sizes affect the results. Since in my opinion, synthetic data are affected by the quality of the generator. Maybe the authors can give some explanation on this and also in the future version some supplementary materials can be provided for more detailed analysis.  \n\n2. Additionally, it will be interesting to investigate why DFME and ZSDB3 perform much worse than IDEAL, what's the cause? \n\n3. Also, is there any domain gap between synthetic data and original data? What will happen if we train the model from scratch with synthetic data?\n\n4. Last, in the Balancing section: why it is necessary to generate the same number of samples in each class? Is there an imbalanced data distribution? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Clear enough, this presentation is clear and easy to follow.\n\nQuality: The paper is well motivated with clear intuition and illustration. The paper is technically sound. The whole paper is well structured and easy to follow. \n\nNovelty: Good, the main ideas of the paper are ground-breaking.\n\nReproducibility: good, key resources (code, data) are available and sufficient details (e.g., experimental setup) are well described.\n",
            "summary_of_the_review": "A strong paper in black-box KD. Technical details are clearly described. It seems like a pioneering contribution towards black-box KD. Hence, I would like to vote for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper216/Reviewer_Ruz3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper216/Reviewer_Ruz3"
        ]
    },
    {
        "id": "DdeOSIrMXO0",
        "original": null,
        "number": 2,
        "cdate": 1666622642534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622642534,
        "tmdate": 1666622642534,
        "tddate": null,
        "forum": "ConT6H7MWL",
        "replyto": "ConT6H7MWL",
        "invitation": "ICLR.cc/2023/Conference/Paper216/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- The paper proposes a zero-shot Knowledge Distillation (\"ZSKD\") approach (i.e., does not require any samples from teacher's training data).\n- Similar to DFME (Truong et al.) and MAZE (Karyiappa et al.), the approach consists of two phases per epoch: (a) training a generator to produce synthetic labels; and (b) training a student network using generator's synthetic images annotated by a black-box teacher model. Unlike previous approaches, the proposed approach is query-efficient since gradients for the generator no longer requires 0-th order gradient estimates. Rather the gradients are generator by backprop-ing through the partially trained student network.\n- Results are verified on a number of datasets (CIFAR10, TinyImages, ...) and compared with many zero-shot KD baselines.",
            "strength_and_weaknesses": "### Strengths\n\n**1. Insight: Identifying query-inefficient bottleneck**\n- Although the proposed approach follows a similar approach to baselines (i.e., iterative training of generator and student model), it rightly identifies and tackles a relevant pain-point: estimating the gradients to update the generator incurs a large query budget. It appears that the drastic improvement in query efficiency can be attributed to addressing this pain-point.\n\n**2. Results - significantly better than prior art**\n- I appreciate the extensive comparison of the proposed approach with 7+ baselines and on 7+ datasets. The results are furthermore promising: the proposed approach results in drastic improvements over baselines e.g., 37.91\u219268.82 in CIFAR10.\n\n### Concerns\n\n**1. (Major concern) Counter-intuitive formulation: Generator loss**\n- I found counter-intuitive how the generator is trained in stage 1. For simplicity, let's assume a perfect generator and student network. In this case, the $L_CE$ would appear to be extremely large (in expectation) -- since the GT target $y$ is randomly drawn and compared with a random generation ($\\hat{y} = S(G(z)), z \\sim N(0, I)$). (Side-note: wouldn't a conditional generator make more sense?)\n- On a more general note, while I understand the algorithm steps, I cannot understand the reasoning behind it. Why is the gradient signal though a noisy student network with mislabeled examples informative enough to perform ZSKD?\n\n**2. KD vs. Model Stealing approaches**\n- I get the impression that the paper tackles the *Model stealing/extraction* problem (black-box teacher, query budget, \netc.), but constantly and unfairly compares to *Knowledge Distillation*.\n- For instance, \"KD methods are based on several unrealistic assumptions ... access teacher's training data ... white-box teacher\". I don't think this is a correct claim, since KD approaches are predominantly tailored for model compression -- Hinton et al. 2015 claim that it can be used to \"... compress the knowledge in an ensemble into a single model...\". In which case, it's a perfectly valid assumption to use a white-box teacher and its corresponding training data.\n- In contrast, the proposed approach is more consistent with model stealing works (several references missing here in the paper; please fetch and discuss relevant citations from DFME) where indeed the teacher model is a black-box prediction API. Unfortunately, the results fall short here -- randomly using a pool of publicly-available images over generated images results in significantly better accuracy scores as well as sample efficiency (typically with 50K queries, see references in Kariyappa et al., CVPR '20).\n- Overall, my concern is that the paper considers KD works as a straw-man, as opposed to model extraction literature. However, I'm slightly overlooking this as the paper is \"data-free\" and does not rely on a publicly-available image data pool (at the expense of results degradation).\n\n**3. Some misc. concerns**\n- Table 1 \"with a limited number of queries\": please mention how many queries per method.\n- \"Contributions ... new problem ... training models with hard-labels\": This has been studied plenty of times before e.g., Tram\u00e8r et al. '16, Knockoff Nets '20)\n- APIs return only top-1 class: This is another unfair claim. Plenty of pay-per-query cloud APIs provide probabilities e.g., [Google Cloud](https://cloud.google.com/vision/docs/labels).\n- The paper many times mentions a \"distillation loss\", but rather refers to simple cross-entropy given that there is no temperature-scaling factor.\n- (Nitpick) Would be nice to have an additional \"upper bound\" column in Table 2, where the numbers reflect the teacher model trained on GT data with the relevant budget.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: Very good. The paper is easy to follow and understand. One nitpick is mis-using definitions e.g., see concern 3 (CE loss referred to as distillation loss).\n\n**Quality**: Average. The evaluation section is quite strong and thorough (numerous KD baselines and many datasets). A concern however is better insights into the data generation step of the approach (see concern 1).\n\n**Novelty**: Average. The paper appears to use KD as a straw-man, while it is more consistent with model extraction formulations. Moreover, the general framework (generation + student network training) is very similar to DFME, MAZE, etc. Some suggestions on improving novelty: more elaboration/technical insights into the approach (under what conditions should it work? analysis), comparisons against model extraction literature, etc.\n\n**Reproducibility**: Very good. Although the code is not provided, I am confident in reproducing results given the implementation details in the paper.",
            "summary_of_the_review": "Overall, the paper proposes a straight-forward approach with strong results (significant improvements over ZSKD baselines). However, a big concern I have is that the rationale of the approach is not evident to me (why does it work, given large losses in ideal circumstances). A somewhat minor concern I have is unfairly using KD as a straw-man (e.g., KD requires white-box teacher).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper216/Reviewer_XB73"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper216/Reviewer_XB73"
        ]
    },
    {
        "id": "6XRqvVyY8Ee",
        "original": null,
        "number": 3,
        "cdate": 1666649479248,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649479248,
        "tmdate": 1670731034167,
        "tddate": null,
        "forum": "ConT6H7MWL",
        "replyto": "ConT6H7MWL",
        "invitation": "ICLR.cc/2023/Conference/Paper216/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of query-efficient learning from a black-box teacher model in a data-free way. This paper considers the additional constraint that only the hard labels (categorical predictions) of the teacher are available, rather than the soft-labels. The authors propose the IDEAL method for this problem which operates in two stages. First, the method uses the student model to train a generator that generates synthetic data. In the second stage, the synthetic data is used to perform hard-label distillation, i.e., train a student model so that its predictions match those of the teacher. The authors present empirical evaluations that demonstrate the query efficiency and effectiveness of the proposed approach.",
            "strength_and_weaknesses": "## Strengths\n* The problem of query-efficient, data-free learning from a black-box teacher that outputs only hard labels is well-motivated by real-world examples (e.g., Google BigQuery). It is a challenging problem that is of high relevance to the ML community.\n* The proposed approach contains novel components compared to prior work and using the student model to generate a diverse set of synthetic data is interesting.\n* Empirical evaluations on various data sets are presented that support the improved effectiveness of the proposed work. Based on the results, IDEAL improves over the state-of-the-art by more than 20% on the evaluated scenarios, which is quite impressive. This trend holds for larger data sets containing a larger number of classes as well. The hyperparameters for the experiments are reported for reproducibility.\n* The authors present ablation studies that help justify the various components of the method (Table 4).\n* The paper is well-written with a clear exposition overall. For example, the accompanying visualization of the data generated by different methods (Fig. 4) is quite interesting and helps understand the benefit of the method.\n\n## Weaknesses\n* The proposed two-stage approach of generating synthetic data -> distilling knowledge using generated data appears in prior work (Wang, 2021). The novelty seems to lie in the synthetic data generation stage.\n* The synthetic data generation seems to be highly sensitive to the number of epochs $E_{\\mathcal G}$ that the generator is trained for (see Appendix A.0.1). It has to be not too high and not too low. This is quite puzzling and the explanation in Sec. A.0.1 is not very compelling. My understanding of the synthetic data generation stage is to generate a diverse set of examples with varying (student predicted) labels. Why would training the generator for more epochs lead to \u201coverfitting of the student?\u201d Shouldn\u2019t it be more conducive in generating a diverse set of synthetic data points?\n* Standard deviations of the results in Sec. 4 averaged over 3 trials are not reported.\n* Why is a scaling factor of $\\lambda = 5$ used for the experiments? How was this choice made and how sensitive is the algorithm to this choice?\n* How does the method fare in computational complexity relative to the compared approaches? My understanding is that the two stage process is done on a per-epoch basis, so it is not clear how much of a computational burden this imposes. To add to this concern, the authors mention downsizing the original images for the ImageNet subset for \u201cfast training.\u201d \n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the section above for notes on clarity, quality, novelty, and reproducibility.",
            "summary_of_the_review": "This paper addresses a challenging problem that is motivated by real-world applications. The authors propose an approach that performs exceptionally well in practice compared to state-of-the-art. I have some concerns regarding the sensitivity of the method to its hyperparameters and some clarifying questions that I raised in the sections above. Overall, I lean towards acceptance and would be willing to raise my score if my concerns are adequately addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper216/Reviewer_S4b4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper216/Reviewer_S4b4"
        ]
    },
    {
        "id": "6DAijo2qBBm",
        "original": null,
        "number": 4,
        "cdate": 1666658693932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658693932,
        "tmdate": 1670776411920,
        "tddate": null,
        "forum": "ConT6H7MWL",
        "replyto": "ConT6H7MWL",
        "invitation": "ICLR.cc/2023/Conference/Paper216/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors propose the \u201cIDEAL\u201d algorithm that can improve the distillation process when there is no data to be used for the distillation process, and also when the teacher provides only the hard-labels (no soft predictions over the labels). IDEAL employs a generator to generate examples on-the-fly in this data-free scenario. The generator aims to balance the class distribution and fit to one of the classes while the distillation progresses. In this particular setting (no-data and hard-labels), authors claim IDEAL can reduce the number of examples to be annotated significantly and still can achieve better performance on the given task.",
            "strength_and_weaknesses": "**Strength**\n* Empirically strong in the particular distillation setup (no data, teacher can only provide hard-labels).\n* Utilizing the generator in the data-limited distillation setup has some novelty, particularly in the joint training of the generator during the distillation process.\n\n**Weakness**\n* The impact is limited on the very specific distillation setup (data-free & hard-label setting). Moreover, the algorithm can only be applicable for the classification set-up, and does not seem to scale up when there are a large set of labels (thousands or millions).\n* Authors seem to be over-claiming their impact. Particularly, I disagree with the author's claim that most KD assumes \u201cusers can directly access teacher\u2019s training data.\u201d Widely, KD focuses on a large set of unlabeled data because a teacher can provide pseudo-labels for them.\n* Writings can be improved to focus on their core motivation, core idea, and core techniques. Please see the \u201cclarity/quality section.\u201d Because of this reason, many of the design choices in Section 3 feel a bit arbitrary.\n* One of the baseline authors chosen, ZSDB3KD does seem to show 96.54% in their paper unlike the very low number in the current paper. Why is the number significantly different?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity/Quality**\n* Abstract and introduction are unclear how exactly this work differs from prior sample-efficient distillation works. Particularly, the abstract focuses mostly on problem settings, not on motivation of this work, nor high-level sketch of what they are trying to do (they just simply refer to it works with two stages (\u201cdata generation stage and queries the teacher only once ...\u201d).\n* Some terminologies are also misleading. For example, \u201cwhite-box\u201d and \u201cblack-box\u201d refers to whether we have prior knowledge of model internals such as function classes or main assumptions. It does not mean the model class produces only the hard-labels (top1) or not (soft-labels). Hence, many classic distillation setup treats teacher models as black-box models (such as the original softmax-based distillation in Hinton\u2019s paper [1] ).\n* Minor question: What are the teacher models used in Section 4?\n\n**Novelty**\nThere are mainly two lines of work that are closely related for this paper.\n* [2] is a very close work that is in the same problem formulation (zero-shot, black-box, and decision base) -- hence, the problem formulation is not novel unlike what authors claim in Section 1.\n* [3,4] proposes to use generators during the training and [4] particularly introduced during the distillation.\nThis paper is basically the combination of the two lines of work, and its novelty is in proposing the joint training of the generator that can maintain the class balance and the confidence, for the zero-shot setting. Hence, the paper does have some novelty.\n\n[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 2.7 (2015).\n[2] Wang, Zi. \"Zero-shot knowledge distillation from a decision-based black-box model.\" International Conference on Machine Learning. PMLR, 2021.\n[3] He, Xuanli, et al. \"Generate, annotate, and learn: Generative models advance self-training and knowledge distillation.\" (2021).\n[4] Zaheer, Manzil, et al. \"Teacher Guided Training: An Efficient Framework for Knowledge Transfer.\" arXiv preprint arXiv:2208.06825 (2022).",
            "summary_of_the_review": "As discussed above, the paper has some contributions to the community. However, the paper does seem to be limited in a very specific setting and lack the clarity/quality for the acceptance bar.\n\n---\n\nPost-rebuttal: After reading author's response as well as other reviewer's recommendation, I agree that the paper has some original contribution to the community (although it's limited). Hence, I updated the score accordingly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper216/Reviewer_Epxx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper216/Reviewer_Epxx"
        ]
    }
]