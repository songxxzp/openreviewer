[
    {
        "id": "cdqERELvCR",
        "original": null,
        "number": 1,
        "cdate": 1666634758636,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634758636,
        "tmdate": 1666634758636,
        "tddate": null,
        "forum": "WOfOf53mVyo",
        "replyto": "WOfOf53mVyo",
        "invitation": "ICLR.cc/2023/Conference/Paper1475/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a topic aware transformer (TAT) to adapt pretrained language models to target domains through topic modeling. The motivation is, to alleviate forgetting, TAT explicitly quantify the domain shift as topic shifts. By introducing a topic steering layer (TSL) on top of transformers, TAT decompose the task of predicting next tokens into two tasks: (i) Firstly, predict the distributions of topics given previous tokens. (ii) Secondly, shift the word distribution according to predicted topics and then predict the next token. Combining the TSL with a newly introduced topic distribution modeling (TDM) loss, which aims at aligning the topic prediction, the proposed TAT achieves better performance of unconditional generation on two datasets, comparing to DAPT+TAPT and prefix tuning.",
            "strength_and_weaknesses": "Strength:\n1. This proposed idea is novel, interesting and well-motivated.\n2. The resulted models can bring us more interpretability of the process of language modeling, by predicting topics before predicting next tokens.\n3. The evaluation on Yelp and Amazon datasets are comprehensive.\n\nWeakness:\n1. It seems that the proposed model can and should go beyond unconditional generation tasks. After introducing an extra TSL layer and learning meaningful topics, it seems unconditional generation tasks cannot fully leverage the potential of learned models. If we treat the learned topic distributions as learned interpretable latent variables, it should be easy and interesting to inject more topic control into the generation process.\n2. While \"alleviating catastrophic forgetting\" is one of the authors' motivations, discussion and comparison of tuning-free adaption is missing. For example, Plug and Play Language Models: A Simple Approach to Controlled Text Generation (https://arxiv.org/pdf/1912.02164.pdf), and FUDGE: Controlled Text Generation With Future Discriminators (https://aclanthology.org/2021.naacl-main.276.pdf). By such tuning-free controlled generation approach, we can also easily adapt PLM to target domains by changing the predicted word distributions.\n3. The presentation can be improved, such as the form of citation makes section 2 really hard to follow. The left part and the right part of figure 1 are also hard to understand.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper proposes a novel idea, with detailed introduction and good evaluation.\nThe selection of datasets and the future code release make it easy to reproduce.\nThere are some points can be further clarified:\n1. What is the catastrophic forgetting while adapting PLMs? The authors cite a lot of prior work without giving clear enough explanations.\n2. How to define \"global word distributions\" and \"target-specific word distributions\" in Sec 3.1? As mentioned in sec 3.2, there might not be very clean definitions, but showing examples can be better.\n3. Figure 1 left: what is \"large\" and \"small\" refer to?\n4. \"The topic vector is the average of zt over each i-th text with Eq (4)\" in sec 4.1. What does \"each i-th text\" mean? Each token or each input text? Writing an equation can help with such ambiguous wording.\n5. What is the motivation for the TDM loss? The authors say \"align topics with each text\", but what is the meaning of aligning the final predicted topics (TID) with previously predicted topics, since TID itself is not related to token generation.\n6. What is the dynamic of topic prediction during inference? Are there any shifting of topics? More discussion on this can be interesting.\n",
            "summary_of_the_review": "I think this paper proposed an interesting and novel idea, with good experiments.\nMy biggest concern is the missing of comparisons to previous work like PPLM and FUDGE, which makes it harder to judge the real contribution of this work.\nI also suggest there should be more tasks to play with the proposed model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1475/Reviewer_qrSx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1475/Reviewer_qrSx"
        ]
    },
    {
        "id": "rLJRxIF_PB",
        "original": null,
        "number": 2,
        "cdate": 1666642561061,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642561061,
        "tmdate": 1666675523495,
        "tddate": null,
        "forum": "WOfOf53mVyo",
        "replyto": "WOfOf53mVyo",
        "invitation": "ICLR.cc/2023/Conference/Paper1475/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new topic-aware transformer that alleviates the gap between the pretrained language model and the target domain. The paper introduces an additional topic latent variable $z$ as an additional condition when generating new tokens. The new topic latent variables come from the topic steering layer, which maps hidden representations into the topic vector. The model is optimized with both topic distribution modeling and maximum likelihood. The backbone of the paper is GPT2. The model is tested on two datasets: Amazon and Yelp. The paper checks the topic coherence and its text-generation ability  The paper did both automatic and human evaluation. The paper also did additional ablation analysis and case study. ",
            "strength_and_weaknesses": "Strength:\n1. The paper proposes a new topic-aware transformer to resolve the domain shift for unconditional text generation. The motivation of the paper is clearly stated. The TAT achieves the best performance for topic coherence and text generation. The whole idea is interesting.\n\n2. The paper conduct both automatic and human evaluation. The paper also did some ablation analysis to show the contribution of each component. The paper also presents some generation results as examples.\n\n\nWeaknesses:\n1. Some parts of the paper are not clearly written which will be discussed in the clarity section.\n\n2. Some topic-related papers are not covered in the related work such as Zhu et al., (2021), and Chang et al., 2021.\n\n3. The citation style is not correct. Authors need to use citep{} The paper uses large space to squeeze tables and captions, making readers hard to read. Authors should not abuse space.\n\n4. The evaluation metrics used in the paper are a little bit old and only concentrate on the token overlaps. The paper needs to include some newer metrics such as BERTscore (Zhang et al., 2019), and BARTScore (Yuan et al., 2021) which can check semantic similarity. \n\n\nZhu, L., Pergola, G., Gui, L., Zhou, D., & He, Y. (2021). Topic-driven and knowledge-aware transformer for dialogue emotion detection. arXiv preprint arXiv:2106.01071.\n\nChang, H. S., Yuan, J., Iyyer, M., & McCallum, A. (2021). Changing the mind of transformers for topically-controllable language generation. arXiv preprint arXiv:2103.15335.\n\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2019). Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\n\nYuan, W., Neubig, G., & Liu, P. (2021). Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34, 27263-27277.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Some parts of the equation are unclear. For example, in equation 5, where does $z$ come from? Although readers can later learn that z actually comes from $LayerNorm(h_{L,t})W_Z$, it would be better to split equation 4 into two formulas. Authors also need to rewrite equation 5, because the definition of $\\mathcal{F}$ is unclear. Which transformation does the paper finally used in the model? The human evaluation details are not clear. For example, the number of annotators in the human evaluation is unknown. The annotation guidelines should be attached to the appendix.\n\n2. The effectiveness of the TAT is verified both in the topic coherence and text generation.\n\n3. The paper provides some implementation details. However, the paper doesn't provide any code or dataset for reproduction.",
            "summary_of_the_review": "Overall, the proposed new model: the topic-aware transformer is quite interesting and follow s motivation. However, some parts of the paper are not very clear. The experiment section and paper format need to be improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1475/Reviewer_DzP1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1475/Reviewer_DzP1"
        ]
    },
    {
        "id": "kV66hHFAj4",
        "original": null,
        "number": 3,
        "cdate": 1666680372121,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680372121,
        "tmdate": 1666765205054,
        "tddate": null,
        "forum": "WOfOf53mVyo",
        "replyto": "WOfOf53mVyo",
        "invitation": "ICLR.cc/2023/Conference/Paper1475/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes to further fine-tune an unconditional language model on a domain-specific corpus via modeling the topics as a discrete latent variable. ",
            "strength_and_weaknesses": "Strengths:\n1. The general idea of modeling topics as latent variables for domain adaptive pre-training does make sense. \n2. The results look good.\n\nWeaknesses:\n1. The format of this paper has been totally changed. I think it is not allowed to change the template. The font looks weird. And those table layouts look very condense and hard to read.\n2. The notations of equations look very confusing. And the  descriptions of methods are also hard to follow. I will pose all my questions on the method section in the following section.\n3. Since I cannot fully follow the method, I have concerns on the effectiveness of the method.\n4. In the results section, those baselines have not been well described so it is impossible for me to judge whether the baselines are enough. Even the metric used for evaluating the topic coherence is not described in enough details so I do not know whether this metric is suitable for the task or not.",
            "clarity,_quality,_novelty_and_reproducibility": "Here are my questions on the method section:\n1. In equation 2, it seems t represents the index of tokens, which aligns with the normal notation for token position. However, in equation 5, it seems that t represent the index of topics. Why do you use the same symbol for two concepts?\n2. In equation 5, you have three kinds of F function for z>0, then which one do you use finally? \n3. Why do you need to describe the attention mechanism in section 3.4 in such details? \n4. In section 4.1, what do you mean by \"while the other text representation, TID is taken as a negative embedding\"?\n5. In equation 5, TID is the average of h, which are the representation vectors after L layers of transformer blocks, while z is the latent variable of topics. Then why do we want to minimize the distance between these two vectors?\n\nHere are my questions on the results section:\n1. In Table 1, the number of Z is set to 100 and 200 for the two datasets. How do you decide it?\n2. What are the details of those baselines used in section 5.2 and 5.3? Could you elaborate them instead of just giving references or links?\n3. What is the topic coherence measure?\n4. In table 3, whar is |z|?\n\nThere are too many abbreviations that look very similar in the paper, which makes it very hard for readers to understand and follow. ",
            "summary_of_the_review": "The method proposed may be good but the bad writing makes it hard for readers to discover its significance. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1475/Reviewer_cPS3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1475/Reviewer_cPS3"
        ]
    },
    {
        "id": "xDf0fyFD26V",
        "original": null,
        "number": 4,
        "cdate": 1666809814706,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666809814706,
        "tmdate": 1666810302061,
        "tddate": null,
        "forum": "WOfOf53mVyo",
        "replyto": "WOfOf53mVyo",
        "invitation": "ICLR.cc/2023/Conference/Paper1475/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates domain adaptation for an autoregressive language model.\nA latent topic distribution is calculated based on the input words, which is then used to modify the token probabilities during generation.\nEvaluation is performed on two review datasets (Amazon and Yelp).",
            "strength_and_weaknesses": "Strengths:\n* The of estimating the topic of the text first, then generating based on that is interesting and possibly promising.\n* The empirical results are good compared to the chosen baselines.\n\nWeaknesses:\n* The clarify of the paper is low, prohibiting understanding of how the method actually works.\n* Experiments are performed only on two datasets, both from the same domain. For convincing results regarding the domain adaptation method, more than one domain should be investigated during evaluation.",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe clarity of the paper is low. The mathematical notation seems to be used in an inconsistent way, leading to impossible equations. There are also many grammatical errors, making understanding even more difficult.\nFor example, Section 3.5 specifies H_L as a vector of vectors (a matrix) of variable length (|x|), yet its length is somehow given as d_h, a simple 1-dimensional vector.\nEq 4 seems to specify that the probability distribution over topics is multiplied together with a probability distribution over words, which should not be possible unless the topics somehow match the vocabulary.\n\nThe task in this paper is domain adaptation for generative (auto-regressive) language models. This is a very well researched are with numerous papers over many years. Yet only a couple of very recent papers are cited and compared against. In addition, some of the chosen baselines are not really designed for this task.\nNeither prefix-tuning nor DAPT+TAPT were designed or evaluated for language modelling itself, but they are methods for improving downstream performance on a supervised learning task (e.g. text classification).\n\nIn Section 3.4 it is mentioned that a bidirectional mask is used, implying that the model is allowed to look into the future. This should not be allowed in a language modelling task.\nSection 4.1 is also specifying that the topic vector is averaged over the whole text. It is not clear if this is also done during testing, which would mean that the model gets to see the inputs it needs to generate, again something that would not be allowed.\n\nThe first sentence of the abstract claims that the proposed method is making it possible to use language models for unconditional generation. That is what language models already do, without any adaptation. They are conditioned only on the previous context, but so is the proposed model, so the claim is confusing.\n\nPlease look into using citet vs citep. The current use of citation formatting is making it difficult to understand the sentence structure.\n",
            "summary_of_the_review": "Possibly interesting and useful ideas.\nThe clarity of the paper needs improvement and the evaluation should be against more relevant baselines and on more datasets.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1475/Reviewer_6wEU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1475/Reviewer_6wEU"
        ]
    }
]