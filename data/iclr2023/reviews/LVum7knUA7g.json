[
    {
        "id": "ct5mjCmmUvA",
        "original": null,
        "number": 1,
        "cdate": 1666561763610,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561763610,
        "tmdate": 1666561763610,
        "tddate": null,
        "forum": "LVum7knUA7g",
        "replyto": "LVum7knUA7g",
        "invitation": "ICLR.cc/2023/Conference/Paper2908/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies a new regularization term for deep reinforcement learning which is inspired by the energy term from quantum physics. In general, the regularization term aims to maximize the adjusted likelihood function over the existing trajectories, which is opposite to the optimization goal of policy gradient which aims to maximize the expected reward in the future. Experiment results suggest that such an approach can improve the trained policy with better value and smaller variances.",
            "strength_and_weaknesses": "Pros:\n- The presentation is easy to follow.\n- The experiment results are comprehensive.\n- The proposed method is easy to implement and has the potential to be applied to more tasks. \n\nCons:\n- Some demonstrations are hard to understand. For instance, In Sec 4.3, I do not understand the Bellman equation used in (a-c). Why there does not exist the discount factor $\\gamma$? If the authors want to discuss the case $\\gamma=1$, they should not simply adopt the original Bellman equation, but the revised one for infinite-horizon MDPs, like (5) in [1]. \n\n[1] Jaksch, Thomas, Ronald Ortner, and Peter Auer. \"Near-optimal Regret Bounds for Reinforcement Learning.\" Journal of Machine Learning Research 11 (2010): 1563-1600.\n\n- Potential drawbacks of the proposed algorithm are not discussed. My understanding about why the proposed regularization term can help the policy have a smaller variance and better value is that it sacrifices the ability to explore. The regularization term $H$ forces the policy to follow the existing samples, especially the samples in the early steps $h\\ll H$. Although such a strategy may work well on Mujoco dataset where the exploration is not crucial, I doubt such a strategy will work well on some harder instances, such as the combination lock example in [2]. \n\n[2] Agarwal, Alekh, et al. \"Pc-pg: Policy cover directed exploration for provable policy gradient learning.\" Advances in neural information processing systems 33 (2020): 13399-13412.",
            "clarity,_quality,_novelty_and_reproducibility": "This work studies a new regularization term for deep reinforcement learning. It is different from the existing regularization term, for instance, the entropy of the policy, which aims to enhance the exploration ability of the policy. It seems that the authors' strategy works well on several benchmarks such as MuJOCO. Therefore, I think this work is novel. ",
            "summary_of_the_review": "This work proposes a new regularization term for policy optimization on deep reinforcement learning which seems a better choice than existing approaches on Mujoco dataset. However, I feel the demonstration of several examples in this paper is hard to understand, and the discussion of the limit of the proposed method is not well-discussed. Therefore, I recommend a reject to this paper. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2908/Reviewer_947x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2908/Reviewer_947x"
        ]
    },
    {
        "id": "4evI2MJ89Km",
        "original": null,
        "number": 2,
        "cdate": 1666629612874,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629612874,
        "tmdate": 1666647136158,
        "tddate": null,
        "forum": "LVum7knUA7g",
        "replyto": "LVum7knUA7g",
        "invitation": "ICLR.cc/2023/Conference/Paper2908/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new policy gradient method, which can potentially reduce the variance of the policy gradient estimator.",
            "strength_and_weaknesses": "### Weaknesses:\n* I feel the motivation for the paper is not so clear. I don\u2019t see any superiority to reinterpret the reinforcement learning problem in this K-spin Ising Model framework. It only covers a very small subset of the Markov Decision Process problem (e.g. the optimal action should be on a lattice), and there are no quantum related things here.\n* I think the authors do not have a clear understanding of statistical mechanics. At least, the term \u201cHamiltonian equations\u201d is created by the authors themselves. We only have Hamilton\u2019s equations of motions (or Hamiltonian mechanics). In fact, the authors only use the Hamiltonian of the K-spin Ising model. I did expect there are some more motivations from statistical mechanics, e.g. we need to develop specific update rules to reserve the Hamiltonian for certain purposes. However, if I understand correctly, what the authors do do not have any relations with this K-spin Ising Model framework.\n* This new policy gradient estimator seems closely related to other estimators, as $\\nabla_{\\theta} \\log (\\Pi_i \\pi_{\\theta}(a_i |s_i)) = \\sum_i \\nabla_{\\theta} \\log \\pi_{\\theta} (a_i|s_i)$. I even suspect by this decomposition the proposed policy gradient estimator is identical to the original one and the only difference between the newly proposed estimator and the original policy gradient estimator is how to estimate the cumulative reward (by the critic or by roll-out). The authors should provide more discussions on this.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall poorly written and hard to follow. Meanwhile, I believe the proposed method is not so novel.",
            "summary_of_the_review": "I don\u2019t think the newly proposed policy gradient estimator has any connections to the K-spin Ising Model. Meanwhile, I suspect the newly proposed policy gradient estimator is identical to the standard one and the only difference is in the empirical estimation of the cumulative reward.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2908/Reviewer_arYu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2908/Reviewer_arYu"
        ]
    },
    {
        "id": "X-QDXwoK6Y",
        "original": null,
        "number": 3,
        "cdate": 1667427949011,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667427949011,
        "tmdate": 1667427949011,
        "tddate": null,
        "forum": "LVum7knUA7g",
        "replyto": "LVum7knUA7g",
        "invitation": "ICLR.cc/2023/Conference/Paper2908/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a regularization term (namely H-term) for stabilizing the training of DRL. The inspiration of the H-terms comes from the K-spin Ising model with energy minimization principle. Experiments on PPO and DDPG for MuJoCo show variance reduction of the proposed method.",
            "strength_and_weaknesses": "Strength:\n\nStabilizing the training of DRL is an important topic. The idea from Ising model for minimizing the energy in policy optimization is novel. \n\nWeakness:\n\nThe relationship of the Hamiltonian policy gradient (HPG) and the REINFORCE policy gradient (RPG) is less clear. From the method description it looks like the HPG is an alternative format for estimating the gradient of objective in RPG, however in the end the H-term is only a regularizer.\n\nSince the variance estimation is important in the paper, I would also suggest to report the results with stratified bootstrap confidence intervals [1].\n\nThe proof of variance reduction using H-term is not convincing. In appendix F.2 the paper gives a theoretical justification of the variance reduction optimizing the policy with an additional H-term. However, it looks like the proof is merely about showing the strong correlation of $H$ and $J$ thus reducing the total variance when $\\nabla_J(\\theta)$ is subtracted by $\\lambda \\nabla H(\\theta)$ with a small $\\lambda$. If this is the case, simply letting $H=J$ with $\\lambda<1$ can reduce the variance, but with no help to the policy gradient optimization. \n\nK-step truncation is less clear. In the paragraph in Sec. 4.2 about truncation of K, what is the truncation error about and what is $\\epsilon$ here, and why does it simply want $\\gamma^K\\ge \\epsilon$?\n\nTypos:\nLine 631 in appendix should refer to `Section 4.2`\nParagraph Physical interpretation in Section 4.2, \u2018monte carlo simulation\u2019.\n\n\n[1] Agarwal, Rishabh, et al. \"Deep reinforcement learning at the edge of the statistical precipice.\" Advances in neural information processing systems 34 (2021): 29304-29320.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing and notations of the paper need to be heavily polished. There are lots of misleading notations in the current version. For example, in Eq. (3) $H(\\theta)$ the $\\theta$ is parameters of the policy, while in the above paragraph the argument of $H$ is the chain of policies, and later in Sec. 4.3 the arguments of $H$ become the state indices. In Sec. 5.1 computational complexity, does the \u2018computations\u2019 here actually mean the gradient computation through the whole network in the backpropagation? It\u2019s very vague to say the number of computations. \n\nIn Section 6.1 the performance metrics, the \u2018Optimal\u2019 and \u2018obj\u2019 are not explained.\n\nThe idea is novel.\n\nThe code is given and should be reproducible.\n",
            "summary_of_the_review": "The paper investigates an interesting problem about stabilizing DRL training. But the proposed H-term is not well justified from a theoretical perspective. The performance improvement of different MuJoCo tasks seem to be marginal. Experiments testify the reduced variances of policy performance but not policy gradient estimation. I would also suggest showing clear evidence of variance reduction of the gradients on small games with the ground truth gradient available.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2908/Reviewer_6uJh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2908/Reviewer_6uJh"
        ]
    }
]