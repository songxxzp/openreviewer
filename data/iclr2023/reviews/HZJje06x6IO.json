[
    {
        "id": "fRHrsYK3D_",
        "original": null,
        "number": 1,
        "cdate": 1666243101206,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666243101206,
        "tmdate": 1668734681466,
        "tddate": null,
        "forum": "HZJje06x6IO",
        "replyto": "HZJje06x6IO",
        "invitation": "ICLR.cc/2023/Conference/Paper1237/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a novel global context vision transformer built with a simple combination of global context self-attention modules and local self-attention modules to model\nboth long-range and short-range interactions. Besides, the authors propose to model the inter-channel dependencies with an inverted residual block.\nThe proposed GC ViT shows encouraging results across image classification, object detection, instance segmentation, and semantic segmentation tasks.\n",
            "strength_and_weaknesses": "> Strengths\n\n\u2705 The presented idea is very simple and easy to follow.\n\n\u2705 The overall writing is of high quality, such as Figure 2, Figure 3, Figure 4, Figure 5, and Figure 6 is all of GOOD quality and help the readers to understand the key\nidea easily.\n\n\u2705 The authors provide rich details and conduct rich experiments to verify the effectiveness of the proposed approach.\n\n> Weaknesses\n\n\u274e The experimental results are relatively WEAK and the authors are encouraged to verify the advantages over Swin-T based on the very recent object detection and segmentation frameworks\nsuch as DINO-DETR[1], H-DETR[2], and Mask2Former[3]. For example, Mask2Former+Swin-B (pre-trained on ImageNet1K) achieves PQ=55.1, AP=46.7, and mIoU=52.4 on COCO panoptic segmentation, COCO instance\nsegmentation, and ADE20K semantic segmentation tasks respectively. I would like to expect the authors to present stronger results and release the related code upon acceptance.\n\n[1] https://github.com/IDEA-Research/DINO\n\n[2] https://github.com/HDETR/H-Deformable-DETR \n\n[3] https://github.com/facebookresearch/Mask2Former \n\n\u274e According to Table 1 & Table 2, the proposed GC Vit backbones require more parameters and GFLOPs than the previous approaches such as CSWin and ConvNeXt. For example, GC ViT-B requires 1018 GFLOPs vs. ConvNeXt-B requires 964 GFLOPs and GC ViT-B only gains 0.2 measured by AP^box.\n\n\u274e The authors are encouraged to verify the advantages of the proposed backbone under larger model scales. Specifically, the authors should construct a GC ViT-L and GC ViT-H instead of only comparing the models under moderate scales as scaling up is one of the most important aspects of an important backbone design.",
            "clarity,_quality,_novelty_and_reproducibility": "> Clarity\n\nGood.\n\n> Quality\n\nGood.\n\n> Novelty\n\nGood.\n\n> Reproducibility\n\nNo code is available.",
            "summary_of_the_review": "Designing new versatile architecture is always very important for the whole computer vision community. The overall quality is OK but the results are relatively WEAK to support the claim that the proposed method really outperforms the previous SoTA methods such as Swin-Transformer.\nPlease carefully address the above-listed weaknesses. I will increase the ratings if the authors could well address these concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No further concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1237/Reviewer_V9pP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1237/Reviewer_V9pP"
        ]
    },
    {
        "id": "QixU8ZHmRkj",
        "original": null,
        "number": 2,
        "cdate": 1666727221804,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727221804,
        "tmdate": 1666727221804,
        "tddate": null,
        "forum": "HZJje06x6IO",
        "replyto": "HZJje06x6IO",
        "invitation": "ICLR.cc/2023/Conference/Paper1237/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduced GC ViT that can efficiently capture global context by utilizing global query tokens and interact with local regions. \nThis paper demonstrate through experiments that this simple modification on traditional ViTs could get SOTA results for image classification on ImageNet-1K dataset and downstream tasks.",
            "strength_and_weaknesses": "Strength:\n1. the idea is simple and straightforward, from the experiment it seems the improvement is reasonable.\n\nWeakness:\n1. The comparison of GC-VIT to other models clearly shows the quantitative improvements in Tab.1. In addition, the ablation study shows the proposed module could be quite plug-able into existing models. It will be more appealing to see how GC module improves existing models by simply plug them into them. This will inevitably add #param  and FLOPS, but it gives more intuitive evidence on the effectiveness. ",
            "clarity,_quality,_novelty_and_reproducibility": "There are no problems with Clarify, Quality and Novelty. \nHowever, since the paper is based on plain empirical/experimental findings. The released code will be critical in verifying the reproducibility of the paper. ",
            "summary_of_the_review": "Overall this paper proposed a simple and yet effective module that could add reasonable improvement to current ViT. This work could be useful to the community once the code is released for verification/comparison.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1237/Reviewer_Fuf9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1237/Reviewer_Fuf9"
        ]
    },
    {
        "id": "RD-aVJwq0HU",
        "original": null,
        "number": 3,
        "cdate": 1666848948699,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666848948699,
        "tmdate": 1669296642387,
        "tddate": null,
        "forum": "HZJje06x6IO",
        "replyto": "HZJje06x6IO",
        "invitation": "ICLR.cc/2023/Conference/Paper1237/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposed a new hirerchical transformers architecture. The paper revisit the attention mechanism and the downsampling layers.\nThe claim of the paper are the following: \n\n\u2022 We introduce a compute and parameter-optimized hierarchical ViT with reparametrization of the\ndesign space (e.g., embedding dimension, number of heads, MLP ratio).\n\n\u2022 We design an efficient CNN-like token generator that encodes spatial features at different resolutions\nfor global query representations.\n\n\u2022 We propose global query tokens that can effectively capture contextual information in an efficient\nmanner and model both local and global interactions.\n\n\u2022 We introduce a parameter-efficient downsampling module with modified Fused MB-Conv blocks\nthat not only integrates inductive bias but also enables the modeling of inter-channel dependencies.\n\n\u2022 We demonstrate new SOTA benchmarks for : (1) ImageNet classification with Pareto fronts on\nImageNet-1K for model size and FLOPs (see Fig. 1), and (2) downstream tasks such as detection,\ninstance segmentation and semantic segmentation on MS COCO and ADE20K, respectively.\n",
            "strength_and_weaknesses": "Strength\n- The paper is well written and easy to follow\n- There is evaluation on different tasks\n\nWeaknesses:\n- Missing comparaison and discussion:\n\na) The global query tokens claim: The proposed global query token behaviour seem to be quite similar to the attention pattern mechanism in the BigBird paper[1] and also the local global attention in the paper EdgeViT[2]. It is necessary to discuss the contribution of the proposed method to these two papers. Because at present the contribution of the paper on this aspect is not clear.\n\n[1] Zaheer et al., Big Bird: Transformers for Longer Sequences\n[2] Pan et al., EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers\n\nb) The compute and parameter-optimized hierarchical ViT and SOTA claim: in Figure 1 and Table 1: Indeed, the EdgeViT paper exploit similar ideas (local-global attention, Hierachical architecture) and seem to have better FLOPs-accuracy and Parameter accuracy trade-off (EdgeViT-S: 11.1M params, 1.9G FLOPs, 81.0 top-1 on ImageNet vs GC ViT-XXT 12M params, 2.1G FLOPs, 79.8 top-1 on ImageNet)  It is therefore necessary to add this architecture to the comparison. Moreover, the paper mention EfficientNetV2[1] because GC ViT exploit some similar blocks and seem also better/similar for the FLOPs-accuracy and parameter accuracy trade-off (EfficientNetV2-M 54M params, 24G flops, 85.1 top-1 on ImageNet  vs GC ViT-L  201M params, 32.6 G FLOPs, 84.6 top-1 ImageNet). So it's important to add also EfficinetNet-v2 in the comparaison.\n\nGiven these elements it is probably best to down tone the state-of-the art claims in the paper. Because it is not clear which state of the art it is.\n\n[1] Tan et al., EfficientNetV2: Smaller Models and Faster Training\n\nb) The efficient CNN-like token generator: Many papers have proposed convolution blocks for token generation like LeViT[1] or XCiT[2].\nThere is no discussion or comparison of performance with the previous approach. It is therefore impossible to evaluate the contribution of the paper on this point.\n\n[1] Graham et al., LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n[2] El-Nouby et al., XCiT: Cross-Covariance Image Transformers\n\n- Overfitting evaluation: For the ImageNet dataset there is no separate test and validation set so it's important to evaluate the level of overfitting by doing evaluation of the models on ImageNet-v2[1].\n\n[1] Recht et al., are we done with ImageNet?\n\n- Missing metrics: There is only params and FLOPs in the different table. It's important to measure other trade-off like Latency and memory consumption. Indeed, the proposed architecture use MBconv which are known to be good on trade-off FLOPs accuracy and parameter accuracy.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. Unfortunately, the claims did not seem to be supported and the novelty seems very marginal. The paper seems to be reproducible",
            "summary_of_the_review": "The idea of the paper is interesting but unfortunately the contribution does not seem to be justified experimentally. The paper seems to be incremental. There is a lack of discussion and comparison with very similar approaches in the literature.\n\n\n====== Post rebuttal ====\n\nThe rebuttal raised a very important concern: \n\nThe authors report maximum accuracy during training by evaluating at each epoch in order to maximise performance on the ImageNet-1k validation set (there is no separate test set for ImageNet). Although the authors argue that it is a common practice, there is no evidence in the mentioned papers that this is the approach used. Furthermore I consider this approach to be detrimental to the community as it puts scientific understanding behind the optimal performance.\n\nOn the 3 logs provided the impact of this practice is important on the log with the bigger model +0.2% and impact the conclusion. The authors no longer have the other logs of their experiments, especially those of the ablation, so it is impossible to know the impact of the method. There is no multi-seed evaluation that shows that 0.1% on ImageNet-v2 is significant although it is claimed in the rebuttal.\n\nIn the current state the experiments are mainly designed to optimise the performance of the GC ViT architecture at the expense of an understanding of the impact of each component of the proposed method.\n\nSo I will lower my score.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1237/Reviewer_t1Sn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1237/Reviewer_t1Sn"
        ]
    },
    {
        "id": "1OtJVS6_8I",
        "original": null,
        "number": 4,
        "cdate": 1666864541672,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666864541672,
        "tmdate": 1666864541672,
        "tddate": null,
        "forum": "HZJje06x6IO",
        "replyto": "HZJje06x6IO",
        "invitation": "ICLR.cc/2023/Conference/Paper1237/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a novel module to combine the local and global context. The key of this method is to introduce global query tokens into the local context module. The advantage of this method is that it can merge the context information from short and long-term ranges. The authors do experiments for image classification, detection, and segmentation tasks to show the effectiveness of the method.",
            "strength_and_weaknesses": "Strength\n1. The experiments show this method is effective and can be generalized to many different tasks.\n2. The problem is essential for network architecture design and the method is straightforward.\n3. The visualization results show the effectiveness of this method.\n\nWeakness\n1. The combination of local and global context is a long-standing problem and has been explored by many methods. The method for merging local and global contexts lacks novelty.\n2. The improvement between the proposed method and other SOTA methods is minor.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to follow. The novelty of this paper is somehow limited. I think researchers can reproduce this paper easily.",
            "summary_of_the_review": "Please follow the previous parts.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1237/Reviewer_YBSt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1237/Reviewer_YBSt"
        ]
    }
]