[
    {
        "id": "DBMui454HP",
        "original": null,
        "number": 1,
        "cdate": 1666629446624,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629446624,
        "tmdate": 1666629446624,
        "tddate": null,
        "forum": "nMwFhKoAo5v",
        "replyto": "nMwFhKoAo5v",
        "invitation": "ICLR.cc/2023/Conference/Paper1529/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The current research piece introduces a dual contrastive learning objective for training GPT-like (or Causal language ) models (CLM). The main motivation comes from the finding shown in previous work (Ethayarajh, 2019) where the representations in upper layers of CLMs  show a particular clustering (the authors called it anisotropy) that limits the capacity of such representations. Authors also hypothesize that text generation might benefit if such anisotropy is reduced.\nThe Model, ContraGen, is trained using two contrastive objectives, one token based and one whole sequence based. Two models using each individual strategy are also included for comparison purposes. ContraGen-Tok and ContraGen-Seq\nThe authors put special emphasis on comparing this approach with SimCTG, a similar approach to ContraGen-Tok with the addition of a temperature in the contrastive loss.\nExperimentation is carried out in many aspects, particularly on tasks where semantic discrimation is needed: for natural language, semantic textual similarity (STS) and text generation, and for code, Code Search and Code Completion.\nBenchmarks on STS and Code Search show a clear advantage of the current approach against baseline (GPT/CodeGEN), the model furter trained with CLM, and SimCTG.\nOn the Generation tasks, the current approach cannot surpass CLM in terms of perplexity on natural language but improves over Pass rate on code tasks\n",
            "strength_and_weaknesses": "The paper is quite ambitious but is really easy to read, clear and concise. \nA vast analysis is included in the Appendix that helps the overall understanding of the approach.\nThe improvements in terms of Semantic similarity and Code search are significant.\n\nOn the weaknesses the two main issues (discussed below) are the justification of the motivation and the results that support it.\nAuthors argue that the anisotropy is the source of limited performance on generation models but reducing that aspect didn\u2019t bring improvement in terms of PPL for natural language, and a limited improvement in terms of code generation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some comments to review \n\n_Abstract The claim \u201dthe expressiveness of its representations is severely limited by the anisotropy issue where the hidden representations are distributed into a narrow cone in the vector space\u201d stated in the abstract should be addressed later in the paper \n\n_ Introduction. The claim \u201cthe resulting anisotropic representations fail to provide the desired semantic discrimination that is crucial for language understanding tasks, e.g., Semantic Textual Similarity\u201d is arguable. The best existing models for semantic text similarity are currently all based on attention-based models such as transformers.  \n\n_ Related Work: \u201c.. the anisotropy issue where the representations are [..] Such degeneration is undesired, especially considering expressiveness capacity\none would expect on modern language models \u201d. Ok from the first comment. But stating that the degeneration is not desired and it might hurt expressiveness it\u2019s not the same as stating that the degeneration \u201cseverely\u201d limits it.\n\n_ Related Work: \u201cHowever, they suffermost on the discriminative tasks such as code search or clone detection (Lu et al., 2021; Huang et al., 2021; Guo et al., 2022) due to the anisotropy issue (Ethayarajh, 2019)\u201d. First of all, none of the papers analyze the limitations of their performance due to the anisotropy issue. Second, Lu et al., 2021 (aka, CodeXGlue paper) only included Bert-based and GPT-based models for baselines purposes. CodeGPT was not even tested on clone detection nor code search. Third, Guo et al., 2022 presented an encoder- decoder model which surpassed other previous encoder only models. \n\n_ Section 4.3.1 Code Search:  Why the authors choose CodeNet challenge and not CodeXGlue (Lu et al., 2021). More models could have been used to locate where the performance of the ContraGen model.\n",
            "summary_of_the_review": "Judging the paper from its conclusion \u201cwe present ContraGen, an effective contrastive learning framework for language generation  ..to resolve the representation degeneration issue\u201d I think the authors did a good job in solving the latter aspect but I fail to bring enough evidence to prove the former. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_Rmep"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_Rmep"
        ]
    },
    {
        "id": "XvOI9e3yue",
        "original": null,
        "number": 2,
        "cdate": 1666647918995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647918995,
        "tmdate": 1666647918995,
        "tddate": null,
        "forum": "nMwFhKoAo5v",
        "replyto": "nMwFhKoAo5v",
        "invitation": "ICLR.cc/2023/Conference/Paper1529/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To address the problem of the anisotropy issue (i.e., hidden representations of transformer models are squeezed into a tiny cone), this paper proposes to use contrastive learning for language generation (henceforth, ContraGen). The model was evaluated on both text generation and code generation. It was also assessed on tasks like semantic textual similarity and code to code search.",
            "strength_and_weaknesses": "Contrastive learning is quite a natural idea for targeting the anisotropy issue. The author has applied the model to various NLP tasks to assess its validity. \n\nThis paper could be a good paper about language modelling, but what makes me uncomfortable is that this paper repeatedly says it is about language generation. Nevertheless, except for the code generation experiment that appears at the very end of this paper, this paper does nothing related to language generation. This results in many errors:\n1. Semantic textual similarity and code search are not downstream tasks of language generation\n2. How the PPL was used in 4.2.2 is actually for evaluating language models\n3. More justification is needed for why ISim, ESim and Disc are evaluating generation quality",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is novel but the clarity needs further improvement.\n\nMore specifically, the model description section is a bit hard to follow. For example, what does h^{j+} mean?",
            "summary_of_the_review": "This paper could be a good paper about language modelling, but it is nearly nothing about language generation. Since this paper does something that is inconsistent with what it claims, I cannot recommend an acceptance. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_9pG3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_9pG3"
        ]
    },
    {
        "id": "feGKblVJPk",
        "original": null,
        "number": 3,
        "cdate": 1666666353364,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666353364,
        "tmdate": 1666666353364,
        "tddate": null,
        "forum": "nMwFhKoAo5v",
        "replyto": "nMwFhKoAo5v",
        "invitation": "ICLR.cc/2023/Conference/Paper1529/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "One weakness of current language models, especially decoder-only ones, is in lack of expressiveness of representations due to the CLM objective it is retrained on, which poses an issue when used for tasks such as STS or retrieval. The authors suggest augmenting this loss with contrastive loss at both a sequence and token level to mitigate this. Additionally, they suggest three metrics that measures representation quality, and show that their training regime improves upon the CLM only baseline for both code search and STS, while also improving upon source code generation.",
            "strength_and_weaknesses": "This work is a step towards making decoder-only models more suited for all tasks. This paper clearly outlines the motivation for improving model representations, defines metrics to do so, and contributes a training regime to improve representation learning. ",
            "clarity,_quality,_novelty_and_reproducibility": "I am not very familiar with representation learning or code generation/search research workstreams, thus cannot speak to the novelty of the paper. It seems very clear and of high quality due to the ablations that the authors made to explore some anomalies in results (i.e. ContraGen-Seq doing worse than ContraGen-Tok in STS-B). ",
            "summary_of_the_review": "To the reader who is somewhat familiar with broader work in PLMs but not specifically in representation learning or code generation/search, this seems like a solid paper with a well-outlined motivation, methodology that makes sense, and results that back up their claims. This work is a step towards making decoder-only models more suited for all tasks (including ones that rely more on representations such as information retrieval), which, if the methodology is novel, would be worth putting in front of researchers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_Jhu9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_Jhu9"
        ]
    },
    {
        "id": "B-RgLvH25y",
        "original": null,
        "number": 4,
        "cdate": 1667538838964,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667538838964,
        "tmdate": 1667539898248,
        "tddate": null,
        "forum": "nMwFhKoAo5v",
        "replyto": "nMwFhKoAo5v",
        "invitation": "ICLR.cc/2023/Conference/Paper1529/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new contrastive learning algorithm to deal with the anisotropy issues of large-scale language model representation. The authors present three variants of their algorithm: CONTRAGEN, CONTRAGEN-Tok, and CONTRAGEN-SEQ, where each variant either includes token loss, sequence loss, or both.",
            "strength_and_weaknesses": "Strength:\n- The contrastive loss is very intuitive and builds on prior work.\n- The experimental setup and very thorough, and most decisions are justified. \n- Experiments are run across different domains (natural language and programming language) and models (GPT2 and CodeGen) to show the generality of the proposed algorithm.\n- Ablations studies are strategically done to address concerns regarding surprising results (e.g., CONTRAGEN-SEQ performing worse than CONTRAGEN-Tok)\n\nWeakness:\n- Most semantic textual similarity models are Bert-based models. Although this paper focuses on CLM, comparing this algorithm to algorithms that use those models seems more relevant.\n- Text generation results lack evaluation metrics: BLEU, BLEURT\tBERTScore, Distinct1, Distinct2, rep-2, rep-3, rep-4, diversity, MUAVE,  etc. It is unclear how the proposed metrics compare with the standard generation metrics.\n- Natural Language results are missing a baseline algorithm, Unlikelihood training. The Natural language section introduces new metrics, but it is unclear how non-contrastive ideas perform with respect to the new metrics.\n- Lack of justification of new metrics for evaluating generation quality.\n- The motivation of the paper is not entirely empirically supported.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written overall, and I understood most of the claims and results. The quality is the paper is very good. The authors do not include their code, so verifying the method's reproducibility is hard.",
            "summary_of_the_review": "The paper is well-written and straightforward to follow. The ideas in this paper are novel, and the authors empirically justify their claims. However, there are some concerns around the experiment setup, as a list in the weakness part. Because of these concerns, my initial score is borderline accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_6wZm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_6wZm"
        ]
    },
    {
        "id": "pbdXLE2tvkS",
        "original": null,
        "number": 5,
        "cdate": 1667854220722,
        "mdate": 1667854220722,
        "ddate": null,
        "tcdate": 1667854220722,
        "tmdate": 1667854220722,
        "tddate": null,
        "forum": "nMwFhKoAo5v",
        "replyto": "nMwFhKoAo5v",
        "invitation": "ICLR.cc/2023/Conference/Paper1529/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates adding contrastive learning objectives to LM to combat the anisotropy issue, under the assumption that increasing representation discrimination would lead to better LM. It shows a positive impact in similarity based tasks and mixed/unclear impact on LM quality.\n",
            "strength_and_weaknesses": "Strength\nIt introduces contrastive learning objectives into LM and shows it benefits similarity based tasks.\n\nWeakness\nThe experiment result cast some doubt on the experiment design and modeling assumptions.\n\n- Each new objective shows mixed results in code experiment compared to CLM, and only when combining both objectives does it outperform CLM. It casts doubt on the assumption that reducing anisotropy improves LM quality.\n- Natural language experiment (sec 4.2) should include some type of automatic metric (e.g. MAUVE) to evaluate the generation quality.\n- Given the performance gap between GPT-style and BERT-style models on similarity-based tasks, shouldn\u2019t the paper include experiments with BERT-style models?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and it could be reproduced with some effort.\n\n- What is the x-axis of fig-1? Why not use a bar chart if the x-axis imply a different model?\n",
            "summary_of_the_review": "The experiment design can be improved and some findings cast doubt on the modeling assumptions.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_PcBG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1529/Reviewer_PcBG"
        ]
    }
]