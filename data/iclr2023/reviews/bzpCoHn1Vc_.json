[
    {
        "id": "da9WUaj1jEU",
        "original": null,
        "number": 1,
        "cdate": 1666799133761,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799133761,
        "tmdate": 1669376129871,
        "tddate": null,
        "forum": "bzpCoHn1Vc_",
        "replyto": "bzpCoHn1Vc_",
        "invitation": "ICLR.cc/2023/Conference/Paper2077/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the convergence of the final iterate of (projected) SGD for (strongly) convex Lipschitz problems defined on bounded domain with almost surely bounded noise in the stochastic subgradients. In this setup, the authors derive the lower bounds for the last iterate of the projected sub-gradient descent that are valid for $T \\geq d$, where $T$ is the number of iterations and $d$ is the dimensionality of the problem. The key idea behind proposed worst-case example is using the oracle that returns zero subgradient during the first $T-d$ iterations (the algorithm starts at the solution) and then the oracle returns \"bad\" subgradients that drift the method away from the solution. The derived lower bounds are valid for $1/\\sqrt{T}$ and $1/\\sqrt{t}$ stepsizes in the convex case and for $1/t$ stepsizes in the strongly convex case ($t$ is the iteration counter). Some upper bounds for SGD are obtained for $1$-dimensional problems that are \"nearly linear\".",
            "strength_and_weaknesses": "## Strengths\n\n1. **Lower bound for the final iterate and fixed dimension.** This result is new for the literature and highlights the differences between the last-iterate and averaged-iterate convergence guarantees. It does not directly follow from the existing approaches for constructing lower bounds.\n\n2. **Several classical stepsize schedules are considered.** The considered stepsize policies are among the most popular ones. This fact additionally justifies the importance of the derived results.\n\n\n## Weaknesses\n\n1. **Proofs contain a lot inaccuracies and missing parts.** I cannot verify the proof of Theorem 15, because it contains a number of inaccuracies and unexplained derivations. In the list of detailed comments, I provide all inaccuracies that I noticed. The authors should make the proofs much clearer according to my comments.\n\n2. **Bounded domain.** The bounded domain assumption is quite restrictive. I have not found the place in the proof of the lower bounds where the authors rely on this (except for the strongly case, since without this assumption the class of the considered functions is empty). So, at least for Theorem 4 and Corollary 8 the authors can remove this assumption, if I am not missing anything. However, in the proof of the upper bound the authors do rely on the boundedness of the domain. This is a significant limitation.\n\n3. **Bounded stochastic subgradients.** The third part of Assumption 1 is very restrictive, since it means that the noise is bounded. Is it possible to remove this assumption to get a similar upper bound?\n\n## Detailed questions and comments\n\n1. I think the paper will benefit from more in-depth comparison with the results from Jain et al. (2019). Although they have non-standard stepsizes, it will be good to provide them in this paper and discuss the differences with the standard ones.\n\n2. The construction used in the proof of the lower bound significantly relies on the fact that the oracle acts in an adversarial manner. This is possible because of the non-smoothness of the problem. However, in many real problems the set of points where the objective is non-smooth has zeroth Lebesgue measure and the methods rarely occur in these points (especially if we consider stochastic methods). Moreover, the choice of the subgradient can be non-adversarial (e.g., one can try to choose the subgradient somehow randomly to avoid bad behavior). That is, it would be interesting to derive some lower bounds in less adversarial setups than in the one considered in this paper.\n\n3. Page 5, the derivation after \"For the other case $T^\\ast + 1 \\leq i < t$\": I do not understand the second equality. Could the authors provided a detailed derivation? If I am not mistake, $h_{i - T^\\ast, j} = 0$ for all $j > i - T^\\ast$ and $h_{i - T^\\ast, i - T^\\ast} = - \\frac{1}{2}$. I do not see how it is reflected in the derivation.\n\n4. The discussion after Remark 7: one should emphasize that this lower bound is valid even for GD.\n\n5. Proof of Theorem 15, the derivation after \"One has\": the first equality is not correct since the events are not independent. One should replace \"$=$\" with \"$\\leq$\".\n\n6. Proof of Theorem 15, \"Without loss of generality\": why the generality is not lost? I think it is possible to have $x_j > n_R$ and $x_{j+1} < n_L$ (due to the stochasticity). How the proof will work in this case?\n\n7. Proof of Theorem 15, \"for some constant $c = \\Theta(1)$\": this should be specified formally in the theorem.\n\n8. Proof of Theorem 15, the upper bound for $W_{(i, T+1]}$: I do not understand how it is derived. Could the authors provided the detailed derivation? In particular, it is unclear why the absolute value is inside $|\\cdot|$.\n\n9. Proof of Theorem 15, the upper bound for $\\ell$: I do not see how this upper bound is obtained. The authors should provide a detailed derivation.\n\n10. The last step in formula (8) is unclear. The authors should provide a detailed derivation.\n\n11. Page 9, \"Applying Freedman's Inequality (Theorem 12)\": Theorem 12 does not provide such inequality. The authors should provide the details on how Theorem 12 is applied here. Because of these inaccuracies, I did not check the next part going after \"And further\". The authors should proofread it as well.\n\n12. Formula (10): why the integration is taken from $0$ to $GD$? Why this is $O(\\lambda)$? The authors should provide a detailed derivation.\n\n13. Appendix C.1: there is a mistake in the definition of sub-gradient.\n\n14. Proof of Lemma 14, \"Without loss of generality\": similar question to 6.\n\n15. Proof of Lemma 14, the derivation after \"As a result\": in then end one should have $\\mathbb{E}[|y_i| \\mid \\mathcal{F}_i]$.\n\n16. Proof of Lemma 14, lower bound for $\\ell$: how is it derived? The authors should provide a detailed derivation.\n\n17. Proof of Lemma 14, last formula, second inequality: why does it hold? The authors should provide a detailed derivation.\n\n18. Proof of Lemma 14, last formula, last inequality: it is not a direct application of Freedman's inequality. The authors should provide a detailed derivation.\n\n## Minor comments\n\n1. Paragraph after the proof of Claim 5: \"supported on its first $t - T^\\ast$\" --> \"supported on its first $t - T^\\ast - 1$\"\n\n2. Lemma 6: \"algorithm 1\" --> \"Algorithm 1\" (and everywhere else)\n\n3. The end of Page 5, the third row of the derivation for $y_{t+1,j}$, the first case: one should not have $-1$ in the numerator.\n\n4. Definition of $S$: I suggest to change the notation to $S_T$ since $S$ depends in $T$.",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is well-structured and easy to follows. However, some parts of the proofs require additional polishing and clarifications. The results are novel.",
            "summary_of_the_review": "Overall, the paper addresses a very interesting problem and the derived lower bounds can be seen as a solid contribution to the literature on SGD. However, some proofs (especially for the results from Section 4) contain a lot of unexplained parts and require a lot of polishing. This is the only reason why I give such a low score: in the current shape the paper cannot be accepted. However, if the authors fix all the raised issues, I will be happy to increase my score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2077/Reviewer_6ixh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2077/Reviewer_6ixh"
        ]
    },
    {
        "id": "HNC99ysaqEb",
        "original": null,
        "number": 2,
        "cdate": 1666846116431,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666846116431,
        "tmdate": 1666846209263,
        "tddate": null,
        "forum": "bzpCoHn1Vc_",
        "replyto": "bzpCoHn1Vc_",
        "invitation": "ICLR.cc/2023/Conference/Paper2077/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper derives upper and lower bounds on the excess risk of last iterate of SGD in small dimensions with \"standard\" step-size schedules. They provide two main results: a lower bound of $\\Omega\\left(\\frac{\\log{d}}{\\sqrt{T}}\\right)$ on excess risk\nfor convex Lipschitz functions (which is also extended to strongly convex functions). Secondly, they define a sub-class which they call nearly-linear functions, and show that SGD with a step-size of $\\frac{1}{\\sqrt{T}}$ achieves the optimal rate of $O\\left(\\frac{1}{\\sqrt{T}}\\right)$ on this sub-class in one dimension.",
            "strength_and_weaknesses": "Strengths:\n\n\nThe authors make progress on a COLT 2020 published open problem, so the the question has certainly been deemed important by the community. In particular, even though SGD is a seemingly \"simple\" and widely-studied procedure, a tight characterization of the error of its final iterate is still not understood even in one dimension. Hence, the question is very natural. Both the results presented in the paper makes important contributions towards this problem. \n\nThe paper is well-written and presents the state of the problem well and give sufficient details of results and techniques of prior works. The proofs aren't long and complicated and it was nice that they could be presented in (almost) full details in the main text itself.\n\nWeaknesses:\n\nI think even though the results are important, in the bigger picture, the scope of the paper seems limited. \nFirstly, the existing gap is only a $\\log T$ factor.\nMore importantly, from an algorithm design perspective, we know (as discussed in the paper) that simple changes like returning the average iterate or last iterate but with a more complicated step-size suffices to get the optimal rate.\nTherefore, the results may be (very) interesting to a very specific sub-community and not to the general ICLR audience.\n\n\nI have some questions as well as minor corrections about the proof details:\n\n- In the lower bound construction in Theorem 4, it seems that the constructed sub-gradient oracle maintains state -- in particular, on the same input, it gives different outputs as sub-gradients based on the iteration counter. This is a more powerful oracle than used in typical lower bound constructions that I am familiar with. I know that it is within the purview of settings in which the upper bounds are typically stated. \nBut is this something which has been used in prior works? Also, do the authors think that it may be possible to get rid of this additional power?\n\n- Small missing argument before Lemma 6 -- for a complete argument, it seems that you should also argue that $H_i(z_t) < H_{t-T*}(z_t)$ for $i<t-T^*$ and then you have $I(x)\\subseteq \\\\{ t-T^*,..d+1 \\\\}$  (it says $=$ currently), right?\n\n- In the proof of Theorem 15, shouldn't it be $\\eta G(T+1-i)\\leq |l|\\leq c\\eta G(T+1-i)$ i.e. the lower bound does not have the factor $c$, since $\\mathbb{E}[y_i] \\in [-c\\eta G,-\\eta G]$? But it seems that this may affect the part of proof where you recover the $O(\\log{T}/\\sqrt{T})$ rate for general convex functions. What am I missing here?\n\n- Typo: In the proof of Lemma 6, right hand side of third equality should have $a_j \\frac{t-j-T^*}{\\sqrt{T}}$\n\n- Typo: In the proof of Theorem 15, shouldn't the definition of $W_{(i,T+1]}$ have $\\tilde y_j^2$ as the summand?\n\n- Typo: should be seminal instead of seminar in third line in page 2.\n\n- Typo? I don't understand the sentence \"Our results are in the strong versions regarding the third item\" just before start of Section 3.",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper well-written with sufficient details to explain the context and tools used. About quality and novelty -- although the authors build upon prior works on this topic, the proposed extensions are not incremental and require creative changes to yield improvements.\nReproducibility - Does not apply.",
            "summary_of_the_review": "I think the authors present important results on a fundamental problem. The only downside is that topic may be too specialized for ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2077/Reviewer_ao7o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2077/Reviewer_ao7o"
        ]
    },
    {
        "id": "rJQjSieyQO",
        "original": null,
        "number": 3,
        "cdate": 1666930781140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666930781140,
        "tmdate": 1666930781140,
        "tddate": null,
        "forum": "bzpCoHn1Vc_",
        "replyto": "bzpCoHn1Vc_",
        "invitation": "ICLR.cc/2023/Conference/Paper2077/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper shows the lower bound for the final iterate of SGD and also upper bound for a class of nearly linear Lipschitz convex functions. The convergence result is dependent on the dimension $d$. ",
            "strength_and_weaknesses": "This paper provides the theoretical results on the upper bound and the lower bound for the final iterate of SGD. The authors consider dimension dependence on the convergence of SGD for non-smooth Lipschitz convex functions (upper bound) and nearly linear Lipschitz convex functions in one dimension (upper bound).\n \nThe first weakness is that the contribution of this paper is not really significant. There have been several lower bound results for SGD in the similar setting. The concept of nearly linear function is not motivated well enough. The contribution of the upper bound seems to be restrictive since it is dimension 1 and the results for SGD are not particularly interesting. In addition, the assumption that the domain is bounded is restrictive. \n\nThe second weakness is that some of the arguments are sloppy. For example:\n- Too much use of asymptotic notations (e.g. Theorem 15) make it really hard to verify the proof. It would be much better if the authors could clarify this.  \n- Theorem 15, without loss of generality: please clarify this.\n- Theorem 15, equation (10): how the conclusion changed into integration?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the comments on clarity and novelty above. \n\nOther comments: The abstract should be one paragraph only, not two. Given that Theorem 4 is for a non-stochastic oracle while SGD is stochastic, the authors may want to add a discussion on that as well. ",
            "summary_of_the_review": "At the moment I do not support this paper because of the weaknesses noted above. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2077/Reviewer_F4uV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2077/Reviewer_F4uV"
        ]
    },
    {
        "id": "RRlSaUC1_a",
        "original": null,
        "number": 4,
        "cdate": 1667220139085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667220139085,
        "tmdate": 1667220139085,
        "tddate": null,
        "forum": "bzpCoHn1Vc_",
        "replyto": "bzpCoHn1Vc_",
        "invitation": "ICLR.cc/2023/Conference/Paper2077/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies loss of the last iterate of the gradient descent in stochastic non-smooth settings. For the case d < T, they show the lower bound  \u03a9(log d / sqrt{T}) on the final iteration loss with the learning rate 1/\\sqrt{T}, and show an upper bound O(1 / sqrt{T}) on the loss for certain class of functions.\n",
            "strength_and_weaknesses": "I have the following concerns about the paper:\n-- The main concern is that the result looks incremental. Basically, the idea is to freeze the iterate for the first T-d iterations, and then follow the proof which achieves \u03a9(log T / sqrt{T}) lower bound.\n-- Related to the previous point, the sqrt{T} factor in the bound looks a bit arbitrary. I understand that the learning rate 1 / sqrt{T} (and other learning rates considered in hte paper) is chosen since it\u2019s a popular choice for the optimization. However, it could as well be almost any other learning rate, giving a different lower bound. To clarify my point: when I started reading the proof, I was wondering why the bound was not \u03a9(log d / sqrt{d}), given that we simply ignore all but last d iterations. And the only reason for that turned out to be the choice of learning rate.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has some notation issues:\n-- H_i is a function, and hence is not in R^d (you also don\u2019t need this notation)\n-- It should be explained that h_{i,j} is the j\u2019th coordinate of h_i *before* h_{i,j} is defined. I would also change the notation to h^{(i)}_j\n-- It should be clearly written that z_i are sub-gradient oracles (and again, before z_i is defined)\n-- I didn\u2019t understand what y_i is\n-- It took me some time to find what D is in Definition 10. It also not explained what\u2019s good about these points\n",
            "summary_of_the_review": "Incremental work which would benefit from better presentation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2077/Reviewer_axZ4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2077/Reviewer_axZ4"
        ]
    }
]