[
    {
        "id": "ryXfjWmKzBF",
        "original": null,
        "number": 1,
        "cdate": 1666206878091,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666206878091,
        "tmdate": 1666206878091,
        "tddate": null,
        "forum": "p7EagBsMAEO",
        "replyto": "p7EagBsMAEO",
        "invitation": "ICLR.cc/2023/Conference/Paper1457/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The papers studies the gradient descent dynamics on the loss $\\frac{1}{2}(1-x^2y^2)^2$ as a model for deep neural networks (it can be viewed as a depth 4 scalar network with a specific initialization). \n\nThey show two (related) results: that for small enough learning rates $\\eta$, there is a region of initializations $\\mathbb{S}_\\eta$ where gradient descent converges to a global minimum whose largest eigenvalue of the Hessian is close to $2 / \\eta$. Secondly, there is a range of learning rates and a region $\\mathbb{S}$ such that gradient descent starting in $\\mathbb{S}$ with any learning rate in the range will converge to a solution with almost minimal stability (i.e. a largest eigenvalue close to $2 / \\eta$).\n\nThe authors then show that the same does not apply to shallow networks, suggesting that this phenomenon is characteristic to deeper networks.\n\nFinally to motivate the requirement to start within a specific region the authors show empirically (as well as motivating theoretically) that the same loss can lead to chaotic behavior in the trajectory of the gradient. This might explain the difficulty in proving any global convergence.",
            "strength_and_weaknesses": "The phenomenon of 'edge of stability' is the subject of much interest and while there are a number of results that assume convergence at a $\\eta$-stable global minima, it is important to also understand how the network converges to such points.\n\nDespite its simplicity, the model exhibit similar behavior to the ones observed in practice. The reason behind the simplifications are quite well explained and motivated. The authors have done a great job explaining the ideas behind the proofs, both visually and mathematically. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well written and clear, though I have two small remarks, see below. The mathematics seem correct. I am not extremely familiar with this line of work, so cannot assert the novelty of this work with certainty, but the related works section is detailed and appears complete.\n\nSmall remarks:\n - I found the summary of the results in page 2 a bit confusing, it almost seems that the theorem and its corollary are the same statement. If I understand correctly the sets  $\\mathbb S$ (in the theorem) and  $\\mathbb S$ (in the corollary) are different. In the theorem  $\\mathbb S$ depends on the learning rate, while in the corollary  $\\mathbb S$ applies to whole range? I think it would be much clearer if you would write $\\mathbb{S}_\\lambda$ to emphasize the dependence on $\\lambda$.\n- I think there is a typo in Equation 6 on page 5, the second line should start with $b_{t+1} = b_t + [\\dots]$ instead of $b_{t+1} = b + [\\dots]$. ",
            "summary_of_the_review": "The paper shows the appearance of Edge of Stability dynamics in a simplified model of deep neural networks. The results are interesting and well explained.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_DD1S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_DD1S"
        ]
    },
    {
        "id": "XqAZwN2M6xy",
        "original": null,
        "number": 2,
        "cdate": 1666484127886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666484127886,
        "tmdate": 1666484127886,
        "tddate": null,
        "forum": "p7EagBsMAEO",
        "replyto": "p7EagBsMAEO",
        "invitation": "ICLR.cc/2023/Conference/Paper1457/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the the phenomenon of \"Edge-of-Statility\" (EOS) training dynamics and analyses the gradient dynamics on a simple example (the example is simple but the analysis is quite complicated).\n\nThe EOS of training dynamics, initially observed by [Cohen et al. 2021], refers the phenomenon that when running gradient descent for deep neural networks with learning rate $\\eta$, despite the fact that the sharpness (maximum eigenvalue of the Hessian) is often larger than stability threshold $2/\\eta$, the loss converges in the long run, and the sharpness at the end is around $2/\\eta$.\n\nThe paper examines the function of $f(x, y, z, w) = \\frac{1}{2}(1- xyzw)^2$ and proves the EOS occurs in a local region. The paper also highlights the importance of considering degree 4 function versus degree 2 (or degree 3), and argues that even for this simple example, global analysis is hard since it corresponds to some problems in chaos theory.\n\n",
            "strength_and_weaknesses": "Strength. I believe this paper has some contribution towards  understanding the EOS phenomenon, especially by pointing out the difference between degree 2 construction and the connection between chaos theory. The analysis seems non-trivial.\n\nWeakness. The paper works on some toy case, as pointed out by the paper, even the toy case seems to be complicated and the paper provides theoretical guarantee only over local region. The major unsatisfactory point for me is that it is hard to generalize the finding of this paper to general regime.",
            "clarity,_quality,_novelty_and_reproducibility": ".",
            "summary_of_the_review": "The paper provides a simple example and analyze the GD dynamics on it, however it is unclear how does it contribute to the broad understanding of EOS.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_sSz2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_sSz2"
        ]
    },
    {
        "id": "wZD8H__fDnb",
        "original": null,
        "number": 3,
        "cdate": 1666699343368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699343368,
        "tmdate": 1669204297565,
        "tddate": null,
        "forum": "p7EagBsMAEO",
        "replyto": "p7EagBsMAEO",
        "invitation": "ICLR.cc/2023/Conference/Paper1457/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the \"Edge-of-Stability\" effect on a toy model of deep linear neural network with single neuron per layer (with a further extension to another toy model of rank-1 matrix factorization). The authors demonstrate that under suitable conditions this model exhibits \"sharpness adaptivity\". Then, the paper shows that, due to the absence of high order terms, this adaptivity need not hold for shallow networks, in agreement with empirical observations. Finally, the paper discusses the fractal structure of the global convergence pattern and draws parallels with chaotic nonlinear systems such as the well-known logistic map. ",
            "strength_and_weaknesses": "EoS is a popular topic, and there are several concurrent submissions explaining this effect from somewhat different points of view (e.g. https://openreview.net/forum?id=nhKHA59gXz, https://openreview.net/forum?id=R2M14I9LEwW). Compared to these other works, the present submission has a rather narrow scope limited to one or two toy models. Accordingly, there are no empirical results involving real world data. I also have the impression that, relative to the other submissions, the exposition is a little bogged down by technical details and conceptually not very illuminating. Though the model is a toy one, its analysis involves cumbersome computations and does not convey easy intuition applicable in more general settings.\n\nOn the other hand, the paper does discuss some interesting points not found elsewhere, particularly the effect of the model degree on EoS and a fractal structure of converged sharpness.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably clearly written. I didn't check the proofs in the appendix.\n\nI find the technical contributions of the paper new and interesting, but I'm not sure about their significance in the present form. Rigorous results are proved only for a toy model and only confirm a known phenomenon. On the other hand, the discussion in section 5 of global sharpness pattern as resulting from \"de-bifurcation\" seems conceptually novel and interesting, but is presented only on a phenomenological level. It is also not clear to which extent it is relevant for real world problems. ",
            "summary_of_the_review": "An interesting paper that clarifies some aspects of the EoS effect, but has several weaknesses, especially in comparison to existing alternative explanations (only toy models; no experiments with real world data; somewhat too technical exposition; no rigorous results confirming the global fractal picture - probably the most interesting part of the paper)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_8B9h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_8B9h"
        ]
    },
    {
        "id": "ZDWmT9Wgman",
        "original": null,
        "number": 4,
        "cdate": 1666990615960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666990615960,
        "tmdate": 1666990615960,
        "tddate": null,
        "forum": "p7EagBsMAEO",
        "replyto": "p7EagBsMAEO",
        "invitation": "ICLR.cc/2023/Conference/Paper1457/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes a simple non-convex function in four dimensions (a four-layer scalar network):\n\n1. which empirically demonstrates the *\"edge-of-stability\"* phenomenon while training with gradient descent, \n2. is qualitatively more similar to deeper neural networks than previously proposed model functions in terms of *\"sharpening\"*, \n3. and is analytically simple enough to show a convergence theoretically to the sharpest global minimum attainable for a given step size. \n\nThe paper provides a local convergence guarantee (which they call sharpness concentration), i.e., for any learning rate, provided the initialization is in a ball around the sharpest global minima (attainable for that learning rate), the optimizer will converge to that minima. And it discusses the roadblock in extending it to a global convergence guarantee (i.e, convergence to the edge of stability for any initialization):\n\n> the boundary separating converging and diverging initializations (Fig. 7a) exhibits complicated fractal structures.\n\nTo underline the limitations of previous works trying to model the phenomenon, the paper discusses the difference between two, three, and four-layered scalar networks. It shows, empirically and analytically, that the phenomenon gets more pronounced, i.e., there is less of a sharpness gap w.r.t. sharpest attainable minima, when depth increases.  The theoretical results are also extended to the vector case, i.e., rank-1 factorization of an isotropic matrix.  ",
            "strength_and_weaknesses": "The paper is well-written and precise. The flow of the paper gives a good intuition for different re-parameterizations, convergence properties of the system, and a rough proof sketch outlining different training phases. Since the paper is supposed to model a complex phenomenon in neural networks, one desirable outcome is to provide an intuition about the edge of stability phenomenon. I believe it does so well. The paper also highlights the theoretical result's limitation and the roadblock, thus providing a balanced view of the contribution. It compares itself to the previous attempts at modeling the problem, highlighting their limitations. \n\nI believe the paper would benefit from some more experiments for completeness. For instance, try to minimize loss (11) and demonstrate edge-of-stability. It would also be good to see some experiments to check if the hardness of showing global convergence is an analysis issue or a genuine artifact. To do this, the authors could explicitly run synthetic experiments on their function and vary the size of the ball for initializing their models. Repeating this for several different step sizes and dimensional problems would be insightful. \n\nIn my understanding, training with SGD may or may not demonstrate the same sharpening depending on the noise level. I am curious if the authors can extend some of their simpler results to incorporate stochasticity, perhaps just simple additive noise. It would be good to discuss how the noise scale qualitatively changes the phenomenon. Intuitively the noise might also smoothen the boundary between initialization that do and do not converge. At least some experiments with SGD can shed light on its behavior. ",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clear and of high quality. There is a technical novelty, and the experiments and calculations seem reproducible. \n\n\n\n  \n\n",
            "summary_of_the_review": "I believe the proposed model captures the essence of the edge-of-stability phenomenon while training with gradient descent. The paper is well-written and highlights why even their simple model has useful insights about different aspects of training.  I recommend accepting the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_wrPK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_wrPK"
        ]
    },
    {
        "id": "VbCM_Sf9Sf",
        "original": null,
        "number": 5,
        "cdate": 1667404699869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667404699869,
        "tmdate": 1667404699869,
        "tddate": null,
        "forum": "p7EagBsMAEO",
        "replyto": "p7EagBsMAEO",
        "invitation": "ICLR.cc/2023/Conference/Paper1457/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work is motivated by the \u201cedge of stability\u201d phenomenon, observed by Cohen et al.: when training neural networks with step size \u03b7, the largest eigenvalue of the Hessian (dubbed the \u201csharpness\u201d) along the training trajectory often stabilizes near 2/\u03b7, which is the stability threshold in convex optimization. The paper carefully analyzes the GD dynamics on the loss function (w, x, y, z) -> (wxyz - 1)^2 (actually because the initialization is taken to be w = y and z = x, it is really a 2-variable analysis, see comment below) and shows that for a region of initializations and step sizes, the limiting sharpness of the GD trajectory is 2/\u03b7 + O(\u03b7).",
            "strength_and_weaknesses": "The strength of the work is that it rigorously establishes the EoS phenomenon with a very careful analysis. Despite only considering a simple objective function, the dynamics are quite intricate and it is technically challenging to establish the result. As it is important to understand non-convex dynamics, especially those capturing the behavior of neural network training, this work makes a valuable contribution to the literature.\n\nHowever, due to the initialization w = y and z = x, the authors really end up analyzing a 2-variable function. In fact, up to rescaling the step size, it is easy to see that the dynamics they analyze are equivalent to GD on the 2-variable function (x, y) -> (x^2 y^2 - 1)^2. Hence I believe that the introduction of the 4-variable function is highly misleading and I ask that the authors reformulate the discussion in terms of the 2-variable objective that they actually analyze.\n\nAlso, it is difficult to claim that this example is minimalistic considering that the analysis and dynamics are so complicated, but I believe it is out of scope of the present work to greatly simplify the analysis.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly clear and of good quality. The novelty value is high since there are not many analyses of optimization in this regime; in particular, many prior works focus on more complex neural network settings with many additional assumptions, whereas this work focuses on a simple example in which it is possible to establish everything from first principles.\n\nTypos:\n- Pg. 1, \u201ctraining trajectory often oscillate\u201d -> \u201ctraining trajectory often oscillates\u201d\n- Pg. 4 \u201cDespite we are able\u201d is improper grammar\n- Pg. 6, the paragraph beginning with \u201cNote that\u201d: the last sentence is a run-on sentence",
            "summary_of_the_review": "I have issues with the way that the results are presented; namely, I believe that the paper should not mention the four-variable objective if it is not used in the analysis. Overall, I think that the contribution is solid, the problem setting (EoS) is important, and the work is technically difficult, and hence it merits acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_jBkM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1457/Reviewer_jBkM"
        ]
    }
]