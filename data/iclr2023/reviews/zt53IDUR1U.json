[
    {
        "id": "4FZPXBTt6q",
        "original": null,
        "number": 1,
        "cdate": 1666515946649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666515946649,
        "tmdate": 1666516110082,
        "tddate": null,
        "forum": "zt53IDUR1U",
        "replyto": "zt53IDUR1U",
        "invitation": "ICLR.cc/2023/Conference/Paper2222/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method named MICN to replace the self-attention module and achieves linear computational complexity. This method is based on convolution structure and proposes Multi-scale Isometric Convolution using multiple branches of different convolution kernels to model different potential pattern information of time series. The MICN has a local-global structure where the local features of the sequence are extracted by a local module based on downsampling convolution and the global correlation is modeled by a global module based on isometric convolution. The results show that MICN achieves state-of-the-art accuracy and isometric convolution is superior in both complexity and effectiveness to self-attention.",
            "strength_and_weaknesses": "Pros:\n- The results show that MICN achieves state-of-the-art accuracy on five real-world benchmarks. MICN yields impressive 18.2% and 24.5% relative improvements for multivariate and univariate time series, respectively.\n- This paper uses a lot of ablation experiments to verify that the proposed Multi-scale Isometric Convolution outperforms the self-attention family and Auto-correlation mechanism.\n- This paper gives a detailed mathematical description of their method, showing how to extract local features to measure short-term changes and to model the global correlations to measure the long-term trend using a convolution structure.\n\nCons:\n- Complexity analysis is needed. One of the motivations of this paper is the high complexity of self-attention and this paper tries to reduce the complexity. However, the full text doesn't analyze the complexity of the proposed method, just saying the complexity of the method is O(L), but 'L' is not mentioned in the full text. And the complexity of the method seems to be N*logI.\n- Isometric convolution is not explained in detail as the core part of the article. How is isometric reflected? Why do convolutions in this form?\n- The paper proves that the proposed Multi-scale Isometric Convolution is superior to self-attention through experiments. However, for the short length of 92, the isometric convolution performance is all lower than self-attention in multivariate forecasting, which is inconsistent with the description of the paper, and there are no further explanations and analysis.\n\nQuestions\uff1a\n- In Section 3.2, what conditions are satisfied with the kernels to ensure that the time series length is constant? How is each kernel determined? \n- In Section 3.3, is 'mean' here an average operation? How to average a time series? How to get the result of o-dimension from i-dimension time series through mean operation?\n- In section B.3, why is the replacement of Isometric convolution and self-attention in MICN-regre instead of the Seasonal Prediction Block?",
            "clarity,_quality,_novelty_and_reproducibility": "Good",
            "summary_of_the_review": "The idea is interesting and somewhat novel. More theoretical analysis would be better.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2222/Reviewer_PRkd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2222/Reviewer_PRkd"
        ]
    },
    {
        "id": "DjRQfn9-Tw",
        "original": null,
        "number": 2,
        "cdate": 1666547303264,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666547303264,
        "tmdate": 1666547303264,
        "tddate": null,
        "forum": "zt53IDUR1U",
        "replyto": "zt53IDUR1U",
        "invitation": "ICLR.cc/2023/Conference/Paper2222/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on long term forecasting. The problem to be solved is to predict values of a variable for a future period. The authors start by providing a comprehensive review of the related papers and end with the conclusion that a good forecasting method should have the  ability to extract local features to measure short-term changes, and the ability to model the global\ncorrelations to measure the long-term trend. Based on that, they propose a Multi-scale Isometric Convolution Network (MICN). They use multiple branches of different convolution kernels to model different potential pattern information of the sequence separately.",
            "strength_and_weaknesses": "The topic of this paper address the problem of long-term series forecasting. The topic is very interesting and the paper is well written. The authors provide a comprehensive review of the related papers. Mathematical models and figures are proposed to understand the problem and the solution provided by the authors. Extensive experiments are provided to demonstrate the effectiveness of the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written overall and easy to understand",
            "summary_of_the_review": "This paper proposes a Multi-scale Isometric Convolution Network (MICN)  based on convolution structure to efficiently replace the self-attention,and it achieves linear computational complexity and memory cost. \nThe  empirical studies show that the proposed model improves the performance of state-ofthe-art methods by 18.2% and 24.5% for multivariate and univariate forecasting, respectively\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2222/Reviewer_FdbB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2222/Reviewer_FdbB"
        ]
    },
    {
        "id": "bS2ow8MJMEx",
        "original": null,
        "number": 3,
        "cdate": 1666670185482,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670185482,
        "tmdate": 1666670185482,
        "tddate": null,
        "forum": "zt53IDUR1U",
        "replyto": "zt53IDUR1U",
        "invitation": "ICLR.cc/2023/Conference/Paper2222/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the Multi-scale Isometric Convolution Network, which can efficiently capture\nlocal-global interaction of time series data for long-term forecasting. It uses multi-scaled\nconvolutions for local information extraction and isometric convolution to capture the global\nrelationship. In general, it achieves the state-of-the-art accuracy in five time series benchmarks\nwith linear complexity.",
            "strength_and_weaknesses": "Strength:\nThe overall paper is well written and most of the concepts are concisely explained. As the paper\nstated, both local patterns and global dependency are crucial for long-term time series\nforecasting. The design is well motivated. The benchmark results are also promising to\ndemonstrate the effectiveness of the framework.\n\nWeaknesses:\nDespite the state-of-the-art accuracy, it seems that the main improvement comes from the\nregression-based trend-cyclical prediction. Trend-cyclical analysis in section 4.2 is not enough to\nunderstand how the regression influence the prediction. I suggest evaluating the regression and\nthe mean prediction separately from the full benchmark performance. Also, the results show that\nMICN-mean has comparable performance (sometimes even worse) than FEDformer, while\nFEDformer is using mean trend prediction. I can\u2019t help but thinking if FEDformer with regression\ntrend prediction can surpass MICN. More discussion in trend-cyclical prediction is needed.\nAnother thing that can be discussed more is the intuition behind isometric convolution. The\nmotivation of the linear complexity is good, but the paper is unclear about how isometric\nconvolution is going to capture the correlations between local time region. Besides, there exists\nmany works that focus on the efficiency of self-attention and many has linear complexity as well\n(e.g., Linformer [1], Fastformer [2], \u2026). If the primary motivation behind isometric convolution is\nthe complexity then these works should be discussed as well.\nI also have concerns over the explainability of the local-global structure. Self-attention is good at\nmaking interpretable prediction (can be crucial for real-life application, as the paper mentioned),\nwhereas I don\u2019t see how MICN can be interpreted at a first glance. It is sad when the paper\nargues \u201cthe framework can deeply mine the intricate temporal patterns\u201d but fails to interpret them.\nThis is also one of the reasons I am curious about how linear self-attention can perform instead of\nisometric convolution.\nOther minor suggestion:\n1. The abstract says that the structure can model patterns separately and \u201cpurposefully\u201d. The\nwording is vague and confusing. Does it indicate that we can incorporate external knowledge\ninto the model?\n2. In section 3.1, briefly mentions that the term (e.g., \u201cAvgPool\u201d, \u201ckernel\u201d) is from the convolution\nperspective. I first thought the \u201ckernel\u201d represents the kernel function.\n3. The difference between ablation table 5 and table 13 can be explained more.\n[1] Wang, Sinong, et al. \u201cLinformer: Self-Attention with Linear Complexity.\u201d ArXiv.org, 14 June\n2020, https://arxiv.org/abs/2006.04768.\n[2] Wu, Chuhan, et al. \u201cFastformer: Additive Attention Can Be All You Need.\u201d ArXiv.org, 5 Sept.\n2021, https://arxiv.org/abs/2108.09084.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with detailed appendix. The general idea of using a hierarchy alike\nstructure for capturing local-global interaction, plus solving the self-attention complexity issue, are\nnot completely novel (many works in vision transformer field), but the authors manage to design a\nstructure specifically for time series forecasting.",
            "summary_of_the_review": "As I mentioned above, the overall method is well motivated and the benchmark results are\npromising, whereas the paper can be further improved by more ablation studies and discussions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2222/Reviewer_eyrH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2222/Reviewer_eyrH"
        ]
    }
]