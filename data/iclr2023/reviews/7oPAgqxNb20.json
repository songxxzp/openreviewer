[
    {
        "id": "JYWrHAbHlFj",
        "original": null,
        "number": 1,
        "cdate": 1666159406174,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666159406174,
        "tmdate": 1666159406174,
        "tddate": null,
        "forum": "7oPAgqxNb20",
        "replyto": "7oPAgqxNb20",
        "invitation": "ICLR.cc/2023/Conference/Paper4255/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a new selection method to make train deep networks efficiently, which dynamically selects a small training \n set from the original data set. The selecting criterion is based on the classification margin. The authors also propose a lighter proxy model to speed up the selection process. The authors conduct experiments on CIFAR-10 and ImageNet to verify the effectiveness, and the results are over many baselines. ",
            "strength_and_weaknesses": "Strength:\n1. The topic of this paper is relatively new and can be used in many applications.\n2. The performance of the proposed paper is strong.\n\nWeakness:\n1. The critical parts of this paper, the \u201cSELECT\u201d function and Power schedule, are missing. There are numerous hyperparameters in the supplementary, but no equation and process in the paper and supplementary. \n2. \u201cThe Gaussian input assumption \u2026 infinitely a wide neural network resembles Gaussian process\u201d. I think this assumption is against the research problem that uses smaller datasets and models. With limited number of data, the input will not follow Gaussian distribution.\n3. The analysis on computational complexity is good but not enough. The comparison of the total training time between the proposed method and the original method from the ResNet paper is necessary.\n\nSmall issues:\n1. In introduction, \u201credundant samples can be left out without sacrificing performance\u201d. From 4, there is a clear drop in terms of performance, and Top 1 Acc. decreases over 1%.\n2. The Top 1 Acc. results of \u201cOriginal\u201d in Table 4 and \u201cFull\u201d in Figure 4 are different. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a good format and easy to follow. \n\nQuality and Novelty are Fair. \n\nSince there are some important equations and processes missing, I think this paper is hard to be reproduced.\n",
            "summary_of_the_review": "I do not recommend this paper now. If the authors can add missing parts, I will re-evaluate this paper. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4255/Reviewer_UJJg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4255/Reviewer_UJJg"
        ]
    },
    {
        "id": "0EokwoGNm2",
        "original": null,
        "number": 2,
        "cdate": 1666552675663,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666552675663,
        "tmdate": 1666552675663,
        "tddate": null,
        "forum": "7oPAgqxNb20",
        "replyto": "7oPAgqxNb20",
        "invitation": "ICLR.cc/2023/Conference/Paper4255/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a subset selection method for training DNNs. The idea is to select the samples that are close to the margin dynamically. To reduce computation, the authors also propose Parameter Sharing Policy for sample selection. The authors show that the proposed DynaMS can converge tot the optimal solution with large probability. Experiments are conducted on CIFAR10 and ImageNet to show the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "Strength:\n\n1. Reducing sample complexity is an important topic in deep learning. The authors propose some interesting ideas to address this problem.\n2. Overall, the paper is clearly written and the proposed idea is intuitive.\n3. Extensive ablation studies are conducted to show the benefit of the proposed approach.\n \nWeaknesses:\n1. The use cases of the proposed approach is not clear. If all the training samples are available for dynamic selection, then why not just use all the samples? Also, the authors did not show results of using all the samples. If sample selection is for computational complexity, there is also no time comparison.  \n\n2. Compared with dataset distillation, does the proposed approach have benefits in terms of accuracy and complexity?\n\n3. It is not clear which samples are selected and how the samples selected will change over time. \n\n4. Several typos:\n\ni. \"Our proposed DynaMS form\" -> \"Our proposed DynaMS forms\"\nii. \"is keep fixed\" -> \"is kept fixed\"\niii. \"as the model evolve\" -> \"as the model evolves\"",
            "clarity,_quality,_novelty_and_reproducibility": "Over, the paper is clearly written. The quality is fair. The idea of dynamic selection is novel. Also, the paper seems easy to reproduce.\n",
            "summary_of_the_review": "The authors propose a dynamic sample selection method for DNN training and conduct extensive experiments to show the benefits. However, there are several issues with the current draft as stated in the drawbacks. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4255/Reviewer_6uhW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4255/Reviewer_6uhW"
        ]
    },
    {
        "id": "5Y7xZ3aB4q",
        "original": null,
        "number": 3,
        "cdate": 1666596700550,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666596700550,
        "tmdate": 1666596889743,
        "tddate": null,
        "forum": "7oPAgqxNb20",
        "replyto": "7oPAgqxNb20",
        "invitation": "ICLR.cc/2023/Conference/Paper4255/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to dynamically select partial training data for efficient learning. The main idea is updating informative subset according to their margin to class decision boundary. A parameter sharing proxy strategy is devised to further evaluate instance prior. As a result, the proposed method achieves superior performance compared with other SOTA data selection strategies on different budgets.",
            "strength_and_weaknesses": "Strength:\n1. Selecting informative subset by class margin is reasonable, and the generalization of DynaMS has been well supported both in practice and theory.\n2. The performance is superior compared with previous SOTA methods.\n3. This paper is well written and the implementation is described in detail which is easy to follow.\n\nWeaknesses:\n1. The need for PSP is not significant. As mentioned in this paper, the selection procedure is efficient and its overhead is negligible since it is conducted only 19 times during training (200 epochs in total). \n2. Training slimming networks is not efficient because multiple sub-networks are trained separately. For example, compared to the original training time, with a 60% budget, the overall training cost of DynaMS+PSP is $0.6\\times (1+\\frac{9}{16}+\\frac{1}{4}+\\frac{1}{16})>1$, while other methods almost strictly cost 0.6 of the original one.\n3. For the results shown in Figure 2(a), only 10 epoch training on one dataset is not convincing enough. Why not report overlap ratio of all evaluated epochs on both CIFAR-10 and ImageNet?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is presented well and the idea of dynamic selection based on class margin is natural and reasonable. The whole pipeline is novel and easy to reproduce with the detailed implementation.",
            "summary_of_the_review": "This paper present a good idea to select informative subset for efficient training. The proposed method achieves superior performance and generalization. However, the PSP module seems redundant for the whole pipeline and it weakens the contribution based on my current understanding. I am glad to change my score if my concerns are well addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4255/Reviewer_hmw5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4255/Reviewer_hmw5"
        ]
    },
    {
        "id": "JQ7tXrDo-x0",
        "original": null,
        "number": 4,
        "cdate": 1666668623235,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668623235,
        "tmdate": 1666668623235,
        "tddate": null,
        "forum": "7oPAgqxNb20",
        "replyto": "7oPAgqxNb20",
        "invitation": "ICLR.cc/2023/Conference/Paper4255/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a dynamic margin selection (DynaMS) method to dynamically construct the training subset by utilizing the distance from candidate samples to the classification boundary. In addition, a light parameter sharing proxy is designed to reduce the additional computation incurred by the selection. Extensive analysis and experiments demonstrate the superiority of the proposed approach in data selection.",
            "strength_and_weaknesses": "Strength\uff1a\n+ A dynamic margin selection (DynaMS) method is proposed to dynamically construct the training subset by utilizing the distance from candidate samples to the classification boundary.\n+ Extensive analysis and experiments are conducted to show the performance of the proposed method.\n\nWeaknesses\uff1a\n- The paper claims that the existing sample selection methods are expensive both in run-time and in memory efficiency, however, the paper lacks comparisons with different methods in run-time and memory.\n- In Table 3, the proposed DynaMS does not perform better than Stat.-based methods, and performs slightly better than Dyna.-based method such as CRAIG and GradMatch.\n- In Table 4, the results of DynaMS+PSP are not competitive, so the contribution of PSP is limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and clarity is good, and the originality of the work is fair.",
            "summary_of_the_review": "The major concerns of this work are as\n- The paper claims that the existing sample selection methods are expensive both in run-time and in memory efficiency, however, the paper lacks comparisons with different methods in run-time and memory.\n- In Table 3, the proposed DynaMS does not perform better than Stat.-based methods, and performs slightly better than Dyna.-based method such as CRAIG and GradMatch.\n- In Table 4, the results of DynaMS+PSP are not competitive, so the contribution of PSP is limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4255/Reviewer_8YeC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4255/Reviewer_8YeC"
        ]
    }
]