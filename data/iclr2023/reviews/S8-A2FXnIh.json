[
    {
        "id": "oOo33cEAm0",
        "original": null,
        "number": 1,
        "cdate": 1666238058780,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666238058780,
        "tmdate": 1668847104461,
        "tddate": null,
        "forum": "S8-A2FXnIh",
        "replyto": "S8-A2FXnIh",
        "invitation": "ICLR.cc/2023/Conference/Paper439/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the compositional zero-shot learning problem using CLIP, a powerful vision-language model that was pretrained on broad data to measure similarity between images and texts. Instead of fine-tuning the entire model, the paper proposes to fine-tune only the attribute and object tokens in a prompt template in the text input space of CLIP. Extensive experiments on three benchmarks, i.e., MIT-states, UT-Zappos and C-GQA, show that the proposed method, despite having a simple design, outperforms several other relevant baselines.",
            "strength_and_weaknesses": "**Strengths**\n\nThe emergence of vision foundation models, particularly vision-language models like CLIP, has offered many opportunities to the computer vision community. This work provides timely insights on how prompt learning, a simple technique that tunes a few parameters in a model's input space, can be used to tackle compositional zero-shot learning. Though soft prompt learning has been applied to CLIP before, the results shown in this work on the compositional zero-shot learning setting are novel and useful to the community.\n\nIt is also worth mentioning that the field of compositional zero-shot learning has been previously dominated by task-specific methods. This work shows that a simple technique based on prompt learning and a large-scale vision-language model could provide significant improvements to the problem. Given the importance of large-scale models, which have become a trend in the community, this work could serve as a strong baseline to build upon for future work.\n\n**Weaknesses**\n\nBelow some minor comments are listed for the authors to address.\n\n1. From the technical point of view, CoOp fine-tunes all context tokens while this work fine-tunes only the attribute token and the object class token. It's good to see that such a seemingly trivial twist can lead to huge improvement. Currently, the paper doesn't provide in-depth analysis in this regard. Would it be possible for the authors to elabroate on this design and explain (perhaps with some evidence) why such a small change could make a big difference?\n\n2. The paper only studies this template, \"a photo of xxx\". Have the authors tried other variants and if so, what are observed in the results?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: Overall, the paper is well-written and easy to follow.\n\n**Quality**: The paper provides comprehensive experiments, which are helpful for readers to understand what benefits the simple prompt learning approach could bring to the area of compositional zero-shot learning. The method is well-motivated and has a simple design, which could be inspiring to the community.\n\n**Novelty**: The research is generally novel because previous work often uses task-specific architectures while this work shows that by applying the simple prompt learning approach to large vision-language models, we can obtain non-trivial improvements. The method and findings could be of interest to both the community of compositional zero-shot learning and those who are working on large-scale models and prompt learning.\n\n**Reproducibility**: The paper seems to contain sufficient details for readers to re-implement the method and reproduce the results.",
            "summary_of_the_review": "The paper provides novel insights on how to apply prompt learning to large vision-language models to solve compositional zero-shot learning. Overall, the paper is well-written, the method is simple and technically sound, and the results are encouraging and useful. So I recommend to accept the paper.\n\n== Post-rebuttal update ==\n\nThe authors have done a good job in addressing the two weakness points. My view about the novelty, results and significance remains the same: the paper should be accepted.\n\nI have also read other reviewers' comments and noticed that some reviewers have concerns about the technical novelty and the resources needed to further tune CLIP. I would like to speak for the authors here.\n- The paper should be evaluated in the context of compositional zero-shot learning. The novelty is then clear because existing methods are mostly based on task-specific models while this paper demonstrates the effectiveness of such a simple prompt-driven paradigm, which is encouraging and could spur a paradigm shift in compositional zero-shot learning.\n- CLIP was indeed trained with massive data but this doesn't mean it must work in any scenario without some adaptation\u2014which is the incomplete nature of foundation models. It would make more sense if we take a look at the counterpart in NLP like GPT or BERT. To adapt large language models, one also needs to apply some adaptation methods, such as learning specialized modules or using prompt learning.\n- It might seem straightforward to apply task-specific learning (e.g., prompt learning) to adapting CLIP to solve compositional zero-shot learning, but such a \"straightforward\" hypothesis has never been scientifically validated\u2014which is done in this work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper439/Reviewer_m1LH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper439/Reviewer_m1LH"
        ]
    },
    {
        "id": "1BUbgEOOIoW",
        "original": null,
        "number": 2,
        "cdate": 1666433753870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666433753870,
        "tmdate": 1666433753870,
        "tddate": null,
        "forum": "S8-A2FXnIh",
        "replyto": "S8-A2FXnIh",
        "invitation": "ICLR.cc/2023/Conference/Paper439/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of a large-scale pretrained vision-language model. In addition, the paper demonstrates that the learned vocabulary generalizes across multiple datasets. Finally, the proposed method substantially improves the results on multiple datasets.",
            "strength_and_weaknesses": "**Strengths:**\n\n1) Overall, this paper is well written, and the technical details are easy to follow. \n\n\n2) The main idea of introducing better prompting techniques for zero-shot compositionality is appealing.\n\n\n3) The main contribution of this paper is the compositional soft prompting technique, which treats the attributes and objects that are composed to define classes as learnable tokens of vocabulary in a prompt as \u201cA photo of [attribute] [object]\u201d. Although I consider this a minor novelty in general, it still meets the bar.  \n\n4) The results of the experiment strongly support the proposed approach, including the strong ablations that were performed.\n\n\n**Weaknesses:**\n\n**Experiments.** Recently, prompting has become a hot topic. Why not add more recent prompting techniques such as VPT [1] and others? Essentially, the proposed technique is a standard CLIP training with additional attribute and object prompting. Therefore, I believe it would be useful to have a more comprehensive comparison of prompting baselines.   \n\n[1] Visual Prompt Tuning, ECCV 2022\n\n**Technical Novelty.** As I said above, the main contribution of the paper is the treatment of the attributes and objects that are composed to define classes as learnable tokens of vocabulary in a prompt as \u201cA photo of [attribute] [object]\u201d. I find it difficult to consider the proposed approach (prompting of attributes and objects together) to be a significant contribution, but I acknowledge that it is the first paper to address this important issue.\n\n\n**Relation to Prior Work.** There are several works in that area based on compositionality for videos or scene graphs, which the author could also add and discuss. This topic is very important, and the related work paragraph has a limited scope, which is fine, but here are some additional references the authors may wish to consider:\n\n[1] Bar et al. Compositional Video Synthesis with Action Graphs, ICML 2021.\n\n[2] Generating videos of zero-shot compositions of actions and objects. ECCV 2020.\n\n[3] Compositional video prediction. ICCV 2019.\n\n[4] Learning Canonical Representations for Scene Graph to Image Generation. ECCV 2020.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I wrote above my concerns regarding the novelty.",
            "summary_of_the_review": "My main concern is that this paper presented a minor approach to tackles the important problem of the compositionality of VL models. I am open to the authors' feedback and other reviewers' opinions.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper439/Reviewer_43mh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper439/Reviewer_43mh"
        ]
    },
    {
        "id": "dagYbAX766",
        "original": null,
        "number": 3,
        "cdate": 1666562887731,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666562887731,
        "tmdate": 1669008073033,
        "tddate": null,
        "forum": "S8-A2FXnIh",
        "replyto": "S8-A2FXnIh",
        "invitation": "ICLR.cc/2023/Conference/Paper439/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Summary: \nThe paper proposes a new method for the task of zero-shot compositionality on top of CLIP style models. The proposed approach learns embeddings for objects and attributes using the CLIP objective. At test time, they are able to recombine the embeddings for attributes and classes for novel compositions and show good zero-shot generalization.\n",
            "strength_and_weaknesses": "\nStrengths: \n- The paper proposes a soft prompting approach for compositional zero-shot learning that improves performance with just a small set of learnable parameters (the tokens corresponding to attributes and classes). \n\n- The paper show many useful experiments -- generalization to higher-order compositions, prompt tuning vs full fine-tuning, etc. Overall, the experiments are enough to demonstrate the effectiveness of the approach on the proposed task. \n\nWeaknesses:\n- Generally speaking, I am unsure if the evaluation is fair / comparable. The authors themselves point out that comparison to existing methods is difficult because they were not trained on large amounts of web data. The authors compare to CLIP and CoOp but the results here are not very surprising either -- task-specific learning should help improve performance.\n\n- CoOp is a weird baseline to compare to. For CoOp, the prefix is learned, while for the proposed approach the token embeddings are learned. I think it will be nice to show if CSP + CoOp help over CSP. If you learn prefix tokens, as well as attribute and class tokens, does that help improve performance over just doing prefix learning / class & attribute token learning. \n\n- Another baseline to compare to would have been learning token embeddings individually instead of together. So instead of using \"A photo of [attribute] [class]\", learn using \"A photo of [attribute object\", and \"A photo of [class]\". Can the model then combine these in interesting ways? \n\n-- Finally, is it possible to avoid training at all on composition of attribute & classes at all. I think it's a little intellectually dissastisfying that despite training on so much web-scale data, we need to do more task-specific prompt tuning for recognizing compositions. Is there a smarter way to combine the two similarity scores (image-attribute, and image-class) that avoids the need to learn these embeddings? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written and easy to follow. The evaluations are thorough. The proposed work combines several existing ideas -- using CLIP for zero-shot classification, task-specific prompt-tuning instead of fine-tuning, etc and shows good results on compositional zero-shot. I think the presented ideas are not novel by themselves, but its application for compositional zero-shot learning might be novel. ",
            "summary_of_the_review": "Overall, the paper was easy to read and the presented idea was simple and intuitive. I am not sure that the paper in its current form presents any new significant insight that is not known to the broader AI community.\n1. We know that CLIP are great at zero-shot tasks.\n2. We know that prompt-tuning works really well instead of fine-tuning in a low-data regime.\n\nI think the paper is missing a good-faith effort of getting CLIP to work for compositional zero-shot without any fine-tuning / prompt-tuning. It's a little dissatisfying that despite training on so much data, CLIP models need further task-specific training to perform well. Doing task-specific prompt-tuning seems like a very natural step, and I am unsure if the paper presents any ideas that will be useful to the broader AI community outside the niche of people who look at the compositional zero-shot learning task.\n\nUpdate after rebuttal: I thank the authors for carefully addressing all the concerns in the rebuttal. I agree with the authors and reviewer  m1LH that the work is interesting in the context of compositional zero-shot learning. I am specially happy to see results for applying CLIP by learning the attribute embeddings without composition, and applying CLIP as it is. To reflect my updated thoughts on the paper, I am updating my score to 8",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper439/Reviewer_wi3v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper439/Reviewer_wi3v"
        ]
    },
    {
        "id": "Qj9eSoyReCa",
        "original": null,
        "number": 4,
        "cdate": 1666914495645,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666914495645,
        "tmdate": 1666914602163,
        "tddate": null,
        "forum": "S8-A2FXnIh",
        "replyto": "S8-A2FXnIh",
        "invitation": "ICLR.cc/2023/Conference/Paper439/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles the compositional zero-shot learning through the use of a large-scale pretrained vision-language model, specifically: CLIP. The paper takes the prompt-based classification approach where the cosine similarity between the text and vision embedding is used as the compatibility score. To improve this scheme, the paper takes a soft-prompt approach by fine tuning the prompt tokens' text embeddings. The fine-tuned object/attribute embeddings are then used in prompt-based zero-shot classification. T",
            "strength_and_weaknesses": "Strengths:\n- The paper presents a rather comprehensive experimental comparison with favorable results. The presented simple approach appears to be effective and overall quite well analyzed in terms of benchmark-based experimental comparisons.\n- The clarity of model selection details is a plus.\n\n\nWeaknesses: \n- The paper appears to be mainly an adaptation of a simple soft-prompting approach to the compositional zero-shot learning case. \n- As noted in the paper,  the underlying CLIP model is trained on a very large training set, even if that's possibly noisy. While CLIP has been used in various works for \"zero-shot classification\", it is hardly zero-shot; therefore, the problem can instead be seen synthesizing MxN-classifiers out of CLIP.  To this end, I find the criticism of \"task-specific architures\" in terms of adaptability and parameter complexity, at the end of Section 2 misleading and imprecise.\n- The model selection discussion emphasizes that all hyper-parameters except the unseen bias are tuned on the validation set wrt unseen performance. Here, I question the validity of tuning the bias term directly on the test set, which breaks the zero-shot setting, even if some prior work has taken this approach. What happens if you were to tune the bias term itself on the validation set, instead of the test set? How much performance drop would it cause? \n",
            "clarity,_quality,_novelty_and_reproducibility": "As noted above, the paper is clear, yet not so strong in terms of originality as it appears to be mainly an adaptation of a simple soft-prompting approach to the compositional zero-shot learning problem.\n\nA question regarding the clarity: do we expect the model to improve the unseen attribute and unseen class based cases at all?  (I guess not, unless the combination includes a seen attribute or a seen object, right?) ",
            "summary_of_the_review": "The paper is a good, mainly-experimental work on the soft-prompting based use of CLIP for compositional classification. The paper is clear and contains good experimental analysis. The technical novelty appears to be rather weak, the work is mainly a simple engineering solution (though I definitely do not question its value). I have concerns regarding the bias-term selection policy. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper439/Reviewer_1rcT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper439/Reviewer_1rcT"
        ]
    }
]