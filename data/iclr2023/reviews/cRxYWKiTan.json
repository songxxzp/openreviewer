[
    {
        "id": "E6STBt0GvS",
        "original": null,
        "number": 1,
        "cdate": 1666038735913,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666038735913,
        "tmdate": 1670675785673,
        "tddate": null,
        "forum": "cRxYWKiTan",
        "replyto": "cRxYWKiTan",
        "invitation": "ICLR.cc/2023/Conference/Paper2381/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the continual federated learning, the stream input data usually is provided non__iid which affects adversely on the learning performance.  The authors used generative replay idea for continual learning, in which there is no need to transfer data among the clients and servers (in federated learning framework), rather the learned parameters can be shared. Namely, Auxiliary classifier GAN (ACGAN), a study from Odena et al. (2017) has been used as the generative model to avoid forgetting of the previously learned tasks. As the mentioned non_iid nature of the incoming data makes the federated learning difficult and using ACGAN as the generative replay fails, the authors proposed two technical tricks which could help this combination succeed.  The first trick, called as model consolidation, in which the clients parameters are aggregated and used to initialize the server parameter. Then, from the clients generators, balanced synthetic data is generated to use for enhancing training in the server side.\nAs the second trick, \u201cconsistency enforcement\u201d, is performed in the client side. Each client\u2019s loss includes two terms. In the first term, the new coming training data  are mixed with the synthetic data generated by the global generator conditioned on the labels from previous tasks, hence the client not only learns the new classes, but also retains the previously learned classes. \nFor the second term, consistency loss function, the idea is to include kullback divergence to enhance generating the same distribution for the real data and the data generated from the global model generator and the generated data of the past or current task generators, when the label is the same. ",
            "strength_and_weaknesses": "The paper is well written and the topic is well motivated in the introduction. The contribution is clearly mentioned and explained.\nThe experimental results seem to outperform the state of the art approaches.\n\nIt mainly addresses the instability in this specific learning framework by enhancing the generalization of both clients and servers. Consequently, some locality should be sacrificed. Have you observed this in your experiments?\n\n\nThe proposed tricks seem to be effective and interesting however I doubt the significance of the contributions make it succeed to be published as a full paper at ICLR.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The core idea is clearly presented and the presented tricks are novel. The study has reasonably good quality. ",
            "summary_of_the_review": "At this point, this paper is a border-line study in my opinion. \n ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2381/Reviewer_qLZV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2381/Reviewer_qLZV"
        ]
    },
    {
        "id": "8TT9nWpKdG",
        "original": null,
        "number": 2,
        "cdate": 1666693470558,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666693470558,
        "tmdate": 1666693470558,
        "tddate": null,
        "forum": "cRxYWKiTan",
        "replyto": "cRxYWKiTan",
        "invitation": "ICLR.cc/2023/Conference/Paper2381/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a continual federated learning problem where each client deals with a class-continual learning problem with a sequence of tasks independently, and the server learns the tasks of all clients via simple model averaging. This paper observes that the aggregate server model suffers high loss and FID score on individual client data, and proposes two methods (model consolidation and consistency enforcement) to handle this issue. Extensive experiments on multiple classification datasets show that their method has significant advantages over baseline methods which naively adopt data-free class-incremental learning approaches on a federated learning framework.",
            "strength_and_weaknesses": "### Strong points\n\n- This paper introduces continual federated learning, which is a challenging but more realistic problem than existing federated learning. Continual learning without a memory buffer in the federated setting is an interesting and important problem\n- The idea of finetuning the server model using individual clients\u2019 generators (expressed as \u201cmodel consolidation\u201d in the paper) seems original to me. Since the server can receive the generators trained with individual local data distributions, fine-tuning the averaged server model with the synthetic data is a straightforward approach for transferring local knowledge to the server model.\n\n### Weak points\n\n- The experiments are difficult to understand, and missing details.\n    - How many communications are performed for each task?? It seems that the degenerated server aggregation issue pointed out in the paper can be alleviated via enough communication per task.\n    - How the classification nodes are added and aggregated when training the next task?\n    - How many synthetic images are generated for each generator per iteration?\n- Evaluation on the more challenging datasets is required such as CIFAR-100 or ImageNet\n- Each client should transfer the generator to the server for every communication round, which can raise privacy concerns since we can easily synthesize the raw data using the generator.\n- The writing is hard to follow, a few statements are vague, and notations are imprecise\n    - Why do the proposed two methods mitigate the increased loss issue at server aggregation?\n    - Subscript at $\\theta$ denotes a client index in the left figure in Figure 1, but the subscript denotes task index in other parts of the paper, e.g., eq. 11\n- Questions\n    - $\\theta_{\\text{global}}$ in Eq. 10 only denotes the parameter of the generator? or denotes parameters of the generator, discriminator, and classifier?\n- FedCIL requires more communication cost than FedAvg or FedProx + DGR since each client communicates all parameters of the model including generator, discriminator, and classifiers.\n\n### Minor comments\n\n1. Typos\n    1. Page 5: federated leading \u2192 federated learning\n2. The paper should include a discussion about the recent federated learning approach [1].\n\n\n### Reference\n\n[1] J. Dong et al., Federated Class-Incremental Learning, CVPR, 2022",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is not written well and is hard to follow. However, the new problem setup and the method proposed in this paper are quite original.",
            "summary_of_the_review": "While the writing needs to be improved and a more comprehensive evaluation is required, I believe that this paper has two major contributions: 1) it proposes a realistic continual federated learning problem, 2) it tackles the problem of training a GAN-based model in federated learning in the presence of data heterogeneity over clients.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2381/Reviewer_S7Qy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2381/Reviewer_S7Qy"
        ]
    },
    {
        "id": "Ob2n2VfcCpB",
        "original": null,
        "number": 3,
        "cdate": 1667664861279,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667664861279,
        "tmdate": 1670530265477,
        "tddate": null,
        "forum": "cRxYWKiTan",
        "replyto": "cRxYWKiTan",
        "invitation": "ICLR.cc/2023/Conference/Paper2381/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the continual federated learning problem which has properties of both federated learning and continual learning, and proposes modifications such as model consolidation and enforced consistency to stabilize ACGAN to solve the proposed problem. The proposed approach achieves the best performance in the proposed setting, and achieves stable training similar to single-model learning of ACGAN.\n",
            "strength_and_weaknesses": "Strength\n1. The work is reasonably well written and easy to follow.\n2. The empirical results of the proposed approach (Table 1) is strong compared to baselines.\n\nWeakness\n1. Overclaiming. The authors frame \u201cproposing the continual federated learning problem\u201d as a major contribution, but much of the related work already focuses on various forms of continual federated learning despite slight difference from the proposed setting (e.g., class overlap). \n2. Relevance. It felt to me that the authors curated a very narrow setting for which their method can outperform existing baselines (e.g., multi-task, global model, no class overlap, etc). If this curated setting is indeed highly relevant, it would be nice to motivate the proposed setting with practical examples.\n3. It is unclear from the experimental results how crucial model consolidation and enforced consistency are in stabilizing ACGAN. Ablations or inclusions of other approaches to stabilization can help with this.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly. The work is not as novel as claimed. The code was not submitted so reproducibility could not be evaluated.",
            "summary_of_the_review": "Due to concerns around the relevance of the problem, the lack of practical motivation, and the overclaimed contribution, I recommend reject for the manuscript.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2381/Reviewer_sMxb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2381/Reviewer_sMxb"
        ]
    }
]