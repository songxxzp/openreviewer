[
    {
        "id": "clSoLn17p-r",
        "original": null,
        "number": 1,
        "cdate": 1666620233751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620233751,
        "tmdate": 1666620233751,
        "tddate": null,
        "forum": "1FsdIfRngtw",
        "replyto": "1FsdIfRngtw",
        "invitation": "ICLR.cc/2023/Conference/Paper6510/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper offers a set of experiments around prompting methods for contrastive vision-language models.\n\nIn particular, the following experiments are presented:\n\n1) Handcrafted & random prompts.\nA number of simple templates are used to do image classification on 11 datasets on top of a CLIP pre-trained model. In addition, two types of random prompts are included in the comparison. The conclusion is that handcrafted prompts do well --some better than others--, even those that oppose our language logic (i.e. \"this is not a photo of\") seem to do a fairly decent job. Random prompts are inferior, but still offer non-trivial (sometimes even strong) zero-shot performance.\n\n2) Prompt learning versus Classifier Fine-tuning.\nMoving away from zero-shot evaluation into the realm of supervised tasks, the paper compares prompt learning (adding learnable parameters for the embeddings of the prompt tokens + finetuning only those) and classifier fine-tuning. The latter, if I understood it correctly, rather than learning the original embeddings for the prompting tokens, directly learns the final representation for each possible class. These will be compared against the final embedding for the input image, and pick the closest class as usual in contrastive learning. Basically, the pre-trained language tower is ignored here. The experiment focuses on a few-shot regime (between 1-shot and 16-shot). It concludes that classifier fine-tuning is almost always superior to prompt learning, and also computationally cheaper / faster.\n\n3) Robustness to Distribution Shifts.\nFollowing up on 2), prompt learning and classifier fine-tuning are evaluated in terms of robustness. Table 4 shows the results of finetuning on ImageNet and then evaluating on other related (out-of-distribution) ImageNet datasets, for a variety of vision towers. While it is not reasonable to expect Zero-shot CLIP to perform as well as finetuned methods, we see that the classifier fine-tuning overall works best (more robust, in this case).\n\n4) Optimality-Generalization Trade-off.\nThis one I didn't fully get. It seems to me one model is trained with CoOp for varying amounts of epochs, and results show that it dominates (in terms of original finetuning and out of distribution evaluation) a new technique (CoCoOp) that uses an auxiliary network.",
            "strength_and_weaknesses": "Overall, the paper seems to be a set of interesting experiments, but with no clear takeaway. While I definitely think there's value in these experiments, I don't think the work is at the publication maturity level required by ICLR at the moment.\n\nQuestions.\n\n- I don't think Linear Probe CLIP is formally defined or explained anywhere. What's the exact method here?\n- In Table 4, what's the amount of few-shot examples per class used for finetuning?\n- I think the classifier finetuning method isn't clearly explained, I was confused during a first reading. Maybe add a diagram or an example.\n- I really struggled to understand Section 6. I think some major re-writing and experiment explanation is needed here.\n- Figure 2 could benefit from a descriptive caption.\n\n- Is the classifier finetuning method a new contribution? Or has this idea been tried elsewhere before? If it's a contribution, it should be made clear (and maybe even re-arrange the story and sections of the paper around it).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper could benefit from some extra work on the writing and exposition side. Some of the methods are not formally and thoroughly presented, and some of the results (tables or plots) do not contain complete captions and lack key contextual information.\n",
            "summary_of_the_review": "The paper offers four interesting experiments around prompting (handcrafted, learned, finetuning \"final class tokens\", robustness, etc). While promising, I feel the paper still requires some work, including a more cohesive flow, and a bit more structured and unified takeaways.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6510/Reviewer_2dEp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6510/Reviewer_2dEp"
        ]
    },
    {
        "id": "0sIVFi0b3ED",
        "original": null,
        "number": 2,
        "cdate": 1666819736943,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666819736943,
        "tmdate": 1666819736943,
        "tddate": null,
        "forum": "1FsdIfRngtw",
        "replyto": "1FsdIfRngtw",
        "invitation": "ICLR.cc/2023/Conference/Paper6510/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper focuses on analyzing the prompt learning paradigm in CLIP. The authors discuss some observations on the use of various types of hand-crafted prompts including class names, basic prompts (e.g., \"a photo of a {CLASS}\"), negative prompts (e.g., \u201cthis is not a photo of a {CLASS}\u201d), and random prompts on 11 benchmark datasets. The highlighted observations include no major drop in performance using negative prompts. Additionally, the paper analyzes classifier fine-tuning as a faster alternative to prompt learning and shows superior performance compared to prompt learning. Experiments on few-shot classification, robustness to distribution shifts, and generalization to unseen classes are included using the CLIP model as the backbone.",
            "strength_and_weaknesses": "**Strengths**:\n\n- The paper is well written and easy to follow.\n- The paper has tried towards analyzing the effect of different compositional handcrafted prompts and studying the performance behavior on several downstream classification datasets.\n\n**Weaknesses**:\n\n- Although the paper states \u201cVision-Language Models\u201d in its title, the experiments are only performed on CLIP models. It would be great to see similar findings for other vision-language models like DeCLIP, FILIP, CLOOB, CyCLIP, etc. Given that the paper is more of an analysis paper instead of a methodology paper, I would expect authors to verify their claims on other CLIP models like CLIP ViT variants beside CLIP ResNet. Does the size of models effect the conclusions presented in this paper? A more thorough comparison and anlysis should be included in the paper.\n- It is interesting to observe that the four types of handcrafted templates used for zero-shot classification provide accuracies in a similar range, but this requires further extensive analysis to draw concrete conclusions. The {CLASS} token is very much important in classifying the images and is present in all the templates which provides the major information to the text encoder. The negative prompt has all the words the same as the revised prompt except \u201cnot\u201d, obtaining accuracies in the similar range is expected as the text encoder is a language model and they lack compositionality which is well studied in NLP [a], and also the way CLIP has been pre-trained using the contrastive loss doesn\u2019t guarantee compositionality. For the random prompts using random tokens and embeddings, the same argument of the presence of the {CLASS} token applies. Thus, a fair comparison between different prompts would be to remove the class token and then compare their performance on the downstream tasks. It might require some modifications at the output as done in \"Learning to Decompose Visual Features with Latent Textual Prompts\".\n- The paper proposes classifier fine-tuning as a faster alternative to prompt learning and shows an average improvement of around 1.5%, but there is no parameter analysis (i.e., number of tunable parameters) provided for this experiment. It is known from the CoOp paper [b] that ensembling improves the performance, I assume that the authors fine-tuned the final projection layer of the text encoder which is of dimension 512x1024 for CLIP-ResNet-50, which has almost 64 times more parameters than learning 16 prompt tokens of 512 dimension. Therefore, it is very much essential to have a fair comparison with CoOp wrt the number of parameters each method uses. A fair comparison would be to use 64 learnable prompts and run CoOp for it. But, I believe from observing the results provided in CoOp [b], only 8 learnable prompts would be able to perform as good as the classifier fine-tuning. This argument applies to all the results in Figure-1 and Table-4.\n- Additionally, the very motivation to learn continuous prompts for large vision and language models was to efficiently adapt the knowledge in the models to downstream tasks, which makes classifier fine-tuning contrasting to the motivation of prompt learning. I would like the authors to discuss on how classifier finetuning is a better alternative to prompt tuning for parameter efficient adaptation of large vision-language models for diverse downstream tasks.\n- It would be good to add the average values across the datasets in table-2.\n- No experimental comparison with other parameter efficient adaptation methods for CLIP, like CLIP-adapter [c], Tip-Adapter [d], UPL [e], PDL [f] etc. has been made in the paper. Authors should compare with these methods to verify the effectiveness of the proposed method over existing methods.\n- Figure-2 is not clear. What are the epoch values corresponding to the points for CoOp?\n- I feel that the section on optimality-generalization trade-off is very subjective and lacks proper experimental evaluation and theoretical support. In the beginning the authors ask the question - \u201cwhy would the prompt learning methods or the improved conditional prompt learning methods have strong generalization ability?\u201d, but get an answer - \u201cOur assumption is that, due to the changed architecture, the improved prompt learning method maybe actually trying to find a better optimality-generalization trade-off.\u201d I am confused on what it means. Additionally, I am not sure how the experiment of training CoOp for multiple epochs helps support the claims. Isn\u2019t it known that training for a higher number of epochs makes networks susceptible to overfitting and is expected to lose generalizability? Also, why was only CoOp run for multiple epochs and not CoCoOp?\n\n[a] Evaluating Compositionality of Sentence Representation Models: https://aclanthology.org/2020.repl4nlp-1.22.pdf\n\n[b] Learning to Prompt for Vision-Language Models: https://arxiv.org/pdf/2109.01134.pdf\n\n[c] CLIP-Adapter: Better Vision-Language Models with Feature Adapters: https://arxiv.org/pdf/2110.04544.pdf\n\n[d] Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling: https://arxiv.org/pdf/2207.09519.pdf\n\n[e] Unsupervised Prompt Learning for Vision-Language Models: https://arxiv.org/pdf/2204.03649.pdf\n\n[f] Prompt Distribution Learning: https://arxiv.org/pdf/2205.03340.pdf\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Although the paper is well-written, the paper lacks clarity on what exactly it tries to study and mitigate. The experimental analysis is not extensive and lacks proper justification to them. ",
            "summary_of_the_review": "I think the identified problem is important but I\u2019d like to rate the current submission as a clear rejection due to limited technical contributions and lack of convincing experiments. The paper needs significant changes including new experiments and possibly methodological improvements before being accepted to any major conference.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8"
        ]
    },
    {
        "id": "2RKD6wPOZK4",
        "original": null,
        "number": 3,
        "cdate": 1667442932039,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667442932039,
        "tmdate": 1667442932039,
        "tddate": null,
        "forum": "1FsdIfRngtw",
        "replyto": "1FsdIfRngtw",
        "invitation": "ICLR.cc/2023/Conference/Paper6510/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper analyzes the behavior of CLIP towards various compositions of hand-crafted text prompts and puts out some observations. One of the key observations of the paper is the behavior of CLIP to negative prompts, in which the authors insert a \u201cnot\u201d word to the prompt, e.g. \u201cthis is not a photo of a dog\u201d. The paper shows that the zero-shot classification performance of CLIP is almost the same using negative prompts as using default prompts without the \u201cnot\u201d word included. The paper also shows that the use of random tokens appended with the class name as prompts also give considerable performance. In the second part of the paper, the authors propose classifier fine-tuning in which they fine-tune the final layer of the text encoder instead of learning the prompts. The authors argue that this tackles the speed issues of prompt learning by not needing to backpropagate gradients through the whole of the CLIP model and gets better classification performance. Experiments include zero-shot classification using various hand crafted and random prompts, few-shot classification, robustness to distribution shifts and studies on optimality and generalization. Results are shown on 11 datasets.",
            "strength_and_weaknesses": "Strengths:\n\n* The observations shown in the paper regarding the performance with various types of handcrafted prompts and their compositional variations is interesting.\n\nWeaknesses/Questions/Suggestions:\n\n* The experiments are only on the CLIP model, while the paper title says Vision and Language models.\n* No parameter count included for classifier fine-tuning experiments. It is very much important to keep a track of the parameter count while proposing an alternative method for prompt tuning. An increase in parameters can increase the classification performance. How many parameters are tuned for classifier fine-tuning? By what number is it larger than CoOp? The parameter count needs to be the same for both the methods for an apples to apples comparison.\n* What is the experimental setup for Figure-2? What are the epoch values?\n* What is the justification for the experiment in Figure-2? Why is CoCoOp not run for multiple epochs?\n* Experimental comparison with different prior works on prompt tuning is missing. Some examples include: Unsupervised Prompt Learning (UPL), Prompt Distribution Learning (ProDA), etc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the paper is an analysis paper of existing prompt learning methods, but lacks clarity on the message it wants to convey, without proper justification of the claims, and comprehensive experimentation. No supplementary material and codes are provided for better reproducibility.",
            "summary_of_the_review": "All of my concerns are mentioned in the weaknesses section. I believe it is important to have clarity and proper experimental analysis which the paper lacks. I believe the paper needs to go through multiple iterations of revisions and rethinking on how to properly analyze and convey the findings for being fit for a publication at a venue like ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6510/Reviewer_pGd5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6510/Reviewer_pGd5"
        ]
    },
    {
        "id": "CvwV6F87Yky",
        "original": null,
        "number": 4,
        "cdate": 1667460820099,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667460820099,
        "tmdate": 1667460981143,
        "tddate": null,
        "forum": "1FsdIfRngtw",
        "replyto": "1FsdIfRngtw",
        "invitation": "ICLR.cc/2023/Conference/Paper6510/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, prompt learning is reexamined, and several unexpected findings that defy accepted notions of the prompt are presented. First , random prompts without learning or fine-grained design may likewise function effectively in zero-shot recognition. Second, direct linear classifier fine-tuning performs more effectively than prompt learning. Furthermore, prompt learning is essentially a subset of parameter-efficient learning and represents a trade-off between generalization and optimality.  Findings across 11 datasets show that the approach presented in this research can significantly influnce the use of trained vision-language models in subsequent challenges.",
            "strength_and_weaknesses": "Strength\n1. The experiments are sufficient and exhaustive.\n2. The results provide the valuable hints that what's the better way to deploy the pretrained vision-language model.\n3. The paper inspire people to think about more effective prompt design.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Weakness,\n\n1. The results are sufficent and the conclusion are well established but the reasons behind the result need to be more explored. For example, why classifier fintuning is much better than learning prompt?\n\n2. The novelty is limited but as a rethinking paper, it is fine.\n\n3. Can other models except clip still support findings?\n",
            "summary_of_the_review": "Overall, this paper provides some valuable hints about prompts but the novelty is limited. Therefore, I give my initial rating as borderline \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6510/Reviewer_4WBY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6510/Reviewer_4WBY"
        ]
    }
]