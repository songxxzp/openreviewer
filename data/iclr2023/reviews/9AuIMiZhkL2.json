[
    {
        "id": "ahOeQM9GXQf",
        "original": null,
        "number": 1,
        "cdate": 1666558450614,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558450614,
        "tmdate": 1669655991997,
        "tddate": null,
        "forum": "9AuIMiZhkL2",
        "replyto": "9AuIMiZhkL2",
        "invitation": "ICLR.cc/2023/Conference/Paper2474/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a self-supervised framework for time series representation learning. It randomly masks out tokenized time series and learns an autoencoder to reconstruct them at the point-level. The proposed model is applied to time series forecasting and clasification.\n\n",
            "strength_and_weaknesses": "Strength\n1. The authors clearly presents the idea of using masked time series autoencoders for time series forecasting and classification.\n2. They also did extensive experiments on multiplier real datasets and compared the proposed method with SOTA ones.\n\nWeaknesses\n1. \"input time series are assumed to follow an integrate distribution\"\nWhat does integrate distribution mean? \n2. In (1), the polynomial series and the Fourier series are not orthogonal thus the learning might have no unique solution. How are the number of polynomial orders and the Fourier orders decided? \n3. It is not clear why 'randomly masks out parts of embedded time series data' can help 'alleviate the distribution shift problem'. Why periodic masking cannot address the same issue? Why do we need randomness here?\n4. The way the time series data is randomly masked will affect the performance and is worth being discussed.\n5. How can the proposed Ti-MAE make forecasting 'for multiple time windows with various sizes without re-training'? This claim in Section 1 is not justified neither structurally or experimentally.\n6. The input time series and the output one has the same dimensionality. This is a quite strong but not necessary assumption. Can the proposed model be applied to forecasting time series with different dimensionality?\n7. The ablation study is not complete. Why does data augmentation give worse results? How the way that masks are randomly sampled (as mentioned in 4) affect the performance?\n8. It will be helpful to show the training and testing errors and the generalization gap between them.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The authors give a relatively clear high-level description of the proposed method.\n2. The idea is marginally novel considering that auto-encoder has been widely used in supervised learning tasks including time series forecasting.\n3. There are many unclear details, e.g, how the parameters of encoder and decoder are specified. Without these details, it is hard to reproduce the results in the paper.",
            "summary_of_the_review": "The idea is marginally novel and the applications on time series forecasting and classification are interesting.\nThere are some claims not well justified and need to be addressed.\n\n\n------------\nI have read the authors' responses and would like to keep the same score. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2474/Reviewer_o7kt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2474/Reviewer_o7kt"
        ]
    },
    {
        "id": "lKkozl4sLt",
        "original": null,
        "number": 2,
        "cdate": 1666628040849,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628040849,
        "tmdate": 1670676261586,
        "tddate": null,
        "forum": "9AuIMiZhkL2",
        "replyto": "9AuIMiZhkL2",
        "invitation": "ICLR.cc/2023/Conference/Paper2474/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work propose a simple and novel self-supervised learning framework for time series representation learning. Instead of using contrastive representation learning, this work directly follow the masked data modeling and optimize the reconstruction loss of the randomly masked data points. Empirical studies on real-world datasets and ablation studies show the effectiveness and scalability of the proposed framework.",
            "strength_and_weaknesses": "Pros:\n\n1. The proposed MAE based self-supervised learning framework is interesting and reasonable. Experimental results also demonstrate improvement on most real-world datasets.\n\n2. The paper is well-organized, and the writing is easy to follow.\n\n\nCons:\n\n1. The experiments are not comprehensive. The baselines reported in the current version are not SOTA. How are the comparison results with the recent transformer based models, like FEDformer and ETSformer as the authors mentioned? Note that there are some lightweight models (Dlinear, DeepTIME, etc.) which can beat the transformer based models. It would be more convincing to compare these models.\n\n\n2. The proposed method is reasonable, but the novelty is limited. It is quite straightforward to apply MAE on time series data. Could the authors elaborate more on the challenges when applying MAE on time series data?\n\nQuestions:\n\n1. Does the proposed method follow the pre-training + fine-tuning paradigm? It seems the randomly mask strategy could align the reconstruction and prediction tasks and eliminate the effort to build an extra forecasting model.\n\n2. How is the model complexity compared with other transformed based forecasting models?",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is clear and reasonable, but novelty is limited. The empirical results would be more convincing to include the STOA results on long sequence forecasting task.",
            "summary_of_the_review": "see above\n____\n\nI would like to thank the author's great effort for the detailed response and additional experiments. However, I would like to keep the original score due to the following concerns. First, the novelty of methodology and technique are limited. I appreciate the explanation of MAE and the proposed method, but I still feel the proposed method is a straightforward solution to apply it on time series data.  Second, I understand the point from the author that there is no need to compare works unpublished, like deeptime, dlinear. But there exists some published paper which achieves much better results than the baselines selected in this manuscript, for example [1,2]. It would be promising  to see a significant improvement with a simple method in this time series forecasting benchmark. Third, the proposed method has quadratic complexity which is worse than the $L\\log L$ complexity of the baseline transformers.\n\n[1] Learning Latent Seasonal-Trend Representations for Time Series Forecasting\n[2] FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2474/Reviewer_5eEm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2474/Reviewer_5eEm"
        ]
    },
    {
        "id": "A0Z2uo2QpJ",
        "original": null,
        "number": 3,
        "cdate": 1666717729269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717729269,
        "tmdate": 1666717729269,
        "tddate": null,
        "forum": "9AuIMiZhkL2",
        "replyto": "9AuIMiZhkL2",
        "invitation": "ICLR.cc/2023/Conference/Paper2474/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper propose a masked time series autoencoders which can learn strong representations with less inductive bias or hierarchical trick. The proposed Ti-MAE bridges the connection between contrastive representation learning and generative Transformer-based methods. Ti-MAE adequately leverages all the input sequence and alleviates the distribution shift problem. The flexible setting of masking ratio makes Ti-MAE more adaptive to various prediction scenarios with different time steps. Experimental results demonstrate the efficacy of proposed method.",
            "strength_and_weaknesses": "Strengths:\n1. It's novel to propose the masked time series auto encoders which can learn strong representations with less inductive bias or hierarchical trick.\n2. The experiments are evaluated on various of settings, e.g. multivariate time series forecasting, classification\n\n\nWeaknesses:\n1. The masking ratio is tricky. It highly depends on the dataset. When dataset changes, the masking ratio may change. The paper didn't mention how to deal with this issue.\n2. The experiments do not compare with some state-of-the-art method, such as NHITS, FedFormer and etc.\n3. No ablation study on the effect of different backbone models, e.g. LSTM, Transformer, TCN and etc.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is pretty clear. The figures are easy to read. The novelty is marginal. It looks reproducible.",
            "summary_of_the_review": "This paper proposes Ti-MAE to bridge the connection between contrastive representation learning and generative Transformer-based methods. It can learn strong representations with less inductive bias or hierarchical trick. Ti-MAE adequately leverages all the input sequence and alleviates the distribution shift problem. The flexible setting of masking ratio makes Ti-MAE more adaptive to various prediction scenarios with different time steps. Experimental results demonstrate the efficacy of proposed method.\n\nHowever, the masking ratio is tricky. It highly depends on the dataset. When dataset changes, the masking ratio may change. The paper didn't mention how to deal with this issue. The experiments do not compare with some state-of-the-art method, such as NHITS, FedFormer and etc. No ablation study on the effect of different backbone models, e.g. LSTM, Transformer, TCN and etc.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NO",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2474/Reviewer_jXSb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2474/Reviewer_jXSb"
        ]
    },
    {
        "id": "yJDkWdFytS",
        "original": null,
        "number": 4,
        "cdate": 1666879036786,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666879036786,
        "tmdate": 1666879036786,
        "tddate": null,
        "forum": "9AuIMiZhkL2",
        "replyto": "9AuIMiZhkL2",
        "invitation": "ICLR.cc/2023/Conference/Paper2474/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an adaptation of the popular MaskedAE model for timeseries data. ",
            "strength_and_weaknesses": "S\n\n- solid experimental work in terms of datasets and models compared\n- good results with multiple ablation tests\n\nW \n\n- lack of connection between motivation (disentanglement) and the actual proposed method\n- non-significant result compared to TS2Vec baseline (figure 6)\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work adapts an existing model to timeseries data. The work is of high-quality and clarity.",
            "summary_of_the_review": "I am positive about the contribution of this paper which might be considered a bit incremental, however, it's the first work in this area. I am reluctant about the comparison with TS2Vec in Figure 6 which shows that both methods are identical. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2474/Reviewer_6CpA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2474/Reviewer_6CpA"
        ]
    }
]