[
    {
        "id": "CxijjHg5vvn",
        "original": null,
        "number": 1,
        "cdate": 1666111093296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666111093296,
        "tmdate": 1672736823183,
        "tddate": null,
        "forum": "Y2E5-_HL0DV",
        "replyto": "Y2E5-_HL0DV",
        "invitation": "ICLR.cc/2023/Conference/Paper5743/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces the MUST framework (multiple user simulators), for training Task-oriented Dialogue systems with Reinforcement Learning and incorporating multiple user simulators.\n\nThe usual way to train these models is with a single system and user simulators but, as the authors correctly point out, a single user simulator might fail to exhaustively capture all possible human behaviors. This paper contributes a stepping stone towards this objective by casting the problem of combining different user simulators as a multi-armed bandit problem. The authors show that this method provides better results than using simpler combination baselines or single user simulators during training.\n",
            "strength_and_weaknesses": "# Sumary of strengths\n\n* To the best of my knowledge, the idea of combining multiple user simulators by casting this problem as a multi-armed bandit is novel.\n* The way the idea is executed is not optimal (experiments on limited dataset), but the authors acknowledge this shortcoming. Despite the limitations of the paper's empirical grounding, its ideas and the evidence it does could provide open interesting avenues for future research on the interaction between user and agent simulators.\n* The paper provides directional evidence showing that combining several user simulators is preferable to not doing so. This is reflected both in terms of overall success rate as measured automatically and by humans, and by showing that models trained through this method are less sensitive to unseen user simulators.\n* It also evidences that adapting the distribution guiding the choice of user simulator during training results in faster training convergence.\n\n# Summary of weaknesses\n\n* There are some paragraphs that are poorly written, and make some key concepts difficult to understand.\n* There are some conceptual gaps that make me doubt about the overall rigor of the paper.\n* The more complex $MUST_{adaptive}$ model is only slightly better than the simpler $MUST_{uniform}$ baseline as measured by the in-domain automatic and human evaluations.\n\n\n# Suggestions and questions for the authors.\n\n* Good work! Do you intend to release the code in case the paper is accepted?\n* In the second paragraph of page 2 you mention that \"Extensive experimental results [...] show that the dialogue system trained by our proposed MUST achieves a better performance than those trained by any single user simulator\", however I feel like the amount of experimentation described is not extensive, so I would suggest toning down this statement.\n* In page 4 you introduce $MUST_{CRL}$ but you don't include that model in any of your results. In the paragraph \"Challenges to leverage multiple user simulators\" you mention some challenges related to this model. Are these challenges the reason why you did not analyze it? If that is the case then I suggest not giving it its own item (II) above, and instead give an overview of the \"adaptive\" variant of your framework. If this is not the case, then why was it not included?\n* In the same \"Challenges to leverage multiple user simulators.\" paragraph you mention: \"unnecessary efforts will be costed for easily-adapted user simulators\". I suggest rephrasing this statement as I don't think it's understandable in its current form.\n* In the first paragraph of page 5 you link the \"uniform adaptation\" to reducing the catastrophic forgetting issue. However there is no experiment providing evidence that without \"uniform forgetting\" there's catastrophic forgetting. I suggest rephrasing this, or providing evidence that this is actually the case.\n* In the first paragraph of section 5.2.2 you mention: \"$SyS-MUST_{merging}$ is trained by $GPT_{IL}$ for implementing $MUST_{merging}$ strategy\". First of all the statement is redundant by mentioning the \"merging\" strategy twice, but more importantly, I had understood that the merging strategy implied sampling dialogues from a set of user simulators and use these to train the final model. Does this statement mean that you generated dialogues with $GPT_{IL}$, which was trained on dialogues produced by 3 agenda-based simulators and MultiWoZ restaurant data, and define this method as sampling dialogues from several simulators?\n* In the \"Automatic Evaluation\" paragraph of section 5.3 you say that the reason for the \"merging\" strategy not performing as well as the \"uniform\" and \"adaptive\" could be \"because the merging strategy cannot effectively leverage multiple user simulators\" which is a cyclical explanation. I suggest removing it or providing a more plausible explanation.\n* In the third paragraph of section 5.3 you mention that the \"uniform\" and \"adaptive\" strategies achieve 2.4 absolute value improvements\", but improvement over what?\n* Finally, I suggest providing more details about the human evaluation. In Appendix B.2 you give some more details and mention that you \"tell them how to judge if the generated dialogue is successful\", \"them\" being the evaluators. But what was your definition of a successful dialogue? Every slot filled, for example? or something else?\n\n\n\n# Typos and minor corrections\n\n* Page 1, paragraph 4, line 3: best-performed -> best-performing\n* P. 2, p. 1, l. 10: You reference challenges $\\textit{i}$ and $\\textit{ii}$ but you used 1 and 2 in the same paragraph for describing challenges. In the abstract you use $\\textit{i}$ and $\\textit{ii}$ for referring to the two types of adaptation rather than the challenges they address. I suggest being consistent with numbering.\n* P. 2, p. 2, l. 5: Here you mention \"$MUST_{adaptive}$ is indeed more efficient for leveraging multiple user simulators by our visualization analysis\", which makes me wonder: more efficient than what? and what visualization analysis do you mean? I suggest clarifying this in the paper.\n* P. 2, p. 3, l. 8: convergences -> converges\n* P. 2, p. 6, l. 4: if accomplishing -> if it is accomplishing\n* P. 3, p. 1, l. 1: Once the database result -> When the database result\n* P. 4, p. 2, l. 1: first sample -> first samples\n* P. 4, p. 2, l. 4: by RL algorithms -> with RL\n* P. 4, p. 4, l. 11: Not sure what is meant by \"unnecessary efforts will be costed for easily-adapted user simulators\". I suggest rephrasing this.\n* P. 4, p. 6, l. 1: recalls us a similar thought -> reminds us of a similar concept\n* P. 4, p. 6, l. 3: weakly-performed -> weakly-performing; well-performed -> well-performing\n* P. 4, p. 6, l. 4: \"should reduce the interaction with user simulators that dialogue system has performed well and allocate more interactions with those user simulators that dialogue system has not performed well\" -> \"should reduce the interaction with user simulators with which the dialogue system has performed well and increase interactions in the opposite case.\"\n* P. 5, p. 2, l. 5: masker's -> maker's\n* P. 5, p. 4, l. 7: \"p is expected to assign lower weights to user simulators that the system agent S already performs well and higher weights to those user simulators that S performs not well\" -> \"p is expected to assign lower weights to user simulators with which the system agent S already performs well and higher weights to those user simulators with which S performs poorly\".\n* P. 5, p. 7, l. 2: what do you mean with \"latter\" and \"former\" terms? Do you mean $\\underbrace{\\bar{x}_j}_\\{\\text\\{exploitation\\}\\} + \\underbrace{\\sqrt{\\frac{2\\ln t}{T_\\{j,t\\}}}}_\\{\\text\\{exploration\\}\\}$ in equation 2?\n* P. 5, p. 8, l. 2: has been interacted so far -> has been interacted with so far\n* P. 5, footnote, l. 2: \"Then the index of the arm will be played from t = K + 1 to T is the sum of two terms: ...\" makes no grammatical sense. Please correct it.\n* P. 6, Algorithm 1: I suggest changing the verbs from -ing form to imperative, so initializing -> initialize, synthesizing -> synthesize, using -> use, evaluating -> evaluate, updating -> update. I also suggest clarifying what the lowercase s mentioned in the input is used for.\n* P. 6, p. 1, l. 2-3: I think here you used both $\\tau$ and $s$ to refer to the smoothing factor for distribution $\\boldsymbol{p}$\n* P. 6, p. 3, l. 3: \"that the dialogue system has performed well\" -> \"with which the dialogue system has performed well\"\n* P. 7, p. 4, l. 4: \"test the systems by them\" -> \"test the systems with them\"\n* P. 7, p. 4, l. 5: \"there usually has a gap\" -> \"there usually is a gap\"\n* P. 9, Figure 2: There's no need to write \"Tested by\" for each subfigure. The name of the user simulator is enough. Also the label for (a) seems to be wrong.\n* P. 14: There's mention to U-GPT here, but nowhere in the main text.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is novel, but clarity and quality could be improved (see comments above for more details) ",
            "summary_of_the_review": "I think the idea is interesting, and that it could inspire interesting follow ups, however the execution leaves a bit to be desired. I will set my score to \"marginally above the acceptance threshold\" due to the lacking points I mentioned in detail above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5743/Reviewer_44kw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5743/Reviewer_44kw"
        ]
    },
    {
        "id": "1ZB8mSvQHYy",
        "original": null,
        "number": 2,
        "cdate": 1666541554394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541554394,
        "tmdate": 1666541554394,
        "tddate": null,
        "forum": "Y2E5-_HL0DV",
        "replyto": "Y2E5-_HL0DV",
        "invitation": "ICLR.cc/2023/Conference/Paper5743/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce a multi-armed bandit approach to training task-oriented dialog (TOD) systems to leverage multiple user simulators. This approach is demonstrated to outperform ablative variants that suffer from catastrophic forgetting and over-fitting to specific \"easy\" simulators.",
            "strength_and_weaknesses": "Strengths:\n- MUST as an adaptive approach seems well-justified\n- Discussion of learning curves motivates further study of MUST as a possible adaptive approach for low data regimes\n- Experiments explicitly target enumerated challenges with existing user simulators and mixtures (catastrophic forgetting, over-fitting to specific simulators)\n\nWeaknesses:\n- It is important to see empirically if MUST generalizes to a larger or smaller pool of user simulators and is not over-tuned for the current set chosen in the paper. For example, ablation studies with different subsets of the user simulators would be helpful\n- Needs more discussion of the limitations on user behavior diversity etc. given that only the Restaurant domain is evaluated here. It is unclear from the experiments here how well the approach can succeed across multiple domains where the pool of user simulators potentially explodes\n- Human evaluation details are relatively scarce - would like to see justification for having human raters apparently rate the success rate (the same as the automatic evaluation metric), rather than other metrics of quality (even subjective quality rating) that leverages human annotators more effectively",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: No problems with clarity.\n\nQuality: The experiments are set up well but the theses would be better served by more comprehensive experiments.\n\nNovelty: The approach is founded in bandit literature but advances beyond prior usages of multiple user simulators in this field.\n\nReproducibility: Code and data are provided in supplement, but I have not verified their accuracy or usability.",
            "summary_of_the_review": "The authors propose a well-justified bandit based method for learning from mixtures of different user simulators to train a TOD model. Automatic evaluation is encouraging but the central assertion of adaptibility probably requires a more comprehensive evaluation setting (models, domains)\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5743/Reviewer_mwD2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5743/Reviewer_mwD2"
        ]
    },
    {
        "id": "mbvK2GuKtD",
        "original": null,
        "number": 3,
        "cdate": 1667619578217,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667619578217,
        "tmdate": 1667619578217,
        "tddate": null,
        "forum": "Y2E5-_HL0DV",
        "replyto": "Y2E5-_HL0DV",
        "invitation": "ICLR.cc/2023/Conference/Paper5743/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed to use multiple user simulators to train a dialogue policy and showed that using multiple user simulators rather than a single user simulator improves the policy performance.\n\nThe authors developed a sampling strategy to sample user simulators during training. The sampling ratios are inversely proportional to the policy's performance on each user simulator. The policy's performance is calculated by the success rate's upper confidence bound.",
            "strength_and_weaknesses": "Strength:\n\n1. The idea of adapting multiple user simulators to train a dialogue policy is interesting. The adaptive phase is closely related to curriculum learning.\n2. Instead of using the mean success rate to calculate sampling ratio, the authors proposed to use the UCB of success rate, which hopefully makes the algorithm more robust. Although there is no ablation study to show if this is true.\n\nWeaknesses:\n\n1. The authors only used 4 user simulators to train MUST_uniform and MUST_adaptive, which makes the work less impressive. In general, we wouldn't expect many rule-based user simulators exist, therefore, we would rely on data-driven user simulators. I think the work is more valuable if the authors extend the current work and learn a large set of (and possibly latent/complementary) user simulators from data, and then apply the proposed methods to see if the policy can benefit from the large pool of user simulators.\n\n2. Follow up on the previous point, the authors didn't answer the question of how many user simulators are enough, and how different should they behave?\n\n3. The related works should include curriculum learning literatures. A lot of works use the loss/rewards to adjust task/example weights. For example, on top of my head, focal loss[1] is one method. There should also be an ablation study to show that the UCB term is useful.\n\n4. The authors proposed to use the inverse of UCB as the weights of user simulators. I am a little bit confused. The z_j in Section 4.2 Equation (3) is the *inverse* of success rate, reflecting the task difficulty. Therefore, we should use the UCB of z_j to calculate sampling ratios. In that case, should we pick lower confidence bound in equation (2)? For example, let's say we have two user simulators, the first one has a CI [0.2, 0.8], the second one has a CI [0.4, 0.6]. The proposed algorithm in the paper will assign lower weight to the first user simulator. However, the first user simulator has a lower bound of 0.2, which means that the user simulator could be very hard to train, and should be assigned a higher weight. \n\n5. Based on the results in Table 2 and Table 3, The performance of MUST_uniform and MUST_adaptive are super close. The performance of MUST_adaptive and SYS_AgenR are also very close. I don't see a clear benefit of the proposed algorithm.\n\n6. The experiments only use MultiWOZ restaurant domain dataset, and no SoTA methods are compared.\n\n[1] Focal Loss for Dense Object Detection https://arxiv.org/abs/1708.02002",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I think the novelty is limited, given the marginal performance improvement over baseline and the limited number of user simulators used for training.",
            "summary_of_the_review": "The authors proposed to use multiple user simulators to train a dialogue policy. While the idea is interesting, it is only trained on a small set of user simulators. The authors didn't justify the usefulness of the proposed UCB-based performance expectation. Finally, the performance improvements are marginal and are not compared with SoTA methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5743/Reviewer_H1xa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5743/Reviewer_H1xa"
        ]
    }
]