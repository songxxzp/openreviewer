[
    {
        "id": "MM-YO0JVErz",
        "original": null,
        "number": 1,
        "cdate": 1666637680416,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637680416,
        "tmdate": 1666637680416,
        "tddate": null,
        "forum": "oMsN9TYwJ0j",
        "replyto": "oMsN9TYwJ0j",
        "invitation": "ICLR.cc/2023/Conference/Paper3370/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- PiFold is a system for fixed backbone sequence design with a new PiGNN layer that combine node and edge information.\n- By using a parallel decoder, the model is able to make predictions many times faster\n- Using more features in the encoder, PiFold is able to still obtain state of the art results.",
            "strength_and_weaknesses": "Strengths:\n- PiFold is compared to a suite of recent protein design architectures\n- The training and testing splits are solid and makes comparison to previous works easy.\n- The model shows great sequence recovery performance and speed improvements.\n- The paper is written in a concise manner and understanding it is easy\n\nWeaknesses:\n- The ablations are interesting but slightly lacking - the \"leave one out\" approach seems imply no one change makes a big difference, PiFold is still state of the art despite ablating so many different changes. Maybe a better ablation is to show which change is making the difference here, and giving it such a large gap between PiFold and other competitors.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a clear way. However, some of the figures lack cohesion, and some terms are used without explanation:\n- Figure 2 PiGNN is written \"ProGNN\"?\n- Sec 4 CATH misspelled CTAH\n- Table 2: Rank is confusing, maybe explain that the rank corresponds to the highest drop in performace in the caption\n- Figure 2: What are the AT Decoder and NAT Decoders?",
            "summary_of_the_review": "The paper is technically strong, the writing could improve slightly, but overall I would recommend an accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3370/Reviewer_DZpe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3370/Reviewer_DZpe"
        ]
    },
    {
        "id": "rmEvfemXBwm",
        "original": null,
        "number": 2,
        "cdate": 1666655000254,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655000254,
        "tmdate": 1668706881604,
        "tddate": null,
        "forum": "oMsN9TYwJ0j",
        "replyto": "oMsN9TYwJ0j",
        "invitation": "ICLR.cc/2023/Conference/Paper3370/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for structure-based protein design that utilizes graph neural networks to capture local and global interactions between atoms in the protein and outputs a sequence of amino acids with the desired input structure. Authors introduce a method to generate useful features from the input structure. These features derived from real atoms and learned virtual atoms are then processed through a stack of attention and MLP-based blocks that can directly generate the amino acid sequence. Through numerical experiments on benchmark datasets, authors demonstrate the performance and efficacy of their algorithm, yielding state-of-the-art recovery with low inference time.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is well-written and organized in a logical manner. It is easy to follow in most parts.\n  \n- The algorithm shows good performance on the benchmarks. It consistently outperforms other methods in terms of recovery and perplexity in most cases and has the fastest inference time.\n  \n- The ablation studies are in-depth, authors put great effort into verifying their design choices.\n  \n\nWeaknesses:\n\n- The source code is missing, and therefore the results are not reproducible.\n  \n- The novelty compared to AlphaDesign and GCA is somewhat limited. One contribution the authors claim is the way features are generated. Here the novelty appears to be the introduction of virtual atoms, other elements can be found in AlphaDesign. The second contribution claimed by the authors is the PiGNN module, however this is based on SGT (in case of modeling local interactions) which has been introduced in AlphaDesign as well. Extracting global context has also been done in GCA in a similar manner. Could authors clarify the original contribution compared with AlphaDesign and GCA?\n  \n- Questions/concerns with respect to the experiments:\n  \n  - How similar are CATH 4.2 and 4.3? ESM-IF has only been evaluated on 4.3 and I am unsure how meaningful it is to include those results in Table 1 as a basis of comparison with the proposed method given they are evaluated on different datasets.\n    \n  - Table 4 shows that when the number of encoders increases from 6 to 8, test time increases by 4 seconds and recovery increases by 0.26%. However, when the number of encoders is increased from 8 to 10 (PiFold), test time only increases by 2 seconds. Furthermore, recovery increases by 0.44% which is almost doubled compared with the increase from 6 to 8 encoders. Can authors explain what causes the significant difference from linear scaling in test time? How does test time scale with the number of encoders? Furthermore, I would expect either linear or saturating behavior of recovery with respect to number of encoders, but this is not what we see in Table 4. Could authors discuss the performance scaling with # of encoders? Why exactly 10 encoders have been used in the final architecture?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clear in most parts. More background on protein folding might be necessary for a general ML audience and would be a good addition to the paper to improve readability.\nQuality: The paper's core contribution appears to be the empirical performance of the proposed algorithm, which is well supported by the experiments and the ablation studies (if the experiments are reproducible).\nNovelty: The novelty of the method is somewhat limited as most components can be found in previous work.\nReproducibility: no repository or zipped code has been provided.\n\nMinor comments:\n\n- Some parts of Related Works matches the AlphaDesign paper's Related work section word-to-word and should be rephrased.\n  \n- Typo in Related Works: \"sovlent-accessible\"\n  \n- Typo in Table 3: \"UpadateEdge\" ",
            "summary_of_the_review": "Overall, I am leaning towards borderline rejecting the paper, as the results are not reproducible and the original contribution compared to some previous work is somewhat unclear (see both under Weaknesses).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3370/Reviewer_4ek8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3370/Reviewer_4ek8"
        ]
    },
    {
        "id": "-S0Dz9G5yl",
        "original": null,
        "number": 3,
        "cdate": 1667228130539,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667228130539,
        "tmdate": 1668652444836,
        "tddate": null,
        "forum": "oMsN9TYwJ0j",
        "replyto": "oMsN9TYwJ0j",
        "invitation": "ICLR.cc/2023/Conference/Paper3370/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the protein inverse folding problem, i.e., predicting the most probable amino acid sequence given protein structure. To this end, they propose to improve two aspects of this task, one is more informative structural features for better representing protein structures, and the other one is a one-shot sequence decoder, PiGNN, for a more efficient generation. Experiments show that the proposed approach, the PiFold, achieves faster and more accurate performance for inverse folding. \n\n",
            "strength_and_weaknesses": "Overall, this paper is well-motivated and easy to follow. Extensive experiments are also presented.\n\nHowever, my major concerns come from the experiments and comparisons, which are performed less comprehensively and are not convincing. Please refer to the questions.\n\n**Questions:**\n1. The respective contributions of the proposed featurizer and PiGNN Layer remain unclear. How would it be if using the proposed features upon other strong models, e.g., protein mpnn? Besides, can PiGNN layers also help improves GVP GNN if placing them upon GVP features and encoders?\n2. Ablation study wrt autoregressivity (Tab 4) seems somewhat unfair, where PiFold uses 10 layers while all the AT competitors use much fewer layers (#Enc + #AT in total, the maximum number of layers are 6 < 10). What\u2019s more, model 4 consisting of 4 Enc and 1 AT decoder performs very closely to PiFold (~-1%). For more convincing conclusions, authors should conduct experiments with AT-based models that must integrate the same number of layers as PiFold, and ablate the proportion btw Enc and AT. \n\nIt would be great if these results would be included during the author response period. \n\nBesides, I also have some other questions:\n\n1. How about the scale of parameters of the proposed model compared to the previous ones? Plus, what would it be if scaling up the size of the parameters? How about scaling up data size (e.g., using alphafold-predicted data like in ESM-IF)? \n2. In most NLP studies, non-autoregressive models are known to lag behind their autoregressive counterparts considerably. However in this study, the proposed non-autoregressive models, though the comparisons are not that fair as I just pointed out, seem to do a really good job and even beat the autoregressive ones. What are the reasons for such differences in these two areas?\n3. Could you further provide results on CATH 4.3 so as to compare with the SOTA method ESM-IF (and their variant with AF2 data augmentation)\n\n**Minors:**\n\n- Sec. 1, page 2, \u201cExperiments show that Pi- Fold can achieve state-of-the-art recoveries on several real-world datasets, i.g., 51.66% on CATH 4.2, 58.72 on TS 50, and 60.42% on TS 500.\u201d: \u201ci.g.\u201d \u2192 \u201ce.g.\u201d\n- Symbols of quotes are misused across the manuscript. For instance, see paragraph Recovery in page 7, and many others.",
            "clarity,_quality,_novelty_and_reproducibility": "n/a",
            "summary_of_the_review": "Overall, I think this paper is mostly well presented, and the contributions can be considered a bit novel. But the empirical evaluations remain poor and not sufficiently convincing. As a  result, I lean to consider this as a boardline work. Acceptance would be possible upon author responses and more comprehensive and fair experimental evidence. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3370/Reviewer_cTW6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3370/Reviewer_cTW6"
        ]
    }
]