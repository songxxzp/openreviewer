[
    {
        "id": "ccdGXCqRJA",
        "original": null,
        "number": 1,
        "cdate": 1666247301678,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666247301678,
        "tmdate": 1666247301678,
        "tddate": null,
        "forum": "mb7VM83DkyC",
        "replyto": "mb7VM83DkyC",
        "invitation": "ICLR.cc/2023/Conference/Paper2490/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackled the modality laziness in current multi-modal fusion methods and suggested leveraging the uni-modal feature in two ways. Depending on the importance of paired features (maybe multi-modal information), simple averaging uni-modal features or knowledge distillation from strong uni-modal models. Also, the generalization degradation is described in theoretical perspective. ",
            "strength_and_weaknesses": "S1) a good motivation for the features yielded from multi-modal fusion modules and the motivation is shown experimentally.\nS2) Mathematical description for the generalization degradation\n\nW1) the detail of the uni-modal model is missed in UMT. If the uni-modal model is such a large network, UMT has benefits in terms of generalization. \nW2) Fewer baselines. Are there no works or techniques to preserve uni-modal information?\nW3) The author insists that the existing fusion modules delete the important uni-modal information somewhat. In UMT, the knowledge distillation is performed before the fusion module. Following the author's insistence, the distilled knowledge can also be corrupted after fusion. What is the reason for the design choice?\nW4) To select UMT or UME, the testing performance is required. It makes the proposed methods less mature. Is there any automatic rule for this? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is fine, and the description is clear.",
            "summary_of_the_review": "Please handle the weakness in the author's response",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2490/Reviewer_fVP6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2490/Reviewer_fVP6"
        ]
    },
    {
        "id": "ATqdeV-sFns",
        "original": null,
        "number": 2,
        "cdate": 1666675176985,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675176985,
        "tmdate": 1666675176985,
        "tddate": null,
        "forum": "mb7VM83DkyC",
        "replyto": "mb7VM83DkyC",
        "invitation": "ICLR.cc/2023/Conference/Paper2490/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Summary:\nThis paper proposes to care about the learning of unimodal features in the setting of multimodal fusion. They find out that one modality may strongly dominate the multimodal learning than other modalities on some tasks, like video classification and action recognition. This observation is consistent of previous related works, like greedy nature of multimodal DNN (Wu et al, 2022), under-optimized unimodal representations caused by strong dominated modality (Peng et al, 2022). They argue that previous works are complex to implement and are empirically inferior on some evaluation datasets (UCF101 and ModelNet40). Instead, this paper introduces Uni-Modal Teacher (UMT) to distill pretrained\nunimodal features to help learn corresponding unimodal features counterparts in the multimodal model. Furthermore, a trick is used to decide when to use the joint multimodal training (Uni-Modal Teacher, UMT) or just use the simple averaging predictions of unimodal models alone (Uni-Modal Ensemble, UME).\n",
            "strength_and_weaknesses": "Strong & Weak:\nS1: The writing is easy to follow. A reproducibility code is given. Better experimental results are achieved than the complex late-fusion models. \nS2: The architecture of Uni-Modal Teacher is straightforward and is empirically better than classic distillation methods including soft-label.\n\nW1: Although this paper cares about the weakness of multimodal learning and focuses on the importance of unimodal feature learning, assuming that unimodal priors are important and meaningful like previous works (Wu et al, 2022; Peng et al, 2022), it may be better to go further to design a unified, principal framework which can also subsume the settings where unimodal priors are not essential, for example on the visual question answer task. In other words, such a unified framework is agnostic to the unimodal priors and works well for both essential priors (including video classification and action recognition tasks) and meaningless priors (including visual question answer task). Perfectly, one does not need to choose which style of multimodal learning should be adopted to solve their own interest tasks at hand.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See Strong&Weak",
            "summary_of_the_review": "See Strong&Weak",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2490/Reviewer_AaMz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2490/Reviewer_AaMz"
        ]
    },
    {
        "id": "NMyEb84sRyS",
        "original": null,
        "number": 3,
        "cdate": 1666745036501,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745036501,
        "tmdate": 1670782250859,
        "tddate": null,
        "forum": "mb7VM83DkyC",
        "replyto": "mb7VM83DkyC",
        "invitation": "ICLR.cc/2023/Conference/Paper2490/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the insufficiency phenomenon of uni-modal feature learning in supervised multi-modal learning problem. The authors identify that the recent late-fusion method suffers from insufficient learning issue, called as Modality Laziness. To understand this phenomenon, the authors conduct a series of experiments and make a theoretical analysis. The authors propose two late fusion learning methods, Uni-Model Ensemble and Uni-Model Teacher, which work better in different cases according to the distribution of uni-modal and paired features. A simple guiding strategy is also provided. Experiments show the advantages of the proposed method in several multimodal datasets. \n\n--- post-rebuttal ---\\\nThanks for the authors' response. After reading the response and other reviewers' comments, my concerns are addressed. I keep my score.",
            "strength_and_weaknesses": "Strength: \n\n-The paper discovers an interesting phenomenon of Modality Laziness, and provides an in-depth understanding in both empirical and theoretical aspects. \n\n-The paper is well motivated. The analytical experiment in Table 1 really shows the severeness of existing supervised multimodal learning. \n\n-I appreciate the efforts that the authors try to explain the key concepts in an intuitive way. For example, the figure 3 is helpful. \n\n-The results compared with several existing methods are competitive. The proposed solution is simple but effective. \n\n \n\nWeakness \n\n-There is one method called \u201cbimodal deep autoencoder\u201d [Ngiam et al.], which can also be applied for joint feature extraction, then used for supervised learning. The discussion along this line is missing. Since  the motivation of this work is address Modality Laziness, the auto-encoder used in [Ngiam et al.] also has the capability to mitigate the insufficient learning. A comparison with this work is strongly recommended. \n\n[Ngiam et al,] Jiquan Ngiam et al., Multimodal Deep Learning, ICML 2011 \n\n-The term of \u201cnaive multimodal training\u201d appears multiple times across the paper, and its suffers from Modality Laziness. While the formal definition of this term is unclear to me. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-motivated and mostly well presented. It addresses an interesting phenomenon in supervised multimodal learning and provides a theoretical analysis. The proposed solution is simple, and easy to reproduce.",
            "summary_of_the_review": "Overall, I think this work is well executed, with a strong motivation and good theoretical and empirical analysis. In terms of existing work, I'd like to see how this work compares with multimodal deep learning [Ngiam et al.]. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2490/Reviewer_DNPE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2490/Reviewer_DNPE"
        ]
    },
    {
        "id": "b8oVPdO30X",
        "original": null,
        "number": 4,
        "cdate": 1666984543236,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666984543236,
        "tmdate": 1666984836246,
        "tddate": null,
        "forum": "mb7VM83DkyC",
        "replyto": "mb7VM83DkyC",
        "invitation": "ICLR.cc/2023/Conference/Paper2490/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper deals with multi-modal joint training methods on deep neural networks.\nRelated to this topic, several works have reported that the best uni-modal networks outperform the multi-modal networks even though the multi-modal networks receive more information.\nThis work is focused on this topic, and the main contributions of this paper are two-fold: (1) to propose an improved methodology with experimental supports on audio-video data for sound classification (VGG-Sound), audio-video data for action recognition (UCF-101, Kinetics-400), front-rear view of object data (ModelNet40), (2) to investigate theoretical results on characteristics among uni-modal features (learned from uni-modal training), and paired features (only learned from cross-modal interaction).\n",
            "strength_and_weaknesses": "*Strength\n- To identify interesting conceptualization on uni-modal features and paired features\n- To provide experimental evidences (mostly shown in Appendices) to justify their arguments and choices on this work\n- To explain theoretical aspects to characterize modality laziness\n\n*Weaknesses\n- It seems that some terms in the paper are NOT so clear: what is the exact definition of modality laziness? Why should we utilize new term? What are the discriminating points for it? \n- This paper is hard to follow due to lack of explanation for core parts such as model architectures of uni-modal teacher, loss functions with \u03bb_task, \u03bb_distill, \u03bb_distill in Algorithm 1 or the citation of reference works, which also needs for reproducibility.\n- The organization of this manuscript makes readers confusing. It seems that theoretical part (Section 3.4) and the others are totally separated. Uni-modal features and paired features also are redefined in Section 3.4. On the other hand, there is no experimental result related to Section 3.4 in main body of the manuscript (but in Appendices). \n- Even though related work Section is not bad, it would be better to compare one of the recent works [1], which deeply related to this work.\n\n[1] Modality Competition: What Makes Joint Training of Multi-modal Network Fail in Deep Learning? (Provably), ICML 2022 ",
            "clarity,_quality,_novelty_and_reproducibility": "If all of details has no problems, and so all of claims are validated, I think that the idea is novel and the contributions are explicit, and this work is worth publishing only considering the achievements and results. \n\nHowever, some parts of details are missing as described Weaknesses above.\nI think that it needs to re-organize and re-write the manuscript with respect to clarity and reproducibility.\n",
            "summary_of_the_review": "The main contribution, a method of combination of Uni-modal Ensemble and Uni-modal Teacher, seems practically utilizable and showing better performance for multi-modal joint training on deep neural networks. \nAlso, I think it is interesting idea to leverage distillation for generalized setting of multi-modal joint training. \nHowever, the writing is confusing and not enough clear to be reproducible.\n\n\n\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2490/Reviewer_5mXz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2490/Reviewer_5mXz"
        ]
    }
]