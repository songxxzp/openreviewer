[
    {
        "id": "wErQWwVBlj",
        "original": null,
        "number": 1,
        "cdate": 1666578457486,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578457486,
        "tmdate": 1668526700694,
        "tddate": null,
        "forum": "vTb1JI0Gps_",
        "replyto": "vTb1JI0Gps_",
        "invitation": "ICLR.cc/2023/Conference/Paper953/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a reinforcement learning based approach for automatically predicting graph augmentations for a graph neural network (GNN) classification problem. The authors argue that label invariance (data augmentations that do not affect labels) is an important, unsolved problem for GNN, and propose a technique to maximize this property. Previous automatic graph augmentation methods did not consider labels, while the proposed method does. Experimental results demonstrate a marked improvement in classification accuracy on several benchmark datasets.\n",
            "strength_and_weaknesses": "The paper highlights and addresses an important problem in graph classification: how to perturb data without affecting label information. To address this problem, a reinforcement learning approach is proposed and extensive experiments demonstrate the utility of the technique. The paper is very well-written and easy to understand. Summary of related work is also extensive.\n\nThe key weakness is the lack of evaluation of the label invariance property. It\u2019s not clear how often does the reinforcement learning method generate graphs that retain the same labels. Since this property is key to the motivation of the paper, I would like to see an analysis of how it is maintained.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well-written. It appears that the idea is novel with respect to prior work, and addresses an overlooked problem in GNN research. The extensive description of the method in the main manuscript and appendix should make it possible to easily reproduce the work. \n\nHere are a few specific questions:\n- The intuition of augmentation category transformations in section 3.4 is unclear. It seems like the p(a_t) is computed by aggregating information from all nodes and edges. Is this node/edge information computed by the reinforcement learning algorithm? \n- Are there any steps taken to ensure stability of the reinforcement learning algorithm?  \n- It would help to see a discussion of the pros and cons of the effect of the fixed/pre-trained reward generation method.\n- Why is it necessary to add a virtual node to the graph? Why is it not possible to predict the augmentation choice directly from the input graph? \n",
            "summary_of_the_review": "Overall, the paper should be a strong contribution to the conference. However, evaluation of the key component of the paper (label invariance) is missing. If the authors are able to address this concern, I will increase my rating.\n\nEDIT: after reading through author rebuttals and other reviews, my concerns have been addressed and I increased my rating.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper953/Reviewer_jQkA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper953/Reviewer_jQkA"
        ]
    },
    {
        "id": "JIk60GZCOZt",
        "original": null,
        "number": 2,
        "cdate": 1666608505471,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666608505471,
        "tmdate": 1666608505471,
        "tddate": null,
        "forum": "vTb1JI0Gps_",
        "replyto": "vTb1JI0Gps_",
        "invitation": "ICLR.cc/2023/Conference/Paper953/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes GraphAug, a new automated graph augmentation method. The method selects augmentation operation and ratio, considering the label-invariance of the operation. The experimental results show that the proposed method improves accuracy on various graph classification tasks, compared to the baseline methods.\n",
            "strength_and_weaknesses": "[Strength]\n1. This paper studies a practical problem.\n2. The main idea of the method is simple and intuitive.\n3. The method achieves superior performance on various datasets for different classification tasks.\n4. The overall presentation is good.\n[Weakness]\n1.   There are some missing prior works. For example, GCA [Zhu and Xu et al. WWW\u201921] proposed an adaptive graph augmentation for graph contrastive learning. Although it does not use \u201clabel-invariance\u201d explicitly, it aims to keep important structures and attributes unchanged.\n2.   The justifications for the reward generation network do not seem sufficient. The proposed method uses the reward generation network to generate the reward. However, in prior RL-based augmentation methods, the accuracy of the child network is given as the reward. So I think there should be justifications for the design decision of the network.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The key idea of the proposed methods is intuitive.\nThe proposed method is distinct from existing augmentation methods.\nThe paper presented the algorithms of proposed methods for reproducibility.\n",
            "summary_of_the_review": "In summary, the paper studied a significant aspect of automated graph augmentation tasks, and the proposed method is based on intuitive and solid ideas. Although the idea of label-invariant augmentation exists in prior works, the design and the implementation of the model can be appreciated. Also, empirical results support its flexibility and superiority.\n\np.s.\nReference of GCA:\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2021. Graph Contrastive Learning with Adaptive Augmentation. In Proceedings of the Web Conference 2021 (WWW '21), April 19\u201323, 2021, Ljubljana, Slovenia. ACM, New York, NY, USA 12 Pages.\nhttps://dl.acm.org/doi/fullHtml/10.1145/3442381.3449802",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper953/Reviewer_2d1h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper953/Reviewer_2d1h"
        ]
    },
    {
        "id": "mQHN8CGotv",
        "original": null,
        "number": 3,
        "cdate": 1666654465215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654465215,
        "tmdate": 1666654465215,
        "tddate": null,
        "forum": "vTb1JI0Gps_",
        "replyto": "vTb1JI0Gps_",
        "invitation": "ICLR.cc/2023/Conference/Paper953/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper worked on data augmentation for graph classification, and proposed an automated method that learns label-invariant augmentations. \n\nIt formulates the graph augmentation problem as a sequential decision-making task (Markov Decision Process), where at each time step one augmentation method is picked given the current graph and transformed graph (as well as the reward). The model is optimized using policy gradient (i.e., REINFORCE) algorithm and the reward is generated using the graph matching network to predict the probability that the current graph and the transformed graph have the same label (so as to ensure label-invariant property). Experiments on both synthetic datasets and real-world benchmarks verify the effectiveness of the proposed method.",
            "strength_and_weaknesses": "S1. This paper is overall well-written and easy to follow. \n\nS2. Extensive experiments on multiple datasets with SOTA baselines validates the effectiveness of the proposed method.\n\nS3. The proposed label-invariant augmentations seems very interesting.\n\nW1. I appreciate the complexity analysis in the appendix. It would be nice to also have runtime comparison with baselines to further validate the efficiency of the proposed method.\n\nW2. In Table 2, sometimes the mixture augmentation would underperform some of the single augmentations. I wonder if the authors have conducted sufficient hyperparameter search for it.\n\nW3. On the larger datasets such as the OGB one, the performance improvements over simple augmentations are not very big. In that case, I wonder if adding the much more complex RL framework would worth the trade-off on time and resource.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is overall well-written and easy to follow.\n\nQuality: The proposed method seems sound to me.\n\nNovelty: While the idea of using RL for auto-augmentation is not novel, the newly proposed label-invariant augmentation is novel and interesting.\n\nReproducibility: Pseudo-codes are provided in the appendix, but the actual code seems not provided.",
            "summary_of_the_review": "Based on my previous comments, I recommend acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper953/Reviewer_iXAY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper953/Reviewer_iXAY"
        ]
    }
]