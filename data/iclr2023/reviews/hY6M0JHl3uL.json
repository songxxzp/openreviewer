[
    {
        "id": "P2SLpJspdU",
        "original": null,
        "number": 1,
        "cdate": 1665621268647,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665621268647,
        "tmdate": 1665621268647,
        "tddate": null,
        "forum": "hY6M0JHl3uL",
        "replyto": "hY6M0JHl3uL",
        "invitation": "ICLR.cc/2023/Conference/Paper961/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper observes barriers in the loss landscape between fine-tune models which have discovered different \"generalization strategies\" in the context of text classification. In contrast, models with the same generalization strategies have no barrier when interpolating linearly between the two models. ",
            "strength_and_weaknesses": "Strengths:\n- The phenomenon uncovered by this paper is thoroughly investigated and will be of great interest to the community.\n- The work is very well contextualized among related investigations.\n\nWeaknesses:\n- The experiments are already very thorough, so while it is not necessary I think the paper would be much stronger with the three following experiments.\n1. A lot of times in this paper the NLP world is contrasted with the vision world. However, a common technique for fine-tuning in the vision world is to first fine-tune a linear classifier, then unfreeze and fine-tune all params [1]. What happens when you first train a linear probe then use this as the common initial head when fine-tuning pairs - do you still see different generalization strategies? Relatedly, what happens when you start with a T5 or T0 so that you are starting from an already good model, instead of with a new classification head so that performance is initially not good.\n2. In vision, LMC also breaks when fine-tuning if you do so with a very high LR. What is the effect of LR in these experiments?\n3. Instead of interpolating, what happens when you ensemble the heuristic and generalized models?\n\n[1] https://arxiv.org/abs/2202.10054\n",
            "clarity,_quality,_novelty_and_reproducibility": "The figures and presentation in general are very clear. Code and models are provided. While I did not run the code, the paper overall appears to be very reproducible. ",
            "summary_of_the_review": "The paper investigates an interesting phenomenon and does so in a clear and reproducible manner. While additional experiments would strengthen the investigations, I believe the paper will be of interest to the community and should be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper961/Reviewer_b2ea"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper961/Reviewer_b2ea"
        ]
    },
    {
        "id": "nP2iOC_YHu",
        "original": null,
        "number": 2,
        "cdate": 1666925054204,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666925054204,
        "tmdate": 1666925054204,
        "tddate": null,
        "forum": "hY6M0JHl3uL",
        "replyto": "hY6M0JHl3uL",
        "invitation": "ICLR.cc/2023/Conference/Paper961/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies linear mode connectivity (LMC) for language tasks. Usually, LMC is studied on the training and test loss landscape, both of which come from the same distribution. [Neyshabur et al.](https://arxiv.org/pdf/2008.11687.pdf) study LMC for several fine-tuning tasks, and show that the LMC can be observed on the fine-tuning landscapes when initializing from the same pre-trained model. However, LMC isn't observed when initializing from random initialization. [Neyshabur et al.](https://arxiv.org/pdf/2008.11687.pdf) developed this observation to understand \"what is being transferred in transfer learning.\" They make the case that pretraining helps by allowing LMC on the target landscape and biasing the optimizer trajectories to converge to a shared basin. \n\nIn this paper, the authors take this further, and instead of just studying LMC on the target task's landscape, they also consider an out-of-domain diagnostic task. The authors show that while fine-tuned models behave well on the target task, only some of them behave well on the diagnostic task. The models which do well (generalizing models) and do not do well (heuristic models) on the diagnostic tasks form separate clusters on the target landscape. Each of these clusters is mode connected, but they are not connected to each other.  The paper then suggests a notion of \"distance\" in the model space called \"convexity-gap,\" which leads to meaningful clustering of the models in terms of their functional behavior. Finally, the paper shows that as training proceeds, different models get closer to their clusters, suggesting that early diagnostic tests of models can predict their final generalization performance. ",
            "strength_and_weaknesses": "The paper asks an interesting question: how does the functional behavior differ between different modes split from a pre-trained model? It discusses several valid and interpretable diagnostic sets for doing this. If the experiments in the paper were performed correctly (see my apprehensions later), it would offer novel insight about LMC.\n\n\n #### **Missing important experimental details**\nMy main issue with the paper is that it is not very precise with the different experimental details. In some cases, it just leads to a non-smooth reading of the paper, but in other cases, it leads to significant confusion about what is being done in the experiments.\n\n1. How were the plots in figure 2 obtained? Are we looking at two random directions, two principal component directions, or something else altogether? One could miss high-dimensional barriers by looking at random directions; non-convex functions can look convex, etc. \n\n2. Since several data sets/tasks are involved, it is crucial to be specific about the loss landscape on which LMC is observed or not observed. I could infer which landscape is being plotted and which models are connected on which task, but only after carefully checking the paper and the appendix. This should be more clearly specified in the figure captions and should not be a matter of confusion. For reference, see section 2 in [this paper](https://arxiv.org/pdf/2008.11687.pdf), and note how non-colloquial markers are used throughout the paper, to be specific.  \n\n3. Regarding footnote 3, were different endpoints obtained using different initializations for the linear layer? This seems to be the case since the same seed is used for initialization and data permutation. If so, this is different from the usual LMC setup, where the branches are split from the same training trunk after several training steps from the same initialization. This is a crucial point, as models training from different initializations don't usually show LMC. Most importantly, **this could be why different fine-tuning runs converge to different basins**.\n\n\n#### **Convexity Gap**\n\nI am less convinced about the convexity gap metric. It is easy to look at figures and enforce one's interpretations. Based on the discussion in the paper, it is unclear if CG dominates other strategies. The clustering in Figure 18 looks comparable to CG. The authors should discuss this in the main paper. In light of this, it is clear that there are clusters as they are visible under all metrics, but it is not clear if CG is the best way to obtain clusters.  Similarly, the discussion in appendix H is critical and can't be pushed to the appendix. It is not clear to me, based on the appendix, that the authors successfully refute the skepticism that clustering is indicative of target domain performance, and the differences can be heightened by looking at diagnostic tasks. Overall, this discussion is not mature yet and requires further investigation. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "#### **Comments on writing**\n\n1. The writing is a bit colloquial in certain places, making it hard for a general audience to read. For instance, it is unclear what the following sentence in the introduction means: \"Early LMC may thus predict final heuristics.\" There is nothing like early LMC; LMC makes sense only when the models have converged. Perhaps the authors should rephrase it to: \"Early performance of models on a diagnostic task can reveal information about the model's final behaviors, provided we observe LMC for the particular task.\" Such writing appears in several places in the paper, and someone unfamiliar with LMC might be unable to make sense of these phrases. \n\n2. I also feel too many abbreviations in the paper make the reading non-smooth. I suggest referring to the diagnostic tasks as \"task  1, 2, 3,\"  etc. The exact nature of these tasks is relevant only when discussing the linguistic connection and saying something specific about the generalization behavior. For instance, sentences like \"HANS performance during interpolation: It is clear from Fig. 1(a) that, when interpolating between LO-heuristic models, HANS-LO loss\" can be made much more straightforward. Section 2, in particular, can be made more readable for the general audience, who understand LMC, but might not be too familiar with the NLP tasks. \n\n3. What does this mean: \"directly pessimizing on the test set.\"?\n\n\n\n\n\n\n\n\n\n\n",
            "summary_of_the_review": "I think the results and discussion in section three are interesting, but I have some concerns about the experimental procedure. I also don't like the writing of the paper, as the authors are not the most explicit about some important details. But most importantly, I believe the metric proposed in section four needs to be investigated further. The experiments don't depict any clear advantage of the metric over simpler measures. Overall, this reduces the novelty of the work. I encourage the authors to make the writing changes, develop more on their metric, add some extensions mentioned in section six, and then resolve future directions to improve the paper. \n\nOn an orthogonal note, I am also curious if the authors have explored the connections to neural mode collapse? ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper961/Reviewer_XZas"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper961/Reviewer_XZas"
        ]
    },
    {
        "id": "Am8q0N6NRm7",
        "original": null,
        "number": 3,
        "cdate": 1667054214377,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667054214377,
        "tmdate": 1667054214377,
        "tddate": null,
        "forum": "hY6M0JHl3uL",
        "replyto": "hY6M0JHl3uL",
        "invitation": "ICLR.cc/2023/Conference/Paper961/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work explores the linear mode connectivity (LMC) feature of neural networks in the context of language models. The authors observe a contrast with the literature, which claims that (LMC) is nearly ubiquitous in neural networks. The authors note that the empirical findings based on these claims are biased toward the image domain and demonstrate this is not necessarily the case in natural language settings. Furthermore, the authors also find that out-of-distribution generalization is not associated with the sharpness of the minima but the density of models in a particular linearly connected basin. ",
            "strength_and_weaknesses": "Strengths:\n\n- Demonstrates that the LMC phenomenon is not universal across the natural language modality.\n- Useful observations in the association of LMC with generalization\n- Clustering analysis using the CG metric appears to be novel and an alternative to sharpness.\n\nWeaknesses:\n\n- Not much investigation of the architecture dependence. For instance, Ainsworth 2022 showed that the LMC phenomenon depended on the neural network's architecture and width.\n- Unclear how pre-training initialized LMC and LMC with random initialization modulo permutations are the same minima.\n- Since the authors demonstrate a lack of connectivity on out-of-distribution data, other hyperparameters may impact results. Ainsworth et al. 2022 noted that the LMC phenomenon does not occur until training has converged to some degree. (See Figure 3 at https://arxiv.org/pdf/2209.04836.pdf). Convergence on in-distribution may not indicate convergence on out-of-distribution. How do the authors define the training criterion to ensure the out-of-distribution has sufficiently \"converged\"?\n- Doesn't the clustering analysis of the CG metric require the diagnostic dataset, whereas sharpness does not? If this is the case, the claim about sharpness being inferior seems overstated. While the overall results of the paper would indicate otherwise, has the clustering of the CG approach been tried on MNLI and QQP?\n\nComments:\n\n- Absence of evidence is not evidence of absence. The LMC hypothesis is not easily refuted empirically because a permutation search procedure's inability to find a viable permutation for LMC does not mean a permutation does not exist. This is why I do not consider it a weakness of this work, but it would be good for the authors to comment on this issue in the manuscript.",
            "clarity,_quality,_novelty_and_reproducibility": "- Parts of the work were difficult to parse because of the number of experiments run on different datasets, some of which were used for training, indicating an in-distribution loss landscape, and some were out-of-distribution. The clarity should be improved by clearly indicating which datasets were used for fine-tuning (MNLI and QQP) for each figure. This gives the reader a better idea of what they should expect to see and what occurs.\n\n- How were the heuristic and generalizing models created? It's unclear whether these models are only different based on random initialization seed and some fine-tuned models generalized while others did not. If this is the case, can the authors comment on the conclusion in the referenced paper they mention (Zhou et al. 2020), which implies that this is more of an issue with the diagnostic dataset rather than the model? ",
            "summary_of_the_review": "Overall, this work appears to analyze LMC from the perspective of generalization on OOD data and find previously unknown connectivity patterns and energy barriers. While there are more questions created in the analysis than answers (see above), this work is a good contribution to the LMC story and why the phenomenon might not be as straightforward as previously thought.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper961/Reviewer_rtsz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper961/Reviewer_rtsz"
        ]
    },
    {
        "id": "z7uyZL34_V",
        "original": null,
        "number": 4,
        "cdate": 1667264909610,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667264909610,
        "tmdate": 1667264909610,
        "tddate": null,
        "forum": "hY6M0JHl3uL",
        "replyto": "hY6M0JHl3uL",
        "invitation": "ICLR.cc/2023/Conference/Paper961/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies linear connectivity from a perspective of out-of-domain generalization for NLP tasks. Experiments show that 'generalizing' and 'heuristic' models (which are defined based on their out-of-domain performance) are separated in terms of what 'basins' they belong to. Further analysis along with clustering techniques show a connection between models' behaviors and their position in the loss surface.",
            "strength_and_weaknesses": "The empirical observations are interesting and show novel connections between a model's behavior (in terms of its out-of-domain generalization and preferences).\n\nThe work proposes new metrics that seem intuitive and are suitable for what they're designed to capture, like the convexity gap, epsilon-basin.\n\nThe main weakness, in my opinion, is that there seems to be lacking information regarding the experiments. Most, if not all, experiments rely on having 'heuristic' and 'generalizing' models, but I couldn't find details on how these models are obtained. Are generalizing and heuristic models trained the exact same way and only differ in the random seed? If different training settings were used, then a detailed discussion on it would be very valuable.\n\nAlthough the paper shows interesting observations, it's unclear whether these have meaningful implications and/or applicability, and are somewhat restricted to the NLP tasks explored.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written however there are unclear points regarding the experimental setup (see above).",
            "summary_of_the_review": "To reiterate from 'strengths and weaknesses':\n\n- Novel and interesting observations.\n- Well-defined metrics.\n- Lacking some information on experimental setup.\n- Limited implications.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper961/Reviewer_bPGJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper961/Reviewer_bPGJ"
        ]
    }
]