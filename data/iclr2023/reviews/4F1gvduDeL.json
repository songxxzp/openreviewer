[
    {
        "id": "SAw8jL3qx2",
        "original": null,
        "number": 1,
        "cdate": 1666493783902,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666493783902,
        "tmdate": 1670405341661,
        "tddate": null,
        "forum": "4F1gvduDeL",
        "replyto": "4F1gvduDeL",
        "invitation": "ICLR.cc/2023/Conference/Paper6412/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors provide a few-shot domain adaptation method to address the channel distribution changes of communication systems. Specifically, using the properties of Gaussian mixtures, they propose a solid domain adaption process for the generative channel model (MDN). Besides, they propose a input-transformation method which transform the input of decoder from target domain into source domain, without modifying the encoder-decoder networks. They also derive experiments on a mmWave FPGA platform and show the strong performance improvements of the proposed method. ",
            "strength_and_weaknesses": "Strength\n1.\tThe paper is well organized and has rich details.\n2.\tThe advantage of the proposed method is clearly stated and demonstrated.\n3.\tThe adaptation approach is based on appropriate assumptions and is well supported by the properties of Gaussian mixtures.\n4.\tThe effectiveness of the method is evaluated by both simulated and real experiments, and there are also experiments when the assumptions could not hold.\n\nWeaknesses\n1.\tSome confusions.\nIn Parameter Transformation part, you state that \u201cThe number of adaptation parameters is given by k (2 d2 + d + 2). This is typically much smaller than the number of MDN parameters (weights and biases from all layers)\u201d. \nIn previous part you state that \u201cThe MDN output with all the mixture parameters has dimension p = k (d(d + 1)/2 + d + 1).\u201d \nWhy the adaptation parameters is much smaller than the number of MDN parameters?\n2.\tSome figures are not self-explanatory. For instance, in Figure 4, the line of No adapt or Finetune are covered by other lines, without additional explanation. \n3.\tMore experiments. How the unsupervised domain adaptation performs based on the baseline model and how it compares with the proposed approach?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and clearly stated. The advantage of the method is well stated.",
            "summary_of_the_review": "The paper is well organized and has rich details. The work is based on appropriate assumptions and the properties of Gaussian mixtures, and the effectiveness is demonstrated by experiments on both simulated and real experiments. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6412/Reviewer_jGKn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6412/Reviewer_jGKn"
        ]
    },
    {
        "id": "2TsZcacoQn2",
        "original": null,
        "number": 2,
        "cdate": 1666677207218,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677207218,
        "tmdate": 1666677207218,
        "tddate": null,
        "forum": "4F1gvduDeL",
        "replyto": "4F1gvduDeL",
        "invitation": "ICLR.cc/2023/Conference/Paper6412/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper models the changes of channel in a communication system as a few-shot domain adaptation problem. They employ the Gaussian mixture density network to specifically model the channel and propose a transformation to compensate for changes in the channel distribution. They perform experiments both on simulated channel distributions and FPGA.",
            "strength_and_weaknesses": "Pros:\n\n1. This paper considers the frequent change of channel in communication systems. They treat the change of channel as distribution shift and link this practical problem with few-shot domain adaptation. I think this is novel and advanced enough in the field of communication.\n\n2. The proposed solution is easy to follow and can be used in more general few-shot DA scenarios. In addition, this method has better real-time performance compared with previous works.\n\nCons:\n\n1. This paper lacks the few-shot domain adaptation methods as baselines, e.g., [1]. Current baselines are all the basic FDA solutions, and I worry about their competitiveness. \n\n2. The evaluation metric they use is only the SER. As the application research article, the performance of the proposed method in practical communication problems is essential. They need to show the advantages of their learning-based method over conventional methods.\n\n3. I find that the number of target data per class is more than 10 in this paper, and maybe such an amount is beyond the scale of few-shot learning. Additional experiments with less than 7 samples per class are important.\n\n[1] Mottian et al. Few-Shot Adversarial Domain Adaptation. NeurIPS, 2017.\n",
            "clarity,_quality,_novelty_and_reproducibility": "These are all seem good. The presentation is clear, and the writing quality is above the bar. For the communication field, I think the novelty is sufficient. They also provide the source code in supplementary materials.",
            "summary_of_the_review": "This paper aims to address the varying channels in communication with the help of few-shot DA. Their solution is simple and easy to follow, and the corresponding theoretical and empirical analysis are thorough. However, some experimental settings (see above) are need to be further optimized.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6412/Reviewer_91WK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6412/Reviewer_91WK"
        ]
    },
    {
        "id": "r6tim9gHgw",
        "original": null,
        "number": 3,
        "cdate": 1666897733293,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666897733293,
        "tmdate": 1666897733293,
        "tddate": null,
        "forum": "4F1gvduDeL",
        "replyto": "4F1gvduDeL",
        "invitation": "ICLR.cc/2023/Conference/Paper6412/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "- The paper addresses the problem of handling domain-shifts that arises in generative learnt channel models in E2E communication systems in a few-shot setting.\n- The proposed domain adaptation approach is tailored around a Mixture Density Network (MDN) representing the channel model. In here, the approach:\n    - learns an adapter layer, which models an affine transform of the original conditional channel distribution\n    - introduces an additional regularization objective to ensure the adapter doesn't converge to bad/degenerate solutions\n    - presents a feature transformation formulation on the decoder side to aid learning on the domain-shifted distributions\n- The approach is evaluated extensively, covering multiple types of distribution changes in both synthethic settings as well on a high-resolution mmWave testbed.",
            "strength_and_weaknesses": "### Strengths\n\n**1. Extensive evaluation**\n- The approach is evaluated rigorously with well-suited baselines and a range of scenarios (e.g., multiple types of domain shifts, real-world evaluation). I especially appreciate evaluations studying when the (reasonable) assumptions are violated.\n\n**2. Motivation and relevant problem**\n- While there has been a lot of attention on generative channel modelling recently, most works in my knowledge largely (and somewhat incorrectly) assume a stationary distribution. This paper takes a step in the right direction by addressing this pain-point.\n\n**3. Insightful approach**\n- The approach overall is insightful and makes sense. By learning an adapter network and learning parameters relevant for the domain shifts (e.g., like FiLM modules), it makes few-shot domain-adaptation more tractable.\n- Furthermore, I find the choice of the channel model representation (MDNs) to also be sufficiently appropriate for the task (as opposed to GANs) for this study. \n\n### Concerns\n\n**1. \"labeled set obtained for free\"**\n- The paper at multiple times claims that few-shot learning is especially possible since we can get labeled dataset for free -- I find this slightly confusing.\n- Wouldn't the labeled dataset be split between the encoder (transmitter) and decoder (receiver) devices? As a result, for a party to have the full labeled dataset, isn't a prerequisite communicating labels back to the other party?\n\n**2. Evaluation: Some observations unclear**\n- I found some patterns in the evaluation was somewhat unclear and would appreciate the authors' answers on the questions below:\n- (a) Oracle-approach gap in Figure 4/5: I'm slightly surprised that proposed approach's symbol error rate does not converge to the oracle with a reasonable number of additional examples (50 * 16-QAM classes = 800), given that there are 50 learnable parameters. Are the authors aware if convergence is possible with even higher examples? Morevover, what is the size of the source dataset?\n- (b) Unchanged error rates in Figure 4/5 for many baselines: Are the authors aware of why the error rates of many baselines do not improve at all in spite of more training examples? Were the \"finetune\" baselines finedtuned only on the new data or a combination? In the case of combination, are domain-invariant features learnt? \n- (nitpick) Please summarize the performance degradation discussions in Ricean fading experiments in the main paper.\n\n**3. Evaluation: Performance under no distribution change**\n- I appreciate that the authors also evaluate under a non-domain shifted dataset in Figure 10. Can the authors clarify why results drop in performance when there is no distribution change?\n- Specifically, it appears that the adapter layers' parameters are initialized such that it produces a identity mapping (page 18), so I'm surprised that this nonetheless degrades performance.\n\n**4. SNR=14-20 dB**\n- Can the authors comment whether a SNR of 14-20dB (which to me appears really large) is a reasonable setting? Did the authors also evaluate SNR vs. error rates for the approach and baselines? I wonder if the results shown here apply only in high SNR regimes.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: Good. It was generally easy reading the paper, thanks to really crisp text and a comprehensive background section. The minor issue I found is that some patterns in the results are not discussed (see concern 2, 3) The only nitpick I have are the figures (esp. Figures 4-6) where legends are highly illegible.\n\n**Quality**: Good. While there are minor discrepancies the approach (e.g., performance slightly deteriorates when there is no distribution change, does not translate well to certain distribution changes), I think it can be overlooked in light of the remaining contributions.\n\n**Novelty**: Very good. The authors tackle a very well motivated problem (see strength 2) and propose an insightful approach to tackle it (see strength 3).\n\n**Reproducibility**: Very good. The main paper (esp. the large appendix) appears to contain many details of the approach. Additionally, the code is provided as well. I'm not sure if the authors plan to release the channels from the mmWave FPGA testbed.",
            "summary_of_the_review": "The paper tackles a relevant bottleneck in generative channel modelling for E2E communication systems (i.e., they are trained assuming a stationary distribution, but this isn't the typical case). The approach is novel and intuitive in my opinion, and is further evaluated extensively in both simulated and real conditions. While I have some minor concerns (e.g., can one really have a labelled dataset for this task in practise?), I don't think they significantly affect the paper's claims and contributions. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6412/Reviewer_fua1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6412/Reviewer_fua1"
        ]
    }
]