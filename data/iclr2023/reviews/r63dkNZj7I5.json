[
    {
        "id": "LqVzdgWqa9Q",
        "original": null,
        "number": 1,
        "cdate": 1666593002312,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593002312,
        "tmdate": 1666593002312,
        "tddate": null,
        "forum": "r63dkNZj7I5",
        "replyto": "r63dkNZj7I5",
        "invitation": "ICLR.cc/2023/Conference/Paper119/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method, metric learning with implicit semantics (MLIS), for generalization zero-shot learning (GZSL). It leverages a metric learning loss during training to refine the visual feature and better align the semantic information. In addition, to alleviate the feature bias towards the seen classes, it also uses a generative model to simulate the unseen class features to improve the overall quality. The proposed MLIS was evaluated on multiple benchmark datasets for GZSL, and competitive performance has been observed.",
            "strength_and_weaknesses": "Strength:\n1. The paper is written clearly and easy to follow\n2. The proposed method is extensively evaluated on multiple benchmark datasets and shows competitive results.\n\nWeakness:\n1. This paper lacks novelty. Using metric learning to improve the feature quality and generative model to synthesize unseen features are well studied in the GZSL domain. The essential techniques used in this paper, including the WGAN-GP for feature generation, and the circle loss, are proposed in previous works.\n2. The empirical performance is not compelling enough to me, considering the method is mainly composed of existing techniques.\n3. It lacks some qualitative results to justify the motivation. The paper was motivated by the fact that fine-grained GZSL classification is more sensitive to intra-class variations. So it is better to have some visualization (like tSNE graphs) to demonstrate that the refined features have fewer intra-class divergences compared to the raw features and other methods.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is written clearly and is easy to understand.\nQuality & Novelty: As I mentioned above, I feel the paper lacks novelty.\nReproducibility: The paper gives clear instructions on the implementation. However, it lacks enough descriptions on its training schema, which makes the experiments hard to reproduce only relying on the paper.",
            "summary_of_the_review": "This paper proposes an interesting method that can improve the feature quality for GZSL through metric learning and feature synthesis. However, it does not have enough novelty, and the performance margin is not large enough to justify an acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper119/Reviewer_WJ1v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper119/Reviewer_WJ1v"
        ]
    },
    {
        "id": "9l2imadhO6l",
        "original": null,
        "number": 2,
        "cdate": 1666627003341,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627003341,
        "tmdate": 1666627003341,
        "tddate": null,
        "forum": "r63dkNZj7I5",
        "replyto": "r63dkNZj7I5",
        "invitation": "ICLR.cc/2023/Conference/Paper119/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces deep metric learning for generalized zero-shot learning, and proposes a novel framework named MLIS to avoid overfitting to seen classes. MLIS disentangles the effect of semantics on feature learning and classification, making the training of the two tasks more effective. Moreover, this paper concatenates semantic descriptors and visual features directly instead of aligning them, reducing the model\u2019s complexity. Experimental results demonstrate the effectiveness of the proposed MLIS.",
            "strength_and_weaknesses": "Strengths\n1.\tThis paper is well written and easy to follow. \n2.\tThe proposed framework is simple and effective, and easy to reproduce. \n\nWeaknesses\n1.\tThe motivation is unclear. The authors consider that semantics used in both synthesizing visual features and learning embedding functions will introduce bias toward seen classes. However, some methods [1][2] using semantics seem to get better results. Please compare with them and give more explanations for the motivation of the proposed paper. Moreover, it would be more convincing to add comparative experiments that use semantics but do not decouple.\n2.\tThe comparison on the results of the CUB is unfair. Most of the methods, such as FREE and f-VAEGAN-D2, utilize 312-dimensional attributes as the auxiliary semantic information. You need to experiment with attributes rather than 1024-dimensional semantic descriptors if you want to compare with these methods.\n3.\tSome recent methods like [2] and [3] are ignored to be compared. \n4.\tI wonder why the results are so low using only ML in the ablation experiments. The results are even lower than some simple early methods like f-CLSWGAN [4] and f-VAEGAN-D2 [5]. More explanations can be given. \n5.\tA minor problem. In section 3.4, the authors said that synthesized features including both seen and unseen classes are used to train the final classifier. However, it seems that only the synthesized unseen features are used. \n\n[1] Generative Dual Adversarial Network for Generalized Zero-shot Learning. CVPR 2019.\n[2] Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification. ECCV 2020.\n[3] Contrastive Embedding for Generalized Zero-Shot Learning. CVPR 2021.\n[4] Feature Generating Networks for Zero-Shot Learning. CVPR 2018.\n[5] F-VAEGAN-D2: A feature generating framework for any-shot learning. CVPR 2019.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. \nThe contributions are incremental.\n",
            "summary_of_the_review": "This paper is clear and easy to follow. However, the contributions still need to be explained more clearly. A fair comparison is also needed on the CUB dataset. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper119/Reviewer_tWPT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper119/Reviewer_tWPT"
        ]
    },
    {
        "id": "opxRKwCZS-D",
        "original": null,
        "number": 3,
        "cdate": 1666699262424,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699262424,
        "tmdate": 1666699309723,
        "tddate": null,
        "forum": "r63dkNZj7I5",
        "replyto": "r63dkNZj7I5",
        "invitation": "ICLR.cc/2023/Conference/Paper119/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes Metric Learning with Implicit Semantics (MLIS) to refine discriminative and generalizable visual features for GZSL. MLIS disentangles the effects of semantics on feature extractor and image classification of the model, so that semantics only participate in feature learning, and classification only uses the refined visual features. MLIS relaxes the visual-semantic alignment requirement, avoiding performing pair-wise comparisons between the image and the class embeddings.Experiments are conducted on golden GZSL datasets.",
            "strength_and_weaknesses": "The organization is good. Nevertheless\uff0c\n\n\n1.The motivation for the usage of metric learning for improving the generalization is not new. In fact, using metric learning to solve ZSL have been explored in previous works (e.g., Yannick Le Cacheux et al. In ICCV19 ). The overall compared methods are limited, and more methods should be surveyed and compared.\n\n2.The framework is actually incremental improvements on existing methods, e.g., improved on Yongqin Xian et al. in CVPR 2018 by feeding additional unseen features as inputs. From this point, the novelty is limited to the community, since this kind of strategies have been explored in the community (e.g., in LsrGAN of ECCV20). Furthermore, the idea of disentangling feature extraction and classifier training is not new. All these aspects reduced the contributions of this paper to the community. \n\n3.The overall writing is poor. The formulas are also confusing. Please amend them accordingly.\n\n4.How to initialize the network weights, from scratch? What\u2019s the specific network architecture? \n\n5.Since additional operation such as two-step decoupled training is used for the proposed method, I am doubted about the tradeoff between the running time and accuracy.\n\n6.The parameter analysis is shown, however, what\u2019s the specific parameters for achieving these results? It seems the authors have choosed to compare with the methods worse than theirs.",
            "clarity,_quality,_novelty_and_reproducibility": "Limited novelty and far below the standard of ICLR.",
            "summary_of_the_review": "Due to limited novelty, I must reject it!",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper119/Reviewer_wRXx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper119/Reviewer_wRXx"
        ]
    },
    {
        "id": "OAvLfX7LvW",
        "original": null,
        "number": 4,
        "cdate": 1667397923846,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667397923846,
        "tmdate": 1667397923846,
        "tddate": null,
        "forum": "r63dkNZj7I5",
        "replyto": "r63dkNZj7I5",
        "invitation": "ICLR.cc/2023/Conference/Paper119/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper talks about ZSL. The authors ground their motivation on the fact that in-class samples could be still very different from each other, and they propose to tackle this problem in ZSL. The main contribution yields the concept of Metric Learning with Implicit Semantics (MLIS) and it is plugged into the training stage of a generative ZSL model.\n\nThe proposed model is basically extending a GAN in ZSL. MLIS classifies the visual samples by a circle loss when the generative model is being trained. An additional cross-entropy loss is also applied. A default GAN loss counts the last part of the model's training objective.",
            "strength_and_weaknesses": "## Pros\n---\n* Clear expression of the method\n* Good reference to the related articles (though it seems to be too much)\n\n## Cons\n---\n* This paper has several unclear or confusing parts. Some may be due to sloppy writing, but some show a lack of in-depth thinking into the problem.\n  * In Fig. 2, the MLIS loss is termed as $L_{Metric}$, but in the main body, it becomes $L_{circle}$. \n  * $L_{circle}$ is based on cosine similarity. So what about $L_\\text{CE}$? There is no clue of how the class-wise probability $p$ is obtained.\n  * The authors need to clarify whether the generated unseen samples are involved in computing $L_{circle}$ and the values of $L$ and $K$. This is important as otherwise the generative model would not be related with $L_{circle}$ and then could be removed from the entire framework.\n* $L_{circle}$ is built upon concatenated features ($h$ and $a$). Usually, they need to be calibrated before concatenation. Uncalibrated features could lead to problems in computing cosine similarity, including\n  * Bad magnitude normalizer\n  * Un-normalized similarity bias provided by $a$\n* $L_{circle}$ and $L_\\text{CE}$ are functionally overlapping, but their presence is different, i.e., circle loss vs cross-entropy. I think the authors need to explain this inconsistency in loss design.\n* The overall design is not very novel to me. GAN + classification loss just looks like InfoGAN with some add-ons, while the circle loss is also off-the-shelf. The concept of MLIS does not bring anything new as it is just a simple modification on the input side of the circle loss. This does not convince me to be a good ICLR paper.\n* Please be aware of the font difference in subscriptions between $L_{circle}$ and $L_\\text{CE}$.\n* Marginal performance gain",
            "clarity,_quality,_novelty_and_reproducibility": "The overall clarity of this paper is fine, but as is discussed above, some parts need improvement.\n\nI am not saying this paper is novel. Currently, it seems like a simple combination of GAN and circle loss. (See weakness above)\n\nZSL experiments are usually easy to reproduce as datasets are quite small. However, in this paper, a lot of implementation details are not given. Such as the hyper-parameter of the circle loss, the number of intra-class and inter-class pairs and the weights of the losses in training. I am also afraid of the performance on CUB as usually, GAN-based ZSL models are not able to produce such a high H score on it.\n\n",
            "summary_of_the_review": "I would say the quality of this paper is below the expectation of a regular ICLR paper. It lacks novelty, and some designs are not properly explained.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper119/Reviewer_iKnN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper119/Reviewer_iKnN"
        ]
    }
]