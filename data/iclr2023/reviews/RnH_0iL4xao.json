[
    {
        "id": "01poQz9pXG2",
        "original": null,
        "number": 1,
        "cdate": 1666601940639,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601940639,
        "tmdate": 1666601940639,
        "tddate": null,
        "forum": "RnH_0iL4xao",
        "replyto": "RnH_0iL4xao",
        "invitation": "ICLR.cc/2023/Conference/Paper4337/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper examines the perspective of interpreting masked language models (MLMs) as a generative model, where the unmasking procedure corresponds to predicting unary conditionals. The paper notes that the different unmasked tokens are predicted in a conditionally independent way, and focuses previous attempts that instead interpret the MLM as a Markov random field to overcome the conditional independence limitations. \n\nFirst the paper shows the limitations of MRF methods by showing that pairwise MRFs (when two tokens are masked) can be worse than the naive conditionally independent model. Second, they propose an iterative optimization problem, with amortization for computational efficiency, that improves MLM for scoring pairwise tokens.",
            "strength_and_weaknesses": "Strengths\n- the paper identifies that previous attempts at interpreting MLMs as MRFs may not be faithful, in the sense that unary conditionals may not match.\n- the paper improves the 'faithfulness' of the joint distribution via a learned interaction layer which allows modeling the joint with just one evaluation pass (by using a mixing function and the independent unary conditionals).\n\nWeaknesses\n- the contribution only focus on pairwise MRFs, which seems incremental. Why not generalize to cliques of size k for some small k, or arbitrary MRFs with low width?\n\n\nQuestion\n- I had a question throughout the paper, not about this particular paper but about the whole general direction of interpreting MLM as MRFs. It seems that we are jumping through a lot of hoops to define a MRF, find joint distributions with faithful conditionals, run expensive inference, etc.... Can't we simply use the MLM unary conditionals directly in an autoregressive way and avoid all of this trouble? E.g. see https://arxiv.org/abs/2110.02037 https://arxiv.org/abs/2205.13554",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and well structured. The studied problem is important and relevant to the conference.\n",
            "summary_of_the_review": "The problem of generation with MLMs is important and has been studied before. This paper shows improvements with pairwise MRFs and has good writing and execution. The scope of the improvement however, may not be that large.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4337/Reviewer_fxig"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4337/Reviewer_fxig"
        ]
    },
    {
        "id": "7lmQTdbzzoz",
        "original": null,
        "number": 2,
        "cdate": 1666614313085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614313085,
        "tmdate": 1666614313085,
        "tddate": null,
        "forum": "RnH_0iL4xao",
        "replyto": "RnH_0iL4xao",
        "invitation": "ICLR.cc/2023/Conference/Paper4337/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies how a masked language model (MLM) can be used as a traditional language model. The authors consider the case of two masked tokens and they proposed pairwise Markov random fields (MRFs). They experimentally show that pairwise MRFs are worse probabilistic models of language from a perplexity standpoint. They studied two formal approaches for deriving better language models and one algorithmic solution using a feed-forward neural layer.",
            "strength_and_weaknesses": "Pros\n\n* Showing that distributions defined with two masked tokens do not define unary distributions close to that of the original MLM\n* Proposal of statistically founded methods to answer this problem\n* Proposal of an algorithmic solution to give a proxy\n\nCons\n\n* The motivations are not clear at least to me",
            "clarity,_quality,_novelty_and_reproducibility": "The paper introduces so-called pairwise MRFs in order to define language models for MLMs when pairs of words are masked. From an NLP perspective, I do not see for which applications it could be useful to mask pairs of words. For applications, the more interesting case should be when the two tokens are contiguous, i.e. the case for which the independency condition is false. Moreover, if such applications do exist, why would it be useful to consider them as language models for these applications. Let us suppose that pairwise MRFs are useful, now is the question why the unary distributions they defined should coincide with the distributions of the original MLM.\n\nI think the paper could be made clearer: it is not always easy to see which MLM you are speaking of (pairwise MLM, original MLM, parent MLM, ...); it should also made clear at the beginning that a pairwise MLM is not trained with masked pairs but that you consider \"classical\" MLMs. This being said, the study is well designed from both a theoretical and experimental perspective. It does not come as a surprise that the AG model performs better given that the model is optimized for a joint distribution which is faithful to the original MLM. The experimental results for the algorithmic approach are promising.",
            "summary_of_the_review": "The paper contains a statiscally funded study completed by interesting experiments. The motivations should be made explicit. For now, I am not convinced by the usefulness of the study at least from an NLP perspective.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4337/Reviewer_zRNa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4337/Reviewer_zRNa"
        ]
    },
    {
        "id": "89E0ngo98J",
        "original": null,
        "number": 3,
        "cdate": 1666914438980,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666914438980,
        "tmdate": 1666914438980,
        "tddate": null,
        "forum": "RnH_0iL4xao",
        "replyto": "RnH_0iL4xao",
        "invitation": "ICLR.cc/2023/Conference/Paper4337/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "See summary of the review below.",
            "strength_and_weaknesses": "See summary of the review below.",
            "clarity,_quality,_novelty_and_reproducibility": "See summary of the review below.",
            "summary_of_the_review": "I think this is an interesting paper, but I found the motivation for the approach lacking. \n\nThe paper describes various issues in going from MLM parameter estimation objectives, to density estimation over sequences. The paper first describes a number of naive methods (section 3.1.1) that are clearly heuristic, lacking clear guarantees, and which not surprisingly lead to bad performance. The paper then describes prior results from pseudo-likelihood that do lead to well-founded estimators, and give better performance. \n\nThis doesn\u2019t feel like a significant enough contribution for ICLR, for two reasons. 1) it\u2019s not surprising at all that using well-founded methods, known a long way back in the pseudo-likelihood literature, give improvements. 2) density estimation of this type is of limited interest, as auto-regressive models do not have these issues; compelling arguments need to be made for why the MRF-based approach is preferable, and how the method might be scaled from pair-wise interactions.\n\n\nIn addition, I think the motivation/discussion in the introduction needs to be clarified, specifically:\n\n\u201cFrom a probabilistic perspective, masked language models (MLM) make strong independence assumptions\u201d. There is *a lot* riding on the phrase \u201cfrom a probabilistic perspective\u201d here. I think what the authors are getting at is going from the MLM training criterion to a full density p(x) over sentences x - and how this may be challenging given the way current MLM objectives are set up. That\u2019s ok, but two comments: 1) the original MLM objective was never motivated for density estimation, i.e., estimation of p(x), it was used for pretraining/representation learning. It was always clear that there might be some connection through pseudo-likelhood to density estimation, but the method never depended on that link being made. The MLM method is also completely sound in estimating a particular conditional distribution that arises from masking the input sentence. 2) the authors themselves do not give a particularly compelling method to density estimation themselves, which leaves the motivation on somewhat shaky ground. \n\nThe introduction continues in this vain: \u201csuch model misspecifications arising from incorrect statistical assumptions may not be catastrophic\u201d. Again, the original MLM objective did not try to derive a model for p(x), so the term \u201cmodel misspecification\u201d is in my opinion highly misleading here. \n\n\u201cHowever, we observe that MLMs are increasingly being employed as probabilistic models of language\u201d - presumably by a \u201cprobabilistic model of language\u201d you mean a density p(x) over sentences? The authors need to clarify these terms.\n\nI\u2019m concerned that a naive reader will get the impression that MLM methods are fundamentally unsound, and that is just not a correct characterization in my opinion.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4337/Reviewer_cQUX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4337/Reviewer_cQUX"
        ]
    }
]