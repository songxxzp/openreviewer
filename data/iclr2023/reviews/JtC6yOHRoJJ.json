[
    {
        "id": "tmRGBDicuBw",
        "original": null,
        "number": 1,
        "cdate": 1666014816756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666014816756,
        "tmdate": 1666016278661,
        "tddate": null,
        "forum": "JtC6yOHRoJJ",
        "replyto": "JtC6yOHRoJJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3064/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The submission introduces an agent called MEME, which is built on Agent57, which in turn is a super inefficient atari player that requires a ridiculous number of training samples, not even remotely being sensible. Numerous ad-hoc or semi ad-hoc techniques are used (most of which with little to no formal ground) to mitigate various issues of Agent57. The resulting agent (MEME) is able to reduce the training samples by a factor of 200.",
            "strength_and_weaknesses": "Strength:\n\n- An obvious improvement over the Agent57.\n\n- Perhaps the only interesting part of the paper is the trust region for the value function. \n\nWeaknesses:\n\n- Atari is simply a benchmark and not a goal. I honestly do not see any merit in these types of so-called research, especially when considering the tremendous amount of carbon emission and un-necessary energy consumption. \n\n- Most of the presented techniques are ad-hoc with little formal analysis and reasoning. This hardly adds to the common knowledge of the community, and in light of the demanding computational requirements in the development stage of such projects, it is unlikely to open up new avenues for research either. \n\n- Approximate GPU hours for the entire project not reported. As \"responsible research\" has become an important part of the AI community at large, such information helps the community to evaluate the merit of this work aside from possible hypes. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: poor\nNovelty: poor\nReproducibility: No code release is expected, but ample amount of information (including choices of hyper params) is given.",
            "summary_of_the_review": "The paper provides several different modifications to Agent57, nearly none of which is formally studied or justified. As a reader, when I went through different sections, they only raised the question of \"why\". What is clear is that this manuscript is far from a research paper. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "details_of_ethics_concerns": "Too much energy consumption for too little result. I can only imagine how many test runs had to be conducted (hence, how many computational hours) before reaching a point to report the outcome. I raised a flag here mostly for the AC and senior AC to looking into such research practices and whether or not they should be considered as acceptable at ICLR today.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3064/Reviewer_SNik"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3064/Reviewer_SNik"
        ]
    },
    {
        "id": "aKhUdokexa",
        "original": null,
        "number": 2,
        "cdate": 1666533327539,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666533327539,
        "tmdate": 1666533327539,
        "tddate": null,
        "forum": "JtC6yOHRoJJ",
        "replyto": "JtC6yOHRoJJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3064/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper improves Agent57 in the following four aspects, faster propagation of learning signals, handling different value scales, efficient NN architecture, robustness about policy update. It achieves great speedup on training human-level Atari AIs, and 200-fold reduction. \nThe ablation experiments are complete and careful. Also, the authors provide very detailed information in the appendix.\n",
            "strength_and_weaknesses": "**Strength:**\nThe quality of writing and survey are good, and the result is state-of-the-art in terms of training speedups at human level. Data efficiency is a major problem in practical DRL cases; the proposed methods exhibit good results.\nSince this paper has shown great improvement in the aspect of the game frame, I would also like to know how fast it is if we use wall time for counting the speedup (like many distributed methods focused).\n\n**Weaknesses:**\nIt is an unclear part to me about the Retrace update: does MEME use transformed Bellman operator? (There is a \u201cValue function rescaling\u201d in the appendix-A)\nWould the transformed value function be related to the part \u201cB\u201d (handling different value scales) or not? I do not see any mention of the transformed value function in both the main paper or appendix.\n\nA minor comment is about references: it is better to cite papers to conferences, instead of arXiv as listed as follows. \n- Distributed prioritized experience replay => ICLR 2018\n- Image augmentation is all you need: Regularizing deep reinforcement learning from pixels => ICLR 2021\n- Fast and Data Efficient Reinforcement Learning from Pixels via Non-parametric Value Approximation => AAAI 2022\n- Prioritized experience replay => ICLR 2016\n- Data-Efficient Reinforcement Learning with Self-Predictive Representations => ICLR 2021\n- V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control => ICLR 2020\n- CURL: Contrastive Unsupervised Representations for Reinforcement Learning => ICML 2020\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear and easy to follow. The details about hyperparameters for reproducing are dedicatedly listed in the appendix; thus, it seems to be reproduced, especially, with good robustness shown in experiments. \n\n200x in Title is a little tricky, since it does not reach 200x for all Atari games? It would be safe to say that in the abstract. I suggest modifying the title. \n",
            "summary_of_the_review": "Overall, this paper is good, providing several improvement techniques for Agent57 with significant speedups. Also, these techniques are well studied with ablations.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3064/Reviewer_tLk9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3064/Reviewer_tLk9"
        ]
    },
    {
        "id": "yvp4dY1gqh",
        "original": null,
        "number": 3,
        "cdate": 1666679042337,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679042337,
        "tmdate": 1666679042337,
        "tddate": null,
        "forum": "JtC6yOHRoJJ",
        "replyto": "JtC6yOHRoJJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3064/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces MEME, a method built off of Agent57 that enables exceeding the human baseline results with 200x fewer samples than Agent57 required.  This is accomplished by identifying bottlenecks and proposing solutions along with strategies to ensure stability and robustness.  The paper includes thorough experiments demonstrating the benefits of the proposed method and ablations providing intuition about each of the components.",
            "strength_and_weaknesses": "Strengths:\n- Identify a range of issues and bottlenecks in the original Agent57 design and systematically address them in addition to any side effects of their interventions, providing a set of tools that could be applied to a variety of problems along with an evaluation of their effectiveness.\n- The result of these proposed methods is strong, at a 200 fold improvement in sample efficiency and competitive performance to MuZero and Muesli.\n- Very thorough ablations, evaluation of methods on R2D2 in addition to Agent57, and an extended appendix including a list of things the authors tried but didn\u2019t work, facilitating future research.\n\nWeaknesses:\n- This is a strong paper which claims impressive improvements on sample efficiency against Agent57 and supports those claims with thorough experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, the results are novel and the information necessary for reproducing the results is provided in the appendices.",
            "summary_of_the_review": "This paper had a clear and significant goal, dramatically improving the sample efficiency of Agent57, which it states concisely and accomplishes through a wide range of well motivated modifications, as demonstrated through extensive experimentation.  I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3064/Reviewer_zSus"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3064/Reviewer_zSus"
        ]
    }
]