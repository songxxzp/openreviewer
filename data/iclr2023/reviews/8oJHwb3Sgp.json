[
    {
        "id": "rH5qLwITPQ",
        "original": null,
        "number": 1,
        "cdate": 1666460714781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666460714781,
        "tmdate": 1669841060327,
        "tddate": null,
        "forum": "8oJHwb3Sgp",
        "replyto": "8oJHwb3Sgp",
        "invitation": "ICLR.cc/2023/Conference/Paper3511/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents an approach for learning POMDP models from data based on the idea of factoring the transition function of the POMDP into a product of two mappings, going trough a low-dimensional bottleneck layer that will effectively represent the hidden state. Although this is a sound idea, there is no empirical verification of how it might work in practice, in comparison with other existing methods for learning POMDPs.",
            "strength_and_weaknesses": "The paper addresses a central problem in learning suitable models of partially observable environments, particularly when the observation space is large and there is reason to believe that the actual system's state is relatively low dimensional. The presented approach revolves around learning low-level representations of the state, and is thus very suitable for this conference. The idea to factor the transition function into a product using a bottleneck layer (a low-dimensional probability simplex) is appealing, and reminiscent of subspace identification algorithms in the area of linear system identification that use SVD to factor observed data. \n\nHowever, there is no empirical evaluation of the advantages of the proposed approach, so we cannot be sure how effective or even feasible this idea is. Absent such a verification, readers would have very little incentive to understand the mathematical analysis and implement the idea in practice. Because of this, I think the paper is still unfinished.\n\nSome minor typos:\n\nP.4 \"a embedding\" -> \"an embedding\"\nP.4 \"learn-able\" -> \"learnable\"\nP.4: \"we aims\" -> \"we aim\"\nP.5: \"a property motivate us\" -> \"a property motivates us\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not written very clearly and is not easy to follow. The use of terminology does not appear to be standard. The transition probability function of the POMDP is called kernel for unknown reasons, and the two functions it is factored into are called features. Since both terms mean something else in the ML literature, their use in this sense is quite confusing.\n\nThe choice of name for the algorithm, embed to control (ETC), is also confusing, because a well known paper on a very similar topic from 7 years ago literally has this name in its title:\n\nWatter, M., Springenberg, J., Boedecker, J., & Riedmiller, M. (2015). Embed to control: A locally linear latent dynamics model for control from raw images. Advances in neural information processing systems, 28. \n\nAnd, there is nothing to reproduce in this paper, as there are no computational experiments in it. ",
            "summary_of_the_review": "Although based on an appealing idea, it is hard to say if this method would be useful in practice. Some kind of empirical evaluation would help a lot to persuade readers to adopt this method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3511/Reviewer_nfDQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3511/Reviewer_nfDQ"
        ]
    },
    {
        "id": "caXcB6fnPB3",
        "original": null,
        "number": 2,
        "cdate": 1666594961960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594961960,
        "tmdate": 1666594961960,
        "tddate": null,
        "forum": "8oJHwb3Sgp",
        "replyto": "8oJHwb3Sgp",
        "invitation": "ICLR.cc/2023/Conference/Paper3511/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a reinforcement learning algorithm named Embed to Control (ETC), which learns the representation at two levels while optimizing the policy.\n(i) For each step, ETC learns to represent the state with a low-dimensional feature, which factorizes the transition kernel. (ii) Across multiple steps, ETC learns to represent the full history with a low-dimensional embedding, which assembles the per-step feature. Using the two approaches, they developed RL algorithms that is scalable, and the corresponding sample complexities are obtained.",
            "strength_and_weaknesses": "The strength of the paper is it obtained an RL algorithm for POMDPs, and obtained the corresponding sample complexity. \nWeaknesses: The paper introduces several assumptions, and these assumptions seem to strong. Therefore, some discussions would be helpful. \nMoreover, sufficient discussions on comparison with existing results are also lacking. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper contains new results in the sense that they obtained an RL for POMDP with sample complexity. \nThe paper seems to have low readability due to notational complexities. ",
            "summary_of_the_review": "The paper seems to contain useful results. \nThe authors adopted several assumptions that are not clear to figure out how strong they are. \nTherefore, it would be better to discuss more about the assumptions. \nSome efforts are needed to improve readability of the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3511/Reviewer_iXuJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3511/Reviewer_iXuJ"
        ]
    },
    {
        "id": "DxqJXk_z87U",
        "original": null,
        "number": 3,
        "cdate": 1666604877474,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604877474,
        "tmdate": 1669635625758,
        "tddate": null,
        "forum": "8oJHwb3Sgp",
        "replyto": "8oJHwb3Sgp",
        "invitation": "ICLR.cc/2023/Conference/Paper3511/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work identifies four structural assumptions of low-rank POMDPs that allow for designing algorithms\nwith sample complexity that scales:\ni) polynomially in the intrinsic dimension $d$ (transition kernel's rank) and the horizon length $H$; and \nii) *exponentially* with the past and future window sizes $\\ell$ and $k$, respectively.\n\nIn essence, these assumptions ensure that the belief state can be recovered,\ngiven past and future observation windows of sufficient length.\n\nThe proposed theory and algorithmic result (ETC) appear to be novel. \nYet, this work should be taken with a grain of salt (see Weaknesses), as it involves:\ni) several strong assumptions; and\nii) the ETC algorithm has prohibitively large (exponential) sample complexity.\n\nNo empirical evidence is provided.",
            "strength_and_weaknesses": "Strengths\n\nMakes a step towards building the foundations of a formal framework for analyzing POMDPs \nunder the assumptions that the transition kernel admits a low-rank decomposition with\ni) past and future sufficiency; and ii) bounded norms of certain pseudo-inverses.\n\n\nWeaknesses\n\n1. There are several strong assumptions, see below. Further, *no argument is given to show \nthe existence of a range of parameters for which these assumptions are mutually satisfied*.\n\nAssumption 3.5 (Future Sufficiency)\n - there exists $k>0$ such that $M$ is invertible\n\nAssumption 5.1 (Bounded Pseudo-Inverse)\n - the norm of pseudo-inverse of $U$ is bounded ($\\nu$)\n\nAssumption 5.2 (Past Sufficiency)\n - there exists $\\ell>0$ such that $F$ is left invertible\n - the norm of pseudo-inverse of $F$ is bounded ($\\gamma$)\n\n2. Under these assumptions, the ETC's algorithm scales *exponentially* with\nthe past and future window size. Namely, the term $|A|^{ O(k + \\ell) }$ in Thm 5.3.\n\nThe ETC's sample complexity is prohibitively expensive, unless both $k$ and $\\ell$ are either \ni) constants independent of the horizon $H$; or\nii) are bounded by a slowly growing function of the horizon $H$.",
            "clarity,_quality,_novelty_and_reproducibility": "Both the paper's main body and it's supplementary are well organized and clearly written.\n\nThe proposed theory and the ETC algorithmic appear to be novel. \nYet, this result should be taken with a grain of salt, as it involves: \ni) strong assumptions; and \nii) the ETC algorithm has prohibitively large (exponential) sample complexity.\n\nMinor Comments:\n\npage 2: L^p space -> L^p norm space\n\npage 4: rephrase -- \"a embedding that are\"\n\npage 4: $\\phi_h^{\\theta}$ has domain $\\mathcal{S} \\times \\mathcal{A}$\n\npage 5: after \"In addition, recall that we define\": $\\tau_h^k$ should be $\\tau_h^{h+k}$\n\npage 8: Alg. 1. -- Notation with left super-script for observations is not defined\n",
            "summary_of_the_review": "The paper is well written and contributes to the better tackling of POMDPs. However, it remains unclear to me to which extent this paper is a big step forward. The theoretical results are missing the set of parameters where all assumptions hold, and no empirical evidence is provided.\n\n-- post rebuttal update -- \nThanks for the clarifications. I increased my score to 6. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3511/Reviewer_6Ax4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3511/Reviewer_6Ax4"
        ]
    },
    {
        "id": "RkLuWyOCsO",
        "original": null,
        "number": 4,
        "cdate": 1666640537944,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640537944,
        "tmdate": 1666640537944,
        "tddate": null,
        "forum": "8oJHwb3Sgp",
        "replyto": "8oJHwb3Sgp",
        "invitation": "ICLR.cc/2023/Conference/Paper3511/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies sample-efficient learning of POMDPs. The paper considers POMDPs with infinite (continuous) state and observation spaces and admitting low-rank latent transitions with features belonging to a certain feature set. The main result is the identification of a sufficient condition called past- and future-sufficiency, along with an algorithm Embed-To-Control (ETC) that achieves sample-efficient learning in this setting.",
            "strength_and_weaknesses": "Strengths:\n* POMDPs is a challenging problem that received a lot of recent attention in the RL theory community, where additional tractability assumptions are required in order to achieve sample-efficient learning even in the basic tabular setting. The paper generalizes such studies into the infinite-state and observation setting and provides a set of sufficient conditions and a sample-efficient algorithm. This could serve as a starting point of future studies on the same topic.\n\n* The past- and future-sufficiency conditions are tailored to the continuous setting, and are new to my best knowledge. The future-sufficiency condition is similar as the multi-step revealing condition of (Liu et al. 2022), but is slightly different in the choice of the norm ($1\\to 1$ instead of $2\\to 2$) on the matrix pseudo-inverse, which in my opinion is a better choice for the continuous setting (where $L_1$ norms have probabilistic meanings whereas $L_2$ norms may blow up). \n\n* The algorithm achieves sample complexity that is polynomial in $A^{k+l}$ and all other problem parameters including the feature dimension and the past- and future-sufficiency parameters.\n\n* The problem setting allows unknown features, generalizing the recent work of Cai et al. (2022) which assumes known features. This improvement is more minor though as now it is pretty well-understood how to do this in a modular fashion, e.g. in fully-observed MDPs by any algorithm for the Bellman rank or FLAMBE setting. Further, everything about function class is wrapped into the density estimation oracle in this paper. \n\nWeaknesses:\n\n* Despite the paper considers the continuous setting, it feels like the actual algorithm builds upon many existing algorithms, which does not seem to be adequately discussed in the main text. For example, the overall algorithm using confidence set (4.2) sounds like a global optimism algorithm similar as the OMLE algorithm of (Liu et al. 2022), where the risk functional (4.1) looks like the OOM-UCB type per-step risk function of (Jin et al. 2020a). It may be good to expand on these similarities in the paper, and what are the differences in the algorithm / analysis required in the current setting. In turn, maybe some current discussions like the \u201cbottleneck factor interpretation\u201d can be put into the appendix (it feels secondary, though I do like that discussion). \n\n* Could the authors provide more intuitions on why the past-sufficiency parameter? The past-sufficiency looks a bit like the decodability condition (Efroni et al. 2022) for me, whereas future-sufficiency looks like the revealing condition. In tabular POMDPs, either one itself is sufficient for sample-efficient learning. Why does the present approach require both past- and future-sufficiency? Is it an artifact of the algorithm (in which case, pointing out why that is the case would be good for future understandings of this approach). \n\n* Minor point: In the sample complexity, capacity of the function class is wrapped entirely in the density estimation coefficient $w_{\\mathfrak{C}}$. For example, when $|\\Theta|$ is finite, is it the case that we can always get $w_{\\mathfrak{C}}\\le \\log|\\Theta|$ automatically by MLE?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is quite clearly presented, and the results are correct to my best effort of inspection. More detailed comments can be found in the above \"strength and weaknesses\" section.",
            "summary_of_the_review": "The paper provides results for sample-efficient learning of POMDPs with continuous state/observation spaces with unknown features, though it may be a more direct extension of recent works along this line than currently claimed. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3511/Reviewer_reeX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3511/Reviewer_reeX"
        ]
    }
]