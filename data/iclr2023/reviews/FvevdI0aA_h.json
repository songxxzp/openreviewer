[
    {
        "id": "JwdnoooEWOU",
        "original": null,
        "number": 1,
        "cdate": 1666982767170,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666982767170,
        "tmdate": 1670099672056,
        "tddate": null,
        "forum": "FvevdI0aA_h",
        "replyto": "FvevdI0aA_h",
        "invitation": "ICLR.cc/2023/Conference/Paper741/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a unified framework for simultaneously detoxifying and debiasing language models. They consider protected groups as well as toxicity as an attribute of the generated text which can be controlled during the decoding time during an attribute classifier. Authors propose to use an embedding representation based attribute classifier for debiasing and a toxicity classifier for detoxifying tasks. In order to make debaising and detoxifying task parameters efficient, they propose to update only bias terms. Experiments show that the proposed method is computationally efficient and leads to minor degradation in the generation quality. ",
            "strength_and_weaknesses": "### Strengths: \n- To the best of my knowledge, this is the first framework to unify debiasing and detoxifying tasks for open-ended generation.\n- Authors have provided clear motivation and intuitions for most of the design choices. Authors have conducted ablation studies to show the effectiveness of their approach.   \n\n### Weaknesses: \n\n- While the authors emphasize on unification, they use two different versions of attribute classifiers. In particular, for debiasing, they use embedding vectors\u2019s principal component but for detoxifying, they use PPLM\u2019s classifier. It\u2019s not clear 1) why do we need two classifiers?  2) what gains do we get if we use PPLM\u2019s classifier for both tasks.  \n- Authors suggest that their framework updates bias terms in only a few layers. However, for debiasing, they update bias terms across all layers. While they frame it as they do it because their framework is efficient, selecting a few bias terms to update is a non-trivial problem. If we select top-k layers for bias update, we might get different results. Authors don\u2019t have any ablation study to show the effectiveness of selecting bias terms for a few layers. \n- Authors mainly rely on perplexity as a proxy to measure quality of generations which is troublesome because 1) perplexity can\u2019t be trusted as a generation quality metric. 2) authors use perplexity as a threshold. Authors present human evaluation results on a subset but do not show correlation with their automated metrics which makes it hard to understand how trustworthy the results are?   \n- Multiple critical details are missing from the paper which makes it very difficult for an individual to reproduce the results presented in the paper. \n  - Authors construct their own dataset using publicly available BOLD dataset.  The exact dataset used for the experiments is not available in public which makes it difficult to reproduce the results presented in the paper.  \n  - A word can be splitted into multiple subwords, it\u2019s not clear how it\u2019s being handled at attribute classifier level. \n  - Hyperparams used for generations are not defined. Recent works [1, 2] show that decoding algorithm hyperparameters alone play a big role on generated text fairness and quality so it\u2019s important to explicitly define these and ideally use a range of hyperparameters instead of a fixed set of hyperparmas.  \n\nReferences: \n-  [1] Aky\u00fcrek, A. F., Kocyigit, M. Y., Paik, S., & Wijaya, D. (2022). Challenges in Measuring Bias via Open-Ended Language Generation. arXiv preprint arXiv:2205.11601.\n-  [2]  Dhamala, J., Kumar, V., Gupta, R., Chang, K. W., & Galstyan, A. (2022). An Analysis of the Effects of Decoding Algorithms on Fairness in Open-Ended Language Generation. arXiv preprint arXiv:2210.03826\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality:\nPaper cites the relevant literature. However, as pointed out in the weaknesses section, some of the experiments are missing.\n\nNovelty:\nThe proposed framework to unify detoxifying and debiasing is novel. \n\nReproducibility:\nAs pointed out in the weakness section, some critical details to reproduce the results are missing. \n\n",
            "summary_of_the_review": "Debiasing and Detoxifying are very important problems for NLG community. While building solution to tackle bias and toxicity, researchers often treat these as two separate problems. I think this paper is a good contribution towards unification of debiasing and detoxifying solutions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper741/Reviewer_4XSk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper741/Reviewer_4XSk"
        ]
    },
    {
        "id": "9AXUFIAU1Wh",
        "original": null,
        "number": 2,
        "cdate": 1667362803732,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667362803732,
        "tmdate": 1670791965672,
        "tddate": null,
        "forum": "FvevdI0aA_h",
        "replyto": "FvevdI0aA_h",
        "invitation": "ICLR.cc/2023/Conference/Paper741/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a decoding algorithm that reduces (racial, gender) bias and toxicity in the generated outputs of language models. The proposed algorithm consists of token-level and context-dependent components that modify the bias terms of selected or all the layers of a language model at decoding time to suppress or encourage desired behaviors measured by attribute classifiers. Applied to reduce bias between different pairs of prompts and toxicity in output text, the author report improvements over standard evaluation measures for control and quality of the output text. ",
            "strength_and_weaknesses": "Strengths:\n1. Substantial empirical improvements over baselines, especially in the debiasing setup which are confirmed by human evaluation.\n2. Most individual losses proposed are simple and straightforward to implement. \n\nWeaknesses:\n1. The definition of bias is not clear. In 3.1, bias is defined as the difference between f(.) value of two outputs xi and xj generated by different prompts ci and cj. It is not clear if this property is being measured per example or distributionally (the latter makes more sense but it is not clarified in the writing). Additionally, why should it to be two different prompts with biased outputs that should be considered. Shouldn't the goal be given a single prompt, the output should equally likely predict outputs related to either class (gender or race)?\n2. In 3.5, a threshold TH is defined to control for perplexity. This threshold in practice is seems difficult to define properly in realistic settings since given the context length and the output sequence length, the perplexity can vary a lot. \n3. The writing is confusing in many places. Multiple definitions of bias is being used without clarification (social bias vs the bias term in neural networks). confusing notations with $x$ used to refer to both whole sequences and just tokens.\n\nQuestions:\n1. What is the intuition behind using Hellinger distance instead of other distances?\n2. For evaluating global bias, how are ci and cj pairs selected? Are there any restrictions placed on whether the prompts talk about the same topic/content?",
            "clarity,_quality,_novelty_and_reproducibility": "There are clarity issues in many places as described in the previous section\n\nThe method appears novel as far as I am aware of this work. And seems like the results can be reproduced given hyperparameter details. ",
            "summary_of_the_review": "The presented algorithm for debiasing and detoxifying language model outputs is simple to implement and has sound motivations. There are some clarity issues which make the paper hard to follow, especially in section 3. This should be addressable in future versions of this paper, and I am willing to revise my score if my questions are answered and clarifications provided. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "This work presents a method of debiasing and detoxification. They also provide an ethics statement listing limitations and future work, which should be reviewed and verified. ",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper741/Reviewer_Syvk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper741/Reviewer_Syvk"
        ]
    },
    {
        "id": "dmFW4i721xJ",
        "original": null,
        "number": 3,
        "cdate": 1667382371890,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667382371890,
        "tmdate": 1667382371890,
        "tddate": null,
        "forum": "FvevdI0aA_h",
        "replyto": "FvevdI0aA_h",
        "invitation": "ICLR.cc/2023/Conference/Paper741/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents an approach to detoxifying and debiasing the outputs for language models at inference time with per-sample-based parameter-efficient fine-tuning. The paper takes two slightly different approaches to debiasing and detoxification but motivates them as being part of a unified Mutual Information Minimization framework between a generated token \"x\" and an attribute \"a\" (ex: male/female for debiasing or toxic/non-toxic for detoxification). \n\nDebiasing takes an adaptive token-level intervention approach wherein parameter-efficient fine-tuning is done to the model when there is a large margin between p(x|c) and p(x|a,c). Optimization is done to minimize this gap based on a MI minimization objective. Detoxification, in contrast, takes an approach similar to PPLM (Dathathriet al., 2020), but adapts only a small fraction of the top layers of the network.",
            "strength_and_weaknesses": "Strengths:\n1. The paper seeks to solve an important problem, and is well written and motivated until about Section 3.3.\n2. The approach, although quite expensive at inference time, avoids re-training the network.\n3. The token-level attribute classifier for debiasing based on similarities with the first principal components of attribute-specific terms is clever.\n4. Results on detoxification against DExperts which is a fairly strong baseline are impressive.\n\nWeaknesses:\nThe main weakness of this paper is its presentation. The paper's intro, related work, and motivation around the use of mutual information minimization as a tool to do both debiasing as well as mitigate toxic responses are great, but the paper after that seems to lose focus and branches off into two fairly different approaches for debiasing and detoxification. These are my concerns about the presentation and structure\n\n1. Reading appendix section D.2 was critical to my understanding of how the \"redo\" approach works - I would recommend moving it to the main body of the paper.\n\n2. Having a pseudo-code block detailing the overall approach after all of the sections (after 3.5) will help the presentation. Right now it's not clear how each component comes together. Specifically what components do detoxification and debiasing share and what is different? Is one a token-level intervention but the other adopts an approach that re-generates things?\n\n3. It is unclear how your initial motivation around minimizing mutual information I(x;a) ties into the final formulation for both section 3.4 and 3.5. There seems to be a disconnect in the presentation between sections 3.2, 3,3, and 3.4. \n\n4. The paper seems to be missing clear definitions of $p_{\\theta}$ and $p_{\\omega}$ and their parameterizations.\n\nClarification:\nIn Section 3.3, shouldn't we be comparing P(x|c) and P(x|a,c) rather than P (x|c) and P(x|a)? Why is context conditioning missing when conditioning on an attribute? You mention an approach for context-aware rectification but you do not seem to use it in the subsequent formulation.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - Clarity of the paper is good until Section 3.2\nQuality - The paper is of reasonably high quality barring presentation.\nNovelty - The approach is not extremely novel, but is novel enough and borrows interesting ideas from parameter-efficient training\nReproducibility - The paper alone certainly lacks the details needed to reproduce this work.",
            "summary_of_the_review": "The paper has interesting ideas around the use of parameter-efficient tuning at inference time only, but would significantly benefit from re-organization and better presentation of the main ideas.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper741/Reviewer_HmKe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper741/Reviewer_HmKe"
        ]
    }
]