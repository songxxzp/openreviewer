[
    {
        "id": "xcqJQd16YmW",
        "original": null,
        "number": 1,
        "cdate": 1666637041954,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637041954,
        "tmdate": 1669091123580,
        "tddate": null,
        "forum": "fgaiMgCpZV",
        "replyto": "fgaiMgCpZV",
        "invitation": "ICLR.cc/2023/Conference/Paper4744/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors study the sensitivity of convolutional networks (CNNs) to perturbations in frequency components of input images. The authors consider linear convolutional networks. By applying the Fourier transform, in some simple settings, the problems can be reduced to,  essentially, a linear regression (or lasso). Then, tools from linear models (e.g. eigen-decomposition) can be applied to compute the closed-form predictor, which can be used to analyze how sensitive the linear predictor is to perturbation in different Fourier modes. ",
            "strength_and_weaknesses": "# Strength \n\n- The problem studied in this paper is quite interesting. \n- The paper is relatively easy to follow. \n\n# Weaknesses\n\n- The setting is quite simple. The authors study linear convolutional networks, which is not that different from linear models. \n\n- Even for linear convolutional networks, the analysis is only restricted to a couple of simple cases (Theorem 4.9). E.g., channel size all equal to 1 or the depth L\\leq 2. The general, more practical setting (e.g., multiple channels), which required new ideas beyond basic Fourier transforms, is not covered in the paper. \n\n- The reliance on Fourier transform also requires the kernel size ( 3 by 3 in practice) to be the same as the image size (e.g., 32 by 32 for Cifar 10). This restriction reduced the interest and relevance of the approach. Finding minimizers in the frequency domain  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The use of the notation $\\cdot$ (`dot`) is confusing and non-standard. The Fourier transform of a `multi-channel` convolution is not pointwise multiplication, but it is a matrix algebra with entries of the matrix being a linear operator (==convolution in the paper). Though the authors correctly defined such mapping, the `dot` notation shouldn't be used here as it is non-standard. \n\n- Definition 4.6. Why introduce a new notation $C_p$ norm? As you mentioned, this is the Schatten p-norm and which is standard terminology. ",
            "summary_of_the_review": "update \n\nI thank the authors for improving the paper. In particular, the current form of theorem 4.9, if correct, is much stronger than the previous version. As such, I increase my score accordingly. However, I am not comfortable accepting the paper at the moment for the reasons below. 1. the paper is still in the linear convolutional setting, aka, Tibshirani, Gunasekar, etc., though the improvement is not as obvious as the previous version. 2. There are some significant changes in the main text of the paper, as well as the main result Theorem 4.9. Because of so, I would prefer the paper to go through a new review process with the proof of Lemma 4.11 carefully checked. \n\n\n------------------------------\n\n\nOverall, the paper has several interesting observations. However, the contribution is incremental and does not go beyond existing work much, e.g., Tibshirani, Gunasekar, etc. The setting only covers some simple cases of linear convolutional networks. The obtained insights (e.g., sparsity, the predictor is more sensitive to low-frequency mode) seem not that new and can be derived from (deep) linear models. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4744/Reviewer_pf5x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4744/Reviewer_pf5x"
        ]
    },
    {
        "id": "SOhhSRP_TNY",
        "original": null,
        "number": 2,
        "cdate": 1666643422452,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643422452,
        "tmdate": 1669675113365,
        "tddate": null,
        "forum": "fgaiMgCpZV",
        "replyto": "fgaiMgCpZV",
        "invitation": "ICLR.cc/2023/Conference/Paper4744/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a theoretical explanation for the observation of imbalanced frequency sensitivity in CNNs. The paper studies linear CNNs under weight penalties, and derives justification for the dependence of such CNNs' frequency sensitivity on the distribution of power in the data spectrum. Two empirical experiments involving actual non-linear CNNs trained on CIFAR are provided to support some predictions of the proposed theory, namely that adding more layers and increasing weight decay coefficient will both emphasize frequency sensitivity of CNNs to lower frequency components.",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper posits an interesting theorem, and motivates it sufficiently well with some prior evidence. The writing is mostly clear, although the format is wrong. While I don\u2019t think the main claim of the paper is correct, the much more restricted claim that is actually proved is interesting and can help explain some observations in practice.\n\n**Weaknesses**\n\nI have the following concerns/questions about the paper:\n\n1) The main result in theorem 4.9 concerns a very specific case of linear CNNs, and as the paper itself points out is a quite straightforward extension of Tibshirani 2021. Now this in itself is not an issue, what becomes an issue is that the paper does not go much beyond this extension, for example considering the effect of size-limited filters and also the more complicated case of simple ReLU non-linearities, which would make the theory much more substantial.\n\n2) The theoretical results really show the sensitivity caused by the norm penalties, rather than the CNN itself, since removing the penalties (lambda=0) will result in a \u201cnon-sensitive\u201d CNN. In other words, the simplification of CNN structure has stripped it off its many inductive biases, and as a result exposing the inductive bias due to norm penalties, which are well-known already (although not explicitly discussed in the spectrum as far as I know). So it is misleading to claim \u201cCNNs inherit frequency sensitivity from image statistics\u201d where the theory only shows this happening due to norm penalties when CNN is extremely simplified.\n\n3) As the paper itself points out in Section 6, the empirical experiments only on a low-frequency prominent dataset (CIFAR) do not provide any evidence for the main claim of the paper: \u201cCNNs inherit frequency sensitivity from image statistics\u201d. Repeating these experiments on datasets where different frequency bands are prominent (which are trivial to construct) are a strict requirement to support the main claim. Also, the empirical results should always report error bars to show statistical significance of the results (no figure currently shows error bars).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper reads well and is clear for the most part. I think the novelty is limited since the main theorem does not go much beyond the straight forward extensions of known theorems, and the empirical results do not really support the theoretical extensions or the main claim of the paper (as detailed above). The results appear to be sufficiently reproducible.\n\n**Typos**\n\nThe formatting is not for ICLR 2023.\n1/p_i in Conjecture 4.13.\nFigure 6 missing caption.",
            "summary_of_the_review": "The paper provides a theory explaining how linear CNNs under certain assumptions and weight penalties tend to have an inductive bias reflecting the spectral composition of the input data. However, the very restricted theorem, and lack of extensive empirical experiments supporting the theorem and the main claim, limits the paper\u2019s significance and validity.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4744/Reviewer_bWYa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4744/Reviewer_bWYa"
        ]
    },
    {
        "id": "pQUGu8vpRgs",
        "original": null,
        "number": 3,
        "cdate": 1666650206682,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650206682,
        "tmdate": 1666650206682,
        "tddate": null,
        "forum": "fgaiMgCpZV",
        "replyto": "fgaiMgCpZV",
        "invitation": "ICLR.cc/2023/Conference/Paper4744/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "It is unclear what concrete contributions of this paper are. The paper does have quite a lot of equations, but it is hard to say that the derivations are upon a good problem. Experiments are severely lacking.",
            "strength_and_weaknesses": "Strength: lots of derivations.\n\nWeakness: lack of experiments and competing results.\n\nOne fundamental issue: it has been long known that CNNs train better with whitened images -- which is equivalent to equalizing the images in the frequency domain beforehand. There is no room for sensitivity once proper preprocessing is done. The analysis of this paper seems irrelevant and may be in a wrong direction.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty and significance is below the standard for ICLR.\n",
            "summary_of_the_review": "I vote for rejection because the significance of the analysis is low.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4744/Reviewer_yP96"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4744/Reviewer_yP96"
        ]
    }
]