[
    {
        "id": "fVQy2Xsyy_g",
        "original": null,
        "number": 1,
        "cdate": 1666472454793,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666472454793,
        "tmdate": 1666472454793,
        "tddate": null,
        "forum": "g8wBdhnstYz",
        "replyto": "g8wBdhnstYz",
        "invitation": "ICLR.cc/2023/Conference/Paper1227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to overcome the training limitations of VAEs as both manifold learners and generative models.\nEssentially, the proposed AEF architecture is one giant coupling layer.\nIt transforms some dimensions (called the \"core\") using a standard normalizing flow (similar to the use of inverse autoregressive flows for latent variables) and conditions on the other dimensions (called the \"shell\").\nThe key trick to make the whole architecture invertible is to save the residuals of the reconstructed x as additional variables (i.e., the other latent dimensions for a flow model).\nThus, the core latent space + the residuals of the shell space is the entire space and can be inverted.\nThe paper extends this to allowing an augmented space where the augmentation variables are functions of the input, and while this disables observed log likelihood computation, it avoids having to choose the shell vs core.\nFinally, the paper shows that this new approach is usually better than the equivalent VAE in terms of FID, bits per dimension, and qualitative inspection of samples and reconstructions.\n",
            "strength_and_weaknesses": "*Strengths*\n- The approach can use standard VAE and AE architectures directly within the framework. Thus, it provides an elegant bridge between VAEs and flows.\n\n- The expanded AEF version does not require specifying the shell and core variables which makes it more broadly applicable as a dro-in replacement for standard VAEs.\n\n*Weaknesses*\n- How does your work compare to SurVAE Flows [Nielsen et al., 2020]? This work also bridges VAEs and flow models. Can your model be cast as a type of SurVAE Flow?  This seems to be a significant oversight in related works and may suggest another reasonable baseline.\n\nNielsen, D., Jaini, P., Hoogeboom, E., Winther, O., & Welling, M. (2020). Survae flows: Surjections to bridge the gap between vaes and flows. Advances in Neural Information Processing Systems, 33, 12685-12696.\n\n\n- (Minor) It is not clear that the sampling procedure for estimating the likelihood for the expanded AEF is good, and it may be impossible to check. While the VAE values are a true lower bound on log likelihood. The FID and qualitative samples definitely help support the overall claims so this is not necessarily a huge concern but it should be noted that the sampling procedure is merely an approximation to the likelihood and could be above or below the true likelihood. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- Appendix A --- or at least a summary of this, should be included in the main paper. Is this a type of SurVAE Flow layer? They also use delta functions to describe exact log likelihood in one direction. The explanation in the appendix seems a little weak or vague.  For example, the constant in (12) is not explained---it is worth it to expand on this even though I think I understand. Can you expand on this and the intuition in the main paper?\n\n- Figure 1 is definitely helpful for understanding how everything fits together. I wonder if it may be possible to introduce an outline of the approach earlier on to help guide the reader through the steps and through the two different approaches. It is hard to put it all together or see how all the pieces fit together.\n\n- It would be helpful to write down the entire $\\Phi$ function for both AEF models to easily verify invertibility and the overall strucure.\n\n- Organization - Splitting the discussion of the partition and expanded AEFs would help clarify that these are two distinct approaches.  Also, clarifying the pros and cons of each would be helpful. I believe partition AEFs allow for exact log likelihood while expanded AEFs do not. But expanded AEFs are simpler to design and sample from since they do not require specifying the core and shell of the original input.\n\n- Putting the VAE equation and the AEF equation directly next to each other would be helpful (and may be worth repeating the equation). Generally, more discussion on the similarities and differences between the objectives would be interesting.\n",
            "summary_of_the_review": "This paper proposes an simple yet useful combination of autoencoder ideas and normalizing flows to enable flexible VAE-like normalizing flows that could be used as drop-in replacements for standard VAEs. Overall, this is an excellent direction and a nice contribution. The main weakness is the lack of comparison to SurVAE flows, which seem to have similar ideas and bridge VAEs and flows more fully. Though I believe the contributions are still interesting especially by making it a drop-in replacement for VAEs, I believe the proposed technical ideas are less novel because of the prior work in SurVAE flows. A good comparison to this prior work could improve my score.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1227/Reviewer_jiVP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1227/Reviewer_jiVP"
        ]
    },
    {
        "id": "u1Zqshx6rd",
        "original": null,
        "number": 2,
        "cdate": 1666565888672,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666565888672,
        "tmdate": 1666565888672,
        "tddate": null,
        "forum": "g8wBdhnstYz",
        "replyto": "g8wBdhnstYz",
        "invitation": "ICLR.cc/2023/Conference/Paper1227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel scheme for training a VAE by embedding it inside a normalizing flow model on the expanded ambient space.  Specifically, the method uses an invertible network to transform the expanded ambient dimensions followed by an affine coupling layer to compute the \"latent\" variable of the embedded VAE.  To satisfy the invertibility requirement, the output of the VAE decoder is augmented with error (residual) terms.  Authors then empirically show that this flow-VAE hybrid scheme leads to qualitatively sharper images and improved log likelihood on MNIST, CelebA-HQ and ImageNet 32x32.",
            "strength_and_weaknesses": "**Strength**\n* Interesting idea with clearly explained motivations.\n* Clear empirical results, both qualitative and quantitative.\n* Thorough discussion on how the proposed method relates to existing approaches.\n\n**Weakness**\n* Title is misleading -- the proposed method does not perform max-likelihood training of a VAE with an arbitrary architecture.  It instead constructs a normalizing flow model in the expanded ambient space and trains that flow model with exact likelihood instead.  See my first question below.\n* Writing is a bit verbose and could be condensed (see \"Clarity\" below).\n\n**Questions**\n* It's unclear to me how the ambient space expansion affects the likelihood.  Authors point out in Sec 3.4 that $p(x)$ does not have a particular advantage over $p(x, w)$ under the manifold hypothesis, but I believe this point is debatable. More importantly, it would be informative to know how $p(x, w)$ relates to the tightness of the ELBO. \n* I'm curious what the learned error distribution $r(\\delta; \\theta)$ looks like.  Speficially: how does actually sampling $\\delta \\sim r()$ change the samples, and what is the learned value of $\\sigma$?\n\n**Minor comments**\n* Eq. 4 uses $g_{\\sigma}$ but the text uses $g_s$\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\nI believe the paper's overall clarity could be improved by merging subsections 3.1-3.4 into fewer sections.  Since the actual model proposed uses the expanded ambient space, the partitioning idea could be skipped altogether.  It may be helpful to explicitly state the two different interpretations of the model next to each other: (1) as a flow model on the expanded ambient space with the expanded dimensions corresponding to the \"prior\", and the original (\"shell\") dimensions corresponding to the reconstruction error, and (2) as a VAE model with deterministic encoder with Gaussian decoder (though I understand that a different choice of error distribution can lead to non-Gaussian decoder).\n\n**Quality**\nBesides perhaps the verbose writing style, this is a high quality paper.  The idea presented is sensible and is followed by a thorough discussion.  I particularly appreciated the fact that the authors listed current limitations of the proposed approaches in Sec 6. \n\n**Originality**\nI believe the idea proposed in the paper is novel.",
            "summary_of_the_review": "The paper proposes a novel training scheme for VAE by embedding it inside a flow model in an expanded ambient space. The idea is well-motivated, and the empirically results show clear improvement.  I believe the proposed method is an interesting hybrid of VAEs an normalizing flows, and is valuable to the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1227/Reviewer_4VSE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1227/Reviewer_4VSE"
        ]
    },
    {
        "id": "3FPWUiCONE",
        "original": null,
        "number": 3,
        "cdate": 1666644143087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644143087,
        "tmdate": 1666644143087,
        "tddate": null,
        "forum": "g8wBdhnstYz",
        "replyto": "g8wBdhnstYz",
        "invitation": "ICLR.cc/2023/Conference/Paper1227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel generative model class for situations when the data distribution is concentrated around a low dimensional manifold in the input space. The authors propose to enlarge the input space by the outputs of a non-linear parametrised mapping that can be thought to represent the manifold coordinates of a data point. They then define a normalising flow model on this enlarged space, which leads to a deterministic invertible encoder. The corresponding decoder/encoder pair can be learned by maximising the data likelihood. Experiments show that the proposed model outperforms VAEs with comparable architectures on datasets like CelebA and ImageNet.",
            "strength_and_weaknesses": "Paper strengths:\nThe idea to enlarge the ambient space by additional variables that can be interpreted as manifold coordinates and then applying a normalising flow to these base coordinates combined with a second invertible mapping that accounts for the remaining part of input, seems to be well suited for the considered assumptions. The remaining part of the latent code can be then interpreted as independent residual noise. The price is however, that parts of the decoder are needed for the full formulation of the encoder.\n\nThe experiments show that the resulting model provides better results than VAEs with comparable encoder/decoder architectures. \n\nPaper weaknesses:\nI would expect that the model can be better justified (ab initio) by starting from appropriate assumptions about the distribution $p(x,w)$, where $w$ denotes the additional dimensions of the enlarged ambient space.\n\nIn order to learn the parameters of the feature expansion mapping by maximising likelihood, the authors propose to interpret it as a deterministic limit of a normal distribution. This and the requirement to assume distributions with learned scale for the residual part of the latent variables are in my view conceptually not fully convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well structured and clearly written. The proposed model is to my knowledge novel.",
            "summary_of_the_review": "The paper proses a novel type of generative models under the assumption that the data distribution is concentrated around a low dimensional manifold in the input space. The derived likelihood maximisation seems to be correct and outperforming comparable VAEs learned by ELBO. It is perhaps possible to achieve a better ab-initio justification of the approach by starting from appropriate structural assumptions for the distribution $p(x,w)$ on the enlarged ambient space.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1227/Reviewer_Mtne"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1227/Reviewer_Mtne"
        ]
    },
    {
        "id": "bhsxTPT2hlZ",
        "original": null,
        "number": 4,
        "cdate": 1666675706613,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675706613,
        "tmdate": 1668711118501,
        "tddate": null,
        "forum": "g8wBdhnstYz",
        "replyto": "g8wBdhnstYz",
        "invitation": "ICLR.cc/2023/Conference/Paper1227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method to train an auto-encoder in which the mapping to a latent space is invertible, making the (exact) likelihood tractable and so allowing maximum likelihood training. The method involves augmenting the data with additional variables (obtained from a learnable function of the data) and then running a single affine normalizing flow layer on the augmented space. This normalizing flow layer encodes these additional variables using an affine function that depends on the original data and sets the remaining latent variables to be the residual of the data after subtracting the prediction given the latents. The prior in the latent space is Gaussian, and a learnable prior for the residuals encourages the network to drive the residuals to zero and therefore use the other latents.\n\nIn my mental model of this, the prior for the residuals has the feel of a pixelwise Gaussian likelihood in a traditional VAE. A significant difference from standard VAEs, though, is that no noise is added by the encoder.",
            "strength_and_weaknesses": "Strengths:\n - A conceptually interesting method and integration of VAEs with normalising flows\n - Clear and well-structured\n\nWeaknesses:\n - The main weakness of this paper is that the architectures experimented with are far from the state-of-the-art, coming from (Kingma and Welling, 2014). Despite having referenced \"compatibility with the VAE literature\", and specifically VAEs with higher-dimensional latent spaces than data spaces, as a motivation for the ambient space expansion, none of the VAEs in the experiments have this property. I would be interested in e.g. whether this method could be applied in some way to more modern (e.g. hierarchical) VAE architectures.\n- Based on Figure 10 and Table 2, it seems like the advantage of AEFs for denoising only holds for low latent dimensionality, and diminishes as the latent dimensionality increases. This suggests that this method is unlikely to be useful for practical denoising tasks. I would also be worried about whether the same scaling with latent dimensionality holds for other tasks (e.g. use as a generative model).\n- Another possible advantage of older VAE architectures, mentioned by the authors, is the \"interpretability\" of their latent space and \"their ability to project complex data into a semantically meaningful set of latent variables\", but the authors do not demonstrate that the interpretability of their method's latent features (or usefulness for downstream tasks) is any better than that of baselines. I mention this merely because, based on my previously listed weaknesses, the method does not appear to have significant advantages as a generative model or as a denoiser. I would see this as a much stronger paper if the authors could make a convincing case for their method's applicability to at least one use-case.\n\nMinor:\n - Acronym \"AEF\" used in paragraph afer Eq. 5 without having been previously introduced.\n\nOther minor comment:\n - It is not clear to me what properties the learned auxiliary variables ($w=h(x; \\gamma)$) would have. Some intuition about what makes a good auxiliary variable would be interesting.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Generally clear\n\nQuality: See strengths/weaknesses\n\nNovelty: Ideas are novel AFAIK\n\nReproducibility: Seems very reproducible - code is provided along with notebooks",
            "summary_of_the_review": "The proposed method is interesting and demonstrates advantages over the VAEs introduced in (Kingma and Welling, 2014). The paper could be improved further with better demonstration of use-cases for this method, or demonstration of compatibility with more modern (e.g. hierarchical) VAE architectures.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1227/Reviewer_LqjZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1227/Reviewer_LqjZ"
        ]
    }
]