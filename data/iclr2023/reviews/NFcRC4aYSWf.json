[
    {
        "id": "F2sjv0O03t5",
        "original": null,
        "number": 1,
        "cdate": 1666470142997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666470142997,
        "tmdate": 1666470142997,
        "tddate": null,
        "forum": "NFcRC4aYSWf",
        "replyto": "NFcRC4aYSWf",
        "invitation": "ICLR.cc/2023/Conference/Paper1213/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents highway RL, a novel dynamic programming-based strategy for off-policy RL that incorporates multistep information without requiring importance weights. Highway RL replaces the traditional Bellman backup by the maximum over both $n$ and $\\pi$ of the $n$-step return of $\\pi$ followed by the maximum over the learned value function (where $\\pi$ is taken from some candidate set of policies $\\hat \\Pi$). The algorithm is proved to converge in the model-based setting and it is shown that the rate can be better than standard VI under an assumption that the candidate policy set covers the optimal policy. The algorithm is then extended heuristically to the model-free setting. Experiments using the model-based algorithm in some toy problems and the model-free algorithm on MinAtar demonstrate performance better than the chosen baselines.",
            "strength_and_weaknesses": "### Strengths:\n\n1. The paper presents an interesting and novel idea that is potentially impactful in the off-policy RL community. It is a longstanding goal to be able to learn off-policy in a theoretically well-motivated way without needing high-variance importance weights. Hopefully the paper could spur follow-up work that tightens up the main ideas in the model-free case.\n2. The proofs seem to be correct, while limited to the model-based setting where the model of the environment is known to allow for exact computation of the relevant expectations.\n3. While the experiments are somewhat small-scale, they present consistent evidence of improvement, especially in the model-based case where the theory actually holds.\n\n\n### Weaknesses:\n\n1. Important baselines are missing from the experiments. First, there is no comparison to any importance-weighted off-policy algorithm like retrace. Such a baseline is important to actually substantiate the claims made in the paper about the benefit of avoiding importance sampling. Second, it seems from my reading that the Highway DQN algorithm uses maxmin DQN but the multi-step DQN baseline does not. The number of Q functions is important, see fig. 5b, so this seems to be an important issue. If this is the case, then a missing baseline is the multi-step DQN with maxmin learning on top to reduce overestimation and make a fair comparison to the implementation of highway DQN (if I have misunderstood how the implementation works, let me know).\n2. There is little to no discussion of the tradeoffs and potential weaknesses of the algorithm. In particular, I am somewhat concerned about what happens to the algorithm in the model-free case when rather than taking expectations of n-step returns under each of the candidate policies, we instead only see the one trajectory that already exists in the replay buffer. Especially in stochastic environments, maximizing over these single-sample returns seems like it may cause worse than usual overestimation problems and high variance.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. \n\nThe proofs seem to be correct. \n\nThe algorithm seems to be novel and interesting, as explained above.\n\nThe code is available for reproducibility purposes, but I did not attempt to run it. ",
            "summary_of_the_review": "Overall, I liked this paper and think it presents an interesting and novel algorithm. There are still a few issues regarding baselines and discussions of tradeoffs, but these do not seem like fatal flaws, so I rate it as a weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1213/Reviewer_dbJn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1213/Reviewer_dbJn"
        ]
    },
    {
        "id": "XyaHUsfofd",
        "original": null,
        "number": 2,
        "cdate": 1666565914516,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666565914516,
        "tmdate": 1670954610668,
        "tddate": null,
        "forum": "NFcRC4aYSWf",
        "replyto": "NFcRC4aYSWf",
        "invitation": "ICLR.cc/2023/Conference/Paper1213/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes highway RL, which adaptively selects the policy and look-ahead steps for the Bellman operator. The proposed algorithm doesn't need off-policy corrections and hence doesn't suffer from the large variance. In addition, it can also achieve an improved convergence rate than the conventional one-step Bellman operator. The authors also present the application of Q-learning for both continuous and discrete domains. Empirically, the proposed variant can attain better results than the baseline methods, including Multi-step DQN, Maxmin DQN, and Multi-step SARSA on a series of RL benchmarks.\n",
            "strength_and_weaknesses": "Strength:\n- The entire paper is clearly written and easy to follow. \n\n-  The idea of highway RL is quite natural. The authors also provide theories for justification of the algorithm, though the derivations seem mostly to follow from the prior works on the one-step Bellman operator. \n\nWeakness:\n-  The computation of the proposed algorithm is very expensive for each step, though it can achieve an improved convergence rate. It needs to search over the product space of possible policies $\\hat{\\Pi}$ and possible lookahead steps $\\mathcal{N}$, which can be very large. Moreover, the algorithm is also much more complicated than the standard Bellman operator.. \n\n-  Theorem 5 shows that the proposed operator can achieve a convergence rate of $\\gamma^N$. However, if we apply the original Bellman operator for $N$ times for each time step, i.e., $B^{\\circ N}$, we should also achieve a convergence rate of $\\gamma^N$, right? If this is true, I am not sure what's the advantage of your operator over the $B^{\\circ N}$ operator. Let alone the per-step computational cost for your operator is $O((|A|+|\\Pi||\\mathcal{N}|)|S|^2)$ , and  $O(N|A||S|^2)$ for $(B)^{\\circ N}$.\n\n\n-  Can you provide a comparison with the method from He et al., 2017? This work is quite relevant, as they also proposed an algorithm for improving the convergence of Q-learning by looking ahead. The high-level idea is that they build the lower and upper bound of the Q function, and make them two constraints. So, when doing the Bellman update, it needs to solve a constrained optimization problem, which can be efficiently solved using the Lagrange multiplier trick. \n\n\n\n-  Abstract: \"We first derive a new multistep Value Iteration (VI) method that converges to the optimal Value Function (VF) with an exponential contraction rate but linear computational complexity\".\nLinear computational complexity in what? It would be better to state it more explicitly in the abstract, though it can be inferred from the later sections.\n\n- As you mentioned in the introduction (page 2), \n> \"It effectively assigns credit over multiple time steps\", \n\n  I feel the term \"credit assignment\" is a bit vague. What do you mean by assigning credit over multiple time steps? Can you provide a more formal explanation of this?\n\n- Can you explain more formally why your operator doesn't suffer from high variance? \n\n- For equations (9) & (11), why do you need to take a max over {0, n'}? \n\n- Do you have any results on Atari games? The MinAtar Games is not a very commonly adopted benchmark. It would be much more accessible for the readers if you have comparisons on more commonly used benchmarks.\n\n- Page 7: I don't understand why $(B)^{\\circ N}$ will incur a greater computational cost. For your algorithm, each step will suffer from a computational cost of $O((|A|+|\\Pi||\\mathcal{N}|)|S|^2)$ cost, while for $(B)^{\\circ N}$, the cost should only be $O(N|A||S|^2)$, which is smaller than yours. \n\n- Won't $|M_{s_0, a_0}|$ become very large for RL tasks, as the policy will be updated frequently in online learning? This will also increase the computational cost for each update step.\n\n - Figure 4 (a): it seems that increasing the number of steps doesn't improve the performance, and it even makes the performance worse. Can you explain why? \n\n\n======\nPost-rebuttal:\nThanks for the authors for answering my questions. I increase the rating to 5.\n\n[1] He, Frank S., Yang Liu, Alexander G. Schwing, and Jian Peng. \"Learning to play in a day: Faster deep reinforcement learning by optimality tightening.\"  ICLR 2017.",
            "clarity,_quality,_novelty_and_reproducibility": "Yes, it's clear. The paper's idea is original. Reproducibility: it's unclear from the paper, as the authors did not submit the code.",
            "summary_of_the_review": "Overall, I think the authors propose an interesting and theoretically justified idea for improving the convergence rate of the Bellman operator. The idea itself is novel. However, the new operator seems doesn't improve the convergence rate, and the per-step computational cost $O((|A|+|\\Pi||\\mathcal{N}|)|S|^2)$ is pretty large. So, I am not convinced about the improvement of the proposed operator over the standard Bellman operator or  $B^{\\circ N}$. I am happy to increase the rating if the authors can address my concerns raised in the strength and weakness section well. At the current stage, I recommend for rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1213/Reviewer_NWV2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1213/Reviewer_NWV2"
        ]
    },
    {
        "id": "h0dEmFly5R3",
        "original": null,
        "number": 3,
        "cdate": 1666625905403,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625905403,
        "tmdate": 1666625978992,
        "tddate": null,
        "forum": "NFcRC4aYSWf",
        "replyto": "NFcRC4aYSWf",
        "invitation": "ICLR.cc/2023/Conference/Paper1213/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is concerned with solving general reinforcement learning problems under the MDP formulation. The author(s) introduced a highway Bellman operator that enjoys nice theoretical properties when compared to standard one-step or multi-step Bellman operators. Built upon this Bellman operator, three RL algorithms are developed. Numerical studies were conducted to support the claims. ",
            "strength_and_weaknesses": "As far as I can see, the advances of the proposal include:\n\n1. the introduction of the highway Bellman-operator;\n2. the development of the associated RL algorithms built upon the Bellman operator;\n3. the established theoretical results accompanying the Bellman operator;\n4. empirical verifications of the highway algorithm. \n\nBelow are some of the points to discuss:\n\n1. The proposed Bellman operator has very nice theoretical properties. Nonetheless, it seems to involve several hyper-parameters, including the set of the policies as well as the choice of the maximal lookahead steps. It remains unclear to me whether the operator and the associated algorithms are sensitive to the set of policies. How would recommend to set these hyper-parameters in practice?\n2. The theories are mainly concerned with the contraction properties of the Bellman operator. Could you please derive some theoretical results for certain highway algorithms?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and is of good quality. The proposed operators and algorithms are novel. GitHub links are included to reproduce the numerical results. ",
            "summary_of_the_review": "The paper develops a class of highway reinforcement learning algorithms based on a highway Bellman operator. The methods are novel in my opinion. The proposal is justified via theoretical results and numerical experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1213/Reviewer_7nDd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1213/Reviewer_7nDd"
        ]
    },
    {
        "id": "tICCvur0Wcc",
        "original": null,
        "number": 4,
        "cdate": 1667072853069,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667072853069,
        "tmdate": 1667072853069,
        "tddate": null,
        "forum": "NFcRC4aYSWf",
        "replyto": "NFcRC4aYSWf",
        "invitation": "ICLR.cc/2023/Conference/Paper1213/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper suggests a new way of doing multi-step RL using a Bellman operator that first performs a few steps of rollout via behavioral policies, and then doing one step of the optimal Bellman operator. The authors term this new method Highway RL. The paper first shows theoretical results (in an exact DP setting) and then shows how the new operator can be used in VI, Q learning, and DQN. Lastly, they show empirical results.",
            "strength_and_weaknesses": "Strengths: \n- The idea of using suboptimal policies that might need to better credit assignment is an interesting one.\n- The numerical results seem to be promising on a set of difficult problems.\n\nWeaknesses:\n- [Theorem 1] The fact that the new operator is a contraction eventually relies on the single step of B. I feel that it is misleading to state the ||GV - GV'|| <= ||BV - BV'||. In the proof of theorem 1, the fact that 0 \\in N is explicitly used (which, in effect, ignores all of the behavioral policies) to show the \\gamma-contraction property and the fixed point property. \n- [Theorem 5] This result was also underwhelming in that it is basically doing multi-step optimal Bellman operators. The assumption is stated in a way that makes it slightly weaker in that it requires one of the behavioral policies to be equivalent to the optimal policy at a subset of states.\n- A few other papers that use multi-step operators are: \n\"Online Planning with Lookahead Policies\"\n\"Feedback-Based Tree Search for Reinforcement Learning\"\n\"Multiple-Step Greedy Policies in Online and Approximate Reinforcement Learning\"",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a clear way. I think the idea itself seems novel and gives interesting numerical results but at this point, it is more of a heuristic and is not well-supported by the theoretical results, which are lacking/misleading.",
            "summary_of_the_review": "It would be nice to see if the authors could take their idea and do a deeper analysis of how the operator behaves as a function of the quality of the behavioral policies. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1213/Reviewer_ftfH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1213/Reviewer_ftfH"
        ]
    }
]