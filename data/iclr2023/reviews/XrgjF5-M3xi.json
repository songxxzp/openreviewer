[
    {
        "id": "6QqEn5cens",
        "original": null,
        "number": 1,
        "cdate": 1666464856554,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666464856554,
        "tmdate": 1666464856554,
        "tddate": null,
        "forum": "XrgjF5-M3xi",
        "replyto": "XrgjF5-M3xi",
        "invitation": "ICLR.cc/2023/Conference/Paper3845/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method, I-CTRL, for incremental class learning that learns a \u2018closed loop transcription\u2019 between each of the classes and a corresponding subspace of a low-dimensional feature space, know as a linear discriminative representation. The closed loop transcription is achieved by optimising an autoencoder (that maps the inputs to the feature space and back) with a rate reduction-based minimax objective. The objective ensures that the subspaces are highly incoherent to each other, such that updating one does not interfere with the others, preventing catastrophic forgetting. Furthermore, since the subspaces are linear, each class can be summarised with an estimated mean and covariance matrix in the feature space. These summaries are stored in memory as training progresses and are then used to optimise the minimax objective in feature space by essentially ensuring that the features are consistent with their sampled reconstructions through the closed loop transcription. Experiments are run on a number of incremental class learning benchmarks (MNIST, CIFAR10 and Imagenet-50), where the method is shown to outperform all the baselines, which include the SOTA among methods that (a) do not expand their architecture throughout training or (b) maintain a large number of raw exemplars. It is also shown that the method can benefit from unsupervised replay of old examples. The method can be used as both a discriminative and a generative model, and some visualisations are provided that demonstrate the quality of generated samples, as well as the block-diagonal structure of the learnt feature space.",
            "strength_and_weaknesses": "Strengths\n- The method demonstrates impressively strong results on a number of tasks in the challenging incremental learning setting, improving greatly over relevant baselines.\n- The idea is compelling, and although it uses techniques from previous works (Wu et al 2021, Dai et al 2022), it is adapted uniquely for the incremental class learning setting restricted to a fixed network capacity. The novelty of the method, its minimal memory footprint and the impressive results will certainly be of interest to the continual learning community.\n- The paper is easy to read and the method clearly described.\n\nAreas for improvement / questions\n- Though it is clear that I-CTRL has the advantage over Redunet that (i) the results are better and (ii) it does not require dynamic expansion of the network, since they seem to use the same objective function, it would be good to have a clearer description of how the two methods differ in how they use it.\n- Minimax objectives can often be unstable and sensitive to hyperparameters - was there evidence of instability during training and how sensitive is the performance to the values of lambda and gamma that weight the rate reduction losses for the new features and old features respectively?\n- Though the results without augmentation are reported in the appendix are still better than the baselines ( though 6-7% worse accuracy than with augmentations), would it not be fairer to use these as comparison to other methods, which presumably do not use augmentation (from a glance at the EEC paper, it does not).\n- Many would disagree with the decision to discard methods that keep raw exemplars as unfair comparisons and on this basis call the results of I-CTRL SOTA (which they would not be otherwise, e.g. Dark Experience Replay outperforms I-CTRL on CIFAR-10 with a relatively modest buffer size of 500 exemplars). Indeed ICTRL still makes use of a memory buffer and one version of iCTRL that is advertised involves a form of replay of previous exemplars. While I certainly believe that the minimal memory capacity required by I-CTRL is a great feature and is rightly highlighted in the paper, the SOTA claim is perhaps a little strong given that one can easily argue that replay-based methods are very practical continual learning methods given that memory storage is generally cheap. \n- It is mentioned in the introduction that EWC requires application-specific external mechanisms - what aspect of EWC is application-specific?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. It is explicit about the techniques it has adopted from other papers, and the use of them appears to be novel, though it would be helpful to include a clearer description of, for example, how the use of the rate reduction objective is different in (Wu et al 2021), and in what setting the closed loop transcription is used in Dai et al 2022. Algorithmic pseudo-code is provided in the appendix but no actual code is provided in the supplementary material.",
            "summary_of_the_review": "This paper presents a novel approach to continual learning along with impressive results on incremental class learning benchmarks with a minimal memory footprint. While the claims of SOTA are potentially misleading, when a large class of methods (dynamic architectures and replay-based methods) are questionably discarded as unfair comparisons, the results are undoubtedly strong and will be of great interest to the CL community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3845/Reviewer_HArM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3845/Reviewer_HArM"
        ]
    },
    {
        "id": "Uy9X8kAV7j",
        "original": null,
        "number": 2,
        "cdate": 1666535175020,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535175020,
        "tmdate": 1668942557091,
        "tddate": null,
        "forum": "XrgjF5-M3xi",
        "replyto": "XrgjF5-M3xi",
        "invitation": "ICLR.cc/2023/Conference/Paper3845/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors present a method to mitigate forgetting in Continual Learning based on closed-loop transcription (CTRL). The authors adapt CTRL to condense the classes' information in a memory composed of an encoder and decoder capable of generating representations of previous tasks. Using linear discriminative representations, CTRL encourages the models to learn compressed representations corresponding to different classes so as to maximize discriminative and generative properties. Taking advantage of these features, the authors propose to store the mean and covariance of the Z representations for each class, and then use these to mitigate forgetting, aiming for the covariance to remain stable while training a new task. The authors show promising results in various benchmarks.",
            "strength_and_weaknesses": "S:\n- The idea presented is very well-motivated not only from CL's point of view but also from the perspective of how we accumulate knowledge as humans.\n- The fact that it is a generative model with fixed capacity makes it an exciting proposal since it does not require adding new weights to a model or re-training models with the problems of overfitting a small group of data.\n- Applying CTRL to an incremental learning environment is a smart idea. Accumulating \"classes\" in separate subspaces and saving this space's mean and covariance can help to mitigate forgetting.\n- In general, the ideas are well explained, and the structure of the paper is adequate.\n- The experiments carried out in section 4.3 are interesting. It might have been interesting to see how it behaves with larger images like ImageNet.\n\nW:\n- One problem is that the method is incremental from CTRL. The authors propose only minor (but critical) modifications to CTRL to apply it to CL.\n- A possible limitation of this model is the subspaces that the model finds. I imagine that as the number of classes increases, finding incoherent subspaces becomes more challenging. To the authors: Did you test with benchmarks with more classes? Like CIFAR100 or TinyImageNet, which has 200 classes.\n\nQ:\n- I wonder if the first subspaces that the model learns can affect the model's behavior in the future. I imagine that learning easy or difficult subspaces in the first tasks can affect in one way or another the subspaces that the model finds later. What are your opinions about it?\n- The constraint added in Eq 8 and 9 can be seen as a regularization similar to distillation on the covariance. The gamma value regulates the importance of maintaining the subspace of the previous classes. Did you carry out experiments on how this value affects training?\n- What does the Union in Eq3 represent? The concatenation?\n- How many times was each experiment run? Could you get the std from the experiments?\n- I like the idea of \u200b\u200busing the nearest neighbor as a classifier. The other methods used a typical classifier or something like iCARL? There is evidence that most forgetting occurs in the classifier, so it might be interesting to compare it equally.\n- In the section \"Improving discriminativeness of memory\". The factor that regulates the difference between covariances, is used? Or is using gamma equal to zero?\n\nI recommend changing Fig5. It is not easy to see that the photos are grouped in rows of two.",
            "clarity,_quality,_novelty_and_reproducibility": "The authors present their motivations and contributions in a precise way. Despite only adding minor modifications to what was presented in past works, these are important for the correct functioning of the method.\n\nI believe that using generative models is a line of research with a lot of potentials.",
            "summary_of_the_review": "I found the paper well written, with its motivations and results correctly showing the contributions. My only concern is a possible limitation not mentioned by the authors: the capacity to store many tasks/classes of the subspace (what happens when we add more than 50 tasks).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3845/Reviewer_72tt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3845/Reviewer_72tt"
        ]
    },
    {
        "id": "gDTNQkahK0",
        "original": null,
        "number": 3,
        "cdate": 1666793557806,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666793557806,
        "tmdate": 1666793557806,
        "tddate": null,
        "forum": "XrgjF5-M3xi",
        "replyto": "XrgjF5-M3xi",
        "invitation": "ICLR.cc/2023/Conference/Paper3845/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides a framework for incrementally learning a generative and discriminative model for multi-class problems using a Linear Discriminative Representation (LDR) and a coding rate reduction objective. The closed-loop transcription framework (CTRL) is formulated as a minimax game wherein the encoder tries to maximize the rate reduction objective and the decoder minimizes it instead. The \"distance\" between Gaussian ensemble subspaces is estimated.\n",
            "strength_and_weaknesses": "Strength\n- Paper has extensive theoretical and empirical analysis\n- It is written coherently and studies related work carefully",
            "clarity,_quality,_novelty_and_reproducibility": "- Well written\n- Quality: High\n- Reproducibility -- Not discussed in the paper",
            "summary_of_the_review": "The paper provides a framework for incrementally learning a generative and discriminative model for multi-class problems using a Linear Discriminative Representation (LDR) and a coding rate reduction objective. \n\nThe closed-loop transcription framework (CTRL) is formulated as a minimax game wherein the encoder tries to maximize the rate reduction objective and the decoder minimizes it instead. The \"distance\" between Gaussian ensemble subspaces is estimated.\n\nThe authors state \"To this end, we simply sample m representative prototype features on the subspace along its top r principal components, and denote these features as Zj,old\" -> This step clearly depends on how well the principal components capture the variance of the data in the first place and affects the samples collected using the mean and variance of Zj,old.\na. How robust is the scheme with imbalanced classes?\nb. If incremental learning process learns from a small subset of data, this could affect the principal components (and hence their variances). How is this dealt with?\nc. Are there empirical results to justify how a change in percentage of variance of principal components affects the overall performance of the algorithm -- in terms of performance metrics, imbalanced multiple classes and relevant details? \nd. Can affinity based subspaces be used to measure the effect of percentage of variance captured by principal components?\n\nAppendix A contains a lot of crucial algorithmic details and the authors should consider incorporating those in the main paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3845/Reviewer_sS4d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3845/Reviewer_sS4d"
        ]
    }
]