[
    {
        "id": "Kr8R1aIpS8",
        "original": null,
        "number": 1,
        "cdate": 1665993873553,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665993873553,
        "tmdate": 1669707154317,
        "tddate": null,
        "forum": "uu1GBD9SlLe",
        "replyto": "uu1GBD9SlLe",
        "invitation": "ICLR.cc/2023/Conference/Paper2605/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "$k$NN-NMT is a straightforward yet powerful approach for fast domain adaptation, but is burdened with massive storage requirements\nand high computational complexity. This paper propose an efficient variant of $k$NN-MT that drastically increase speed in two folds:\n1) search similar sentences by search engine instead of searching over all tokens in training datasets\n2) use a distance-aware adaptor to adaptively incorporate the kNN retrieval results into the pre-trained NMT models.\n\n",
            "strength_and_weaknesses": "**Strength**\n* The method is simple yet effective on multiple adaptation tasks.\n* The writing is clear and the paper is easy to follow.\n\n**Weaknesses**\n* $k$NN-MT is not only good at (narrow-)domain adaptation, but also good at improving performance on large-scale open-domain task like WMT en-de, however there is no corresponding experiments in this paper. Actually, I suspect that the sentence-level retrieval may only work well on narrow-domain data as reported in [2].\n* Regarding the two major contribution of this paper, i.e. 1) search similar sentences by search engine and 2) distance-aware adaptor, there are existed works [1][2][3]. I would give authors the credit of combining of them together in $k$NN-MT setting, but I think the contribution may not be significant enough. Directly comparing the proposed method with [1] and [2] and siginificant improvements could address my concern.\n* Although the compuation cost of searching $k$NN is alleviated, the proposed method still need to compute hidden representation of $k$ nearest sentences on-the-fly, this is actually another heavy compuational burden on GPU.\n\n**References**\n\n[1] Search engine guided neural machine translation. Jiatao Gu et al., AAAI 2018\n\n[2] Guiding Neural Machine Translation with Retrieved Translation Pieces. Zhang et al., NAACL 2018 \n\n[3] Adaptive nearest neighbor machine translation. Zheng et al., ACL2021",
            "clarity,_quality,_novelty_and_reproducibility": "See strengths and weaknesses",
            "summary_of_the_review": "Overall, the experimental results on adaptaion tasks seems good, but lack of experiments on large-scale open-domain dataset like WMT. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2605/Reviewer_XTZD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2605/Reviewer_XTZD"
        ]
    },
    {
        "id": "4jFUgFq7AKw",
        "original": null,
        "number": 2,
        "cdate": 1666585294017,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585294017,
        "tmdate": 1669077344662,
        "tddate": null,
        "forum": "uu1GBD9SlLe",
        "replyto": "uu1GBD9SlLe",
        "invitation": "ICLR.cc/2023/Conference/Paper2605/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes two very simple methods to improve the efficiency and effectiveness of k-nearest neighbor machine translation by 1) dynamically creating a datastore of bilingual sentences given an input sentence using BM25, and 2) dynamically adjusting a coefficient to scale the contribution from the similarity scores of extracted instances.\n\nExperiments are carried out in two scenarios, MT domain adaptation and online learning. In the domain adaptation task, the proposed work achieved significant speed up without loosing the MT qualities. In the online learning task, e.g., iteratively enlarging the training data, the proposed method also achieved better results when compared with prior baselines.",
            "strength_and_weaknesses": "Strength\n\n* The use of data retrieval based on BM25 sounds a simple yet efficient method to solve the computational problem in kNN-MT. The experiments in Table 3 shows reduced latencies by extracting only relevant bilingual sentences without loss in translation qualities as shown in Table 2.\n\n* The dynamic adaptation for the coefficient in Equation 6 is very simple and experiments demonstrate the gains.\n\nWeakness\n\n* This work presents experiments in two tasks, domain adaptation and online learning, and does not present any results regarding translation in general, e.g., training data on news domain and testing on news domain. This work should also include those results to discuss whether the proposed method also works in a standard task setting. Even if the results were negative, they should be provided for further discussion.\n\n  - The additional experimental results comparing for generic dataset sounds reasonable to me. \n\n* The proposed dynamic coefficient scaling seems to be very effective in adjusting the tradeoff of the extracted dataset, but it is not clear whether the approach is only effective to the BM25-based filtering or not. This work should compare the use of the dynamic scaling for other baselines, e.g., the original kNN MT and its variations, to investigate whether the approach is orthogonal to the retrieval methods or not.\n\n  - I also like the additional experiment, and that should be included in the main body.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to follow and the proposed method is very clear. The proposed method will be easily reproduced given the simplicity of the proposed approach.",
            "summary_of_the_review": "This work presents an interesting and very simple approach to solve the efficiency problem in kNN MT. Experiments are well designed, but it is missing an important question on how well the proposed approach might work on a standard setting, i.e., training and testing on the same domain with large data. Also, the dynamic scaling sounds a nice contribution to the field, but it is not clear whether the approach could be employed in other retrieval methods or not.\n\n- Additional experiments will be useful for this community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2605/Reviewer_TZ41"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2605/Reviewer_TZ41"
        ]
    },
    {
        "id": "9y5SzosprJ",
        "original": null,
        "number": 3,
        "cdate": 1666649131148,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649131148,
        "tmdate": 1669663562566,
        "tddate": null,
        "forum": "uu1GBD9SlLe",
        "replyto": "uu1GBD9SlLe",
        "invitation": "ICLR.cc/2023/Conference/Paper2605/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents SK-MT which makes storage efficiency improvements over the kNN-MT model. The efficiency comes in the form of a dynamic datastore construction strategy that uses token-level retrieval (BM25) to first get a set of relevant translation pairs, and then construct a datastore from this smaller subset. To alleviate noise from a smaller datastore, an adaptive interpolation parameter is computed based on the ratio of the top-1 neighbor\u2019s distance and the softmax temperature used with kNN-MT. This is a simpler adaptation approach than adaptive kNN-MT that involves learning a separate network for adaptation.\n\nExperiments are conducted on two MT tasks, reporting BLEU and character n-gram f-scores.  \n- Domain adaptation using the multi-domains data: the proposed SK-MT models are on par with the full-datastore baselines (kNN-MT and adaptive kNN-MT). The proposed SK-MT method reduces storage size of the datastore considerably and eliminates the need for a faiss index, but doesn\u2019t necessarily run faster than the full datastore baselines when they search the faiss index on the GPU.\n- MT with humans in the loop providing post-edits and corrections: this involves growing the datastores dynamically as the human edits become available. The two corpora include EMA and JRCAquis. The proposed SK-MT method is on par with the KoK baseline that operates over the full datastore with an adaptive interpolation parameter.\n",
            "strength_and_weaknesses": "Strengths:  \n1. This paper brings the approach from Gu et al. (2018) to modern transformers and kNN-MT style approaches that don\u2019t require training the model, while having much lower storage costs for the datastore which makes brute-force search faster.\n2. It proposes a simplification to adaptive kNN-MT that does not require training a separate network. Ablations show that the proposed adaptation is especially important for the dynamically constructed datastores.\n3. The proposed approach is on par with the full datastore kNN-MT and variants.\n4. The paper provides latency scores as well as an analysis of word accuracy across methods based on the frequency of the words.\n5. The paper provides an analysis of the rate at which various approaches adapt to domain-specific words.\n\nWeaknesses: \n1. The paper\u2019s primary proposed approach for building the index dynamically seems very similar to Gu et al. (2018) and specifically their shallow fusion variant. Yet, the paper doesn\u2019t really explore this connection or even attribute this approach to them. The results and analyses are interesting, but a discussion on this connection seems important and is currently missing. \n\n2. While the decrease in storage costs is nice, the decoding speed improvements aren\u2019t clearly there due to faiss-gpu, especially when the number of retrieved translation pairs increases from 2 to 16. The current story and discussion should be revised to remove the claim that this approach improves decoding speed, or it should be revised to clearly state that a trade-off exists \u2013 better decoding speed comes at the cost of lower performance. Adding a plot on latency vs. performance would go nicely with a discussion on the trade-off. \n\n3. The paper calls this approach scalable but it is unclear how the number of retrieved translation pairs (m) scales with the size of the corpus. Is this claim justified somewhere in the paper through trends based on corpus size? The original kNN-MT paper shows that larger datastores provide diminishing returns so perhaps there is some evidence of this in the literature but missed the discussion in the paper.\n\n4. The discussion on online learning experiments needs more detail. It\u2019s not very clear what the setup is for the different approaches, especially SK-MT. Is there some constraint that selects human provided translations, or are the token-level retrievals only over the human provided translations?\n\n5. Related to the previous discussion, the analysis on R-values is also a bit confusing. Given the current explanation of the online learning experiments, shouldn\u2019t the R-value for all kNN approaches converge to the same value for tokens that have appeared 9+ times? Why do the full datastore retrievals ignore the human feedback but SK-MT doesn\u2019t, or is something else going on? Also how important is it to upweight the retrievals from human feedback? What if there were a completely separate kNN function operating on the human feedback combined with kNN over the dataset and the base MT system?\n\n6. Overall, given the connection to Gu et al. (2018) and the lack of decoding speed improvements, it isn\u2019t clear if the approach makes enough of a case for added value even though the results and analysis are interesting \u2013 at least not in the current narrative of the paper. Why is this approach better than just adaptive kNN-MT if it doesn\u2019t run faster? Both sets of experiments show similar performance to full datastore but adaptive versions of kNN-MT. Maybe one argument is that lower storage makes this viable for running kNN-MT on-device where there are storage constraints? \n\nQuestions:\n1. Why are numbers in Tables 2 and 4 for IT different?\n\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. Search engine guided neural machine translation. In AAAI, 2018.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper could improve on clarity. See suggestions in the previous section. Also add descriptions of the baselines to at least the appendix (or main paper if there is space). \n\nThe experiments and analysis are interesting, even given the connection to previous work which hurts novelty a bit. But, the framing and discussion are not clear or comprehensive which currently hurts the quality of the work. \n\nThere is no mention of code release, but the paper includes details on hyperparameters and setup that would probably help reproducibility, at least on the domain adaptation experiments.\n",
            "summary_of_the_review": "Recommending rejection because the current version of the paper is missing an important connection to prior work and a number of claims are unclear and/or unjustified. Open to revising the recommendation after the discussion period.\n\nEDIT after author response:  \nThe authors did a good job of addressing the primary concerns raised in the initial review. While the novelty of their approach is still limited, this paper makes contributions on the empirical side and provides data points that could be valuable for the community. For this reason, I\u2019m raising my score to a 6.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2605/Reviewer_n9iv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2605/Reviewer_n9iv"
        ]
    },
    {
        "id": "iH-K7jL-kU9",
        "original": null,
        "number": 4,
        "cdate": 1667466419004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667466419004,
        "tmdate": 1667466419004,
        "tddate": null,
        "forum": "uu1GBD9SlLe",
        "replyto": "uu1GBD9SlLe",
        "invitation": "ICLR.cc/2023/Conference/Paper2605/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work is to improve the efficiency of the KNN-MT framework. In this work, the author proposed a simple and scalable framework to speed up the KNN search. By utilizing sentence level retrieval approach (BM25), the author reduced the search pool for each input. The experiment results shows comparable performance with KNN-MT and reduced time/storage cost.",
            "strength_and_weaknesses": "Strength: The proposed method is reasonable and effective to improve the kNN-MT framework by reducing the search space. \nWeakness: More analysis on m and k choice is needed. If k=1 or 2, is the always best choice for kNN-MT, which means the rest examples are all far away, then we just keep top-1 token pair is ok and does not need sentence retrievel.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and the quality is good. The proposed idea is reasonable and novel but simple.",
            "summary_of_the_review": "Overall, the work is simple but effective but needs further justification mentioned in the k setup to verify if the method is solid or it is due to the task dataset.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2605/Reviewer_wAt2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2605/Reviewer_wAt2"
        ]
    }
]