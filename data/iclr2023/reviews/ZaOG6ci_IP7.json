[
    {
        "id": "eFw-8LnX_X",
        "original": null,
        "number": 1,
        "cdate": 1666410982492,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666410982492,
        "tmdate": 1666410982492,
        "tddate": null,
        "forum": "ZaOG6ci_IP7",
        "replyto": "ZaOG6ci_IP7",
        "invitation": "ICLR.cc/2023/Conference/Paper231/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a method called kaBEDONN that produces \"relevant\" samples as a local explanation. ",
            "strength_and_weaknesses": "Strengths:\n- The idea of generating more relevant samples as local explanations is interesting.\n\nWeaknesses:\n- No comparison to other post-hoc methods that generate samples as explanations, despite referencing them.\n- Method performs worse than a CNN (table 1). This could be acceptable, since this is a post-hoc method. Standard deviations would help in judging the significance of these results.\n- Interpretability evaluation a bit lacking -- are the sample explanations claimed to be more relevant generated by the method actually perceived as more relevant / helpful / interpretable by humans?",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of generating more relevant samples as local explanations is interesting, but in my opinion this paper falls a bit short in demonstrating the value of relevance towards interpretability. The method appears to be a direct application of an existing neural net model, Semi-Quantized Activation Neural Network (SQANN), without significant modification. More clarity in the writing would also help this paper.\n\nNits:\n- \"damage control\" term in the abstract --> a bit casual\n- I found many of the figures too small and hard to read.\n- Figure 4 presents some samples and asks the reader to compare to a particular figure in Koh and Liang 2017. This seems a bit unusual.",
            "summary_of_the_review": "The paper has an interesting idea of generating more relevant samples as local explanations. However, the empirical evaluation felt incomplete (see above), and a user study to link relevance to interpretability is missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper231/Reviewer_ZXST"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper231/Reviewer_ZXST"
        ]
    },
    {
        "id": "ABbs_zIMDe",
        "original": null,
        "number": 2,
        "cdate": 1666551288380,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666551288380,
        "tmdate": 1666551288380,
        "tddate": null,
        "forum": "ZaOG6ci_IP7",
        "replyto": "ZaOG6ci_IP7",
        "invitation": "ICLR.cc/2023/Conference/Paper231/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents kaBEDONN, a method for post hoc explaining neural network classifiers by identifying relevant data. It is inspired by existing sample importance-based explanations, such as the influence function and representer point. ",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper tackles an important problem: the interpretability of neural networks. \n\nThe proposed architecture seems novel. \n\n**Weaknesses**\n\nThe writing quality is poor. Generally, this does not feel like a well-polished academic paper. I would recommend the authors to ask others not involved in this project to read the paper and identify points of confusions. \n\nIt is not clear how this model supports the three objectives described in Sec. 1, especially the latter two, which seem to be not demonstrated. \n\nThere is no comparison with existing sample-importance methods in the experiment. \n\nIt is not clear what conclusions can be drawn from Fig. 4 results. It looks like main node often is the same as data sample. Why should this happen? If the data sample is from the test set, there shouldn't be a duplicate in the training set? It's not clear what additional insights we could get from the WR/Sub nodes either. \n\nWhy are the results in ImageNet not centrally highlighted? I would expect those to be much more interesting than that on MNIST and CIFAR, but most of the results are on the latter. \n\nMinor: \n\nI recommend the authors to familiarize themselves with latex commands `\\citep` and `\\citet`. ",
            "clarity,_quality,_novelty_and_reproducibility": "See above. ",
            "summary_of_the_review": "Due to the numerous issues with the current draft, I vote for rejection. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper231/Reviewer_gXuS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper231/Reviewer_gXuS"
        ]
    }
]