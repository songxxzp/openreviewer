[
    {
        "id": "fqiLA5g_mJ",
        "original": null,
        "number": 1,
        "cdate": 1666138464542,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666138464542,
        "tmdate": 1668617029618,
        "tddate": null,
        "forum": "Bo7eeXm6An8",
        "replyto": "Bo7eeXm6An8",
        "invitation": "ICLR.cc/2023/Conference/Paper744/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a framework for deriving multi-lingual data from an execution-based dataset (e.g., MBPP, HumanEval, MathQA), which entails applying rule-based transformations based on static analysis to the prompt and test cases. The key idea is that the canonical solution does not need to be translated since evaluation is done by running the test cases. Through this, they translate MBPP into 10+ languages. They train large decoder-only monolingual and multi-lingual models, of various sizes, using three languages, and they evaluate them in out-of-domain and in-domain settings. They highlight the benefits of the large multi-lingual models especially in the out-of-domain settings. They also consider few-shot learning, which results in fewer errors, and is most advantageous in out-of-domain evaluation. They further consider other evaluation tasks like zero-shot translation, code insertion, and summarization. Additionally, their framework also allows deriving synthetic canonical solutions for tasks which require it.\n\nContributions:\n- A framework for acquiring multi-lingual data from a monolingual dataset, without directly requiring translation of code solutions.\n- A large multilingual dataset which can be used for studying a variety of code-related end tasks, spanning multiple languages.\n- Empirical results and observations, spanning many languages, model sizes, settings, and tasks.\n",
            "strength_and_weaknesses": "Strengths:\n- Deriving multilingual code data without having to translate code solutions (especially considering how error-prone code translation models are) is a neat idea and useful.\n- Since it is challenging to get parallel data to study tasks like code translation, this framework which can give synthetic canonical solutions could be particularly useful for studying that tasks.\n- There are extensive empirical results, including supplementary analyses (e.g., best combos of source/target pairs for zero-shot translation) and even minor observations (e.g., sampling efficiency, validation losses) that can be beneficial for guiding future work.\n\nWeaknesses:\n- One concern I have is regarding whether potential biases/assumptions in the original MBPP dataset can transfer over to the MBXP datasets. These could be possible ambiguities (as the authors have mentioned  in Section 2.1) but also more fundamental biases/assumptions, such as language-specific ones. Could the formulation of the prompt and the types of tests that are needed drastically change based on the specific language, depending on for example, whether it is a language that requires explicit memory management?\n- While the synthetic canonical solutions may be useful for training during different tasks like translation, I find that using them for test to be slightly problematic because these are model-generated, and possible errors/biases in this output could propagate, and we don\u2019t want to be evaluating using such references. However, these execution-based sets like MBPP are quite small, so it is not clear whether they offer enough training data to have a meaningful impact.\n- It is not clear whether training the monolingual and multilingual models in Section 3.1 was necessary. As described in Appendix M, CodeGen entails monolingual and multilingual models trained on a subset of the languages studied in this work, including Python Java and Javascript, which were used in this work. Based on the results shown in  Appendix M, it seems that many of those models are better performing than the ones trained in this work. Most (if not all) of the analyses could have been done with these.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is well-written and easy to follow. One minor point is that there is some ambiguity in which model is being shown in Figure 3.\n\nNovelty:\nThe idea to effectively \u201ctranslate\u201d the prompt and test cases, which may have simpler structures, is novel and interesting. It can also have broader applications for different natural languages, when extrinsic-evaluation is used. There is empirical novelty in terms of a new dataset that spans multiple programming languages, as well as with a lot of new empirical results for many different programming languages (e.g., in-domain vs out-of-domain training, monolingual vs multi-lingual training, model sizes, source/target pairs for translation). Additionally, the technical novelty seems to be somewhat limited, as the rule-based static analysis techniques used to build the framework seem to be derived from existing work/tools. \n\nQuality:\nThe experiments are well-designed and findings are well-grounded. \n\nReproducibility:\nIf the datasets are publicly released (including the pretraining ones), the experiments should be reproducible. There is a detailed Appendix with training details provided in Appendix N.\n\n\n",
            "summary_of_the_review": "Overall, the intuition and motivation behind this work are neat. The paper presents interesting empirical results and observations related to multilingual aspects of code generation, especially considering much of the focus in recent years has been on only a few languages like Python and Java. They also present a dataset spanning multiple languages, which could potentially be useful for the broader research community, although I have some concerns about whether their framework which effectively \u201ctranslates\u201d prompts and test cases in existing datasets might be propagating biases. Additionally, the technical novelty behind the framework seems to be somewhat limited. Nonetheless, I would say that the empirical novelty is still relatively significant.\n\n[After Author Response]\nI have read the response, and the authors have addressed most of my concerns. I have increased my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper744/Reviewer_rPgV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper744/Reviewer_rPgV"
        ]
    },
    {
        "id": "azaxuwsG_L",
        "original": null,
        "number": 2,
        "cdate": 1666172420852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666172420852,
        "tmdate": 1666174332794,
        "tddate": null,
        "forum": "Bo7eeXm6An8",
        "replyto": "Bo7eeXm6An8",
        "invitation": "ICLR.cc/2023/Conference/Paper744/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new benchmark MBXP for 10+ programming languages to facilitate the evaluation of execution-based code completion task. By the design of the conversion framework to translate prompts and test cases from the MBPP dataset, MBXP can evaluate code generation models in a multi-lingual manner. Furthermore, by the code generation model and some defined strategies, they can also provide the canonical samples in different programming languages and these data can also used for different code-related tasks such as source code summarization, code translation etc.",
            "strength_and_weaknesses": "Strength:\n\n- The provided benchmark is valuable for AI4Code community. Currently, MBPP is a dataset designed for python, which is limited for evaluation, This paper proposes a new benchmark based on MBPP to enrich the evaluated programming languages in Python, Java, Go, etc. (13 in total) and this dataset is very valuable for the community.\n\n- The extensive evaluation on this benchmark is very appreciated. Besides the code completion task including  multi-lingual version and  mono-lingual version, this paper also evaluates on zero-shot translation, prompt robustness, code insertion and the summarization task to show the effectiveness of the constructed benchmark.\n\n- There are also some findings by the extensive evaluation and these findings can provide the readers with some insights about these large-scale pre-trained models.\n\nAlthough I appreciated the authors for the extensive evaluation, I also find some problems and hope to fix these issues in the next version.\n\n- The synthetic canonical solutions are generated by the models, so I do not know how diversity they have compared with the real semantic-equivalent data?\n\n- In the abstract section, the authors mention that they will release part of the benchmark, but the main contribution for this paper is this benchmark, why not release the entire benchmark for the following-up researchers? \n\n- I suggest authors provide more details about the quality check via reviewers part. How many reviewers do you have? How to evaluate the converted version? How to ensure the converted problem is correct? How long do you take to evaluate these converted problems?\n\n- In Figure 3, I cannot know which line belongs to which model.  It should add some clarifications.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity The paper is easy to follow\n- Novelty: The technical novelty is limited, however this paper conducts extensive experiments to provide some empirical findings. Furthermore, the constructed benchmark is valuable.\n- Quality: The experiments are extensive and some findings are interesting.\n- Reproducibility: The experiments should be reproducible if all data and models are released.\n\n",
            "summary_of_the_review": "This paper propose a framework to construct a new benchmark across different programming languages for different code-related tasks. Based on this bechmark, an entensive experiment is conducted. Although the technique novelty is limited, I appreciate the efforts in this paper for some interesting findings. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper744/Reviewer_DMjf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper744/Reviewer_DMjf"
        ]
    },
    {
        "id": "G8lHVsDIFI_",
        "original": null,
        "number": 3,
        "cdate": 1666450539488,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666450539488,
        "tmdate": 1666450539488,
        "tddate": null,
        "forum": "Bo7eeXm6An8",
        "replyto": "Bo7eeXm6An8",
        "invitation": "ICLR.cc/2023/Conference/Paper744/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper describes a new framework to convert prompt and test cases, for code generation models, to a new target programming language. The framework is used to generate an execution-based code completion benchmark for more than 10 programming languages.\nThis enables an easier evaluation of a \"multilingual\" model versus the standard monolingual model.\n\nThis work shows that multilingual models tend to be better than the monolingual model and show that code generation models have zero-shot capabilities.\n  ",
            "strength_and_weaknesses": "Strengths:\n- the paper is very well written and illustrated. For someone like me who is not an expert of this area, I find this paper relatively easy to follow.\n- the framework proposed is well-motivated and very simple\n- this work can easily be applied to extend many benchmarks to many languages.\n\n\nWeaknesses:\n- The appendix is huge. It contain tons of information that would be more useful in the main paper. The limits for instance should definitely be included before the conclusion.\n- most of  the details needed for reproducibility and fully understand the experiments are in the appendix. I couldn't check it thouroughly.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The method is clear and the paper well-written.\n\nIt looks novel (although I might have missed previous work since I'm not an expert of this area).\n\nBased on the main paper, I don't think this is easily reproducible (of course I didn't read the 77 pages to check it...).",
            "summary_of_the_review": "The paper is very interesting and the method will be useful. My only concern is that the size of the PDF file submitted is huge and that I couldn't check all of it thouroughly. Some sections of the appendix must be move to the main paper before acceptance.\nI would definitely recommend the authors to submit the entire paper to a journal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper744/Reviewer_3QeX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper744/Reviewer_3QeX"
        ]
    },
    {
        "id": "Q5V93UcQGM",
        "original": null,
        "number": 4,
        "cdate": 1667386933560,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667386933560,
        "tmdate": 1667386933560,
        "tddate": null,
        "forum": "Bo7eeXm6An8",
        "replyto": "Bo7eeXm6An8",
        "invitation": "ICLR.cc/2023/Conference/Paper744/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a way to make an existing dataset (MBPP), an execution-based code completion benchmark, multilingual: while the original MBPP only contained python code, the new dataset (MBXP) contains a total of 10 programming languages. Problems in function completion datasets consists of *prompt*, *test statement*, and *canonical solution*. The proposed dataset creation is based on the insight that, for an execution-based benchmark, only translation of the prompt and test statement are necessary.\n\nThe paper further evaluates multiple monolingual and multilingual models on MBXP. This evaluation results in multiple interesting findings, for instance, models have a non-zero performance on languages they have not been trained on, what the authors attribute to code in multiple languages appearing in the same files. The authors further explore few-shot approaches and translation. Multilingual models mostly outperform monolingual ones.",
            "strength_and_weaknesses": "Strengths:\n- The paper is very clear and contains a coherent set of experiments.\n- The paper presents a new dataset, which will be useful for future work.\n\nWeaknesses:\n- The proposed dataset creation and the experimental setup aren't very creative -- but the results are interesting, so this isn't a big weakness.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is very clear and rigorous. It's also original, even though it might not be very creative.",
            "summary_of_the_review": "As the paper provides interesting insights and contributes a useful dataset, I believe it should be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper744/Reviewer_n1wH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper744/Reviewer_n1wH"
        ]
    }
]