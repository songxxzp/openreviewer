[
    {
        "id": "aa15wdnXrl5",
        "original": null,
        "number": 1,
        "cdate": 1666352747676,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666352747676,
        "tmdate": 1669209946881,
        "tddate": null,
        "forum": "e1u9PVnwNr",
        "replyto": "e1u9PVnwNr",
        "invitation": "ICLR.cc/2023/Conference/Paper17/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides one benchmark named BiBench which is desifned to test the efficiency, robustness and generalization of binary algorithms. It mainly contains two parts, one for accuracy and another is for efficiency, and each part has several components. The authors build quantitative indicators to measure the binary algorithms, which is very significative. The BiBench is the first benchmark in the area of binary models.",
            "strength_and_weaknesses": "Strengths:    \n1. This paper provides the first benchmark in the area of binary algorithms. It consides the accuracy and efficiency of binary models. Specially, the authors explore the track of hardware inference, which is very meaningful.    \n2. With very much exiperiments, the paper analyzes the pros and cons of existing methods.    \n\nWeakness:   \n1. As a benchmark, more principles about binary algorithms are expected, such as its formula mode in mathmatics and softwares, its compatibility with current hardware.   \n2. Some abbreviation of letter are not explained. In the XNOR-Net of Appendix, sub-tensors in 'I', then what if the meaning of 'I'?    \n3. Because there are many experiments, many details are unclear in the paper. The open source code of this benchmark is very helpful to this area.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper is very good, its easy to understand. As a benchmrah, its novelty is less important, its main contribution is that it can help newcoming methods to be more value. Its reproducibility needs the open code.",
            "summary_of_the_review": "The BiBench explores the advantages and disadvantages of different binary algorithms, and it shed a light to furture methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper17/Reviewer_kQWB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper17/Reviewer_kQWB"
        ]
    },
    {
        "id": "cx2caK2ajuZ",
        "original": null,
        "number": 2,
        "cdate": 1666623518507,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623518507,
        "tmdate": 1666623518507,
        "tddate": null,
        "forum": "e1u9PVnwNr",
        "replyto": "e1u9PVnwNr",
        "invitation": "ICLR.cc/2023/Conference/Paper17/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a benchmark for binary neural networks. The perspectives of the proposed benchmark include BNN methods, architectures, multiple tasks, and inference tests on various hardware. The authors summarize some insights and offer practical guidance.",
            "strength_and_weaknesses": "Although I very much agree with the motivation of this work, I also see very interesting settings and conclusions. However, I still have the following concerns:\n\n- Why I think the evaluation of BNN on many downstream vision tasks does not make much sense now: The classification task model is usually used as the Backbone of other downstream tasks. For example, the imagenet pre-trained full precision ResNet backbone is widely used in a large number of vision tasks. Therefore, for a binary model with a serious accuracy degradation problem, it is of practical significance to first achieve sufficient accuracy on ImageNet, and then verify it in more downstream tasks. Therefore, the robustness of BNN does not seem to be an immediate concern until a feasible level of accuracy is reached.\n\n-  There is certain doubt about the significance of the assessment. For example, the authors concluded that \"There is no obvious difference in the theoretical complexity among binarization algorithms. \". I think this is mainly because similar architectural bases have been selected in the comparison. Many earlier methods such as XNOR, DoReFa, BiReal, xnor++ etc., are based on ResNet18 Backbone. More recent works selected in this paper as ReCU and FDA are based on the ReActNet network structure. So it is not surprising that there is no difference in theoretical complexity and inference time. Furthermore, the acc improvements of ReCU and FDA are about within 1% compared to ReActNet. Therefore, using the same Backbone with only marginal acc improvement brings little meaning to their benchmarking.\nMore BNN works should be included, especially those having different architectural designs, e.g., [1,2,3,4], etc.\n\n- At present, the optimization of BNN has the problem of objective mismatch, and we usually need to use latent weight for training. Therefore, from the shape of the model parameters (only two states +1 and -1), the parameter update method (sign flipping), and the information flow path mode are significantly different from the fp model, why the author only considers the use of the full-precision model for the Architecture benchmarking? Why do the authors think that full-precision architectures are naturally suitable for BNN? In this work [3], the author verified that many full-precision model designs, such as bottleneck convolution, ConvBlock without shortcut connections (e.g., VGG, inceptionNet) are actually completely unsuitable for BNN.\n\n- The results of the robustness analysis are nice. However, they are not new as well. [5,6] show that discrete BNNs exhibit superior stability and robustness against adversarial attacks.\n\n- An important work [7] is missing in the training consumption track. [7] considers sign flipping without using the latent weight in the backward pass, which could significantly reduce the training epochs. The most compared optimizers are from the same type relying on the fp latent weights. Most of those proposed gradient approximation methods only got marginal contributions in their improvements. The most acc gains were from the architecture design.\n\n- The inference speed test was originally the part I was most expected in this paper. However, as the authors said, extremely limited inference libraries make the contribution of this part minimal, and basically no new insights. Since most of the inference libs used by the authors have ceased development and maintenance, most of them do not have any performance optimizations for recent BNN models, so the evaluation results provided may not be the best performance. For example, ReActNet-A is based on the MobileNet-V1 Backbone. Neither larq nor DaBNN provide an implementation based on this Backbone and thus no corresponding instruction-set optimization and assembly optimization for the model on Arm. Without specific optimization, the implementation of a new model architecture usually does not have a significant efficiency gain.\n\n\n[1] Zhang, Yichi, Zhiru Zhang, and Lukasz Lew. \"PokeBNN: A Binary Pursuit of Lightweight Accuracy.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[2] Bethge, Joseph, et al. \"Meliusnet: Can binary neural networks achieve mobilenet-level accuracy?.\" arXiv preprint arXiv:2001.05936 (2020).\n\n[3] Bethge, J., Yang, H., Bornstein, M., & Meinel, C. (2019). Back to simplicity: How to train accurate bnns from scratch?. arXiv preprint arXiv:1906.08637.\n\n[4] Martinez, B., Yang, J., Bulat, A., & Tzimiropoulos, G. (2020). Training binary neural networks with real-to-binary convolutions. arXiv preprint arXiv:2003.11535.\n\n[5] A. Galloway et al. Attacking binarized neural networks. arXiv:1711.00449, 2017. \n\n[6] E. Khalil et al. Combinatorial attacks on binarized neural networks. arXiv:1810.03538, 2018.\n\n[7] Helwegen, Koen, et al. \"Latent weights do not exist: Rethinking binarized neural network optimization.\" Advances in neural information processing systems 32 (2019).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow. Novelty is very limited. Reproducibility seems to be not a problem.",
            "summary_of_the_review": "Rather than saying that this is a benchmark paper, it is actually more like evaluating some selected BNNs from different perspectives. The author's original intention is constructive. However, the selected work has certain limitations, with a large number of ancient BNNs, some newer architecture designs, and optimization methods that are missing. In addition, it is not very reasonable to directly use the architectures of the fp-network for architecture benchmarking. The inference speed tests are greatly limited by the libs used, and their results provide very limited guidance. It is difficult for me to get a clear takeaway from a lot of experiments and data.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper17/Reviewer_Byew"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper17/Reviewer_Byew"
        ]
    },
    {
        "id": "h2EgHANBHk",
        "original": null,
        "number": 3,
        "cdate": 1667474611193,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667474611193,
        "tmdate": 1669197422949,
        "tddate": null,
        "forum": "e1u9PVnwNr",
        "replyto": "e1u9PVnwNr",
        "invitation": "ICLR.cc/2023/Conference/Paper17/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a benchmark to evaluate Network binarization. Network binarization is a compression method for neural networks that transforms the layers into binary vectors. Given the lack of a comprehensive evaluation and benchmarking methodology, the paper proposes sets of tasks, measures, hardware, and methods to validate new algorithms. Finally, the paper shows how network binarization is not a method that can seemingly apply to any network with no tuning.",
            "strength_and_weaknesses": "Strengths:\n\n(S1) The paper sheds a light on important issues related to network binarization, such as robustness and empirical time improvement.\n\n(S2) The benchmark is extensive and covers a number of architectures, learning tasks, hardware, and robustness evaluation.\n\n(S3) The paper evaluates 8 different network binarization methods, from older to more recent ones.\n\n(S4) The proposed benchmark BiBench is useful for the ML community.\n\nWeaknesses:\n\n(W1) The paper is poorly written: there is a large number of typos, broken sentences and bad use of English. The paper would benefit from thorough proofreading and rewriting. Some sentences make no sense or are hard to understand. For reference, here is an incomplete list of sentences and typos:\n\n- severe accuracy challenges\u00a0**but**\u00a0diminishing\n- 1 corruption benchmarks\n- pushing network binarization research to be accurate and efficient\n- Binarization\u00a0~~technology~~\u00a0compresses\n- to fully exert the generic of binarization technology.\n- We train the 1\u00d7 number of training epochs\n- which requires specifically studied in binarization research.\n\n(W2) Clarity: The paper should introduce the concepts more smoothly and walk the reader through them. Many concepts are assumed to be known and dropped without reference. The paper is therefore hard to read. Examples of unclear/unexplained concepts or claims are:\n\n- \u201cThe most aggressive quantization technology\u201d according to whom?\n- \u201cimaging modality task\u201d: what is that? how is it defined?\n- the 1-bit specialization of quantization\u201d - missing reference\n- \u201crequiring specified local structures\u201d such as?\n- \u201cmodel is exported in the ONNX\u201d \u2192 Is this a common format? Where does it come from?\n- What do the colours in Table 2-3 represent?\n\n(W3) The paper would benefit from more rigour. E.g.,\n\n- Popcount is never defined\n- Examples of $\\alpha$ and $w$ should be provided\n- \u201cThe quadratic mean form is uniformly applied in BiBench to unify all metrics.\u201d Why is this a good choice?\n\n(W3) The intuition behind the evaluation metrics in Eq. 2 - 9 is unclear. Why not consider standard deviation as well?\n\n(W4) The related work analysis should be more extensive and explain what are the choices determining the current selection of algorithms. A concurrent work [1] presents other models for binarization in its related work section. How are the algorithms in Table 1 chosen? Why are they representative?\n\n[1] Shang, Y., Xu, D., Zong, Z. and Yan, Y., 2022. Network Binarization via Contrastive Learning. ECCV\n\n(W5) As a benchmark, it should probably compare with other, simpler strategies to make the network more compact. For instance, one such strategy could be dropout or model quantization. There is no need to be exhaustive there, but there should be the possibility for researchers working on Network binarization to assess their methods on more traditional techniques.\n\n(W5) It is not clear to what extent the chosen datasets and tasks are challenging or representative. The paper should elaborate more on why some of the tasks have been chosen. The description in Section 3.1 assumes the reader knows the tasks but does not provide additional information on why they are representative.\n\n(W6) Code: The implementation is not available. A benchmark should provide the code (in this case anonymous) for reproducibility. Moreover, the code has to be clear, well-documented, and easy to run.",
            "clarity,_quality,_novelty_and_reproducibility": "Although the paper proposes a novel benchmark, the clarity of the explanation is not sufficient (see W2). Moreover, a number of typos and bad sentences render the reading even more strenuous. In terms of reproducibility, I could not find the code. For a benchmark, the presence of well-documented and easy-to-use code is paramount.",
            "summary_of_the_review": "Although the paper is interesting and the problem relevant, the presentation is sloppy and most of the decisions and claims not fully motivated. For this reason, I believe the paper is not ready for publication at ICLR. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper17/Reviewer_LQy6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper17/Reviewer_LQy6"
        ]
    }
]