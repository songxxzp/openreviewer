[
    {
        "id": "BT9mIyijR_",
        "original": null,
        "number": 1,
        "cdate": 1666586165375,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586165375,
        "tmdate": 1666768730746,
        "tddate": null,
        "forum": "ndYXTEL6cZz",
        "replyto": "ndYXTEL6cZz",
        "invitation": "ICLR.cc/2023/Conference/Paper2991/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presents a post-hoc activation shaping strategy for improved out-of-distribution (OOD) detection. The technique mainly consists of pruning the activation vector for a given sample in a chosen particular layer (favorably the penultimate layer). The paper proposes three algorithmic variants for the purpose of pruning and shaping the activations. The work is supported by empirical results on standard benchmark datasets (CIFAR-10/100 and ImageNet-1k) and detailed ablations. \n",
            "strength_and_weaknesses": "Strengths:\n\n1. Well written and easy to follow.\n2. Empirical analysis supports the effectiveness of the proposed algorithms.\n3. Simple post-hoc idea based on activation shaping.\n\nWeakness:\n\n1. Although the empirical results are quite significant, the paper lacks a proper justification for why simply shaping the activations provides an improvement in OOD detection. Previous works [1] and [2] have already shown that pruning the activations provide an improvement in OOD detection. To make the paper stronger, the authors should provide a clear justification on how scaling the activations is further helping.\n2. In Algorithms 2 & 3 (ASH-B, ASH-S), after pruning non-negative values are scaled to some constant value. However, proper reasoning on how this constant is chosen is missing. Can the constant be chosen as some arbitrary value and still get similar OOD performance?\n\n3. It is not clear how the value of the hyper-parameter p is chosen. I shall suggest the authors to provide more detailed explanation on this. \n\n4. It would be also interesting to see the performance on Hard OOD detection tasks such as CIFAR-100 vs CIFAR-10. Currently, the paper mainly shows results on OOD datasets that are significantly different from ID data and easier to detect.\n\nAlthough not a major concern, the related works section can be improved by adding some recently proposed post-hoc works such as [3].\n\n[1] Sun, Y., Guo, C. and Li, Y., 2021. React: Out-of-distribution detection with rectified activations. In NeurIPS 21.\n\n[2] Sun, Y. and Li, Y., 2022. Dice: Leveraging sparsification for out-of-distribution detection. In ECCV 22.\n\n[3] Sun, Y., Ming, Y., Zhu, X. and Li, Y.. Out-of-distribution Detection with Deep Nearest Neighbors. In ICML 22.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and presentation of the paper are good. I have doubts regarding the novelty of the work. The algorithm proposed is closely related to a few previous works [1,2]. A proper justification for \"Why it is performing better\" is also missing. Adding reasoning would make the paper stronger. \n\n[1] Sun, Y., Guo, C. and Li, Y., 2021. React: Out-of-distribution detection with rectified activations. In NeurIPS 21.\n\n[2] Sun, Y. and Li, Y., 2022. Dice: Leveraging sparsification for out-of-distribution detection. In ECCV 22.\n",
            "summary_of_the_review": "This work proposes a simple-to-use algorithm to prune activations in a post-hoc fashion to improve OOD detection. The method is empirically supported through experimental results on standard benchmarks but lacks reasoning and motivation behind the working principle of the algorithm. Hence, I recommend borderline reject. However, I am willing to increase my score based on the rebuttal.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2991/Reviewer_UzHM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2991/Reviewer_UzHM"
        ]
    },
    {
        "id": "KDWq23wk_0",
        "original": null,
        "number": 2,
        "cdate": 1666623446491,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623446491,
        "tmdate": 1666623446491,
        "tddate": null,
        "forum": "ndYXTEL6cZz",
        "replyto": "ndYXTEL6cZz",
        "invitation": "ICLR.cc/2023/Conference/Paper2991/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": " The authors hypothesize that over-parameterized DNNs extract too many features and can prune a large proportion of the features without drastically reducing ID accuracy, while significantly improving the OOD detection task. They propose a simple activation method to validate this hypothesis.  The proposed method prunes a large portion of a sample\u2019s activation and rescaling the rest activation. The empirical evaluation on ImageNet shows that the improvement of the proposed methods is significant.",
            "strength_and_weaknesses": "Strength:\n\n- The proposed method is simple and effective.\n- The paper is well written.\n- This work presents extensive empirical evaluations.\n\nWeakness:\n\n1. The choice of hyperparameter $p$ is mentioned in both the main text and the appendix. The authors did not discuss how the ID influences the choice of $p$. If only ID data is available, I think the hyperparameter selection scheme is important. \n2. If the OOD detection task is CIFAR10 vs CIFAR100, does the ID-OOD trade-off still hold?  \n3. Table 5 shows that ASH improves on existing methods. Does ASH improve KNN? Does the choice of $p$ have a significant effect on the performance of KNN+ASH?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good. The content is well-organized and is easy to follow. Quality and Reproducibility are good. This work provides extensive experimental evaluations and sufficient details. Novelty is fair. ",
            "summary_of_the_review": "I recommend acceptance. The experimental evidence is strong.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2991/Reviewer_Pwvn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2991/Reviewer_Pwvn"
        ]
    },
    {
        "id": "DWQYYhFVUZ9",
        "original": null,
        "number": 3,
        "cdate": 1666644601832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644601832,
        "tmdate": 1668985529387,
        "tddate": null,
        "forum": "ndYXTEL6cZz",
        "replyto": "ndYXTEL6cZz",
        "invitation": "ICLR.cc/2023/Conference/Paper2991/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper is about weight pruning in a single layer and its relationship with out of distribution detection performance. The authors propose to prune activations at a single layer using the top p percentile, and candidate activations can either be set to zero (pruned), scaled, or binarized. Then the well known energy score is used to produce a score for out of distribution detection. This produces state of the art performance in out of distribution detection in all tested benchmarks.\n\nThe contributions of this paper are:\n- A post-hoc and simple method for OOD detection based on activation shaping/sparsification.\n- New state of the art results on three ID datasets and ten OOD datasets, including ImageNet, CIFAR10, and CIFAR100 as ID datasets over two different network architectures.\n- A good evaluation and ablation results that justify some of the chosen hyperparameters and activation shaping\n- A good study of the effect of activation shaping/sparsification and its relation to OOD performance. I think previous work only focuses on weight pruning/sparsification.\n",
            "strength_and_weaknesses": "Strengths\n- The paper is very well written, it is easy to understand, I have no more comments about writing.\n- The proposed technique is very simple to understand and implement, prune activations with a threshold given by the p percentile, decide how to remove these activations, and then use the energy score to output a score for out of distribution detection.\n- Post-hoc methods are preferable, as there is no need to retrain the model or use special training methods, and this method can be applied to any pre-trained model.\n- The evaluation is solid, using multiple models, multiple ID (3) and multiple OOD datasets (10), multiple metrics (AUPR, AUROC, FP95), and a good selection of baselines, including many post-hoc methods. I do not have doubts about the validity of these results and conclusions. I think the only possible missing baselines are uncertainty-based method (Ensembles, Dropout, DUQ, etc), but I am not sure if these make sense given the post-hoc use of the proposed method.\n- I really like Figure 2, it shows the trade-off between in-distribution accuracy and out of distribution performance, with state of the art methods and the proposed method at different activation pruning thresholds, clearly showing the advantage of the proposed method, but also showing how these methods compare by themselves. It is known that OOD detection methods might reduce performance but it is the first time I see such comparison. It is clear from this result that the proposed method has the best ID/OOD performance trade-off (best in both accuracy and AUROC).\n- There is an improvement in out of distribution performance as measured by the AUROC and AUPR on ImageNet, in all benchmarks using ResNet, and in most benchmarks using MobileNets. For CIFAR10/100 using DenseNet, there is also a large improvement in all comparisons.\n- There is a good set of ablation results to show the effect of some hyper-parameters, in particular the p percentile used as a threshold to shape/sparsify the activations, and also on which layer this method should be applied, which justifies the final decision made by the authors.\n\nWeaknesses\nAfter rebuttal, I  see no weaknesses.\n\n~~- I think there is no theoretical justification provided in the paper on why this method works. By this I mean the paper does not do a theoretical analysis (which is fine), but the paper does not provide an intuition on why the method works. There is a good improvement in most out of distribution detection benchmarks, but then there is the question, why? Why does pruning or shaping activations over the p percentile improves OOD performance? Maybe the authors can give some intuition that guided the construction of the proposed method.~~\n\n~~- I see that in Table 2, when using MobileNet, DICE outperforms your method in most benchmarks, I think this deserves some analysis or acknowledgment, it seems that the proposed method has some degree of dependency with the network architecture. Could the authors comment on this?~~\n\nMinor Issues\n- I think there is a double blind leak in page 18, in a footnote, I do not include this link here but this should not happen, I think the link is not anonymized and leaks some information about the authors.\n\n~~- In Figure 2, please specify which OOD datasets were used to make this plot, the paper mentions four OOD datasets, but which ones?~~\n\n~~- Table 5 shows combinations of the proposed method with other OOD detection methods (very nice), but why is there an improvement between energy score and energy score + ASH? The proposed method ASH already uses the energy score, this combination seems very strange to me. Please clarify.~~",
            "clarity,_quality,_novelty_and_reproducibility": "About clarity and quality, this paper is clear to me, it is very well written and presented, and overall the presentation is very high quality.\n\nAbout novelty, the novelty of this paper is the proposed ASH method for out of distribution detection in post-hoc way, producing state of the art performance in OOD detection with ImageNet and four OOD datasets (which are difficult), and also on CIFAR10/100 and six other OOD datasets (also difficult).\n\nThe proposed method is simple to understand and implement, and has very good performance. The only issue I see with the method is basically why it improves OOD detection performance, I believe the paper does not touch that subject, or introduce some intuition about this.\n\nThere is some similar previous work that the authors have references, like DICE which does weight sparsification, while the proposed method in this paper does activation sparsification, and it actually outperforms DICE. So I find that there is both a high degree of originality and novelty.",
            "summary_of_the_review": "This is overall a good and high quality paper, the proposed method is a good improvement for out of distribution detection without requiring large modifications to the model or as training procedure. The proposed method is well studied, with many comparisons to the state of the art in a good selection of in-distribution and out-of-distribution datasets (including ImageNet and CIFAR10/100), and an excellent selection of ablations to justify the decisions made by the authors.\n\nThe only issues I find in the paper are minor, I mentioned them in weaknesses, mostly about intuition or analysis of why activation sparsification improves OOD performance, and some effects of network architecture on ASH performance.\n\nAfter rebuttal, I can confirm that this paper should be accepted as the paper has improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2991/Reviewer_CKHN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2991/Reviewer_CKHN"
        ]
    },
    {
        "id": "byD4Bv0uMAV",
        "original": null,
        "number": 4,
        "cdate": 1666676073434,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676073434,
        "tmdate": 1666676779091,
        "tddate": null,
        "forum": "ndYXTEL6cZz",
        "replyto": "ndYXTEL6cZz",
        "invitation": "ICLR.cc/2023/Conference/Paper2991/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on detecting OOD samples in the inference time by pruning a large portion of an input sample's activation and lightly adjusting the remaining. The method can be easily combined with previous OOD scores (like MSP, ODIN, Energy and ReAct). When combined with the energy score, it shows a significant improvement in OOD detection, on both moderate and large-scale image classification benchmarks.",
            "strength_and_weaknesses": "Pros:\n\n1. The paper is well-written.\n\n2. The method is simple and easy to reproduce.\n\n3. The experiments are sufficient and performance are quite well on both imagenet and cifar benchmark.\n\n\nCons:\n\n1. In algorithm ASH-S, I think the scaling factor exp(s1/s2) is a hyperparameter. Are there other functions that can replace the exponential function? In other words, when the application scene changes, do the authors need to replace the exponential function to adapt to the current scene to obtain the optimum?\n\n2. The paper designs three algorithm and mostly ASH-S performs best (like in Table 1 and Table 2). Sometimes, ASH-B performs better than others, like Table 1 (MobileNet). So I wonder how to choose the algorithm when meeting a new dataset and a new model?\n\n3. As shown in Table 1, when the model is ResNet, ASH-S can achieve 95.12% AUROC and 22.18% FPR95. But when the model is MobileNet, the method can not outperform the SOTA. I think the generalization of these algorithms is not very good, and when the model changes, the performance difference is too large.\n\n4. The placement in Figure 5 and the results in Table 6 (Appendix A) show that the method can not work when it is deployed in 1-3th Block of ResNet. The result in 4 Block is not good enough and the highest performance is in the penultimate Layer. I think the results show that the method only works in the penultimate Layer, and the generalization of insertions in other positions in backbone is extremely poor.\n\n5. The placement of the method is in the penultimate Layer, which is the same with DICE. The method discards some features at the penultimate layer, which is actually partially equivalent to dropping out some connection weights at the penultimate layer (DICE's approach). So when this method is deployed to the penultimate layer, I think it has a high similarity to DICE.\n\n6. The theoretical proof of why the algorithm works is insufficient, as the operations of Algorithms 2 and 3 seem to be the result of empirical design rather than rigorous theoretical derivation. The authors may consider improving the interpretability and theoretical basis of this article.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has a clear presentation, and the reproducibility is good.\n\nThe quality and novelty are not well supported by the experiment. The proposed method looks heuristic, meaning that motivation and evaluation should be stengthened.",
            "summary_of_the_review": "The paper focuses on detecting OOD samples in the inference time by pruning a large portion of an input sample's activation and lightly adjusting the remaining. The method can be easily combined with previous OOD scores (like MSP, ODIN, Energy and ReAct). When combined with the energy score, it shows a significant improvement in OOD detection, on both moderate and large-scale image classification benchmarks. However, the motivation of the proposed method is not clear and not convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2991/Reviewer_kqRP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2991/Reviewer_kqRP"
        ]
    }
]