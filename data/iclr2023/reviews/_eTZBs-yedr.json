[
    {
        "id": "a_4Ly7-zTO9",
        "original": null,
        "number": 1,
        "cdate": 1666630323103,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630323103,
        "tmdate": 1666630323103,
        "tddate": null,
        "forum": "_eTZBs-yedr",
        "replyto": "_eTZBs-yedr",
        "invitation": "ICLR.cc/2023/Conference/Paper4838/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new compression-aware minimizer dubbed CrAM that modifies the optimization step in a principled way, which results in dense models trained via CrAM can be compressible post-training, in a single step, without significant accuracy loss. Experimental analysis on image classification and language modelling shows that CrAM models can be pruned one-shot at a wide range of sparsity levels, while resulting in sparse models that are competitive with existing gradual pruning methods. Also, CrAM produces sparse models which perform well for transfer learning and works for semi-structured pruning patterns.",
            "strength_and_weaknesses": "Strength.\n1) CrAM is simple to implement as part of a regular training loop and has a single scaling hyper-parameter.\n2) CrAM enables training a dense model, which can later be compressed to different target levels, with minimal or no recalibration.\n3) To derive the CrAM update, the paper provides the theoretical analysis to justify the choices made in designing the training method.\n4) Sufficient Experiments demonstrate the wide range of application, including one-shot pruning, quantization, semi-structured N:M sparsity patterns and transfer learning.\n\nWeaknesses.\n1) For most of the comparisons, the paper choose the SAM as the target, such as Table 2,3,4 etc, which is not suitable because it trains models that potentially converge to flatter minima for better generalization rather than compressible models. It is more appropriate to compare with compression-aware C-SAM.\n2) Compared with the gradual method DPF, CrAM performs worse at high sparsity, such as 90% and 95%. The paper explains that CrAM was not explicitly trained for such high sparsity. However, in the table 7, compared with C-SAM, CrAM still not perform very well at 80% and 90% sparsity with the same experimental configurations. It is better to clarify more about the phenomenon.\n3) In the paper, the computation cost is not specific, such as twice as many forward-backward passes or the cost of BNT is minimal. It is better to show the cost explicitly in the comparison tables, such as the training time for single iteration.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The idea of CrAM is somewhat novel and supported by theoretical analysis, but the experimental results are not significant compared with C-SAM. And the paper provides the details of algorithm and hyperparameters such that readers should be able to reproduce the main results.",
            "summary_of_the_review": "The idea of CrAM is overall novel and supported by theoretical analysis. And it is the first carry over the sharpness-aware idea to the task of obtaining compressible models. Although the experimental results are not significant compared with the closed works, the paper is likely to have modest impact within the subfield. Hence, I recommend for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4838/Reviewer_fNyD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4838/Reviewer_fNyD"
        ]
    },
    {
        "id": "Q_sEPXISI_",
        "original": null,
        "number": 2,
        "cdate": 1666748510879,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666748510879,
        "tmdate": 1666748510879,
        "tddate": null,
        "forum": "_eTZBs-yedr",
        "replyto": "_eTZBs-yedr",
        "invitation": "ICLR.cc/2023/Conference/Paper4838/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to have a compression-aware minimizer that can compress dense DNN into a sparse one, and most importantly, without much performance drop. Inspired by the sharpness-aware minimizer, the authors leverage the flat minima to find the weights' coordinates that are stable to the perturbation. With the price of doubled computational overhead, the proposed compression-aware minimizer can prune dense models in a one-shot to 70-80% sparsity with a small drop in accuracy. The authors also give theoretical proof to guarantee the convergence of the proposed CrAM. The experiments on the ImageNet dataset verify the effectiveness of the proposed CrAM. The paper is well-organized and easy to follow. \n",
            "strength_and_weaknesses": "Pros:\n1. The paper is well-written and well-organized. It is easy to follow and read. \n2. It is inspiring to leverage the flat minima for the model pruning task. This paper encourages the application of sharpness-aware training.\n3. The methodology and appendix E are good for readers. The derivation is sufficient; the theoretical proof makes the paper complete. \nCons:\n1. It would be better if the authors could have more experiments\u2014for example, some parameter studies; some model pruning baselines. (One should be enough for the rebuttal period)  \n2. Appendix E is important; it would be better not to be placed the last part of the appendix. \n3. There are too many variants of CrAM in the experiments. It is better to give a universal parameters set of CrAM. ",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper is good. Because the motivation, the addressed problem, the derivation of the solution, and the experiments are clear and easy to follow. The novelty of this paper is okay, as it proves the convergence of updating for their proposed CrAM. ",
            "summary_of_the_review": "\nTo conclude, this paper is a good research paper. It is well-written, and the proposed method is sufficient in derivation and effectiveness. It would be better if some more experiments could be added. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4838/Reviewer_FqjP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4838/Reviewer_FqjP"
        ]
    },
    {
        "id": "lWZwJ7goz9A",
        "original": null,
        "number": 3,
        "cdate": 1666962866031,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666962866031,
        "tmdate": 1669941035468,
        "tddate": null,
        "forum": "_eTZBs-yedr",
        "replyto": "_eTZBs-yedr",
        "invitation": "ICLR.cc/2023/Conference/Paper4838/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method to train a neural network, CrAM (and CrAM+), which results in a trained network that can be compressed (by pruning or quantization) while maintaining accuracy. The method is developed based on plugging compression in the SAM\u2019s inner maximization step; the perturbed parameters are obtained by taking the ascent step (for maximization) and applying some compression on them. The authors attempt to provide theoretical justification for such an algorithm based on robust optimization literature. The authors present details on some extensions including batch norm tuning or SAM+multi, and further, experimental results on Imagenet as well as SQuAD for different comparisons.\n",
            "strength_and_weaknesses": "Strength\n* provide a range of experiments making comparisons on different scenarios, baselines, and domains\n* develop theory based on the mini max framework, by which compression interpreted as perturbation can be effectively performed as robust optimisation\n* N:M sparsity pattern is nice in practice adding it is definitely plus\n\nWeakness\n* CrAM, the basic version of the method, does not work well, whereas the provided theoretical justification is supposed to indicate CrAM should be enough, not CrAM+. The theoretical justification itself is only weakly justifying the method. For instance treating the mask operator and straight-through estimator the way the author does is very weakly justified.\n* overclaim for generality of the method. present as if it works for any compression methods, but mostly focuses on top-k based on magnitude of parameters. a quantization scheme is provided in the appendix but not enough to claim for compression in general.\n* far below the sparsity level compared to existing methods. present some effectiveness up to 90%, but considering the sparsity literature it is far below. for 95% sparsity it is a level at which the method does not work.\n* coated with various experiments, yet fails to concentrate on the central part. Although they conduct many interesting experiments, they are often quite partial point-by-point based and far from being comprehensive.\n* The only effective result is that CrAM/CrAM+ can give a network that is one-shot prunable. However, this is not considered enough given the fact that this method requires additional hyperparameter tuning such as the choice of sparsity levels during training, that there exist other one-shot training (and pruning afterwards) methods without requiring SAM framework or double the forward-backward computations.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper directly borrows the SAM framework and tests a few different algorithmic changes in the order of compression operations. In that sense it is hard to say the idea is original.\n* In my view writing should improve in terms of organization, focus, and clarity.\n",
            "summary_of_the_review": "The paper explores an interesting combination of SAM and sparsity, but importantly lots of parts in the paper is not quite tight and needs further exploration.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4838/Reviewer_VFwD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4838/Reviewer_VFwD"
        ]
    },
    {
        "id": "TDvIsTVivX",
        "original": null,
        "number": 4,
        "cdate": 1667188766417,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667188766417,
        "tmdate": 1667188766417,
        "tddate": null,
        "forum": "_eTZBs-yedr",
        "replyto": "_eTZBs-yedr",
        "invitation": "ICLR.cc/2023/Conference/Paper4838/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a specialized optimizer for optimizing the DNN to have good compression ability, i.e. the optimized model can be pruned/quantized to smaller models which have good accuracy.\n\nThe paper is based on a formulation related to the sharpness aware minimization, by adding a compression related constraint projection on the inner \"sharpness-aware\" gradient ascent step.\n\nExperiments result show that the proposed method can train a model which not only performs good on dense model but also on sparse models.",
            "strength_and_weaknesses": "Strength\n- The proposed method can train a single set of model weights which performs well on multiple compression rate, i.e. sparsity ratios.\n- The formulation is a good extension of sharpness-aware optimization to the direction of optimization with constraints.\n- The authors also considered and discussed the computation complexity of the algorithm.\n- The experiments result show that the dense model results can be even better compared to training a standalone dense model.\n\nWeaknesses\n- Although the paper derives the method from the view of optimizer, the updating rule is almost the same as using SAM optimizer on supernet/one-shot NAS methods such as once-for-all network[1], bignas[2]. The novelty of this method is not very significant from this point of view.\n- It's better to have some discussion and comparison to the method which directly combines SAM with once-for-all network, e.g., is there any difference in the actual updating rule and how much difference in the results.\n\n[1] Cai, Han, et al. \"Once-for-all: Train one network and specialize it for efficient deployment.\" arXiv preprint arXiv:1908.09791 (2019).\n\n[2] Yu, Jiahui, et al. \"Bignas: Scaling up neural architecture search with big single-stage models.\" European Conference on Computer Vision. Springer, Cham, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and easy to understand.\n\nThe derivation and formulation in this paper are in good quality.\n\nThe novelty of this paper is not significant, as it's very close to once-for-all and other supernet based methods in essence. (see \"Strength And Weaknesses\" for more details.)",
            "summary_of_the_review": "The paper provides an intact derivation of extending SAM optimizer to the context of constrained optimization and especially model compression.\n\nThe experiments validate the proposed algorithm on different sparsity ratios and also dense model.\n\nFrom the updating rule, the proposed algorithm is the almost the same as using SAM on once-for-all network, so the novelty of this paper is not very significant.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4838/Reviewer_6AwX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4838/Reviewer_6AwX"
        ]
    }
]