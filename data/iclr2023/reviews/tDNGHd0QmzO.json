[
    {
        "id": "DjQtpJFvRRQ",
        "original": null,
        "number": 1,
        "cdate": 1666165341339,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666165341339,
        "tmdate": 1670044196155,
        "tddate": null,
        "forum": "tDNGHd0QmzO",
        "replyto": "tDNGHd0QmzO",
        "invitation": "ICLR.cc/2023/Conference/Paper1285/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduced a new type of neural layer, which is inspired by the well-known Taylor expansion. In particular, for each layer, the learning model is represented by a Takyor summation within two orders. To address the high dimensionality of the Hassien tensor, the low-rank Tucker decomposition is taken into account. This paper also discussed suitable ways for the initialization of the new layers. In the experiments, this paper considered various tasks, including image recognition, identification of dynamical systems, and NLP.",
            "strength_and_weaknesses": "++Strength\n\nThe new model can achieve comparable performance to some deep neural networks on many tasks.\n\n\u2014 Weakness\n\nThe basic idea of the TaylorNet is not new. It is strongly close to the old-school polynomial network (not overviewed in the related works), which was widely studied 30 years ago. A good book on this point can be found in (Nikolaev and Hitoshi, 2006). More recently, similar ideas are discussed in the tensor community, such as (Qiu et al., 202 and Rabusseau et al., 2019). In these works, they discussed multi-linear functions as the learning model. The proposed initialization trick is also not novel. A similar idea has been discussed by Pan et al. (2022), which is also not cited in the paper.\n\n*Nikolaev, Nikolay, and Hitoshi Iba.\u00a0Adaptive learning of polynomial networks: genetic programming, backpropagation and Bayesian methods. Springer Science & Business Media, 2006.*\n\n*Qiu, Hejia, et al. \"On the Memory Mechanism of Tensor-Power Recurrent Models.\"\u00a0International Conference on Artificial Intelligence and Statistics. PMLR, 2021.*\n\n*Rabusseau, Guillaume, Tianyu Li, and Doina Precup. \"Connecting weighted automata and recurrent neural networks through spectral learning.\"\u00a0The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.*\n\n*Pan, Yu, et al. \"A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks.\"\u00a0International Conference on Machine Learning. PMLR, 2022.*",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality:**\n\nSome statements are not clearly given in the paper. One happens in the second to the last paragraph of sec 1, where the authors say, \u201cMeanwhile, our Taylor initialization reach an accuracy that is \u2026.higher than the Xavier and Kaiming initialization\u2026.\u201dThis is misleading! It makes readers recognize that the new initialization outperforms Xavier and Kaiming in deep learning. But it is not valid. We can see from sec 5.1 that the superior performance by the new initialization is ONLY evaluated for the proposed TaylorNet. It is a crucial precondition for the statement, but it is not clearly mentioned when summarising the contributions.\n\n**Novelty:**\n\nThe novelty of this work is weak from the technical perspective (for reasons see the \u201cstrengths and weakness\u201d). Still, it is good that the authors highlight the potential interpretability of polynomial networks.\n\n**Reproducibility**\n\nThe experimental settings were introduced clearly.",
            "summary_of_the_review": "I agree that the polynomial learning model (such as the proposed TaylorNet) would be a good candidate as a universal approximator with potentially better interpretability than the existing deep neural networks. This paper clearly pointed out these potentials and also mentioned the main challenges, such as the dimension explosion and the stability. However, all these challenges are still not resolved well so far in this paper (I cannot believe that only using a good initialization can guarantee the stability of the model in a deep regime).\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1285/Reviewer_D6Yc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1285/Reviewer_D6Yc"
        ]
    },
    {
        "id": "dmy1kMSzXXf",
        "original": null,
        "number": 2,
        "cdate": 1666257323286,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666257323286,
        "tmdate": 1666257323286,
        "tddate": null,
        "forum": "tDNGHd0QmzO",
        "replyto": "tDNGHd0QmzO",
        "invitation": "ICLR.cc/2023/Conference/Paper1285/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new architecture that relies on polynomial expansions. The idea is to use polynomial layers, where each layer expresses a polynomial expansion of order $k$ (where $k=2$ in practice). The new architecture uses a Tucker decomposition to reduce the learnable parameters. The authors also introduce a variant called reducible-TaylorNet, where the parameters are further reduced using a re-parametrization. The architecture is validated in small image datasets (CIFAR10/100), two dynamical systems and a small scale NLP task. ",
            "strength_and_weaknesses": " * [+] Improving the generalization in models that non-black box DNNs is important. \n\n * [+] Using the Tucker decomposition in a smart way in Mixer-like architectures is new to me, even if the Tucker decomposition and polynomial expansions have been used in the past in vision and deep learning approaches. \n\n * [-] Several technical parts are unclear, while the paper requires proofreading to make it more understandable.   \n\n * [-] The empirical evaluation is weak. The MLP-Mixer considered as a baseline is evaluated both on Imagenet, and VTA-1k which includes 19 datasets. Similarly, $\\Pi$-nets that are considered another baseline include more experiments in classification, generation, or mesh representation learning. \n\n * [-] The literature review on both Tucker decomposition and its use in deep learning and the polynomial nets could be significantly updated. (see below)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The Taylor polynomial in eq. (1) is a bit unusual for me. Typically in a Taylor polynomial, there are the higher-order derivatives of the function f at $x_0$, right? Could the authors clarify where this definition without the mentioned derivatives originates from? \n\nThere is a significant part that I am not certain after studying the manuscript and I would appreciate the authors\u2019 clarification. The paper is mentioned as \u201cTaylorNet\u201d and introduces \u201cinductive bias to DNNs with Taylor series expansion\u201d. Where are the derivatives computed?\n\nContinuing in the same line as above, why are the $\\mathbf{\\mathcal{W}}^{[k]}$ mentioned as derivatives? Aren\u2019t they optimized with SGD (or a variant)? \n\n\nIn sec. 2 the paper mentions that $\\Pi$-nets are a special form of TaylorNet at $x_0=0$. In sec. 5.2 the paper mentions that the expansion of $x_0=0$ is used for the proposed method. That would mean that the method named as Taylor-Mixer is a $\\Pi$-net in reality. Could the authors elaborate on that? \n\nContinuing in the logic mentioned above, the related works of polynomial nets have not been included in the comparisons, which makes it unclear how the proposed method fares with respect to the existing polynomial nets. \n\nThis is not the first time an initialization has been proposed for polynomial expansions. The ReLinear initialization has been introduced in \u201cOn Expressivity and Training of Quadratic Networks\u201d; could the authors include this in their comparisons in sec. 5.1?\n\nThis work is motivated by mentioning that physical priors are important in ML. However, I am not certain what the physical prior is in this case. Could the authors elaborate on this point? \n\nWhat is the \u201c!!\u201d symbol used in sec. 4.3? It is not defined in the main paper (or the supplementary table). \n\nOne suggestion beyond the parameters is to include the number of FLOPs. I expect those might be more than the MLPMixer, or even $\\Pi$-nets, however this provides a more complete idea to the reader. \n\nA number of works have also considered polynomial nets for tackling the task, for instance you can find several in the \u201cAugmenting Deep Classifiers with Polynomial Neural Networks\u201d paper. It would be recommended to discuss those related works in more detail. Additionally, this paper identifies that the Taylor polynomials introduce a different inductive bias, however there are already works who have been exploring the inductive bias from a theoretical perspective. \n\nWhat are the ranks used in each Tucker decomposition from the input to the output of the final network? It would provide further insights to the readers to explain and ablate this point. \n\nThe paper requires proofreading, since there are a number of issues currently: \n* The \\citep{} command needs to be used appropriately. For instance, in sec. 2 \u201cTensor decomposition Kolda and Bader (2009) aims [...]\u201d; change to citep. In the same paragraph several instances of this exist. \n* \u201cand 5) Our approach\u201d: why is the O capital?\n* The notation mentions that the matrices are denoted with uppercase boldface letters, however $\\mathbf{\\mathcal{X}}_{(n)}$ is with calligraphic letter, etc. \n* The notation could be simpler. For instance, in sec. 4.1 the $k$ is used as the order of the polynomial expansion, while in sec. 4.2 as an index. Keeping track of all the variables becomes harder, so it would be easier to use different variable names. \n\nI am not certain how eq. (4) and eq. (3) which are purely about tensors dictate how to move from eq. (8) to eq. (9), so some further explanation in the manuscript would make it easier for the reader. \n\nLastly, I suggest to the authors to include a Lemma of how to go from the two mode-m products of eq. (6) to the product of eq. (7). It is not trivial to realize the first one is not contracting the tensor, since it's a mode-m product with a matrix. \n",
            "summary_of_the_review": "The idea of using polynomial expansions is interesting, while the use of the Tucker decomposition to reduce the parameters can be a useful tool. However, the paper currently has several flaws as identified above, making it hard for the reader to appreciate the contribution. At the same time, the experiments are weak. Nevertheless, if the revised version addresses all of the concerns, I am willing to reconsider my rating. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1285/Reviewer_LGk5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1285/Reviewer_LGk5"
        ]
    },
    {
        "id": "8BUsysibHz",
        "original": null,
        "number": 3,
        "cdate": 1666660975509,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660975509,
        "tmdate": 1670697301384,
        "tddate": null,
        "forum": "tDNGHd0QmzO",
        "replyto": "tDNGHd0QmzO",
        "invitation": "ICLR.cc/2023/Conference/Paper1285/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a neural architecture called TaylorNet. It is designed to mimic a taylor-series approximation of a multivariate function. The network itself is expressed as a composite function of Taylor layers, where each layer is a 2nd order taylor expansion. A key characteristic of the model is that it is free of non-linear activation functions, and the paper also proposes a customized weight initialization method. Experiments are performed on three tasks: image classification (CIFAR-10 and CIFAR-100), dynamical systems (Duffing Equation, Non-Linear Fluid Flow), and sentiment analysis (IMDB).",
            "strength_and_weaknesses": "Strengths:\n1. The custom Taylor initialization is interesting, especially in the context of activation free networks.\n2. The concept of activation free networks is interesting, although its practical utility is unclear.\n3. Utility of TaylorNet is evaluated across a diverse set of tasks, image classification, sentiment analysis and dynamical systems.\n\nWeaknesses:\n1. The motivation for the work is not clear, especially given the existence of $\\Pi$-Nets. As far as I know, $\\Pi$-Nets are very similar, one can use 2nd order approximation in $\\Pi$-Nets and there is a version of it without activation functions as well.\n2. The motivation for networks without activation functions is also not very clear. TaylorNet is a composite function of TaylorBlocks. While TaylorBlocks themselves are interpretable, TaylorNet itself is too complex to interpret. The motivation is even less clear in tasks like image classification. \n3. Experimental comparisons to important baselines are missing. For example, $\\Pi$-Nets [1] in the case of image classification, SINDy [2] and its variants in the case of system identification for dynamical systems.\n4. Objectively the results are middling at best. There are a plethora of alternative CNN based architectures that achieve similar or better performance on CIFAR datasets compared to the results presented here. The advantage presented by TaylorNet over such models is unclear.\n\nOther Comments:\n- The claim about interpretability is too strong. The dynamical systems example is too simplistic.\n- In the experiment in 5.1, the conclusion that Taylor initialization is better than Xavier or Kaiming initialization is drawn from a simple two-layer network. Can you comment on whether this is expected to hold over larger/deeper networks as well?\n\n[1] $\\Pi$-nets: Deep polynomial neural networks, CVPR 2020\n[2] Discovering governing equations from data by sparse identification of nonlinear dynamical systems, PNAS 2016",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clear for the most part. The quality of the writing is adequate.\n\n- While novelty is a subjective concept, I believe that apart from the TaylorNet specific weight initialization, the rest of the concepts are already prevalent in the literature in one form or the other. From that perspective, in my opinion, the contributions of the paper are limited.\n\n- Unless I missed it, the paper does not propose to release code publicly. While the image classification and sentiment analysis experiments might be reproducible from the hyper-parameter details in the supplementary, I think the system identification experiments cannot be reproduced.",
            "summary_of_the_review": "\nOverall, the paper presents an interesting architecture based on the idea of Taylor approximation of multivariate functions. However, a majority of the conceptual contributions in this paper already exist in the literature. From a practical perspective, the contribution of TaylorNet over existing architectures is unclear.\n\nPost Rebuttal Update:\n\nI read the revised paper, comments from other reviewers, and the author's responses. The rebuttal addressed some concerns, especially related to experimental comparisons to $\\Pi$-Nets and SINDy. I increased the rating for the paper, but there are still some outstanding concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1285/Reviewer_9i73"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1285/Reviewer_9i73"
        ]
    }
]