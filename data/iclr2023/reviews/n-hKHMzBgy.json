[
    {
        "id": "mE2nhRjj3r",
        "original": null,
        "number": 1,
        "cdate": 1666492473703,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666492473703,
        "tmdate": 1666492473703,
        "tddate": null,
        "forum": "n-hKHMzBgy",
        "replyto": "n-hKHMzBgy",
        "invitation": "ICLR.cc/2023/Conference/Paper3761/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops a new type of generalization bound for both the linear classification setting and the quadratic ground truth function learned via a two-layer neural network in the non-linear setting. The main result of this paper is to show that above a certain signal-to-noise threshold, any near-max-margin classifier will generalize, while in a certain regime of the signal-to-noise threshold, uniform convergence generalization bounds are vacuous but the new generalization bound shows that memorization can coexist with generalization.",
            "strength_and_weaknesses": "Strength:\n\n- The new generalization bound developed in this paper is in contrast to the existing uniform convergence bounds, which have been shown to be vacuous in some settings. \n\n- The main discovery of this paper is to show that when the SNR of the data is in a certain range, uniform convergence bounds fail but near-max-margin solutions generalize. In this case, near-max-margin classifiers contain some generalizable components and some overfitting components that memorize the data. \n\nQuestion:\n\n- The data samples are generated by perturbing two points with random noises. What would happen if the data samples from both classes are overlapping and do not have a clear margin? ",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is well-written and the presentation is clear. ",
            "summary_of_the_review": "See above",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3761/Reviewer_qDFy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3761/Reviewer_qDFy"
        ]
    },
    {
        "id": "Hik2CQUKdm",
        "original": null,
        "number": 2,
        "cdate": 1666493873157,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666493873157,
        "tmdate": 1666493873157,
        "tddate": null,
        "forum": "n-hKHMzBgy",
        "replyto": "n-hKHMzBgy",
        "invitation": "ICLR.cc/2023/Conference/Paper3761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is an extension of the existing paper by Nagarajan & Kolter. \nPreviously, Nagarajan & Kolter show that there exist cases such that the true generalization gap is non-vacuous while uniform convergence fails, using an overparameterized linear regression case. \nIn this paper, the authors show that such a phenomenon happens in both linear and non-linear cases and provides the threshold for the phenomenon happens. \nOverall, I think this paper is very interesting and extends the scope of generalization analysis. ",
            "strength_and_weaknesses": "Strength:\n1. This paper focuses on a very important topic: generalization. \n2. This paper extends the previous results on linear cases to non-linear cases.\n3. Further, the authors also propose two thresholds for the signal-to-noise ratio. Under the first threshold, UC fails, and under the second threshold, generalization fails. The two thresholds are proven to be tight.\n4. The authors further provide some insights on the notion of margin, which explains how generalization works while UC fails. \n\nFlaws:\n1. I am not sure how many technical contributions are contained in this paper. The authors could restate the theoretical contributions more carefully. For example, from the linear case to the non-linear case, which technique is the core? And how does the non-linear case differ from the other non-linear cases since the XOR case proposed in this paper is solvable? \n2. It would be more interesting if the authors could provide some difficulties when generalizing the results to general cases, which, of course, is pretty hard. \n3. Although the title claims \"max margin\" and \"small margin\", this is only discussed in the final part of the paper and has a weak relationship with the main text. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper extends the existing papers. The techniques in this paper are somehow novel. \nBesides, this paper is well-written and easy to follow. ",
            "summary_of_the_review": "Although there are some small flaws in this paper, I believe this is an interesting topic and brings some insights into the deep learning community. \nTherefore, I give a general rating of acceptence. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3761/Reviewer_pEEx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3761/Reviewer_pEEx"
        ]
    },
    {
        "id": "4UseuJBz6r",
        "original": null,
        "number": 3,
        "cdate": 1666776783785,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666776783785,
        "tmdate": 1666776783785,
        "tddate": null,
        "forum": "n-hKHMzBgy",
        "replyto": "n-hKHMzBgy",
        "invitation": "ICLR.cc/2023/Conference/Paper3761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is motivated by the failure of uniform convergence based generalization bound. Accordingly, this paper proves a new type of margin based generalization bound in two settings: linear and two-layer nonlinear network model. In particular, this paper provides a threshold on the signal-to-noise ratio to indicate under which regime the margin based results and uniform convergence based results will success or fail. The authors also show that the max-margin solution will contain both generalizable components and some overfitting components.\n",
            "strength_and_weaknesses": "\nStrength:\n* This paper provides a new margin-based generalization bound for linear and two-layer network models on certain data set.\n* This paper shows that the max-margin solution will contain both generalizable and overfitting components.\n* This paper clearly identifies the settings that the margin-based results and uniform convergence based results will give vanishing or vacuous generalization bound.\n\nWeakness:\n* The generalization analysis in linear case is quite similar to that in [Chatterji and Long, 2021], a thorough discussion should be added. In fact, [Chatterji and Long, 2021] study the generalization error of gradient descent for learning high-dimensional Gaussian mixture model (their dimension assumption is a bit stronger than this paper as they require $d>n^2\\log(n)$), which can be also extended to derive certain margin-based generalization results.\n\nChatterji, N. S., & Long, P. M. (2021). Finite-sample Analysis of Interpolating Linear Classifiers in the Overparameterized Regime. J. Mach. Learn. Res., 22, 129-1.\n* As claimed by the authors, the max-margin solution will contain both generalizable and nongeneralizable (or overfitting) components, then why should one still prefer to consider max-margin solution? I can understand this for linear case since GD will finally give max-margin solution, but for nonlinear case, I have two concerns: 1) is (2.3) a good definition of the margin; and 2) why is studying such a max-margin solution, or near max-margin solution a good direction to understand the generalization?\n* In Proposition 3.3, the failure of near-max-margin solution will not generalization is only shown in terms of existence, while it is not clear whether this \u201cbad near-max-margin\u201d solution is able to be discovered by training algorithms. For instance, assuming the gradient descent will finally converge to the max-margin solution, then applying proper early-stopping will likely give a  near-max-margin solution. Then will this type of near-max-margin solution give bad generalization?\n* For xor case, it would be better to also consider $h\\ge 2$, as studied by [Cao et al., 2021] (though the over-parameterization conditions are different). Besides, a discussion in terms of the theoretical results compared with this prior should be added (after the theorem). \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality are generally good. In terms of novelty, the authors may need to add more discussions on the comparison to prior works, as commented in the weakness section.\n",
            "summary_of_the_review": "Overall this paper is well written. More discussions should be added according to my comments in the weakness section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3761/Reviewer_TxXm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3761/Reviewer_TxXm"
        ]
    },
    {
        "id": "8ePJ_MChY7",
        "original": null,
        "number": 4,
        "cdate": 1666865035858,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666865035858,
        "tmdate": 1670232767447,
        "tddate": null,
        "forum": "n-hKHMzBgy",
        "replyto": "n-hKHMzBgy",
        "invitation": "ICLR.cc/2023/Conference/Paper3761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers generalization aspects of learning algorithms on toy data. For the considered data distributions uniform convergence generalization bounds, as well as polynomial margin bounds, provably fail. Nevertheless, a maximum margin classifier can still generalize on such distributions.\nThis is a pure theory paper. The contributions are the quantification of the aforementioned effects, together with their proofs and discussions.",
            "strength_and_weaknesses": "Strengths\n- The paper extends prior results by Nagarajan & Kolter (NeurIPS 2019), as it shows that not only uniform convergence but also polynomial margin bounds, might be unable to explain generalization in deep learning.\n- The existence of regimes where different generalization bounds apply is very interesting and to some extent unexpected.\n- In addition to linearly separable data, a variant of the XOR problem is analyzed.\n\nWeaknesses\n- Section 3 of the paper (which presents the results) is **very** difficult to understand and technical. A careful and extensive polish is necessary. This includes a more thorough comparison with related work.\n- Reproducibility: The proofs are kept rather short. In consequence, I am unable to confirm the correctness of the results.",
            "clarity,_quality,_novelty_and_reproducibility": "**Regarding Novelty**\n\nThe Theorem 3.4 on the vacuity of uniform convergence bounds is similar to a prior result from Nagarajan & Kolter (NeurIPS 2019) and uses the same proof technique. Theorems 3.5 and 3.6 on the vacuity of polynomial margin bounds are novel as far as I am aware. While equally being based on the proof technique from Nagarajan & Kolter (NeurIPS 2019), this is a key contribution, particularly in combination with Theorems 3.1 and 3.2 which show that the maximum margin classifier still generalizes.\n\nHowever, the differences between this paper and Nagarajan & Kolter (NeurIPS 2019) are not obvious and demand a proper comparison.  \n1) Nagarajan & Kolter (NeurIPS 2019) already consider UC for the ramp loss, i.e. a polynomial margin loss.  \n2) According to the paper, the definition of usefulness is \"essentially equivalent\" to the definition of algorithm-dependent uniform convergence bounds from Nagarajan & Kolter. Yet, usefulness seems to be necessary for the results on margin bounds, i.e. algorithm-dependent uniform convergence cannot be used there.  This suggests that usefulness is not equivalent, but a stronger assumption.  \n3) Nagarajan & Kolter (NeurIPS 2019) argue that \"showing failure of uniform convergence in these [linear] models is, in a sense, the most interesting.\" I would like to hear the authors perspective on this, as they do consider two layer networks specifically.  \n4) The third paragraph of the introduction is confusing. It begins with that the results of Nagarajan & Kolter do not apply to many margin-based generalization bounds but later it states that the results suggest that \"classical margin bounds may not be useful\".\n\nThe intuitions section it shows, that the max margin classifier corresponds to the empirical mean of the data. Since Theorem 3.1 assumes that the data distribution is from a family parametrized by $\\mu$, is achieved, if the parameter $\\mu$ is estimated accurately enough. This suggests that Theorem 3.1 could be considered as a classic mean estimation problem and the relation to the max-margin might only be by chance (or construction). I would like to see a discussion.\nThis observation also suggests a more direct proof; see the end of the review.\n\n--------------------\n\n**Regarding Clarity** \n\n- The paper is difficult to understand and the results in Section 3 are presented quite technically. If theorem statements could be changed so that they focus on the key findings, this would be a major improvement.\n\n- The main assumption seems to be the value of the \"signal to noise ratio\" $\\kappa$, as it determines the generalization regime (see Fig. 1). However, in the theorem statements, there are additional implicit assumptions on $\\kappa$ (e.g. Theorem 3.4 requires $\\frac 1 {\\kappa \\sigma^2} = d/n\\ge c$, Theorem 3.1 requires $d/n\\ge c \\frac 1 {\\kappa^2}$, which is equivalent to $\\kappa\\ge c \\sigma^2$). Similarly, the section \"key intuitions for generalization theorems\" requires $\\frac n {d \\sigma^4} = \\frac \\kappa {\\sigma^2}$ to be large.\nIt seems that $\\kappa$ alone is insufficient to determine the generalization behavior and Figure 1 is too simple.\n\n- Connected to the previous point is that the assumptions of the theorems are not clear.\nMany assumptions are interconnected and identifying the validity region of the theorems is difficult.\nFor instance, Proposition 3.5 requires $\\frac n d  \\le \\frac 1 c \\kappa^2(\\kappa_{uc}-\\kappa)^4$ or equivalently $\\sigma^2 \\le \\frac 1 c \\kappa(\\kappa_{uc}-\\kappa)^4$. But $\\kappa$ is again a function of $\\sigma$, so this inequality actually determines the range for $\\sigma$ given $n/d$.\n\n- The remark after Theorem 3.2 indicates that for a given probability distribution, i.e. given $\\mu, \\sigma, d$, the theorem only holds if the sample size $n$ is **small enough**. This is quite unusual.\n\n- The theorems are not limited to the maximum margin classifiers but consider also ($1-\\epsilon$)-maximum margin classifiers. I might be missing something, but this does not appear to be a strengthening of the results. If the bounds hold for maximum margin classifiers, then continuity implies that the bounds still hold in a sufficiently small neighborhood, e.g., in a sublevel set of the margin operator.\n\n-----------------------------\n\n**Regarding Reproducibility**\n\nThe proofs are kept short and many steps must be filled in by the reader. Due to the short reviewing period, I could only check proofs for the linear setting.\n\nBelow, I will list points, where I am not sure, if the proof is correct and I just missed something, or if there is a mistake. And if so, if it can be corrected such that the theorems still hold.\n\n\n- **Proof of Lemma B.1:** Why does invertibility of $\\Xi^T \\Xi$ imply $v = \\Xi (\\Xi^T \\Xi)^{-1} c$? By assumption $\\Xi^T v = c$ and so $v= (\\Xi \\Xi^T)^{-1} (\\Xi \\Xi^T) v = (\\Xi \\Xi^T)^{-1} \\Xi c$. Further, why does this imply that  $\\|v\\|$ is in the interval specified in Eq. B.5?\n\n- **Proof of Lemma C.1:**  How is Lemma B.1 applied here?\nWhat would be $c$ and what would be the corresponding minimum vector $v$?\n\n- **Proof of Lemma C.3:** Typo. In Eq. C.17, \"$\\le$\" needs to be replaced with \"$\\ge$\".\n\n- **Proof of Lemma A.1:** Could you elaborate on Eq C.36. I understand that $f_v^2$ is beta(1/2, (d-2)/2) distributed, but then I am out.\n\n- **Proof of Theorem 3.1:** The last inequality uses $d>n$, but this is not listed as an assumption.\n\n------------------\n\nUnrelated to score. A more direct proof of Theorem 3.1 (and perhaps 3.2) seems possible. In the intuitions section it is shown, that the max margin classifier corresponds to the empirical mean of the data and that its \"overfitting component\" $v$ is the empirical mean of the spherical noise. Thus, concentration inequalities, e.g. vector Bernstein, allow to bound $\\|| v\\||$ with high probability and could be used instead of Lemma C.2.\n\nTypo on page 7 $\\sqrt{\\gamma(w_b,S)^2+  \\gamma(w_b,S)^2}$ change to $\\sqrt{\\gamma(w_g,S)^2+  \\gamma(w_b,S)^2}$",
            "summary_of_the_review": "The observation that there exists data sets where max-margin classifiers work but polynomial margin bounds fail is quite interesting. This will be a good paper, if presentation is improved, i.e. theorems simplified and and put into context. However, doing so might require a major revision of the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3761/Reviewer_DBpB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3761/Reviewer_DBpB"
        ]
    }
]