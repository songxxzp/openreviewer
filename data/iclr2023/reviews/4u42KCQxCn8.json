[
    {
        "id": "9AC6eLN2HN",
        "original": null,
        "number": 1,
        "cdate": 1666373687063,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666373687063,
        "tmdate": 1671482222328,
        "tddate": null,
        "forum": "4u42KCQxCn8",
        "replyto": "4u42KCQxCn8",
        "invitation": "ICLR.cc/2023/Conference/Paper5194/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes Joint Demo-Language Task Conditioning, DeL-TaCo, as a new way of training and specifying goals for goal-conditioned task-learning agents. DeL-TaCo learns a demonstration encoder and uses a frozen, pre-trained language encoder to embed task demonstrations and language-specifications for simulated robots to perform pick-and-place tasks with various objects/colors/shapes. DeL-TaCo is compared to language-only and demonstration-only ablations, showing that both demonstrations and language are important to achieve the highest success rates, and two different encoders are compared, a pre-trained CLIP vs. a pre-trained DistilBERT + trained demonstration encoder.",
            "strength_and_weaknesses": "Strengths:\n* The method is clearly described and intuitive, and is a natural next-step after prior work.\n* Combination of demonstrations and language appears to provide significant improvements over either modality individually.\n* The work is positioned clearly with respect to BC-Z.\n* The authors developed a simulator/simulated task for their work.\n\nWeaknesses:\n* The proposed method feels like a solid first-step, but many natural follow-up questions are left unexplored (what if the language encoder is fine-tuned? Why not pass the CLIP embeddings through a small, fine-tuned MLP? How does a pre-trained vision module work for the demo encoder? etc)\n* Training the demonstration encoder seems to require prior knowledge on the number of possible tasks, limiting the generality of DeL-TaCo  to situations with fixed numbers of tasks, despite the goal-encoding being quite general.\n* Experimental reporting feels disjointed and incomplete. While the discussion seems to suggest that language is superior to demonstrations (e.g., saying that language is worth 50 demonstrations), in fact the demo-only method performs better than the language-only method in both experiments. There are also no upper-bound numbers for Table 2, so we can see that DeL-TaCo is better than demo/language-only, but we don't know how far it is from \"perfect\" performance.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n* The work is clearly explained and the method is easy to understand.\n* The task description is more difficult to understand directly from the text, but the appendix helps to understand what objects were used, what \"shape\" means and how it is distinct from \"object\", etc.\n\nQuality:\n* The work is high quality and appears to have been conducted rigorously, though natural questions around training details or specifications appear to be left unanswered.\n\nNovelty:\n* The work has not compared thoroughly to related work, except through ablations of the proposed method.\n* Prior work on language encoding for goal-conditioned agents is missing [1, 2]\n* The overall method is a relatively straightforward combination of prior work without significant additional contributions, though this is not a reason to reject as the method is well-explained, well-motivated, and performs well.\n\n[1] Sodhani, Shagun, Amy Zhang, and Joelle Pineau. \"Multi-task reinforcement learning with context-based representations.\" International Conference on Machine Learning. PMLR, 2021.\n\n[2] Silva, Andrew, et al. \"LanCon-Learn: Learning With Language to Enable Generalization in Multi-Task Manipulation.\" IEEE Robotics and Automation Letters 7.2 (2021): 1635-1642.\t",
            "summary_of_the_review": "The proposed method, DeL-TaCo, is explained clearly, is intuitive and straightforward, and appears to work well. The paper suffers from incomplete reporting in the results, a relative lack of novelty, and unanswered questions about training details or design decisions. With simple additional experiments around fine-tuned language encoders, clearer results for demonstration-vs-language goal-specification and performance upper-bounds, and position to recent related works, I would advocate for acceptance. As the paper reads currently, it does not seem publication-ready.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5194/Reviewer_5Uyd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5194/Reviewer_5Uyd"
        ]
    },
    {
        "id": "cLrWqjecaL",
        "original": null,
        "number": 2,
        "cdate": 1666556138850,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556138850,
        "tmdate": 1670904990374,
        "tddate": null,
        "forum": "4u42KCQxCn8",
        "replyto": "4u42KCQxCn8",
        "invitation": "ICLR.cc/2023/Conference/Paper5194/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper argues that solely using language or video demonstrations to learn robotic manipulation tasks is inadequate as it creates ambiguities in learning. To alleviate this confusion, the authors propose to train a multi-task policy that condition on *both* the video demonstrations as well as the language.\n\nThey work in the paradigm of imitation learning, wherein they train a single policy for all the robotic pick-and-place tasks.",
            "strength_and_weaknesses": "**Strengths**\n\n1. The paper is mostly well-written, and the methodology section is easy to understand in one go.\n\n2. Section 5.3 which compared how much was a language worth was a good discussion -- which drives the point that having language as input is indeed beneficial (as corroborated by several other works in the field)!\n\n\n**Weaknesses + Questions:**\n\n**Claims**\n1. The authors claim the following in Sec 2.2\n\n> \"We argue that conditioning the policy on both a demonstration and language not only ameliorates the ambiguity issues with language-only and demonstration-only specifications but is much easier and more cost-effective for the end-user to provide.\"\n\nI'm not sure what the authors mean by \"more cost-effective for the end-user to provide\" -- I agree that providing numerous demonstrations (at least 50 according to the experiments) has a high cost on the user, but having only language would have a lower cost (as done in [1])! \n\n\n**Methodology**\n\n2. The rationale behind the Task Conditioning Architecture: How was the architecture selected? Was it based on previous works of [2], and [3]? \n\n    A simple variant (baseline) would be to concatenate the $z_{demo}$ and $z_{lang}$ and use that as input to the policy. Was this baseline tried? If so, what were the observations?\n\n**Experiments**\n\n3. Success Rate: I am not sure why this complicated way of computing the Success rate was adopted. Specifically, I'd like the authors to address the following questions:\n\n   a) Why are the rollouts *during* the training being considered? What is the problem with computing the success rate after training (800-900k steps)?\n\n   b) How does the current definition of success rate avoid *cherry-picking* when compared to (a)?\n\n4. Training + Test Sets (a Scenario C): In all the experiments, the three objects in the scene are *visually distinct*. Having the same objects with a) the same colors, as well as b) different colors in the test set would be a crucial experiment to test the generalization of the method. I would like to see the results of such an experiment. Since it is only testing, I believe it will not take a lot of time.\n\n5. Baseline Comparison: I would expect a comparison with [1] to be present in the paper. I understand that the code for [1] is unfortunately not open-sourced (which I don't hold against you, and blame the authors of [1]), but the method in [1] can be adapted to your codebase wherein you have a shared latent space -- which is essentially the crux of [1].\n\n\n-------\n**References**\n\n[1] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. In RSS 2021.\n\n[2] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In CoRL 2021.\n\n[3] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "**Writing**\n\n6. How were the demonstrations collected? : On page 7 under the sub-section of *Data*, the authors mention that a \"scripted policy\" was used to collect demonstrations. What was this scripted policy -- a policy that uses inverse kinematics? Please mention the details of it.\n\n**Section 4.1**\n\n7. The variables $m$ and $n$ are introduced in the first paragraph and nowhere in the main paper are they defined. It is only in the appendix that their descriptions are given. Please define what they are for clarity.\n\n8. Under the *Task Conditioning Architecture* sub-section, please briefly describe what *FiLM* does. One sentence should be sufficient -- otherwise, it makes the reader switch back and forth between the original paper and this.\n\n\n**Visual Aesthetics (not considered for decision-making)**\n\n9. I believe adding plots instead of the table would be much easier to read (except for Table 3). I say this after looking that the authors have used plots on the project webpage, which have better readability and also look better.\n\n**Reproducibility**\n\n10. The authors do mention the details of the model in the appendix including the hyperparameters they used. However, I would like to ask if the authors plan to open-source their code, if accepted to the conference?",
            "summary_of_the_review": "Even though the paper's contribution in methodology is significant enough for the community, due to the lack of extensive experiments and proper baselines as mentioned in the Weaknesses section, I would vote 5 : *marginally below the acceptance threshold*. However, my final decision is subject to change based on the authors' rebuttal and comments of other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5194/Reviewer_Ch3U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5194/Reviewer_Ch3U"
        ]
    },
    {
        "id": "uxh5b0YZHI",
        "original": null,
        "number": 3,
        "cdate": 1666666685322,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666685322,
        "tmdate": 1668916716158,
        "tddate": null,
        "forum": "4u42KCQxCn8",
        "replyto": "4u42KCQxCn8",
        "invitation": "ICLR.cc/2023/Conference/Paper5194/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops an approach, Joint Demo-Language Task Conditioning (DeL-TaCo), for behavior cloning in a multi-task setting where the task is specified by both language and a demonstration. The approach includes a contrastive loss to guide the learning of the language and demonstration embeddings, which are then fed into a policy head. Results are conducted in simulation and show that virtual robot manipulation tasks can be learned by DeL-TaCo and that this approach generalizes better to novel tasks.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper clearly describes a supervised learning approach to multi-task learning with tasks conditioned on both a natural language instruction and a demonstration of the task. \n\nThe figures are relatively informative.\n\nPseudocode is provided.\n\nThe paper reports positive, empirical results on a relatively large dataset of multiple tasks.\n\nTable 3 shows how much data is required to enable a demo-only policy to match the performance of the proposed method, DeL-TaCo, showing the proposed method is better up until reaching ~50 demonstrations for the dataset for the novel task.\n\nA limitations and future work section is included.\n\nWeaknesses:\n\nThis approach requires both language and instructions to function. As such, one would want to show superior performance to methods that require only one. For example, recent work by Wen et al. (2022) required only a single demonstration on a novel task:\n\nWen, B., Lian, W., Bekris, K. and Schaal, S., 2022. You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration. arXiv preprint arXiv:2201.12716.\n\nOn the language side, there are quite a few recent papers show strong performance conditioning on only language:\n\nSodhani, S., Zhang, A. and Pineau, J., 2021, July. Multi-task reinforcement learning with context-based representations. In International Conference on Machine Learning (pp. 9767-9779). PMLR.\n\nSilva, A., Moorman, N., Silva, W., Zaidi, Z., Gopalan, N. and Gombolay, M., 2021. LanCon-Learn: Learning With Language to Enable Generalization in Multi-Task Manipulation. IEEE Robotics and Automation Letters, 7(2), pp.1635-1642.\n\nNair, S., Mitchell, E., Chen, K., Savarese, S. and Finn, C., 2022, January. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning (pp. 1303-1315). PMLR.\n\nShridhar, M., Manuelli, L. and Fox, D., 2022, January. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning (pp. 894-906). PMLR.\n\nThis reviewer notes that the authors discussed CLIPort and the Nair et al. (2022) paper; however, these works were not benchmarked against. The paper does not present a clear argument against benchmarking against these approaches (other than the implicit argument that they only condition on language, which is actually an advantageous quality).\n\nThis paper is a relatively straightforward design of a neural network-based architecture where embeddings are learned for language instructions and demos (which are forced to be the same through a contrastive loss) and then fed into a policy head for supervised learning. It is unclear whether new theories or insights are gleaned (other than adding more input data can enhance performance by 5-9%).\n\nMore insight into why DistilBert was used could be helpful. What would other language embeddings look like?\n\nAn ablation study showing the efficacy of the contrastive loss and other components would be helpful.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively clear, and the figures are quite nice.\n\nThe novelty of the paper is relatively minor. Prior work has explored conditioning on language and demonstrations, though, the combination of the two is more unique.\n\nThe paper does not appear to have code for reproducibility, though, additional details are provided in the appendix.",
            "summary_of_the_review": "This paper provides positive results for conditioning on language and demonstrations in multi-task learning for zero-shot generalization. However, the paper would be improved by more baselines and a more compelling case that the results are novel and significant.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "This paper builds upon large language models, and the ethical issues of those are not discussed in the limitations. This can readily be corrected by the authors.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5194/Reviewer_p2aG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5194/Reviewer_p2aG"
        ]
    },
    {
        "id": "VEpsTaG-0o9",
        "original": null,
        "number": 4,
        "cdate": 1666712174699,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666712174699,
        "tmdate": 1666712174699,
        "tddate": null,
        "forum": "4u42KCQxCn8",
        "replyto": "4u42KCQxCn8",
        "invitation": "ICLR.cc/2023/Conference/Paper5194/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Interesting paper, it uses language and demonstration conditioning to improve learning in an behaviour cloning set up for robot manipulation. The paper proposes:\n1. a new architecture for training an eval conditioned on language and demonstrations together\n2. some level of generalization with this",
            "strength_and_weaknesses": "Strengths:\n- pretraining demonstration and language using contrastive loss is an interesting idea\n- it's intuitive that conditioning on both improves performance, is a simple and elegant idea\n- generalization results look decent\n- the conclusion that language is worth 50 demos is useful, but this breaks down once you've 1000s of tasks\n\nWeaknesses:\n- tested on only sim, where data is very clean\n- concerned that conditioning on one demo would make a policy rely on the spurious correlations of that demo : specific action situations, background, etc. \n- given several demos of the same task, how would you sample the one to condition on? how do you scale this for large datasets\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- very clear\n\nQuality\n- good quality\n\nNovelty\n- reasonably novel\n\nReproducibility\n- high",
            "summary_of_the_review": "The paper proposes a method to condition on language and demo to solve robotic behaviour cloning. This is a decent set up in small-scale robot learning set ups. The idea is novel and simple. The claims of the paper are reasonably well supported within the data distribution used. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5194/Reviewer_4Ugt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5194/Reviewer_4Ugt"
        ]
    }
]