[
    {
        "id": "t_l78s7w01",
        "original": null,
        "number": 1,
        "cdate": 1666042937188,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666042937188,
        "tmdate": 1666042937188,
        "tddate": null,
        "forum": "egaddkwMOd3",
        "replyto": "egaddkwMOd3",
        "invitation": "ICLR.cc/2023/Conference/Paper1448/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to incorporate the return information in implicit behaviour cloning. Empirical study confirms the efficacy of the proposed algorithms in some domains",
            "strength_and_weaknesses": "Strength:\nThe paper is well written and easy to follow. The idea is simple but proves to be effective in some domains. \n\nWeakness:\n1) The improvements in adroit and kitchen are impressive. However, the reason for the failure in Mujoco is not clear. Since the main contribution of this work is empirical, I feel the authors should dig into Mujoco more deeply to understand this failure.\n2) I think some important baselines are missing, e.g., the authors should include comparison with offline RL methods with return-conditioned policy with explicit model, mentioned in Section 4.2. \n3) Table 1 results from only 3 seeds without metrics on variation. So it is not as informative as it should be.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality are good. The algorithm seems easy to reproduce. The proposed algorithm seems novel to me.",
            "summary_of_the_review": "see above",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1448/Reviewer_ExRr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1448/Reviewer_ExRr"
        ]
    },
    {
        "id": "RzUo6wL1zVy",
        "original": null,
        "number": 2,
        "cdate": 1666560197007,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560197007,
        "tmdate": 1666562424164,
        "tddate": null,
        "forum": "egaddkwMOd3",
        "replyto": "egaddkwMOd3",
        "invitation": "ICLR.cc/2023/Conference/Paper1448/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes using implicit models for offline reinforcement learning. More specifically, this paper proposes using an implicit model to model the distribution of actions and returns conditioned on observations, and to bias sampling towards actions with high returns. The paper evaluates using implicit models against state of the art offline reinforcement learning methods on the ADROIT, Mujoco, and FrankaKitchen settings.",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-written and clear to follow.\n- The proposed method is interesting and seems to be pretty simple and elegant. \n- The toy examples and plots are nice in providing some intuition about the approach.\n\nWeaknesses:\n- It's not clear why IRvS is better than the naive way of incorporating return information by combining IBC with RWR. Furthermore, IRvS only does slightly better than IBC w/ RWR, and the performance is similar on most tasks. IRvS with the best eta does perform better, but requires tuning for eta. Much more thorough analysis is needed here, as this is the main contribution of the paper.\n- Training EBMs with Langevin Dynamics can be difficult and requires many hyperparameters. \n- Sometimes reward information is unavailable or difficult to get, particularly in real robotics, and the method may struggle in these cases.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: paper is well-written.\nQuality: See strengths and weaknesses above. In the experiments, the proposed method does not convincingly work better than other approaches for robot tasks. \nNovelty: Technical novelty is somewhat limited, as much of the proposed method does not introduce many novel concepts. \nReproducibility: The authors would need to provide better documented code, and with all environments, in order to reproduce this work. ",
            "summary_of_the_review": "I think this is an interesting direction, but due to the limitations in the performance of the method and drawbacks of the method itself, I think the paper is not ready for acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1448/Reviewer_3Dga"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1448/Reviewer_3Dga"
        ]
    },
    {
        "id": "EVGIZExERPY",
        "original": null,
        "number": 3,
        "cdate": 1666667782917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667782917,
        "tmdate": 1666667782917,
        "tddate": null,
        "forum": "egaddkwMOd3",
        "replyto": "egaddkwMOd3",
        "invitation": "ICLR.cc/2023/Conference/Paper1448/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an Implicit Reinforcement Learning via Supervised Learning (RvS) methods by leveraging the implicit model---the composition of $\\arg\\min$ with a general function approximator $f_\\theta$ to represent the policy ($\\hat{a} = \\arg\\min_a f_\\theta (s, a)$) [1]---instead of the traditional explicit model ($a = f_\\theta (s)$) in RvS methods to solve offline RL problems. Moreover, the authors provide empirical results to show the superiority of the implicit model. Experiments demonstrate the performance improvement of the implicit model in RvS methods for offline RL.\n\n[1] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson. Implicit behavioral cloning. In Proceedings of the 5th Conference on Robot Learning. PMLR, 2022.",
            "strength_and_weaknesses": "Strength: \n1. The authors leverage the exponential tilt density [1] to learn policies that can head toward the largest rewards from offline datasets collected by policies with different expertise levels.\n\nWeaknesses:\n1. The authors claim that they bridge an important gap between IBC [2] and RvS by modeling the dependencies between the state, action, and return with an implicit model on Page 6. However, noticing that IBC proposes to use the implicit model to model the dependencies between the state and action, I think the contribution of this paper is to introduce the return from RvS to the implicit model. Thus, the proposed method looks like a combination of IBC and RvS.\n\n2. The authors conduct experiments in Section 5.1 to show the advantages of the implicit model. However, such advantages are similar to IBC, which could hurt the novelty of this paper. The authors may want to highlight the novelty of the proposed method against IBC.\n\n3. The discussions of the empirical results in Sections 5.1 and 5.2.2 are missing. The authors may want to explain: 1) why the RvS method fails to reach either goal and converges to the purple point in Figure 4(b); 2) why the explicit methods perform better than implicit methods on the locomotion tasks.\n\n4. The pseudo-code of the proposed method is missing.\n\n[1] S\u00f8ren Asmussen and Peter W Glynn. Stochastic simulation: algorithms and analysis, volume 57. Springer, 2007.\n\n[2] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson. Implicit behavioral cloning. In Proceedings of the 5th Conference on Robot Learning. PMLR, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThis paper has several typos as follows.\n1. $\\eta^{-1} = 5.0$ in the caption of Figure 2(b) is inconsistent with the value in the sentence \u201cIn Figure 2b, we study the impact of increasing $\\eta^{-1}$ to 3\u201d on Page 4.\n2. In Extrapolation of Section 5.1, \u201cFigure 4c a)\u201d should be \u201cFigure 4c i).\u201d\n\nNovelty:\nThe novelty of this paper is marginally significant.\n\nReproducibility:\nThe authors provide the code for reproducibility.\n",
            "summary_of_the_review": "I think the proposed method is a simple combination of IBC [1] and RvS. The claimed major novelty of this paper is the same as that proposed by IBC.\n\n[1] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson. Implicit behavioral cloning. In Proceedings of the 5th Conference on Robot Learning. PMLR, 2022.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1448/Reviewer_aCoh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1448/Reviewer_aCoh"
        ]
    }
]