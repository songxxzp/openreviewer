[
    {
        "id": "HwGZRKfYd3",
        "original": null,
        "number": 1,
        "cdate": 1666059609747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666059609747,
        "tmdate": 1670862347102,
        "tddate": null,
        "forum": "935WW9F8ALr",
        "replyto": "935WW9F8ALr",
        "invitation": "ICLR.cc/2023/Conference/Paper5222/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThe paper presents an approach to suggesting code changes that improve the performance of programs.\nThe paper first analyzes the distribution of (correct) solutions to competitive programming problems, finding a large gap between the efficiency of median-performance and top-quantile-performance solutions.\nThe paper proposes a generative approach to suggesting these code changes, based on a discrete VAE style approach.\nTo train, the approach uses Transformer encoders to encode a program and a corresponding optimized version of that program, takes the difference between the embeddings to generate an edit vector, discretizes the edit vector, and then decodes a new program using that discretized edit vector.\nAt evaluation time, the approach selects a random edit vector from a dictionary of trained edit vectors (representing different possible program edits), then decodes that using the original slow program to produce a new program with that edit applied.\nThe authors find that this approach qualitatively results in programs with optimizations applied, and quantitatively results in programs becoming more similar to semantically equivalent, optimized programs.\n",
            "strength_and_weaknesses": "\nStrengths:\n* Interesting problem under study, interesting and novel solution to the problem\n* Very clearly described approach\n* Strong quantitative evaluation of the results\n\nWeaknesses:\n* Weak qualitative evaluation of the results (see \"Quality\" below)\n",
            "clarity,_quality,_novelty_and_reproducibility": "As a note on my background, I am not sufficiently familiar with the relevant education and software engineering literature to be able to judge the usefulness of the proposed model of suggesting code edits.\n\n## Clarity\n\nOverall, the paper is very clear. There are minor issues with the description of the approach, but these are not major concerns (and did not significantly impair my understanding of the paper).\n\n* The ROUGE-8 metric is not described in the paper\n* Figure 1, Figure 4: are these help texts (\"Algorithmic Difference: Use a...\", \"Efficiency Idea: This is...\", etc) generated by the approach? I didn't see any description of these, though during my initial reading I assumed they were. It would be more clear if the Figure 1 used phrasing that made it clear that these were descriptions, rather than prescriptions, and if Figure 4 did not include this text.\n* Figure 3, and approach broadly:\n  * The roles of the \"prior sampler\" and \"commitment_loss\" in Figure 3 are not discussed in the text\n  * \"The total input to the decoder is...\". Does the decoder not also take the raw text of the slow program? Or is this only used during training?\n* Nit: \"Additionally, Figure 2(b) indicates that the data is multi-modal\". Figure 2(b) only shows a single real mode of program lengths, around 125 tokens.\n\n## Quality\n\nThe approach is a well-thought-out approach and seems like a reasonable solution to the task.\nThe quantitative aspect of the evaluation is also well done.\nHowever, the qualitative aspect of the evaluation is lacking, and it's not clear to what extent the generated suggestions are practically useful, diverse, or interpretable.\n\nThere is essentially no qualitative analysis of the edits. What types of edits are learned? How interpretable are the different $z_i$, as the paper claims? The paper provides a couple of examples in Table 2, but this analysis isn't sufficient.\n* E.g.: does the edit in Table 2(a) applied to other code blocks always result in adding early termination to a while loop? What happens if it is applied to a program without such an optimization opportunity? Do any other latent edits make this same change?\n* In 2(b), Edit 2 seems to speed up the program by making the loop never execute (and doesn't apply the edit discussed in the caption)\n* It's also not clear to what extent the results in Table 2 are randomly sampled v.s. cherry picked\n\nThe utility of these edits would ideally be demonstrated with a user study, to support the utility of the model and the underlying assumption that directionally (but not necessarily literally) correct edits are a useful hint for programmers.\nBarring that, the paper should provide a more detailed qualitative analysis (including potentially a detailed set of case studies, randomly sampled edits, a user study, analyses of how consistent and interpretable the $z_i$ are, or other systematic empirical analyses).\n\nThe quantitative analysis is well-done, but not sufficient for understanding the results -- all of these scores have clear issues in the case where the generated transformation is correct and useful, but does not already exist in the dataset.\n\n## Novelty\n\nThe approach is novel to the best of my knowledge, in that it provides a new problem formulation for helping users write more efficient code.\n\n--------\n\n## Update after author response\n\nThanks to the authors for their response. Based on this response, my score remains unchanged.\n\nMy main concern with the paper is that the core design decision of the VQVAE model, and several of the claims around it, are essentially unvalidated. For instance, the contributions section claims:\n\n> We qualitatively demonstrate that the learned discrete latent variables represent different edits, and that the edits that are assigned to one latent variable are generally consistent.\n\nThe paper does not give sufficient evidence to validate this claim.\n\n* There are not enough methodological details to understand Table 2. Are these proposed edits from the same or different latents?\n* Table 2 also does not give enough examples to convincingly argue that there is consistency or diversity among the proposed edits.\n* In Figure 5, showing that the edits apply what looks like a translation in PCA to 3 dimensions does not convincingly show that these edits are semantically consistent.\n* Figure 6 is a step in the right direction, with a more clear statement of methodology showing that the edits applied are from the same latent. However, the data presented here is still very sparse, only showing some amount of consistency for one latent across three similar programs. The data do not show that different latents generate different edits.\n* I'm also generally concerned about whether these results are cherry picked. The paper doesn't include any statement of methodology for how these examples are selected.\n\n\nHaving looked more at these results, I'm also concerned about the quality of the data, and the corresponding semantics of the task. All of the original programs in Figure 6 (and many of the other programs throughout the paper -- e.g., those in Table 2) either immediately crash or don't execute anything (but would crash if they were to execute). What does it mean to apply an edit to speed these programs up?\n",
            "summary_of_the_review": "Weak reject.\nThough the paper shows promise, the near complete lack of a qualitative evaluation means that the paper does not rise to the level of acceptance.\nI would be willing to raise the score if the authors provide any of the following:\n* A user study demonstrating that the suggested edits lead to better code performance (while maintaining correctness)\n* A significantly more thorough analysis of the different $z_i$, showing the extent to which they consistently lead to the same proposed types of edits (on the same original programs and across different programs), the extent to which they encode semantically different types of edits with different $z_i$,\n* Generally, more detailed case studies of proposed edits\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5222/Reviewer_39mZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5222/Reviewer_39mZ"
        ]
    },
    {
        "id": "srj0D8chuRO",
        "original": null,
        "number": 2,
        "cdate": 1666210303584,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666210303584,
        "tmdate": 1666615505535,
        "tddate": null,
        "forum": "935WW9F8ALr",
        "replyto": "935WW9F8ALr",
        "invitation": "ICLR.cc/2023/Conference/Paper5222/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper addresses the task of improving the efficiency of code, by providing feedback hints. The authors curated a dataset from the Google Code Jam competition, and trained a VQ-VAE model to automatically provide feedback on how to improve the runtime efficiency of the solutions. ",
            "strength_and_weaknesses": "## Strengths \n* The task of improving code efficiency is important and exciting\n* The application of improving code efficiency is novel, as far as I know.\n* The idea of curating a dataset of code competition solutions is clever, since these solutions can be executed and their runtime can be measured, and this runtime can serve as a signal for training.\n* The proposed VQVAE model is interesting and performs better than the straightforward transformer (although not a contribution of this paper).\n\n## Weaknesses\n* Evaluation - The main weakness is evaluation. To train the model, the authors canonicalize the solutions in a way that does not allow to execute the edited programs anymore. That is, there is no way to measure whether the generated \"improved\" programs actually run faster than their original versions.\nInstead, the authors use varieties of BLEU to measure whether the prediction is lexically similar to the reference. This measure is problematic, since lexical similarity does not guarantee faster execution. In fact, the solutions were clustered using lexical similarity initially (ROUGE-8), so the \"slow\" program and the \"fast\" reference program are already supposed to be quite lexically similar.\n\nThere is no need to generate executable programs in all papers; however in this paper, since it focuses on improving the efficiency of programs, demonstrating that the generated programs are indeed more efficient is crucial.\n\n* Confusing terminology - some claims in the introduction are very confusing, and might not necessarily be correct.\nConcretely, I think that the authors confuse \"long runtime\" with \"low performance\", and assume that every solution that takes a lot of time to run must be low-performance. For example, the introduction says `we find wide variance in computational cost: the runtime difference between a median solution and the 90th percentile solution is over two-fold`. But why are solutions to different problems comparable? This variance might stem from simply the variance in the questions, instead of the variance in the solutions, as the authors imply.\n\nFor example, an extremely inefficient solution for \"binary search over 100 numbers\", implemented in `O(n)` instead of `O(log(n))`, might run much faster than a super-efficient solution for \"Bellman-Ford algorithm over a clique of 1000 nodes\", but the inefficient solution might still run faster than the efficient solution, if we compare their absolute runtimes.\n\nThe paper concludes that `The scarcity of high-performance solutions highlight the difficulty of our task`, and `high-performance solutions are uncommon`. However, I was not convinced that high-performance solutions are scarce or uncommon. A solution may be high-performance - *but still have a long runtime*, because the problem it is solving requires polynomial time complexity, or its test inputs are longer, for example. In other words,\n**a solution to a problem that requires polynomial time complexity is not necessarily inefficient** compared to the other potential implementations of the same problem.\n\n* Analysis -  Since the VQ-VAE model is not the straightforward transformer, and the authors claim that `each discrete latent variable represents a different learned category of code-edit`, it would be nice to see some analysis of the learned latent variables and what have they captured, to understand why does the VQ-VAE perform so much better than the transformer.\n\n## Additional Questions\n1. The introduction says that `compilers and current tooling have more difficulty identifying higher-level optimizations, such as more efficient algorithms to the same problem`. Does the proposed model actually find \"more efficient algorithms to the same problem\"?\n2. Why are Figures 2(a), 2(b) and 2(c) meaningful? Why is it meaningful to compare solutions of different problems? The variance in program length or runtime might be natural and stem from the variance in the questions. For example, one question might have logarithmic complexity and another question might have exponential complexity. Why should we compare their solutions' runtime, and conclude that the exponential one is \"not high-performance\"?\n3. Does clustering of the submissions according to ROUGE-8 **really** find solutions to the same problem? Did the authors perform any manual annotation to verify that this is indeed the case? I can imagine examples where inter-problem solution similarity might be higher than intra-problem solutions. That is, solutions to different problems might be almost identical to each other, while different solutions to the same problem will be very different.\n4. Section 4.3 say that `We use **syntactic** program similarity as a tool to measure model performance`. But in fact, all the metrics that are described below are variations of BLEU. Why is this *syntactic* similarity?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clear and reproducible. \nHowever, some figures such as Figure 1, Figure 4 and Table 2 contain tiny text, which makes them very difficult to read.",
            "summary_of_the_review": "The paper addresses an important problem using a clever source of data.\nHowever, it suffers from a major evaluation weakness, since the main claim of \"improving efficiency\" could not be measured.\nThus, I must vote for rejection at this time, but I hope to see the paper's evaluation improved in the future.\n\nI **will increase my score** if the evaluation could show an actual convincing improvement in runtime.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5222/Reviewer_hCSu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5222/Reviewer_hCSu"
        ]
    },
    {
        "id": "ESIjqFk-i8_",
        "original": null,
        "number": 3,
        "cdate": 1666570712275,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666570712275,
        "tmdate": 1666633102822,
        "tddate": null,
        "forum": "935WW9F8ALr",
        "replyto": "935WW9F8ALr",
        "invitation": "ICLR.cc/2023/Conference/Paper5222/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors propose a variational auto encoder that suggests code edits to improve performance of programs. The system is trained using programs from a Google Code Jam competition. Data also contains the performance of the solutions. Authors show improvement of their VAE model vs sequence-to-sequence baseline model.",
            "strength_and_weaknesses": "Strengths:\n\n- Improved hint/change generation results by using proposed VQ-VAE instead of Transformer model\n- In experiments the VQ-VAE performs better on hard efficiency metric that may indicate transformations that are correct and more efficient\n- Ablation study of Edit VQ-VAE vs VQ-VAE and Edit-VAE\n\nWeaknesses:\n\n- Does not compare proposed VQ-VAE to results achievable from large language models (LLMs)\n- Due to canonicalization of the code, authors cannot check if provided hints generate a correct and efficient program. Instead they use similarity metric to determine if program with changes/hints is correct and more efficient. This shortcut may be problematic if changes/hints only seem to be correct and more efficient, but in practice are not. This needs to be solved for more accurate evaluation in the future.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written.\nIn my opinion, there is some novelty in the VQ-VAE approach for improving efficiency of programs that authors propose.\nThe paper should be reproducible using the available dataset and description of methods in the paper.\n\n------------------------------------------\nICLR review form does not provide a section for comments/questions about the paper. I am going to post them here.\n- \"We find that this canonicalization approach enables the model to learn on this dataset, but also means that we lose the ability to execute the edited program to evaluate run-time\" - this rather compromises the correctness and efficiency evaluation. Are there ways to convert hints/changes back into the precannonicalized program form?\n- \"To quantify efficiency, we rely on textual similarity to other known correct code in the data-set that is more efficient.\" - for exact match, this should work well although it still has a problem of cannonicalized hints; for inexact match via dBLUE, this might miss performance regressions.\n- It would be great if you tried using LMMs with a hint input with or without adaptation.\n",
            "summary_of_the_review": "I think that the paper introduces interesting and well performing approach using VQ-VAE to provide hints/changes to improve efficiency of programs.\nI think that some additional work like comparing the approach to using LMMs would improve this paper.\nIdeally, it would be interesting if hints/changes could be translated back to pre-cannonicalized program form and programs evaluated there. Alternatively it would be good if authors did a user study and evaluation of providing the hints/changes.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5222/Reviewer_SDKK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5222/Reviewer_SDKK"
        ]
    }
]