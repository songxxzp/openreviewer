[
    {
        "id": "xChc4cj8Gae",
        "original": null,
        "number": 1,
        "cdate": 1666529551069,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529551069,
        "tmdate": 1666529551069,
        "tddate": null,
        "forum": "Rg1LG7wtd2D",
        "replyto": "Rg1LG7wtd2D",
        "invitation": "ICLR.cc/2023/Conference/Paper5775/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper discovers that there may exist heteroskedasticity in realistic offline RL datasets, making current offline RL methods with distributional constraints suffer from performance degeneration. To address the problem, the paper proposes a novel method ReDS to convert distributional constraints into support-based constraints. Several new heteroskedastic datasets are constructed to demonstrate the superiority ReDS over current offline RL methods.",
            "strength_and_weaknesses": "The paper is well-written, theoretically motivated, and technically sound. I like Figure 1 which clearly illustrates the motivation of ReDS. However, I have two concerns.  \n\nFirst, how well does ReDS perform over non-heteroskedastic datasets? \n\nSecond, can ReDS be applied to offline datasets with a small number of samples? Do insufficient samples impose difficulty in estimating the distribution?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly presented, and novel.",
            "summary_of_the_review": "I recommend acceptance",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5775/Reviewer_HyhP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5775/Reviewer_HyhP"
        ]
    },
    {
        "id": "qpCjIHJ6irY",
        "original": null,
        "number": 2,
        "cdate": 1666690839878,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690839878,
        "tmdate": 1668746281130,
        "tddate": null,
        "forum": "Rg1LG7wtd2D",
        "replyto": "Rg1LG7wtd2D",
        "invitation": "ICLR.cc/2023/Conference/Paper5775/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an algorithm called CQL (ReDS), which uses reweighted data distribution to make the algorithm act like having a support constraint instead of a distribution constraint. For most of the complex offline RL problems, dataset distribution will be heteroskedastic, i.e. will contain different variances of actions in each state, and having a distribution constraint, in this case, can lead to a limited performance because we can always be overly pessimistic or optimistic for certain states. To prevent this, this paper show a simple way to achieve a support constraint instead with a small modification from original CQL algorithm, by using a mixed distribution $\\pi^{re}$ instead of $\\pi$ used to regularize in CQL.\n\n$\\pi^{re}$ is half $\\pi$ and half $\\rho$, where $\\rho$ is some distribution of actions that is high when $\\pi_\\beta$ is high and $\\pi$ is low, and $\\rho$ will be low when $\\pi_beta$ is low and $\\pi$ is high. By having this additional $\\rho$ term, policy $\\pi$ of CQL(ReDS) can now be dramatically different from $\\pi$. Experiment results show that by having this modified regularization the algorithm improves much.",
            "strength_and_weaknesses": "Strengths\n- High-quality descriptions with good examples\n- Extensive evaluations over many different domains\n\nWeaknesses\n- Theoretical results are hard to follow and somewhat misleading:\n\n1. Theorem 3.1 is especially hard to understand. While the policy improvement is shown with $\\zeta^+$, the constraint is over $\\zeta$, which makes it confusing because $\\zeta$ and $\\zeta^+$ do not seem to be related to each other in my opinion. And there is nothing about $\\delta$ even though the theorem holds under prob. $1-\\delta$. What is the variable that is constrained by the constraint? Is it $\\alpha$ that is determined by the constraint? If it is the case, $J(\\pi_\\alpha)$ is determined by $\\alpha$, which should not be used outside the maximization of eq (5). Overall, it is really hard to interpret the theorem. \n\n2. Why do we need to use $g(\\exp(A(s,a)))$ instead of $g(\\pi(a|s)$? Can't we directly use $g(\\pi(a|s))$ to weight the objective of eq (9)? It seems there isn't anything blocking us from doing such.\n\n3. Lemma 4.2 seems to be quite misleading. Authors argue that we can put the notion of support with $\\epsilon$ in the second term, but doesn't $\\pi_\\beta$ cancels out in the second term due to the definition of $\\rho$? And the argument authors made seems to be also able to be applied on CQL case, since we can say that CQL regularization can be arbitrarily high when $\\pi$ is high and $\\pi_\\beta$ is low. If it doesn't make CQL a support constraining algorithm, I think (ReDS) cannot be said to be a support constraining algorithm.\n\n- Minor point: it seems gradient signs $\\nabla_\\psi, \\nabla_\\phi$ are omitted in Algorithm 1.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation quality seems to be very high, including a number of explanations with clear examples. However, technically, I do not agree with some of the ideas used to propose the algorithm. Theoretical results seem to be somewhat misleading. The experiment section is great. The algorithm is somewhat original, but it seems there are lots of ongoing researches that try to weight CQL regularization differently in each state. ",
            "summary_of_the_review": "Overall, I think the presentation quality is sufficient for being accepted. However, there are some points about the theory that is not clear to me, and I cannot recommend acceptance in its current form. I am ready to raise my score if those concerns are resolved.\n\n----------\nReading the authors' response, I decided to raise the score. Thanks for the detailed response!",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5775/Reviewer_d9Ww"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5775/Reviewer_d9Ww"
        ]
    },
    {
        "id": "PaSAj9x8At",
        "original": null,
        "number": 3,
        "cdate": 1666700809243,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700809243,
        "tmdate": 1666700809243,
        "tddate": null,
        "forum": "Rg1LG7wtd2D",
        "replyto": "Rg1LG7wtd2D",
        "invitation": "ICLR.cc/2023/Conference/Paper5775/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers a novel setting, i.e., learning policy from a heteroskedastic dataset with the demonstrated behaviors varies non-uniformly across states. For example, for a given maze environment and some non-optimal behavior policies, the states in the narrow hallways may exhibit a narrow action distribution, while more uniform actions for a wide room. In my view, this is true. In many cases, we cannot obtain the optimal policy with narrow action distribution for every state (if we can, we don\u2019t need offline RL.). And suboptimal policies prefer to show different actions for a given state.\n\nThe authors propose empirical and theoretical evidence (Theorem 3.1) showing that explicit distributional constraints between the policy action and the (non-optimal) dataset actions may undermine the performance of the learned policy . \n\nThe authors try to achieve a support-based policy constraint to improve over the non-optimal behavior policies that generated the heteroskedastic datasets. That is, if the policy decision is within the support of the behavior policy, then the constraint should be zero. This constraint is weak than the distributional constraint. It thus holds the promise to derive a better policy on the states with more uniform action (non-expert) distributions. \n\nTo practically achieve the support constraint, the authors reweigh the behavioral policy into \\pi_{re} and implicitly minimize D(\\pi, \\pi_{re}, where\n$\\pi_{re}(\\cdot |s) = \\frac{1}{2}\\pi(\\cdot \\pi) + \\frac{1}{2}[\\pi_{beta}(\\cdot |s) \\cdot g(\\exp(A(s, \\cdot)))]$.\n\nThis work proposes an offline RL algorithm with support-based policy constraints. Although the evaluation is only based on the heteroskedastic dataset, I believe this pradgigm enjoys priority over the distributional constraint methods, such as the recent strong baseline algorithm, TD3+BC. \n",
            "strength_and_weaknesses": "## Pros\n- Achieving support-based constraints is well-motivated and elegant. In fact, there exist support-based constraint methods, e.g., BEAR. But MMD constraint can not be accomplished in practice. Thus it performs poorly on the medium-expert dataset (with a wider action distribution from the medium-level policy). This paper implicitly achieves support-based constraints without estimating a behavior policy, which is elegant. \n\n- The evaluation is conducted on four different types of datasets, including (1) D4RL Mujoco locomotion (*-medium-expert and *-medium-replay) datasets, (2) noisy/biased D4RL maze datasets, (3)the manipulation tasks, and (4) Atari games. The baseline algorithms, in my view, are also comprehensive.\n\n## Concerns\n- (Setting) The author says that the dataset for offline RL algorithms must illustrate the consequences of a diverse range of behaviors. As a result of this coverage, e.g., combining many realistic sources of data, algorithms should be able to learn from heteroskedastic datasets. But (1) I don't understand the logic of the argument. If such coverage benefits offline RL algorithms, why do we need to modify the existing offline RL algorithms for the dataset with good coverage? (2) Another concern is that recent work [1] shows that vanilla off-policy RL agents can outperform carefully designed of\ufb02ine RL algorithms with suf\ufb01ciently diverse exploratory data. Could you please compare the heteroskedastic data and the diverse exploratory data? (3) Also, I am concerned about the motivation of the setting. The author clarifies the characteristics of heteroskedastic datasets, i.e., some dataset states correspond to narrow actions while others may be wider. But I can't easily see how this dataset relates to real-world applications.\n\n- (Algorithm) The authors build their proposed algorithm on top of CQL, with improvements focused on implicit support based constraint. The modification on the CQL regular term is described in Eq. (8), which involves sampling over the re-weighted policy, \\pi^{re}. I understand the idea that applying a distributional constraint on this re-weighting imposes a support based constraint. But I am not sure why the reweighted policy is deigned as a mixture of the learned policy \\pi and the \\rho, as described in Eq. 7. Could you please explain why the coefficient is designed as 0.5 and 0.5, or why not only use a \\rho? By the way, the sampling strategy of Eq. 9 and the third line of the algorithm seem to be inconsistent.\n\n- (Evaluation) The authors completed tests on three different datasets. Evaluations on the D4RL medium-expert and medium-replay datasets are really motivated, as these types of behavior policies can be common. But I think the dataset used in the second experiment, i.e., the noisy and biased D4RL antmaze dataset, is a bit contrived. \n\n- (Minor)It could be better if the authors add a discussion about the scope of the method. The problem considered in this paper can be well summarized in Figure 3: when the actions corresponding to the dataset states contain both expert and non-expert decisions, the support-based constraint method should be used. I think this is quite true, as actions from experts and non-experts can be different. However, when only non-expert dataset actions are available for some dataset states, how does the support-based constraint approach work? This can be true, as non-expert actions often result in a transfer to a non-expert state that differs from the state visited by expert behavior policy. Finally, I want to say that I don't think this discussion detracts from the contribution of this paper. It is entirely up to the authors to decide whether or not to give such a discussion.\n\n## Some typos.   \n- Page3, the first line, \\mu should be replaced with \\pi.\n- Page6, under equation 7, Eq.3 should be Eq.2.\n- Page6, under equation 11, Eq.30 should be Eq.11.\n- Page7, \u2018constrain the entropy of \u2026 to be uniform\u2019.\n\n\n[1] Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning. https://arxiv.org/abs/2201.13425",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "See above",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5775/Reviewer_N7S6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5775/Reviewer_N7S6"
        ]
    }
]