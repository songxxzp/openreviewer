[
    {
        "id": "DqZJlJySiQ",
        "original": null,
        "number": 1,
        "cdate": 1666623855640,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623855640,
        "tmdate": 1666623855640,
        "tddate": null,
        "forum": "jBEXnEMdNOL",
        "replyto": "jBEXnEMdNOL",
        "invitation": "ICLR.cc/2023/Conference/Paper3524/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of domain generalization with both fairness and accuracy guarantees. To this end, the authors establish an upper bound for accuracy (resp. fairness) for general domain generalization algorithms, which in turn provides guidance on designing their proposed algorithm. A complementary lower bound is also provided in the paper, explaining why previous representation learning methods may not be good choices. The proposed method is tested on real data, showing substantial improvement over previous practices. ",
            "strength_and_weaknesses": "In general, this paper is well-written. The presentation is clear, references are well-cited and motivation is sufficiently discussed. The upper and lower bounds on accuracy and fairness transferability are interesting because they serve as good guidance for algorithm design. Below I have some comments:\n1. Section 2 is a bit dense with too many notations and concepts, which may be hard for readers not super familiar with the terminologies. It would be better if some of the definitions/terminologies can be introduced within a context.\n2. On page 3, it is clear what $\\mathcal{H}$ and $\\mathcal{H}\\Delta \\mathcal{H}$ are (they seem to be defined in the Appendix). Maybe add a reference here.\n3. On page 4, the footnote mark 2 and 3 should be put after punctuations.\n4. Above Theorem 2, \"bellow\" -> \"below\"\n5. On page 5, maybe add a sentence or two to explain the claim \"Note that these results are consistent with Thm. 1 and Thm. 3 above.\"\n6. I assume a similar lower bound as Thm 2 can be established for EO as well?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. It can be improved, however, if some of the notations and concepts can be introduced within a context. The theoretical results are interesting because they provide guidance on designing better algorithms. ",
            "summary_of_the_review": "In general, this is a well-written paper, with a clear presentation and sufficient discussion on previous works. The theory presented here is novel and interesting; the advantage of the proposed algorithm is validated by numerical experiments.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3524/Reviewer_dQPF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3524/Reviewer_dQPF"
        ]
    },
    {
        "id": "cB4Xs5R_ZV",
        "original": null,
        "number": 2,
        "cdate": 1666665488948,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665488948,
        "tmdate": 1668880119922,
        "tddate": null,
        "forum": "jBEXnEMdNOL",
        "replyto": "jBEXnEMdNOL",
        "invitation": "ICLR.cc/2023/Conference/Paper3524/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the representation learning problem in domain generalization setting with fairness constraints (DP, EO). The paper presents upper and lower bounds for accuracy, and also an upper bound for fairness violation. Empirically, the paper aims to solve a minmax problem to account for fairness when learning invariant representations.",
            "strength_and_weaknesses": "## Strength\n\nThe paper takes effort to make theoretical results mathematically rigorous and provides multiple bounds w.r.t. accuracy and fairness. An empirical two-stage algorithm is also presented for the purpose of learning invariant representation.\n\n## Weakness\n\n### 1. w.r.t. dense notations and clarity of presentation\n\nOverall, while I appreciate the effort of presenting rigorous arguments, I find the paper hard to follow from time to time because of the dense notations and (occasionally unnecessarily) complex presentation. For example, in Section 4, the overall optimizing objective, the description of the proposed two-stage algorithm, the calculation of density of learnt representation are not presented in a clearly organized way. Personally, I would benefit a lot from, e.g., the numbering of the key equation (e.g., the objective), the implication and/or importance of the theoretical results, and the correspondence between the specific step in algorithm to previous theoretical results. As another example, the sufficient conditions provided in Theorem 4 seem to be rather straightforward, I think a concise summary would also serve the purpose without sacrificing mathematical rigor.\n\n### 2. the relation between the proposed FATDM algorithm and previous theoretical results\n\nWhile I appreciate the mathematical rigor when deriving upper-/lower- bounds for accuracy and upper bound for fairness. I am having some difficulty drawing connection between the theoretical results and the empirical two-stage algorithm. Does the theoretical characterization provides the ground for framing the problem in terms of invariant representation learning? Or, does the theoretical analysis motivate the design of algorithm? After all, as authors already noted, in the domain generalization setting, we do not have access to target domain, in this case, I am wondering if authors can share some insight regarding how to parse the theoretical results in the evaluation of empirical algorithm.\n\n## Additional comment\n\n### w.r.t. the theoretical analysis on Fairness transferability\n\nThe paper discusses previous approaches in extensive detail, which is clear and informative. Only for the purpose of completeness, I would like to provide a pointer to a previous work that also provided fairness transferability bounds for DP and EO notions: \"Fairness Transferability Subject to Bounded Distribution Shift\" by Chen et al., (2022). I am aware of the fact that the paper is recent (arxiv June 2022, to appear in NeurIPS), but considering that they are considering the same fairness notions in a very similar problem setting -- fairness bounds for DP and EO under domain shift, I want to bring their paper to the attention of authors. I am just a little bit hesitant to agree with the claim that the paper is \"the first work studying domain generalization with fairness consideration\". In fact, their bounds are more general, e.g., they do not require $A$ and $Y$ to be binary for EO when deriving the fairness bound.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I found the paper not easy to follow. It would be greatly appreciated if contents can be organized to better illustrate the implication of results, especially in the part where notations are dense (e.g., Section 4).\n\nThe proposed algorithm is novel (to the best of my knowledge). The paper also specifies the technical details for reproducing the experimental results, as well as additional experiments in the appendix.",
            "summary_of_the_review": "As presented in the paper, the bounds for accuracy when there are domain shifts have been discussed extensively. Fairness bounds are actually also discussed in some recent literature (as I mentioned in \"Strength and Weakness\"). I think largely the contribution of the paper comes from the empirical invariant representation learning algorithm. Additional clarifications of the connection between theoretical and empirical results would be very helpful.\n\n=== Post-rebuttal ===\n\nThank authors for the additional discussions and clarifications. I have updated my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3524/Reviewer_PEwK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3524/Reviewer_PEwK"
        ]
    },
    {
        "id": "a1P1lW0TjO4",
        "original": null,
        "number": 3,
        "cdate": 1666889586871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666889586871,
        "tmdate": 1666889586871,
        "tddate": null,
        "forum": "jBEXnEMdNOL",
        "replyto": "jBEXnEMdNOL",
        "invitation": "ICLR.cc/2023/Conference/Paper3524/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to develop fair and accurate models in unseen environments. The use of multiple environments and invariant representation learning for training fair models is an interesting approach. The paper also provides a theoretical analysis on the efficiency of invariant representation learning in transferring fair and accurate models across domains while using multiple source domains. The empirical results support the theoretical claims. \n\n \n",
            "strength_and_weaknesses": "Strength And Weaknesses:\n[Strength]\n\nS1: The paper tries to demonstrate how to achieve fair and accurate predictions in unseen environments without relying on assumptions about a specific type of distribution shifts across domains, which is an important and practical issue.\n\nS2: The theoretical analyses in the paper give several nice insights. For example, I enjoy reading Sections 3 and 4.\n\nS3: The proposed algorithm is effective in specific empirical scenarios, as shown in their theoretical results.\n\n[Weakness]\n\nAdding discussion about how the tradeoff between accuracy and fairness is affected by the number of source domains would be helpful. A major issue in certain settings could be acquiring multiple domain data, in which case one is restricted to a single domain. It would be helpful to address this point. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and the theoretical results are interesting.",
            "summary_of_the_review": "The paper tries to solve the important issue of achieving fair and accurate predictions in unseen environments. As the paper gives several insights into their theoretical discussion, I vote for accepting this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3524/Reviewer_nacW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3524/Reviewer_nacW"
        ]
    }
]