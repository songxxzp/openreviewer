[
    {
        "id": "tLsCU_3Ojx",
        "original": null,
        "number": 1,
        "cdate": 1666482583297,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666482583297,
        "tmdate": 1670801148391,
        "tddate": null,
        "forum": "PUIqjT4rzq7",
        "replyto": "PUIqjT4rzq7",
        "invitation": "ICLR.cc/2023/Conference/Paper6016/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for improving the attribute binding and compositional ability of denoising diffusion probabilistic models for image generation. The authors utilize the fact  that the attribute-object relation pairs can be obtained as text spans for free from the parsing tree of the sentence used for conditioning image generation. Using this free source of information, the authors propose  a method to combine the structured representations of prompts with the diffusion guidance process. Specifically, using the text spans from the parsing tree, the method obtains a cross-attention output that is a weighted average of the attention map of the full prompt multiplied by the value tensor of the full prompt and the value tensors of individual noun-pairs obtained by the parsing tree. To evaluate the method, the authors create two datasets. The first dataset, Attribute Binding Contrast set (ABC-6K) evaluates attribute binding ability of the method, by creating prompts with common and uncommon colors binding to objects in the COCO dataset. The second dataset, Concept Conjunction 500 (CC-500), tests the model\u2019s ability to compose together objects that rarely appear together in reality. Pairwise comparisons in human evaluation study are used to compare the method to the baseline model (Stable diffusion) and to Composable diffusion for the CC-500 dataset. The proposed approach, StructureDiffusion, is preferred by users over Stable Diffusion on ABC-6K, while it performs worse on CC-500. However, StructureDiffusion outperforms Composable Diffusion on CC-500. FID scores on COCO are comparable to original Stable Diffusion. ",
            "strength_and_weaknesses": "Strengths: \n1. The proposed idea of using the free supervision from structure in the prompts is interesting and worth investigating. \n2. The problems of attribute-binding and composition in image generation are important and retain scope for improvement. \n3. The authors propose two well-crafted datasets for evaluating the attribute-binding and compositional ability of image generation models and perform a comprehensive comparison of the proposed approach with baseline using a human study.\n4. The paper is well written and organized, which makes it easy to follow and reproducible. \n\nWeaknesses/suggestions:\n1. It was not clear to me how human evaluators assess \u201cimage fidelity\u201d in table 1. Please clarify this in the rebuttal.   \n2. It was not clear to me why the authors claim that  \u201c[On CC-500] our method outperforms Stable Diffusion by around 5-8% in terms of both image-text alignment and image fidelity\u201d, when the reported numbers in Table 1 show that StructureDiffusion wins against Stable Diffusion (SD) only 31.8% of the time, while SD wins 38.9%. These numbers make me conclude that SD is actually superior to StructureDiffusion for compositionality. Please clarify this in the rebuttal.  \n3. If I am interpreting the results in Table 1 correctly, then  StructureDiffusion is 20% better than SD in alignment, but 8% worse in compositionality. These results are not discussed in detail in the paper, which could help us understand how the proposed approach is helping in these tasks. \n4. No qualitative experiments are performed to show how the weighed cross-attention maps affect image formation (see the Hertz et al. paper for examples, which is used as an inspiration for the proposed approach) during the diffusion process. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The proposed idea is original, but it is not clear to me how effective it is in solving the problems of attribute binding and especially compositionality. ",
            "summary_of_the_review": "I am on the fence about how effective the proposed approach is in solving the problems of attribute binding and especially compositionality. It is also not clear to me why and how the proposed approach is working in different cases. So currently, I consider the paper to be borderline, but I am happy to re-evaluate after the rebuttal stage if I get clarification on the issues raised above.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6016/Reviewer_mgR1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6016/Reviewer_mgR1"
        ]
    },
    {
        "id": "vWyXY5pRZXJ",
        "original": null,
        "number": 2,
        "cdate": 1666582186160,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582186160,
        "tmdate": 1666582186160,
        "tddate": null,
        "forum": "PUIqjT4rzq7",
        "replyto": "PUIqjT4rzq7",
        "invitation": "ICLR.cc/2023/Conference/Paper6016/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new diffusion-based T2I models to improve the compositional skills. The linguistic structures are incorporated with the diffusion guidance process based on the controllable properties of manipulating cross-attention layers in diffusion-based T2I models. The model is built based on the stable diffusion, and the proposed cross-attention design requires no additional training samples. The model can achieve SOTA compositional skills in qualitative and quantitative results.",
            "strength_and_weaknesses": "Strength:\nThis paper proposes to manipulate the cross-attention representations based on linguistic insights. And the compositional performance can be obviously improved.\n\nWeakness:\n\n1. As shown in Figs. 4 and 5, the proposed diffusion model's effects are mainly observed with the color attributes. The authors should provide the comparison with more types of attributes to demonstrate the improvement of compositional skills.\n\n2. Why the quantitative comparisons between Stable Diffusion and Composable Diffusion are not conducted with the metrics of IS, FID, and R-prec?\n\n3. The authors have claimed that their method can also take the scene graphs as the inputs. They should also compare the results with existing scene-graph to image approaches. And the visual cases in Fig. 6 have some errors. For example, there is not beach in the middle column of Fig. 6, while beach exists in the scene graph.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper is good, with clear originality.",
            "summary_of_the_review": "I think the proposed linguistic insights strategy is a useful trick for T2I diffusion models. However, the experimental results are not convincing enough.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "The manipulation technique can cause the problem of privacy.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6016/Reviewer_QzN2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6016/Reviewer_QzN2"
        ]
    },
    {
        "id": "7_A7vgqZc5",
        "original": null,
        "number": 3,
        "cdate": 1666665659447,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665659447,
        "tmdate": 1666665659447,
        "tddate": null,
        "forum": "PUIqjT4rzq7",
        "replyto": "PUIqjT4rzq7",
        "invitation": "ICLR.cc/2023/Conference/Paper6016/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a way to improve compositional text-to-image in large, pre-trained diffusion models without additional data or training requirements. To achieve this, language parsers are used to extract high-level information from a given caption (e.g. noun phrases) and use those to obtain additional text embeddings. These text embeddings are then used in combination with the original ones to scale the outputs of cross-attention layers. Experiments show that the approach can improve attribute binding and compositionality in generated images.",
            "strength_and_weaknesses": "Previous work has shown that the cross-attention layers have a lot of influence on the final content of generated images and, therefore, it makes sense to look into those more closely in this respect. The proposed approach does not need any additional training data or training of the model, making it easy to use in practice. The qualitative evaluation shows some improvement over the baseline model (Stable Diffusion) and human user studies indicate that the generated images are indeed better.\n\nSome questions and concerns:\n- I am not sure about the human user study; the authors say that they generate images from both approaches (their approach and the baseline Stable Diffusion model), then remove 20% of the most similar images, and only then run the user study; I understand that a lot of the images are similar since the same seed is used, but given that the performance of both approaches is probably similar on those 20% of the images this will affect the final results; in reality, the performance of the two models may be much closer to each other than the human user study indicates\n- Table 1, CC-500, comparison to SD seems that SD is actually better, but the text seems to indicate otherwise?!\n- Does it scale to longer captions with many constituents?\n- Also, looking at real-life use people seem to append a lot of words for specific styles, how would behavior like that affect the performance? Captions like *a photograph of an astronaut riding a horse portrait pixiv art soft lines fantasy anime smooth illustration murata range 8 k studio ghibli sharp focus wlop kyoto animation style art viewer anime art close sharp detailed pixiv kawaii concept art high quality cute sunset exile intricate hd elegant 4 k cute face artgerm madhouse stunning ufotable valley alphonse mucha detailed background anime portrait yoshinari yoh artgem detailed face japanese anime cinematic studio lighting pink hue white background thick lines large eyes*",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, although I believe the notation of the cross-attention descriptions can be improved (e.g. $l$ and $d$ in Fig 2 do not seem to align with equations) and described more intuitively.\nThe approach itself seems novel to me.",
            "summary_of_the_review": "Overall I think the approach is novel, may increase the performance in some specifc settings, and does not need any additional training or data. I'm not entirely sure about the evaluation with the human user study and how well the approach will scale to more complex/longer captions or the kinds of captions that users seem to use in reality.\nHowever, I see value in the approach and its simplicity and easy applicability are advantageous.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6016/Reviewer_ZtBV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6016/Reviewer_ZtBV"
        ]
    },
    {
        "id": "8Gs60p7Luv",
        "original": null,
        "number": 4,
        "cdate": 1666802126937,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666802126937,
        "tmdate": 1671173318853,
        "tddate": null,
        "forum": "PUIqjT4rzq7",
        "replyto": "PUIqjT4rzq7",
        "invitation": "ICLR.cc/2023/Conference/Paper6016/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a modification of stable diffusion to better reflect per-object attributes in a sentence to the corresponding objects in the image and prevent missing objects.\nFor example, feeding \"a red car and a white sheep\" into stable diffusion produces and image with a red car and a red sheep.\nFor experiments, the authors introduce a new dataset for evaluation.",
            "strength_and_weaknesses": "Strengths\n\n(+) The proposed method does not require training samples.\n\n(+) This paper introduces a benchmark dataset to evaluate attribute binding.\n\n(+) The experiments visualizes the causes of incorrect attribute binding (Figure 8).\n\n(+) Sec. 2.1 provides thorough background.\n\nWeaknesses\n\n(-) Missing analysis: How do the attention maps from NPs and sentence evolve throughout the timetsteps?\n\n(-) The contributions are limited.\n- The attributes are limited to colors.\n- The authors do not provide comparisons in the scene graph setting.\n- Evaluation of the compositionality is limited to user study.\n- Preventing objects from missing is not evaluated.\n",
            "clarity,_quality,_novelty_and_reproducibility": "(+) The human evaluation protocol is clearly described.\n\n(-) Abstract and intro\n- we observe that attribution-binding and compositional capabilities are still considered major challenging issues. <- The readers cannot guess what attribution-binding and compositional capability from abstract.\n- Abstract has peripheral aspects (e.g., cross-attention) instead of the key component (tokenizing sentences).\n\n(-) What is \"functionality between attention maps and token semantics\"?\n\n(-) t and t_i are confusing.\n\n(-) Eq. 4 maintains the image layout  <- What is the original image layout? Maybe the image layout from a full sentence embedding?\n\n(-) Assuming above guess is correct, why does Mt remain unchanged although the sentence embedding and the token embeddings are different?\n\n(-) c denotes hierarchies but NPs in a same hierarchy have different cs in Figure 3.\n\ntypo\n- downsample and upsampling blocks -> downsampling and upsampling blocks\n",
            "summary_of_the_review": "I value the colors being correct in the resulting images. However, contribution of this paper is limited as mentioned in weaknesses. I consider the bar for ICLR is much higher.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6016/Reviewer_5ZP1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6016/Reviewer_5ZP1"
        ]
    }
]