[
    {
        "id": "tMxYoRb2-YQ",
        "original": null,
        "number": 1,
        "cdate": 1666616333020,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616333020,
        "tmdate": 1666616333020,
        "tddate": null,
        "forum": "3lge0p5o-M-",
        "replyto": "3lge0p5o-M-",
        "invitation": "ICLR.cc/2023/Conference/Paper2464/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes DIFFEDIT, a new method for image editing based on diffusion models. The main contribution is a method to automatically discover the areas that need to be edited by contrasting the predictions conditioned on different texts. Besides, the paper shows that using the latent inference capability of DDIM achieves more consistent editing. Experiments on ImageNet, COCO, and an Imagen generated dataset demonstrate that DIFFEDIT achieves better image editing than previous baselines.",
            "strength_and_weaknesses": "Strength:\n\n- The paper proposes a new method to automatically estimate the mask that needs to be edited, which reduces users' effort. The method of contrasting the predictions conditioned on different texts looks reasonable to me.\n- DIFFEDIT proposes to use the reverse DDIM step to encode the image, which is shown to lead to more consistent image editing results.\n- Combining the proposed two techniques, the method achieves better image editing than previous methods both qualitatively and quantitatively on several datasets.\n\nWeaknesses:\n\n- For the COCO dataset, only qualitative results of the proposed method are shown. It is recommended to add a qualitative comparison with other baselines in the Appendix.\n- For the current method, the predicted mask is much more reasonable when a reference text is provided. However, in the application of real image editing, the reference text is often not available, so a minor issue is that the author needs to provide both query text and reference text for the method to perform better. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and well organised. Given the description I think it is reproducible. While the current description in Sec 3.2 Step 1 is acceptable, I would prefer to use an equation to describe how the mask is obtained. Besides, the meaning of Dr in Eq(4)(5) is not explained. I think it\u2019s the DDIM decoding process, but it should be mentioned.\nThe method is simple but is new. In general, the quality of the paper looks good to me.",
            "summary_of_the_review": "While I feel the proposed method is pretty simple, it is reasonable and effective in producing region-specific and consistent image editing as proved in experiments. Object specific image editing is an important problem and I think this work would be quite useful. A minor weakness in qualitative evaluation could be further improved. To summarise, I tend to accept this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2464/Reviewer_hxgu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2464/Reviewer_hxgu"
        ]
    },
    {
        "id": "i0rXmievZl",
        "original": null,
        "number": 2,
        "cdate": 1666722127760,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666722127760,
        "tmdate": 1666722127760,
        "tddate": null,
        "forum": "3lge0p5o-M-",
        "replyto": "3lge0p5o-M-",
        "invitation": "ICLR.cc/2023/Conference/Paper2464/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a 3 steps image editing pipeline that 1) generate segmentation mask from the change in text query, 2) encode the image with diffusion process until a time step \u2018r\u2019, and 3) decode it back to the image via reverse diffusion process condition on the text query, and with mask as the guidance. They show results on ImageNet and coco dataset, as well as image generated from imagen. ",
            "strength_and_weaknesses": "Strength\n - theoretical analysis is nice to have\n - the generated mask seems reasonable\n - the method is simple. Auto generate the mask from 2 version of slightly different texts then encode/decode to the appropriate image.\n - experiments are on (relatively) large scale datasets \n\nWeakness\n - The use case is not quite clear to me. The mask generation from the different in text query and reference text means that you need to have both. So from what I understand, this is only applicable for image editing on an existing text-to-image pipeline? (Also Isn\u2019t that more like giving more control over image generation than an image editing?)\n - I think the simplicity of the method is both the strength and the weakness. It is good that it is simple, so more people can use it. But it also mean there isn\u2019t much insight/impact gain from reading the paper.\n- From the result figure, it is not an obvious improvement to me among each baselines. Using mask does help with keeping the background the same as the original query, but that seems to be the only benefit of the proposed method. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to understand, \n\nThe reproducibility should be possible given the simplicity of the method",
            "summary_of_the_review": "Unfortunately, I don\u2019t think I currently see enough contributions to recommend acceptance. Given the weakness I mentioned, I think it is slightly below the bar for acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2464/Reviewer_zQQb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2464/Reviewer_zQQb"
        ]
    },
    {
        "id": "7qsHItervpR",
        "original": null,
        "number": 3,
        "cdate": 1666836357824,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666836357824,
        "tmdate": 1666836357824,
        "tddate": null,
        "forum": "3lge0p5o-M-",
        "replyto": "3lge0p5o-M-",
        "invitation": "ICLR.cc/2023/Conference/Paper2464/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents a method for performing text-based semantic image editing. The key ideas are \n1) identifying where to edit by comparing the image differences between query text-guided and reference text-guided (or unguided) image generation and\n2) use DDIM encoding for preserving the contents that are outside the generated masks. \n\nFor 1), compared to the existing semantic editing method, the proposed mask generation automates the step of manually specifying the mask for editing. \nFor 2), the DDIM encoding preserves the background content more faithfully when compared with current practices (that add noise to the original image as in SDEdit).\n\nThe method can achieve semantic editing while maintaining the background contents in the original image. \nThe paper evaluates the method in three settings, ImageNet, images generated from Imagen, and COCO. The experimental results validate the capability of the proposed method for image editing.",
            "strength_and_weaknesses": "Strength:\n+ The paper is very well-written. The exposition is clear. The figures are informative. \n+ The proposed approach is technically sound. \n+ The evaluation of three datasets (ImageNet, images generated from Imagen, and COCO) is solid and convincing. Both the quantitative and qualitative results demonstrate the effectiveness of the method.\n+ The ethics statement and potential misuse of the techniques are thoroughly discussed.\n\nWeakness:\n- The concepts of predicting spatial masks for semantic image editing have been extensively explored in other image-to-image translation literature. Examples include:\n[CVPR 2018] Da-gan: Instance-level image translation by deep attention generative adversarial networks\n[NeurIPS 2018] Unsupervised Attention-guided Image-to-Image Translation\n[ICLR 2019] InstaGAN: Instance-aware Image-to-Image Translation\nThe difference here is that the generative models at the time were GANs instead of diffusion models.\n\n- Another core difference is the use of DDIM encoding as opposed to directly adding noises in the original image. DiffusionCLIP [CVPR 2021] also uses the DDIM step to encode the input image. The key difference is that the proposed method relies on a pre-trained text-2-image diffusion model as opposed to CLIP used in DiffusionCLIP. This allows us to avoid the costly model fine-tuning.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: I think the paper is very clear.\n\nQuality: The results are of high quality. The extensive evaluation shows the state-of-the-art performance when compared to many recent baselines.\n\nNovelty: As discussed in the weakness section, I think the weakest part of the paper is that both spatial masking for editing and DDIM encoding are not intrinsically novel. \n\nReproducibility: The paper builds on publicly available models and data. The paper also includes sufficient implementation details. I believe that reproducibility would not be an issue. ",
            "summary_of_the_review": "The paper combines the ideas of spatial masking and DDIM encoding to improve the quality of semantic image editing. None of the techniques are new, but it appears to be an interesting combination in this application context. The paper shows strong evaluation results on three datasets and reports convincing quantitative and visual results. I think the community would benefit from this simple approach for semantic image editing. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "Image editing applications naturally have potential harmful use (e.g., creating fake news). The paper discussed in the ethics statement that they will release the code under a license similar to the Stable Diffusion.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2464/Reviewer_ZwKd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2464/Reviewer_ZwKd"
        ]
    },
    {
        "id": "_mSgItV9dpP",
        "original": null,
        "number": 4,
        "cdate": 1666973728135,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666973728135,
        "tmdate": 1666973728135,
        "tddate": null,
        "forum": "3lge0p5o-M-",
        "replyto": "3lge0p5o-M-",
        "invitation": "ICLR.cc/2023/Conference/Paper2464/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel method for text based image editing, that addresses the shortcomings from previous methods:\n- either a mask had to be provided by the user\n- or the edits modified the background\n\nIn this paper the user does not have to provide a mask and the background will not change either. They achieve that by estimating the mask instead of requiring the user to provide it.\n",
            "strength_and_weaknesses": "Strengths:\n\nThe process is extremely simple: \n- step1: estimate the mask (that a user otherwise would provide)\n- step2: run DDIM encoding\n- step3: run DDIM decoding conditioned on query text, and keep pixel values outside the mask\n\nThe key idea is very intuitive:\n\"When the denoising an image, a text-conditioned diffusion model will yield different noise estimates given different text conditionings. We can consider where the estimates are different, which gives information about what image regions are concerned by the change in conditioning text.\"\n\nThe paper is very well written.\n\nThe solution is supported by theoretical analysis, where they show bounds on how far the edited images are.\n\nAblations show the main effect on design choices (fig 5, fig 6) and quantitativelly on the main hyperparameters (fig 6 binarisation threshold and fig 4 encoding ratio).\n\nThe method is extensively evaluated on different datasets and against other methods, and it beats the SOTA (fig 4)\n\nWeaknesses:\n\nI did not find any",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nVery well written. Just by looking ant fig 2 the reader understands what is happening inside a minute.\n\nQuality:\n\nVery high.\n\nNovelty:\n\nNovel to me.\n\nReproducibility:\n\nJust by reading the paper one can easily reproduce the method, provided one has the access to the trained models.\n",
            "summary_of_the_review": "The paper have strengths in many areas. Unless I missed something, it is a clear accept.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2464/Reviewer_WyT5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2464/Reviewer_WyT5"
        ]
    }
]