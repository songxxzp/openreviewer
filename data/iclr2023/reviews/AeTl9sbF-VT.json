[
    {
        "id": "YcwPcOFmFB",
        "original": null,
        "number": 1,
        "cdate": 1666534169601,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666534169601,
        "tmdate": 1666534169601,
        "tddate": null,
        "forum": "AeTl9sbF-VT",
        "replyto": "AeTl9sbF-VT",
        "invitation": "ICLR.cc/2023/Conference/Paper2606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers empirical attacks against models protected with randomized smoothing.\nThe paper proposes some simple optimizations and shows to find smaller adversarial examples than other attacks.",
            "strength_and_weaknesses": "- The paper is not particularly well motivated: the whole point of certified defenses is that we get a proof of robustness so optimizing attacks against these models is not necessarily super interesting (unless maybe the point is to check how tight the certification guarantees are).\n- The empirical comparison mainly considers prior attacks that were developed for deterministic defenses (with the exception of autoattack). So naturally such attacks will not perform well against randomized smoothing.\n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably well written. At times, the paper's choice of notation clashes with that commonly used in the literature which is a bit annoying (e.g., in the existing literature, \\epsilon *always* refers to the perturbation size, and not to the step-size)\nThe paper's novelty is the main limiting factor, and the scope is quite limited: the paper focuses on minimal-norm adversarial examples against randomized smoothing. There are many works on minimal-norm attacks. There are also many works on attacking randomized defenses. It isn't quite clear what new things this paper brings to the topic.",
            "summary_of_the_review": "My recommendation is mainly due to a perceived lack of novelty and motivation for the current paper.\nThe scope is very limited, and within this limited scope there are many alternative baselines that could be considered.\n\nFor example, all the considered attacks should be combined with some form of expectation-over-transformation (EOT) since the defense is randomized. Moreover, instead of using a gumble-softmax trick, it seems a lot simpler to just run a \"multi-targeted\" attack (https://arxiv.org/abs/1910.09338) that targets each possible class in term.\n\nUltimately, even if the evaluation were extended to better compare against these attacks, it remains unclear what to make of these results. Certified defenses are certified to be robust! And many existing works already show that these robustness guarantees are close to tight (i.e., if a randomized smoothing defense has a certified accuracy of say 50% for perturbations of size 1, then we already know empirically that the best attack with that perturbation budget will bring the defense's accuracy down to roughly 50%).\nThis paper instead considers minimal-norm attacks, but it isn't clear why this distinction matters. Certified defenses aim at maximizing robust accuracy at a fixed perturbation level \\epsilon. They don't aim to make any guarantees that successful perturbations will be any larger than \\epsilon.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2606/Reviewer_Ms23"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2606/Reviewer_Ms23"
        ]
    },
    {
        "id": "UOA3IlL7Aq",
        "original": null,
        "number": 2,
        "cdate": 1666659540428,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659540428,
        "tmdate": 1666659540428,
        "tddate": null,
        "forum": "AeTl9sbF-VT",
        "replyto": "AeTl9sbF-VT",
        "invitation": "ICLR.cc/2023/Conference/Paper2606/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper designs an adversarial attack method that acts on randomized smoothing based models, which in fact provide provable guarantee on adversarial robustness. Specifically, the paper approximates the non-differentiable operation of randomized smoothing with Gumbel-softmax, and a CW-like attack objective, and a heuristic on the attack step-size based on the sample-wise certified radius given by Cohen et al. (2019). Experimental results on MNIST, CIFAR-10 and Tiny-ImageNet show that the proposed method can find a smaller adversarial examples from a smoothed classifier compared to other empirical attack methods.",
            "strength_and_weaknesses": "**Strength**\n\n* The paper is easy-to-follow\n* An extensive experiment is performed to confirm the effectiveness\n\n\n**Weakness**\n\n* Overall, I feel I do not understand the motivation of the paper - In what practical scenarios that one can be interested in finding smallest adversarial examples for certifiably robust models, i.e., smoothed classifiers? Providing more examples on concrete applications of this technique would be really helpful to get readers more motivated. For example, could the adversarial examples found from this method transfer to other (possibly non-smoothed) models? Also, as another example, the paper could perform an in-depth analysis from the found adversarial examples to broaden understanding of smoothed classifiers.\n* The paper should discuss and compare the proposed method with SmoothAdv [Salman et al., 2019] which also propose an adversarial attack specialized for randomized smoothing. Also, could this method be used to improve SmoothAdv, i.e., to improve adversarial training of smoothed classifiers?\n* The paper lacks on ablation study on the proposed components.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The methodology is generally clearly presented. The technical novelty is questionable as it is in some sense a simple extension of SmoothAdv [Salman et al., 2019] in flavor of CW-attack. ",
            "summary_of_the_review": "I generally feel from the paper a lack of motivation for the proposed method. Specifically, for now I am not quite convinced on a demand for a stronger attack method against certifiably robust classifiers. In terms of evaluation, I think the paper should compare with SmoothAdv [Salman et al., 2019] to claim its technical novelty. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2606/Reviewer_YX18"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2606/Reviewer_YX18"
        ]
    },
    {
        "id": "DACWGau0YM_",
        "original": null,
        "number": 3,
        "cdate": 1666686539846,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686539846,
        "tmdate": 1666686539846,
        "tddate": null,
        "forum": "AeTl9sbF-VT",
        "replyto": "AeTl9sbF-VT",
        "invitation": "ICLR.cc/2023/Conference/Paper2606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to attack against models defended by randomized smoothing, and identify smaller adversarial perturbations for smoothed classifier than previous methods.\n",
            "strength_and_weaknesses": "\nStrength:\n1) Propose a novel method to attack randomized smoothing/\n2) Achieve  state-of-the-art performance compared with previous attack methods.\nWeakness:\n1) Not enough experiments to support the statements: \na. Only evaluate on randomized smoothing model of Cohen19. The performance of the attack on other randomized smoothing methods (such as SmoothAdv, MACER, Carlini22:https://arxiv.org/abs/2206.10550) is unclear.\nb. The setting of the /sigma in Table 1 is not aligned with Cohen19. Experiments under smaller /sigma like 0.12 or 0.25 are not shown.\nc. Experiments are limited to specific model architecture and datasets. Model architecture ResNet18 and Dataset Tiny-ImageNet are simple compared with Cohen19\u2019s.\n\n2) Ambiguous explanation of evaluating metrics of the experiments:\nDo not have an explicit explanation of the evaluation metrics in Table 1 like Success, Best, r_50, %-Cohen, which makes me confusing. \n\n3) Questions about AutoAttack\nAs I know, AutoAttack ensembles 4 attack methods which include APGD-CE (PGD with adaptive step size) and APGD-DLR (similar to target CW attack). Why does AutoAttack have a worse performance than PGD and CW? \n",
            "clarity,_quality,_novelty_and_reproducibility": "See above. ",
            "summary_of_the_review": "Novel task and attack method under certified robustness via randomized smoothing, but considering lack of experiments and detailed explanation, I could not recommend this paper for acceptance at this moment.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2606/Reviewer_rMga"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2606/Reviewer_rMga"
        ]
    },
    {
        "id": "GN_or8165gi",
        "original": null,
        "number": 4,
        "cdate": 1667323135619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667323135619,
        "tmdate": 1667323135619,
        "tddate": null,
        "forum": "AeTl9sbF-VT",
        "replyto": "AeTl9sbF-VT",
        "invitation": "ICLR.cc/2023/Conference/Paper2606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a certification-aware attack on randomized smoothing defense. To compute the gradient through non-differentiable randomized smoothing defense, they replaced arg max layers with the Gumbel-Softmax layer. Additionally, using the certification certificate, the perturbation step size can be automatically adjusted during the course of the attack, which is a novel idea for adversarial attacks. In the experiments, they compared their method with PGD, Carlini&Wagner, AutoAttack, and DeepFool attacks. Overall, the method improved the attack\u2019s efficacy against randomized smoothing defense.",
            "strength_and_weaknesses": "### Strengths\n\n- Proposed certification-aware adversarial attack, which improves the attack\u2019s accuracy in comparison with existing attacks.\n- A new way to adjust the perturbation step size based on the certification certificate.\n\n### Weaknesses\n\n- The paper considered only randomized smoothing and wasn\u2019t applied to any other adversarial training or certified defenses.\n- The comparison might be skewed in favor of the proposed method since the standard adversarial attacks are not aware of the randomized smoothing. Adding the expectation of transformation [1] and proper tuning of the attack\u2019s parameters might change the results of the comparisons. Based on the provided experimental details, it is not clear if the authors conducted necessary hyperparameter tuning for other attacks.\n\n[1] Athalye, A., Engstrom, L., Ilyas, A., & Kwok, K. (2018). Synthesizing Robust Adversarial Examples. In J. Dy, & A. Krause, Proceedings of the 35th International Conference on Machine Learning (pp. 284\u2013293). Stockholmsm\\\"assan, Stockholm Sweden: PMLR.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper uses very convoluted language and terminology at times, which makes it difficult to read and requires rereading some passages in the main paper. The authors could improve and simplify the presentation of the ideas. Overall the paper\u2019s idea is novel but the technical novelty is limited. To improve the novelty, the authors should include additional experiments with other certified defenses to show that the proposed method can be useful and applied to other types of defenses.",
            "summary_of_the_review": "The authors proposed a Certification Aware adversarial attack, however, the proposed attack was only tested against randomized smoothing defense. The authors should have tested the attack against other certified defenses and empirical defenses. Due to the limited technical novelty and lack of detailed experimental comparison, my suggestion for this paper is to reject it. Nevertheless, the idea is quite interesting and promising.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2606/Reviewer_5baj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2606/Reviewer_5baj"
        ]
    }
]