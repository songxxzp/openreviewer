[
    {
        "id": "DKB-Ty1zW1r",
        "original": null,
        "number": 1,
        "cdate": 1666425226927,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666425226927,
        "tmdate": 1666425226927,
        "tddate": null,
        "forum": "X4Jj-SmWX_i",
        "replyto": "X4Jj-SmWX_i",
        "invitation": "ICLR.cc/2023/Conference/Paper5275/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a multi-view strategy for subgraph representation learning. The multi-view strategy aims to achieve better performance by combining with arbitrary graph data augmentation methods and GNN models.  Experimental results are provided to demonstrate the effectiveness of the proposed strategy.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is clearly written, and the proposed framework is well articulated. \n2. The idea of this paper is straightforward and easy to understand.\n3. Experimental evaluation shows that the multi-view strategy works well when collaborating with two graph augmentation methods on 3 GNN models.\n\nWeaknesses:\n1. The technical contribution of the paper is limited.\nThe main idea behind the proposed multi-view strategy is to generate augmented samples by adding perturbations to the original (sub)graph, and to maximize the similarity between the representations of these (sub)graphs. However, similar ideas have been around in the literature of contrastive learning. Although most graph contrastive learning methods are designed for node- or graph-level tasks, they can be easily applied to subgraphs [1] [2].\n\n2. The application potential of the proposed method is limited. \nThe paper only adopts the proposed multi-view strategy to subgraph tasks, which could limit its application potential. A natural question is: can this multi-view strategy also improve node- and/or graph-level tasks (e.g., when combined with graph pooling methods [3] [4]).\n\n3. Some implementation details are missing, which could harm the reproducibility.\n   (1) For the pooling function Pool() in Eq. (4), which one was chosen in the experiments? Is there any option other than MaxPool and AvgPool? \n   (2) What is the design of the contrastive loss in Eq. (6)?\n\n4. Experimental study should be enhanced.\n   (1) The authors only conduct experiments with basic GNNs, such as GLASS, GCN and GraphSAGE. More SOTA GNN methods should be added to the comparison as well. \n   (2) No comparison with Graph Contrastive Learning methods was performed.\n   (3) The choice of the hyperparameter \u03bb should be further explored.\n\n5. Some typos.\nIn the caption of Figure 1, blue -> green, G\" -> G\u2019\u2019\n\n\n[1] Suresh S, Li P, Hao C, et al. Adversarial graph augmentation to improve graph contrastive learning. Advances in Neural Information Processing Systems, 2021, 34: 15920-15933.\n[2] Yin Y, Wang Q, Huang S, et al. Autogcl: Automated graph contrastive learning via learnable view generators. Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(8): 8892-8900.\n[3] Lee J, Lee I, Kang J. Self-attention graph pooling. International conference on machine learning. PMLR, 2019: 3734-3743.\n[4] Zhang Z, Bu J, Ester M, et al. Hierarchical graph pooling with structure learning. arXiv preprint arXiv:1911.05954, 2019.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, but its technical contribution is limited. The proposed multi-view strategy works better than basic GNNs but further experimental evaluations are needed.",
            "summary_of_the_review": "Major concerns are: (1) the technical contribution of the proposed strategy is limited; (2) Baseline models are weak. More comparisons are needed. (3) Some implementation details are missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5275/Reviewer_mpjc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5275/Reviewer_mpjc"
        ]
    },
    {
        "id": "Fge6QSE-WW",
        "original": null,
        "number": 2,
        "cdate": 1666686367216,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686367216,
        "tmdate": 1666686367216,
        "tddate": null,
        "forum": "X4Jj-SmWX_i",
        "replyto": "X4Jj-SmWX_i",
        "invitation": "ICLR.cc/2023/Conference/Paper5275/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed a multi-view approach applied to both the original and augmented subgraphs to learn subgraph embeddings/tasks. They used existing graph augmentation techniques, and then align the augmented new graph with the original one and extract subgraph embeddings using available subgraph GNNs. ",
            "strength_and_weaknesses": "The paper is easy to follow and well-presented, with a good set of well-conducted experiments and ablation studies. \n\n1) My biggest concern is the technical depth of this work. It would be helpful if the author would further characterize the augmentations and demonstrate to us how multiple-view + augmentation affects subgraph learning. And what exact benefits would we get? Under what conditions? \n2) Related to 1), it would be helpful if we can see how this method work on a larger set of various tasks. \n3) As for the performance, a question is how significantly augmentation helps. It looks like in most cases, the margin between the original and the augmented multiple views is small and likely not statistically significant. It seems to help more in some of the GCN cases. \n4) For computational time comparison in table 6, please add the original, i.e., w/o augmentation \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to read. Well executed and of good quality. The novelty is a bit low. ",
            "summary_of_the_review": "A well-executed, good quality paper, yet I would like to see a bit more depth and novelty. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5275/Reviewer_7uEX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5275/Reviewer_7uEX"
        ]
    },
    {
        "id": "11CzUN2M6W",
        "original": null,
        "number": 3,
        "cdate": 1666780536761,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666780536761,
        "tmdate": 1666780536761,
        "tddate": null,
        "forum": "X4Jj-SmWX_i",
        "replyto": "X4Jj-SmWX_i",
        "invitation": "ICLR.cc/2023/Conference/Paper5275/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes multi-view augmentation for learning representations on subgraphs. The proposed method improves the efficiency of existing standard graph augmentation method.",
            "strength_and_weaknesses": "**Strengths**\n\n(1) This paper greatly improves the efficiency of graph augmentation for subgraphs.\n\n**Weakness**\n\n(1) I think that the novelty of this paper is limited. To augment subgraphs, this paper uses existing graph augmentation methods such as DropEdge and GauG. Also, the contrastive loss of proposed method is from MV-GNN. Even though embedding augmented subgraphs into original graph is somewhat novel, I think it would be better to propose subgraph specific graph augmentation methods.\n\n(2) The proposed methods have marginal performance gain compared to baselines. For example On PPI-BP, GLASS w/ DropEDGE MV has 0.002 performance gain compared to GLASS w/ DropEDGE.",
            "clarity,_quality,_novelty_and_reproducibility": "**************Clarity**************\n\nThe paper is clearly written to understand overall method.\n\n**************Quality**************\n\nThe performance gain is marginal compared to existing methods.\n\n**********Novelty**********\n\nThe proposed method has a limited novelty. \n\n******************************Reproducibility******************************\n\nThe authors share their source code in the supplement. The paper has good reproducibility.",
            "summary_of_the_review": "Overall, I am leaning towards rejection. My major concern is the novelty and performance of proposed methods. If you address my concerns, I will raise my score. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5275/Reviewer_qUyv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5275/Reviewer_qUyv"
        ]
    }
]