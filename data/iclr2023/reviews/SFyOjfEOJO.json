[
    {
        "id": "d_0s-j-f78V",
        "original": null,
        "number": 1,
        "cdate": 1666624495031,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624495031,
        "tmdate": 1666624495031,
        "tddate": null,
        "forum": "SFyOjfEOJO",
        "replyto": "SFyOjfEOJO",
        "invitation": "ICLR.cc/2023/Conference/Paper207/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to combine symbolic representation and distributed representation together to improve image classification. The proposed model first uses a pre-trained fast RCNN to perform object detection on every example, and then automatically construct a rule set, in which every rule is a combination of the discovered objects in one example. Then, these rules are used for extract relational features from training examples, which will be fed into a seq2seq deep learning model. Combined with the CNN features, the final feature vector will be further processed by an MLP to make the final prediction.",
            "strength_and_weaknesses": "### Strengths:\n- This paper targets at solving an important problem, which is highly related to this conference.\n\n### Weakness:\n- The presentation of this paper can be significantly improved, there are many grammatical errors that need to be revised. The only one equation is not numbered and is unexplained, and many descriptions are confusing. Apparently, this work needs proofreading before submission and is not ready for publication now.\n- The authors claim that the proposed approach aims at bridging neural and symbolic models, however, the symbolic representation is converted into embeddings again. Since relational features are extracted by a pretrained fast RCNN, why not directly attach the RCNN to the final predictive MLP? What is the purpose of extracting combinations of objects as rules?\n- The hyper-parameters on rule confidence seem arbitrary.\n- The experiments are weak, no state-of-the-art scene classification models were compared.\n- There are some works trying to learn visual concepts using combinations of sub-concepts in a symbolic form, for example:\n  - Zhongyi Han, Le-Wen Cai, Wang-Zhou Dai, Yu-Xuan Huang, Benzheng Wei, Wei Wang, Yilong Yin, \u201cAbductive Subconcept Learning.\u201d Science China Information Sciences, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of this paper are poor, the novelty of the proposed approach is limited, and the reproducibility is weak because codes and many details about the experiments are unavailable.",
            "summary_of_the_review": "This paper in its current condition is not ready for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper207/Reviewer_vv79"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper207/Reviewer_vv79"
        ]
    },
    {
        "id": "PFXlpiz0KVJ",
        "original": null,
        "number": 2,
        "cdate": 1666625481111,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625481111,
        "tmdate": 1666625796018,
        "tddate": null,
        "forum": "SFyOjfEOJO",
        "replyto": "SFyOjfEOJO",
        "invitation": "ICLR.cc/2023/Conference/Paper207/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce a neuro-symbolic image classifier based on feature fusion.  The idea is to glue together (1) the image representation provided by embedding layers with (2) BERT and Word2Vec embeddings of (a textual representation of the body of Horn) rules that match objects detected in the image, and then (3) feed the resulting vector to a neural classifier.  The two embedding approaches are chosen so as to capture different types of information, namely structural and semantic/conceptual, intuitively complementing each other.  The proposed approach is compared against VGG16 and MobileNet V3 on a single indoor scene classification data set.",
            "strength_and_weaknesses": "PROS\n- Proposed method outperforms basic image models on the selected data set.\n- Ablation experiments.\n\nCONS\n- Text is readable, but contains a number of grammatical mistakes.\n- Method description not clearly distinguished from experimental details.\n- Model is not formalized properly.\n- Very limited empirical comparison on a single task against non-SOTA, non-NeSy baselines.\n- Related work is quite comprehensive but poorly structured.\n- Overall, the work generally feels incomplete and rushed.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The text is somewhat readable, but it is not well structured and contains a number of grammatical errors.  The text, in my opinion, is not yet of publication quality.  The mathematical formalization of the proposed approach is severely lacking (i.e., most aspects of the model are not formalized at all).\n\nI strongly recommend the authors to clearly separate between method description (in its own Section), where a proper formalization of all components of the method should be given (clearly describing each element - the object, the image embedder, the rule embedder, the classifier - formally and specifying the intended inputs and outputs), from data separation and experimental setup (in a different, later Section).\n\nI also suggest the authors to fix the citation style.  It is sufficient to replace \\cite{} or \\citet{} in the text with \\citep{}.\n\n**Quality** The idea of using word embeddings of rules as input to a neural predictor might have some merit - in that it propagates information about what objects have been detected in a scene and what predictions are more likely.  In principle, this is fine.\n\nThe main issue with the paper is the evaluation, which is carried out on a single data set against only basic baselines.  Considering that this is an image classification task, I would have expected, at the bare minimum, a comparison against state-of-the-art ViTs, not against VGG16 (published in 2015) and MobileNet V3 (definitely newer, but specifically designed to trade off performance for size).  Notice that these baselines are rule-agnostic, meaning that any nesy approach (which makes use of the extra information provided by the rules) is unfairly advantaged.  A better option would have been to compare against SOTA neuro-symbolic strategies.  A comparison against at least Logic Tensor Networks is definitely doable, considering that LTNs have been evaluated in the context of object-relation recognition.  The ablation experiments are a good addition, but they are not enough to make the empirical results compelling.\n\nAll in all, I think that the empirical evaluation needs to be improved substantially.\n\n**Novelty** There may exist early works on NeSy integration using feature fusion to take rules or concepts into account, but I cannot find the right references.  However, I think that the specific combination of object detection, text embeddings, and feature fusion proposed here is novel.\n\nThe related work section is quite exhaustive, in that it cites a number of relevant papers.  However, their relationship with the proposed approach is often not mentioned.  I urge the authors to streamline and clarify the related work, focusing on what approaches can and cannot considered direct competitors, and why.\n\n**Reproducibility**  Implementation details of the empirical evaluation (e.g., choice of hyperparameters) are sparse and insufficient for reproducing the experiments.  I could not find the source code in the supplementary material, and I don't think the authors promised to release the code upon acceptance.",
            "summary_of_the_review": "The intuition behind the method might have some potential, but the text is not up-to-par and the experiments are severely lacking.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper207/Reviewer_XLX8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper207/Reviewer_XLX8"
        ]
    },
    {
        "id": "POkNGrLw-tO",
        "original": null,
        "number": 3,
        "cdate": 1666740159782,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666740159782,
        "tmdate": 1669651746866,
        "tddate": null,
        "forum": "SFyOjfEOJO",
        "replyto": "SFyOjfEOJO",
        "invitation": "ICLR.cc/2023/Conference/Paper207/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a complex design to classify   Images: it   involves w2v, bert, user defined and bayesian features, that are sent to DNN,,\nResults look good., but on the author'\u015b data.\n",
            "strength_and_weaknesses": "Strengths\n it merges important ideas\nthe results are good.\nWeaknesses\u00a8:\nWhy? Often decisions are not clearly explained, eg Bert +/vs w2vec\nThe probabilistic rules deserve an example.\nThere is little info on how the system is configured\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: mostly OK.\n\nNovelty: weak\n\n{Quaity: avg\n\nthe paper seems to have been written in 17 (eg,Intro\n\nNovelty maybe the bayesuan attributes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClarity, mostly ok \n\nQuality, needs more arguments and more results\n\nNovelty mostky the use of probabilistic rules\n\n And Reproducibility Bad\n\n\n\n\n\n\n\n\n",
            "summary_of_the_review": "The paper needs to give us a more formal presentation and evaluation.\n\nMore in detail;the key idea of the paper consists in feeding high--level attributes to a DNN. This leads the authors to a 3 layer arch:\n\n1. The  first step is to run  a cnn and Fast-Run CNN to obtain high-level features from the images. This is followed by a filtering step. It is not clear if the parameters were chosen ad-hoc or if  there was some kind of search, It is also not clear whether the new features are fed to the DNN, or just used by step 2.\n\nThis is also not clear in Fig 2, which ssuggest the CNN runs in parallel with Fast CNN. I got a different impression from the text.\n\n2, These features may be used by:\n- probabilistic rules obtained by what seems like a variation of a Priori.It woould be useful to have more  details on how  you compute the parameters. Also, you may want to look into softening to avoid 100% probabilities.\n\n- User-written rules: not much detail is given, Table2 might help, f it was there ?\n\n3. you use w2vec, bert and a singke-layer perceptron> Why these choices?\n\nResults\u00a8\n\n- related systems: why these? How were they trained?\n\nThe english needs work, 3.4.2 is a particularly bad ex.\n\nFinally, i was hoping, but I failed;\n to understand why your model works\n to see other applications\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper207/Reviewer_jnEW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper207/Reviewer_jnEW"
        ]
    },
    {
        "id": "I8S3Hbw9Cbb",
        "original": null,
        "number": 4,
        "cdate": 1667654709926,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667654709926,
        "tmdate": 1667655231962,
        "tddate": null,
        "forum": "SFyOjfEOJO",
        "replyto": "SFyOjfEOJO",
        "invitation": "ICLR.cc/2023/Conference/Paper207/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A neuro-symbolic approach is proposed where a convolution neural network is extended with structured if-then symbolic rules based on word embeddings to improve image classification.",
            "strength_and_weaknesses": "**Strengths**: \n* **Motivation and general idea**: The paper does well in motivating the potential benefits of combining learned classification with crispier logical rules and the introduction is completing and setting the right content and necessary background. \n* **Results**: The proposed approach achieves improved results compared to the baseline.\n\n**Weaknesses**:\n* **Convoluted and heavy-engineered embedding approach**: The pipeline that combines object detection, BERT and word2vec makes the approach unnecessarily complicated and weakens the main idea of the paper, since it becomes unclear whether the empirical improvements in performance come from the approach itself or since we use multiple pretrained models that had access to further strong object-level and textual supervision. Would be good both to have more experiments to justify the contribution of each component, and look for ways to simplify the approach. The most obvious one is the use of both BERT and Word2Vec -- is that really necessary or maybe the reliance on two parallel approaches could be eliminated? Are there ways to maybe e.g. combine representations from different layers of BERT to avoid using also Word2Vec?\n* **Usage of heuristics**: For instance, in the object-detection section, the paper uses a heuristic to avoid duplicated objects by counting them. But faster R-CNN gives many potentially overlapping predictions with different degrees of confidence. Extracting an object count using such heuristic may not work effectively. Likewise, the arbitrary probabilities given to objects, selected by a person, are unjustified, and the need to use rules made by hand might greatly limit the robustness and applicability of the approach to different domains and distributions\n* **No use of standard datasets**: The paper uses a custom dataset rather than standard ones for a task where there could be plenty of public datasets that could be explored. This is a major weakness and this choice is unjustified in the paper either. The constructed dataset is also very small (733 images (440/146/146 for training/validation/testing)), weakening the statistical strength of the empirical results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The writing quality is ok but could be improved, but the paper is generally straightforward. Some sections of the paper could be reworked to become more focused. E.g. the 1-page approach overview in the modeling section, before diving into the architecture details, stands as some sort of a second introduction. It may be good to rework the presentation of this part together with the intro to reach the main ideas faster. The multiple diagrams and visualizations though are semantically clear and helpful to convey the ideas of the paper. Would be good to improve the resolution and some formatting mistakes in e.g. figure 3.\n\n**Novelty**: The high-level idea of augmenting convolution with logical rules is a simple and nice idea in my opinion, that is novel as far as I\u2019m aware (but not certain). From technical perspective, at the more concrete level the novelty of the paper is a bit limited. \n\n**Reproducibility**: The paper provides hyperparameter selection and describes each component of the approach. The use of non-public data reduces the reproducibility though. I didn\u2019t see a mention of releasing the constructed dataset either.\n\n**Minor Comments**:\n* **Spaces**: missing space on the second and fourth line of the introduction. \n* **Citations**: I believe the wrong format is used for some of the citations (citet vs citep). \n* **Subtitle**: on page 7 would be better to call the subsection title \u201cResults\u201d instead of \u201cResult\u201d.   \n",
            "summary_of_the_review": "The paper explores a nice and simple idea, but the approach is potentially too complicated and heavy-engineered and the actual technical novelty is quite limited. This is more of an application paper than one that studies core new research. The experiments are also limited both in terms of dataset explored and baselines compared to. I therefore at this point unfortunately recommend rejection but encourage the authors to keep working on the paper to improve the discussed aspects.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper207/Reviewer_fQgQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper207/Reviewer_fQgQ"
        ]
    }
]