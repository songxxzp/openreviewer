[
    {
        "id": "YwePGKPyk0w",
        "original": null,
        "number": 1,
        "cdate": 1666506408107,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666506408107,
        "tmdate": 1666844326418,
        "tddate": null,
        "forum": "zWwrB9wenY1U",
        "replyto": "zWwrB9wenY1U",
        "invitation": "ICLR.cc/2023/Conference/Paper902/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is written to introduce two notions of evaluation for model explanations: soundness and completeness. The definitions for these metrics are straightforward but require prior knowledge of the ground-truth important features. The authors therefore suggest practical alternatives, which appear to reduce to procedures very similar to existing ones: making predictions with different subsets of (un)important features and analyzing how the predictions change.",
            "strength_and_weaknesses": "--- Strengths ---\n\n- Several flaws about some existing evaluations are pointed out.\n\n--- Weaknesses ---\n\nAbout the method:\n- One of the key challenges with the method is that we don't have access to the ground-truth important features. The authors thus resort to approximate approaches, which seem geared primarily at comparing two attribution vectors rather than scoring one in isolation. Furthermore, the final procedure involves re-doing the prediction repeatedly with different amounts of (un)important features held out. Ultimately, this procedure is extremely similar to insertion/deletion (Petsiuk et al, 2018), but perhaps with a couple extra steps. The novelty thus seems quite limited.\n- The ideas behind soundness and completeness are very simple, there's no need to resort to vague notions of \"algorithm theory.\" As a reader it's a bit frustrating to see \"algorithm theory\" mentioned multiple times and then find out what the authors actually mean by it. You can just say what this method is: testing if the explanation identifies the important features but nothing more.\n- Describing other methods, the authors write: \"While prior works about attribution method evaluations mainly focus on explaining whether a method is correct or not, they ignore the fact that violation of either soundness or completeness can result in a partially correct algorithm.\" I don't get the point of this, it's not like other methods return a 0/1 value of whether the attribution is correct - they also return scores. What's the significant difference?\n- Theorem 3.3 is a simple result, perhaps it should be re-named as a proposition?\n- Mixing the ideas of set cardinality and summation within a set seems a bit sloppy. For example, in the proof for theorem 3.1, $|I \\setminus A| = |I \\setminus (A \\cap I)|$ is a valid equality if we're talking about set cardinality, but not if we're talking about summation. \n- When talking about the \"information level\", the notation $I_v$ defined as $I_v \\subseteq I$ s.t. $|I_v| = v$ doesn't make sense to me. There can be many subsets of that size, so which one is it? \n- In theorem 3.4, what is the relationship between $\\mathcal{A}^*$ and $\\mathcal{A}$? Should we have $\\mathcal{A}^* \\subseteq \\mathcal{A}$? If so, this should be specified.\n\nAbout discussions of prior work:\n- In describing prior work, the authors write that one issue is duplicate or conflicting definitions (e.g., \"sensitivity\"). I agree that it's an issue, but by the authors' account it seems like an issue with terminology rather than the metrics themselves, which is perhaps not as interesting.\n- In describing prior work, the authors frequently refer to \"order-based\" and \"model-retraining\" as the two groups of existing methods. This categorization of metrics doesn't make sense to me, they're overlapping categories and some methods belong to neither. For example, ROAR (Hooker et al, 2019) is both order-based and involves model retraining. Sensitivity-n (Ancona et al, 2018) is neither. This seems like a strange way to group existing evaluation methods.\n- The cost of model retraining is indeed a problem, and this is a complex issue because naive approaches like replacing with zeros are problematic in their own way (see Hooker et al, 2019). However, the authors don't actually \"solve\" this problem, they just impute the missing features. I'm not sure this can be described as a novel solution, as it's the same approach used by many other methods already (e.g., Petsiuk et al 2018, Ancona et al 2018).\n- The authors make a point of criticizing methods that are only sensitive to ordering, but what about the methods that aren't? For example, sensitivity-n (Ancona et al, 2018, already cited) and faithfulness [1]. These test the specific attribution scores rather than just their ordering.\n\n[1] Bhatt et al, \"Evaluating and aggregating feature-based model explanations\" (2021)",
            "clarity,_quality,_novelty_and_reproducibility": "The method has limited novelty, and the relationship with prior work could be improved.",
            "summary_of_the_review": "The authors propose two new criteria for evaluating model explanations. They are intended to address shortcomings in existing methods but re-use many of their key ideas. Thus, the novelty and significance are limited.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper902/Reviewer_rKgk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper902/Reviewer_rKgk"
        ]
    },
    {
        "id": "zTSZoyFljL",
        "original": null,
        "number": 2,
        "cdate": 1666517944085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666517944085,
        "tmdate": 1666517944085,
        "tddate": null,
        "forum": "zWwrB9wenY1U",
        "replyto": "zWwrB9wenY1U",
        "invitation": "ICLR.cc/2023/Conference/Paper902/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors propose two new metrics to evaluate feature attribution explanations: completeness and soundness which are based on algorithm theory. Authors show limitations of several existing order-based and model-retraining based metrics. The metrics are evaluated on several explainability methods such as GradCAM, DeepSHAP, IG, etc. for images. ",
            "strength_and_weaknesses": "+ \nOverall I think evaluating explanations is an important topic and new metrics/techniques to help evaluate explanations would be very useful to the community. \n\n-\nThe significance of these two metrics is not clear based from the submission. What is the risk of not computing these metrics?\nThere are new pieces of work based on computing ground truth: https://arxiv.org/pdf/2104.14403.pdf\nIt would be useful to show empirically if the proposed new metrics could become a good substitute for evaluating explanations without the need for ground truth, at least to some extent. \n\nIt would be good to show the results for tabular and text data. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- \nThe definitions of soundness and completeness are not somehow explained clearly in the submission and is confusing since the metrics depend on each other. It would be good to improve this. ",
            "summary_of_the_review": "See above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper902/Reviewer_7Hq4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper902/Reviewer_7Hq4"
        ]
    },
    {
        "id": "tDOOMcljf7F",
        "original": null,
        "number": 3,
        "cdate": 1666537638423,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666537638423,
        "tmdate": 1666537638423,
        "tddate": null,
        "forum": "zWwrB9wenY1U",
        "replyto": "zWwrB9wenY1U",
        "invitation": "ICLR.cc/2023/Conference/Paper902/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed two new metrics, completeness and soundness, for evaluating attribution maps. The two most important characteristics of these two metrics are that they do not require model retraining and are sensitive to the actual values of the attribution scores, not just their ordering. These two metrics are evaluated for many popular saliency maps, and several conclusions are drawn. ",
            "strength_and_weaknesses": "**Section 1 and 2**\n\nWhile I agree with the authors on their motivation to make evaluations that are not order-based or require model-retraining, I believe that there are some misrepresentations of the prior work, in particular (Hooker et al. 2019) and (Zhou et al. 2022) which I happen to be quite familiar with. \n\n> (Zhou et al. 2022) assumes that a retrained model only learns watermarks added into the original images of a semi-nature dataset\n\nFrom my understanding, they reassigned the labels to be only correlated with the injected features (e.g. watermarks). While this is kind of an extreme operation, this does guarantee that none of the other features are actually correlated with the label, and the model could not use any other features to achieve better than random chance performance. \n\n> Many evaluation metrics are only order-sensitive, meaning that they only evaluate whether one feature is more important than another while ignoring how differently the two features contribute to the output.\n\nThe attr% metric used by (Zhou et al. 2022) does depend on the actual value of the attribution. \n\n> We use the modified dataset for retraining and report the accuracy on the unperturbed test set to evaluate how many remaining features are learned during retraining.\n\nThis is in reference to (Hooker et al. 2019) paper on the discussion of retraining. Here, the authors used the perturbed data in training and unperturbed data in testing, creating a distribution shift, which is exactly what this paper argued to avoid, by introducing the retraining step (perturbing both training and test data). Thus, I fail to see what the results actually demonstrate, because if the model (retrained or not) is tested on a different distribution, its performance can be affected by many OOD issues. \n\n**Section 3**\n\nI have many questions on it. \n\nAssumption 1: $\\rho$ is defined as a performance metric (e.g. accuracy). However, there can be features that have high impact on model prediction, but not in a way that contributes to the model performance. For example, a feature could improve the performance on a subset of inputs and hurt that on another subset, balancing out the $\\rho$ metric. This is related to \"Model behavior\" in Fig. 1 of [1]. \n\nSec. 3.2.2: I don't quite understand the definition of $I_v$ and information level. Do we assume an ordering on $I$, such that $I_v$ is the top-$v$ important features in $I$, or something else? \n\nRelated to the point above, I guess throughout, we assume that the saliency map generates a binary split of features into $A$ and $F\\backslash A$. Most attributions are continuous. I think the authors used some thresholding, which may be related to the information level, but I don't find the details. \n\nThe authors mentioned that $I$ is unknown (i.e. the unknown ground-truth problem). However, reading through Sec. 3, I still don't understand how $I$ is in practice inferred or approximated. I would appreciate a high-level overview and roadmap at the beginning of this section before going into technical definitions and proofs. \n\nMinor: Def 3.1 and 3.2 can be written as $I\\subseteq A$ and $A\\subseteq I$. \n\n[1] https://arxiv.org/pdf/2011.14878.pdf\n\n**Section 4**\n\nThe experiment covers a wide range of explanation methods. However, at the end of the day, all conclusions are drawn based on the two proposed metrics. Their relationship with other metrics is not clear. In other words, since these two metrics are again *defined* (starting from Def. 3.1, 3.2 and Assumption 1), it is not clear how or why they are a better alternative to existing approaches, and what their limitations are. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: I do have quite some complaints on the writing, mainly in Sec. 3. As a result, I could not exactly follow the exposition of the idea. \n\nNovelty: many ideas in this paper have been seen in the literature, such as correlating model prediction/performance (e.g. $\\rho$) with feature importance, and using imputation to solve OOD/leakage issues. \n\nReproducibility: the attached code seems to be of high quality, so I believe that reproduction of the work would be easy. ",
            "summary_of_the_review": "Overall, I do not believe that this paper makes a sufficiently significant contribution to merit acceptance. However, I would also want to note that the non-clarity of writing (in my opinion) prevents me from giving a very confident assessment. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper902/Reviewer_iZ2R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper902/Reviewer_iZ2R"
        ]
    },
    {
        "id": "cxNnaot8wA5",
        "original": null,
        "number": 4,
        "cdate": 1666843532518,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666843532518,
        "tmdate": 1666843532518,
        "tddate": null,
        "forum": "zWwrB9wenY1U",
        "replyto": "zWwrB9wenY1U",
        "invitation": "ICLR.cc/2023/Conference/Paper902/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the author propose an evaluation method for feature attribution based on the concept of soundness and completeness. The two notions are described along with how they might be approximated when ground truth feature importance is unavailable. The results show the metrics to behave sensibly when saliency maps are manipulated in specific ways.",
            "strength_and_weaknesses": "The problem of evaluating saliency method is a pertinent one and several ways have been proposed in recent literature approaching from varying perspectives. The proposal in this paper is quite sensible, paralleling the well understood notion of precision and recall (in case of soundness and completeness respectively). However, I had a few concerns:\n\n1. While the framing is unique, it appears to me that the proposed metrics have strong relationship to the insertion and deletion based metrics. For example, to my best understanding, completeness is essentially the deletion metric with the only difference that instead of ordered removal (remove top x% attributed pixels), it uses absolute thresholds (remove pixels with attribution greater than y) before measuring the performance change. Did I miss something? What\u2019s the justification for introducing new terminology if the change is indeed that minor?\n\n2. How does this proposal differ from ROAD other than using values instead of order as noted above?\n\n3. I find the connection between the formalization and the actual implementation a little tenuous. While considerable space is devoted to the theory, the actual algorithms are delegated to the appendices. I also found it difficult to see how the theory truly informed the algorithms.\n\n4. The results section is somewhat lacking. The experiments on the toy dataset conform to the expectation which is good but not enough evidence of the goodness of the metrics. Then the section proceeds to benchmark the attribution methods using the proposed metrics. While interesting, the goal of the results section should be to convince the reader of the merit of the proposal. Just applying the metric to existing techniques doesn\u2019t build any confidence in me for the work in the paper. Any insights thus generated have a confound of whether the observations say something about the proposed metrics or about the techniques studied. \n\n5. Minor: \u201csemi-nature datasets\u201d, \u201cnature constraints\u201d. The formatting style of I in equation (1) and proof of Theorem 3.4 is inconsistent in places.",
            "clarity,_quality,_novelty_and_reproducibility": "Quite clear, fair quality, some novelty, and should be reproducible using the code in the supplementary material.",
            "summary_of_the_review": "Overall, while there is some merit to the submission in its attempt to use formal and standardized way for evaluating attribution methods, I am not entirely convinced of its novelty beyond the introduction of new terminology. The writing is fairly clear but the details on actual implementation are not present in the paper\u2019s main body. Based on these observations, I am not inclined for the paper to be accepted at the venue. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper902/Reviewer_4ijn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper902/Reviewer_4ijn"
        ]
    }
]