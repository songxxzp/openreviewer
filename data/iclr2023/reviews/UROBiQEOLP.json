[
    {
        "id": "tIwPhXi3Iil",
        "original": null,
        "number": 1,
        "cdate": 1666273463998,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666273463998,
        "tmdate": 1666273463998,
        "tddate": null,
        "forum": "UROBiQEOLP",
        "replyto": "UROBiQEOLP",
        "invitation": "ICLR.cc/2023/Conference/Paper2368/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a method for training autoregressive generative models in a way that casts these models into Energy-Based models. In essence, the method uses the logits from the Categorical output distribution of an autoregressive model to define an energy function; this is the basis for the subsequent treatment. As maximization of the energy function entails a contrastive divergence type of sampling, which may prove ineffective, the authors provide an importance sampling-based variant. \n",
            "strength_and_weaknesses": "Pros: Interesting idea, extensive experimental evaluation against the standard baseline, correct derivations. \nCons: The method is a minor extension upon existing techniques; the authors do not explain how more computationally demanding this method is compared to the baselines, and whether these extra costs are worth the extra accuracy. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and of good quality. \nThe novelty is mediocre, but the method could be useful.\nThe experiments are reproducible quite well.",
            "summary_of_the_review": "The method is interesting, and the recasting of the categorical output distribution into the potential function of an energy-based model is nice. The training algorithm entails contributions of some merit. \n\nOn the other hand, the experiments are not convincing at all; they do not elaborate on computational costs required for the extra accuracy, and do not perform an ablation study with different architecture choices which may affect performance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2368/Reviewer_W653"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2368/Reviewer_W653"
        ]
    },
    {
        "id": "K-u53XvvHx",
        "original": null,
        "number": 2,
        "cdate": 1666349006620,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666349006620,
        "tmdate": 1666349006620,
        "tddate": null,
        "forum": "UROBiQEOLP",
        "replyto": "UROBiQEOLP",
        "invitation": "ICLR.cc/2023/Conference/Paper2368/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to use the softmax logits in the output of discrete autoregressive models as an energy function, converting an autoregressive model into an energy-based model. They then use contrastive divergence training, using an importance weighted estimate of the \u201cnegative phase\u201d gradient. Through experiments on language modeling and translation tasks, they show that this results in more coherent and in-distribution samples, as compared with autoregressive baselines.",
            "strength_and_weaknesses": "**Strengths**\n\n- **Relatively simple/general technique.** The basic idea of the proposed technique is to identify the softmax logits of an autoregressive model as an energy function, allowing them to replace the typical maximum log-likelihood training procedure with a contrastive-divergence-based training procedure. This is reminiscent of previous methods that have interpreted classifiers as energy-based models (e.g., Grathwohl et al., 2020). In this way, the model is explicitly trained by decreasing the likelihood of generated samples, hopefully reducing the effect of \u201cexposure bias.\u201d Notably, this procedure can, in principle, be applied to any discrete autoregressive model, making this a rather general technique.\n\n- **Experiments on multiple datasets with relevant baselines.** The authors demonstrate their proposed approach on language modeling and translation tasks, with three language modeling datasets and one translation dataset with six translation pairs. The authors compare their approach with the baseline autoregressive model, as well as Residual EBM, another method that combines autoregressive and energy-based models. The proposed approach generally compares favorably.\n\n- **Clear description, for the most part.** The descriptions within the paper are generally clear, with both text and mathematical terms well-defined. One way to improve the clarity of the paper even further would be to include some form of diagram.\n\n\n**Weaknesses**\n\n- **Additional computational overhead for sampling.** One downside of employing a contrastive divergence-based training method is that it requires sampling during training. Sampling from autoregressive models is typically quite costly, and I would guess that this significantly impacts training speed (although this isn\u2019t discussed in the main paper).\n\n- **Only applicable (at least currently) to discrete models.** The current method, as presented, is only applicable to discrete autoregressive models. While this encompasses a wide variety of applications, the authors have not demonstrated how to apply this technique to continuous autoregressive models, e.g., those that output Gaussian densities. In this sense, the title is a bit misleading, as the technique does not apply to all \u2018autoregressive models.\u2019\n\n- **Only demonstrated on text data, i.e., no images or audio, etc.** The authors only demonstrate their technique on text data (language modeling and translation, i.e., conditional language modeling). However, even with discrete autoregressive models, this technique could be readily applied to image and perhaps audio modeling. Currently, the paper may only appeal to a subset of the machine learning community focused on language, but with these added application areas, the paper may receive wider attention.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity** \u2014 The paper is generally clear, with the basic aspects of the approach well-described in mathematical expressions / equations. I appreciated the algorithm box, which clarified the training procedure. Results tables are also well-formatted and explained in the text.\n\n**Quality** \u2014 The paper is fairly high-quality. The authors take a set of principled ideas stemming from energy-based models and apply them to autoregressive models to solve some well-known issues with the latter. Experiments on multiple datasets with relevant baselines demonstrate that the approach is beneficial, and further experiments demonstrate improvements with respect to exposure bias and coherence. In total, the paper tackles known challenges using principled ideas and clearly demonstrates an improvement in relevant empirical settings.\n\n**Novelty** \u2014 The paper is somewhat novel. While many of the main ideas (treating non-energy-based models as energy-based models, contrastive divergence, importance weighting) are established to varying degrees, combining these components to train autoregressive models is a novel contribution, as far as I am aware.\n\n**Reproducibility** \u2014 I am unsure of the reproducibility of the results, but I suspect that they are reproducible. The authors demonstrate their proposed approach on multiple datasets in various settings, although it\u2019s unclear whether multiple random seeds are used. Generally, their proposed approach outperforms the relevant baselines, with more targeted analyses demonstrating these benefits. While I am unsure of how difficult it would be to reproduce these results, their consistency leads me to believe that they are.",
            "summary_of_the_review": "The paper presents a fairly general and sound technique for improving autoregressive models by training them as energy-based models. Results are demonstrated on several text datasets, comparing with relevant baselines. In total, the generality of the idea, combined with the paper\u2019s convincing empirical demonstration, leads me to suggest acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2368/Reviewer_nMbm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2368/Reviewer_nMbm"
        ]
    },
    {
        "id": "XToTURAEHt",
        "original": null,
        "number": 3,
        "cdate": 1666770083200,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666770083200,
        "tmdate": 1666770083200,
        "tddate": null,
        "forum": "UROBiQEOLP",
        "replyto": "UROBiQEOLP",
        "invitation": "ICLR.cc/2023/Conference/Paper2368/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "AR models are very common in many domains, especially language models. Maximum likelihood is the widely used approach for training these models. However, ML training of AR models causes some other issues such as exposure bias. Techniques such as scheduled sampling have been introduced before to address these issues. \nThis paper introduces a new training algorithm for AR models by viewing it as an energy-based model.  \nHere the authors define the model probability as $p(x_1, \\cdots, x_k) = q(x_{<k}) \\frac{e^{-Q(x_k, x_{<k})}}{Z_\\theta}$. They defined $Q(x_k, x_{<k})$ using the logits before the softmax of the $k$th token, so $p(x_1, \\cdots, x_k) = q(x_i, \\cdots, x_k)$ (am I right?). \nHowever instead of ML training which normalize $e^{-Q(x_k, x<k)}$ with $Z_\\theta$, they are training $p(x_i, \\cdots, x_k)$ using contrastive divergence (CD). This formulation helps to better regulate the distribution on the $k$th softmax since CD training contrast $Q(x_k, x_{<k})$ with a sample from the model (using importance sampling given the introduced form for the energy) rather than the log partition function. \nThe final loss function is a combination of cross entropy and CD.\n\nThe authors study the proposed method for different tasks of language modeling, machine translation, as well as image generation.\n\n",
            "strength_and_weaknesses": "\nStrength:\n-The paper is well-written and easy to follow.\n-The idea of viewing AR models as EBMs is not novel but the provided formulation is neat and interesting. \n-I like the extensive study of the proposed approach.\n\nWeakness:\nThe improvement especially in the translation tasks is not considerable. Other training algorithms on top of ML training of AR can achieve similar improvement, for example, combining it with RL training. \nBhattacharyya et al. (2021) reported a significant improvement over base AR-NMT by using an external EBM defined over the whole sentence (although some of the improvements come from using pre-trained language models).  \nAlso, their training algorithm can directly be applied to AR by viewing E as \\sum_i E_i. \n\nQuestions:\n1) The author claims this approach can help with long-range dependencies, but I am not seeing how defining the energy model using the last softmax can help with that. Is that the effect of weights on training log q(x<k)? Even so, it is not similar to defining an explicit energy model over the whole sentence! \n\nBhattacharyya S, Rooshenas A, Naskar S, Sun S, Iyyer M, McCallum A. Energy-based reranking: Improving neural machine translation using energy-based models. ACL 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow. The proposed method is novel although some parts such as using importance sampling for estimating partition function have been introduced before for similar joint distribution form: p(x) = q(x) exp (-E(x)).\n\n\n",
            "summary_of_the_review": "The paper is interesting but the experimental results do not show considerable benefits for the approach. I don't think that the proposed training algorithm would be adopted by the community for training AR models. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2368/Reviewer_FpT4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2368/Reviewer_FpT4"
        ]
    }
]