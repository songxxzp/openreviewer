[
    {
        "id": "HChirKWLHN",
        "original": null,
        "number": 1,
        "cdate": 1666551475717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666551475717,
        "tmdate": 1666551475717,
        "tddate": null,
        "forum": "qxRscesArBZ",
        "replyto": "qxRscesArBZ",
        "invitation": "ICLR.cc/2023/Conference/Paper5427/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an improved graph learning algorithm based on a robust Gromov-Wasserstein discrepancy (RGWD). Following the previous GDL by Vincent-Cuaz et al. 2021, the new method replaces GWD by a new RGWD which uses a minimax formulation. Moreover, robust graph dictionary learning algorithm is also proposed based on stochastic gradient descent. Overall, the paper has a certain novelty on designing a discrepancy form with theories, and the numerical experiements on synthetic and real-world data sets have shown the performance of the proposed algorithm. ",
            "strength_and_weaknesses": "Strengths:\n1. A new robust variant of GWD is proposed with discussions on its properties such as asymmetry. \n2. A projected gradient descent algorithm for RGWD is provided with convergence guarantees. \n3. Performance of the proposed method on synthetic and real-world datasets is outstanding. \n\nWeaknesses:\n1. In Sec 3.2, the Moreau envelope of the maximum function turns out to be a minimax problem, and the corresponding proximal operator is not straightforward. Some remarks about the role of this in Theorem 1 could be made. \n2. It's not clear why a negative quadratic term is necessary in eqn.(4), which also makes the objective nonconvex and may cause convergence issues. In addition, guidance on choosing the parameter $\\lambda$ for some datasets could be given.\n",
            "clarity,_quality,_novelty_and_reproducibility": " The quality, clarity and originality of the work is good. ",
            "summary_of_the_review": "The paper is interesting, reads well and has clear motivation and contributions. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5427/Reviewer_2db5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5427/Reviewer_2db5"
        ]
    },
    {
        "id": "yKkvTC-zCS",
        "original": null,
        "number": 2,
        "cdate": 1666601950305,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601950305,
        "tmdate": 1666601950305,
        "tddate": null,
        "forum": "qxRscesArBZ",
        "replyto": "qxRscesArBZ",
        "invitation": "ICLR.cc/2023/Conference/Paper5427/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies the problem of dictionary learning for graphs. It proposed an improved graph dictionary learning algorithm based on a robust Gromov-Wasserstein discrepancy (RGWD). It also provided some theoretical results and numerical results. ",
            "strength_and_weaknesses": "Strengths:\n1. Graph-level dictionary learning is a challenging problem and hasn't been well studied. The paper proposed an effective method called RGWD though heavily relying on previous work.\n\n2. The authors provide some theoretical analysis including some properties of the proposed metric and the convergence of the optimization algorithm.\n\n3. The numerical results especially those of the synthetic data showed the effectiveness of the proposed method in comparison to GDL.\n\nWeaknesses:\n1. In terms of the model, the contribution is not significant because compared with previous work of GWD, the proposed method only added an error term E. Thus RGWD can be regarded as a natural relaxation of GWD. The connection and difference should be highlighted. In the numerical results, what caused the improvement of RGDL over GDL? Is it the noise term E or optimization algorithm?\n\n2. It seems that the time complexity is $O(n^4)$ and the authors did not discuss the time cost in the experiments.\n\n3. The role and effectiveness of E are not clearly explained. Could one use other constraint on E instead of the infinity norm?\n\n4. In Theorem 1, it is not clear why RGWD cannot reach zero.\n\n5. Is RGWD symmetric?\n\n6. In the experiments, are C_i similarity matrices? If yes, setting $\\epsilon$ to large values such as 10 and 30 in Table 1 looks a little weird.\n\n7. In Table 4, it seems that all methods failed in clustering IMDB-B and IMDB-M because the ARIs are less than 0.1, which makes the experimental results useless. The authors should consider more real datasets on which the proposed method can work well. One MUTAG dataset is not enough.\n\n8. It is not clear why the authors did not show the influence of $\\epsilon$ on the clustering performance of the real datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality can be improved. The novelty is not very significant when we consider the previous work GWD and GDL. The numerical results are not sufficient.",
            "summary_of_the_review": "Overall, the idea of robust dictionary learning is interesting though robust dictionary learning has been widely studied in many previous work of vector data. The novelty is fine and the experiments should be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5427/Reviewer_Lcq7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5427/Reviewer_Lcq7"
        ]
    },
    {
        "id": "sN3i37hTC_",
        "original": null,
        "number": 3,
        "cdate": 1666668580912,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668580912,
        "tmdate": 1666668580912,
        "tddate": null,
        "forum": "qxRscesArBZ",
        "replyto": "qxRscesArBZ",
        "invitation": "ICLR.cc/2023/Conference/Paper5427/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a robust graph dictionary learning algorithm based on a metric named robust Gromov-Wasserstein discrepancy (RGWD).  With the proposed method,  a set of atoms can be learned from noisy graph data. Experiments are conducted on synthetic datasets as well as public real-world datasets to evaluate the proposed method.",
            "strength_and_weaknesses": "Strengths\nTheoretical analysis about the properties of the proposed RGWD is provided.\n\nWeakness\n1. Lacking the motivation of using the projected gradient to update the transport plan.\n2. Insufficient experimental evaluation.\n\nDetailed comments:\n1. What\u2019s the advantage of using the projected gradient to update the transport plan.? Can it be solved by employing the Sinkhorn algorithm? More theoretical or experimental evidence needs to be provided to compare their efficiency/convergence and results.\n2. Three datasets named MUTAG, IMDB-B, and IMDB-M are used to evaluate the proposed method with the metric of ARI score. However, as these datasets are usually used for graph classification, it\u2019s suggested to provide the classification performance on these datasets as well.\n3. More graph learning methods could also be compared, e.g. graph kernel-based methods, other than only graph dictionary learning. Specifically, a deep graph dictionary learning method is proposed in [a] recently. It is suggested to compare this work by discussing the advantages and disadvantages, as well as comparing the performance.\n[a] T. Zhang, et al. Deep Wasserstein Graph Discriminant Learning for Graph Classification. AAAI 2021. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to read. It improves a previous work by adding the perturbation to the Gromov-Wasserstein metric to make it robust to noises, which is somehow marginally significant or novel. The code is provided in the supplementary material.",
            "summary_of_the_review": "The motivation of the way for optimizing the transport plan should be made clear, and more experimental results should be provided with a comprehensive comparison.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5427/Reviewer_ia1M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5427/Reviewer_ia1M"
        ]
    },
    {
        "id": "G1sHrxz1fOE",
        "original": null,
        "number": 4,
        "cdate": 1666821994634,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666821994634,
        "tmdate": 1668717861810,
        "tddate": null,
        "forum": "qxRscesArBZ",
        "replyto": "qxRscesArBZ",
        "invitation": "ICLR.cc/2023/Conference/Paper5427/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a robust approach for dictionary learning on graphs. The method is based on a robust version of the Gromov-Wasserstein discrepancy, which involves a minimax optimization problem that is neither convex/concave. The authors proved several properties of the RGWD. The authors demonstrate the efficacy of their approach via experiments and simulations. ",
            "strength_and_weaknesses": "- Strengths: \n\n- The approach is well motivated and natural. The drawbacks (lack for robustness under noise) of the current GWD approach is stated, and then a solution that address the robustness drawback is proposed. \n\n- The paper is well-written. The background material on optimal transport and graph dictionary learning is presented clearly. \n\n- The proposed method is a very natural extension of GWD. \n\n- Some nice (although pretty basic) properties of the RGWD, such as triangle inequality is posed. \n\n- In experiments and simulations, RGWD appears to perform quite substantially better than competitors. \n\n- Weaknesses: \n\n- The comparison in the simulations is limited. In the simulations, the authors used RGWD with a wide variety of epsilon parameters and compare all of them against only GDL. While the performance of RGWD is good/better than GWD across a variety of epsilon choices, comparison against only one alternative (on a simulated graph clustering task, of which many alternatives should be available) does not convince the reader strongly enough that the RGWD is truly superior to a broad set of competitors. I highly suggest that the authors compare the performance of RGWD against a few other graph clustering methods to make their simulations more convincing. \n\n- With any distance/divergence or other tool for graph comparison, certain specific basic properties should be shown. Of particular importance is whether the divergence/distance etc is invariant to the representation of the graph. For example, If I am comparing graph G_s and G_t, would the resulting number/distance be invariant to vertex label permutations? I.e. distance(G_s, G_t) = distance(permutation1(G_s), permutation2(G_t))? I think this property is very important since it makes sures that the distance is comparing graph structure, not some arbitrary label/representation. As of now it is not clear whether the RGWD satisfies this property. If the property is satisfied, then it would be great to state it explicitly. If it is not satisfied, then at least the authors could mention/discuss it. \n\nMinor Points: \n\nThere are some related work/references on comparing graphs of different sizes/number of nodes with no alignments that I think the authors could cite: \n\nNetwork Portrait Divergence: \nAn information-theoretic, all-scales approach to comparing networks. James Bagrow and Erik Bollt\n\nEmbedded Laplacian Distance: \nMultiscale Graph Comparison via the Embedded Laplacian Distance. Edric Tam, David Dunson\n\nGraph Diffusion Wasserstein Distance: \nBarbe et. al. Graph Diffusion Wasserstein Distances. ECML-PKDD 2020",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and presented. \n\nThe idea appears to be novel and original, and tackles a known limitation of a very new/state of the art method. \n\n",
            "summary_of_the_review": "I think overall the paper is well written, the approach is natural, and the contributions are sufficiently novel/original. However, there remains some areas for improvement that I think if addressed, would make the paper much stronger. In particular, if they could compare against more alternatives in their simulations, add the very related references that are suggested, and comment on whether their method satisfies the permutation invariance property which is very important and natural. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5427/Reviewer_q8cY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5427/Reviewer_q8cY"
        ]
    }
]