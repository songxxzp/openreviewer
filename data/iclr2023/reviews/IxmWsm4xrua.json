[
    {
        "id": "BSWXf4VjJ5j",
        "original": null,
        "number": 1,
        "cdate": 1666609052205,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609052205,
        "tmdate": 1670983571360,
        "tddate": null,
        "forum": "IxmWsm4xrua",
        "replyto": "IxmWsm4xrua",
        "invitation": "ICLR.cc/2023/Conference/Paper1740/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "For modeling long sequences, as an alternative to the attention+softmax based weights that are typically utilized in the transformer models, this paper proposes to use relative position dependent weights that are generated by a \u2018relative position encoder\u2019 network, together with an exponential decay bias on the weights.  Due to the use of position encoder network to generate weights the number of parameters in the model is fixed regardless of the length of sequence being modeled.  Also, since relative position dependent weights naturally occur in a Toeplitz structure, the paper proposes to utilize special properties of Toeplitz matrix-vector multiply to achieve computation efficiency.",
            "strength_and_weaknesses": "Pros:\n* An efficient approach for long range dependency modeling that relies only on relative position dependent weights\n\nCons:\n* The performance of baseline S4 model is substantially lower (AVG. 70.87) as compared to that reported in the S4 paper by Gu et al. (AVG. 80.48).  What is the reason for this discrepancy?\n* Paper lacks clarity, especially in describing the proposed model.  For instance:\t\n\t* Define d in Eq. 1\n\t* What role does W play in Section 3.2?  Does Eq. (4) need a +W term?\n\t* Section 3.3 states that inputs to the position encoder network are -(n-1) \u2026 (n-1).  Does this mean the network has 2n-1 inputs?  Or is it a single positive integer as input?\n\t* Similarly, section 3.3 states that output of the position encoder network is a d dim vector, but Fig. 2 indicates it is (2n-1) d dim vectors?\n\t* Is there a separate T matrix for every layer?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: lacks clarity, especially in stating the proposed model and in the Long Range Arena baseline numbers.\nNovelty: somewhat novel for long sequence modeling\nReproducibility: should be reproducible once proposed model is clarified",
            "summary_of_the_review": "See pros and cons above.  My main reason for a 'reject' rating is unclarity around the LRA baseline numbers.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1740/Reviewer_kNVV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1740/Reviewer_kNVV"
        ]
    },
    {
        "id": "g_2fOjbuCB",
        "original": null,
        "number": 2,
        "cdate": 1666667519047,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667519047,
        "tmdate": 1666667519047,
        "tddate": null,
        "forum": "IxmWsm4xrua",
        "replyto": "IxmWsm4xrua",
        "invitation": "ICLR.cc/2023/Conference/Paper1740/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new token mixing architecture called Toeplitz Neural Networks (TNN). In particular the attention in the popular transformer architecture is replaced with a Gated Toeplitz Unit which is a GLU but before the scalar multiplication the transformed input is multiplied (\"mixed\") with a toeplitz matrix. The parameters for the matrix are predicted by a neural network in order to make the architecture compatible with any sequence length. The paper shows in various experiments that TNNs work equally well to transformers while being significantly faster both in practice and asymptotically.",
            "strength_and_weaknesses": "Strengths\n------------\n\n- The idea of the paper is very intuitive and has several precursors like S4s.\n- The use of a network to predict the toeplitz matrices based on the sequence index is interesting.\n- The results show that TNNs perform equally well to transformers in the example tasks.\n\nWeaknesses\n---------------\n\n- The tasks except the LRA benchmark are not particularly large sequence tasks. It would be great to have some experiments for instance on autoregressive image generation or larger scale transformer experiments.\n- Similarly to the above, LRA is not a particularly good indicator of real world performance as it can be seen also by the rest of the experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the idea is simple enough that it can be easily reproduced.",
            "summary_of_the_review": "I believe that the paper has sufficient novel contributions and adequate experimental evaluation to make a very good ICLR paper. My only grievances had to do with the evaluation which at the same time is both lacking and thorough. Namely lacking larger and more convincing experiments, but also has enough ablations and experiments with different modalities to be sufficient.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1740/Reviewer_k2X2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1740/Reviewer_k2X2"
        ]
    },
    {
        "id": "1v7Mr4IKAU",
        "original": null,
        "number": 3,
        "cdate": 1666674767125,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674767125,
        "tmdate": 1670593613556,
        "tddate": null,
        "forum": "IxmWsm4xrua",
        "replyto": "IxmWsm4xrua",
        "invitation": "ICLR.cc/2023/Conference/Paper1740/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new way to model the attention mechanism in the transformer architecture for sequence modeling. The proposed approach, while retaining comparable performance compared to other transformer architectures and approaches, improves the efficiency of transformer's capability in sequence modeling in two respects: 1) efficiency of the attention matrix operation (via the Toeplitz matrix formulation); and 2) efficiency of long sequence (which can be longer than the sequence on which the model is being trained) modeling during inference (via a generalized exponential decay bias on the attention scores). \n\nThe experiments demonstrate improved efficiency and performance in particular in long range sequence modeling. In addition, the authors also demonstrate comparable performance of the proposed approach with baselines in various text modeling tasks and an image modeling task, with a table showing the theoretical improved efficiency in the matrix operation.\n",
            "strength_and_weaknesses": "### strengths \n- The idea of using Toeplitz matrices is interesting and, to my knowledge, new\n- Combining Toeplitz matrices and exponential decay bias lead to improved performance and efficiency in long range sequence modeling\n\n### weaknesses\nOne of the main contributions of the proposed approach, in my opinion, is the improved efficiency without sacrificing performance. However, I wish the experiments can include more evidence to support this contribution.\n\n- My first concern is a lack of clear explanation of the role of Toeplitz matrices and exponential decay bias, respectively, in modeling long sequences (e.g., for the LRA benchmark). It seems to me that, what enables the proposed approach to be able to operate on long sequences is not because of Toeplitz matrices but because of a generalized version of ALiBi. Since ALiBi and the proposed approach have similar performance (according to Table 14, last two columns), an experiment comparing the **efficiency** between ALiBi and the proposed approach would make it very clear that Toeplitz matrices are indeed more efficient for long sequence modeling during inference. \n- Since the performance of the proposed approach is similar in the other experiments, I wish the authors can also include, in addition to the performance metrics, the computational efficiency for the experiments in addition to the LRA experiment. I understand that the authors have a table on the theoretical efficiency improvement; I think some empirical evidence would make this efficiency claim much stronger. For example, Table 2, 3, and 6 do not seem to demonstrate the proposed method's advantage because 1) performance is similar to the baseline(s) and 2) efficiency comparison is missing.  \n\nSome other comments/questions\n- Does multi-head attention change any part (e.g., implementation) of the proposed approach?\n- Another approach for improving the efficiency of transformers is to leverage sparse/structured matrices (e.g., [1]). It seems that references to this body of work is missing from the paper (or not discussed much). Could the authors comment how their work relates to prior work in speeding up transformers by leveraging sparse/structure matrices?\n- What is the unit of the metric being reported in Table 4?\n- Why do the authors choose GLU instead of other operators (also: GLU is introduced without defining it)?\n- It seems that a citation in third paragraph above Section 2 is not anonymized (in the third line from the last line in that paragraph). But I think it does not matter at this point.\n\n[1] https://openreview.net/pdf?id=Nfl-iXa-y7R",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly presented for the most part. Some experiments can include more explanations, perhaps deferring to the appendix due to space constraints in the main text.\n\nI think if the authors include a figure about the key components in classic transformer architecture design and place it side by side with the proposed approach in Figure 2, and highlight the difference and similarities, would help make the proposed approach's uniqueness.",
            "summary_of_the_review": "The proposed approach is a very interesting idea of exploiting Toeplitz matrices' structure for improving transformer architecture's computational efficiency. However, I think the empirical evidence does not fully substantiate the efficiency claim: empirical computational efficiency results are missing for all experiments except for LRA. It is also unclear how the proposed approach compares to other works in exploiting matrix sparsity/structure for improving transformer's efficiency. I think this paper has a lot of potential. I'm very open to increase my score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1740/Reviewer_gJML"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1740/Reviewer_gJML"
        ]
    },
    {
        "id": "Ab4KrYf9rgI",
        "original": null,
        "number": 4,
        "cdate": 1667324160679,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667324160679,
        "tmdate": 1667324160679,
        "tddate": null,
        "forum": "IxmWsm4xrua",
        "replyto": "IxmWsm4xrua",
        "invitation": "ICLR.cc/2023/Conference/Paper1740/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new attention mechanism that depends only on the relative positions of tokens and is independent of the features themselves. Since the attention coefficient depend only on the relative positions, the resulting attention matrix is Toeplitz. This results in a reduction in the number of parameters and in time complexity of feature updates, since Toeplitz matrices can be applied in O(n log n) time, where n is the sequence length. The authors demonstrate that the proposed method closely matches the performance of conventional attention models, while being faster in terms of training time. ",
            "strength_and_weaknesses": "Strengths: \n\nThe proposed idea is very interesting and novel. It is quite surprising to see that attention coefficients that depend on just relative positions perform well. The paper is well-written and the the flow if ideas is great. The provided experimental evidence is sufficient to demonstrate the merit of the proposed ideas. \n\nWeaknesses:\n\nThe paper is quite good as is and does not have many weaknesses. I do have the following suggestions/ questions to the authors:\n\n1. In designing the RPE, the authors claim that learning the weights of Toeplitz matrix performs worse compared to the RPE network. This is a bit unintuitive, since the former should be able to degenerate to the latter. I am surprised to see that learning the weights directly does not perform well. Maybe the authors could shed more light on this. \n\n2. It would be great to have visualizations of the learnt attention. I am curious to see the kind of associations that are learnt on the datasets considered. Does the model identify semantic relationships in the language? What is the intuition behind using only relative position based attention to capture associations between words?\n\n3. Are there any extensions possible to incorporate multi-headed attention?",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The paper is well-written and is an interesting read. \n\nClarity: The paper is clearly written.\n\nOriginality: The presented ideas are quite novel. ",
            "summary_of_the_review": "The paper presents a very interesting and novel idea. The  proposed ideas are backed up with good experimental evidence. Overall, this paper is quite refreshing to read and I recommend acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1740/Reviewer_4U54"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1740/Reviewer_4U54"
        ]
    }
]