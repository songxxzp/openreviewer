[
    {
        "id": "r8uulO_YumE",
        "original": null,
        "number": 1,
        "cdate": 1666636343650,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636343650,
        "tmdate": 1666636343650,
        "tddate": null,
        "forum": "fnDbEm6RxqH",
        "replyto": "fnDbEm6RxqH",
        "invitation": "ICLR.cc/2023/Conference/Paper3903/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a transformer-encoder framework for multipleThe authors present a encoder/transformer method for learning representations of EHR data,  for multiple prediction tasks. They demonstrate that their method out-performs other DL methods on one prediction task using MIMIC-III data.\n",
            "strength_and_weaknesses": "\nStrengths:\n- This paper is well written\n- it focuses on an important problem (they also motivate the problem very well)\n- the methods seem sound, though I am not an expert\n- the experiments use relevant data, and the results demonstrate that their method works well\n- their experiments use public data\n\nWeaknesses\n- There has been a lot of work on this topic, and the authors don't cite or compare with existing methods. Here is a short list of papers that look relevant to this topic, but were not mentioned or compared with in the paper:\n-- https://doi.org/10.1109/ICDM50108.2020.00050\n-- https://doi.org/10.1145/3394486.3403129\n-- https://doi.org/10.1109/ICDM51629.2021.00060\n-- here is a relevant review: https://doi.org/10.1016/j.jbi.2020.103671\n- there is no comparison against state-of-the-art risk prediction models in medicine, or any non-DL models at all. there are several such models used in medicine (which don't use DL), which would be good baselines to compare against here. \n- the authors don't provide any code\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity\n- this paper is very clear\n\nQuality\n- the methods and experiments all seem sound\n\nNovelty\n- I think there are issues with the novelty of this work (see \"weaknesses\" above)\n\nReproducibility\n- The authors don't mention that they will share code, but their explanations of their methods are fairly clear, so I think someone could independently reproduce their method.\n\n",
            "summary_of_the_review": "This is a well-written paper on an interesting and important topic. The experiments are somewhat compelling andt he methods appear to be sound. The main issue is that the authors don't disucss or compare with other methods from the literature. There has been a lot of work in the encoder/transformer space for EHR data, and the authors don't engage with this literature. Since they are presenting their method as a general approach for EHR data (and not a method specific to AKI), they should make sure to engage & compare with exisitng work.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3903/Reviewer_M4Hz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3903/Reviewer_M4Hz"
        ]
    },
    {
        "id": "JzAiM8CHJrH",
        "original": null,
        "number": 2,
        "cdate": 1666661754691,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661754691,
        "tmdate": 1666682111610,
        "tddate": null,
        "forum": "fnDbEm6RxqH",
        "replyto": "fnDbEm6RxqH",
        "invitation": "ICLR.cc/2023/Conference/Paper3903/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the fairness problem in a healthcare setting involving outcome prediction from electronic health records (EHR). To this end, the authors propose the Masked Triple Attention Transformer Encoder (MTATE), an adaptive multi-task learning model capable of automatically learning and selecting optimal yet fair data representations without explicit domain adaptation or domain-specific bias correction. The experimental results suggest that MTATE learns representations of diverse subpopulations that lead to improvements in predictive performance and fairness on the task of mortality risk prediction for critically ill Acute Kidney Injury requiring Dialysis (AKI-D) patients.",
            "strength_and_weaknesses": "Strengths:\n\n* The problem addressed in this paper is of considerable importance as approaches that account for patients\u2019 backgrounds and subpopulations are quite relevant to personalized medicine and reliable prediction of patient outcomes.\n\n* The proposed MTATE model is capable of learning optimal yet fair representations of EHR sequences, which is of great importance to outcome prediction for patients with different demographic characteristics and clinical histories.\n\n* MTATE incorporates time-wise, feature-wise and representation-wise attention mechanisms. This allows for (1) capturing associations among timesteps (which are quite informative in the case of EHR data), associations among features, and (2) summarizing feature-attended representations for downstream patient outcome prediction.\n\n* While it is unclear whether domain-invariant or domain-specific data representations should be used for prediction tasks concerning EHR data, the authors claim that MTATE learns EHR representations that can be domain-specific, domain-invariant, or even a mixture of the two.\n\n* Experiments conducted on a real-world EHR dataset demonstrate that MTATE outperforms several outcome prediction baselines on mortality risk prediction for critically ill AKI-D patients, yielding improvements in both classification metrics as well as fairness metrics, thus mitigating bias towards different subpopulations in the data.\n\n* A few traditional classifiers (XGboost, SVM and RF) have achieved performance comparable to that of MTATE and outperformed some of the compared baseline methods; which suggests that MTATE\u2019s representations can also serve as pre-trained EHR data representations for various classifiers applied to downstream prediction tasks.\n\n* The authors conducted an insightful analysis of the relationship between the patient outcome loss, the domain loss and the representation-wise attention.\n\n-------------------------------------------------------------------------------------------------\n\nWeaknesses:\n\n* The proposed MTATE model appears to resemble a multi-task attention classifier with an output layer that combines the representations learned for all \u2018tasks\u2019 (i.e. domains) into a single outcome. Due to this resemblance, I believe that a comparison of MTATE to multi-task classification models should have been carried out. I would encourage the authors to consider such a comparison or provide their reasoning as to why multi-task classification models should not be considered among the baselines.\n\n* In Section 2.3, the authors state the following: \u201cSince features for the different domains are not equally important, we randomly masked out some number of latent features along all time steps differently for each FR-Attention module. In return, the masking procedure forces each sub-module to learn different feature focuses for each domain to generate unique domain representations.\u201d However, no experiment has been conducted to inspect the impact of the random masking on the learned feature representations and thus on the overall patient outcome prediction performance. I am aware of the ablation study (Table 3) conducted to assess the performance with and without masking, nevertheless, such a study does not provide an insight into the effect of different random masking trials. I would suggest that the authors elaborate on this aspect of the random marking in their response.\n\n* One of my main concerns is that the resulting representations learned for each domain can be domain-specific or domain-invariant, depending on the domain classification loss. Thus, target domains that are \u201ceasier to classify\u201d will tend to have smaller loss values. On the other hand, other target domains might be difficult-to-classify yet contain more informative patterns, in which case such domains will have higher loss values and may need many iterations for their losses to be substantially minimized. This case does not appear to be accounted for in the model design. I would encourage the authors to clarify the potential behavior of MTATE in the aforementioned scenario.\n\n* In Section 2.6, the authors mention that \u201cthe final representation C of EHR is concatenated with all static features, such as demographics and comorbidity, followed by a linear layer with a sigmoid function for the outcome binary prediction\u201d. I assume that the demographics and comorbidity features are one-hot encoded, in which case it would be desirable that the range of the learned representations\u2019 values is around [0, 1]. Is a certain normalization technique applied to the final representations to scale them within the same (or similar) range as that of the static features? If this is not the case, I would like to ask the authors to provide their thoughts on this point in their response.\n\n* The authors have generated 30 samples from each patient\u2019s EHR history with a random start and end times. Nevertheless, I am wondering whether there is a specific reason behind this data preprocessing choice. Why not consider the entire EHR sequence for each patient? Were the sequence samples generated just for data augmentation purposes, i.e. so that representations can be learned from a larger dataset created by generating multiple samples from a single EHR sequence?\n\n* In Section 3.2, the authors mention that all models have been compared on both imbalanced and balanced sets having positive/negative ratios of 1:4 and 1:1, respectively. However, I am not certain if 1:4 can be considered imbalanced (of course, the ratio itself is not a balanced ratio, but the classification problem cannot be considered a typical case of an imbalanced classification problem, or at least not a highly imbalanced classification problem).\n\n* In addition to the considered baselines, I believe that MTATE should have been compared to several recently published state-of-the-art representation learning methods for EHR data including, but not limited to, the methods proposed in:\n\n    Darabi, S., Kachuee, M., Fazeli, S., & Sarrafzadeh, M. (2020). Taper: Time-aware patient ehr representation. IEEE journal of biomedical and health informatics, 24(11), 3268-3275.\n\n    Bang, S. J., Wang, Y., & Yang, Y. (2020). Phased-lstm based predictive model for longitudinal ehr data with missing values.\n\n    Bai, T., Zhang, S., Egleston, B. L., & Vucetic, S. (2018, July). Interpretable representation learning for healthcare via capturing disease progression through time. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining (pp. 43-51).\n\n-------------------------------------------------------------------------------------------------\n\nMinor weaknesses:\nThere are also certain grammatical and typographical errors and remarks that require attention. Some of them are summarized as follows:\n- First paragraph on page 4: The sentence \u201cThus, the attention weights computed from the query and key represent how much focus each time step is associated with other time steps.\u201d is rather unclear and should be revised accordingly.\n- Page 4: In the last sentence of Section 2.4, \u201cgenerate\u201d should be replaced with \u201cgenerating\u201d and a word seems to be missing between \u201cfor\u201d and \u201cin\u201d.\n- Page 5: \u201cC\u201d should be bolded in the paragraph after Eq. (9).\n- Last paragraph on page 6: In the sentence \u201cLSTM is the most competitive method since it has the same highest accuracy\u201d, I assume that the authors meant \u201csame highest ROCAUC\u201d instead of \u201csame highest accuracy\u201d.\n- Page 7, last paragraph of Section 4.1: I believe that \u201cFigure 4.1\u201d should be replaced with \u201cFigure 3\u201d. Later in the same paragraph, \u201cFigure A.3.2\u201d should be replaced with \u201cFigure A4\u201d.\n- In Section 4.3: replace \u201cnegative correlated\u201d with \u201cnegatively correlated\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is well written, organized, and technically detailed to a satisfactory extent. The notation is clear and consistent throughout the paper.\n\nQuality:\nThe design and justifications for the proposed Masked Triple Attention Transformer Encoder are technically sound. The observations made regarding the improvements in patient outcome prediction performance introduced by the proposed MTATE are empirically well supported. Overall, the work seems to be fairly well developed in terms of technical quality.\n\nNovelty:\nFrom a methodological perspective, the contribution of this work can be considered rather incremental. In essence, the authors leveraged several existing attention-based modules and integrated them into a single architecture for the purposes of learning EHR representations that preserve different types of associations within the data. Nevertheless, each of these attention mechanisms are already established in the literature which limits the novelty of this work.\n\nReproducibility:\nThe reproducibility of this work is somewhat limited since all experiments have been conducted on, what appears to be, a proprietary dataset which has not been made publicly available by the authors. Even if the authors could not make this dataset available, they should have evaluated MTATE and the baselines on at least one widely-used publicly available EHR dataset (e.g., MIMIC-III [1]). As for the code for the proposed MTATE model, it is also not made available by the authors, however, the architecture of the model is clearly explained in the paper, thus it is safe to say that one should be able to implement MTATE by following its description in Section 2.\n\n[1] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L. W. H., Feng, M., Ghassemi, M., ... & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific data, 3(1), 1-9.\n",
            "summary_of_the_review": "Overall, this paper proposes an adaptive multi-task learning model capable of learning optimal and fair data representations without explicit domain adaptation or domain-specific bias correction. Addressing fairness in a healthcare setting involving patient outcome prediction is of considerable importance. Nevertheless, the work lacks methodological novelty and is rather incremental in that respect considering that it combines already established attention mechanisms. That being said, I recommend that this work is not considered for acceptance at ICLR due to reasons including, but not limited to, the following:\n\n(1) the degree to which this work is application-oriented rather than a work that makes novel methodological contributions in representation learning;\n\n(2) the absence of a comparison of the proposed model to more recently published models for representation learning from EHR data;\n\n(3) unjustified data preprocessing decisions (outlined in the list of weaknesses);\n\n(4) experiments conducted solely on proprietary data despite the availability of public EHR datasets that also contain records of AKI-D \npatient among other conditions.\n\nDespite the above reasons, I am still looking forward to the authors\u2019 response and I would be willing to adjust my score in case I have misunderstood or misinterpreted certain aspects of the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3903/Reviewer_H5nZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3903/Reviewer_H5nZ"
        ]
    },
    {
        "id": "UwTet9ACLs",
        "original": null,
        "number": 3,
        "cdate": 1666670902745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670902745,
        "tmdate": 1666670902745,
        "tddate": null,
        "forum": "fnDbEm6RxqH",
        "replyto": "fnDbEm6RxqH",
        "invitation": "ICLR.cc/2023/Conference/Paper3903/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose an attention-based encoder model to learn data representations that are fair for prespecified subpopulations. The proposed method was tested on real data to learn EHR representations and predict 72-hour mortality after dialysis.",
            "strength_and_weaknesses": "The experiment was well conducted. The proposed model has shown advantages in both prediction performance and fairness across prespecified subgroups. \n\nMore explanation and intuitions can be provided for the proposed method, for example, the rationale for introducing each model part and their connection with existing methods. Also, the reason why the model can achieve desired fairness can be better described.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The method was presented clearly with formulas, and the experiments were well conducted. It connects domain-specific and domain-invariant representations to achieve fairness and better model prediction performance. \n\nHowever, the code was not provided for reproducibility purposes. Also, are there any tuning parameters for the proposed method? How were they selected and how much do they affect the model results?\n",
            "summary_of_the_review": "The method in the paper is proposed to balance domain-specific and domain-invariant representations with the aim of learning fair and unbiased representation. The experiment has shown its advantage empirically. Overall, I think the proposed method could be useful. But more details can be provided in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3903/Reviewer_UWzS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3903/Reviewer_UWzS"
        ]
    },
    {
        "id": "nkBYj0zgnt",
        "original": null,
        "number": 4,
        "cdate": 1667211739366,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667211739366,
        "tmdate": 1667211739366,
        "tddate": null,
        "forum": "fnDbEm6RxqH",
        "replyto": "fnDbEm6RxqH",
        "invitation": "ICLR.cc/2023/Conference/Paper3903/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method to learn representation that is invariant to sensitive groups (or domains as claimed by the authors). This is achieved through a network structure that has a transformer to generate time-wise attention, domain-specific feature-wise attention (to extract features that are specific to each domain/subgroup), and finally all these representations are concatenated and passed to a network to generate a universal representation that can be used for downstream tasks.",
            "strength_and_weaknesses": "Strength: \n- The problem is very important especially in health care domain. \n\nWeakness: \n- The paper is not easy to follow \n- I think using the word \"domain\" might by non-intuitive in this context. It is usually referred to as sensitive or protected groups. \n- The method seems to be adhoc without a principle way to motivate the structure of the network. \n- It is claimed in the paper that \"To the best of our knowledge, MTATE is the first model that automatically trains and determines optimal and fair data representations.\" There are many papers about learning fair representation. The one that needs to be highlighted here is \"Creager, Elliot, et al. \"Flexibly fair representation learning by disentanglement.\" International conference on machine learning. PMLR, 2019.\" The method in that paper learns representation that is invariant to the sensitive groups, which is the main objective of the current paper. \n- Missing important baselines to compare to. ",
            "clarity,_quality,_novelty_and_reproducibility": "Not easy to follow. Method novelty is minor and very adhoc. ",
            "summary_of_the_review": "Read the strength and weakness section. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3903/Reviewer_icM1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3903/Reviewer_icM1"
        ]
    }
]