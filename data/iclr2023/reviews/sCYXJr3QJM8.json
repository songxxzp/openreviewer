[
    {
        "id": "JK14SOAh3x",
        "original": null,
        "number": 1,
        "cdate": 1666572269230,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666572269230,
        "tmdate": 1666572269230,
        "tddate": null,
        "forum": "sCYXJr3QJM8",
        "replyto": "sCYXJr3QJM8",
        "invitation": "ICLR.cc/2023/Conference/Paper2914/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel method to generate realistic images on the target sparse domain by transferring a large-scale generator from the source domain without retraining the generator. Also, the proposed method can generate diverse target samples. The key idea is to solve an optimization problem on the latent domain of the generator at inference time. The proposed methodology is clearly stated and extensive comparisons with state-of-the-art methods have been studied. The proposed method has shown very competitive numbers in the experiments.",
            "strength_and_weaknesses": "*Strengths*\n- The presentation quality of the paper is of good quality: the proposed method is well stated.\n- Comparison with previous literature is well studied.\n- The proposed method is novel in that it provides an intuitive yet efficient way to reduce overfitting and increase diversity.\n- Numerical studies and comparisons with previous methods have been well conducted. The results show that the proposed method is at least comparable to or better than state-of-the-art methods.\n*Weaknesses*\n- Although this paper is experiment-biased, it could be more plausible to include theoretical discussions on why the introduction of the latent space learner could help reduce overfitting and increase diversity.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper has good writing quality and clarity. \n- Although the proposed method looks simple and incremental to previous studies, the numerical results have shown the effectiveness and efficiency of the proposed method.\n- The code is anonymously provided for better reproducibility.",
            "summary_of_the_review": "The proposed method shows great potential in mitigating the overfitting and diversity issues in solving domain transfer problems. The experiments have shown promising results with the proposed method. The paper is overall well presented. I would recommend acceptance of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2914/Reviewer_G9qY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2914/Reviewer_G9qY"
        ]
    },
    {
        "id": "KbuBRyR5_G",
        "original": null,
        "number": 2,
        "cdate": 1666644158019,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644158019,
        "tmdate": 1666644158019,
        "tddate": null,
        "forum": "sCYXJr3QJM8",
        "replyto": "sCYXJr3QJM8",
        "invitation": "ICLR.cc/2023/Conference/Paper2914/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper provides a simple approach to learn a particular style of generative model using a base model as StyleGAN. It learns a small latent encoder embedding network which encodes the normal distribution into the target latent space. It learns using the style loss proposed by Gatys et al and the StyleGAN Discriminator's adversarial loss. The results are comprehensive and the technique is beneficial for face editing applications.",
            "strength_and_weaknesses": "Strengths:\n\n* Simple technique for learning a particular style using a set of few images (~10)\n* Easy to train and the training converges in a reasonable time\n* Provides an alternative to StyleGAN NADA where the style cannot be explicitly described in a few words \n\nWeaknesses:\n\n* The style loss using Gram Matrix only allows particular kinds of texture based styles to be enabled by this technique\n* Geometric transformations such as adding a smile or wink would be harder with this kind of technique.",
            "clarity,_quality,_novelty_and_reproducibility": "The text has been written succinctly and the experiments back the text in terms of clear rationale. The quality of the text is quite high and the process is fairly novel. The techniques have been well described and can easily be reproduced in a reasonable amount of GPU resources. ",
            "summary_of_the_review": "The simplicity of the approach by learning an encoder to transform normal distribution to the appropriate target latent space of the StyleGAN network provides an interesting approach to face stylization. The work provides the right references as well as compares against well known techniques such as StyleGAN NADA on well backed metrics. \n\nThe paper deserves to be accepted to ICLR as it adheres to its high standards. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No Concerns about ethics of the submission.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2914/Reviewer_8gcS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2914/Reviewer_8gcS"
        ]
    },
    {
        "id": "tTpgkKcZI8",
        "original": null,
        "number": 3,
        "cdate": 1666757480298,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666757480298,
        "tmdate": 1668800560889,
        "tddate": null,
        "forum": "sCYXJr3QJM8",
        "replyto": "sCYXJr3QJM8",
        "invitation": "ICLR.cc/2023/Conference/Paper2914/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a method to generate samples from a target domain containing very few shots using a fixed pretrained GAN. ",
            "strength_and_weaknesses": "Strengths:\n\n- concise and straightforward method for performing the task at hand\n- generally well-written and easy to follow\n- compelling results for this very-few-shot domain adaptation generation\n\nWeaknesses:\n\n- The paper feels like it could be a bit shortened. The related work and background are longer than I'd expect, and there are fewer qualitative examples than I'd like in the main text. \n- I am concerned about this method's generalization to higher number of shots. I know other few-shot works use 30 or 100 examples. (I don't mind needing to retrain the networks for each new generation batch). How does this method perform at different shot capacity?\n- I am curious about the effects of latent learner's capacity. What happens with an extremely small model (1 fully connected layer)? When the model is the identity function, do the learned point simply become training set? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear and of good quality. I find it to be easy to read and, as far as I'm aware, it is a novel method.",
            "summary_of_the_review": "The proposed method is simple and intuitive, with strong quantitative/qualitative results. However, the focus on exclusively extremely few shot setting is somewhat limiting, so I advocate for a weak accept.\n\nPost rebuttal: My concerns have been addressed, therefore I increase my score to accept. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2914/Reviewer_ubZS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2914/Reviewer_ubZS"
        ]
    },
    {
        "id": "ZXuxGTWcm9",
        "original": null,
        "number": 4,
        "cdate": 1667429047436,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667429047436,
        "tmdate": 1667429047436,
        "tddate": null,
        "forum": "sCYXJr3QJM8",
        "replyto": "sCYXJr3QJM8",
        "invitation": "ICLR.cc/2023/Conference/Paper2914/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper shows a few shot GAN by embed latent factor into an existing GAN to transfer to a new style. This paper tries to mix pre-trained\nGAN with few shot learning. \n\nTheir contribution can be concluded as: \n-propose a simple procedure to utilize a GAN trained on large-scale source-data to\ngenerate samples from a target domain with very few (1-10) examples.\n- Their procedure is shown to be capable of generating data from multiple target domains\nusing a single source-GAN without the need for re-training or fine-tuning it.\n- Extensive experimentation shows that our method generates diverse and high-quality target\nsamples with very few examples surpassing the performance of the baseline methods.",
            "strength_and_weaknesses": "Strength:\n- Highly achievable and reasonable.\n  Generally the pain part of GAN is that latent space is always not fully exploitable through training. Existing methods focus too much on \nmodel itself but not build on transfer learning. \n\nWeakness:\n- This method still needs some high resolution and complex dataset to demonstrate. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has good clarity and quality.\n\nThe novelty is not high but quite reproducible and applicable. ",
            "summary_of_the_review": "In this work, authors present a methodology to build data-efficient generative models, demonstrating that existing source models can be used as it is (without retraining or fine-tuning) to model new distributions with less data. The method is not very complicate but it shows a \ngood direction and very reproducible work as a direction.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2914/Reviewer_jAjp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2914/Reviewer_jAjp"
        ]
    }
]