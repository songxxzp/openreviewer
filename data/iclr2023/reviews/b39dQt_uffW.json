[
    {
        "id": "INszl_pZkUz",
        "original": null,
        "number": 1,
        "cdate": 1666170019868,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666170019868,
        "tmdate": 1666170019868,
        "tddate": null,
        "forum": "b39dQt_uffW",
        "replyto": "b39dQt_uffW",
        "invitation": "ICLR.cc/2023/Conference/Paper6112/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends SLAC [1] to make it applicable to constrained POMDPs. The proposed approach combines model-free RL with a latent representation that is learned by a generative model. Constraints are satisfied by learning a constraint critic and optimizing a Lagrangian, which boils down to extending the regular SAC objective by another critic component that is weighted by a Lagrange multiplier. The method performs on par with the state-of-the-art on the safety-gym benchmark\n\n---\n\n[1] Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model, https://arxiv.org/pdf/1907.00953.pdf",
            "strength_and_weaknesses": "**strengths**\n\n* The paper proposes a method for safe RL in POMDPs which is competitive with the state-of-the-art.\n* The method mostly consists of previously established work that is combined in a new way. I see this as a strength.\n\n**weaknesses**\n\n* The performance gap to previous work is not clear. As such, the paper is not an improvement over the current state of this setup. This is my main reason for the score I've given to the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "* The authors point out that the pessimistic ensembling techniques used in LAMBDA [2] might be overly pessimistic. While I agree that this is a risk, I would like to point out that LAMBDA is a model-based algorithm where policy and critic are trained on model rollouts. These will have some error, and the pessimism might be necessary in that setup. SLAC, on the other hand uses model-free RL (with the exception of the latent representation) and pessimistic/optimistic approximations might simply not be necessary here. The performance gap between safe SLAC and LAMBDA appears quite small and I don't see a strong evidence to support the claim that \"Safe SLAC is less pessimistic than LAMBDA\".\n* Regarding the performance gap between safe SLAC and LAMBDA, I'm not sure it is a good decision to use Figure 3. It is somewhat difficult to read due to the outlier caused by DoggoGoal1. Figure 5 is easier to read in comparison, though at the cost of removing DoggoGoal1 from the benchmark. Perhaps the authors could give the reader some intuition about the policy's behavior. You are saying that the policy achieves a very high reward by acting unsafely. What does this mean here? Does it completely ignore constraints in a way that would lead to hazardous behavior in a real-life setup? The point is, either the policy's behavior is acceptable or it is not. If it is not, then it shouldn't appear in the comparison, as it makes it look like safe SLAC has a huge performance edge over LAMBDA. If it is, then you should point out why. If it is difficult to judge whether the policy behaves acceptably, I would advise using Figure 5 instead of Figure 3.\n* The authors suggest that using the latent state instead of a history of interactions as input to the policy works better. I can completely see why this would be the case, though I think it should be pointed out that this strategy will not work on any given POMDP, since some POMDPs require the agent to learn to actively acquire new information (e.g. the heaven and hell task from [3]), which is only possible by working with interaction histories (or sufficient statistics thereof) or belief states, which the latent state does not necessarily substitute. I don't see this as a huge issue, since POMDPs come in a large variety, and using the latent state will be a viable approach for many of those, but this shortcoming should be addressed in the text.\n\n---\n\n[2] Constrained Policy Optimization via Bayesian World Models, https://arxiv.org/pdf/2201.09802.pdf\n\n[3] Monte Carlo POMDPs, https://papers.nips.cc/paper/1999/file/299570476c6f0309545110c592b6a63b-Paper.pdf",
            "summary_of_the_review": "This paper is an iteration over SLAC, which modifies that work with a constraint critic, allowing application to constrained POMDPs. The method consists mostly of existing approaches combined in a new form, which has the benefit that each component has a history of related work supporting it. That being said, the method does not have a clear edge over the closest related work (LAMBDA). Thus, I see this paper as a new approach for attaining state-of-the-art performance in a benchmark for safe-RL. I believe this is still a contribution worthy of acceptance, since it will widen researchers' knowledge of which methods work and which do not.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6112/Reviewer_4rUn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6112/Reviewer_4rUn"
        ]
    },
    {
        "id": "bFm9ua5Ip7x",
        "original": null,
        "number": 2,
        "cdate": 1666656696564,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656696564,
        "tmdate": 1669912791276,
        "tddate": null,
        "forum": "b39dQt_uffW",
        "replyto": "b39dQt_uffW",
        "invitation": "ICLR.cc/2023/Conference/Paper6112/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approach for safe reinforcement learning from pixel observations. This is framed as a POMDP problem where the agent receives separate cost signals from the environment in addition to rewards. At a high-level, this work adapts the Stochastic Latent Actor-Critic framework (which learns a latent representation along with a dynamics model in the latent space) to handle safety constraints. Particularly, soft constraints since the agent can violate them at every time step for a cost. Thus the resulting learned policy becomes \"safe\" by minimizing some notion of expected cumulative cost in addition to maximizing the expected return. In this work, safety is considered through the addition of a separate safety critic in the policy objective that predicts the expected cost return given a latent state and an action. ",
            "strength_and_weaknesses": "The trained policy is empirically evaluated on a set of challenging, fairly-recent safety-gym environments (Ray et al. 2019). The method seems to learn safe, competitive policies in most of the environments (with DoggoGoal1and PointPush as exceptions). However, it is difficult to understand whether these policies are any good. A potential quick fix here is to plot a line similar to the budget line on the cost return plots but for the policy. This line would represent the reward return for the worst policy that still solves the problem. Later in the paper, there is a discussion on computational requirements where the running time of the method is compared to that of LAMBDA (As et al. 2022). However, it is not clear to the reader whether or not this difference was due to implementation details or computational complexity.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. Furthermore, the presented modifications to the SLAC framework onto the safety-constrained setting are straightforward, making the method practical. Equations (2) and (3) along with the preceding paragraph could be more explicit. For example, what do the superscripts of the latent variables z_i^j mean?",
            "summary_of_the_review": "This paper presents a straightforward set of modifications to the SLAC framework onto the safety-constrained setting. This would make implementation and benchmarking fairly simple since the environments used are also standard in this setting.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6112/Reviewer_asV6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6112/Reviewer_asV6"
        ]
    },
    {
        "id": "0Pjv7LbeN3",
        "original": null,
        "number": 3,
        "cdate": 1666660100447,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660100447,
        "tmdate": 1666660100447,
        "tddate": null,
        "forum": "b39dQt_uffW",
        "replyto": "b39dQt_uffW",
        "invitation": "ICLR.cc/2023/Conference/Paper6112/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an algorithm for solving safety-constrained decision problems with unknown dynamics that are also partially observable, which is one of the most difficult type of decision problems. The approach is to learn a safety-constrained POMDP model that has latent state, and also includes a safety critic. All components of the model are estimated simultaneously from experienced transitions, by modifying the training objective from only maximizing cumulative discounted reward to also maintain cumulative costs, in the sense of safety violations, under a specified threshold. Empirical evaluation in several environments shows computational advantages with respect to existing baseline algorithms.",
            "strength_and_weaknesses": "The paper attempts to solve one of the most difficult decision problems possible, and achieves favorable empirical results in comparison to existing algorithms. Moreover, it does it by learning a latent state model in the form of a POMDP, which makes the paper very suitable for this conference. Although the verification is in simulation and on relatively simple navigation problems, the dimensionality of the observation space is high, because observations are images, so solving this kind of problems is far from trivial, especially in the partially observable setting.",
            "clarity,_quality,_novelty_and_reproducibility": "The contribution of the paper appears to be novel and original. An anonymized GitHub repository is provided for the purposes of reproducibility. The paper is generally well written, but the notation is quite confusing, and can hardly be understood without referring to other papers. The POMDP model includes several kinds of latent variables z, distinguished by superscripts, which apparently do not mean powers, but something else. It doesn't help that the function p_\\psi is overloaded in five different ways in Equation 2, so it is hard to tell which parameter does what. Similarly, in Equations 4 and 5, it is not at all clear what M is - probably not the model, because the parameters \\psi appear to be defining the model. Is it perhaps a data set of samples? But then, D is used for that in Algorithm 1. ",
            "summary_of_the_review": "This paper appears to be a good contribution to the field of learning and solving POMDP problems, with the further complication of safety constraints. The explanations of the algorithm and the mathematical notation can be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6112/Reviewer_sCDz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6112/Reviewer_sCDz"
        ]
    },
    {
        "id": "tnM6_G3244",
        "original": null,
        "number": 4,
        "cdate": 1666872874381,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666872874381,
        "tmdate": 1666872874381,
        "tddate": null,
        "forum": "b39dQt_uffW",
        "replyto": "b39dQt_uffW",
        "invitation": "ICLR.cc/2023/Conference/Paper6112/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\n\nThis paper extends the stochastic latent actor-critic method to safe reinforcement learning by a safety critic. The experiments show the competitive performance of the proposed methods over existing approaches on benchmark pixel input tasks. ",
            "strength_and_weaknesses": "\n\nStrength:\n\nThe presentation of the paper is very clear. \n\nThe experimental results are also sufficient to show the effectiveness of the proposed methods.\n\n\n\nWeakness:\n\nThe main weakness is that the proposed method is not novel. The author claimed that 3 contributions of the method. They are minor to me. In my view, this paper extends SLAC to safe RL case by introducing the cost mechanism to the prediction target and discounted cost return, respectively. None of them are novel.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n\nThe method of the paper is very clear. \n\nthis paper is a combination of existing methods to me and is not novel.\n\nThe author also presents open-source code which is good for reproducibility.\n\n",
            "summary_of_the_review": "\n\nGenerally, this paper is a combination of existing methods to me and is not novel. The paper takes a lot of content on introducing existing methods (constrained POMDP, SLAC). Using the Lagrange multiplier is also not novel for safe RL [1,2].\n\n\n\n\n\n[1] Geibel, P. and Wysotzki, F. Risk-sensitive reinforcement learning applied to control under constraints. CoRR, abs/1109.2147, 2011. U\n\n[2] Stooke A, Achiam J, Abbeel P. Responsive safety in reinforcement learning by pid lagrangian methods[C]//International Conference on Machine Learning. PMLR, 2020: 9133-9143.\n\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": " ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6112/Reviewer_kqqD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6112/Reviewer_kqqD"
        ]
    }
]