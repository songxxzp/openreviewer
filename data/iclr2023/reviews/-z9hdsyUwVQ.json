[
    {
        "id": "4BaOz3UkhJ",
        "original": null,
        "number": 1,
        "cdate": 1666646498173,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646498173,
        "tmdate": 1666646498173,
        "tddate": null,
        "forum": "-z9hdsyUwVQ",
        "replyto": "-z9hdsyUwVQ",
        "invitation": "ICLR.cc/2023/Conference/Paper508/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes linearly converging variants of NPG and Q-NPG methods, with log-linear policy class and compatible function approximation. The convergence results bare a similar spirit as the recent development of policy gradient methods in the tabular setting. In the stochastic setting, the authors establish an $O(1/\\epsilon^2)$ sample complexity by showing an $O(1/T)$ convergence rate for the policy evaluation. Both of these results seem to improve upon previous results on policy gradient (NPG) method in the linear function approximation setting. ",
            "strength_and_weaknesses": "Strength:\nThe paper is technically correct, and addresses the existing literature sufficiently. In addition to showing an improved convergence rate for the policy optimization, the paper also exploits a faster convergence result for the policy evaluation procedure, which seems the crucial factor in obtaining the final $O(1/\\epsilon^2)$ complexity.\n\nWeakness:\nThe only comment and suggestion that I have is the presentation of assumptions. There are some assumptions that seem less clearly motivated (e.g. Assumptions 3 and 4). Although some of these appear also in prior literature, it would be beneficial to provide some more context on why/where certain assumptions are needed.\n\nMinor comment: The policy evaluation (e.g., Algorithm 6) seems to require the state-action pairs (Line 2, Algorithm 6) to be independent for different timesteps. Can the author comment on the feasibility of using Markovian sampling?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has a clear presentation, with new results on the analysis of policy gradient methods in the linear function approximation setting. ",
            "summary_of_the_review": "I appreciate the technical effort this paper puts into developing new understandings of how natural policy gradient converges with compatible function approximation, yielding a linearly converging policy optimization error and $O(1/\\epsilon^2)$ sample complexity. The analysis bears certain novelty in terms of two perspectives. First, the analysis of policy optimization with compatible and linear function approximation is established, while accounting for both the error in policy evaluation and function approximation error. Second, the policy evaluation analysis exploits a sharper result than the one used in prior literature, which is needed for establishing the $O(1/\\epsilon^2)$ sample complexity. \n\nI believe the paper contains enough new results to the field of policy gradient methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper508/Reviewer_NuUP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper508/Reviewer_NuUP"
        ]
    },
    {
        "id": "s2q-TCYy0F5",
        "original": null,
        "number": 2,
        "cdate": 1666680871557,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680871557,
        "tmdate": 1666680871557,
        "tddate": null,
        "forum": "-z9hdsyUwVQ",
        "replyto": "-z9hdsyUwVQ",
        "invitation": "ICLR.cc/2023/Conference/Paper508/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers infinite-horizon discounted Markov decision process with finite state and action space. By re-formulating the natural policy gradient (NPG) and the Q-NPG methods as approximate versions of the policy mirror descent (PMD) method, the paper shows that both methods enjoy linear convergence rates and \\tilt{O}(1/{\\epsilon}^2) sample complexities with the help of the compatible function approximation. ",
            "strength_and_weaknesses": "Strength:\n  The paper shows the both the natural policy gradient (NPG) and the Q-NPG methods have linear convergence rate and \\tilt{O}(1/{\\epsilon}^2) sample complexities, which is an improvement to the limited tabular setting shown in the previous works.\n\nWeakness:\n  Although the result is new and is an improvement to the existing literature, the technics it uses are very incremental to the existing tabular setting one. By using linear function approximation and reformulating the resulted update rules as approximate versions of the policy mirror descent (PMD) method, it's very incremental to demonstrate the linear convergence rate results in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to read. For the novelty, please refer to my previous comments.",
            "summary_of_the_review": "The paper has improvement in terms of demonstrating the linear convergence rate of the natural policy gradient (NPG) and the Q-NPG methods with the log-linear policy for the linear function approximation setup in the infinite-horizon discounted Markov decision processes. However, the efforts are incremental considering the previous works on the policy mirror descent result on tabular setting as well as the linear function approximation used in similar settings.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper508/Reviewer_sgtm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper508/Reviewer_sgtm"
        ]
    },
    {
        "id": "vkPNsqXubL8",
        "original": null,
        "number": 3,
        "cdate": 1667021499633,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667021499633,
        "tmdate": 1669042234011,
        "tddate": null,
        "forum": "-z9hdsyUwVQ",
        "replyto": "-z9hdsyUwVQ",
        "invitation": "ICLR.cc/2023/Conference/Paper508/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the geometric convergence of NPG and Q-NPG parametrized by log linear function. The authors assume that approximation of Q-functions by linear function, and approximation of policy with log-linear function has some error bound, and they show that employing NPG or Q-NPG with a geometrically increasing step size results in exponential convergence to a ball around the global optimum. This ball is proportional to these function approximation errors. For the Q-NPG algorithm, the authors state their result under two different set of assumptions, and for NPG, the authors consider one set of assumption. Furthermore, by a standard sample based analysis of their result, the authors show 1/epsilon^2 sample complexity of their algorithm, which matches the best known result for Q-learning. ",
            "strength_and_weaknesses": "Strength: \n- The authors consider a wide range of assumptions. \n- For these settings the authors establish a geometric rate of convergence, which is superior to the prior work with function approximation\n- The authors establish a tight 1/epsilon^2 sample complexity\nWeakness: \n- Some of the constants, such as C_v, can be arbitrary bad, and might be in the cardinality of state*action space. Appearance of this constant in the upper bound besides the 1/(1-\\gamma), can result in a very poor bound. \n- The contribution of the paper is limited compared to the prior work, specially Xiao (2022). It seems just a direct extension of Xiao (2022) theorem 10 to the linear function approximation, where we through all the errors due to function approximation to a big constant in the upper bound.\n- The error terms appearing on the upper bound of Theorem 4.7 can be very loose. In particular, we have a very bad dependency in terms of 1/(1-\\gamma). Can the authors comment on (possible) lower bounds of NPG or Q-NPG in terms of 1/(1-\\gamma)?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly, and it is easy to follow. The results seem to be sound.",
            "summary_of_the_review": "I believe the paper is a minor extension of the prior work. The different set of assumptions that the authors consider are not all that different. \n---------------------------------------------------------------------------------------------------------------- \nAfter reviewing the rebuttal, I believe there are interesting points compared to the prior works. Specifically, the extension of this work compared to Xiao (2022) is not direct, specifically for the sample complexity analysis. I am increasing my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper508/Reviewer_Cu4x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper508/Reviewer_Cu4x"
        ]
    },
    {
        "id": "8eK1PwpIJl",
        "original": null,
        "number": 4,
        "cdate": 1667677816757,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667677816757,
        "tmdate": 1668916179113,
        "tddate": null,
        "forum": "-z9hdsyUwVQ",
        "replyto": "-z9hdsyUwVQ",
        "invitation": "ICLR.cc/2023/Conference/Paper508/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the analysis improvement of existing policy gradient methods. The authors apply the recent new analysis of policy mirror descent to natural policy gradient methods ( and its Q-based version ) under log-linear policy class. The provided convergence results improve the state-of-the-art rates, using either adaptively increasing stepsize or constant stepsize. The authors also present a sample-based algorithm and sample complexity which improves previously-known results in the same setting. ",
            "strength_and_weaknesses": "**Strengths**\n\n- The paper studies log-linear policy class for natural policy gradient methods. This linear function approximation setting is important since it is a basic case of widely-used policy gradient methods using general neural policy parametrization\n\n- The authors employ the policy mirror descent analysis from Xiao, 2022 in the tabular case to analyze the log-linear version of natural policy gradient methods in the function approximation setting. This optimization perspective is meaningful to practical RL methods since it offers a more careful characterization of problem-dependent parameters and algorithm stepsizes. \n\n- The established convergence results improve the results from Agarwarl et al. 2021 from sublinear convergence to linear convergence (both up to some errors), with new choices of algorithm stepsizes. Achieving fast convergence is beneficial to training RL algorithms with better efficiency.\n\n\n**Weaknesses**\n\n- The established convergence rates depend on the distribution mismatch coefficients and the concentrability coefficients. Since these coefficients either depend on the optimal policy or the policy iterates that are unknown before training, it is hard to estimate them. It is not very clear how useful these convergence rates are in practice. It is useful to discuss how bad the rate could be, e.g., all these coefficients are chosen in the worst way. \n\n- The distribution mismatch coefficient can be very large in large MDPs even when the initial state distribution is uniform. Large distribution mismatch coefficients lead to poor convergence rates and dependence on function approximation errors, simultaneously. It is useful to discuss the reason for this limitation, and the possibility to address it.\n\n- The established theory seems to be unique to the log-linear policy class. It is more useful if the authors could discuss its extension to the general policy class.  \n\n- The paper hasn't supplied empirical evidence to demonstrate the practical use of the suggested theory. Since function approximation is ubiquitously employed in RL, it is useful to provide experimental results as additional support. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and all claimed results are supported by rigorous proofs. However, since many terminologies are borrowed from the literature, e.g., compatible function approximation, relative condition number, concentrability coefficient, and transfer error, it is helpful if the authors could add more explanations. Also, the comparison with existing results seems to be very technical. It is helpful if the authors could make a table to list all pros/cons for clarity. Although linear rate results are interesting, the improved sublinear rate is also important and I recommend stating it in the main context.  \n\nThe paper builds on the policy mirror descent analysis of Xiao, 2022 while a careful introduction of problem-dependent parameters and function approximation errors could be an additional novelty. The improved linear rates are more expected, but they do advance our understanding of the natural policy gradient method in the function approximation setting. Overall, the technical novelty is average. \n\nFor some other questions, please find them here for consideration.\n\n- Why call Eq. (18) 'approximated NPG' or 'approximated mirror descent'? It can be confused with approximation in the policy update, which is not your case. \n\n- Is the analysis restricted to the log-linear policy? How about other normalization of linear function approximation instead of using softmax functions? \n\n- There is a discrepancy between the transfer error type bound and the approximation error type bound. It is not very clear which one is better. \n\n- How do you choose features that satisfies Eq. (32)? \n\n-  Is there a way to increase stepsize when the discount factor is unknown? ",
            "summary_of_the_review": "The paper studies the natural policy gradient methods ( and its Q-based version ) under log-linear policy class by applying the recent new analysis of policy mirror descent. The established convergence rates improve the state-of-the-art rates using either adaptively increasing stepsize or constant stepsize. However, due to the problem/algorithm-dependence, it is not very clear how useful these convergence rates are in practice. The authors also present a sample-based algorithm and sample complexity which improves previously-known results in the same setting. The paper hasn't supplied empirical evidence to demonstrate the practical use of the provided theory. \n\n============================\n\nPOST-REBUTTAL. Thank you for your response. Since my concerns are addressed in the new draft, I am inclined to recommend acceptance. The score has been updated. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper508/Reviewer_yaHy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper508/Reviewer_yaHy"
        ]
    }
]