[
    {
        "id": "OO0uz4E_kOl",
        "original": null,
        "number": 1,
        "cdate": 1666446504140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666446504140,
        "tmdate": 1666446504140,
        "tddate": null,
        "forum": "t-hNmA0cVSW",
        "replyto": "t-hNmA0cVSW",
        "invitation": "ICLR.cc/2023/Conference/Paper354/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is mostly clearly written. The transformer decoder and the pixel-wise distribution matching loss are adopted in the paper. The results are far better than the-state-of-art methods on four datasets which is quite good.The codes might be released for validation.",
            "strength_and_weaknesses": "Strength: The performance is dramaticly improved.\n\nWeakness:\n1) The N stands for the pixel number in line 189 while it means N regions in 200. It is contradictionary and with no further explanation. v1 and v2 in equation (7) is not explained.\n2) How does the interleaving dual branch structure work in semi-supervied learning. \n3) What's the relation between (a) and (b) in Figure1 . What's the difference in Branch 1 and 2 in Figure 1.\n4) How are the density levels generated from the annotations?\n5)cross attention lacks referred paper before equation (5).",
            "clarity,_quality,_novelty_and_reproducibility": "Most of the work is innovations based on the previous work.Most of the innovations are reasonable.",
            "summary_of_the_review": "It's recommended as an acception if the problems mentioned above are clearly explained.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper354/Reviewer_FHRP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper354/Reviewer_FHRP"
        ]
    },
    {
        "id": "tBxx7RnWCI",
        "original": null,
        "number": 2,
        "cdate": 1666579706586,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579706586,
        "tmdate": 1666579706586,
        "tddate": null,
        "forum": "t-hNmA0cVSW",
        "replyto": "t-hNmA0cVSW",
        "invitation": "ICLR.cc/2023/Conference/Paper354/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this manuscript the authors propose a semi-supervised crowd counting model, which include a pixel-wise distribution matching loss to measure the difference in pixel-wise density distribution between predicted and the ground-truth, density tokens to augment Transformer decoder, and an interleaving consistency self-supervised learning mechanism to efficiently learn from unmarked data.",
            "strength_and_weaknesses": "Strengths:\n(1) Transforming the density map regression task into a density-level classification task in semi-supervised crowd counting is reasonably efficient and alleviates the noise problem in the traditional semi-supervised learning of directly predicting density maps.\n(2) The proposed method achieves good results on several datasets in semi-supervised crowd counting.\n\n\nWeakness:\n(1)In line 127 of page 4, [b1, b2] duplication occurred, which should be a clerical error.\n(2)In line 215 of page 5, it is mentioned that the initialization method of density tokens in this manuscript is compared with randomly initialized queries. Ablation study about the initialization method should add to prove this assertion.\n(3)In Table 1, \u201cThe results of other methods under the 40% labeled setting are referred to (Meng et al., 2021)\u201d. However, according to (Meng et al., 2021), the quantitative results they show for the semi-supervised learning method use 50% of the labeled data. Please check to verify it.\n(4)In Table 1, the methods of comparison were all proposed before 2022. If there are new articles on semi-supervised crowd counting in 2022, please cite them and compare.\n(5)A visual presentation of the predicted density map compared to SOTA's semi-supervised population counting method seems to be missing in this manuscript, please add if space allows. ",
            "clarity,_quality,_novelty_and_reproducibility": "The overall expression of the manuscript is clear and of high quality, and with beautiful figures, but some mathematical expressions are not described clearly enough. The idea of introducing the idea of discrete density classification into semi-supervised population counting is also relatively novel.",
            "summary_of_the_review": "Although there are still some flaws in this manuscript, the ideas are novel and the arguments are basically sufficient. I think it is a high quality manuscript.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper354/Reviewer_ccra"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper354/Reviewer_ccra"
        ]
    },
    {
        "id": "PBGj9VmkPs",
        "original": null,
        "number": 3,
        "cdate": 1666772760228,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666772760228,
        "tmdate": 1670206711949,
        "tddate": null,
        "forum": "t-hNmA0cVSW",
        "replyto": "t-hNmA0cVSW",
        "invitation": "ICLR.cc/2023/Conference/Paper354/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method of semi-supervised crowd counting which only leverages a subset of labeled data and learn from the unlabeled data. This method relies on pixel-wise distribution matching and leverage optimal transport in the optimzation process.",
            "strength_and_weaknesses": "Strength:\n Based on Table 1, it seems that this approach achieve good performance in certain benchmarks.\n\nWeakness:\n1. The idea of leveraging pixel-wise uncertainty/distribution for crowd counting is already proposed in previous work[1], which is not discussed at all.\n\n2. The intervals mentioned in Eq.2 is highly depends on Gaussian kernel value in density generation step and the pixel-wise value of crowd density is generally very small, which makes the chosen of interval values less reliable in different settings.\n\n3. Eq.4 is not Wasserstein distance at all, the real Wsserstein distance is an optimization process that with respect to specific constraint(transport map), thus requires iteration-based solver to get the result, while Eq.4 is a normal L2 norm. Actually, it is higly unefficient for pixel-wise Wasserstein distance as it takes extremely long time for a single forward pass.\n\n4. How to use unlabeled images is not very clear to me. \n\n\n[1]Liu, Weizhe, Nikita Durasov, and Pascal Fua. \"Leveraging Self-Supervision for Cross-Domain Crowd Counting.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n\n------------------------------------------------------------------------------\nPost-Rebuttal Comments:\n\nI would like to thank the austhors provided such detailed rebuttal information, here is my comments after reading all of them:\n\nThe Wasserstein distance does have closed-form solution for 1-d formulation, but I still do not think  Eq.(4) with one-hot vector formulation is the correct formulation of 1-d Wasserstein distance as you mentioned in the reference paper. Besides, I even doubt if this formulation has any advantage over simply soft-max loss. As the code is not available, I'm not able to judge it.\n\nI raised my recomendation considering the rebuttal did address some of my concerns.",
            "clarity,_quality,_novelty_and_reproducibility": "No code available.",
            "summary_of_the_review": "This paper proposes a method to leverage unlabeled image by pixel-wise distribution matching, as I mentioned, I think the methodology is not sound especially the formulation of optimal transport. Therefore, I think this paper is not qualified for ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper354/Reviewer_BsbD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper354/Reviewer_BsbD"
        ]
    },
    {
        "id": "1Y5PNGT0up",
        "original": null,
        "number": 4,
        "cdate": 1666882284884,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666882284884,
        "tmdate": 1666882745588,
        "tddate": null,
        "forum": "t-hNmA0cVSW",
        "replyto": "t-hNmA0cVSW",
        "invitation": "ICLR.cc/2023/Conference/Paper354/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of predicting density maps for counting given semi-supervised datasets.\nIt proposes a novel transformer-based architecture, a distribution-matching loss function for quantized density prediction, the use of two overlapping discretisations and a consistency loss for the two discretisations that is only applied for confident predictions in unsupervised images.\nThe method achieves strong results in both fully and partially supervised settings, achieving state-of-the-art results.\nSome aspects of the design are demonstrated to be important through ablative experiments.",
            "strength_and_weaknesses": "**Strengths**\n\n1. The distribution matching loss is nice.\n1. It's neat to use the uncertainty of the predicted discrete distribution to obtain the mask. I like that the non-confident examples are ignored by the ECR loss.\n1. The supervised loss gives a significant improvement over the baseline methods in the fully-supervised setting (Table 5, appendix).\n1. To me, it was surprising that the ECR loss, despite being simple, gave a large improvement in the semi-supervised setting (Table 2).\n1. The attention map visualizations look great (appendix).\n\n**Weaknesses**\n\n1. The authors argue that predicting a distribution is more credible and less noisy than predicting a single value. However, the targets are always one-hot and the distribution is simply used to parametrize the prediction via its expectation, so it seems like simply an alternative parametrization and loss for predicting a single value. Another plausible explanation is that this parametrization and loss simply improve learnability and/or generalization for training a deep network.\n1. The loss in eq. 4 seems more general than the loss which is actually used, since $\\mathbf{y}$ is always one-hot in practice. Can the loss be simplified for one-hot targets?\n1. The transformer architecture was not well justified, either by arguments or by empirical evidence. It's not clear why it's necessary. The number of categories is fixed, so couldn't we just predict logits for the categories using a conv-net? The appendix includes some variations of decoders. What about the effect of completely removing the decoders and simply comparing the learnt density tokens to the patch features?\n1. The first self-attention within the decoder seems like it could be removed completely? It always takes the same set of tokens as input. Why not just learn the output tokens directly?\n1. Several other ablative experiments are missing. What is the effect of using ECR loss without the mask (or varying the threshold $\\xi$)? What is the effect of setting $\\omega = 0.5$ instead of using the max-norm?\n1. To compare the supervised loss alone to its baselines (Table 3), it seems like it would make more sense to consider the fully-supervised setting, rather than the 5% supervised setting. The fully-supervised comparison in Table 5 (appendix) does not include the CE baseline.\n1. In the comparison of supervised losses (Table 3), Bayesian loss (BL) and DM loss are worse than cross entropy (CE). What could explain this?\n\n**Minor issues**\n\n1. It should be more clear in the main text that \"semi-supervised\" means that the training set is a union of fully-supervised and unsupervised sets.\n1. The description of \"query initialisation\" seemed to refer to the fixed semantic meaning of the tokens, rather than the initial values of the query vectors, which I assume are randomly initialised and trained? This is unclear. Or if I have misunderstood, then the initialisation procedure was unclear.\n1. I felt that absolute error (MAE) alone would be sufficient in the main text, and would make the tables easier to interpret. The squared error (MSE) makes the tables more cluttered and doesn't add much.\n1. It wasn't clear how the 2D Gaussian smoothing was applied. Does this replace the ground-truth point annotations with a 2D Gaussian? How is the sigma chosen?\n1. Why not also use the PDM loss to encourage the two discretizations to have similar outputs? (accounting for the shift)\n1. The fact that the multi-head attention modules have multiple layers (4 layers) was not mentioned until the experimental details.\n\n**Nitpicks**\n\n* \"rationale for\" or \"motivation of\" is more appropriate than \"rationality of\"\n* Several uses of \"forward\" and \"forwards\" as a noun. It's unclear what this means. Model evaluation?\n* Some typos and grammar errors.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is quite clear, although some of the choices needs to be better justified, particularly the transformer architecture.\n\nQuality: The method is well-designed and the empirical evaluation seems rigorous.\n\nNovelty: The method is sufficiently novel.\n\nReproducibility: I believe there are enough details to reproduce the experiments. Code will be released.",
            "summary_of_the_review": "The paper describes an effective loss and parametrization for predicting continuous densities, as well as a simple consistency loss for unsupervised images. The transformer architecture is not demonstrated to be necessary and I wonder whether a simple conv-net would work as well using these loss functions. The ablative experiments should also be expanded, and it would be better to evaluate the supervised loss in the fully-supervised setting. I'm leaning towards accept. I may increase my rating if these issues are addressed or decrease it if they are not.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns",
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "The paper uses datasets containing images of a large number of people, mostly sourced from the internet. (Note that the paper does not introduce any new datasets, it only uses existing datasets.) It seems likely that these people did not consent to their image being used for this purpose. It's also unclear whether the datasets contain a diverse sample of people. Besides these concerns, I don't foresee any harmful impacts of the work itself, since it considers person counting rather than recognition or classification. I'm not an expert in the problem, so I'm not highly familiar with the datasets.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper354/Reviewer_yGdj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper354/Reviewer_yGdj"
        ]
    }
]