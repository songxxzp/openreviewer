[
    {
        "id": "rk7TEsaByGp",
        "original": null,
        "number": 1,
        "cdate": 1666565499442,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666565499442,
        "tmdate": 1668797315180,
        "tddate": null,
        "forum": "6SRDbbvU8s",
        "replyto": "6SRDbbvU8s",
        "invitation": "ICLR.cc/2023/Conference/Paper3205/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes LeMDA (Learning Multimodal Data Augmentation) as a general multimodal data augmentation method. Specifically, the authors design the augmentation transformation as a learnable module applying on the latent representations of the inputs from different modalities. Since the module is end-to-end learned, it is naturally adaptive to different cross-modal relationships and can be applied to any modality combinations. The empirical experiments show that the proposed method consistently boosts accuracy.",
            "strength_and_weaknesses": "The paper flows well. But I am confused at some major settings. First, how $\\mathcal{F}$ and $\\mathcal{G}$ is trained. For the optimization problem definition under section 3.2, do you mean this (as I inferred from the context),\n\nmax_$\\mathcal{G}$ $\\mathbb{E}$\\_{$x\\sim\\mathcal{X}$} (L($\\hat{y}$ \\_{$\\mathcal{G}$})+L\\_{consist}($\\hat{y}$, $\\hat{y}_{\\mathcal{G}}$)).\n\n(sorry there are some formatting issue from OpenReview) I am asking because I do not see why $L(\\hat{y})$ is dependent on $\\mathcal{G}$, and why this is a maxmin problem. If so, the notations in the algorithm and the figure are also wrong.\n\nAlso, for the VAE implementation, do you only adopt the architecture of VAE or both the architecture and VAE loss?\n\nIn practice, you do not have a coefficient to balance different losses? \n\nIn practice, how do you balance the training of the task network and augmentation network? For example, do you iteratively train either of the networks like GAN?\n\nThe experiment results look promising. The authors also provide comprehensive ablation study. \n\nAs an important motivation, it would be better if the claims on the relationships are verified via experiments. For example, what if in other methods/in the proposed method, only one of the complementary modalities is augmented?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: This is a timely work, proposing a data augmentation method under multi-modality setting. The method works well empirically. \n\nClarity: The paper generally flows well, except for some major confusing/missing points.\n\nNovelty: Although the idea of training a separate NN to do data augmentation is not new, the multimodality setting is comparably under-explored yet. \n\nReproducibility: The authors promise they will release the code.\n",
            "summary_of_the_review": "Overall, this is a timely paper with good empirical performance. However, I am not recommending acceptance because I am not sure I am understanding the main method correctly. Please do elaborate in the rebuttal on my questions above.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3205/Reviewer_nNn5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3205/Reviewer_nNn5"
        ]
    },
    {
        "id": "jdgMn47jP4L",
        "original": null,
        "number": 2,
        "cdate": 1666619405011,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619405011,
        "tmdate": 1666619405011,
        "tddate": null,
        "forum": "6SRDbbvU8s",
        "replyto": "6SRDbbvU8s",
        "invitation": "ICLR.cc/2023/Conference/Paper3205/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a Multimodal Data Augmentation model, which can augment multimodal data in feature space.",
            "strength_and_weaknesses": "The idea is interesting. However, the experiment is limited. The authors should consider the multimodal dataset with audio and visual modalities. And other multimodal data augmentation methods should be compared.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is interesting. The writing seems to be good. However, the experiment is limited. The authors should consider the multimodal dataset with audio and visual modalities. And other multimodal data augmentation methods should be compared.",
            "summary_of_the_review": "The idea is interesting.  However, the experiment is limited. The authors should do more experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NONE",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3205/Reviewer_cbnb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3205/Reviewer_cbnb"
        ]
    },
    {
        "id": "Y5S30LOvM-",
        "original": null,
        "number": 3,
        "cdate": 1666648515143,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648515143,
        "tmdate": 1666648515143,
        "tddate": null,
        "forum": "6SRDbbvU8s",
        "replyto": "6SRDbbvU8s",
        "invitation": "ICLR.cc/2023/Conference/Paper3205/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work identifies two critical challenges in multimodal data augmentation: First, Non-trivial augmentation for certain modalities such as numerical or categorical, and preservation of labels when the different modalities are augmented in isolation. This work proposes an easy-to-adapt learnable multimodal augmentation technique( LeMDA) that applies augmentation in the latent space. These augmentations are learned through adversarial training and semantic correspondence is ensured using consistency regularization. \nExperiments on three modalities of images,  text, and tabular data show the approach's effectiveness and the improvement in accuracy with the proposed augmentation.",
            "strength_and_weaknesses": "Strengths:\n+ The method is applicable to any modality. The general framework proposed in this work can be applied across different input modalities. This is certainly a strong point of the work and most of the prior work on multimodal data augmentation caters to specific modalities. \n\n+ Adequate experiments are performed to show the effectiveness of the approach. Experiments are performed for three modalities which is a positive. The performance improves across tasks and modalities. The usefulness of the consistency regularizer is also supported by ablations.\n\nWeaknesses:\n- The model formulation has limited novelty $f_{before}$ and is also present in the other related work [1,2,3,4,5] as pre-trained feature representations much like in this work. $\\mathcal{G}$ is usually a VAE [1,2,3,5] encoding the latents and then the outputs of the decoder are used to minimize a certain loss (a reconstruction for consistency or a classifier in the latent space[1]). What are the crucial advantages of the current approach over these existing methods and how can be term the current approach as an \u201caugmentation\u201d approach since we do not modify the input data? In essence, can we use the above-mentioned approaches to sample many \u201cz\u201d to get the same effect as with the current approach?\n- The architectural details are not provided. What kind of activations and VAEs are considered in work? What is the motivation/experimental reasoning behind using specific architectures? What is the design of the cross-attention module? This needs to be discussed in detail. \nSupport the following claims with an example:\n In related work: With early fusion, we capture the interaction between low-level features useful for multimodal tasks with strong cross-modal correlations.\nThe work in multimodal learning with late fusion is sparsely discussed. For example, the following work is relevant to the paper and should be adequately cited:\n   1. Cross-Linked Variational Autoencoders for Generalized Zero-Shot Learning. Sch\u00f6nfeld et. al. CVPR 2019.\n2. Latent normalizing flows for many-to-many cross-domain mappings. Mahajan et. al, ICLR 2020. \n3. Learning two-branch neural networks for the image-text matching tasks. Wang et al, TPAMI 2019.\n4. Learning robust visual semantic embeddings. Tsai et al, CVPR 2017. \n5. Context object-split latent spaces for diverse image captioning.  Mahajan et al, NeurIPS 2020. \n5 also introduces pseudo-captions for augmentation. How does it compare to the work proposed in the paper?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe design choices such as using a VAE in the augmentation is not adequately discussed. While VAEs are effective, generative models such as diffusion models, GANs and normalizing flows could be suitable for the tasks. The reason for choosing VAEs should be elaborated.\n\nQuality:\nThe paper is well-written and easy to follow and most of the claims are supported by experiments. \n\nNovelty: \nThe aspect of experimenting with more than two modalities with the same network is novel, however, the framework is similar to prior work pointed to above. \n\nReproducaibility: \nThe work is not reproducible with the details provided in the paper. The manuscript mentions that the code will be released upon publication. It is important to provide necessary architectural details in the supplemental if not the main paper.",
            "summary_of_the_review": "Overall, the paper introduces an approach for generalized multimodal data augmentation for any modality. The experiments are performed on various tasks and datasets where the method outperforms the prior state of the art. There are some concerns with the overlapping related work in vision and language (which can be addressed).\n\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3205/Reviewer_huNS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3205/Reviewer_huNS"
        ]
    },
    {
        "id": "BEWr3Ihi3ft",
        "original": null,
        "number": 4,
        "cdate": 1666677546677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677546677,
        "tmdate": 1666677546677,
        "tddate": null,
        "forum": "6SRDbbvU8s",
        "replyto": "6SRDbbvU8s",
        "invitation": "ICLR.cc/2023/Conference/Paper3205/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This manuscript proposed a new data augmentation strategy applied on the feature space for multi-modal classification task. Specifically, the authors applied learnable augmentation network, in the form of VAE, to perturb the encoded embeddings from different modalities. Experiments are conducted on 8 multimodal datasets and compared with the baseline, the proposed augmentation strategy significantly improves the classification results (up to 6%).",
            "strength_and_weaknesses": "# Strength\nThe paper is well-written and easy to follow. The experimental results are strong and the improvements from the proposed method are significant. The ablation studies are well-designed and clearly illustrated the effectiveness of the design choices.\n\n# Weakness\n* I wonder if there are typos on the gradients for updating augmentation network G. Should the gradient be written as -\\partial L(y^hat_G) + \\partial L_{consistent} instead? Updating network G with the gradient -\\partial L(y^hat) doesn't make sense to me as i believe \\partial L(y^hat) \\partial G is zero? (this notation consistently appears in Fig. 2, Alg. 1 and main text).\n* I feel one table illustrating the training cost (FLOPS/Params/CPU wall time) when using different augmentation is necessary to better understand the proposed augmentation strategy.\n* Is the update step on augmentation network G and task network F happening within one iteration (using the same batch of training data) or the updating steps between G and F are alternating as in GAN training? \n* When updating these two networks, is there any hyperparameter that need to be adjusted (e.g. the weight between two losses). If there is any augmentation specific hyperparameter, is it consistently used across different experiments/datasets?\n* The overall improvements brought by the proposed augmentation across different datasets have large variance (from 0.4% - 6%). It would be better if the authors provides more discussions and insights on why the proposed method works better on certain dataset than others.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: There are some questions that I posted on the Weakness session. But overall the idea and logic is clear to me.\nQuality: The written is in good quality and the figures/tables/experiments are well-designed.\nNovelty: I think the proposed meothod is a novel idea.\nReproducibility: The manuscript provided some implementation details and promised to open-source code upon publication.",
            "summary_of_the_review": "Overall I think this manuscript is a good paper. I give marginal accept for now and would like to hear authors' feedbacks on my comments in the weakness session.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3205/Reviewer_PrKb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3205/Reviewer_PrKb"
        ]
    }
]