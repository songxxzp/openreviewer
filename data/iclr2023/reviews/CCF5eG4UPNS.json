[
    {
        "id": "eePrZRDT4aI",
        "original": null,
        "number": 2,
        "cdate": 1666355717762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666355717762,
        "tmdate": 1666549776456,
        "tddate": null,
        "forum": "CCF5eG4UPNS",
        "replyto": "CCF5eG4UPNS",
        "invitation": "ICLR.cc/2023/Conference/Paper4297/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work tries to understand what a particular generative adversarial network (GAN) learns and how we can use a pretrained GAN. Specifically, this model mainly analysis one of the class-conditional GANs named BigGAN with a channel probing technique. Although its findings and application look attractive at first, it is hard to estimate the true effect before comparing them with the previous work or other models that meet the class-conditioning + ReLU requirement.",
            "strength_and_weaknesses": "#### Strength\n- This paper is well-written and easy to follow. \n- Carefully drawn figures help the reader catch what this paper is about and the implications of their findings. \n- Application in various ways, including segmentation examples, sounds interesting to explore.\n\n#### Weakness\n- Although a few methods are mentioned in related work, estimating the performance gap between this method and the rest of the literature is tough. \n- Since this work is mainly based on BigGAN (+ although supplementary presents few details about BigGAN-Deep), it is hard to catch whether this method generally works for other class-conditional GANs beyond BigGAN. ",
            "clarity,_quality,_novelty_and_reproducibility": "#### Major\n1. Generality: This work mainly relies on BigGAN. Since this work is based upon class-conditional (re)scaling + ReLU setting, which is quite general in class-conditional GAN, other pretrained models can be used for applying this technique. It would be great if the author could present the result with a completely different architecture and discuss the similar/different trends between those models.\n\n2. Comparison with other methods: The author claimed that this approach differs from others, but I failed to see any of the baselines in the manuscript. For instance, I think (Wu et al., 2021) for (2) can be compared with this method when we provide a fixed set of classes embedding a specific class. For various experiments, it would be great if the author could present the difference between their approach and others for all experiments.\n\n\n#### Minor\n- Figure 1 caption (line 8): maybe you want to remove the period at the end of the sentence before the parenthesis?\n- Figure 5 caption (line 2): \"e select\" -> \"We select\"\n- Table 1 caption (line 5): \"more ks\" -> how about replacing all k there with $k$?\n- Page 6 line 2: classes(e.g.,  -> classes (e.g.,",
            "summary_of_the_review": "This model tries to understand what class-conditional GAN learns and how we can play on top of the pretrained models. Although the results and the applications are fun to see, I would like to see a more direct comparison with other methods and other GAN models.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4297/Reviewer_vDMD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4297/Reviewer_vDMD"
        ]
    },
    {
        "id": "3nIYr_oyN-q",
        "original": null,
        "number": 3,
        "cdate": 1666588447252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588447252,
        "tmdate": 1666588447252,
        "tddate": null,
        "forum": "CCF5eG4UPNS",
        "replyto": "CCF5eG4UPNS",
        "invitation": "ICLR.cc/2023/Conference/Paper4297/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes channel awareness based on class-conditional batch normalization (CCBN) to study how a single channel contributes to the final synthesis. Specifically, they conduct experiments on BigGAN pre-trained on ImageNet. The authors show that only a subset of channels primarily contributes to a specific category and some channels are shared by similar categories or all classes. Finally, they show the applications by editing specific channels for image editing, category hybridization, segmentation and image synthesis evaluation. \n",
            "strength_and_weaknesses": "### Strengths \nThis paper is well-written and it is easy to follow. \nThe application of category hybridization, segmentation and synthesis evaluation is interesting. \nThe authors promise open-source code. \n\n\n### Weaknesses \n\n**Clarity**\n\nThere are many missing details. For example, \n1. the authors forget to introduce the images that appeared in Figure 1(b). What do they mean? What are the differences between red, green, and blue bounding boxes? \n2. The authors labeled the edited images in Figure 6. Are they labeled manually or by pre-trained classifiers? \n3. The authors should provide more details about how to achieve category hybridization and image editing in Section 5. \n4. In Figure 7, the authors mixed similar categories, e.g., monkey and dog are both animals, How about mixing the monkey and a car? Or mixing a tree and a dog? \n\n**Novelty** \n\n1. There are some missing references that have studied the relationships between the channels and the categories (i.e., semantics), and they also achieved image editing by editing channels [A]. The authors should discuss and compare with them. \n\n[A] Learning semantic-aware normalization for generative adversarial networks. H Zheng, J Fu, Y Zeng, J Luo, ZJ Zha. Advances in Neural Information Processing Systems 33, 21853-21864. \n\n**Quality** \n\n1. The authors study channel awareness only on one model (i.e., BigGAN) on one dataset (i.e., ImageNet), which limits the credibility of channel awareness. We suggest the authors study it on more GAN models and more datasets to verify it. \n2. In Section 3, the authors calculate category-oriented channel awareness on sampled images. How many samples? 1,000 or 10,000 samples mentioned in the implementation details are not convincing enough since ImageNet has 14 million images. \n3. In Figure 7, the authors claimed they perform better style mixing than StyleGAN, however, they didn\u2019t provide the results of StyleGAN for comparisons. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is overall well-written and it is easy to follow. There are some missing details and references that should be provided in the paper. Please find more details in *Strength And Weaknesses.",
            "summary_of_the_review": "The presentation of this paper is clear. My concerns are mainly about the novelty and the quality mentioned in *Strength And Weaknesses. Specifically, the authors conduct experiments only on BigGAN on ImageNet, which is not convincing enough. The authors should also provide comparisons with some SOTA approaches (e.g., StyleGAN). If all the concerns mentioned in Weaknesses are addressed, I will raise my rating. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4297/Reviewer_EESf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4297/Reviewer_EESf"
        ]
    },
    {
        "id": "r4-TN_cX0VI",
        "original": null,
        "number": 4,
        "cdate": 1666643378589,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643378589,
        "tmdate": 1666643378589,
        "tddate": null,
        "forum": "CCF5eG4UPNS",
        "replyto": "CCF5eG4UPNS",
        "invitation": "ICLR.cc/2023/Conference/Paper4297/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the conditioning mechanism of BigGANs, i.e., Class-Conditional Batch Normalization (CCBN), and proposes \"channel awareness\" in order to quantify the impact of each feature channel in the the final synthesis. Using channel awareness, the paper asserts that a) only a subset of channels is primarily responsible for the generation of a particular category, b) that similar categories (such as cats and dogs) share some channels (they are activated jointly), and c) that some channels are \"global\" in the generation  process of all classes. Finally, the paper presents results on a number of downstream tasks (image editing, hybridization between two classes, image segmentation).",
            "strength_and_weaknesses": "Strengths\n---\n\nThe paper presents a very simple method for quantifying the impact of each feature channel in the generation process of a BigGAN (by studying its conditioning mechanism, i.e., CCBN). Its simplicity is its main strength, while the provided applications are useful and potentially useful.\n\n\n\nWeaknesses\n---\n\nMy main point of criticism concerns the generality of the proposed method in conditional GANs that do not use CCBN as a conditioning mechanism. The paper investigates the use of CCNB in BigGANs, but leaves without any discussion another recent and powerful conditional GAN, namely StyleGAN-XL [1]. To the best of my knowledge, BigGAN is the only relatively recent conditional generator to use CCBN as a conditioning mechanism. This renders the paper BigGAN-specific, and potentially reduces its applicability in other class conditional architectures, mainly StyleGAN-XL that gives significantly better results, as far as I know.\n\nMoreover, another weakness of the paper concerns its novelty in the reported applications. The paper states that the proposed method enables several novel applications (such as image editing and segmentation) with conditional GANs, but this is not accurate. Conditional GANs have been studied in terms of finding interpretable directions (e.g., [2,3]), as well as in the applications of image editing [2,3,4] and segmanation [3,5].\n\nFinally, whilst the above applications do seem interesting and the provided method helps towards, the \"key findings\" of the paper, as stated in the manuscript (and listed above), are not surprising or totally new.\n\n\nReferences\n\n[1] Sauer, Axel, Katja Schwarz, and Andreas Geiger. \"StyleGAN-XL: Scaling stylegan to large diverse datasets.\" ACM SIGGRAPH 2022 Conference Proceedings. 2022.\n[2] Tzelepis, Christos, et al.. \"WarpedGANSpace: Finding non-linear RBF paths in GAN latent space.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n[3] Voynov, Andrey, and Artem Babenko. \"Unsupervised discovery of interpretable directions in the gan latent space.\" International conference on machine learning. PMLR, 2020.\n[4] Oldfield, James, et al. \"Tensor Component Analysis for Interpreting the Latent Space of GANs.\" arXiv preprint arXiv:2111.11736 (2021).\n[5] Andrey Voynov, et al. Big GANs are watching you: Towards unsupervised object segmentation with off-the-shelf generative models. arXiv preprint arXiv:2006.04988, 2020",
            "clarity,_quality,_novelty_and_reproducibility": "The work appears to be original, it is well written and easy to follow. The novelty, and potentially the applicability of the paper's ideas are limited (see weaknesses above).",
            "summary_of_the_review": "The main reason I cannot suggest acceptance of the paper at this time is the limited applicability of the proposed method, since it only studies CCBN (BigGAN), but not other class-conditional generators, mainly StyleGAN-XL.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4297/Reviewer_dhhh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4297/Reviewer_dhhh"
        ]
    }
]