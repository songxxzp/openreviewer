[
    {
        "id": "gbz5zK4nBD",
        "original": null,
        "number": 1,
        "cdate": 1666249568163,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666249568163,
        "tmdate": 1666249568163,
        "tddate": null,
        "forum": "3WYtm7UzsR",
        "replyto": "3WYtm7UzsR",
        "invitation": "ICLR.cc/2023/Conference/Paper6472/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a FL framework named F2L. It combines hierarchical network design and knowledge distillation to solve the problem of non-IID of data. Its contributions are threefold:(1) it shows the traditional FL algorithms are unstable for non-IID data. (2) it proposes a new knowledge distillation algorithm Label-Driven Knowledge Distillation. (3) it proposes a new FL algorithm F2L.",
            "strength_and_weaknesses": "**Strength**\n\n(1) it shows the traditional FL algorithms are unstable for non-IID data. (2) it proposes a new knowledge distillation algorithm Label-Driven Knowledge Distillation. (3) it proposes a new FL algorithm F2L.\n\n**Weaknesses**\n\n(1) what is the meaning of the first step of FL, direct LKD may get better results. (2) an additional dataset (Reference Datapool in Fig. 1) is introduced. Whether the distribution of this dataset is similar to the distribution of the teachers' training dataset. The comparison with other methods is not fair and the experimental results are not convincing. (3) lack of explanation of some symbols in the paper makes it less readable. (4) some lines in Fig. 1 are misplaced. (5) some references are missing. Many FL works have introduced KD. For example, \n[1] Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. [2] FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning. [3] FedMD: Heterogenous Federated Learning via Model Distillation. [4] Ensemble Distillation for Robust Model Fusion in Federated Learning.",
            "clarity,_quality,_novelty_and_reproducibility": "See the Strength And Weaknesses.",
            "summary_of_the_review": "See the Weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6472/Reviewer_jW2m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6472/Reviewer_jW2m"
        ]
    },
    {
        "id": "v_ZGXu0TZG",
        "original": null,
        "number": 2,
        "cdate": 1666662518676,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662518676,
        "tmdate": 1666662518676,
        "tddate": null,
        "forum": "3WYtm7UzsR",
        "replyto": "3WYtm7UzsR",
        "invitation": "ICLR.cc/2023/Conference/Paper6472/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a novel FL framework called Full-stack FL (F2L). It aims at solving two important problems in FL - scalability, and robustness in the presence of heterogeneous data. To solve the first problem, they propose a hierarchical design in FL where several smaller FL systems are connected via a global server. To handle heterogeneity, they propose a new label-driven knowledge distillation (LKD) technique at the global server which leverages the advantage of the hierarchy in the design to learn useful information from the sub-regions in the network to achieve fast convergence by reducing the generalization gap between regions. ",
            "strength_and_weaknesses": "Strengths - \n1.\tThey offer to solve the two problems of scalability and data heterogeneity simultaneously.\n2.\tSince a hierarchical model is able to train the sub-regions before doing a global aggregation, the proposed design is claimed to also achieve computational efficiency.\n3.\tThe objective function for online distillation in Eq. (9) is well formulated that includes the goal of reducing the generalization gap between the regions and changes in the global model; this is so that the model does not forget the crucial characteristics of the old global model.\n4.\tEquations (11) and (12) recommend how to set the \\lambda_2 and \\lambda_3 in terms of \\lambda_1. This reduces the number of hyperparameters to be chosen by the user.\n5.\tF2L is compatible and integrable with other FL techniques. \n\nWeaknesses - \n1.\tThe main drawback of the design is that it relies heavily on a centrally available dataset. One of the two primary goals of the system is to handle non-IIDness in the data, which raises the question - how does the performance of F2L depend on the quality of the root dataset at the server. How well does the root dataset represent the non-IIDness present among the clients? How is scalability affected if the root dataset is not updated to well represent the newly joined clients? More experiments are required to convince the reader that the system can do well even when the root dataset does not exactly represent the data distribution among the clients.\n2.\tTable 1 shows that F2L performs significantly better than Fed-Distill. The lower performance of the other benchmarks can be attributed to the fact that they do not leverage any information from a root dataset. What essentially leads to this improvement with respect to Fed-Distill? Do they both use the same root datasets? Is Fed-Distill well tuned for best performance?\n3.\tFigure 2c shows the performance of F2L when a client is injected into the system midway during the training. F2L can be seen to perform better than vanilla FL. Can this be attributed to knowledge distillation? How would it compare with Fed-Distill? How sensitive are the observations with respect to the knowledge distillation parameters - lambda and temperature?\n4.\tF2L relies on switching between LKD and FedAvg after sufficient convergence has happened. How is this threshold chosen? What can be a general way to choose this value for any dataset?\n5.\tFigure 3 shows that a student can outperform a teacher in F2L. This experiment was performed on EMNIST. Does this observation hold in general, independent of the dataset? If not, what conditions does this depend on?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some clarification questions: \n\u25cf\tDoes the performance of F2L depend on the number of regions R?\n\u25cf\tTable 1 provides the results for two extreme values of alpha - 0.1 and 1. It would be helpful if the results are also shown on a non-extreme third value.\n\u25cf\tCould the system be extended to more than 2 levels of hierarchy? Would there be a benefit in doing that?\n\u25cf\tDo Figures 2a and 2b represent training on EMNIST? Fig 2b needs more description.\n",
            "summary_of_the_review": "Both hierarchical learning and KD exist in literature. The authors claim their KD method is novel though. The contributions are significant only if we have a well represented root dataset in my opinion, which may not be able to cope with scalability in practice, which is in fact the main contribution claim of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6472/Reviewer_3LcY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6472/Reviewer_3LcY"
        ]
    },
    {
        "id": "G4k-ss2-e7",
        "original": null,
        "number": 3,
        "cdate": 1666922419975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666922419975,
        "tmdate": 1666922605145,
        "tddate": null,
        "forum": "3WYtm7UzsR",
        "replyto": "3WYtm7UzsR",
        "invitation": "ICLR.cc/2023/Conference/Paper6472/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a hierarchical FL framework with a new label-driven distillation method to handle non-iid FL scenarios",
            "strength_and_weaknesses": "S1. The proposal of a hierarchical structure for FL is reasonable.\n\nW1. The paper is very hard to follow, as a lot of design considerations are proposed, but which parts are the most novel ones are unclear.\n\nW2. The comparison with baselines is unfair. It seems that the authors assume there is some test data in the server so that every region\u2019s model can be validated for each label; then label-driven distillation can be proposed. However, the baselines seem not to have this assumption and can work well without the centralized test data. In other words, the authors\u2019 method uses external knowledge (i.e., server test data), then performance improvement is expected.\n\nW3. In practice, it is very hard to collect test data for the server; even if the server can collect some, such data may be very biased (because many clients will not allow the server to collect data; this is actually why FL is needed). Then, the server\u2019s validation accuracy may be doubtful. Hence, the application of the proposed mechanism in practice is doubtful.\n\nW4. The experiment settings are manually controlled so whether the regional hierarchy can work in practice is still unknown. From my reading of the paper, I think the mechanism may work well when different regions have non-iid samples (the experimental setting); however, if non-iid happens for the clients in a region, the mechanism may still fail. The authors need to use realistic data/region partitions instead manually controlled ones to validate the usefulness of the proposed mechanism.\n\nW5. While the hierarchy structure of FL is sensible, the authors may have real experiments to deploy hundreds of clients in different regions to verify the practicality of the hierarchy structure. Otherwise, how this structure is useful in practice is unclear.\n",
            "clarity,_quality,_novelty_and_reproducibility": "clarity: the paper is a bit hard to follow\n\nquality: the experiments need to be improved\n\nnovelty: the hierarchy structure of FL is somehow new if the authors can really build such an FL system including hundreds of clients distributed in different regions (instead of only simulation in one computer)\n\n",
            "summary_of_the_review": "In general, I think that the hierarchy structure has potential in real FL systems. Meanwhile, the challenge of building such a system may be more in other parts, e.g., communication/node joining & dropping, instead of gradient aggregation. For the label-driven distillation, I think the authors' comparison with baselines is a bit unfair, as the authors' mechanism has more information (i.e., test data in the server).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6472/Reviewer_yujU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6472/Reviewer_yujU"
        ]
    }
]