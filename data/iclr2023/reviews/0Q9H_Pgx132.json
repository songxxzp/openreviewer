[
    {
        "id": "X2z4YR6hdwK",
        "original": null,
        "number": 1,
        "cdate": 1666433830042,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666433830042,
        "tmdate": 1668504488223,
        "tddate": null,
        "forum": "0Q9H_Pgx132",
        "replyto": "0Q9H_Pgx132",
        "invitation": "ICLR.cc/2023/Conference/Paper1123/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends Suzuki\u2019s work (ICLR2018) to parallel neural networks in the Besov space as well as BV functions, studies the generalization properties in terms of approximation error via B-spline basis and estimation error via covering number, and demonstrates the $\\ell_2$ norm (weight decay) in parallel ReLU neural networks is equivalent to a certain sparse regularization term.\n",
            "strength_and_weaknesses": "\n**Pros:** \n\n1. This work allows for over-parameterization without the extra sparsity condition\nThe obtained bound of the covering number is independent of the number of subnetworks, which is substituted by the Frobenius norm.  \n\n**Cons:**\n \n1. The motivation on locally adaptivity makes sense, comparing linear estimators and DNNs. Nevertheless, the motivation on the comparison of (Suzuki ICLR2019)  is relatively weak. The authors claim that Suzuki\u2019s work requires certain conditions on the width, the depth, the sparsity to achieve the minimax rate on deep ReLU networks, which is different from practical uses. Nevertheless, this work does not focus on this issue but centers around another architecture, i.e., parallel neural networks. Even though parallel neural networks have theoretical and empirical applications as the authors suggested, the \u201ccorrect\u201d motivation is to solve certain conditions in Suzuki\u2019s work under the same architecture.\n\nMore importantly, this work also requires several conditions on the width, the depth, etc. In the \u201cNo architecture tuning\u201d part, the required architecture of the model still depends on $\\alpha$, see Theorem 1. It is hard for me to accept the authors\u2019 motivation.\n\n\n2. In theorem 1, the number of subnetworks is exponentially large enough w.r.t the input dimension $d$. This is unacceptable in both theory and practice.\n\n\n3. In Proposition 4, the equivalence between Eq. (2) and Eq. (12) is imprecise via the Lagrangian\u2019s method in optimization. The constraint value P doesn't appear in Eq. (2) and it is also not clear how the dual variable is handled. In fact P (as well as P\u2019) is unknown or uncomputable, which also makes the upper bound of the covering number appear infeasible in Theorem 5 depending on P\u2019. It is ok for ease of description but is not rigorous in a theory paper corresponding to the first contribution.\n\n4. After reading this paper, I think the technical contribution is to derive the covering number bound of the parallel neural networks for the estimation error, i.e., Theorem 5 and Lemma 6. The proof for approximation error follows Yarotsky (2017); Suzuki (2018). However, this paper works under a fixed design, which is much easier to derive the estimation error.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written. Apart from the above mentioned issues, I have other questions that\n\n1. I agree with the authors that kernel methods, e.g., NTK, lacks local adaptivity to some extent due to RKHS. I\u2019m wondering that, the mean field regime (instead of NTK) is also suboptimal to local adaptivity?\n\n\n2. For the random design, the proof of estimation error can be similarly established by [1]?\n\n[1] Schmidt-Hieber, Johannes. \"Nonparametric regression using deep neural networks with ReLU activation function.\" The Annals of Statistics 48, no. 4 (2020): 1875-1897.\n",
            "summary_of_the_review": "This work has some good results on the sparsity, the separation between linear estimators and DNNs in Besov spaces as well as BV functions. Nevertheless, the derived results appear vacuous in terms of imprecise Lagrangian dual and curse of dimensionality of the input dimension. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1123/Reviewer_ikZk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1123/Reviewer_ikZk"
        ]
    },
    {
        "id": "A4eBv7FSV6",
        "original": null,
        "number": 2,
        "cdate": 1666677431707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677431707,
        "tmdate": 1668802036440,
        "tddate": null,
        "forum": "0Q9H_Pgx132",
        "replyto": "0Q9H_Pgx132",
        "invitation": "ICLR.cc/2023/Conference/Paper1123/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper analyzes parallel neural networks for nonparametric regression (including approximation error and regression error analysis). Suppose the regression function has nonhomogeneous regularities, e.g., Besov functions or functions of bounded variation. Parallel neural networks trained with weight decay achieves near optimal estimation, indicating the adaptability of parallel neural networks.",
            "strength_and_weaknesses": "============= Strength =============\n\nThe paper is clearly written with both theoretical and empirical results. The theoretical results are correct and empirical results are supportive.\n\nThe adaptability of neural networks is studied in many recent works, including its adaptability to data geometry and adaptability to structures in function spaces. This paper considers parallel neural networks and uses weight decay for controlling the complexity of the network, which circumvents sparsity constraints in many existing network approximation results.\n\n============= Weakness =============\n\nThere is no related work section. Some of papers share similar ideas to parallel neural networks, for example, \"Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks\" and \"Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks\" both rely on implementing block sparse feedforward networks. The resulting (residual) convolutional network does not need a sparsity constraint in training.\n\nThe obtained rate of estimation is slightly slower than the optimal rate, which is attainable (up to log factors) in many recent works on using ReLU neural networks for nonparametric regression.\n\nFrom experiments, parallel neural networks seem to achieve comparable performance to neural networks and conventional methods.\n  ",
            "clarity,_quality,_novelty_and_reproducibility": "Some of the technical contributions need highlighting. For example, the conversion of weight decay objective to the constraint form in Equation (5) can be highlighted as a contribution -- if it does not appear in Parhi & Nowak (2021a). Another example is the decomposition of \\mathcal{F}_{\\parallel} and \\mathcal{F}_{\\perp}.\n\nA discussion on why parallel neural networks can only achieve the optimal rate in the asymptotic sense ($L \\to \\infty$) is helpful. Is it because the weight decay introduces additional bias?\n\nIn Theorem 1, it is good to state how to choose weight decay $\\lambda$.\n\nLemma 6 is relative difficult to understand, mainly because the first sentence begins with an equation.\n\nOverall, the paper is well written. There are some typos and typeset issues, for example, \"errorand\" --> \"error and\" above Section 4.2",
            "summary_of_the_review": "I think the paper is interesting. I am currently giving a slightly negative rating, due to some concerns of the results and missing references. I am happy to raise to a positive rating after paper revision and an effective author feedback.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1123/Reviewer_wKTh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1123/Reviewer_wKTh"
        ]
    },
    {
        "id": "CYiRC0glX8",
        "original": null,
        "number": 3,
        "cdate": 1666713645510,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713645510,
        "tmdate": 1668794598118,
        "tddate": null,
        "forum": "0Q9H_Pgx132",
        "replyto": "0Q9H_Pgx132",
        "invitation": "ICLR.cc/2023/Conference/Paper1123/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to analyze the theory of deep neural network from the perspective of non-parametric regression. The authors show that training an $L$ layer neural network with weight decay is basically the same as penalized regression. And in the case of functions in the Besov space and bounded variation space, neural networks achieves almost near-optimal rates. When the number of layers increase, the error decreases and thus explains why deep networks performs better than shallow ones. The authors also validate some of their claims using experiments.",
            "strength_and_weaknesses": "Strength:\n  1. The paper is well-written and mostly clear.\n  2. The theoretical analysis provided by the authors are, as far as I am aware of, novel and interesting.\n  3. The obtained results do provide some insights on, for example, why deep networks do no overfit and why deep networks perform better than shallow networks.\n\nWeaknesses:\n  \n First off, I want to say that I am very new to the area of non-parametric regression and the function spaces the authors use in this paper. I have some experience with deep learning theory but this direction is completely new to me. Therefore, I might not be the best reviewer for this paper. Some of my comments might be unfair/wrong and I hope the authors and the other reviewers can help me understand this paper a little more during the discussion period. \n\n  Now back to this paper, I do have some questions about the setting and the results the authors provide. \n  \n  1. The authors have chosen to optimize a set of parallel feedforward networks with $L_2$ regularization (I don't really like using the word \"weight decay\" here because there is no mentioning of the optimizer, e.g. SGD/Adam, and thus weight decay does not really exist). I wonder why the authors didn't include the regularization for the bias in Eqn. (2)? After looking at the Appendix, I think the authors do analyze the network with the bias term, so why is the regularization for bias missing? I think in practice, people have both regularization for weights and bias.\n\n  2. In Theorem 1, I am confused by the statement \"with proper choice of the parameter of weight decay $\\lambda$\". How large does the parameter need to be? Does it need to be time-varying, for example? If the choice of $\\lambda$ needs to depend on the other parameters such as $M, L, p, q$, please state it explicitly in the theorems so that we can understand it more clearly.\n  \n  3. I am a little confused by the results in Corollary 2 and 3. For example, in Cor 2, although the $o(1)$ term is negligible, does it mean that the rate of deep neural network is worse than that of traditional nonparametric regression models? Are there models that can achieve the minimax rates that the authors have state Page 3? I think it might be better to have a table to compare the results in this paper with the results in the literature. Also, what is going to happen if $L > 100 C \\log n$? For very deep neural networks, do the theorems still hold?\n\n  4. For Cor 3, although I know that most of the paper is focused on $L\\gg 2$, the authors might want to mention that the results here fail for $L=1, 2$. Also, the claim that \"DNN has an advantage over kernels\" is a little confusing, shouldn't there be an assumption on $L$ (L> some function of m) so that the bound for DNN is strictly better than that of kernels? I hope the authors can state it more clearly.\n\n  I do not have enough time to go through all the proof details, but the proof overview section does give me enough intuition of how the theorems are derived. The experimental results are a good supplement to the theory.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, novel, and reproducible with the appendix.",
            "summary_of_the_review": "Overall, this seems to be a good paper with nice contributions. However, I am confused by some settings and theoretical results of the authors. Again, I am no expert in this field, and if there is anything wrong in my review, please correct me and I am happy to increase my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1123/Reviewer_YNWp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1123/Reviewer_YNWp"
        ]
    },
    {
        "id": "b9IG_Hd9mS",
        "original": null,
        "number": 4,
        "cdate": 1666840651087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666840651087,
        "tmdate": 1668877486330,
        "tddate": null,
        "forum": "0Q9H_Pgx132",
        "replyto": "0Q9H_Pgx132",
        "invitation": "ICLR.cc/2023/Conference/Paper1123/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper examines training a specific neural network (namely parallel neural net) using weight decay and shows that the error rate matches the minimax bound for a specific class of functions (i.e., functions with bounded variation). \n\nIt looks like a quite interesting effort and the result also appears to be significant. But I worry the paper is not presented at the level for general audience in ICLR. For example, I have a few questions/confusions after reading the submission: \n\n1. What exactly is weight decay? I actually could not find its definition/elaboration except for eq. 2, which is just L2 regularization? I was guessing it could relate to learning rate, momentum, some dynamic shrinkage on the fly...\n2. Do we assume that Eq.2 can be optimally solved? It is a minimax result related to sample complexity so I believe the authors assume that Eq. can be optimally solved --- I think sometimes ignoring computational tractability is fine (and perhaps had been a standard practice in statistics) but it would be helpful to bring this upfront. \n3. What's so special about parallel NN? I think most audience wants to understand at a higher level where the magic happens and why the magic wont happen for the standard MLP, e.g., only limit of existing theoretical tools or parallel NN possesses some special structure? \n----\nThank you for addressing the presentation concern. I have raised my scores to 6 -- do not feel am an expert in this area so no confidence to move to 7. \n\n\n\n",
            "strength_and_weaknesses": "Strength: The authors can show that a specific kind of neural net achieves minimax rate for an interesting function family. Some of the techniques appear to be significant (e.g., hacking into covering numbers). \n\nWeakness: there is room for improvement in presentation. ",
            "clarity,_quality,_novelty_and_reproducibility": "I can mostly follow the paper but was confused by some details from time to time. ",
            "summary_of_the_review": "I think the substance is above the bar of ICLR; the revision addressed my presentation concern. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1123/Reviewer_UB7m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1123/Reviewer_UB7m"
        ]
    }
]