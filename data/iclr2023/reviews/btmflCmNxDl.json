[
    {
        "id": "OBpfGApPbz",
        "original": null,
        "number": 1,
        "cdate": 1666626847968,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626847968,
        "tmdate": 1670759380210,
        "tddate": null,
        "forum": "btmflCmNxDl",
        "replyto": "btmflCmNxDl",
        "invitation": "ICLR.cc/2023/Conference/Paper2411/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a co-operative technique for Generator and Discriminator to cooperate and aid each other instead of the standard adversarial game of of the GAN. The proposed technique uses an augmentation scheme to augment the learning data of the generator and data to aid each other's learning progress. ",
            "strength_and_weaknesses": "Strengths:\n\n* Simple technique for assisting the learning of generators and discriminators using a co-operative technique rather than adversarial technique \n\nWeakness:\n\n* Several cooperative GAN based augmentation systems have been proposed and the novelty is extremely low.\n* The experiments only show some NLP based experiments and no Vision based experiments. \n* Ideally the model should also work with image generation but the conspicous absence indicates the non-generality of the technique\n* The proofs are hand wavy and not concrete. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly clear. The writing can be significantly improved. The quality of the technical writing especially the proofs are significantly lower than ICLR standards. \n\nThe novelty is quite low. Several other works such as https://www.nature.com/articles/s41598-019-52737-x have proposed Data Augmentation schemes with GANs. \n\nThe zip file provided with the submission and the code looks that it can be used to reproduce the experiments easily. There are a bunch of differences that haven't been highlighted in the paper which could be improved in the text. ",
            "summary_of_the_review": "The paper significantly lacks novelty. The proposed technique should ideally also be beneficial to image generation. However, the lack of results raise the suspicion on the generalizability of the technique. The metrics reported on the text based datasets are moderate at best. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No Concerns from my side. ",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2411/Reviewer_YrRh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2411/Reviewer_YrRh"
        ]
    },
    {
        "id": "MN0z08ywamr",
        "original": null,
        "number": 2,
        "cdate": 1666797116864,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666797116864,
        "tmdate": 1666797116864,
        "tddate": null,
        "forum": "btmflCmNxDl",
        "replyto": "btmflCmNxDl",
        "invitation": "ICLR.cc/2023/Conference/Paper2411/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a a self-consistent learning framework to train a discriminator and a generator cooperatively in a closed-loop form. The discriminator and the generator enhance each other during multiple rounds of alternating training. Experimnetal results are given to validate that the proposed framework is easy to train and free from instabilities such as mode collapse and non-convergence. ",
            "strength_and_weaknesses": "Strength:\nThe paper proposes a self-consistent learning framework to enable cooperative training of the generator and the discriminator. The generator and the discriminator are trained alternatively until reaching a score consensus.  This framework makes plenty use of the limited labeled data and large-scale unlabeled domain-related corpus. \n\nWeaknesses:\nThe paper formulates the main theoretical result into theorem 1, but the proof of theorem in the Appendix is not obvious. In particular, how do you derive from the first line optimization problem to the second optimization problem. Could you provide further detailed proof for the theorem?\nThe choice of the dynamic threshold looks quite arbitrary, could you provide some criterion for the parameter selection?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is good, but the proof of the main result needs to be further validated.",
            "summary_of_the_review": "The paper proposes a self-consistent learning framework to enable cooperative training of the generator and the discriminator. The framework can utilize both limited labeled data and large-scale unlabeled domain-related corpus. But the proof of the main theorem still needs to be validated.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2411/Reviewer_8YkB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2411/Reviewer_8YkB"
        ]
    },
    {
        "id": "Kaq4fAMa7h4",
        "original": null,
        "number": 3,
        "cdate": 1666856528489,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666856528489,
        "tmdate": 1666856528489,
        "tddate": null,
        "forum": "btmflCmNxDl",
        "replyto": "btmflCmNxDl",
        "invitation": "ICLR.cc/2023/Conference/Paper2411/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this study, authors proposed a self-consistent learning algorithm to generate data samples to improve downstream performance. The self-consistent learning was designed on generator-discriminator framework which is similar as GAN, but different from that of GAN, the generator and discriminator were cooperatively learned to enhance each other. In the self-consistent learning, authors designed a data selection mechanism to select the generated data with a dynamic threshold, and the selected data were further applied to enhance the generator and discriminator (similar as data augmentation). Finally, authors examined their idea on several sentence semantic matching experiments, and confirmed the effectiveness of their method.",
            "strength_and_weaknesses": "Strength: different from GAN where generator and discriminator are in a competitive learning strategy which is difficult to train, and difficult to use the generator data in further training (it is difficult to judge the quality of the generated data samples ), authors' idea with self-consistency learning could learn both the generator and discriminator in a cooperative way to enhance each other. Therefore, the training is stable and guarantee to converge.\n\nWeakness: The dynamic threshold design as given in Eq. 3 seems important in deciding the selection of the data, however, authors gave a simple incremental formulation in later part for designing the dynamic threshold. And there is no further explanation and ablation study on the design of the dynamic threshold. ",
            "clarity,_quality,_novelty_and_reproducibility": "The purpose and motivation were clearly explained, and is novel as providing a new way for training generator and discriminator with data selection. Authors claimed to open their code and data. Also with the explanation in the paper, the idea is not difficult to reproduce.  ",
            "summary_of_the_review": "The paper proposed a novel cooperative training of both generator and discriminator to generate data for enhance model training. In the model, authors designed a dynamic threshold for data generated data selection in a self-consistent learning framework. Authors experiments showed effectiveness of their idea.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2411/Reviewer_6Yjm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2411/Reviewer_6Yjm"
        ]
    },
    {
        "id": "vUmOlxWsF7",
        "original": null,
        "number": 4,
        "cdate": 1667450468203,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667450468203,
        "tmdate": 1667450468203,
        "tddate": null,
        "forum": "btmflCmNxDl",
        "replyto": "btmflCmNxDl",
        "invitation": "ICLR.cc/2023/Conference/Paper2411/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The work introduced a self-consistent learning framework, where a generator and a discriminator are trained cooperatively and alternatively. A selection mechanism is proposed to filter samples in the training process. The sentence semantic matching task is demonstrated as an example to support the effectiveness of the proposed framework. The results show the framework outperforms standalone discriminative models on four datasets in both zero-shot and full-data settings. ",
            "strength_and_weaknesses": "Strength: The paper is well-organized, and the main ideas are interesting, and easy to understand. The proposed method is evaluated on multiple datasets with good performance in sentence sematic matching. \n\nWeakness: 1. The work included both positive and negative samples in the training of discriminator, and claimed cooperative setting can lead to better training stability than adversarial setting. The idea makes sense but it is not clear to me how this point is supported either in theoretical or experimental aspects. 2. The definition of the output probability for generator $p_G$ cannot be found. 3. It is mentioned in Eq (1) that $p_D$ depends on the embedding representation H of generated samples. Since it is shown in the paper that the selective mechanism is important to the performance, I think it would be helpful to provide more details on this embedding process, and some evidence or explanation to justify the resulting output probability is somehow \u201cgood enough\u201d for selective mechanism. 4. It is mentioned in page 4 that both positive and negative samples would be included in the training of discriminator, (which is a key difference between this method and GAN.) Due to the lack of information on the embedding representation, I fail to see why Eq (2) can make sure of this. ",
            "clarity,_quality,_novelty_and_reproducibility": "In my opinion, the work is written in good quality and clear in general. Some questions mentioned above may need some clarifications. Codes were provided in supplementary materials, so it should be reproducible. ",
            "summary_of_the_review": "The paper includes interesting ideas and shows the proposed approach can improve performance for sentence sematic matching task. It may be more influential if fundamental evidences or analysis are available to compare cooperative and adversarial setting. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2411/Reviewer_r4Wp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2411/Reviewer_r4Wp"
        ]
    }
]