[
    {
        "id": "ZpNFL0NJPY",
        "original": null,
        "number": 1,
        "cdate": 1666554760363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666554760363,
        "tmdate": 1668756602613,
        "tddate": null,
        "forum": "74A-FDAyiL",
        "replyto": "74A-FDAyiL",
        "invitation": "ICLR.cc/2023/Conference/Paper4477/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": " Using KDE queries (as black box), the paper proposes to effciently solve various gram matrix related problems such as spectral approximation/sparsification, simulating random walks, weighted sampling, etc. In each case, a theoretical bound on the approximation/quality along with no. KDE queries is presented. Simulation results on low rank approximation are provided.",
            "strength_and_weaknesses": "Strengths:\n1. The theoretical results in the theorems as well as the remarks are interesting. They also seem to compare well wrt. existing works.\n2. The writeup is very well organized by summarizing the theoretically results and proofs in an intuitive way. I appreciate the discussion in section 1.2\n\nWeakness:\n1. Though I understand the contribution is theoretical, the simulations seem  to be too restricitive. Only low rank based approximations are compared. Also, here, it would have been nice to see a comparison between time vs approximation, without which it is hard to evaluate the improvement.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nI feel the paper is written exceptionally well. Tries to summarize theoretical findings in an intuitive way.\n\nQuality:\nThe work begins with a core idea of using KDE queries and explores how a hist of different problems can be solved. It also seems to comprehensively theoretically contrast with existing results. I feel the technical contributions are strong.\n\n",
            "summary_of_the_review": "In view of the strong theory and nice presentation I tend to recommend an acceptance though I feel simulations section needs a heavy upgrading.\n\n--\nAfter seeing the rebuttal and other reviews, I increase my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4477/Reviewer_dkUW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4477/Reviewer_dkUW"
        ]
    },
    {
        "id": "cEckEAn7eU",
        "original": null,
        "number": 2,
        "cdate": 1666596714565,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666596714565,
        "tmdate": 1668672206479,
        "tddate": null,
        "forum": "74A-FDAyiL",
        "replyto": "74A-FDAyiL",
        "invitation": "ICLR.cc/2023/Conference/Paper4477/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The primary focus of the paper is in the use of KDE queries to speed up many of the primitive operations applied to kernel matrices (Gram matrices) for the purpose of speeding up approximate executions of important linear-algebraic and graph theoretic operations/computations. The fundamental idea is that KDE queries allow for the approximation of the sums of kernel evaluations between an entire data set and a given query point in time independent of data set size and pre-processing which is linear in the data set size*.\n Algorithms are given for how these primitive operations are subsequently applied to important applications in linear algebra and computational graph theory.\n The paper concludes with an experiment illustrating the potential of the proposed method for the ubiquitous Singular Value Decomposition operation.",
            "strength_and_weaknesses": "Strengths:\n- The paper includes a fairly extensive coverage of the important operations required in popular machine learning applications.\n- The content is comprehensive enough that a reader can conceivably extend the algorithms given to any (most?) operations not covered explicitly in the paper.\n- The presentation is reasonably good overall, but I will provide some comments on this going forward.\n- The approach taken in the paper is a natural application of the KDE queries to the problems covered, which makes much of the content intuitive and easier than it would otherwise be to follow.\n\nWeaknesses:\n- There are some issues with clarity (I will provide some examples in the following)\n- I have some, what I believe to be important qualms with the way in which the content is presented. In particular, although there is no apparent incorrectness in the paper, the statements are arguably overblown in respect of their practical relevance. For example, as my closest connections to the content in the paper are from the points of view of spectral clustering and non-parametric smoothing using kernels, it is, in my experience, extremely rare that the scaling parameter (\\sigma in the paper) used in determining the kernel weights/similarities does not depend at least implicitly on the data set size (n). For example:\n-- For most non-parametric smoothing problems one has \\sigma ~ scale(X) n^(-1/(4+d)), where scale(X) is a measure of the scale of the data set. Using this, and for example the Gaussian kernel, one has \\tau ~ exp(-c*n^(2/(4+d))) for c = diam(X)^2/2scale(X)^2, where diam(X) is the diameter of the data set.\n NB: I see that the authors mention that some literature states that often the smallest element in the Gram matrix is a fixed constant, however that does not seem to be consistent with the kernels suggested in the paper unless the data distribution is compact and the scaling parameter does not tend to zero. In addition, I imagine that in general for good performance the bound on tau needs to be very small, and then the question is \"For what sort of data set size do we actually start to realise a substantial improvement in run-time while still maintaining accuracy?\"\n\n\nLet me clarify that I in no way think this invalidates the proposed approach, but I think it is a very important point which is worth discussing and, at the very least noting in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "For the most part the content is clear, and I will provide instances where this is not the case (which I found) in the following section. Overall the paper appears to include a substantial amount of novel and quality work. A conscientious reader should be able to reproduce and even extend the methods and experiments given in the paper, with sufficient effort.",
            "summary_of_the_review": "Overall the paper is reasonably well written and includes potentially influential and important methodology in very relevant areas in machine learning. The theory is comprehensive enough to likely even allow readers to fairly easily extend the general method to operations/problems not explicitly in the scope of the paper.\n\nMy main concern relates to the fact that in many practical situations, the important \"parameter\" tau depends on sample size or likely is very small in practice, but this point seems to be almost glossed over in the paper (as well as the related literature). I think the contribution is significant enough that such a suppression (I do not intend to imply that it is deliberate) is unnecessary, and it is important to clarify/discuss where this is and is not an important factor. We see in the experiment that the performance is good FOR THE GIVEN GRAM MATRIX, however it may be that the scaling used may have oversmoothed the structure in the data to the extent that much of the local information is lost. Again, I do not mean to imply this is the case, only that as a reader one does not know.\n\nBelow I will conclude with some smaller questions/clarifications/suggestions/issues:\n- Using the term sub-linear suggests to a reader, in my opinion, sublinear in the data set size and not sublinear in the size of the Gram matrix. This is unnecessary.\n- Definition 1.1 does not seem to be correctly stated. Surely it should read that the query point y satisfies k(x, y) \\geq \\tau for all x \\in X\n- Theorem 1.3 requires setting/defining \\epsilon\n- Theorem 1.4 needs either to state that v is unit norm, or the LHS in the inequality should be divided by ||v||^2.\n- Sometimes one sees \"... with probability a/b...\" and other times \"... with probability d%...\". Why the inconsistency?\n- Below Theorem 1.5: \\omega has not been defined\n- The statement of Theorem 1.6 is rather clumsy, and could be made clearer. Also, \\epsilon features in the number of queries without being defined and also not present in any of the other terms. I presume it is somehow hidden in a probability somewhere, but the statement is \"with certainty\" and no probability is mentioned.\n- In the paragraph below Theorem 1.6, I presume the authors mean they compute the top few eigenVECTORS of the Laplacian\n- The notation of the edge-vertex incident matrix is quite confusing. Should the subscript x_i not be i? Also, although not ambiguous, subscripting with the edge is not mathematically precise. Is there a way to maybe define H as a map rather than a matrix?\n Further the expression  pi >= (1-\\epsilon)||r_i||^2 = 2(1-\\epsilon)k(x_i, x_j) is confusing since index j has not been defined. I presume first you set j and then can sample the rows with such probability?\n- Am I totally out of date by thinking the Nystrom approximation is a relevant alternative? How does the empirical performance on the experiment in Section 2 compare when using this ubiquitous approximation method?\n- The authors say that in the figure in the experiment, all the points in the true and approximate squared row norms lie close to the y=x line. This is very questionable, especially in the Glove data. Is it not important that lots of the approximate row norms are essentially zero when their actual values are far from that?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4477/Reviewer_vw4c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4477/Reviewer_vw4c"
        ]
    },
    {
        "id": "PXw2psncV2X",
        "original": null,
        "number": 3,
        "cdate": 1666887268360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666887268360,
        "tmdate": 1666887268360,
        "tddate": null,
        "forum": "74A-FDAyiL",
        "replyto": "74A-FDAyiL",
        "invitation": "ICLR.cc/2023/Conference/Paper4477/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies broad linear algebraic and graph processing applications using the kernel density estimation (KDE) framework which provides sublinear-time algorithms in the number of vertices. A core approach is to sample vertices and edges with respect to the (kernel-based) weights using the KDE in sublinear time and construct a sparsified Laplacian, which guarantees a spectral approximation. Such a (spectrally) well-approximated Laplacian provides rigorous guarantees of downstream problems, such as low-rank approximation, graph clustering, and so on. Experimental results on low-rank approximation and graph sparsification are provided. ",
            "strength_and_weaknesses": "Strength:\n\n- The KDE is a fundamental problem in a wide range of areas and this work enhances the impact of the KDE by exploring various practical problems. Moreover, all problems preserve sublinear runtimes that are from the efficient KDE. \n- For efficient usage of the KDE, these works propose a multi-level KDE, which combines a tree structure. This incurs a logarithmic overhead hence sublinear-time advantage is preserved.\n- All the proposed methods use the KDE as a black-box approach, so further improvements on the KDE will automatically improve that of these tasks.\n- All applications have rigorous guarantees, which make the usage of the KDE reliable and much stronger.\n\nWeakness:\n\n- Due to the page limit, the main paper contains the overview only and all key contents are in the appendix. \n- Some of the applications are somewhat straightforward and trivial. So, I think it might be better if the authors reduce all applications to some specific ones, and make the main contributions thin that are not straightforward.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is fairly well-written and the results are very interesting. I think this would shed the light on how the KDE problem can be embedded into various applications while preserving the sublinear time efficiency.\n\n\nSmall typos:\n\n- In Theorem 1.8, \u201c$\\alpha$\u201d should be \u201c$\\alpha_G$\u201d\n- In Theorem 1.8, does \u201c$\\tilde{O}$\u201d mean hiding logarithmic dependency? I think it does not need to use \u201c$\\tilde{O}$\u201d or remove $\\log n$.\n- In page 20, \u201cline 8 or line 11\u201d should be \u201cline 7 or line 10\u201d\n- In line 12 of Algorithm 8, \u201cgraph G\u201d should be \u201cgraph G\u2019\u201d\n- In page 24, the $\\kappa$ should be defined before it is mentioned\n",
            "summary_of_the_review": "The results of this paper are strong and impactful. The paper is well-written and very clear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4477/Reviewer_Eddv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4477/Reviewer_Eddv"
        ]
    }
]