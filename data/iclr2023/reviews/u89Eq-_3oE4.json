[
    {
        "id": "WLQ-_wQi01",
        "original": null,
        "number": 1,
        "cdate": 1666150180087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666150180087,
        "tmdate": 1666152721187,
        "tddate": null,
        "forum": "u89Eq-_3oE4",
        "replyto": "u89Eq-_3oE4",
        "invitation": "ICLR.cc/2023/Conference/Paper2213/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies shot boundary detection, a long-standing problem, in a particular case: short-form videos. The challenges come from complicated shot gradual transitions, vertically ternary structured videos, and intra-shot change in virtual scenes. The authors collect a new public Short video sHot bOundary deTection dataset, named SHOT, consisting of 853 complete short videos and 11,606 shot annotations. Based on this dataset, the paper proposes to optimize the model design for video SBD, by conducting a neural architecture search in a search space encapsulating various advanced 3D ConvNets and Transformers. It achieves higher F1 scores than previous state-of-the-art approaches.",
            "strength_and_weaknesses": "Strength: \n- The paper shows its approach in detail, and the structure is formal. \n- The experiments are tested on different datasets.\n\nWeaknesses: But I have the following significant concerns on quality, and novelty:\n\n- Dataset construction needs several steps, including data collection, annotation, and verification. The authors may need to clarify that the collection process is not challenging. The data may come from web crawling or database accessing if they are from a video company. And at the same time, the copyright is not protected. How to handle and protect the copyright of video creators is questionable.\n\n- The annotation is challenging since it requires frame-wise, but it is not reasonable. The annotation approach that uses a thumbnail cannot provide enough information for the annotators to well handle the challenging cases. Instead, the results may be similar to conventional methods that detect shots based on appearance. The details of the annotation are not clear in the supplementary. How to ensure quality? With double check or voting?\n\n- The method lacks novelty that does not show much difference from the exiting NAS papers on classification. The challenges mentioned by authors are actually very good points for design, e.g., incorporating an adaptive sliding window to handle the gradual transition, or separately handling the background and subject for intra-shot changes, or using rules of thumb for ternary videos. A well-designed lightweight mixture-of-expert network may well handle the challenges. \n\n- The experiments are less interesting that cannot provide insights on the challenges mentioned by the authors, noting that the challenges are the motivation of the authors to bring out a new setting. And the qualitative results are misleading, and it is hard to tell the difference of different methods. And this also adds to my concerns about the quality of annotation.\n\nMinor:\n- What is the dataset scale comparison with the existing datasets?",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the above for clarity, quality, and novelty. For reproducibility, I think a graduate student can reproduce it.\n",
            "summary_of_the_review": "Considering the novelty and quality of this paper, I think it does not reach the bar of the top-tier ICLR conference and vote for rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2213/Reviewer_3zbz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2213/Reviewer_3zbz"
        ]
    },
    {
        "id": "7mEqRHGnrE",
        "original": null,
        "number": 2,
        "cdate": 1666613058814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613058814,
        "tmdate": 1669034955997,
        "tddate": null,
        "forum": "u89Eq-_3oE4",
        "replyto": "u89Eq-_3oE4",
        "invitation": "ICLR.cc/2023/Conference/Paper2213/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "For the task of shot boundary detection, this paper releases a new public dataset SHOT, which is complementary to existing related datasets. In addition, the model design is optimized by conducting neural architecture search in a search space encapsulating various advanced 3D ConvNets and Transformers. Experiments show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1)\tThe newly released dataset SHOT is valuable, which will facilitate the development of community of short video understanding.\n2)\tThe proposed neural architecture search based method is new for shot boundary detection, which outperforms previous SOTA approaches.\n\nWeakness:\n1)\tIt is difficult to understand for the readers without the neural architecture search background. It's technical contribution is limited. \n2)\tThe hyper-parameter sensitivity analysis is missing in the experiments.\n3)\tThe dataset is relatively small, which consists of 853 videos. A larger dataset will be more meaningful. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is novel and reproducible. However, some details of the method are not be clarified, as discussed in Weakness.",
            "summary_of_the_review": "This paper releases a new public dataset, as well as proposes an interesting method for shot boundary detection, which achieves SOTA. It is suggested to accept this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2213/Reviewer_wabR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2213/Reviewer_wabR"
        ]
    },
    {
        "id": "nZL5c6I_3vT",
        "original": null,
        "number": 3,
        "cdate": 1666898587607,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666898587607,
        "tmdate": 1666898587607,
        "tddate": null,
        "forum": "u89Eq-_3oE4",
        "replyto": "u89Eq-_3oE4",
        "invitation": "ICLR.cc/2023/Conference/Paper2213/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new dataset, SHOT, for short video shot boundary detection (SBD). Together with the dataset, the authors propose a baseline model, AutoShot, through NAS among 3D ConvNets and Transformers components. The authors have demonstrated the unique statistics of the SHOT dataset, arguing that SHOT can be used to solve challenges of short video SBD. The AutoShot model is evaluated on three public benchmarks against previous state-of-the-art approaches, and the performance is promising.",
            "strength_and_weaknesses": "Strengths:\n- The dataset fills the blank for shot boundary detection on short videos. Previous datasets do not contain the vertically viewed and short videos which is very popular these days.\n- Apparently the AutoShot model achieves state-of-the-art performance on all three benchmarks, ClipShots, BBC and RAI.\n\nWeaknesses:\n- Size of the SHOT dataset seems small, as it contains 853 videos and most of which are shorter than 1 minute. A few reference dataset:\n  - ClipShots, 4039 videos with 128636 cut transitions and 38120 gradual transitions. Length of most videos is greater than 2 minutes.\n  - Short Video Dataset (SVD, Jiang et al. 2019), over 500,000 short videos and 30,000 labeled pairs of near-duplicate videos. SVD is designed for the video retrieval task.\n- In terms of the proposed model, there is no design that is specifically motivated by the unique challenges that short video pose. Instead, NAS is performed on top of existing state-of-the-art work. In other words, the model design is not well motivated but more through brute-force engineering upon existing models.\n- It's unclear if the challenges mentioned in Figure 1 have been well tackled by the AutoShot model. How well does the best performing model do on the challenging cases described in Figure 1? Are there some qualitative demos on each type of challenge?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. In my view the work is original as there is no existing dataset of such. The authors promise to release the code and data so hopefully the results are reproducible.",
            "summary_of_the_review": "Overall this is a good paper with a hopefully useful dataset that the video community can benefit from, especially due to the overwhelming trend of short and vertical videos. Although obtained by NAS on top of existing modules, the AutoShot model sets new standard of performance on multiple public benchmarks. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "The data are user generated videos, which may inevitably contain private information of personnels. I do recommend that the authors can describe how they handle the privacy issues in the revised version of this paper.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2213/Reviewer_6hJo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2213/Reviewer_6hJo"
        ]
    },
    {
        "id": "NBWwJG94a1",
        "original": null,
        "number": 4,
        "cdate": 1667311982513,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667311982513,
        "tmdate": 1667311982513,
        "tddate": null,
        "forum": "u89Eq-_3oE4",
        "replyto": "u89Eq-_3oE4",
        "invitation": "ICLR.cc/2023/Conference/Paper2213/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is about shot boundary detection of short-form videos (cf. TikTok or YouTube Shorts).\nAccording to the authors, short videos are one of the most prevailing medias in these days and shot boundary detection is a fundamental task for many following semantic understanding tasks.\nMore specifically, the paper carefully motivates the need for an additional dataset for this task, and announces the new dataset \"SHOT\" along with a baseline (AutoShot).\nThe baseline is the outcome of the neural architecture search over a search space spanned by existing architectures (3D ConvNets and Transformers), specialised for shot boundary detection.\nThe such searched model gives a gain of 4% on the new dataset and 1% on other public benchmarks.\n\nThe main contributions of the paper are the new benchmark with a highly competitive baseline (accuracy-wise and compute-wise) found by a neural architecture search.\n",
            "strength_and_weaknesses": "Strengths:\n- Provides a dataset/benchmark for shot boundary detection on short videos. \n- Solid baseline with a model architecture that is specialised for shot boundary detection.\n\nQuestions:\n- Boundary detection is crucial for long videos. Is this still true for short videos or would a less pipelined/more end-to-end approach be more appropriate, the more as \"the frame-wise shot boundary annotation is a heavy task\"?\n- Is this task relevant for a broader/broad enough audience?\n\nOrganisation of paper: \n- Section 3.2: It would be useful if this would be the place with all data-related statistics, including frame size, FPS, type/format of annotation, etc. In the current version, the information is spread across the abstract, introduction, appendix. \n- Section 4: I suggest to move the details on the basic shot boundary detection model from Section 4.2 into a separate paragraph (e.g. 4.0).",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is easy to follow and clearly written (see above for some suggestions for improving the organisation of the paper). \nThe experimental part looks solid.\nThe main novelty of the paper is the new benchmark.\nGiven the fairly involved multi-step pipeline, I wouldn't be confident in being able to reproduce the results from the paper. But the authors state that they will release both the dataset and source code so reproducibility shouldn't be an issue.",
            "summary_of_the_review": "On one hand, the recent popularity of short videos comes with the need for new public benchmarks to facilitate development and comparison of models, and the paper fills in on this. \nOn the other hand, the contribution in terms of modelling, algorithms or analysis is limited.\nWhich makes my recommendation \"borderline\".\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2213/Reviewer_Ar2e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2213/Reviewer_Ar2e"
        ]
    }
]