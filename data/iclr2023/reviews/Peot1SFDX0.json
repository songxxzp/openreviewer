[
    {
        "id": "ItjfYT3dtK5",
        "original": null,
        "number": 1,
        "cdate": 1665739710125,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665739710125,
        "tmdate": 1665740454291,
        "tddate": null,
        "forum": "Peot1SFDX0",
        "replyto": "Peot1SFDX0",
        "invitation": "ICLR.cc/2023/Conference/Paper3720/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce a novel method for Preference-based Reinforcement Learning, which targets the problem of non-Markovian rewards. Namely, that human trajectory preferences cannot be assumed to be based on a sum of Markovian rewards, as expected in common RL and PbRL settings. The method learns non-Markovian rewards, based on the assumptions, that trajectory segment returns can be defined as weighted sum over history dependent state, action rewards. The learning algorithm uses a transformer architecture to these ends. Besides the novel PbRL algorithm, the authors contribute an empirical evaluation, demonstrating the advantages on 5 different domains (with 2-4 variants each).",
            "strength_and_weaknesses": "The consideration of non-Markovian trajectory preferences is an important issue in PbRL, because the Markov property is usually not assumed to hold in this setting. Therefore, non-Markovian PbRL is important to move from synthethic evaluations to real world applications. The paper is mostly well written, but some assumptions are hidden and some clarifications are required. (See next Section) The evaluation covers a sufficiently divers set of domains and real, human evaluations are usually to be preferred over pure synthethic trials. However, the evaluation is lacking in several parts:\n\n- Evaluation is only done in terms of reward. However, user preference may not be aligned with the given reward function. In fact, if it is, the domain does not require a non-Markovian approach. Therefore, evaluation should be performed in terms of preference agreement of the resulting policy. Furthermore, the number of queries is usually also deemed an important metric, because human evaluations are costly.\n- The scripted evaluation uses a Markovian reward, but the NMR and PT approaches are still able to outperform the MR variant in several domains. This needs to be better explain and evaluated. In fact, a non Markovian reward should be used for the evaluation.\n- The RL algorithm used to maximize the reward assumes a common MDP structure. This means, it will likely not correctly consider the non-Markovian property or the weighting factors. Considering it works, it may be that the learned reward \"collapses\" to an Markovian variant (also see next Section concerning this issue). To resolve this issue, it should be validated that the learned reward cannot be approximated with a Markovian definition and/or a PO-MDP or N-state MDP RL algorithm should be used.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned, the authors need to clarify some issues and make some assumptions explicit:\n- $r_t$ is only defined inline in the text, which makes it hard to determine how many states are available for the reward approximation. To my understanding, 1 to H states are available, depending on the position of the state, action pair in the sequence?\n- Compared trajectory segments always have same length (H), but trajectories can be longer than H? This would be a deviation from most other PbRL algorithms and poses some more questions. Namely, usually PbRL usually assumes that compared trajectories are starting in the same state. Otherwise, comparability may not be given, because the frame of reference differs (two different \"problems\").\n- Due to the same length, weighted sum and weighted average are equivalent (up to a multiplicative factor)\n\nThese clarifications are important, because if my understanding is correct, and important issue may arise: For several state, action rewards, no history information is available. This means, the algorithm can only learn a conventional, Markovian reward. In case, very high numbers of preferences are used, this will affect many states, potentially leading to a Markovian solution in general. Therefore, it is a bit in question if the reason for the observed improvements are really due to the non-Markovian modelling. To resolve this issue, all rewards should have access to the same number history states. Formally, this would allow an N-state history MDP formalization.\n\nThe presentation is good, only colors in Fig.4 should not be reused (scripted, human). The algorithm is novel and interesting, especially the combination with a transformer-based model. Reproducibility should be possible, given source code an hyperparameters are given.\n",
            "summary_of_the_review": "The paper is a nice read and the method is novel and interesting. However, the stated issues with a main property of the algorithm prevent an accept. Although, to be fair, the authors claims are not directly related to the property and only refer to \"real human preferences\" and are therefore mostly substantiated. Some issues concerning the evaluation metrics and clarity remain, but can likely be resolved without substantial time effort. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3720/Reviewer_v17x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3720/Reviewer_v17x"
        ]
    },
    {
        "id": "m9Gazucaslw",
        "original": null,
        "number": 2,
        "cdate": 1666671671383,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671671383,
        "tmdate": 1666714282443,
        "tddate": null,
        "forum": "Peot1SFDX0",
        "replyto": "Peot1SFDX0",
        "invitation": "ICLR.cc/2023/Conference/Paper3720/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper describes an approach for learning a reward function in preference-based reinforcement learning based on transformers.  This provides the advantages that the model allows credit assignment within the behavior trajectory to correctly weight significant state/actions, and that less feedback samples that has typically been required to learn the reward.",
            "strength_and_weaknesses": "+The paper is well written\n\n+The transformer approach naturally lends itself to credit assignment, which is important for human preferences as human are significantly influenced by surprising events.\n\n-Given an advantage of this approach is feedback-efficient, it would have been nice to have compared against PEBBLE given that is designed for efficiency.\n",
            "clarity,_quality,_novelty_and_reproducibility": "In Table 1 \u2014 is there an explanation for why NMR performs so poorly in hopper-medium-replay-v2?\n\nYou show the agreement between humans and scripted feedback, but I am wondering about the inter-annotator agreement amongst the authors.  Also, how does the agreement change as the model improves?\n\nHow was the feedback signal formed from the human annotators?  Was it majority vote?  What about ties?\n",
            "summary_of_the_review": "Reward learning for preference-based RL is receiving increasing interest, so this is timely.  Transformers are a natural way to accommodate the problem of credit assignment over a trajectory.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3720/Reviewer_EobQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3720/Reviewer_EobQ"
        ]
    },
    {
        "id": "Sh0Fw9IMO_i",
        "original": null,
        "number": 3,
        "cdate": 1666683324677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683324677,
        "tmdate": 1666683462673,
        "tddate": null,
        "forum": "Peot1SFDX0",
        "replyto": "Peot1SFDX0",
        "invitation": "ICLR.cc/2023/Conference/Paper3720/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Given: just preferences over agent trajectories (and not rewards),\nand the \"underlying\" reward being non-Markovian and weighted.\n\nApproach: the authors propose the Preference Transformer model which learns the \"latent reward\" based on the whole past being fed into a Transformer to output current reward and weight. Then from this reward they can classically train an agent.\n\nThe contribution includes this architecture as well as experiments\n",
            "strength_and_weaknesses": "**Strengths.**\n\nExcept for details, overall the paper is well written.\n\nThe preference RL setting is interesting,\nand,\nif in deed the rewards are non-Markovian,\nthe idea of Transformers is well motivated.\n\nSo there is a conceptual as well as empirical contribution.\n\n\n**Weaknesses and improvement points:**\n\nI think the precise contribution over previous work needs to be clarified:\n* From a first look at the paper, I had these questions: does a preference transformer already exist, but just for Markovian, unweighted rewards, and the contribution is just to extend it to the non-Markov, weighted setting? Or is the contribution, to introduce the preference transformer thing altogether, and directly in the general from (non-M. ...)?\n* When looking into it in more detail, I understand it as follows: it seems (Early 2022) is closest, since they also address preference RL with non-Markov rewards, and with a similar relation between rewards and preferences, but the difference is that (Early 2022) (1) use LSTM instead of Transformers and (2) don't allow for weighing. This, in particular, should be made more clear.\n\nRegarding experiments, there need to be some clarifications, and some results are counterintuitive:\n* For the tasks that are studied, are the true reward functions actually Markovian or not? I don't find that info. Or is there no true reward for some tasks?\n* Sec 5.4: if the reward is actually Markovian, then the much simpler MR baseline model (which assumes a Markovian reward via an MLP) actually has *no approximation error in that sense (i.e., up to functional approximation, the dependencies between the variables can in principle perfectly be captured by this model class)*, but is much simpler in terms of capacity (so, should be more data efficient). I'm really surprised that nonetheless PT outperforms it. Can you explain why PT outperforms it?\n\n**Minor points:**\n\n* Note that non-Markovian rewards can be turned into markovian my changing the state.\n* How does it relate to V/Q value function? It's not necessary, but 1-2 sentences could be interesting.",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "Overall, it is an interesting idea to use Transformers for non-Markovian reward and weighting, as part of preference RL. The writing overall is OK-good, though some clarifications are needed. \n\nThe main weaknesses for me are that the contribution is interesting but rather limited compared to (Early 2022) (replace LSTM by Transformer and add weighting). Additionally, I'm not sure how often actually non-Markovian rewards are needed, since they can be mitigated by good state representations. And, in the experiment, if the reward is Markovian, I didn't understand why nonetheless the non-Markovian method performs better.\n\nWhen ignoring significance/size of contribution, I lean accept; when not ignoring it, I lean reject. In favor of the doubt, overall I lean accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3720/Reviewer_gNsz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3720/Reviewer_gNsz"
        ]
    },
    {
        "id": "P1Ri3kLoGm",
        "original": null,
        "number": 4,
        "cdate": 1666696969912,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696969912,
        "tmdate": 1666696969912,
        "tddate": null,
        "forum": "Peot1SFDX0",
        "replyto": "Peot1SFDX0",
        "invitation": "ICLR.cc/2023/Conference/Paper3720/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a transformer-based neural architecture, Preference Transformer, to model human preferences in preference-based reinforcement learning. Specifically, this work adopts the assumption of non-Markovian rewards and utilizes transformer-based architecture in modeling sequential data to capture temporal dependencies in human decisions and infer critical events in a trajectory.",
            "strength_and_weaknesses": "Strengths:\n1. This paper is well-written and easy to understand.\n2. The proposed new preference predictor that depends exponentially on the weighted sum of non-Markovian rewards is reasonable and effective.\n3. Conducted Experiments are well-designed. Preference Transformer are evaluated both on real human preferences and synthetic preferences. The authors demonstrate the relationship between them and show that existing benchmarks based on synthetic preferences may not be enough.\nWeaknesses:\n1. It would be interesting to see justifications for the architecture selection. Did you compare the GPT with other transformer architectures in experiments?\n2. In Section 5.2, the authors demonstrate that capturing long-term information is the reason why PT is more competitive than other baseline models, but LSTM-based NMR could also capture temporal dependencies. There should be detailed explanations to clarify the demonstration.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and easy to follow.\nThe proposed preference predictor based on non-Markovian assumption is novel and effective.\nThe released code seems easy to run and install.\n",
            "summary_of_the_review": "This paper introduces a novel transformer-based architecture to model human preferences under the non-Markovian assumption in preference-based reinforcement learning. The overall framework seems to be effective and performs well according to the experiment results. Though some of the claims have minor issues, the overall paper is well-written and easy to follow.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3720/Reviewer_WbtG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3720/Reviewer_WbtG"
        ]
    }
]