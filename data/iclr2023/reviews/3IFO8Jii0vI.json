[
    {
        "id": "mpJHUH3x_A",
        "original": null,
        "number": 1,
        "cdate": 1666574248185,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574248185,
        "tmdate": 1666574248185,
        "tddate": null,
        "forum": "3IFO8Jii0vI",
        "replyto": "3IFO8Jii0vI",
        "invitation": "ICLR.cc/2023/Conference/Paper2041/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors deal with the problem of identifying the different regions of space as they are partitioned by the parameters of a ReLU neural network, specifically identifying what combinatorial properties govern the induced partition.\n\nThe authors provide a combinatorial description of the canonical polyhedral complex of ReLU nets and its dual. They rely on something called the \"sign sequence complex\" and use it to determine the facets relations and the locations of vertices of the complex.\n\nHaving established these theoretical insights, they proceed by implementing and cheking an algorithm to obtain the sign sequence complex. The algorithm appears to work for small values of input dimension, and to be polynomial in the #neurons. Furthermore, the algorithm is numerically well-behaved.\n\nThe theory developed also is used to understand some topological properties of ReLU networks and points to the fact that depth is more important than width. (see quick question below too)",
            "strength_and_weaknesses": "Strengths are:\n+ the paper builds upon the theory of fully-connected ReLU feedforward networks and yield interesting insights about the geometry of neural nets\n+ the theory is then used to understand different types of networks, e.g., shallow vs deep, and to identify the exact topology of the linear regions.\n+ the paper can serve as further evidence of the benefits of depth in neural nets\n\nWeaknesses/Unclear things to clarify/Questions:\n- in terms of theory, I am not convinced about the necessity of exponential dependence in the dimension. Is there an example or counterexample that shows the true dependence should behave like this? (maybe there is a simple example that I am missing out)\n- in terms of experiments, I would like to have seen a comparison of the algorithm with simpler algorithms like naive searching of the input space and how it is mapped, or random sampling and tweaking of a starting point moving along some curve or sth. I know these may be trivial baselines, but it's always nice to see \"failure\" cases too.\n-in terms of experiments, can the theory developed be used to learn something interesting about MNIST let's say? or even ImageNET? about the topological properties of the networkds used?\n- I am confused about the 4th bullet in the contributions part of the intro (page 2). The statement that at initialization depth seems to be more important than the #aparams of intermediate neurons, isn't is contradicting recent work of Hanin and Rolnick that is cited in the paper (end of section 4.3)? Can there be a clarification please? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Pretty well-written.\n\nSome typos/clarifications: \n-abstract: \"determines\" should be \"determine\" as it refers to locations I believe.\n-theorems are numbered 14 and 20, but it would be better to have 1 and 2 so that reader is not confused. Also, the numbering is not consistent or consecutive numbers. Eg Lemma 21 appears after Lemma 23 and 22.\n\nOne More Question:\n-Even though this is a nice theoretical paper, I would like to ask whether more applications can be described. Is the insight gained useful let's say for optimization purposes? In conclusion, the paper states generalization which makes sense to me.\n",
            "summary_of_the_review": "Overall, the paper looks at an interesting theoretical question regarding how the different linear regions of a ReLU network are positioned into space, how they intersect or not, what is their boundary. The main result is a characterization and an algorithm that finds the different linear regions. Then this is used to understand topological properties of small networks. I believe the algorithm is nice, and perhaps the one that is the natural algorithm to run in order to obtain the boundaries. (looping over intersections and how they are passed through the net).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2041/Reviewer_Pp96"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2041/Reviewer_Pp96"
        ]
    },
    {
        "id": "j1HjujMgPr",
        "original": null,
        "number": 2,
        "cdate": 1667441427764,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667441427764,
        "tmdate": 1667441427764,
        "tddate": null,
        "forum": "3IFO8Jii0vI",
        "replyto": "3IFO8Jii0vI",
        "invitation": "ICLR.cc/2023/Conference/Paper2041/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents the mathematical theory and corresponding methods for computing a full characterization of all the linear regions of a neural network with ReLU activations. In other words, it provides the structure of the polyhedron associated with each of the linear regions defined by a rectifier network and its lower-dimensional faces, which I understand as a generalization of the face lattice that would describe a single polyhedron (i.e., one linear region). They further show that the union of the vertices across all such polyhedra along with a sign characterization of those vertices with respect to each neuron (-1, 0, +1) is sufficient to determine the facial structure of the same polyhedra. Whereas this can be combinatorially explosive, the authors argue that this can be used to evaluate the linear regions of a neural network with respect to the low-dimensional subspaces of the input.\n\nIn full disclosure, I have previously reviewed this paper when it was submitted to NeurIPS 2022. I do bid to review the same papers when I see potential in them, since I believe this would make the process easier for the authors than starting all over again. Consequently, the wording in my review is very similar to what I wrote before, but I did compare the two versions side by side to make sure my assessment remains the same.",
            "strength_and_weaknesses": "On the one hand, the authors are boldly approaching a rather intractable problem, and one which I believe would help us understand what neural networks encode across all their linear regions. In terms of presentation, I liked how the theorems from the appendix were brought in at the point that they would be helpful to continue the discussion. Moreover, the authors included some missing references that overlapped with their contribution and corrected some issues that I spotted when I reviewed their work previously.\n\nOn the other hand, it is still my impression that this paper expects too much from the reader and cannot be read on its own. I understand that it is hard to do justice to this work in so few pages, but I believe that would have helped to informally explain the main ideas and how they relate before going very technical (even if that means sending more of the theory to the appendix). One interesting example of that done in practice is Boris & Hanin (2019b). I was really hoping that the authors would have altered their paper to be a little more like that.",
            "clarity,_quality,_novelty_and_reproducibility": "In comparison to the version submitted to NeurIPS, I see that there were some significant changes in the second paragraph of the introduction to explain some terms that I could not understand previously. If I am not mistaken, the authors also included a third paragraph after the NeurIPS rebuttal to provide even more context, but this paragraph was not included in this submission. \n\nI have also noticed other minor modifications elsewhere. I understand that it is difficult to start a paper with too many definitions and that it is hard to define everything that might have been used in related work, but I was hoping to see more definitions presented by section 3 rather than the reader still being referred to the appendix for definitions.\n\n",
            "summary_of_the_review": "I anticipate other reviewers objecting to the usefulness of a computationally expensive characterization such as the one studied by the authors, but I do believe this is a necessary step if we want to understand what neural networks represent.\n\nWith that said, I was hoping for deeper changes that could potentially make this paper more accessible to the machine learning community at large, and I cannot say that the current submission is at the stage yet.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2041/Reviewer_6J8n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2041/Reviewer_6J8n"
        ]
    },
    {
        "id": "BH0xwqXWaJ",
        "original": null,
        "number": 3,
        "cdate": 1667576849325,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667576849325,
        "tmdate": 1667576849325,
        "tddate": null,
        "forum": "3IFO8Jii0vI",
        "replyto": "3IFO8Jii0vI",
        "invitation": "ICLR.cc/2023/Conference/Paper2041/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies linear regions of a ReLU neural network, which naturally form a \"canonical polyhedral complex\". They introduce a dual complex, the so-called \"sign sequence complex\", which is also a generalization of activation patterns. They prove that the sign sequences of the vertices of the canonical complex is sufficient to determine the facet poset structure of the sign sequence complex. They develop and implement algorithms for computing sign sequences and the canonical polyhedral complex, which can be used to obtain topological information of the decision boundary.",
            "strength_and_weaknesses": "Strengths\n-------------\n\nThe strengths of the paper is its clarity and its novel approach. They introduce a generalization of activation patterns, which is very elegant due to its simplicity and in the sense that they induce a polyhedral complex that is dual to the canonical polyhedral complex formed by the linear regions. The authors provide code for computing the objects they introduced, and clear instructions on how to use it.\n\nWeaknesses\n-----------------\n\nThe biggest weakness of the paper is that it does not clearly answer the following question: What is the advantage of working with the sign sequence approach and the sign complex S(F) over working with the canonical polyhedral complex C(F)?\nIn detail:\n\n   * There doesn't seem to be an algorithm for computing S(F) without computing C(F). Section 4.1 is titled \"Obtaining the sign sequence complex\", but the algorithm works by computing C(F).\n\n   * The point about the numerical stability is unclear. Section 4.3 states: \"As long as the error in computing solutions to linear equations is small compared to the size of the cells in the polyhedral complex, the proposed algorithm will find the correct sign sequence [...], and [...] the correct combinatorics of the polyhedral complex\". But under the same assumptions shouldn't an approximately computed C(F) also have the same combinatorial structure as the exact C(F)?\n\n   * The point about the algorithmic complexity is unclear. Section 4.3 states: \"the number of possible combinations [...] is also polynomial in the total number of hidden units\". But does that really mean that the algorithm is \"polynomial expected time in the number of hidden units\"? For each combination one still needs to compute the intersection points, and it is not clear whether the expected number of intersection points is polynomial in the number of hidden units?\n\nAdditionally, the paper talks about \"decision boundaries\", however it only considers networks with a single output dimension, i.e., decision boundaries of binary classification tasks. It would be good if the authors could comment how their work can be applied to decision boundaries of non-binary classification tasks. If the work is only restricted to decision boundaries of binary classification tasks, then this should be clearly stated.\n\nA few very minor comments:\n\n   * Section 3.2, first paragraph: double \"a\" in \"may serve as a a labeling scheme\"\n\n   * Section 3.2, first paragraph: missing \"the\" in \"encode all face relationships and [] topology of a network's canonical polyhedral complex\"\n\n   * Section 4.1, computing sign sequences: In Point 1., \"F_ij\" should be \"F_1j\"\n\n   * Appendix A.1, Definition 1: The definition for polyhedron is wrong, it is for open polyhedra and not closed polyhedra. H_i^+ needs to be the euclidean closure of a connected component of RR^n \\ H_i.\n\n   * Appendix A.1, Definition 5: Mention that \\pi_j denotes the projection onto the j-th coordinate\n\n   * Appendix A.2, Definition 11: Some of the indices are wrong:\n     - F^i maps from RR^{n_{i-1}} (by Definition 4)\n     - F_i maps to RR^{n_i} (by Definition 4)\n     - R^(i-1) lives in RR^{n_{i-2}} (by Definition 2)\n\n   * Appendix A.2, Definition 13: Mention that sgn(F_ij(C)) denotes the sign of the linear function restricted to C (and not the sign of the points in the image of C under F_ij)\n\n   * Appendix A.2, Lemma 18 onwards: sign sequences are denoted with a capital \"S\", and not a small \"s\" anymore.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the mathematical portion of the work is great admirable. The paper is both nicely structured and written. The only complaints are minor inconsistencies and typos outlined in \"Strength And Weaknesses\". Additionally, the code comes with basic documentation and clear guidance for both installation and usage. This makes the work easily reproducible. However, some of the claims that are supposed to demonstrate the advantage of the approach in the paper are unfounded or not well evidenced, see also \"Strength and Weaknesses\".\n\nThe reviewer is not aware of any other work discussing the computation of the canonical polyhedral complex using sign sequences.",
            "summary_of_the_review": "The paper describes a very interesting approach to study linear regions of ReLU networks. It is very nicely written, and even closer inspection revealed no mathematical errors in the proofs.\n\nHowever, the claims that advocate for the superiority of the approach in terms of numerical stability and algorithmic are unfounded. This does not mean that the claims are wrong, just that the paper lacks proof or evidence. Additionally, the main part of the paper restricts to decision boundaries of binary classification tasks without ever mentioning it explicitly in the introduction.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2041/Reviewer_3fLq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2041/Reviewer_3fLq"
        ]
    }
]