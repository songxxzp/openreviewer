[
    {
        "id": "TaHNmGFHzk",
        "original": null,
        "number": 1,
        "cdate": 1666454539834,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666454539834,
        "tmdate": 1671342923949,
        "tddate": null,
        "forum": "QwKvL6wC8Yi",
        "replyto": "QwKvL6wC8Yi",
        "invitation": "ICLR.cc/2023/Conference/Paper2202/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper gives an improved one-shot coreset selection method for classification problems. A one-shot coreset is a subset of the training set, and the goal is to find a coreset/subset of a given size such that the training error on the coreset is minimized.\n\nThe paper first studies the ability of the coreset to \u201ccover\u201d the data distribution, and justifies why this is important. To this end, the notion of partial cover was proposed. Roughly, a subset S is a p-partial r-cover for some distribution, if the total probability measure of the union of all radius-r neighborhoods around each point in S is p. This generalizes a previous notion which is essentially p = 1. In Theorem 1, this paper proves that if a coreset is a p-partial r-cover, then it has a bounded error w.h.p. on the training set (where the randomness comes from the training set). Hence, this is a valid error measure. To further stabilize the measure, the AUC (area under curve) measure is considered, which may be viewed as an average/overall measure of the radius r, over all cover percentage.\n\nThe paper observes that the AUC of several existing coreset algorithms is worse than a naive uniform sampling, when the pruning rate is high (i.e., 90%). The focus is thus to propose a new algorithm that improves the performance when the pruning rate is high. The key observation is that the previous results drop points too aggressively under high pruning rate. Hence, given a set of importance score of data points, the new algorithm, called CCS, first excludes a \\beta fraction (which is smaller than the total pruning rate) of the hardest examples (according to the score), and then picks the data points to be kept in a somewhat uniform way.\n\nExperiment results show that the proposed method has a significant improvement over previous coreset methods when the pruning rate is high, regardless how the importance score is defined (tested on at least 4 different scoring schemes), while still matches, or slightly outperforms, the existing methods when the pruning rate is low.",
            "strength_and_weaknesses": "# Strength:\n\nThe technical idea and the reasoning is well grounded by theoretical and empirical analysis, and the comparison with previous methods seem to be comprehensive, which is convincing. The new algorithm makes sense, and the empirical result shows a significant improvement over previous results.\n\n# Weakness:\n\nThe reasoning generally makes sense, but I do find a few important aspects not discussed (I might be wrong since I mostly work on coresets for clustering instead of classification). In particular:\n\n1. From the literature of coresets for clustering, once importance score is given, the next step is usually importance sampling: sample each point x in the data set with probability *proportional to its importance score*. Note that this distribution can sample both easiest and hardest points, and more importantly, if the easy points are a lot, each of them is sampled with small probability but the combined probability of sampling an easy point can be very significant. Hence, this seems to resolve the issue that you mention, where easy points tend to be very \u201cdense\u201d, and simply focusing on hard points may miss them. Unfortunately, I don't see this very natural approach discussed/reflected in the paper.\n\n2. Actually, I don\u2019t find any reference about coresets for clustering (and related problems), especially those using importance sampling. Consider to cite and discuss the typical ones, such as\n\n* Turning Big Data Into Tiny Data: Constant-Size Coresets for k-Means, PCA, and Projective Clustering, Feldman et al, SICOMP 20.\n\n* A unified framework for approximating and clustering data, Feldman and Langberg, STOC 11.\n\n3. The entire paper seems to assume the importance score is given which can be arbitrary. However, the quality of the importance score can also greatly affect the performance, but this does not seem to be modeled/considered in the algorithm? Moreover, I find it unclear whether improving the sampling algorithm or improving the importance score is the most significant issue of the existing algorithms. For instance, it is a fact that the previous works perform worse than random sampling at 90% pruning rate, but why this has to be caused by a bad sampling algorithm? What about using an improved importance score? ",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\n\nThe paper did a comprehensive comparison to directly related works. Key assumptions and choice of parameters are discussed, and sufficient technical details have been provided. However, as mentioned in \"Weakness\", the comparison to the study of coreset and importance sampling techniques in general is poorly covered.\n\nMinor comments:\n\n1. Section 4, paragraph \"Baselines\". If I understand correctly, many of the listed algorithms are not really \"baselines\", since they define how the importance score is picked which is your input, instead of how the coreset is chosen which is your algorithm. Please clarify.\n\n2. In the description of Algorithm 1, it's better to state that \\beta \\leq 1 - \\alpha (if I get it correctly).\n\n3. In Section 2.1, you used \"b\" to denote the size of the coreset. However, in other places we are mostly talking about \"pruning rate\". What about consistently mentioning one of these, throughout the paper?\n\n# Quality\n\nThe empirical improvement over baselines seem to be significant. The writing quality is excellent. The claims are grounded by either theoretical or empirical analysis. However, the theoretical analysis, Theorem 1, seems to be weak, particularly that it is only about the \"ideal\" case which the algorithm may not achieve. The (worst-case) guarantee of the algorithm is not analyzed at all.\n\n# Originality\n\nThe main novelty is a new sufficient condition for a good coreset (which can also be used to evaluate a coreset selection method empirically), plus a new sampling scheme that outperforms existing results in high pruning rate. However, both are mostly based on existing methods, and I don't find it requires significant modifications. But indeed, the exact combination of techniques may be nontrivial to discover, especially considering that it performs well in empirical evaluations.",
            "summary_of_the_review": "This is a nice paper. However, I would suggest a weak reject for now, due to the concerns that I raised in the \"Weakness\" part. They can significantly affect the justification of novelty of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2202/Reviewer_6QHN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2202/Reviewer_6QHN"
        ]
    },
    {
        "id": "6M9VU77_G_",
        "original": null,
        "number": 2,
        "cdate": 1666581037386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581037386,
        "tmdate": 1669493090811,
        "tddate": null,
        "forum": "QwKvL6wC8Yi",
        "replyto": "QwKvL6wC8Yi",
        "invitation": "ICLR.cc/2023/Conference/Paper2202/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study a known shortcoming of the SoTA coreset selection methods, which they call \"catastrophic accuracy drop\". The SoTA methods usually perform well under low-pruning regimes, but at high pruning rates they work even worse than random sampling.\n\nThe authors hypothesize that this is due to reduction in \"data coverage\" at high pruning rates. As such, they define \"p-partial r-cover\" for a selected subset and based on it propose the AUC_pr metric to quantify coverage. Then using this new metric they show that the SoTA methods indeed suffer from low data coverage at high pruning rates and AUC_pr of a selected subset correlates well with the performance of the models trained on it.\n\nFinally, they propose a method, called Coverage-centric Coreset Selection (CCS), which uses stratified sampling over a given scoring mechanism. This method can complement the existing score-based pruning methods. They then experimentally show that their method can highly improve the existing SoTA methods at high pruning rates, both based on their proposed AUC_pr metric and the actual final model performance.",
            "strength_and_weaknesses": "Strengths:\n- The argument is well-grounded: it starts with an observation, then a hypothesis is presented, a metric is proposed to validate the hypothesis, and finally a method is proposed that empirically fixes the issue\n- The improvements are quite large: the proposed method not only improves the SoTA at high-pruning rates, it beats the random baseline, which has been very strong at high-pruning rates until now, by a large margin\n\nWeaknesses:\n- The final method seems hacky and looking more closely, I'm not convinced that it does address the \"coverage\" issue directly. I can see how this method is way better at getting a distribution with higher coverage than the existing \"hard cut-off\" method. Anyway, maybe I'm expecting too much from the paper\n- Relatedly, the method requires some hyper-parameter tuning, but the authors show that it's mostly invariant to one parameter and provide some intuition as to how to tune the other\n- The method is only tested on datasets that are both small and fairly similar. Having results on ImageNet would make the results way more convincing.\n\nQuestions:\n- The way I see it and by looking at Figure 2, AUC_pr metric might be dominated by the \"r\" values of the high coverage (high \"p\") regions. Do you think some normalization could be applied here to remove this bias or is that not necessary?\n- I'm totally confused about the way CCS is represented in Figure 2. Looking at Algorithm 1, I'd expect CCS to have some a peak on the right side (i.e. hard cases that are removed on line 2 of Alg. 1) and then I'd expect CCS to have a more-or-less uniform distribution for the rest of the data (i.e. to be a scaled version of \"all data\") as you claim to take samples from all strata, not just a few. I'm not sure what I'm missing here.\n- You show that the value of k is not important, but only show it for k>25. If you set k=1 and beta=0, do you recover random sampling? Because if you claim that k is not important, the only difference with random sampling would be just that you prune the hard samples. Is that correct?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the hypothesis and the metrics are well-defined, but I find the CCS method needlessly complicated. Also my understanding of Algorithm 1 does not align with how it is portrayed in Figure 4.\n\nQuality: the paper is of high quality, the reasoning and the metrics as well.\n\nNovelty: the analysis and the fix are both novel, and the improvements are quite large.\n\nReproducibility: the code is provided, which is great (I did not test it for myself, though).",
            "summary_of_the_review": "The authors' argument and observation seems quite valid and well-supported, but I'm confused by their method. I believe the AUC_pr metric could be quite valuable to the community, either in its current form, or an improved version of it. It will allow further research to take into account an important aspect that has not been easily addressable before.\nAs for the method, the improvements are quite large, but I have a hard time figuring out how the stratified sampling part is different from normal random sampling. I would've also like to see some evaluation of the method on a dataset such as ImageNet.\n\nThe main concerns that I have are that: 1) the proposed method seems a bit complicated/hacky and doesn't directly address the issues that are raised; and 2) I see some conflicts between Figure 4 and Alg 1.\n3) I believe the metric might also need some adjusting in the future, but currently it should be adequate.\n\nWhatever my reservations about the CCS method itself, the results are better than I expected.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2202/Reviewer_17rD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2202/Reviewer_17rD"
        ]
    },
    {
        "id": "-H5rGRV0U8W",
        "original": null,
        "number": 3,
        "cdate": 1666699685147,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699685147,
        "tmdate": 1666793461030,
        "tddate": null,
        "forum": "QwKvL6wC8Yi",
        "replyto": "QwKvL6wC8Yi",
        "invitation": "ICLR.cc/2023/Conference/Paper2202/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries to ameliorate the serious performance degradation when a substantial portion of data is pruned. It is argued that the problem is caused by poor coverage of the selected subset. Then a new data selection strategy CCS is then proposed. CCS jointly considers sample coverage and the informativeness of each example. The effectiveness of CCS is validated on Cifar-10 and Cifar-100.\n",
            "strength_and_weaknesses": "Strength:\n\n1. Selecting subsets to retain the full training performance is important for efficient training, hyperparameter tuning as well as. The problem this paper trying to address is important.\n\n2. The authors do a good job presenting their intuitions developing CCS.\n\n3. The proposed method seems to be technically sound. Theoretical analysis on the risk of training only on subset is provided.\n\n4. The paper is clearly written. Experiments and ablation studies are well performed.  Many details, including the implementation are provided for reproducing. \n\nWeakness:\n\n1. Coverage, or \u201cdata representativeness\u201d is widely discussed in the very related active learning literature [1][2]. Active learning shares a lot in common with data selection considering scoring examples and maintaining representative samples. In this field, informativeness (data importance) and representativeness (coverage) are explicitly formulated as two main considerations. This undermines the novelty of this work. \n\n2. There lacks experiments on large scale dataset like ImageNet. The selection strategy behaves pretty inconsistent on ImageNet and Cifar-10, results on small data may not be reliable enough. Besides, ImageNet experiments are commonly conducted in recent data selection methods [3,4,5], so I recommend experiments on ImageNet to further strengthen the paper. \n\nReferences:\n\n[1] J T. Ash et.al. Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds. ICLR \u201820 \n\n[2] G.Citovsky et.al. Batch Active Learning at Scale. NeurIPS \u201821 \n\n[3] K. Killamsetty et.al. GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training. ICML\u201921\n\n[4] Ben Sorscher et.al. Beyond neural scaling laws: beating power law scaling via data pruning. arXiv: 2206.14486\n\n[5] Cody Coleman et.al. Selection via Proxy: Efficient Data Selection for Deep Learning. ICLR\u201919\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the proposed method is somewhat novel.",
            "summary_of_the_review": "The paper is overall well written, however, the problems mentioned in the weakness part undermine its significance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2202/Reviewer_Vae7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2202/Reviewer_Vae7"
        ]
    },
    {
        "id": "phM2B0BRxt",
        "original": null,
        "number": 4,
        "cdate": 1667593397433,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667593397433,
        "tmdate": 1667593397433,
        "tddate": null,
        "forum": "QwKvL6wC8Yi",
        "replyto": "QwKvL6wC8Yi",
        "invitation": "ICLR.cc/2023/Conference/Paper2202/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the following problem. Given a set of training examples, select a smaller subset of it which minimizes the expected model error. The goal is that the smaller subset can be used to train a different model with minimal drop in accuracy in comparison to training the model using the entire training dataset.\n\nThe SOTA approaches solve the problem in the following way: They compute an importance score for each of the training example for the given model. The examples with smaller importance score are pruned as they are not important enough for learning of the model. The importance score is computed in different ways. 1/ Forgetting score (Toneva et al., 2018) is defined as the number of times an example is incorrectly classified after having been correctly classified earlier during model training. 2/ Area under the margin (AUM) (Pleiss et al., 2020): AUM represents the probability gap between the target class and the largest other class across all training epochs. A larger AUM means higher difficulty and importance. 3/  EL2N (Paul et al., 2021): EL2N scores estimate data difficulty by the L2 norm of error vectors. Examples with large difficulty score are more important. 4/ Entropy (Coleman et al.,2019): The entropy of outputs reflects the uncertainty of training examples, and a high entropy indicates an example containing more information and is more important.\n\nThe above described methods beats the baseline random selection method in moderate pruning regime but performs poorer than random method in high pruning regime, say 90% pruning.\n\nThe paper identifies that poor performance of the SOTA methods in comparison to random selection is because SOTA methods prune all the easy examples which correspond to the high density region of training examples. Hence the paper proposes stratified sampling instead of pruning all the low-score examples. It implemented stratified sampling on top of each of the SOTA methods described above and shows that it significantly improves performance in high pruning regime in comparison to the low-score pruning of the SOTA method.\n\nThe paper also extends the Lipschitz constant based bounds on pruning based loss function when pruning is done using core-set radius r, to the p-partial r-cover based bounds, where radius r covers only p fraction of the probability density space.\n",
            "strength_and_weaknesses": "Strength: A well defined practical problem is solved using a simple method. An investigation of the SOTA methods revealed the problem with the existing approaches in high pruning regime, and a simple stratified sampling based fix solves the problem. Though the theory is not directly relevant to the proposed approach and is not innovative, but the extension of r-cover core-set bounds to the p-partial r-cover bounds does show the tradeoff between p-partial cover and r-radius cover. The numerical experiments show a significant improvement over the baseline methods in the high pruning regime.\n\nWeakness: The paper lacks novelty, and the given theory is very tangential to the proposed solution. A theoretical bound on loss function for stratified sampling in combination with any of the SOTA importance score estimation method would be more useful.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and its original as far as application of stratified sampling to the pruning of training examples in combination with importance score is concerned. However, the stratified sampling per-se is a common approach in this domain.",
            "summary_of_the_review": "The paper provides a simple approach for a well defined problem in high pruning regime. However, the approach lacks novelty and the provided theory is very tangential to the proposed approach.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2202/Reviewer_f2Gh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2202/Reviewer_f2Gh"
        ]
    }
]