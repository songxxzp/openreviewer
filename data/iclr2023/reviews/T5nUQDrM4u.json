[
    {
        "id": "IzLVBLJaiX",
        "original": null,
        "number": 1,
        "cdate": 1666648768067,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648768067,
        "tmdate": 1670015892107,
        "tddate": null,
        "forum": "T5nUQDrM4u",
        "replyto": "T5nUQDrM4u",
        "invitation": "ICLR.cc/2023/Conference/Paper4822/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes sparse upcycling -- copy the model into a sparse model for faster and more efficient training. They conduct experiments on JFT-300M and English C4 dataset. The empirical results show its superiority.",
            "strength_and_weaknesses": "The paper proposes a training strategy to make MoE training faster. The proposed sparse upcycling is very reasonable. The experiments also show the superiority of this training strategy. \n\nI have the following concern and doubts:\n\n1. The proposed sparse upcycling only increases training speed, but not performance. As shown in Figure.4 right, upcycling and moe from scratch would finally converge.\n\n2. The proposed method seems to take as long as the moe from scratch to finally converge. In Fig.4 and Fig.5, the training curve is not finally converged so it occurs to me that sparse upcycling are just faster in the early stage but could take as long as the moe from scratch to converge.\n\n3. The training speed still seems to be at the same level as training a normal moe. The improvement in time is not a game changer.\n\n4. In the ideal case, MoE only increases the params size but not computation cost or computation speed. (which depends on topk) So the dense model training time should be similar to the sparse model training time even though the sparse model is much larger. I am not sure if that really matters to increase the training speed. A demonstration of the relation between params size, computation cost, and the training speed using normal training or the proposed upscale training is missing from the paper. In another word, under what circumstances (params size, FLOPS) do we care about the training speed, and how much this method can improve?\n\nAbove all, the method is simple and solid. My major concern is that the speed improvement does not seems to be significant and crucial in a real application and the paper does not demonstrate why this is useful. I am also not sure if upcycling is on a different speed level of a normal MoE with decent optimization. \n\n\n\n----------Afrter Rebuttal---------------\n\nI have read the response from the authors. I still have doubts about the speed improvement: not likely to be significant and mostly comes from the slow MoE implementation. I would keep my rating.\n",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "see above",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4822/Reviewer_vrXq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4822/Reviewer_vrXq"
        ]
    },
    {
        "id": "0N36mp_NRk",
        "original": null,
        "number": 2,
        "cdate": 1666653318014,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653318014,
        "tmdate": 1666653318014,
        "tddate": null,
        "forum": "T5nUQDrM4u",
        "replyto": "T5nUQDrM4u",
        "invitation": "ICLR.cc/2023/Conference/Paper4822/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents an interesting and useful idea for reusing pretrained dense checkpoints to initialize larger sparse models for better performance on different vision and language tasks. This method helps to improve performance with some additional computational budget but does not require training large sparse models from scratch. The paper conducts analysis on T5 and ViT models with various sizes and discusses different design decisions for upcycling.",
            "strength_and_weaknesses": "### Strengths\n- Sparse upcycling clearly shows improved performance for both vision and language pretraining, with significant improvements when some non-trivial computation budget is available for further training.\n- importance of this paradigm is in general great to reuse/recycle old models\n- Performance improvement is fairly consistent irrespective of how long the dense pretraining was done, which is promising.\n- Authors discuss various design choices for upcycling clearly. Ablation experiments are thorough and clearly justifies different design choices.\n\n### Weaknesses\n- Initializing MOE models from dense checkpoints is not novel and is somewhat a known practice. \n- Figure 3 is missing the finetuning performance for XL models for language tasks. \n- The performance on downstream language tasks has a lot of variance which increases with larger models and somewhat seems that upcycling does not help in SuperGLUE tasks to a large extent. Is this dependent on downstream tasks? Is this behavior same on other downstream language tasks as well?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed idea is not entirely new and it has been a practice to initialize MOE models with dense checkpoints. However, the insights from this work are of high quality which provides some important design choices when there is limited computation budget and we want to get better performance. The ablation studies have clearly demonstrated the design choices. \n\nIn terms of reproducibility, for the sparse models in this work, will depend on the availability of the pretraining datasets like JFT. However, the learnings from this work will be widely applicable and the thorough discussion of the design choices will help reproducing this work on other datasets. I would encourage the authors to release the code/model/data as far as possible to make this study reproducible.",
            "summary_of_the_review": "Overall, this is an interesting work which shows that we can get improved performance by initializing sparse models with dense pretrained checkpoints when there is additional computational budget available. Initialization of sparse models from dense checkpoints is not new to the community but this work shows some good design choices which are important and will be useful. Hence I recommend this rating, and look forward to the author discussion phase.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4822/Reviewer_puDi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4822/Reviewer_puDi"
        ]
    },
    {
        "id": "wvTImMsRDjk",
        "original": null,
        "number": 3,
        "cdate": 1666670694522,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670694522,
        "tmdate": 1666670694522,
        "tddate": null,
        "forum": "T5nUQDrM4u",
        "replyto": "T5nUQDrM4u",
        "invitation": "ICLR.cc/2023/Conference/Paper4822/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an approach for initializing sparse Mixture of Expert (MoE) models from dense checkpoints. The approach called upcycling reuses an already trained dense checkpoint by copying all the parameters from the original checkpoint (except for the MoE router parameters). The experts in this new model are replicas of the original MLP layer(s) in the dense model. The experiments demonstrate that the upscaled models outperform the sparse models trained from scratch when using the same computation budget as the initial dense pretraining. ",
            "strength_and_weaknesses": "The paper is generally clear and easy to follow. The problem of using already trained dense checkpoints for initializing sparse models is important, especially given the promising results of sparse MoE models in different domains. I found the idea of replicating the MLP layer(s) and randomly initializing the routing mechanism simple yet powerful. I have the following questions and comments:\n1. How do the distribution of token to expert assignment look like? Are we seeing few experts which dominate the probability distributions? \n2. How many experts are involved in processing each token (on average)?\n3. How do the self-similarities and intersimilarities of inputs between experts look like?\n4. The related work, is missing some works on converting a dense model to its corresponding MoE version. See for instance:\n    1. MoEfication: Transformer Feed-forward Layers are Mixtures of Experts \n    2. MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation\n    3. DEMIX Layers: Disentangling Domains for Modular Language Modeling (not directly related but uses a similar idea of replicating the MLP units). \n5. It would be nice to add a comparison to one of the aforementioned approaches to show the advantages of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and the idea is novel. ",
            "summary_of_the_review": "Overall, I like the idea and the fact that the paper covers a relatively comprehensive set of experiments for both vision and language domains. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4822/Reviewer_36d4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4822/Reviewer_36d4"
        ]
    },
    {
        "id": "pMIA3yQU4I",
        "original": null,
        "number": 4,
        "cdate": 1666724963567,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724963567,
        "tmdate": 1666725126736,
        "tddate": null,
        "forum": "T5nUQDrM4u",
        "replyto": "T5nUQDrM4u",
        "invitation": "ICLR.cc/2023/Conference/Paper4822/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a novel yet simple method to reuse pretrained dense Transformer checkpoints to initialize larger sparse\nmodels. \nThe authors showcase that using this technique will result in a more performant MoE model versus continuing training the dense counterparts on vision and language domains up to 3B scale. \nExtensive ablations are provided for the design choices. \n",
            "strength_and_weaknesses": "Summary Of Strengths\n- the paper is clearly written and presented;\n- the efficacy of the sparse upcycling is verified both across domain (vision & language), transfer (downstream) and scales (up to 3B); \n- extensive ablations are provided (router, #dense pretrain, #MoE layers, #experts, MoE locations, and initialization) to illustrate the design choices, which may pave the road for future researchers and practitioners in the efficient computing area. \n\nSummary Of Weaknesses\n- Fairer comparison: it is clearly presented in figure 2 that a sparsely upcycled model outperforms a continual dense counterpart on vision, and language pretraining. However, the sparsely upcycled model also has more capacity, I am wondering if will it be a fairer comparison for:\n\n  (i) upcylce a dense model to a large model (though it will introduce more inference cost) as in [1,2,3,4], especially [1,2] show that staged warm starting may also help the efficiency of training; \n\n  (ii) continue to train a MoE model as shown in Figure 4, and why the Appendix Table 2 seems to disagree what presented in Figure 4?\n  7+7 extra epochs of MoE models seem to outperform a sparsely upcylced model with 7+ extra epochs. \n\n- Why continual training of a dense model turns to hurt the downstream performance on SuperGLUE; \n- How sensitive are the design choices like in the following will affect (1) the initial transition; (2) the ending performance? \n\n    (i) the learning rate schedule: resuming/restarting [1], (also in Appendix A.1.1 suggest using a learning rate schedule where the dense checkpoint left off, does that suggest you will initially plan for more epochs, what if the learning rate has already decay to 0)\n\n    (ii) MoE losses: [5] uses half loading balance + half importance and strong regularization like noise jitter and larger capacity ratio to make the V-MoE stable. Will all the techniques (except for the BPR) also be used here and will all these hyperparameters also be sensitive. \n\n- the intuition in Figure 4 that why will a upcycled model will outperform continue-training MoE\n\n   In Figure 3 and Appendix that the initial transition of the sparsely upcycled model underperforms a dense model, which I assume will also underperform a MoE model. Then using the same schedule, will the sparsely upcycled model consistently underperform a MoE counterpart that trained from scratch?\n\n\n- Is it possible to upcycle MoE models?\n\n[1] Rae, Jack W., et al. \"Scaling language models: Methods, analysis & insights from training gopher.\" arXiv preprint arXiv:2112.11446 (2021).\n[2] Shen, Sheng, et al. \"Staged Training for Transformer Language Models.\" ICML 2022.\n[3] Gu, Xiaotao, et al. \"On the transformer growth for progressive bert training.\" NAACL 2021.\n[4] Gong, Linyuan, et al. \"Efficient training of bert by progressively stacking.\" ICML 2019.\n[5] Riquelme, Carlos, et al. \"Scaling vision with sparse mixture of experts.\" Advances in Neural Information Processing Systems 34 (2021): 8583-8595.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly presented, and extensive experiments verified the efficacy; \n- the idea of using dense for upcycling is novel and effective; \n- the author promises to release the upcycle code upon acceptance. \n\nTypo - Appendix B.1 \"on a per step basis, provided we also use Batch Priority Routing (BPR)\" -> \"on a per step basis when using Batch Priority Routing (BPR)\"",
            "summary_of_the_review": "it is no doubt that sparse upcycling tries to leverage the inference & scaling benefits from MoE and pre-trained weight from a smaller Dense model, and it achieves better results given the more introduced capacity; \n\nThe major concerns for me are the inherited instability of training MoE also appears/worse in upcycling and will that affect the initial transition and how will the initial transition affect the end performance; \n\nMore details for the hyperparameter sensitivity will also be valuable to add. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4822/Reviewer_zFGk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4822/Reviewer_zFGk"
        ]
    }
]