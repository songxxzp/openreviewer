[
    {
        "id": "vNE-c-8LiI",
        "original": null,
        "number": 1,
        "cdate": 1666253552138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666253552138,
        "tmdate": 1666253552138,
        "tddate": null,
        "forum": "Oc2vlWU0jFY",
        "replyto": "Oc2vlWU0jFY",
        "invitation": "ICLR.cc/2023/Conference/Paper1409/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new neural network architecture design based on GLOM (Hinton, 2021) called the Reversible Column Networks (RevCols). RevCols has multiple sub-networks (called columns) of the same structure, between which multi-level reversible units are added. The paper extends RevNets (Gomez et al., 2017) in a recursive way, then splits features into groups and reorganizes groups as columns. The paper leverages two auxiliary heads (Wang et al. 2021) to help disentangle the features. Experiments show that RevCols achieves competitive performances on several vision tasks including image classification, object detection, and semantic segmentation tasks.",
            "strength_and_weaknesses": "Strong points.\n1. The paper introduces a new CNN-based design with reversible transformations and GLOM architecture, which can inspire future research.\n2. The experiments show the competitive performance of the proposed method on several vision tasks.\n3. The paper provides detailed information for reproduction.\n\nWeak points.\n1. The paper sometimes draws conclusions without enough contextual information/justifications or leaves analysis far away from the conclusions, which makes it difficult for readers to understand, especially in the macro and micro design sections.\n\n2. The paper does not differentiate itself clearly from existing methods, especially from (Hinton, 2021; Wang et al. 2021) and (Chang et al. 2018; Jacobsen et al. 2018; Mangalam et al. 2022).\n\nMinor issues.\nLine 9 Section 1, \"Researches\" -> \"Researchers\"\nThe proposed method is sometimes called \"RevCols\" but sometimes called \"RevCol\". \n\nQuestions.\n1. The difference between the proposed method and GLOM is not clear. The \"Relation to previous works\" only mentions one difference but does not explain the reason and advantages/disadvantages.\n2. Sec. 2.2.1 simplifies the multi-column reversible of Sec. 2.1. Would this still satisfy the reversible criterion?\n3. Will code be released?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally written well, but it would be better if figures/tables could be arranged to where they are referred to for the first time. In the network design part, the paper sometimes simply describes the design choices without explanations of motivations and justifications or puts them far away. The paper describes its key designs clearly for reproduction. Personally, I feel that it is interesting to combine GLOM architecture with reversible transformations, and this is the key contribution. ",
            "summary_of_the_review": "I am on the positive side of this paper. I think it is promising to combine GLOM architecture with reversible transformations. However, I am confused but interested to know the motivation behind it and the detailed analysis between these two works. I am happy to increase my rating if the authors can address this ambiguity.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1409/Reviewer_cXh3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1409/Reviewer_cXh3"
        ]
    },
    {
        "id": "AFvqQ5ld8n",
        "original": null,
        "number": 2,
        "cdate": 1666516710309,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666516710309,
        "tmdate": 1666516710309,
        "tddate": null,
        "forum": "Oc2vlWU0jFY",
        "replyto": "Oc2vlWU0jFY",
        "invitation": "ICLR.cc/2023/Conference/Paper1409/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is mainly about convolutional neural networks for image classification. The authors propose a new paradigm to fuse features from different depths. The are some parallel branches of convolutional layers. Features from each branch are fused through a complex topology as stated in Figure 2(c). Some deep supervised loss are adopted for some branches.",
            "strength_and_weaknesses": "The experiments are conducted among many areas, including image classification, object detection, semantic image segmentation and machine translation. Through these experiments, I feel that there are some kinds of competitiveness in this method, but not many. \nFor example, in Table 1, it shows this method behaves consistently worse than EfficientNet, and SwinV2 is missing. In Table 9, it shows this method behaves consistently worse than SwinV2. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is good and clear. There are many details about parameter settings.",
            "summary_of_the_review": "Abound\u00a0experiment\u00a0are\u00a0designed, on many topics, and The results of ablation study of feature fuse and deep supervision support the conclusion. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1409/Reviewer_BtZK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1409/Reviewer_BtZK"
        ]
    },
    {
        "id": "RXB1USNhiuf",
        "original": null,
        "number": 3,
        "cdate": 1666619338589,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619338589,
        "tmdate": 1666619338589,
        "tddate": null,
        "forum": "Oc2vlWU0jFY",
        "replyto": "Oc2vlWU0jFY",
        "invitation": "ICLR.cc/2023/Conference/Paper1409/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new neural network paradigm, which aims at gradually disentangling features in the forward propagation. The whole network is constructed with multiple copies of sub-networks. The total information can be maintained rather than compressed during propagation. The proposed method is also evaluated on several typical computer vision tasks and shows competitive performance.",
            "strength_and_weaknesses": "**Strength**  \n* It is important to explore strong and disentangled representations by designing new neural architectures. The proposed reversible column manner is reasonable to maintain both high- and low-level information.\n* The proposed method shows promising experimental performance.\n\n**Weaknesses**\n* It is expected to explain the intrinsic difference between the proposed method and previous ones, including but not limited to HRNet and NAS (neural architecture searched) networks. Especially, networks [1,2] searched in the cell-based/-like search space have many in common with the proposed one. A more comprehensive analysis is suggested, not only the provided demonstration from the motivation perspective.\n* The shown experimental performance is competitive but the promotion over previous ones is not so evident. I admit the number is almost saturated, but any other advantage this method could bring needs to be demonstrated.\n* Real throughput/latency needs to be measured to more accurately validate the model budget, not just FLOPs or #Params. The introduced connections seem to introduce larger latency on real hardware which is not so related to FLOPs numbers.\n\n\n[1] Liu C, Chen L C, Schroff F, et al. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 82-92.   \n[2] Xie S, Kirillov A, Girshick R, et al. Exploring randomly wired neural networks for image recognition[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 1284-1293.",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity**: most contents are clear and easy to follow.\n* **Quality**: The proposed method is evaluated and studied on substantial benchmarks and settings.\n* **Novelty**: The design principles have some in common with previous methods, but some details are new.\n* **Reproducibility**: Most implementation details are provided but the code is not available.",
            "summary_of_the_review": "The overall idea is good while the main concerns lie in the demonstration of advantages or differences with previous methods and speed analysis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1409/Reviewer_AtgT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1409/Reviewer_AtgT"
        ]
    }
]