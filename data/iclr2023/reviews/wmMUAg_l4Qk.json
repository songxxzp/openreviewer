[
    {
        "id": "pH_X0obmCk",
        "original": null,
        "number": 1,
        "cdate": 1666244686987,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666244686987,
        "tmdate": 1666244686987,
        "tddate": null,
        "forum": "wmMUAg_l4Qk",
        "replyto": "wmMUAg_l4Qk",
        "invitation": "ICLR.cc/2023/Conference/Paper285/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Summary.\n\nThis paper is dedicated to developing efficient generative adversarial networks (GANs). The authors apply dynamic sparsity training to GAN models. They point out that the balance of the generator and discriminator is the key to reaching good performance. The authors propose a quantity BR to measure the balance and use dynamic sparsity training for both the generator and discriminator. STL-10 and CIFAR-10 are used for their experiments.",
            "strength_and_weaknesses": "Pros.\n\n1. The writing is good and easy to follow.\n2. The design of the balance ratio makes sense.\n\n\n\nCons.\n\n1. Lack of insights and incremental novelty. The balance problem has already been pointed out in previous GAN pruning papers. The authors directly apply dynamic sparse training to the GAN model. Furthermore, directly training sparse GANs has also been explored by (Liu et al., 2022).\n2. The balance ratio also has design limitations since it is sparsity/density invariant. It can not directly reflect the capacity balance or topology balance between the generator and discriminator.\n3. It is very hard to read the results in Fig 3.\n4. The paper claims efficient GAN. However, only small GANs like SNGAN are evaluated on small datasets like CIFAR-10 and STL-10. Large datasets with advanced GANs are highly needed, like ImageNet/FFHQ with BigGAN and StyleGAN.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good.\n\nQuality and Novelty are low.\n\nReproducibility is fine.",
            "summary_of_the_review": "Limited novelty and highly insufficient experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper285/Reviewer_edZK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper285/Reviewer_edZK"
        ]
    },
    {
        "id": "0JTp38AyGO",
        "original": null,
        "number": 2,
        "cdate": 1666527761706,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666527761706,
        "tmdate": 1666527761706,
        "tddate": null,
        "forum": "wmMUAg_l4Qk",
        "replyto": "wmMUAg_l4Qk",
        "invitation": "ICLR.cc/2023/Conference/Paper285/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors extend the dynamic sparse training (DST) technique to train unconditional GANs(SNGAN). In addition, the authors propose a new metric called balance ratio to explore the role of sparsity in generator and discriminator. Extensive experiments are conducted to show the influence of different sparsity of generators and discriminators in SNGAN. ",
            "strength_and_weaknesses": "The authors propose a novel metric to balance the dynamic sparse training technique in generative neural networks. The problem is challenging and the experiments are extensive. However, before the reviewer recommends the acceptance of this manuscript, several concerns should be solved:\n(i)\tApplying the DST technique for training GAN is not new. Liu et al 2022 also propose STU-GAN to train GANs. We recommend the authors give a more detailed discussion with STU-GAN in the introduction and related work section. The current discussion with STU-GAN needs further clarification. \n(ii)\tThe results in the experiments merely restrict to SNGAN, which is too specific to conclude the phenomenon. We recommend the authors provide more exploration on other typical unconditional GANs, such as BIGAN (CIFAR10 or ImageNet with low resolution) and StyleGAN2 (CIFAR10 or FFHQ). \n(iii)\tNo baselines are provided in the experiments. We recommend the authors compare their results with several unconditional GAN baselines. The current experiments cannot tell the readers which sparse training technique is better for obtaining a lightweight GAN. \n(iv)\tThe unstructured sparse GAN training can not achieve real training acceleration or inference acceleration.  We recommend the authors extend the weight dynamic sparsity to channel dynamic sparsity. In addition, more baselines are required to demonstrate the effectiveness of the proposed approach. \n(v)\tCan the author explain why the performance is largely degraded compared with dense training? In general, applying dynamic sparse training techniques on training ResNet, there is almost no performance degradation. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written. However, more experiments should be conducted to verify the effectiveness of the proposed approach. ",
            "summary_of_the_review": "See the comments above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper285/Reviewer_J5kM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper285/Reviewer_J5kM"
        ]
    },
    {
        "id": "bSLB5memxW",
        "original": null,
        "number": 3,
        "cdate": 1666588488087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588488087,
        "tmdate": 1666588488087,
        "tddate": null,
        "forum": "wmMUAg_l4Qk",
        "replyto": "wmMUAg_l4Qk",
        "invitation": "ICLR.cc/2023/Conference/Paper285/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed double dynamic sparse training to train sparse GANs from scratch dotted DDST. It directly follows the work of STU-GAN (Liu2022) (a sparse GAN training approach only dynamically change the topology of generators) and proposes balance ratio to further adjust the sparsity of discriminators during training. They first confirm the finding of STU-GAN that the balance between discriminator and generator is important for sparse GAN training and then observe stronger discriminators usually lead to better GAN performance. The proposed balance ratio is a good indicator to check the overfitting problem of discriminators and further adjust the sparsity. The results on the small dataset CIFAR-10 and STL-10 demonstrate the effectiveness of DDST.",
            "strength_and_weaknesses": "I will first go through the strength of the paper:\n## Strength\n(1) The paper is well-motivated. STU-GAN only dynamically changes the sparsity of the generator, leaving the discriminator fixed. The proposed double dynamic sparsity can balance the sparsity of GAN's two components during training, which plays a similar role as the famous Stylegan2-ADA in dense GAN training (https://arxiv.org/pdf/2006.06676.pdf). \n\n(2) The capacity of balance ratio to reflect the overfitting problem of sparse GANs is interesting. \n\n(3) The performance improvement over SGAN demonstrate the effectiveness of DDST.\n\n## Weakness\n(1) While the proposed method is sound, the evaluation is insufficient. DDST is solely evaluated with SNGAN on small datasets such as cifar10 and STL-10. To draw solid conclusions on the practical usage, the evaluation of DDST on large-scale settings is required, such as the de-facto standards for conditional image generation BigGAN on ImageNet, or StyleGAN2 on FFQH. \n\n(2) The method is relatively complicated (many hyperparameters and dynamically adjusted sparsity levels), while the performance improvement over the baseline SDST-strong (RigL) is marginal. I have a concern that the method is too complicated and not practical. \n\n(3) While DDST shows better performance than STU-GAN, it uses relatively larger discriminators (around 50% sparsity), which intuitively increases the training FLOPs. Given the marginal performance improvement, the contribution of DDST is diluted. Moreover, why the training FLOPs of DDST and SDST are exactly the same? I expect DDST to have higher training FLOPs than SDST. Coming up with some smart ways to adjust the capacity of discriminators without changing sparsity would be more convincing, like playing with random pruning or adding some perturbations when overfitting happens. \n\n(4) The balance ratio is very similar to the overfitting heuristics used in StyleGAN2-ADA (https://arxiv.org/pdf/2006.06676.pdf), but without clearly citing it. Is balance ratio a better heuristic than ADA? \n\n(5) I am confused by the second meaning of the word double: the discriminator enjoys two levels of dynamic flexibility, namely density level and parameter level.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-motivated. The method is sound but with insufficient evaluation and marginal performance gains.",
            "summary_of_the_review": "Overall, I like the idea of dynamic adjusting both discriminators and generators. However, the experiment is not very solid and the method itself is too complicated compared with the marginal performance gains. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper285/Reviewer_cSFY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper285/Reviewer_cSFY"
        ]
    },
    {
        "id": "2KEvSA7G6MF",
        "original": null,
        "number": 4,
        "cdate": 1666710743815,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710743815,
        "tmdate": 1666710743815,
        "tddate": null,
        "forum": "wmMUAg_l4Qk",
        "replyto": "wmMUAg_l4Qk",
        "invitation": "ICLR.cc/2023/Conference/Paper285/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors propose a new dynamic sparse training method for GANs. The key idea is to balance the performance of the generator and discriminator during training by developing a quantity called balance ratio. Based on this quantity, the authors adjust the densities of the generator and discriminator during training to keep balance. This quantity can be integrated with existing sparse training algorithms flexibly. Some experiments are conducted to evaluate the performance of the proposed method. ",
            "strength_and_weaknesses": "Strength:\n\n1. The proposed quantity can be integrated with existing sparse training algorithms flexibly to stabilize the training process. \n\n2. The experimental results show that the proposed method can stabilize the training process. \n\nWeaknesses:\n\n1. The performance in Table 2 shows that the improvements achieved by the proposed method over SDST are marginal in some tasks. The authors are recommended to look into this phenomenon.\n\n2. Some theoretical analysis is needed for the proposed balance ratio. I understand it is difficult to prove the convergence of training.  But some analysis is essential to show the insights and properties of the proposed balance ratio, this paper looks too heuristic. \n\n3. This paper is not well organized. For example, the authors present SDST for more than 1 page, which can potentially make the readers confused. \n\n4. Some notations are abused. For example, the $\\alpha$ in balance ratio and $f_{decay}$ should be different. \n",
            "clarity,_quality,_novelty_and_reproducibility": "See the strength and weaknesses. ",
            "summary_of_the_review": "The performance of the proposed method is marginal in some tasks.\n\nThis paper looks too heuristic since no theoretical analysis are provided for the proposed balance ratio.  \n\nThis paper is also not well organized. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper285/Reviewer_Fvhb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper285/Reviewer_Fvhb"
        ]
    }
]