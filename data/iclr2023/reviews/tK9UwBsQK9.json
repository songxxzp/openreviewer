[
    {
        "id": "jL7Ml1RSwc",
        "original": null,
        "number": 1,
        "cdate": 1666063260758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666063260758,
        "tmdate": 1666063260758,
        "tddate": null,
        "forum": "tK9UwBsQK9",
        "replyto": "tK9UwBsQK9",
        "invitation": "ICLR.cc/2023/Conference/Paper3023/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents an isometry aware training approach that improves robustness. The main intuition is that if the latent representations (pre final layer in this case), are (locally) Lipschitz, the network will be robust to adversarial perturbations. The algorithm enforces this Lipschitz constraint using a regularizer which penalizes any deviation in pairwise distance matrices of the outputs and inputs. The paper presents evidence for this in form of experiments on several synthetic datasets (constructed with specific topographic properties) and MNIST. ",
            "strength_and_weaknesses": "**Strengths**\n1. The paper is well motivated with principled arguments about why the locally isometric network should work. \n2. The writing is concise, clear, and well organized.\n\n**Weaknesses**\n\n1. The evaluation is very sparse compared to the standards accepted by the community. Most adversarial robustness papers test their approach on a variety of networks, and datasets. In addition, the attacks used here are not enough to ensure robustness (see Tramer et al (On adaptive attacks to adversarial example defenses), Carlini et al. (On evaluating adversarial robustness) for a more detailed discussion). \n2. In addition, the paper does not present any comparisons with corresponding methods (adversarial training, TRADES, MART) which all present counters to PGD type attacks.\n3. Another issue would be scalability. While the synthetic data and MNIST are mostly well separated, real world image data is both high dimensional and non-separable. My conjecture is that the second term of the loss will lead to significant accuracy losses for CIFAR-10 and Imagenet. I suggest the authors update the paper with these experiments if possible.\n\nMinor concern: While the idea of enforcing local Lipschitzness is inspired, it is not new. Several works [1,2,3] have approached the problem of adversarial robustness by constraining local robustness. The actual implementation differs in each case, however, the overall approach is similar. \nRefs:\n1. Mao et al., Metric Learning for Adversarial Robustness, Neurips 2019\n2. Finlay et al, Lipschitz regularized Deep Neural Networks generalize and are adversarially robust\n3. Yang et al, Adversarial Robustness Through Local Lipschitzness",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. However, the overall novelty is suspect as there have been several works which leverage local lipschitzness (see minor concern above.). The authors have shared code and results, and therefore have satisfied any reproducibility concerns. THe overall quality of the paper is however not up to the mark, given the lack of experiments on harder datasets, and testss with more powerful attacks. ",
            "summary_of_the_review": "Overall, the paper needs some additional work -- especially with better experiments -- to be acceptable. While local Lipschitzness does imply robustness, it is really hard to execute for larger datasets. I am willing to change my opinion if the authors show some evidence of their approach succeeding on CIFAR-10.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3023/Reviewer_RnHw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3023/Reviewer_RnHw"
        ]
    },
    {
        "id": "jWbHuMd4Re2",
        "original": null,
        "number": 2,
        "cdate": 1666598506904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598506904,
        "tmdate": 1666598506904,
        "tddate": null,
        "forum": "tK9UwBsQK9",
        "replyto": "tK9UwBsQK9",
        "invitation": "ICLR.cc/2023/Conference/Paper3023/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes to learn isometric representations in neural networks to preserve the structure in the output end. Then locally isometric layer is proposed to learn the required output representation, which is shown to be robust on MNIST compared with the cross-entropy training baseline.",
            "strength_and_weaknesses": "Strength:\n\n1. The Locally Isometric Layer (LIL) is proposed to avoid the structure-breaking property in neural network classification, which sounds interesting. \n\n2. The proposed method is clearly presented. \n\n\nWeaknesses:\n\n1. The reason why LIL makes the NN more robust is not well-explored. In Section 4.2, there is a discussion on the relationship between Lipschitz constant and the LIL, but no empirical study is provided to compare the performance of LIL to other methods that also reduces Lipschitz constant, such as [1,2,3].\n\n2. The experiment is only carried out with MNIST, which is a quite simple dataset. To show the effectiveness of LIL, the experiment on CIFAR10 is at least needed. \n\n3. The experiment result on MNIST is quite poor if compared with existing benchmarks such as [4]. The robust accuracy under $\\epsilon=0.3$ constraint is over 80%, while Fig. 3 only shows an accuracy of ~20% when $\\epsilon=0.3$, which is a huge gap. Thus, I seriously doubt the effectiveness of LIL.\n\n[1] Chao Chen, Xiuyan Ni, Qinxun Bai, and Yusu Wang. A topological regularizer for classifiers via persistent homology. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2573\u20132582. PMLR, 2019.\n\n[2] Judy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with jacobian regularization. arXiv preprint arXiv:1908.02729, 2019\n\n[3] Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, and Or Litany. Learning smooth neural functions via lipschitz regularization. arXiv preprint arXiv:2202.08345, 2022\n\n[4] https://github.com/MadryLab/mnist_challenge",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\n\nQuality: Borderline \n\nNovelty: Good",
            "summary_of_the_review": "The paper has a major flaw in experiment as mentioned in the Weaknesses, so I tend to reject the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3023/Reviewer_78Vo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3023/Reviewer_78Vo"
        ]
    },
    {
        "id": "00bL96nDTxa",
        "original": null,
        "number": 3,
        "cdate": 1666623783989,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623783989,
        "tmdate": 1669659421002,
        "tddate": null,
        "forum": "tK9UwBsQK9",
        "replyto": "tK9UwBsQK9",
        "invitation": "ICLR.cc/2023/Conference/Paper3023/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to regularize neural networks using isometric properties. The paper presents some intriguing ideas along with some preliminary results that would be worth exploring further.",
            "strength_and_weaknesses": "Strength: The idea is relatively straightforward and intuitive. Some preliminary experimental results are provided. \n\nWeakness:\nHowever, the results in the current form are too premature. There is also a fundamental concern with the premise of the proposed approach.  \nMaking the representation isometric will certainly increase the robustness to perturbation in the input as it directly constrains the Lipschitz continuity. However, this does not touch on the key issue of robustness. Making a neural network robust to image classification tasks requires the network to be sensitive to the on-manifold perturbations (those that change the perceptual properties, e.g., the identity of an object), while being insensitive to the nuisance variability (e.g., increasing the contrast of the stimulus). For the approach proposed by the authors, increasing the contrast of the stimulus or a global scaling of the image would pose a major challenge to the network for solving the image classification task. The preliminary results in Table 1 are consistent with this interpretation and there is a corresponding drop in the performance of the network (as acknowledged by the authors). \n\nOther comments:\nThe second paragraph in the Introduction is a misinterpretation of the existing neuroscience literature. The results of Stringer et al are inconsistent with the classic efficient coding idea. The representation they proposed is closer to the \u201cfractional\u201d representation. Traditional neural population coding models with broadly tuning neurons will create representations that are more consistent with the isometric idea. The paper below seems to contain an additional neuroscience example that might be useful for motivating the study: Gao et al. \"On Path Integration of Grid Cells: Group Representation and Isotropic Scaling.\" Advances in Neural Information Processing Systems 34 (2021): 28623-28635. But they\u2019re dealing with a low-dimensional problem, i.e., physical space.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear. \nThe novelty is somewhat incremental. \nThe results are a bit premature, and the basic idea may need be to investigate in more detail. ",
            "summary_of_the_review": "This is a submission with an intriguing idea. The results are premature.\n\n\n**** Edits after the rebuttal:\n\nI appreciate the authors' responses to my critiques.  In its current form, I consider the study to be premature. Performing additional research along the directions the authors raised in their response will likely make the study stronger.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3023/Reviewer_1G3T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3023/Reviewer_1G3T"
        ]
    },
    {
        "id": "a9RisW_Va2L",
        "original": null,
        "number": 4,
        "cdate": 1666634093270,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634093270,
        "tmdate": 1666634093270,
        "tddate": null,
        "forum": "tK9UwBsQK9",
        "replyto": "tK9UwBsQK9",
        "invitation": "ICLR.cc/2023/Conference/Paper3023/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to combine the existing neural network's loss with an isometry-preserving loss, which they argue would allow for preserving the geometry, which the authors complement with some theoretical results. \n\nThe two main experiments aim to show that the resulting model is more robust to adversarial attacks and leads to better accuracy when the geometry is preserved across layers. \n\nThe paper is somewhat reminiscent of [1], combining similarity-preserving objectives and standard objective function layer-wise. The empirical results nonetheless are very promising but too sparse to be of real value. \n\n[1] N\u00f8kland, Arild, and Lars Hiller Eidnes. \"Training neural networks with local error signals.\" International conference on machine learning. PMLR, 2019.\n",
            "strength_and_weaknesses": "The paper is well written, and the problem is very interesting to the community. The clarity made the paper enjoyable to read. \n\n\nThe paper, however, falls short in various aspects. \n\n1) the experimental evaluations are too sparse to evaluate the models. It is tested on one dataset per task. As mentioned earlier, this paper is reminiscent of that of N\u00f8kland et al. 2019, but not cited in the paper. \n2) the models consider have various parameters, and none of their influences are thoroughly studied. \n3) as mentioned by the authors, competing models exist, yet the current model is not compared to those. \n\nThe idea is clear and interesting, but the claims are not well-supported. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear explain and somewhat novel. I offer a suggestion of a possibly relevant paper to contrast with. \n",
            "summary_of_the_review": "The paper is interesting and the current experiments are ok. However, there is a lot that is missing to be able to actually confirm the claims made by the authors. We would like more thorough evaluations of the models. It is too premature to be accepted as a publication given that it is not possible to clearly evaluate the approach. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3023/Reviewer_aCCN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3023/Reviewer_aCCN"
        ]
    }
]