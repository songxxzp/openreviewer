[
    {
        "id": "L9eU3dOsV4",
        "original": null,
        "number": 1,
        "cdate": 1666683699117,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683699117,
        "tmdate": 1666683699117,
        "tddate": null,
        "forum": "CJd-BtnwtXq",
        "replyto": "CJd-BtnwtXq",
        "invitation": "ICLR.cc/2023/Conference/Paper2895/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a theoretical analysis of oversmoothing in linear GCNs in an SBM setting where the features and labels are assumed to be Gaussian. It provides bounds for shallow GNNs rather than convergence rates in the infinite-layer limit. The theoretical results are supported with empirical data showing the effect of oversmoothing for synthetic and real-world datasets as the number of layers increases.",
            "strength_and_weaknesses": "General comments\n-\n\n- In the experiments section, your test accuracies for Cora, Citeseer, and Pubmed with weights appear much lower than in other papers. Is there a reason for this based on your experimental setup?\n\nStrengths\n-\n\n- The paper improves upon theoretical results on oversmoothing from past work, which proved crude bounds using Laplacian eigenvalues. In contrast, this paper finds bounds for shallow networks which are a more realistic regime for node classification.\n- Contextual SBMs are a good choice of framework for the theory that capture causes of oversmoothing in real-world graphs.\n\nWeaknesses:\n-\n\n- The node feature vectors are assumed to be 1-dimensional; is this strictly necessary to prove the results of the paper?\n- The Bayes optimal classifier is a simplification (as noted by the authors) and may not be applicable to the training of GNNs.\n- Performance does not seem to degrade much for Cora, Citeseer, and Pubmed in the case without weights even for very deep networks. This is in sharp contrast to the CSBM data, indicating that there are different factors contributing to the synthetic data.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. As far as I can tell, the results are new and different in scope from existing theory on oversmoothing.",
            "summary_of_the_review": "The paper answers questions about the extent of oversmoothing in GNNs, and for a specific model exhibiting community structure, shows that oversmoothing occurs after a relatively small number of layers which can be explicitly predicted. While several simplifying assumptions are made, enough structure is imposed on the synthetic graphs for the results to be meaningful.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2895/Reviewer_K5CA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2895/Reviewer_K5CA"
        ]
    },
    {
        "id": "lQSWlQIZCz",
        "original": null,
        "number": 2,
        "cdate": 1666807675333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666807675333,
        "tmdate": 1666807707632,
        "tddate": null,
        "forum": "CJd-BtnwtXq",
        "replyto": "CJd-BtnwtXq",
        "invitation": "ICLR.cc/2023/Conference/Paper2895/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper characterizes the mechanism of oversmoothing by a non-asymptotic analysis. It distinguishes undesirable mixing effect and the desirable denoising effect on graph convolutions and gives an estimation of when the mixing effect dominates the denoising effect, by quantifying both the effects on random graphs sampled from Contextual Stochastic Block Model (CSBM). On the basis of this analysis, it explains the oversmoothing phenomenon at a relatively shallow depth. To show that the framework can be applied to other message-passing schemes, specifically, the authors analyze the performance of Personalized Propagation of Neural Predictions (PPNP) and Approximate PPNP (APPNP). Their results suggest that PPR-based architectures mitigate oversmoothing at deep layers. Additionally, some numerical experiments support the theoretical results and indicate that the difficulty of optimizing parameters exacerbates oversmoothing.",
            "strength_and_weaknesses": "\n1. This work provides a non-asymptotic analysis formulation of oversmoothing, closing the gap between finite-depth GNNs and the previous asymptotic analyses. The authors provide an explanation theoretically for why the optimal number of GNN layers for node classification is relatively small and gives a sweet spot for the required number of layers.\n2. The distinguishment of the mixing effect and denoising effect is helpful to understand counteracting effects in neural message passing. \n3. Experimentally, they observe that the difficulty of optimizing weight in deep GNNs aggravates oversmoothing. It makes sense and might be inspiring in further message passing designs.\n4. It seems that all the analysis relies on the assumption that the graph is generated as $\\mathcal(A,X)\\sim\\mathrm{CSBM}(N, p, q, \\mu_1, \\mu_2, \\sigma^2.)$ The authors point that their analysis cannot capture graph characteristics like degree heterogeneity. But except it, whether the analysis formulation can be applied to general feature distributions? \n\n**Questions**\n\n1. Could you please state more about the advantages of non-asymptotic analysis? To my knowledge, oversmoothing is associated with the Dirichlet energy decay rate in the previous works. In this work, the domination of the mixing effect is considered as the reason of it. Is there any connection between the two demonstrations (decay rate and domination)?\n2. The `sweet spot\u2019 given in the paper is indeed a lovely choice for avoiding oversmoothing. I am curious about whether it is a sweet spot for the whole learning process. For example, is it possible that the sweet spot can be too short for some long-range problems?\n3. Could you give a notation instruction for $\\omega(logN/N)$ in Page 3 and $\\Omega(logN/N)$ in Page 4? And could you explain more about Figure 2? There are few illustrations, especially on graphs B and C.\n4. In the abstract, it says \u2018PPR-based architectures still achieve their best performance at a shallow depth and are outperformed by the graph convolution approach on certain graphs.\u2019 And Section 4, says \u2018This drawback would be especially notable at a shallow depth, where the denoising effect is supposed to dominate the mixing effect. PPNP/APPNP would perform worse than the baseline GNN on these graphs in terms of the optimal classification performance.\u2019 How does PPNP perform at a shallow depth after all?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this paper is clear and helpful work for understanding oversmoothing in GNNS. To my best knowledge, it is a novel work that shows a new theoretical view on the oversmoothing phenomenon.",
            "summary_of_the_review": "This paper provides a novel perspective to understand oversmoothing and a non-asymptotic framework to analyze it. It gives a series of theoretical results in both the main body and the appendix with detailed proofs. Their experiments support their main results and imply the detriment of the difficulty of optimizing deep GNN models. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2895/Reviewer_KQXD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2895/Reviewer_KQXD"
        ]
    },
    {
        "id": "6xgTdC9m9R",
        "original": null,
        "number": 3,
        "cdate": 1666859369021,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666859369021,
        "tmdate": 1666859369021,
        "tddate": null,
        "forum": "CJd-BtnwtXq",
        "replyto": "CJd-BtnwtXq",
        "invitation": "ICLR.cc/2023/Conference/Paper2895/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors focus on the over smoothing issue prevalent in GNN and demonstrate the presence of two underlying mechanisms which dictate when and how over smoothing issue crops up i.e., an undesirable mixing effect and a desirable de-noising effect. The authors demonstrate the efficacy of their approach via empirical results.",
            "strength_and_weaknesses": "Here below are some strengths of the current work :\n\n1) The authors focus on an important problem in the GNN research community and make an effort to decipher it in this current work. Specifically the authors try to explain why over smoothing happen at a relatively shallow depth which is very important to understand.\n\n2) The authors demonstrate the presence of counteracting effects of applying graph convolutions i.e., a mixing effect which homogenizes node representations and a de-noising effect which homogenizes node representations belonging to the same class. The authors also provide an estimate as to when one effect dominates the other.\n\nHere below are some weakness of the current work :\n\n1) The current work focusses on a simplified setup of two classes and uses Contextual Stochastic Block Model as the generative model which does not provide an accurate representation of real-world graphs. \n\n2) The authors do not demonstrate how to measure over-smoothing as well as the two effects in practice. Without being able to 1) adequately measure and quantify the two effects and 2) demonstrate the authors can adequately explain over-smoothing in a general setting, the presented approach seems hypothetical. \n\n3) The authors do not demonstrate the practicality of the current approach to the research community. Theoretical analysis is great however without being able to provide utility, the scope of the work is significantly reduced. \n\n4) As the authors themselves point out, the current work's analysis is based on oracle classifier, however in practice, we work in a semi-supervised setting. Thus the utility associated with the current work is pretty limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The current work is novel albeit in a highly specific set-up. The problem statement, motivation and proposed approach are clear. ",
            "summary_of_the_review": "The authors focus on an important problem for the GNN research community and try to decipher it via breaking it down to two counteracting effects of applying graph convolutions. However the analysis is very focussed on a specific set-up and is not generic in nature. Additionally it makes strong assumptions and due to this and prior issues, the usefulness of the current work is pretty limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2895/Reviewer_2Zpy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2895/Reviewer_2Zpy"
        ]
    }
]