[
    {
        "id": "UghJ5zCZL2F",
        "original": null,
        "number": 1,
        "cdate": 1665720654201,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665720654201,
        "tmdate": 1665720654201,
        "tddate": null,
        "forum": "Pqi9ZxxdjM",
        "replyto": "Pqi9ZxxdjM",
        "invitation": "ICLR.cc/2023/Conference/Paper2970/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to leverage the 3D information of natural images in contrastive learning. Specifically, the authors use the depth signal to generate multiple views from different camera positions, so that they can produce the 3D augmentation for contrastive learning. The idea is simple and natural, and validated on different popular baselines such as BYOL and SimSiam. Experiments on various styles of Imagenet demonstrate the effectiveness of the proposed methods.",
            "strength_and_weaknesses": "**Strengths:**\n\n(1). The idea is novel and interesting. I feel this is a very natural extension from the regular 2D image contrastive learning. This technique seems also very general and can be used in different CL backbones.\n\n(2). The paper is well-written and easy to follow. I can understand the main idea by just reading the abstract.\n\n(3). Experiments are strong. The improvements are not marginal.\n\n\n**Weaknesses:**\n\n(1). My main concern is that the monocular depth estimation usually needs a lot of human annotations (in pixels). In this case, when we really want to use the technique proposed in this paper, we will have to put lots of effort into annotation. More importantly, as we know that the effectiveness of a pretrained monocular depth estimation model may hevalily depend on the parameters/setting of the camera, we can not say whether the depth estimation is accurate for different datasets.\n\n(2). Details should be further clarified. It is unclear how the model performs the estimation and how it generates the new views via modifying the camera position. These are important technical details, but they are missing.\n\n(3). Since the idea is rather straightforward, I think the authors should consider releasing their code (maybe on some anonymous URL).",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this paper is well-written even though some critical technical details are not discussed. The idea is interesting and useful.",
            "summary_of_the_review": "I would like to vote for a \"borderline accept\" due to the above weaknesses I mentioned before. I would consider increasing my score if they are well solved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2970/Reviewer_nFbm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2970/Reviewer_nFbm"
        ]
    },
    {
        "id": "W2dI_6bKW2",
        "original": null,
        "number": 2,
        "cdate": 1666675430161,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675430161,
        "tmdate": 1666675430161,
        "tddate": null,
        "forum": "Pqi9ZxxdjM",
        "replyto": "Pqi9ZxxdjM",
        "invitation": "ICLR.cc/2023/Conference/Paper2970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a novel self-supervised learning method for 2D image classification. Instead of augmenting image in 2D space, the authors first use RGB-to-Depth to estimate the depth channel for the given input image. Then a new view of the image is synthesized from the RGB-D tensor. The synthetic views are then used in a contrastive learning framework. The authors validate their method on several small  scale datasets, including a subset of ImageNet.",
            "strength_and_weaknesses": "[Strength]\n\n* Using RGB-to-Depth to get 3D presentation of image and then synthesizing a new view from 3D space is a novel idea in SSL.\n* The proposed method improves the classification accuracy significantly on the benchmarked dataset. \n* The idea is very simple and straightforward but very effective.\n\n[Weakness]\n\n* The novelty of this work is more or less incremental in nature. The authors combine two techniques from two deep learning domain. That is saying, this work itself dose not contribute any novel algorithm, except for the algorithm combining.\n\n* Lack large-scale experiments as previous SSL works. ImageNet-100 is clearly too small for a serious study especially in SSL researches. Suggest to report results on ImageNet-1k and ImageNet-21k if possible.\n\n* Possible information leak when using pre-trained RGB-to-Depth models. If the pre-trained models are trained on large-scale labeled dataset, using these pre-trained models is not fair then. This is because SSL aims to reduce the number of manual labels in deep learning. If this method requires a pre-trained model learned on an even larger-scale dataset, it simply makes no sense.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is well written and easy to follow.\n* Experiments are well presented with sufficient ablation studies.",
            "summary_of_the_review": "The proposed method is motivated from a simple but interesting idea which mimics binocular vision system in self-supervised learning. The method shows promising improvement on small-scale image datasets but lacks large-scale experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2970/Reviewer_9hga"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2970/Reviewer_9hga"
        ]
    },
    {
        "id": "MoQDURAr_6",
        "original": null,
        "number": 3,
        "cdate": 1666749766570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666749766570,
        "tmdate": 1671084997852,
        "tddate": null,
        "forum": "Pqi9ZxxdjM",
        "replyto": "Pqi9ZxxdjM",
        "invitation": "ICLR.cc/2023/Conference/Paper2970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to incorporate depth in the commonly used contrastive learning based self supervised learning frameworks. The depth map is generated using a state of the art RGB to depth predictor, namely DPT (Rangftl et al. 2021). The depth is simply appended to the input RGB image for the purpose of performing SSL procedure. Another contribution by the authors is in the use of AdaMPI (Han et al. 2022) to generate multiple views as an additional augmentation method. The overall  SSL procedure remains the same as the baseline method.\n",
            "strength_and_weaknesses": "Strengths:\n- The paper is easy to follow and clearly written.\n- The idea of incorporating the depth maps is interesting and to the best of my knowledge, it is the first attempt to use depth maps in the SSL framework.\n- The proposed augmentation method and depth appended yields consistently better results on ImageNet100 and ImageNette dataset. \n\nWeaknesses:\n- The technical novelty in the proposed method is quite limited. The main contribution of the authors is to integrate existing approaches such as AdaMPI (Han et al. 2022) and DPT (Rangftl et al. 2021) in existing SSL procedures.\n- As it\u2019s an empirical paper, one would expect large scale experiments to test the proposed idea. The SSL experiments are performed on the subset of ImageNet dataset with 10 and 100 classes. To validate the idea, full Imagenet experiments will be needed. The main advantage of SSL methods is to learn on a large dataset of unlabeled images. \n- The experiments are all performed on downstream tasks of classification. SSL literature also focuses on downstream tasks such as object detection and segmentation.\n- In Table 2, adding 3D views results in worse performance. Can the authors explain their understanding why 3D views have negative effects on the SSL procedure? Why do some methods perform better at KNN based evaluation and some at Linear evaluation?\n- In case of training with 3D views, are the novel views generated like depth maps beforehand or during training? Is there any computational overhead with the proposed method?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is easy to understand and clearly written\nQuality and Novelty: Technical novelty is limited and empirical performance in some cases is good.\nReproducibility: I would suggest the authors to release the code during the review process.",
            "summary_of_the_review": "Since the technical novelty is limited and the main contribution is to combine two papers in terms of augmentation in existing SSL procedure, I think the paper is not good enough for publication in the conference. For an empirical contribution, I would expect authors to validate their idea with further analysis on large scale datasets.\n\nPost-rebuttal:\nThanks to authors for the response to my comments. After reading the authors response and other reviewers concerns, I decide to stick to the original rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2970/Reviewer_N68U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2970/Reviewer_N68U"
        ]
    },
    {
        "id": "XW5KuAgbdf",
        "original": null,
        "number": 4,
        "cdate": 1666858694425,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666858694425,
        "tmdate": 1666858694425,
        "tddate": null,
        "forum": "Pqi9ZxxdjM",
        "replyto": "Pqi9ZxxdjM",
        "invitation": "ICLR.cc/2023/Conference/Paper2970/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes to integrate depth signal into self-supervised learning methods as an additional signal in 2 ways. The first method simply concatenates the depth channel onto the RGB image and trains the SSL methods with RGBD input. The second method uses the single-view view synthesis methods to generate new views and use them as augmentations. Experiments show significant improvements over the baseline methods especially in corruption datasets, ImageNet-C and ImageNet-3DCC.",
            "strength_and_weaknesses": "o\tStrengths\n\uf0a7\tThe paper is well written.\n\uf0a7\tExperimental results seem promising as the results in both 2 methods outperform the baseline results.\n\uf0a7\tAblation experiments are well conducted although some experiments are missing.\no\tWeaknesses\n\uf0a7\tThere is no computational complexity information about adding the depth information in both methods. For the first method, concatenating a depth channel to the input, depth maps are pre-extracted before the training and used in augmentations, only in Random Crop and Horizontal flip transforms. In other types of transforms, depth map is assumed to be the same and as stated in the paper, it has a stronger correspondence.\nFurthermore, only one of the two images is augmented (the other is the original image) rather than both augmented images in the originally proposed methods, SimSiam [1], BYOL [2] and SwAV [3]. Whether there is a specific reason to this choice is not described.\n\uf0a7\tExperiments are detailed except for the SwAV experiments in Table 1, ImagiNette, (+3D views are not presented.) and Table 2, ImageNet-100D, (there is no SwAV experiment.)\n\uf0a7\tThis idea lacks of novelty. This paper can be considered as well performing components for each task, which are Depth Prediction Transformer, SwAV and AdaMPI but no new model is proposed.\n\uf0a7\tThis is a nice application/engineering paper but ICLR is not the right venue for this paper.\n\uf0a7\tAt least in ImageNette dataset, computing the depth map for each corresponding augmentation and training the methods as originally proposed should be presented. Also, SwAV experiments in Table 1 and 2 should be completed.\n\n[1] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750\u201315758, 2021.\n[2] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena \u00b4 Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.\n[3] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. \nThe paper combines the preexisting well performing methods. \nIts code is not shared, therefore, it is not reproducible.",
            "summary_of_the_review": "As stated above, although this is a nice application/engineering paper, ICLR is not the right venue for this paper. It\u2019s more suitable for a computer vision conference.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2970/Reviewer_TCcc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2970/Reviewer_TCcc"
        ]
    }
]