[
    {
        "id": "9Beu8t6kxm",
        "original": null,
        "number": 1,
        "cdate": 1666481505453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666481505453,
        "tmdate": 1666664860760,
        "tddate": null,
        "forum": "GUMLIArCIwB",
        "replyto": "GUMLIArCIwB",
        "invitation": "ICLR.cc/2023/Conference/Paper3074/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "1. The authors target the communication bottleneck and scalability of multi-client federated learning (FL). They formalize and tackle the client-to-server model updates compression during training under Secure Aggregation (SECAGG) primitive, which is a core component of Federated Learning pipelines that allows the server to aggregate the client updates without accessing them individually.\n\n2. The key techniques they adopt to make SECAGG efficient and scalable include: scalar quantization, pruning, and Secure Indexing (SECIND, a variant of SECAGG to enable product quantization). \n\n3. They claim to achieve a 40x compression ratio in uplink communication on LEAF benchmarks without meaningful loss.\n",
            "strength_and_weaknesses": "strength\n\n1. It aggressively saves the uplink communication bandwidth and the authors claim that there is no loss in security.\n\n\n2. The key approach is to modify compression techniques to share hyperparameters globally across all clients so that aggregation can be done by uniformly combining responses from all clients. This seems to be portable to existing FL works without much implementation efforts.\n\nweakness\n\n1. The global sharing strategy, which is their key idea, seems to be quite straightforward and lacks novelty.\n2. I guess the performance drop introduced by shared mask (pruning) and shared quantization parameters (quantization) is depending on the data variance across clients. I suppose this could potentially limit the usability of the work.\n3. Compatibility with DP as they stated in section 6.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. It would be great to be specific about what \u201cmeaningful loss\u201d means in the abstract.\n2. I am not an expert in this field, and from an non-expert point of view, it would be good to show the breakdown for FL workloads, demonstrating the uplink communication is indeed the system bottleneck.\n3. The demonstration is based on a linear layer, and is this method adoptable on convolutional layers and other types of layers? If so, what is the difference in terms of accuracy, privacy and compression ratio?\n4. Is aggregation overflow the only reason why scalar quantization is not compatible with SECAGG? If so, why must the server use the same bit width as the clients? Can the server use more bits to avoid overflow? I suppose another reason is the different quantization parameters are not compatible for aggregation, but this is not mentioned in the \u201cChallenge\u201d part of 3.2.1.\n5. Is the global hyperparameter sharing strategy for compression a possible source for information leakage? If not, what\u2019s the price for adopting your solution besides more download time (given the no-free-lunch principle)?\n6. In Figure.1, DP is omitted for clarity. Just want to be clear, is g_i already calculated by data after adding the DP noise? Is the experimental results in Figure 2 considering DP?\n7. Plan for open source (ignore if I missed the link in the paper) ?\n",
            "summary_of_the_review": "Overall I enjoyed reading this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3074/Reviewer_dmTz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3074/Reviewer_dmTz"
        ]
    },
    {
        "id": "9eaUWAho1d",
        "original": null,
        "number": 2,
        "cdate": 1666712448898,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666712448898,
        "tmdate": 1666712448898,
        "tddate": null,
        "forum": "GUMLIArCIwB",
        "replyto": "GUMLIArCIwB",
        "invitation": "ICLR.cc/2023/Conference/Paper3074/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides an efficient approach on how to integrate scalar quantization, product quantization and random pruning into existing secure aggregation algorithms. The paper makes extensive experiments to show that the proposed method can reach a good balance between communication efficiency and accuracy.\n",
            "strength_and_weaknesses": "Strengthen:\n1. The paper proposes a novel mechanism for the application of compression techniques to secure aggregation, and such compression is bi-directional.\n2. Based on the experiment, the method can reach a high accuracy with a high compression rate.\n\u200b\n\nWeakness:\n1. Compression masks also contain certain information about the clients, I am not sure whether adding m_i only in the compressed domain will affect the security. The authors didn\u2019t emphasize it. \n2. It\u2019s only designed for uplink compressions, which in my opinion, can only reduce up to half of the overall communication bits\n3. To ensure that this framework could go through, the authors need to have the same compression mask (e.g., pruning mask) in each round. It might work in the iid setting but in the non-iid setting, such limitation may lead to a huge downgrade in performance.  \n\n\u200b\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Fig.1 is a bit hard to follow now. It\u2019s better to give an illustration on the uplink and downlink communications\n2. It\u2019s not clearly stated why adding the mask will not affect communication efficiency. The authors should emphasize that.\n",
            "summary_of_the_review": "The paper gives an efficient approach on how to integrate scalar quantization, product quantization and random pruning into existing secure aggregation algorithms, and the experiment on the proposed experiment is extensive. However, it is only for uplink compression and the non-iid setting is not considered. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3074/Reviewer_Tmdx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3074/Reviewer_Tmdx"
        ]
    },
    {
        "id": "52lfDKPSn-",
        "original": null,
        "number": 3,
        "cdate": 1667008168959,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667008168959,
        "tmdate": 1667008168959,
        "tddate": null,
        "forum": "GUMLIArCIwB",
        "replyto": "GUMLIArCIwB",
        "invitation": "ICLR.cc/2023/Conference/Paper3074/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper addresses the problem of compressing the client-to-server model updates in a Federated Learning (FL) setup implementing secure aggregation (SecAgg) protocols. \n\nFirst, the paper discusses the incompatibility of standard compression methods (scalar quantization,  pruning, and product quantization) with secure aggregation protocols. \n\nNext, a solution to address this incompatibility has been proposed, which primarily relies on using common/shared information across all clients. This common/shared information includes scalar quantization parameters (scale $s$, zero point $z$, bit-width $b$ and SecAgg bit-width $p$ with $p > b$), pruning masks, and product quantization parameters (block size $d$, codebook size $k$, and codebook C). \n\nA \"Secure Indexing\" (SecInd) protocol has also been proposed for product quantization, which leverages the linearity of the computations (since the eventual goal is to obtain sum of the weight matrices, one can easily compute this by counting the number of occurrences of a code at a specific location and multiplying it with the value of the code).   \n\nExperiments show that product quantization with SecInd achieves the highest compression rate at iso-accuracy. ",
            "strength_and_weaknesses": "Strengths: \n1.\tThe motivation is clear and well-defined.\n2.\tThe paper is well-organized and easy to follow.\n\nWeaknesses: \n\n1.  There is very little technical novelty in the proposed work. As discussed in the summary, the core idea is to select common parameters for the compression methods across all the clients. This appears to be an obvious and naive solution. The proposed SecInd method is also fairly obvious, involving only re-arranging some computations. \n\nThe claim that \"operation to take assignments to produce an aggregated update is not linear\" is not a valid one. For example, suppose that we want to select the j-th element of a vector v. This can be made linear by computing the dot product between v and an indicator vector i, where the vector i has zero everywhere except the j-th element, which is set to 1. This is exactly the idea used in this work - since an outer summation is also involved, one can sum up the indicator vectors to obtain an \"histogram\" before computing the dot product.   \n\n2. The whole concept of using common parameters across all clients is unlikely to work well in the non-iid setting (statistical heterogeneity). No effort has been made to investigate the effectiveness of the proposed compression methods in the non-iid scenario.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well-written. There are no concerns about the reproducibility. The main limitation is the lack of any novel ideas.",
            "summary_of_the_review": "A well-written paper that address a valid problem, but the proposed solutions are obvious, straightforward, and lack novelty.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3074/Reviewer_r6Pi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3074/Reviewer_r6Pi"
        ]
    },
    {
        "id": "R0Svkzh1ln",
        "original": null,
        "number": 4,
        "cdate": 1667174628450,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667174628450,
        "tmdate": 1667174628450,
        "tddate": null,
        "forum": "GUMLIArCIwB",
        "replyto": "GUMLIArCIwB",
        "invitation": "ICLR.cc/2023/Conference/Paper3074/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper tackles the communication bottleneck in the scalability of federated learning. A linearity constraint on the decompression operator is introduced, together with some popular scalar quantizations to to reconcile security and communication efficiency. The proposed algorithm reports the best results on LEAF benchmarks. ",
            "strength_and_weaknesses": "Strength: This paper is well written. The proposed model to reconcile security and communication efficiency is interesting and efficient. \n\nWeakness:The proposed model may suffer the applicability to privacy models such differential privacy.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of good quality and clarity. The idea is novel as far as I know.",
            "summary_of_the_review": "Overall, I think that the studied model in this paper is well-motivated. The proposed algorithm is sound and evaluated with better performance compared to previous related works. Therefore, I recommend \u201cweak accept\u201d.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3074/Reviewer_wijR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3074/Reviewer_wijR"
        ]
    }
]