[
    {
        "id": "SUG1KcYq1MS",
        "original": null,
        "number": 1,
        "cdate": 1666213245613,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666213245613,
        "tmdate": 1669167963408,
        "tddate": null,
        "forum": "3yJ-hcJBqe",
        "replyto": "3yJ-hcJBqe",
        "invitation": "ICLR.cc/2023/Conference/Paper2115/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper enhances evidential deep learning (EDL) for open set detection by incorporating distributionally robust optimization with a scheduler function to boost hard samples to predict while at the same time enhancing the boosting for minority classes so they can be learned.  The new method is termed  adaptive robust evidential optimization (AERO), and experimental results clearly demonstrate the utility of the method. ",
            "strength_and_weaknesses": "The strength of the paper is the that the method appears novel, and the experimental results are impressive.  Furthermore, the paper includes some theoretical linkage to boosting (which this reviewer does not have time to verify).  The weakness of the paper is that the Kullback-Leibler regularization term in EDL seems to be ignored, which makes the results questionable. Also, the composition of the scheduler function as a weighted sum of convex and concave functions is a bit ad-hoc.  The main concern is that the convex functions are parameterized by beta, but this does not seem to be the case for the concave functions.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written.  The approach seems novel and for the most part makes sense despite what could be a fatal flaw. The experimental results are impressive, but this reviewer really does not have a good grasp of the MAP metric in this context.  The AUROC is much more intuitive for this reviewer. \n\nIn any event, the concern is that the EDL loss function is only composed of the sum of square loss in (3) and is missing the KL term that was used in the EDL paper.  Without the KL term as shown in the original EDL paper, the evidence would go to infinity to minimize the sum of the square loss, meaning that EDL has no way to estimate uncertainty.  The KL regularization term anchors the uncertainty by decreasing evidence when the test samples cannot be predicted well. The problem is that the KL term must be properly annealed so that in can initially learn to discriminate before learning how to calibrate the evidence. If the lack of inclusion of the KL term in (3) was a simple reporting error, the paper still would need to explain how the annealing of its weigh parameter lambda is balanced in relation to opening up the DRO set via eta from epoch to epoch.\n\nOn page 4, the statement - \"...it provides a low uncertainty mass for minority/difficult samples while remain high (in terms of uncertainty mass) for those open set samples.\" seem to conflict with an earlier statement that minority/difficult samples are assigned high uncertainty.",
            "summary_of_the_review": "The paper is very interesting with impressive results for open set detection.  The description of the method neglects a critical regularization term used in EDL, which is one of key component the proposed AERO method builds upon.  Without proper discussion of this term and how it is incorporated into AERO, the results in this paper are suspect. \n\nThe authors have addressed my major concerns, I can increase my rating to over the acceptance threshold to 6.  This paper has a lot of merits, and I would have no problem if it appears in the program.  The use of EDL, DRO, and MSF has novelty, but the integration of all three methods does not seem to require any new processing ideas or theory that would bring this paper to a definite accept.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2115/Reviewer_c1ud"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2115/Reviewer_c1ud"
        ]
    },
    {
        "id": "RLDehtKwvd6",
        "original": null,
        "number": 2,
        "cdate": 1666370910685,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666370910685,
        "tmdate": 1668776277580,
        "tddate": null,
        "forum": "3yJ-hcJBqe",
        "replyto": "3yJ-hcJBqe",
        "invitation": "ICLR.cc/2023/Conference/Paper2115/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an Adaptive Robust Evidential Optimization (AREO) technique to tackle sample uncertainty by evidential learning. The main emphasis in the paper is evidential learning in the presence of minority classes. The paper starts with motivating the need for a tradeoff between distributive robust optimization (which deals with difficult samples in the training set) vs. oversampling (which deals with minority classes). An adaptive evidential learning strategy is proposed that gradually increases the size of the uncertainty set by a composite scheduler function. This function optimizes the weight to be assigned to each training sample at every epoch depending on whether they are from minority classes or majority ones. The optimization involves jointly solving for parameters of the model and the composite scheduler function. A bilinear optimization based on population-based training is proposed. Additional valuable insight about the connection between AREA and AdaBoost is provided under certain assumptions.   \n",
            "strength_and_weaknesses": "\nStrengths:\n\nEvidential learning in the presence of minority classes while acknowledging the potential presence of difficult samples in majority classes is a real-world problem that is often overlooked in most open-set classification and evidential learning papers. On that regard this paper studies a highly significant problem and proposes a scheduling algorithm that adjust uncertainty set size during network training.\n\nThe connection between AdaBoost and AREO is quite motivating.   \n\nIt is good to see that the improvement in performance holds even when different number of minority classes with different sizes are included in experiments. Qualitative analysis ranking sample difficulty by different techniques was also very useful.\n\nWeaknesses:\n\nIt is not immediately clear whether the improvement is due to population-based training (PBT) or weighting samples by the proposed composite scheduler. An ablation study would be very useful.  This is a concern because none of the compared techniques seem to be using PBT. \n\nIt is also concerning that no run-time comparison is provided between proposed technique and others. PBT would significantly increase run-time and the limited improvement in some datasets may not quite justify this significant increase in run-time.\n\n\n-----------------\nAuthors' responses satisfactorily address my main concerns about PBT and run time. If there was a score of 7 I would have upgraded my ranking, regrettably there is none, and I don't think this is an \"8\" paper.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: \nThe paper is organized well and reads smoothly. However, there were many typos some of which are listed here. \nPlease proofread the entire paper including the appendix.\n\nVarious SVM based techniques  have proposed for OSD --> please correct as \"have been proposed\"\n\nThe grammar in the following sentence is not right. Not sure what the subject of the second part of the setence is starting with \"while remain\" or (remaining).  \n\n\u2026 thei predicted uncertainty scores. Should be \"their\".\n\nThis process is known is exploitation \u2026 Should be \"as exploitation\"\n\nIt was not clear how (w and \\beta) were different than (w' and \\beta'). Not mentioned in the text. \n\nQuality/Novelty:\n\nComposite scheduler idea and the way to jointly optimize model parameters and  hyperparameters of the composite scheduler by bilinear optimization and population based training seems to be novel.  The connection between Adaboost and AREO is also interesting.\n\nReproducibility:\n\nLink to code is provided. However, it is hard to judge whether the results are reproducible without running the code given some key parameters are not discussed (such as P).\n\n\n",
            "summary_of_the_review": "The paper studies an important aspect of open-set classification often overlooked in the literature. The proposed idea of using composite scheduler function for adaptive evidential learning is quite intriguing. However, there were some concerns about whether the improvement comes from training P different models at the same time or by optimizing the scheduler function that adjusts the size of the uncertainty set. An ablation study would have been useful to disentangle these two components. It would also be very useful if the weights assigned to different samples during training and evidence computed for open-set samples during testing could have been demonstrated in a simple illustrative set-up.\n\nQuestions: \n\n1. It was not also immediately clear what the significance of optimizing \\beta would be. SF_m functions can be prefixed,  perhaps a large number of them with different \\beta as it is commonly done in composite kernel learning techniques[*]\n\n*Fung, G., Dundar, M., Bi, J., & Rao, B. (2004, July). A fast iterative algorithm for fisher discriminant using heterogeneous kernels. In Proceedings of the twenty-first international conference on Machine learning (p. 40).\n \n2. Training uses the Adam optimizer with a fixed learning rate of 0.001. Neural nets tend to  learn from easy samples (dominant patterns) in earlier epochs and from difficult samples (rare patterns) in later stages. However the fact that no learning rate scheduler is used may not allow enough time for the network to learn from difficult samples, because training may stop prematurely because of the high learning rate without fully exploring the weight space. When comparing the proposed approach against state-of-the-art it may be more convincing if the AREO is also compared against an evidential network where the learning rate is gradually decreased. \n\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2115/Reviewer_gFy7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2115/Reviewer_gFy7"
        ]
    },
    {
        "id": "IZDnZMJFAY",
        "original": null,
        "number": 3,
        "cdate": 1666461067019,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666461067019,
        "tmdate": 1666461067019,
        "tddate": null,
        "forum": "3yJ-hcJBqe",
        "replyto": "3yJ-hcJBqe",
        "invitation": "ICLR.cc/2023/Conference/Paper2115/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors focus on open set recognition (detection) and propose a method when the known classes are imbalanced. The proposed method is built based on evidential learning and distributionally robust optimization. To this end, the authors first integrate the evidential learning loss function to distributionally robust optimization and introduce distributionally robust evidential optimization. Then Adaptive Robust Evidential Optimization method is proposed by utilizing multi-schedular function. The main idea is to introduce an optimal training behavior that gives sufficient attention to the difficult samples and minority class and at the same to detect the unknown class samples with high accuracy. The authors show the connection between their proposed method and AdaBoost and compare their method to the related methods. Better accuracies are reported against the rival tested methods.",
            "strength_and_weaknesses": "The main strengths of the paper can be summarized as follows:\ni) Despite some Grammar mistakes, the paper is mainly well written.\nii) Although the authors build their proposed method based on the known methods, the resulting method seems effective. Especially, integrating evidential optimization loss in distributionally robust optimization makes sense. \niii) Showing the theoretical connections between the proposed method and AdaBoost is a plus.\niv) The proposed method outperforms other tested methods.\nThe main weaknesses of the paper can be summarized as follows:\ni) There are some missing references related to general open set recognition methods. Especially there are recent successful open set recognition methods that estimate compact class acceptance regions for open set recognition. These methods must be also discussed in the paper and used for experimental comparison.\nii) The authors use mAP scores for assessing accuracy. However, most of the papers reported Area Under the ROC curve (AUC) scores since this is the common setting used for measuring the detection of performance of the unknown samples.  Therefore, please report AUC scores as well.\n\nMinor Issues:\nThe authors use the term \u201cclose set\u201d, but the correct term is the closed set. Please correct it. Also, there are some minor Grammar mistakes that must be corrected (e.g., have been proposed \u2013 page , at the beginning of Section 2).\n\nReferences:\n[1] G. Chen, P. Peng, X. Wang, Y. Tian, Adversarial reciprocal points learning for open set recognition, in: arXiv:2103.00953, 2021.\n[2] P. Perera, V. I. Morariu, R. Jain, V. Manjunatha, C.Wigington, V. Ordonez, V. M.  Patel, Generative-discriminative feature representations for open-set recognition, in: CVPR, 2020.\n[3] H.-M. Yang, X.-Y. Zhang, F. Yin, Q. Yang, C.-L. Liu, Convolutional prototype network for open set recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence (2020) 1\u20131 doi:10.1109/TPAMI.2020.3045079.\n[4] H. Cevikalp, B. Uzun, O. Kopuklu, G. Ozturk, Deep compact polyhedral conic classifier for open and closed set recognition, Pattern Recognition 119 (2021).\n[5] A. R. Dhamija, M. Gunther, T. E. Boul, Reducing network agnostophobia, in: Neural Information Processing Systems (NeurIPS), 2018.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is written well and the proposed method makes sense and it has some novelty. Experimental studies support the main claims regarding the proposed method. The authors provide algorithm for the proposed methods, but the source codes are needed to reproduce the experimental results.",
            "summary_of_the_review": "In general this is a good paper. The proposed method is built based on existing methods, yet it has some novelty. Experimental results support that the proposed method outperforms related methods. However, some important references on open set recognition are missing, the paper also lacks comparison to these methods as well. Also, in addition to mAP scores, AUC scores should be reported as well. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2115/Reviewer_FxMN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2115/Reviewer_FxMN"
        ]
    },
    {
        "id": "ZTIE_8Ov6Tb",
        "original": null,
        "number": 4,
        "cdate": 1666666550340,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666550340,
        "tmdate": 1666666550340,
        "tddate": null,
        "forum": "3yJ-hcJBqe",
        "replyto": "3yJ-hcJBqe",
        "invitation": "ICLR.cc/2023/Conference/Paper2115/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors address the problem of imbalanced data in open set detection (OSD) and proposed AERO (Adaptive Robust Evidential Optimization).  Based on evidential deep learning (EDL), their approach includes an uncertainty mass for the unknown class, which is used for detecting the unknown class.  Also, they use DRO (Distributionally Robust Optimization) to handle imbalanced data via learning weights for each instances to focus on harder instances.    They propose distributionally robust evidential loss (DREL), which is a combination of DRO and EDL.  They replace the loss function for each instance in DRO with the loss function from EDL.  To learn from easy to hard instances (hence not focusing only on hard instances), they propose MSF (mutli-scheduler function) to adaptively increase the size of uncertainty set.  MSF is a weighted sum of multiple scheduler functions, and the weights are learned.  Adaptive Robust Evidential Loss (AREL) adds an adaptive size of the uncertainty set $\\eta$ according to MSF to DREL.   To increase the chance of minority instances to be included in the uncertainty set (in additional to hard majority instances), they increase the weights for the minority instances.  During training, they alternate between AREL loss and MSF loss.  In their theoretical analysis, they link their approach to AdaBoost.\n\nThey compare their proposed AREO with 8 existing algorithms and 5 datasets.  Their results indicate AREO compares favorably.\n",
            "strength_and_weaknesses": "Strengths:\n\n1.  The problem of OSD with imbalanced data is interesting\n\n2.  The empirical results indicated the proposed AREO compares favorably with 8 algorithms and 5 datasets.\n\nWeaknesses:\n\n1.  The proposed AREO is a combination of  the existing approaches DRO and EDL, with MSF (multi-scheduler function), which also is not new.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written.\n\nBelow Eq.6: $\\eta$ could be defined.  The uncertainty set seems to be central to the approach, a clear description of it would be helpful.\n\nThe novelty is not high because the proposed AREO is a combination of existing approaches.",
            "summary_of_the_review": "The paper addresses an interesting problem in OSD with imbalanced data.  The authors proposed combining existing EDL (for OSD) and DRO (for imbalanced data) with MSF (for adaptively increasing the uncertainty set).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2115/Reviewer_HU7D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2115/Reviewer_HU7D"
        ]
    }
]