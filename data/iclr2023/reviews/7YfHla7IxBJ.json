[
    {
        "id": "yHyVx8ZKWON",
        "original": null,
        "number": 1,
        "cdate": 1666216543504,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666216543504,
        "tmdate": 1668971268924,
        "tddate": null,
        "forum": "7YfHla7IxBJ",
        "replyto": "7YfHla7IxBJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6550/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "For small-scaled sequential data, transformers tend to overfit, while RNNs has better inductive bias that prevents the overfitting. However, RNN can't be trained in a parallel way like transformers. In this work, the authors find that a linear RNN has a simple form of masked linear aggregation, which can be formulated as a specific self-attention module, thus can be easily incorporated in to transformers and jointly trained in a parallel way. The authors propose to combine regular self-attention and RNN module together, and use a gating function to decide the weight of the two paths, making it possible to fit data with different scales. Experiments on several sequential modeling tasks show the advantage of the proposed combination of transformers and RNNs.",
            "strength_and_weaknesses": "+ The motivation is clear and the algorithm is sensible.\n\n+ The proposed method is tested on several benchmarks.\n\n- RNN block discussed in the paper is basically a masked linear aggregation of the tokens (``masked'' means each token can only attend to the previous tokens), with the aggregation weights ($P_{mask}$) specially designed. It would be helpful if the authors can compare to the baseline where the $P_{mask}$ is learnable, i.e., using a learnable masked linear aggregation. Another baseline would be a learnable unmasked linear aggregation. These comparisons can tell us if it's the RNN formulation that matters or just the linear aggregation.\n\n- The authors argue that, transformers on small-scale recurrent data will overfit, thus introducing the RNN module which has a better inductive bias can help prevent the overfitting. However, there is a learnable gate-control parameter to decide whether the model should rely more on self-attention or RNN. Won't that encourage the model to rely more on self-attention in order to overfit the training data?\n\n- Since the authors use a linear RNN, I wonder how much model capacity are we losing by linearizing the RNN, i.e., how huge is the gap between linear and non-linear RNN, performance-wise?\n\n- In Section 3, the authors denote by $p_{total}$ the total temporal patterns in the data. How is this value defined?\n\n- Section 2.1, $b \\in \\mathbb{R}^{d_{in}}$ -> $b \\in \\mathbb{R}^{d}.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is easy to follow. I've read the theoretical justification and found no major issues. The method is somewhat novel in that it shows the equivalence between (linear) RNN and self-attention, although similar results have been demonstrated in other areas such as vision, for example, the equivalence between convolution and self-attention.",
            "summary_of_the_review": "Overall the paper is solid in both theoretical and empirical parts, although comparisons to the baseline models are missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6550/Reviewer_ZrmK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6550/Reviewer_ZrmK"
        ]
    },
    {
        "id": "LVq6RJH4O-3",
        "original": null,
        "number": 2,
        "cdate": 1666634282815,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634282815,
        "tmdate": 1666634282815,
        "tddate": null,
        "forum": "7YfHla7IxBJ",
        "replyto": "7YfHla7IxBJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6550/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Transformers have less inductive bias; while it is well generalized. RNNs have large inductive bias; while it is sample-efficient. Authors proposed the REM which combines the advantages of Transformer and RNNs which are famous sequential models. Experimental results have shown the effectiveness of the proposed model.\n",
            "strength_and_weaknesses": "It is novel enough to combine the advantages of two famous models (Transformer, RNN). Also, the combining method looks applicable to a variety of scenarios. The experimental results are impressive, showing superior performance to previous Transformer.\n\nI think the draft would become better if there is a more complete explanation and figures about the self-attention with recurrence (RSA) operation. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The draft includes details for implementing the methods. ",
            "summary_of_the_review": "I think the novelty of this draft is enough for the publication and the experimental results are impressive. English is good enough as well. I recommend weak accept for the draft.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6550/Reviewer_f1my"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6550/Reviewer_f1my"
        ]
    },
    {
        "id": "F19iOIELCf9",
        "original": null,
        "number": 3,
        "cdate": 1666663024954,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663024954,
        "tmdate": 1670344134218,
        "tddate": null,
        "forum": "7YfHla7IxBJ",
        "replyto": "7YfHla7IxBJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6550/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper tackles the problem of endowing Transformers with the ability to encode information about the past via recurrence. The proposed architecture can leverage the recurrent connections to improve the sample efficiency while maintaining expressivity due to the use of self-attention. ",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is easy to read, and generally well written.\n- The paper evaluates the proposed method on various different tasks such as time-series forecasting, code and language modelling. The paper augments the proposed method to various different transformer variants and compares the performance with respect to the unmodified baseline. \n\nWeakness:\n\n- The problem of integrating recurrence and self-attention is an important research problem. There exists some existing ways on how to augment transformers with recurrence such as Temporal Latent Bottleneck [1] and Block-Recurrent Transformers [2]. The idea behind TLB is to \"divide\" the sequence into chunks, and within a chunk use self-attention and to access information across chunks the model needs to use recurrence. It would be useful to compare the proposed method to these variants to futher analyse the pros and cons. \n- It may also be useful to study the proposed method by varying the capacity of the network to see how well the underlying idea scales. \n\n[1] Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning, https://arxiv.org/abs/2205.14794 (NeurIPS'22) \\\n[2] Block Recurrent Transformers, https://arxiv.org/abs/2203.07852 (NeurIPS'22)",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is easy to read.\n\nQuality: The paper tries to tackle an important problem.\n\nNovelty: Even though the problem is not \"new\" per se, but the underlying idea is interesting.\n\nReproducibility: The paper should be easy to reproduce.",
            "summary_of_the_review": "The paper proposes a way to incorporate recurrence and self-attention by modifying the positional encoding. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6550/Reviewer_mvWh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6550/Reviewer_mvWh"
        ]
    }
]