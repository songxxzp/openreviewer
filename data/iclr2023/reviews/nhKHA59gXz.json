[
    {
        "id": "A4ZBvwsX7A",
        "original": null,
        "number": 1,
        "cdate": 1666636382536,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636382536,
        "tmdate": 1666636382536,
        "tddate": null,
        "forum": "nhKHA59gXz",
        "replyto": "nhKHA59gXz",
        "invitation": "ICLR.cc/2023/Conference/Paper3250/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the edge of stability phenomenon. They propose to incorporate the cubic term in the convergence analysis and show that there is a self-stabilization property caused by this cubic term for general nonconvex optimization problems. Moreover, they show that GD is inherently regularizing the sharpness, i.e., it is close to a constraint optimization problem that explicitly adds the sharpness as the constraint. Last, they provide very promising simulations showing that the predicted trajectory mimics the actual GD trajectory on many realistic tasks.",
            "strength_and_weaknesses": "## Strength\n- This work provides a new direction to understand the edge of stability problem. Though the author argues that their method can only explain the EoS phase. By the argument in Stage 1, I feel like it can also be used to explain the progressive sharpening phase. It would be very great if the author can help me to understand what's the gap to preclude them to claim that this method can be used to study the first phase.\n- I really like the motivation section (Section 4), which is very clear and convincing to me.\n\n## Weaknesses\n- At the end of the first paragraph in Section 1.2, they say that \"Unlike progressive sharpening ..., self stabilization is a general property of gradient descent.\" However, in Section 5, the main theorem relies on many assumptions, which are only verified on neural network training. Hence, I am a little bit concerned about its generality. Apart from the numerical verification, I can only find the toy example provided in Appendix B, which is just a cubic function. It would be great if the author can provide either more general examples, or verify that those assumptions hold for a general function class. \n- As argued in Section 4.3, they show that the distance between $\\theta_t$ and $\\theta_t^{\\dagger}$ is bounded by $\\delta$. However, it is unclear how large this $\\delta$ can be. Note that $\\delta=\\sqrt{2\\alpha/\\beta}$. In the appendix, it can be seen that $\\alpha$ is not small. Hence, it is not clear to me why $\\theta_t$ and $\\theta_t^{\\dagger}$ are close.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is good and clear. The idea of incorporating the cubic term is novel to me.",
            "summary_of_the_review": "Please see the above comments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3250/Reviewer_qpmy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3250/Reviewer_qpmy"
        ]
    },
    {
        "id": "f1q60hYqJY",
        "original": null,
        "number": 2,
        "cdate": 1666783953063,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666783953063,
        "tmdate": 1666783953063,
        "tddate": null,
        "forum": "nhKHA59gXz",
        "replyto": "nhKHA59gXz",
        "invitation": "ICLR.cc/2023/Conference/Paper3250/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study a phenomenon they call \"self-stabilization\", which is proposed to explain common empirical observations such as sharpening and convergence at the edge of stability. \n\nFormally, the authors consider the iterates of (discrete time) gradient descent and assume that the product between the gradient of the objective and the gradient of sharpness is negative, where sharpness is defined as the largest eigenvalue of the Hessian. For simplicity, the authors also assume that the Hessian has at most one negative eigenvalue. Under these assumptions, the authors present a simplified local analysis of the self-stabilization phenomenon. Then, they extend this analysis to general loss functions, but under much more restrictive assumptions, such as Assumption 4, on the gradients of sharpness and loss. The main result is given in Theorem 1, which shows how, up to high-order terms, the loss and sharpness coevolve.\n\nFinally, the authors present numerical evaluations serving as a justification for their assumptions and validating some of their claims.",
            "strength_and_weaknesses": "## Strengths  \n1. The work sheds some light on the recently observed phenomena in deep learning, which may help us understand how deep networks learn the data.\n2. The proposed explanation is quite intuitive and is supported by empirical evaluations.\n3. The paper has a good flow and starts with simple ideas that finally lead to the main result.\n\n## Weaknesses  \n1. From the mathematical point of view, this paper is not strong. Assumption 4 intuitively implies self-stabilization because it guarantees a significant negative correlation between the gradients of loss and sharpness. While the conclusions still require quite a bit of technical work to do, which the authors did well, the assumptions are very stringent. I find this to be the main weakness of this paper.\n2. The studied problem rarely comes with deterministic gradients, and the impact of noise in stochastic training is not considered.\n3. The implications of the results do not seem to change our understanding of how neural networks need to be trained. The notion of self-stabilization does not seem to suggest much more than the edge of stability already tells us.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and I'd like to thank the authors for their great effort. There are a few small issues with the presentation, some of which I list below.\n\n### Minor issues\nI did not understand the meaning of this sentence: \"Note that $y_t$ is approximately to the change in sharpness from $\\theta^\\star$ to $\\theta^t$\". Maybe \"approximately equal\"?   \nIt's a bit strange that $^\\star _{v_t}$ is defined in Appendix C and not explained, at least on some basic level, in Section 5.2. Since $^\\star _{x_t}$ and $^\\star _{t_t}$ depend on $^\\star _{v_t}$, it makes it a bit harder to understand their meaning.  \nI did not understand why it is fine to assume that $\\min{t\\le \\mathcal{J}}|^\\star _{x_t}| \\ge c_1\\delta$.\n",
            "summary_of_the_review": "I enjoyed reading the paper and I believe it will be of interest to the community. While the assumptions behind the theory are far from perfect, the derivation is still of interest. The empirical evaluations are meaningful and support the theory.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3250/Reviewer_dCeH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3250/Reviewer_dCeH"
        ]
    },
    {
        "id": "pGCBtErD1S",
        "original": null,
        "number": 3,
        "cdate": 1666794499531,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794499531,
        "tmdate": 1666794499531,
        "tddate": null,
        "forum": "nhKHA59gXz",
        "replyto": "nhKHA59gXz",
        "invitation": "ICLR.cc/2023/Conference/Paper3250/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the Edge-of-Stability effect from a rather general perspective. It identifies progressive sharpening and the oscillatory behavior of optimization trajectories at EoS as resulting simply from the alignment of the loss and sharpness gradients. This alignment is determined by a particular cubic term of the Taylor expansion of the loss. The resulting dynamics is related to the well-known Lotka-Volterra predator-prey model. The paper first discusses a simplified scenario where this effect can be easily demonstrated and understood. After that a theorem is stated addressing a general setting. Finally, a series of experiments with real world data sets and neural networks is presented, showing agreement with theory. ",
            "strength_and_weaknesses": "I quite like this paper. There are several other current studies of EoS (e.g., https://openreview.net/forum?id=p7EagBsMAEO, https://openreview.net/forum?id=R2M14I9LEwW), but the present paper paints an especially general and appealing picture of this effect. The exposition is very clear. The theory is confirmed by multiple experiments with real data.   \n\nOne natural question that does not seem to have been answered in the paper is why can we generally expect Assumption 1, the key alignment assumption, to hold? It seems that the authors leave this question for future work?",
            "clarity,_quality,_novelty_and_reproducibility": "I'm not an expert in EoS, but the results seem novel and interesting to me. \nThe appendix contains various additional details and the proof of the main result (Theorem 1). I didn't check the proof, but the theorem looks plausible.  ",
            "summary_of_the_review": "A good paper, accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3250/Reviewer_UEL4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3250/Reviewer_UEL4"
        ]
    }
]