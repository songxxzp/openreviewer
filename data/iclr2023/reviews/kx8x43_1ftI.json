[
    {
        "id": "qz-iOzqqAi",
        "original": null,
        "number": 1,
        "cdate": 1666132681864,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666132681864,
        "tmdate": 1668808502314,
        "tddate": null,
        "forum": "kx8x43_1ftI",
        "replyto": "kx8x43_1ftI",
        "invitation": "ICLR.cc/2023/Conference/Paper1240/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a method that performs massive uniform exploration of simulated environments and then trains goal-conditioned SAC with an annealed success condition to achieve states discovered during exploration. They demonstrate this exploration procedure to be superior on locomotion environments to other methods.",
            "strength_and_weaknesses": "## Strengths\n\n**Clarity:** \n\n- The figures are, for the most part, very nice and clear.\n\n**Experiments/Results:**\n\n- ChronoGEM does work well as an exploration procedure, and GIFs in the supplementary demonstrate that their goal-conditioned policy can reach many arbitrary poses.\n- C3PO is able to do well on goal distributions generated by other methods, verifying the fact that ChronoGEM is a good exploration algorithm.\n\n## Weaknesses\n\n**Clarity:** \n\n- Quite a few grammatical hiccups throughout. Please fix (some that I found are listed in the minor details heading)\n\n****************Framing:****************\n\n- I somewhat doubt the utility of this work. It requires a specific setting (a cheap simulator that can be sampled en masse in parallel, and reset to arbitrary states), and has extremely poor sample efficiency due to this assumption. Essentially, ChronoGEM tries to visit all of the states in the simulator. In anything other than locomotion tasks, (e.g. robotic manipulation) these states will mostly be useless and having so many states to try to achieve can hurt learning.\n- What is the point of being able to achieve arbitrary positions and poses? The authors state in the abstract that it would allow for easier control and be re-usable as a key building block for downstream tasks. But they don\u2019t show this. The experiments simply demonstrate that Goal-conditioned SAC can reach the position and poses given.\n- Conclusion: \u201cIn the real world, no reward function is provided.\u201d THis is true, however in the real world we also can\u2019t run ChronoGEM. This goes back to how I think the paper is not framed too well.\n\n**Experiments:** \n\n- Related to the above point about framing in achieving arbitrary positions and poses, the authors should show experiments where their method allows for better finetuning performance on some downstream tasks (maybe goal conditioned, maybe not) to validate the claims made in the abstract and intro.\n\n****************************Minor details:****************************\n\n- Grammar\n    - Intro last paragraph: \u201cits\u2019 \u2192 its\u201d\n    - Sec 2.1: \u201cnecessary bounded\u201d \u2192 \u201cnecessarily bounded\u201d\n    - \u201cup-left room\u201d \u2192 \u201ctop-left room\u201d\n    - \u201clet it enough time\u201d \u2192 \u201clet it run for enough time\u201d\n    - \u201cwe says\u201d \u2192 \u201cwe say\u201d\n- Questions\n    - In chronoGEM, how exactly do you sample the ****next**** states? Do you just reset BRAX simulation to those exact states and continue from there?\n\n**********************Formatting:**********************\n\n- The formatting is incorrect. The margins are way too small. This is unfair to other papers who fit everything into the page limit without the small margins. Regardless of whether this was intentional or on accident, I am giving a strong reject no matter what, until the authors fix this issue during the rebuttal.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, of OK quality, incrementally novel, and seemingly reproducible provided sufficient computational resources.",
            "summary_of_the_review": "While some of the results are nice, I have 3 main issues with this paper, as detailed above: 1) Framing, 2) The formatting, and 3) The experiments.\n\nEach of these is important and the formatting is critical (there are many papers already desk-rejected for incorrect formatting), and as such, I cannot recommend anything better than a strong reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1240/Reviewer_4RNg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1240/Reviewer_4RNg"
        ]
    },
    {
        "id": "Y0BSNKXc8L4",
        "original": null,
        "number": 2,
        "cdate": 1666392933288,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666392933288,
        "tmdate": 1666398143483,
        "tddate": null,
        "forum": "kx8x43_1ftI",
        "replyto": "kx8x43_1ftI",
        "invitation": "ICLR.cc/2023/Conference/Paper1240/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method called Entropy-Based Conditioned Continuous Control Policy Optimization (C3PO) that tackles the problem of learning a general goal-conditioned policy in two stages. In the first stage, using an exploration algorithm called ChronoGEM, an exploration dataset consisting of diverse goals is collected. In the second stage, the goals collected in the first phase are used as goal targets in goal-conditioned RL with soft-actor critic (SAC) as the base algorithm. The paper demonstrates the method\u2019s effectiveness on Gym environments: walker, hopper, halfcheetah, and ant. It demonstrates that C3PO is not only able to reach a wider distribution of states compared to RND and SMM, training goal-conditioned policies on these states improves the ability of these policies to solve a range of goals.",
            "strength_and_weaknesses": "Strengths:\n- The ChronoGEM algorithm appears empirically to generate more uniform state distributions compared to random walks, SMM, and RND exploration strategies on the maze and ant environments.  It is also conceptually simple to understand and implement.\nThe experimental results training goal-conditioned RL on the goals collected by ChromoGEN outperforms policies trained on the other goal distributions, which is a positive signal that the distribution of goals for ChromoGEM is relatively wide.\n\nWeaknesses:\n- The theoretical argument in section 2.1 does not seem convincing to me. The statement seems to be that the state visitation will eventually converge to be uniform over the entire state space, but the theorem in Appendix A only demonstrates that the states selected for the next step of the greedy procedure can be approximately uniform from KN sampled states. As the authors mention, playing uniform actions would not necessarily lead to a uniform sampling over the possible next states. So it seems like the \u201cinductive\u201d step here is missing. So I don\u2019t agree with the statement in the conclusion that the method generates \u201chigh entropy behaviors, in theory (Theorem 1)\u201d. \n- I think it would help the paper to have a comparison to a method like Skew-Fit[1] that has a similar exploration strategy to the one proposed in the paper by rebalancing the distribution of states to set as goals.\n- There isn\u2019t an ablation study conducted on the impact of the tolerance annealing component from SAC to see how much of the improved performance comes from that compared to the goals from ChronoGEM.\n- The EWGA metric is an interesting way to evaluate the performance across evaluation sets that may have varying difficulties, but why not evaluate using another strategy like uniformly sampling goals across the [x, y, z] space? That seems like it would be less noisy, and that observation space has already been constructed.\n\n[1] Skew-Fit: State-Covering Self-Supervised Reinforcement Learning (Pong et. al, 2019)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clearly written, and experiments are conducted thoroughly with multiple random seeds. The method is described with sufficient implementation details that it seems likely to be reproducible.\n\nQuestions: \n- Given that the entropy needed to be upper bounded in section 3.2, how is the entropy used for the computation of the EWGA metric?\n- Is the entire observation of each environment replaced by the xyz positions, or are they appended to the default state spaces?\n\nOther comments/nitpicks/typos:\n- HalfCheetah has inconsistent capitalization throughout the manuscript\n- I think the first sentence in Section 3.4 is overstated: while it does seem that ChronoGEM performs the best for the studied environments, the statement is rather general and this hasn\u2019t been shown for all goal-conditioned setups.\n",
            "summary_of_the_review": "The proposed method has promising empirical results, but I find the current version of the theoretical justification unconvincing and the analysis of why the method works well could be more thorough. Therefore I do not think the paper should be accepted in its current form, but if the authors can address many of my concerns above, that would be very helpful.\n\nI also want to leave a note that this paper seems to be improperly formatted: the margins are much smaller than in the original template, the ICLR header is missing, and the title / author list formatting is nonstandard.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1240/Reviewer_3jQz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1240/Reviewer_3jQz"
        ]
    },
    {
        "id": "ac2Y6lon4c4",
        "original": null,
        "number": 3,
        "cdate": 1666628877164,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628877164,
        "tmdate": 1666628877164,
        "tddate": null,
        "forum": "kx8x43_1ftI",
        "replyto": "kx8x43_1ftI",
        "invitation": "ICLR.cc/2023/Conference/Paper1240/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "this paper is proposing a new unsupervised reinforcement learning scheme for being able to train a more taskable agent that can reach a large base of goals in the environment. the motivation for this work is to not only collect the data and have the agent be able to explore but at the same time be able to train a goal condition to policy such that if we wanted the agents to reach a particular state in the world, we could specify that during test time. the method proposes to do a type of incremental search outward from the initial State distribution and slowly add states that the agent can learn how to reach in this case in an effort to be able to sample from the state distribution such that goals can be reached uniformly across the state space. the method does perform some analysis showing that qualitatively the agents appear to learn and display a more uniform type of visitation strategy across the state space while visiting goals. and the method is compared across a handful of mujoco-based robotic simulations.",
            "strength_and_weaknesses": "pros\n- The paper proposes finding a more helpful policy that can be trained inside the environment compared to prior methods. Most prior curiosity-based methods do help the agents learn to discover different states that are more difficult to reach via exploration bonuses, but this method also seems to train a goal condition policy making it more taskable as well as prior methods.\n- They are also able to show through some illustrative examples that their proposed method can or at least appears to explore the environment in a more uniform manner.\n\ncons\n- There are some limitations in the comparison and analysis that should be addressed. In particular, some prior methods are mentioned in the paper that would be important to compare to understand better if the proposed method is better than prior work. this also notes and is based on the concept that they're training a goal condition policy to explore the state distribution and not just a curiosity-based method.\n- This method appears only to be applicable to simulators. As the authors note, it's also extremely data-hungry.",
            "clarity,_quality,_novelty_and_reproducibility": "Additional Comments on the paper:\n\n- It's not discussed very clearly in the paper, but there are obvious practical challenges in being able to create a policy that can reach all of the states in a very uniform distribution. There will be more probability for states that are closer to the initial State distribution for the trained RL policy.\n- What are the assumptions that go along with the proposed algorithm in section 2.1? From this somewhat difficult-to-understand analysis it seems that one additional assumption is that the environment is deterministic and can reach these states given similar paths and actions. Generally, the assumption and motivation that these methods will work really well in simulation can be a problematic hypothesis and can't imply that this method will not be helpful on any real-world problems that have data limitations, stochasticity and partial observation.\n- It's not clear what is the goal of algorithm one. The first sentence after that algorithm states that it only requires NKT samples, but it doesn't say what these are required for.\n- The stopping condition for individual episodes that depends on $$\\epsilon$$, how was $$\\epsilon$$ chosen for each of the environments? In particular, how was it chosen such that it does not bias the method proposed in the paper compared to other methods? \n- The appendix links in the paper appear to be broken.\n- Why are the XYZ positions of everybody part added to the state in the simulation and not just the position information for the center of the agent? For example, the center of mass could be used or the agent's position of the root link.\n- The beginning of section 3.2 on entropy upper bounds needs citations to inform the reader where these axioms are derived from.\n- The results in figure 4 that compare different methods depending on the measure proposed for entropy over a prior distribution appear to be somewhat self-fulfilling. The method proposed in this paper may be optimizing almost this exact objective, while prior methods are not. how can we be sure that this is the most critical form of the objective to be optimized and that the other methods aren't performing much better, giving some other analysis of the entropy over the state space?\n- Why is this method not compared to a PT and APS? both of those methods perform types of entropy maximization to explore states in the environment. this makes them appear to be very likely candidates for solving a similar problem.\n- In addition, this method should also be compared to the UPSIDE algorithm.\n\nAdditional works that should be cited in the paper:\n- Skew-Fit: State-Covering Self-Supervised Reinforcement Learning, Vitchyr H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, Sergey Levine",
            "summary_of_the_review": "The paper proposes a helpful algorithm to allow for training a goal-based agent to reach many goals. The work requires further details and comparisons to prior methods to understand if it is an improvement over prior work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1240/Reviewer_nd1d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1240/Reviewer_nd1d"
        ]
    }
]