[
    {
        "id": "EGj1tSMyyEn",
        "original": null,
        "number": 1,
        "cdate": 1666508641796,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666508641796,
        "tmdate": 1666508705318,
        "tddate": null,
        "forum": "kAfl36VUr95",
        "replyto": "kAfl36VUr95",
        "invitation": "ICLR.cc/2023/Conference/Paper3632/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a differential private learning framework for consensus learning over networks. Some theoretical results are established for the proposed algorithms. Several numerical studies are demonstrated.",
            "strength_and_weaknesses": "Strength:\nThe targeted problem is interesting. This paper is easy to follow.\n\nWeakness:\n1. Differential private decentralized learning has been widely studied. The work aims to provide a general framework by simply combining SGD and SPGD algorithms. The contribution seems to be incremental.\n2. In terms of the theoretical study, can the authors provide a detailed analysis of optimization error with differential private mechanisms? What are the convergence rates of the algorithms with SGD, and SPGD, respectively? \n3. We would expect a more comprehensive experimental study. For example, the current experiments only focus on least square regressions with/without l1 regularized term. Can the proposed algorithm apply to classification problems such as logistic regression? What is the effect of the proposed algorithms under different privacy levels (epsilon, delta)? \n4. Also, the proposed algorithm involves some parameters. The authors should provide a sensitive study on these parameters.\n5. The text in the figures is too small.\n6. How about the support recovery of the regularized least square regression with an l1 penalty with a noise-adding mechanism? It seems that the noise-adding ruins the spare structure easily. Can the authors provide an analysis of the noise-adding mechanism on structure recovery in addition to generalization error?\n7. In Theorem 5, what is the sequence {a_m}? It makes delta^{\\prime} different.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well organized but the presentation has minor details that could be improved.\n\nQuality: The paper has minor technical flaws. For example, the proof of a theorem has some fixable errors or the experimental evaluation is weak. \n\nNovelty: The paper contributes some new ideas or represents incremental advances.",
            "summary_of_the_review": "This paper provides a differential private learning framework for decentralized learning. However, the contribution seems to be incremental. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3632/Reviewer_HapE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3632/Reviewer_HapE"
        ]
    },
    {
        "id": "1Ezs661vdR",
        "original": null,
        "number": 2,
        "cdate": 1666648259555,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648259555,
        "tmdate": 1666648259555,
        "tddate": null,
        "forum": "kAfl36VUr95",
        "replyto": "kAfl36VUr95",
        "invitation": "ICLR.cc/2023/Conference/Paper3632/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a general DP decentralized learning framework based on stochastic Decentralized Kras\u0002nosel\u2019ski\u02c7\u0131\u2013Mann (D-KM) iteration, which can represent the common first-order algorithms, like SGD, SPGD, and ADMM. Also, based on previous truncated Laplace mechanism, they proposed a truncated generalized Gaussian mechanism. ",
            "strength_and_weaknesses": "Strength:\n1. Achieving privacy in decentralized learning is interesting and important.\n2. The paper gives general framework based on D-KM iteration. \n3. Proposed a truncated generalized Gaussian mechanism with higher utility. \n\nWeakness:\n1. Lots of decentralized learning with DP works are not considered.  Please check the corresponding works in \nLi, Zhize, Haoyu Zhao, Boyue Li, and Yuejie Chi. \"SoteriaFL: A unified framework for private federated learning with communication compression.\" arXiv preprint arXiv:2206.09888 (2022).\n2. Please provide the experiments comparisons with previous works. For example, In Figure 4, please compare the proposed methods with previous DP-ADMM algorithms to show the effectiveness.  The same things for Figure 2 and 3. \n3. The motivation for proposing truncated generalized Gaussian mechanism is not clear. Instead of the advantage of $l_2$ sensitivity, what is the advantage of truncated generalized Gaussian when compared with Laplace noise? Also, here is a work also proposed truncated Gaussian mechanism. [1] Cesar, Mark, and Ryan Rogers. \"Bounding, concentrating, and truncating: Unifying privacy loss composition for data analytics.\" In Algorithmic Learning Theory, pp. 421-457. PMLR, 2021.\n4. Please also provide the generalization and optimization error bound with previous DP-SGD, DP-SPGD and DP-ADMM. \n5. Please provide more experiments over large datasets and neural networks models. ",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of this paper is limited, which directly adds noise to the stochastic Decentralized Kras\u0002nosel\u2019ski\u02c7\u0131\u2013Mann (D-KM) iteration. ",
            "summary_of_the_review": "Based on the weaknesses in previous part, the paper needs more concrete comparisons with previous works from theoretical and experiments and insights. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3632/Reviewer_9m8T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3632/Reviewer_9m8T"
        ]
    },
    {
        "id": "MMpgFkfbEz7",
        "original": null,
        "number": 3,
        "cdate": 1666732815353,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666732815353,
        "tmdate": 1670370327258,
        "tddate": null,
        "forum": "kAfl36VUr95",
        "replyto": "kAfl36VUr95",
        "invitation": "ICLR.cc/2023/Conference/Paper3632/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops a new differentially private (DP) framework to tackle the problem of decentralised consensus learning. This framework is based on the stochastic Decentralised Krasnosel'skii-Mann (D-KM) iteration (its privatised version referred to as \"DP-KM\" in this paper). It provides both theoretical and empirical results for their approach.",
            "strength_and_weaknesses": "Strength:\n1. A new private framework is provided to deal with this problem that utilises generalised versions of distributions, like Laplace and Gaussian. There is mathematical rigour in this paper, which I appreciate.\n\nWeaknesses:\n1. One major complaint I have is about the writing quality of this paper. It pretty much makes it impossible for me to evaluate its significance and quality. I'll describe more in the next section.\n2. I don't completely understand the objective or the motivation of this paper, to be honest. What assumptions are being made/removed about the loss functions, and why is this framework stronger and more general-purpose than in the prior work we have seen so far? What is it trying to improve on -- what is the prior work here? What are the baseline (trivial or non-trivial) approaches I could compare this work with? This point does tie in with the previous point I made here.\n3. Theoretical comparisons with some prior work could have given more context.",
            "clarity,_quality,_novelty_and_reproducibility": "I have a number of issues with the writing/clarity of this paper.\n1. The major one -- the graphs are impossible to read and interpret because of the font and the size. How am I supposed to see anything here? Am I supposed to zoom in and squint to see what is going on in there by looking at blurred images and text? This will sound harsh, but I really am not willing to do that because I feel that it is the responsibility of the authors to convince the reviewer about the quality of their work. The text in Section 6 is not enough for me to be able to justify what you obtained in those plots either. Where is the discussion on your experimental results? I would recommend by starting off by improving this section to be able to actually convey what you achieved through this algorithmic framework.\n2. More context of this high-level problem of decentralised consensus learning would be more useful to differentiate with other versions of distributed learning. As a reader, I would want to understand your contributions in terms of the problem being solved first. Yes, there is a section on the problem statement, but it's not really sufficient to distinguish from the other well-known versions of the problems under the same umbrella.\n3. How are the assumptions you make about the loss function (or anything else) different from those in the prior work? My understanding was that this work was also supposed to weaken those, and obtain more a general framework.\n4. Around Definition 2, it might be helpful to provide more context on this D-KM algorithm. The concepts and the ideas seem to be coming out of thin air right now. For example, what is the high-level idea of the bigger algorithm that uses this iterate? What algorithmic techniques are relevant for this particular design?\n5. Also, is improving on the communication complexity one of the main highlights of this approach? The end of page 4 vaguely has a comment of this nature. If so, then how does it compare with that of the relevant prior work?\n6. In Theorem 1, why just bound the expected sensitivity? How does this translate to a bound on the worst-case sensitivity? More context here would help! Also, the second summation should have $K$, instead of $k$.\n7. The results and the settings are stated in very confusing ways, for example, in Theorem 5. I think the authors could do a better job of providing more context behind what the results are about and what they actually say. It's pretty hard to interpret the results the way they are right now.\n8. The previous point brings me to my next complaint that the organisation and the flow of this paper are a bit confusing and haphazard. Sure, the order of the sections makes complete sense, but the ordering within them didn't feel as friendly. For example, the $\\Delta_K$ sensitivity in Section 4 is defined, but its theorem is about its expected value, and then there is a table below about that for different graph topologies. What is the context here? Why do these graph topologies matter in particular? How are these three things connected? This was just one example. This may sound a bit vague, but the flow seems a bit jittery right now, in general, and maybe another iteration of revising it maybe helpful for future.\n9. Some additional results about these generalised distributions (Laplace and Gaussian) could be more meaningful. For example, the privacy guarantees in terms of the extra parameters, or some tail-bounds. They don't add much value here in terms of clarity otherwise because I don't really know how they are useful for your DP framework.\n10. Some theoretical comparisons with prior work could be useful.\n\nIn terms of the quality and the novelty, I don't really have much to say. I'm not very convinced about either of those because this paper wasn't really friendly to read or interpret.",
            "summary_of_the_review": "I don't think I got enough information from this paper to be able to provide a convincing argument in favour of accepting it. I hope my suggestions/comments on the writing help the authors.\n\nUpdate: I appreciate and respect the authors' willingness to make all the suggested edits to improve the readability. However, I see that a *very significant* portion of the paper had to be rewritten in order to introduce more clarity, so I am hesitant to change my score. The readability has improved a lot, but I think the authors might benefit from spending some more time on the presentation of the paper, and providing a more convincing argument about the problem and the technical challenges and novelty.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3632/Reviewer_Xztu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3632/Reviewer_Xztu"
        ]
    },
    {
        "id": "sw9PCGLUtr",
        "original": null,
        "number": 4,
        "cdate": 1667269416533,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667269416533,
        "tmdate": 1667269416533,
        "tddate": null,
        "forum": "kAfl36VUr95",
        "replyto": "kAfl36VUr95",
        "invitation": "ICLR.cc/2023/Conference/Paper3632/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper claims to give a general DP framework for decentralized data. The authors claim that the algorithm retains all the desirable properties that we have in non-private setting: generalization or stability and finite sample performance. ",
            "strength_and_weaknesses": "The main idea in this paper seems to be to use the private version of stochastic D-KM algorithm. The paper analyzes its sensitivity and from there uses truncated generalized Gaussian mechanism for privacy. The paper claims privacy proof for truncated generalized Gaussian mechanism as well. I believe this can be casted as one of the novel contribution of the paper, too. ",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the privacy proof for truncated generalized Gaussian is a novel contribution. For the rest of the contribution, it seems to me as more like following the familiar route of privatizing a known non-private algorithm.",
            "summary_of_the_review": "I was unable to read the proofs of the main claim, so I cannot make a judgement of their correctness. For now, I would rely that the authors have done their paper in writing the correct proof. \n\nI have one central question. Ganesh and Zhao proved that generalized Gaussian is DP for all b \\leq \\log(k), where k is the number of queries (in their paper). The current submission makes similar statement but only for b=2. How is the proof different or anyhow technically novel considering the paper by Ganesh and Zhao (2020) and adapting their proof for truncated Gaussian just like Geng et al. did for truncated Laplacian? Am I missing something here? The authors have not cited Ganesh and Zhao, so I am wondering if the authors are aware of the paper. \n\nSecond, the authors claim that they present their result for equal sample size for the ease of presentation. Is it just for the ease of presentation or is it the case that the analysis only goes through when sample sizes are equal. I would imagine that if the sample size comes from a very skewed distribution, then we should not be able to say anything meaningful about the generalization error. \n\nWhy do we call the mean parameter vector a consensus vector? Is it common in this literature?\n\nWhat is $\\lambda$? I am guessing it is the rate of the convergence to the stationary distribution.\n\nWhat is the notation $\\| \\ell \\|_\\infty$?\n\nTheorem 5: What is $C_{KL}$ and is other $\\widetilde X(i)$ defined similarly to $\\widetilde X(1)$? Theorem statement should be precise and complete on their own. \n\nThe notation $\\mathbb P$ is overloaded. \n\nDisclaimer: I am trying to read the proof. I hope to be able to engage with the authors much more during the discussion phase. My current scores are for placeholder. \n\n\n\n\n[1] Ganesh and Zhao. https://arxiv.org/abs/2010.01457",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3632/Reviewer_boyW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3632/Reviewer_boyW"
        ]
    }
]