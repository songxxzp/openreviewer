[
    {
        "id": "2_wdaubtV5",
        "original": null,
        "number": 1,
        "cdate": 1666050445256,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666050445256,
        "tmdate": 1670865031092,
        "tddate": null,
        "forum": "k7qRYoxUlB",
        "replyto": "k7qRYoxUlB",
        "invitation": "ICLR.cc/2023/Conference/Paper3406/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents autograph, a GNN-based approach to automatic loop vectorization.\nAutograph represents the program as a graph, computes an embedding for each node using a standard GNN forward pass, then trains a RL agent to predict vectorization factors based on node embeddings.\nThe paper evaluates on a set of standard CPU benchmarks, showing that autograph outperforms the most related prior work in this area, neuro-vectorizer.\n",
            "strength_and_weaknesses": "Strengths:\n* The paper providers a novel approach to a relevant problem, automatic loop vectorization\n* The results, if they stand up (see weaknesses) are strong, significantly outperforming O3 and prior work in neural loop vectorization\n\nWeaknesses:\n* The clarity of the approach is low (such that the approach could not reasonably be implemented by a reader) -- see the clarity section below\n* The quality of the evaluation is low, such that I do not have confidence that the approach actually beats the baselines (especially the comparison to neuro-vectorizer) -- see the quality section below\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nOverall, the paper is not sufficiently clear.\nPrimarily, the approach is not described precisely enough for a reasonable reader to implement the approach described in the paper.\nI included a number of quotations below that were not sufficiently clear to me.\n\n* The definitions of \"vectorization factor\" and \"interleaving factor\" in Section 1 are not clear.\n* \"In our implementation, we use PrograML to generate code graphs.\". Given that PrograML is used, what is the purpose of learning the binary classifier for link prediction\n* \"The forward pass allows the model to find the probability of the existence of an edge by computing a score between incident nodes with a function f\". What is this function $f$?\n* \"Given a sequence of graphs G(V, E)...\" What do these sequences of graphs represent? Is this a sequence of graphs for a specific input example, or is this sequence describing the training procedure?\n* \"We next learn a binary classifier ... by performing link prediction\". To be clear, the binary classifier is predicting whether a link exists or not? Or does the training process use link prediction?\n* \"... evaluated with any metrics such as area under curve...\" is AUC the evaluation metric you use?\n* \"...link score...\" What is a link score?\n* I don't understand the forall notation in equation 2.\n* Equations 3 and 4 both define h_i^(l+1). Is this a typo?\n* \"Therefore, by adopting this approach, the size of the node embedding is the same as the size of the vocabulary\". Is this ultimately the dimensionality of embedding that the GNN learns?\n* \"... is that two graphs could be identical but the vectorization parameters are totally different, due to different number of iterations in the loops\". Maybe I misunderstand the embedding technique, but if two graphs are identical wouldn't that imply that their loop conditions are identical? If that is the case, how would there be different numbers of loop iterations?\n* What are loop counts? I see a potential definition of these as \"... the number of loops in each file of a benchmark\", but I don't understand what this is.\n* \"... and further refines them in terms of better accuracy and performance improvement over O3 via graph supervised learning\" what refining is this talking about?\n* \"...the state space consists of hidden feature embeddings learned from the supervised model\". Do these states change during the execution of the agent? How?\n* \"The autograph framework incorporates GraphSAGE (Hamilton et al., 2017) to extract feature embeddings in an unsupervised fashion...\" What are these feature embeddings used for? I don't see this described anywhere in Section 3.\n\n\n## Quality\n\nThe quality of the evaluation is also not sufficient.\nI am not convinced that the presented metrics (accuracy) are appropriate for evaluating the approach.\nI am also not convinced by the speedup results: the train/validation/test splits aren't clear, I'm not convinced by the choice of baselines, there's no information provided on the transfer learning methodology, and the presented results for neuro-vectorizer do not align with those in the neuro-vectorizer paper.\n\n* How is the dataset split into train/validation/test? Given that these benchmarks may share kernels, is there any deduplication performed? (Or is this not an issue)\n* What is the definition of accuracy for the experimental results? Assuming it means just matching the brute-forced solution, is comparing the KL true label accuracy / divergence the best metric? This is not a binary problem -- some incorrect choices are better than others.\n* How long (in terms of wall-clock time) does it take to train the model? To evaluate? To generate the brute-force solutions?\n* Does autograph ever result in kernels slower than O3?\n* The neuro-vectorizer paper (Figure 8) shows that neuro-vectorizer consistently achieves something close to 90% of brute force's speedup. Figure 6 in this paper shows a result closer to 50%. Why is there such a large discrepancy?\n\n## Novelty\n\nThe paper is sufficiently novel, in that it proposes a new architecture for loop vectorization.\nI do have one concern, which is that the paper itself overclaims the novelty:\n* \"autograph is the first to propose a structured learning framework for auto-vectorization\" (Section 2): I don't think that this is true. https://dl.acm.org/doi/10.5555/3454287.3455597 also uses a graph neural network to learn a vectorization policy (albeit, using SLP rather than loop vectorization).\n\n## Reproducibility\n\nAs discussed in the clarity section above, the paper is not sufficiently clear to be reproduced.\nBeyond that, I don't see significant barriers to reproducing the results of the paper.\n",
            "summary_of_the_review": "Reject.\nThe method is not sufficiently clear, such that a reasonable reader could not reimplement the approach.\nThe evaluation is not sufficiently high quality, such that the presented results are not convincing.\n\nI would be willing to raise my score conditioned on:\n* The authors providing a significantly more clear description of the approach.\n* The authors explaining the choice of accuracy metric in the evaluation (or present a different metric)\n* The authors clarifying the train/validation/test split methodology and the transfer learning methodology\n* The authors explaining the discrepancy between the neuro-vectorizer results in this paper and the results in the neuro-vectorizer paper\n\n\n-------\n\n## Update after author response\n\nThanks to the authors for the detailed response. Based on this response, I'm raising my score to a weak accept. Specifically:\n* The revision includes a much more clear description of the approach (page 4)\n* The authors correctly pointed out that I missed the definition of accuracy in the original submission. Though I still think this is a weak metric, I agree with the authors that the speedup metric is ultimately the one that matters (which the paper does report).\n* The author response does not state whether or not any kernels are shared between the training/test dataset (and further, do not mention any validation set). The author response is also not sufficiently clear on the response about transfer learning: my point of confusion was whether or not any fine-tuning is performed on the other task (as is typically implied by the term transfer learning). The author response implies that no fine-tuning is performed, but is not 100% clear on that.\n* The explanation of the neuro-vectorizer performance is satisfactory. Though it's surprising that neuro-vectorizer does not perform anywhere near as well as in the original paper (and raises some questions about whether it is a fitting baseline as-is), I don't view this as a reason to reject this paper.\n\nIf the paper is accepted, I would still like for the authors to include the following in the final version of the paper:\n* Stating to what extent the different applications in the train and test set share kernels (or if they do not share any kernels)\n* Stating how hyperparameters were tuned (given the lack of a validation set, I'm assuming that everything was tuned to maximize training set accuracy, but this should be explicitly stated)\n* Making clear whether any fine-tuning or other transfer learning methodology was performed, or whether the transfer learning test is just a test of out-of-distribution generalization\n* Including this discussion of neuro-vectorizer results\n* Further improvements to the clarity of the approach",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3406/Reviewer_Coc4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3406/Reviewer_Coc4"
        ]
    },
    {
        "id": "9k0xUXHOopA",
        "original": null,
        "number": 2,
        "cdate": 1666647981622,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647981622,
        "tmdate": 1666647981622,
        "tddate": null,
        "forum": "k7qRYoxUlB",
        "replyto": "k7qRYoxUlB",
        "invitation": "ICLR.cc/2023/Conference/Paper3406/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper introduces a DRL based approach to perform Loop Vectorization in LLVM IR. They represent LLVM IR as a graph and then learn an embedding in an unsupervised manner using a GNN, which they later refine using a FCNN to decide the VF and IF in the loop vectorizer.",
            "strength_and_weaknesses": "Strengths:\n* The paper improves on structural representation of the code from the NeuroVectorizer paper, even though the basic premise is the same.\n* Encouraging results from the LORE suite.\n\nWeaknesses\n*  Important related work that uses the structure for compiler auto-vectorization is missing. This leads to wrong novelty claims made at the end of section 2. This is a major flow.\n* The paper is low in novelty, where essentially they added learning based on structure to the NeuroVectorizer. Adding structure to learning in the context of vectorization is also not new as mentioned above.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: Very low, important related work missing with no comparison. Consider [1], they use the graph structure of the program in their auto-vectorization scheme. The authors have not cited or compared against their approach, which leads to the wrong novelty claim, \"In contrast to neuro-vectorizer, autograph is the first to propose a structured learning framework for auto-vectorization\". I find this a major flaw in the paper and diminishes the value of the contributions.\n\nClarity: The unsupervised training part of the paper was confusing to me, specially its training objective. Rest of the paper was ok to read.\n\n[1] Mendis et. al \"Compiler Auto-Vectorization with Imitation Learning\", NeurIPS 2019.",
            "summary_of_the_review": "IMO, the paper lacks novelty. Important related work are missing and this leads to wrong claims. I have mentioned them under novelty. Further, important work related to cost models (e.g. [1],[2] ) and how representation learning on graphs are used to learning embeddings that can be used for this task is missing. \n\nI find the ablations on pretraining (unsupervised learning) to be informative. However, it is unclear how the random walks are done to get negative examples. This is not the usual code language model pretraining that is used here. Better writing of this section would help. Further, it is unclear why this unsupervised learning objective was chosen, given that code language models usually opt for MLM based pertaining at least in the contextual sense.\n\nAlso, I found the \"upto\" speedups a little misleading considering the geomean speedups that are reported.\n\nOverall, I think this paper is below bar for acceptance at ICLR. I am more concerned with novelty related claims.\n\n[1] Baghdadi et al. \"A Deep Learning Based Cost Model for Automatic Code Optimization\" MLSys 2021\n\n[2] Mendis et. al \"Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks\", ICML 2019",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3406/Reviewer_pVcf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3406/Reviewer_pVcf"
        ]
    },
    {
        "id": "eLWHOv2UVFH",
        "original": null,
        "number": 3,
        "cdate": 1666660210271,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660210271,
        "tmdate": 1666660210271,
        "tddate": null,
        "forum": "k7qRYoxUlB",
        "replyto": "k7qRYoxUlB",
        "invitation": "ICLR.cc/2023/Conference/Paper3406/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a learning-based auto-vectorization method. The idea is to represent the code as a graph, learning embeddings for nodes in the graph, and use the embeddings to predict two vectorization parameters: VF and IF. ",
            "strength_and_weaknesses": "## Strength\nThe paper is well writing. The presentation of the technique is clear. \n\n## Weakness\n1. While the presentation of the technique is clear, it is unclear why it outperforms previous method. Can you give an example that shows your method obtains better vectorization than previous methods? \n\n2. I do not find any source code. To make your work more convincing, please consider open source your code. Auto-vectorization is a well studied topic in compiler community and many techniques are implemented in real compilers. If you want to claim an advantage over the existing work, you need to show the code. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear. The idea is new. No code is provided. ",
            "summary_of_the_review": "The paper is easy to follow. The techniques look new to me. However, for such learning-based code optimization work, a real implementation is important. For this reason, I give the paper a weak reject. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3406/Reviewer_NSgq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3406/Reviewer_NSgq"
        ]
    },
    {
        "id": "DqfoLrZkr9",
        "original": null,
        "number": 4,
        "cdate": 1667493912109,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667493912109,
        "tmdate": 1667493912109,
        "tddate": null,
        "forum": "k7qRYoxUlB",
        "replyto": "k7qRYoxUlB",
        "invitation": "ICLR.cc/2023/Conference/Paper3406/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper mainly focuses on the problem of loop-based vectorization and proposes a graph-based learning framework named autograph. Autograph aims to automatically predict the correct vectorization factor and the interleaving factor. Experimental results based on several data sets are reported.",
            "strength_and_weaknesses": "There are some weaknesses of this paper:\n\n-\tThe rationale of employing deep reinforcement learning in the proposed method remains unclear. More discussion and empirical evidence (e.g., the ablation study) should be provided about how the deep RL helps the loop-based vectorization problem.\n-\tImportant baselines are missing in the experiment. The state-of-the-art loop-based vectorization method (e.g., Ref Wang and O\u2019Boyle, 2018) is missing in the comparison, which brings more concerns about the performance improvement of the proposed method.\n-\tIn addition, the loop-based vectorization problem is likely to have an impact within only a subfield of learning representation, which reduces the significance of this paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper needs further improvement. Important baselines are missing in the experiment. The rationale of employing deep reinforcement learning in the proposed method remains unclear.",
            "summary_of_the_review": "The rationale of employing deep reinforcement learning in the proposed method remains unclear. Important baselines are missing in the experiment. In addition, the loop-based vectorization problem is likely to have an impact within only a subfield of learning representation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3406/Reviewer_9X1Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3406/Reviewer_9X1Y"
        ]
    }
]