[
    {
        "id": "sfBln30gyY",
        "original": null,
        "number": 1,
        "cdate": 1665740113381,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665740113381,
        "tmdate": 1665740141403,
        "tddate": null,
        "forum": "gEvzRWqFoCO",
        "replyto": "gEvzRWqFoCO",
        "invitation": "ICLR.cc/2023/Conference/Paper2125/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To tackle the problem of OOD detection (especially in text classification task), the authors propose a method called Contrastive Novelty Learning (CNL). The core idea is to generate OOD examples by prompting a large language model twice, first to generate label, then to generate the content. Experiments show effectiveness of the proposed method.",
            "strength_and_weaknesses": "**Strength**\n* The idea to use large pretrained generative model to generate OOD example is inspiring.\n* The experimental results are strong.\n\n\n**Weaknesses**\n* While the results seems promising, I'm curious whether the choice of pretrained model (e.g. replace GPT-3 with GPT-2) impact the performance much. If the proposed method only works well on very large pretrained generative models, the computational cost could be extremely high, since the authors generate 100k OOD samples.\n\n* The proposed method seems hard to be extended to other tasks like sequence labeling and other domains like computer vision.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, and the idea to use large pretrained generative model to generate OOD example is novel.",
            "summary_of_the_review": "See Strengths/Weaknesses",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2125/Reviewer_r6H5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2125/Reviewer_r6H5"
        ]
    },
    {
        "id": "VUdJNSJaNFH",
        "original": null,
        "number": 2,
        "cdate": 1666374127831,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666374127831,
        "tmdate": 1666374127831,
        "tddate": null,
        "forum": "gEvzRWqFoCO",
        "replyto": "gEvzRWqFoCO",
        "invitation": "ICLR.cc/2023/Conference/Paper2125/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study out-of-distribution detection for text\nclassification.  They propose \"contrastive novelty learning,\" which\ncombines two methods: 1) data augmentation with GPT-3 for\nout-of-distribution classes; and 2) a loss function which encourages\nthe maximum probability scores to be lower on out-of-distribution\nlabels. The authors report a few accuracy points of gain compared to\nprior methods, and conclude by exploring a number of questions, e.g.,\nabout limiting GPT-3 budget, and training with larger models.",
            "strength_and_weaknesses": "The data augmentation method is straightforward/creative, and I am\nglad to see that it works: it's a direct approach to ood detection\n(i.e., generating ood data) -- I am a fan of things that work :-) The\npaper is easy to follow, and the baselines which the authors compared\nto are thorough. I appreciated the oracle experiments which frame how\nmuch potential room for improvement there may be in follow up\nwork. The evaluation metrics made sense. The authors report averages\nover a number of runs, yielding confidence intervals.\n\nThe biggest negative of the paper, in my view, is that the performance\ngains (particularly of the new loss function) are marginal. The new\nloss function outperforms OE mostly because OE seems to fail on\nTREC-10 (else, it's competitive). The reason for OE failing is\n\"attributed to noisiness in generation\" of the OOD data --- this\ncauses some concern, because while the new loss function seems to\novercome this noise in the other dataset cases, I would be a bit\nnervous using it with any loss function given the massive performance\ndrop on TREC-10: is it worth the risk of involving GPT-3 for a few AUC\npoints? The new data augmentation, while resulting in performance\nimprovements over kFolden and Contrastive, also has more dependencies,\ni.e., access to GPT3. In more detail, a few technical concerns:\n\n- All of the out-of-label-distribution examples are synthetic,\n  whereas, all of the in-label-distribution examples are (presumably)\n  human written from the original corpus. Are models implicitly just\n  doing a \"human-authored\" vs. \"machine-authored\" classification task?\n  This question is partly addressed --- given that the test sets are\n  human-written, the method isn't only doing machine vs. human. But,\n  I'd suggest to add an additional experiment that goes the other way:\n  that is, an experiment where, at test time, the\n  out-of-label-distribution examples are human-written (from the\n  corpora), and the in-label-distribution examples are autogenerated.\n\n- The authors propose a new loss function in eq. 1 that goes beyond\n  the uniform distribution loss of Hendrycks et al. 2019. While I can\n  somewhat qualitatively see why this loss might be different than the\n  uniform loss, the authors argue that it works better because it\n  enables \"some novel examples can be more novel than others.\" Can\n  this claim be validated by examining the logits of the OE model\n  vs. this one? The uniform loss presumably (in a soft way) also\n  allows some examples to be more novel than others. The empirical\n  impact of this loss function is also not entirely clear. OE seems to\n  do badly on TREC-10 for some reason, but it generally seems to match\n  (or sometimes outperform) the new proposed loss on the other corpora.\n\n- I appreciated the oracle experiments --- but, somewhat\n  inexplicably, OE seems to do much better with the gold data\n  available vs. CCL. The reason for this is not explored deeply ---\n  why is that?\n\n- All experiments are conducted with BERT-Base, which is a 4 year old\n  model at this point. While the authors take some steps to run with\n  improved models like Roberta-Large, it would have been nice to see\n  additional scaling runs, e.g., with larger models.\n\n- The authors method can only work with text classification tasks,\n  which is great, but --- some of the competing methods are more\n  general, and the Wikitext + CCL method (which simulates the more\n  general setting of not having GPT-3 but only an OOD set) only\n  marginally outperforms prior solutions (kFolden, for example,\n  doesn't even depend on the external set).",
            "clarity,_quality,_novelty_and_reproducibility": "see above for detail.\n\n\nTypos:\n\n- Eq 1 has a missing parenthesis.\n- Table 1's caption: \"Novlty\"",
            "summary_of_the_review": "Overall: the authors propose to do OOD-label detection by explicitly\ngenerating OOD data with GPT3: a new loss function seems to overcome\nnoise in this process which causes a prior loss function to fail. The\npaper is well-written, creative, and the results promising. But, I\nhave some doubts that the performance gains of the new method (a few\nAUC points) will entice folks to undertake the suggested protocol in\npractice, esp. given the GPT-3 dependence and also, the fact that\nchoosing a slightly different loss function can cause catastrophic\nfailure (e.g., on TREC-10).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2125/Reviewer_KAeh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2125/Reviewer_KAeh"
        ]
    },
    {
        "id": "ePTNVA5gAGK",
        "original": null,
        "number": 3,
        "cdate": 1666593745377,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593745377,
        "tmdate": 1666593745377,
        "tddate": null,
        "forum": "gEvzRWqFoCO",
        "replyto": "gEvzRWqFoCO",
        "invitation": "ICLR.cc/2023/Conference/Paper2125/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper deals with OOD examples in classification. It leverages a huge PLM and generates a few examples that distribute differently from the ID examples. Subsequently, it learns a classifier with a contrastive confidence loss. Experimental results on a few public datasets indicate the effectiveness of the proposed method on classification and OOD detection. ",
            "strength_and_weaknesses": "Strength:\nGenerating OOD examples with huge PLMs is an interesting idea. \n\nWeaknesses:\nSome important details are missing.\nImprovements over the baselines are not fully convincing. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nsome important details are missing.\n\nNovelty:\nIt brings some new ideas to OOD detection and classification.\n\nReproducibility:\nOthers can reproduce the results. ",
            "summary_of_the_review": "The paper attempts to train a classifier that can accurately detect OOD examples. The interesting part is novelty prompting in which examples that are semantically novel and bear some resemblance to ID examples are generated with prompts using huge language models. However, I have some concerns regarding to the presentation and the empirical results:\n(1) CCL+few shot data augmentation: the description is \"prompt with only an instruction and zero or more ID training examples\", then how many ID training examples are involved in CCL+few shot? What is the difference between the proposed method? \n(2) what is the classification model? \n(3) Looking at Table 1, I am curious why OE+novelty prompting performs such badly on TREC-10. If it is sensitive to noise, then why it renders descent results on the other three datasets? This is critical, as the baseline outperforms the proposed method on two of the four datasets but loses finally on the \"average score\". \n(4) the improvement over CCL+few-shot is marginal. That's why I am curious about the difference between the proposed method and CCL+few-shot. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2125/Reviewer_i6wa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2125/Reviewer_i6wa"
        ]
    },
    {
        "id": "Yw1uDTzJu7e",
        "original": null,
        "number": 4,
        "cdate": 1666803537951,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666803537951,
        "tmdate": 1666803537951,
        "tddate": null,
        "forum": "gEvzRWqFoCO",
        "replyto": "gEvzRWqFoCO",
        "invitation": "ICLR.cc/2023/Conference/Paper2125/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a two-step generation-based method for open set selective classification. The generation process starts with first generating a set of OOD labels then examples conditioned on these novel labels. The paper also proposed a new training objective that pushes the max probability of ID predictions above those of OOD predictions, which is less restrictive than existing proposals by avoiding global uniformity. The proposals are evaluated on a few classification tasks on NLP and empirically shown to be effective, with ablation on some selected aspects of the setup.",
            "strength_and_weaknesses": "Strength\n\n- The idea is well motivated, both wrt the novelty prompt generation and the contrastive confidence loss.\n- Empirical results are overall positive, with reasonable baselines and comparisons and some ablation studies.\n- The writing is clear and easy to follow.\n\nWeaknesses\n\n- There are some details that are not very clear\n  - S4.3 For contrastive baseline, I'm not sure how to interpret \"pairwise distant but internally compact\" in the context here. Are there novel classes and examples involved? Might be good to clarify this question for the baselines here?\n  - S5.1 it was a bit difficult to follow the various comparisons made in ref to Table 1. E.g., I had a hard time figuring out what is being compared at \"This accuracy decrease is smaller\"\n  - S5.2 \"To isolate the factors...\" what are the factors being isolated here and how is the isolation achieved by Table 2?\n  - S5.3 Generator size is concerned only with the example generator, while the novel-label generator is always GPT3? What happens if one generates novel labels with smaller models?\n- There's not much error analysis other than offhand comments with limited support.\n  - For example, when discussing vanilla outperforming OE+NP (\"We attribute this behavior to the noisiness of generation: some novelty prompted examples are similar to ID examples\"), although I intuitively agree with this sentiment, it would be great to have some empirical support by quantifying example similarity.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall clarity and quality of the work is good. Use of GPT3 might limit reproducibility but it won't be a permanent problem.",
            "summary_of_the_review": "The paper proposed a reasonable solution to an important problem with good empirical results and decent result analysis. Publication would benefit the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2125/Reviewer_b3bf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2125/Reviewer_b3bf"
        ]
    }
]