[
    {
        "id": "6V-MMIM0QGI",
        "original": null,
        "number": 1,
        "cdate": 1666576661686,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576661686,
        "tmdate": 1666576661686,
        "tddate": null,
        "forum": "b1F-_7dUo0w",
        "replyto": "b1F-_7dUo0w",
        "invitation": "ICLR.cc/2023/Conference/Paper5491/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work attempts to address the curse of high dimensionality problem for the tabular data. The authors build their model, PLATO, on the well formed insight that auxiliary KG about the feature connections can improve performance as the sparsity induced in the model helps with the generalizations in the cases where d>>n. Their method predicts the first layer weights of the MLP and then the remaining layers of the MLP are trained using the loss function. They evaluate their method on different datasets and report improved performance.",
            "strength_and_weaknesses": "\nPros:\n1. Figure 1 explains their methodology in a good and succinct manner. \n\nComments:\n1. It will be helpful to cover some related graph recovery methods to get auxiliary graphs for more adoption. \n2. (I) In sec 3.2, the rationale given to share the parameters of the first layer does not seem principled to me.  \u201cTypically this intuition is missed since T learns the parameters \u0398 [1] j and \u0398 [1] k associated with two features j and k independently by gradient backpropagation.\u201d The features do have common parameters in the subsequent layers through which their commonalities can be modelled. \n3. This methodology seems to learn a projection of the input data to another space, say R^h (here referred to as layer 1 of the MLP). The method comes up with `d\u2019 vectors in R^h space that are put together as a matrix to obtain the $\\Theta[1]$. This is done by running message-passing on the KG which takes individual feature inputs as one-hot vectors to obtain a vector in R^h space.\n4. How does it perform for out-of-distribution samples? \n5. Please share details about any ablation study done, which empirically shows that having the first layer obtained from this method works better than just training a simple MLP. What happens when there are more than 3-4 layers in MLP and what if there are no learnable MLP layer and just this direct projection layer? \n\nI have some major concerns about this methodology (pts 2&3 in comments). The rationale given by the authors seems to be hand-wavy and not grounded. I hereby request the authors to develop a mathematically rigorous approach to support their claims. \n",
            "clarity,_quality,_novelty_and_reproducibility": "<mentioned in comments above.>",
            "summary_of_the_review": "I have some major concerns about this methodology (pts 2&3 in comments). The rationale given by the authors seems to be hand-wavy and not grounded. I hereby request the authors to develop a mathematically rigorous approach to support their claims. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5491/Reviewer_Ddzi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5491/Reviewer_Ddzi"
        ]
    },
    {
        "id": "HNKFlvaq1N",
        "original": null,
        "number": 2,
        "cdate": 1666651853223,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651853223,
        "tmdate": 1666651853223,
        "tddate": null,
        "forum": "b1F-_7dUo0w",
        "replyto": "b1F-_7dUo0w",
        "invitation": "ICLR.cc/2023/Conference/Paper5491/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This submission contributes a learning method for high-dimensional data that uses an associated knowledge graph between the features to create a lower-dimension representation. The methods pretrains a data-transformation mechanism on the knowledge graph, creates attention weights, and used message passing on the corresponding graph neural network to output an embedding of input data, which consistutes the intermediate layer of an MLP. The method is benchmarked on 6 biomedical datasets, with a dozen thousands features and hundreds samples, where it outperforms classic methods.\n",
            "strength_and_weaknesses": "The idea of using a knowledge graph to build a graph neural net on the features is interesting.\n\nHowever, this paper seems more targetted as computational biology applications, and not tabular data in general. Indeed, outside computational biology, it seems quite unlikely to find the configuration of many columns all corresponding to entries of a knowledge base. Can this paper be applied outside computational biology? If so, it would be interesting to demonstrate it.\n\n\nDid the author compare to classic graph-regularized models? There is a literature that is more than 10 years old on the topic, including in computational biologie.\n\nI do not understand where things such as the line 3 of algorithm 1 come from. I just do not understand how was the algorithm designed. By trial and error?\n\nHow are the error bars computed on the tables? Given the small number of n, I would expect much larger error bars.\n\nPCA is not a supervised method. How can it be used in the experimental tables?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is reasonably clear but not stellar. In particular it does not explain how the specificities of the method where obtained.\n\nThe novelty is limited given the huge amount of papers on graph regularization.\n\nGiven that no code is available, the reproducibility is limited.\n",
            "summary_of_the_review": "There is an interesting idea, but this seems more like a computational biology paper than a general tabular learning but. The paper does not contribute understandible and insight on top of the graph regularization work.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5491/Reviewer_yX2X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5491/Reviewer_yX2X"
        ]
    },
    {
        "id": "96EG3GFvLq1",
        "original": null,
        "number": 3,
        "cdate": 1666675770968,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675770968,
        "tmdate": 1666675770968,
        "tddate": null,
        "forum": "b1F-_7dUo0w",
        "replyto": "b1F-_7dUo0w",
        "invitation": "ICLR.cc/2023/Conference/Paper5491/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To deal with the problem when tabular data has high dimensionality, the paper introduces a knowledge graph to help determine the weights of an MLP. The proposed PLATO method achieves good results on various datasets.\n",
            "strength_and_weaknesses": "To deal with the case that the dimensionality of the data is very larger than the sample size, the authors introduce an additional knowledge graph as a prior of the feature relationship. Then the weights of the MLP are better determined.\n\nThere are some suggestions for the paper:\n1. Why the weights of the first layer of the MLP is more important than the others? \n2. Do the results depend on the quality of the knowledge graph? How to compare with other methods in a fair manner since an auxiliary knowledge graph is introduced.\n3. Pre-train the node embedding in Eq. 2 is not with a self-supervised node embedding method is not clear enough. How will the pretraining method influences the final results?\n4. The layer-wise predicted embeddings are concatenated in line 9 in the algorithm. It works like an ensemble. How much does the concatenation step improve in the whole method?\n5. Please consider comparing more tabular data methods, such as FT-Transformer based on some dimensionality reduction methods.\n6. It seems the column names are required. Does it limit the field of the method?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear, but the method is a bit complicated. The authors may add the size of matrices in the algorithms.\n",
            "summary_of_the_review": "The main idea of the paper is reasonable. An auxiliary knowledge graph will introduce more information especially since there are many features. The method is clear and the results are good. Please check the weakness part for some questions.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5491/Reviewer_rHdL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5491/Reviewer_rHdL"
        ]
    },
    {
        "id": "RHRxm1A1Dhi",
        "original": null,
        "number": 4,
        "cdate": 1667095706362,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667095706362,
        "tmdate": 1667095706362,
        "tddate": null,
        "forum": "b1F-_7dUo0w",
        "replyto": "b1F-_7dUo0w",
        "invitation": "ICLR.cc/2023/Conference/Paper5491/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors define PLATO, a model for underserved scenario of short and fat datasets (d>>n). PLATO works when we have an auxiliary knowledge graph describing the d features and the relations between them. Tabular datasets with d>>n are common in physical sciences and biology where data is collected through expensive experiments. \n\nPlato is an MLP where the first layer is trained by leveraging the knowledge graph as follows:\n1. A c-dimensional embedding M_j is learned for each feature j\n2. A message passing algorithm that uses the knowledge graph is used to learn another embedding Q_j for each feature j based on its neighbors. In each round the embedding of a feature is a weighted combination of its embedding in the previous round and all its neighbors in the previous round.\n3. A neural network outputs P the weights of the first layer.\n\nExperimental results are presented on multiple datasets and compared with multiplied methods (both statistical and deep-learning based). Some experimental results are presented to explain the motivation around using a message passing protocol and large knowledge graphs describing features even if not all are in dataset.",
            "strength_and_weaknesses": "Strengths\n1. Solves for an important set of applications that are underserved in deep learning focused research geared towards large datasets.\n2. Elegant way of incorporating prior information via a knowledge graph (KG). While not all applications have associated knowledge graphs, a sizeable set do. KG is a distillation of several studies, and to be able to incorporate effectively is significant. \n\n3. Experimental results are rigorous. On multiple datasets and compared with multiplied methods (both statistical and deep-learning based) Established protocols for tabular dataset are followed.\n4. The weights of the first layer of MLP are learned in 3 steps described above. An ablation study shows the importances of each step showing the added value of a message passing routine and also of adding knowledge about features not present in the datasets.\n5. Clearly describes architecture, datasets, and results.\n\nWeaknesses:\nAblation study is presented only on a single BRCA dataset. It would be valuable for readers to know that findings hold across datasets.\n\nThe intro and related work sections both spend significant ink on related work, including describing works like graph-based prediction that are only tangentially related. Instead, the intro can incorporate a summary of the contributions in the paper. Currently, the reader has to read all through methods and results to understand the significance of the contributions. Examples of items to include earlier are how message passing the graph structure of KG is used effectively to incorporate additional info. This is a significant result not sufficiently highlighted.\n\nThe current paper is not incomplete without these, but some suggestions for future work:\n\nStudy impact of missingness in KG. Is it possible to support datasets where some subset of features are not described in KG. The paper currently assumes every feature in dataset is a node in the KG. \n\nA study on model's sample complexity e.g., we broadly cast this problem as d>>n. e.g., can we afford to drop some rows from the dataset and the models achieve the same performance.. What is the point of diminishing returns. It would be interesting to see how few examples the model can get away with.",
            "clarity,_quality,_novelty_and_reproducibility": "Clearly written paper. Experimental results are rigorous and following best practices including hyper parameter optimization and results reported over a large number of runs. As far as I am aware, this is a novel approach to solving for an underserved set of applications common in biology and physical sciences where d>>n",
            "summary_of_the_review": "Overall, the authors present a novel approach to supervised learning tasks on datasets where d>>n. Given the novelty of the approach, significance of the setting for biological and physical sciences and elegance of the solution, I vote accept. \n\nThe major weaknesses of the paper are largely open questions that can be addressed in a future iteration of the paper. The paper remains of interest to the community  at this iteration.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5491/Reviewer_AKNa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5491/Reviewer_AKNa"
        ]
    }
]