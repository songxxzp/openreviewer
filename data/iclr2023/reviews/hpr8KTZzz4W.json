[
    {
        "id": "EmlYZCl1I_",
        "original": null,
        "number": 1,
        "cdate": 1666482677995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666482677995,
        "tmdate": 1669755729915,
        "tddate": null,
        "forum": "hpr8KTZzz4W",
        "replyto": "hpr8KTZzz4W",
        "invitation": "ICLR.cc/2023/Conference/Paper3912/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider approximate inference in Bayesian networks, specifically in the subset of Bayesian networks that can be represented as plate diagrams, which are common for large population studies, the population is composed of subgroups and each group is composed of many individuals. Given global the population parameters, each of the group parameters are i.i.d. and given group parameters, the individuals are i.i.d., a common hierarchical structure.\n\nAssuming a fully specified Bayesian generative model of the data, and only leaf nodes are observed, the authors propose a variational inference method to learn the full approximate posterior distribution to all of the unobserved nodes. The method combines normalising flows with the generative model priors to approximate the posterior. The method is applied to illustrative toy datasets and shows results to be expected and a high dimensional neurological time series dataset (but results are not presented in the main paper).\n",
            "strength_and_weaknesses": "Overall, I struggled with the paper and I have given a low confidence score due to possible misunderstandings.\n\n**Strengths**\n- the Supplementary material seems very comprehensive and answered many of the questions I had after reading the main paper.\n- the method seems well designed and thought out\n- the encoding size experiment provided good confirmation that the degrees of freedom must match the sufficient statistics.\n\n**Weaknesses**\n\nI feel this paper is very hard to recommend for acceptance due to the writing and the somewhat basic numerical evaluation.\n\nI felt the writing and the structure of the paper was very poor, it took me a great deal of effort and multiple rounds of re-reading and re-evaluating my own understanding of the paper before finally appreciating the paper and its contributions (and I have experience in graphical models).\n  - I would make section 2.1 into it's own section that really makes the mathematical problem clear. E.g. a plate diagram, the generative model in  equation (1), a real world example with cardinalities and (non-linear) link functions and latent variables. Describe the practicalities of inferring parameters in such a model and why it is impractical scaling with cardinality.\n- I don't know if this is standard or not but I was personally thrown off by the terminology \"RV template $\\theta_i$\" and \"ground RVs $\\theta_{i,n}$\" and \"grounding a template with cardinalities\". Is is possible to reduce notation? E.g. $\\mathcal{T}$ is introduced but never used, plates $\\mathcal{P}$ is simply indicated by $i$, Card($\\mathcal{P}$) is just $N_i^{full}$. Perhaps $N^{full}$ and $N^{redu}$ could be replaced by $N$ and $\\tilde{N}$, i.e. use tilde \"~\" to denote reduced version of various quantities?\n- what does $\\pi(\\theta_{i,n})$ explicitly represent in equations (1) and (2)? I believe they should be precise, known, values if they are conditioning a distribution, but this is seems vaguely defined in the paper. In Ambrogioni 2021b they say \"$\\theta_j$ is the array of values of the parents of the parents of the $j^{th}$ variable, and $\\pi_j(theta_i)$ and $\\pi_j()$ is the link function\".\n- what is the prior for an RV, $p(\\theta_{i,n}| \\pi(\\theta_{i,n}))$? My intuition is that prior is the should be the generative model up to $\\theta_{i, n}$ with all of the parent variables marginalized out? Or is it the generative model term with conditioned on given parent values? In the latter case, having such a distribution as the input to a normalizing flow doesn't feel like it would lead to a good posterior approximation, parent values change, prior changes, normalizing flow function is constant but the input changes?\n- Section 2.3: please give dimensions of all **X**, and $\\theta$ and I assume one row/col of  **X** is a subgroup indicator variable?\n- (nitpick) I am not entirely sure I agree with the use of the word \"amortization\" in the PAVI-F, the fact that $\\phi_i$ is shared across all nodes in a layer does not makle inference faster at test time for new nodes? I still have to learn $E_{new}$ from scratch? (aAlthough amortization means \"spreading out\", in this community, I feel it refers to predicting an approximate posterior in one shot given observed variables.\n- the experiments are toy examples and the only non toy example is rather brief with no quantitative results in the main paper\n- Lack of Technical Novelty\n  - randomly subsampling nodes in the DAG seems like a very simple straightforward idea and I think the unbiasedness is significant and shows that this isn't \"just a hack\" but a real valid method, however it is rather incremental.\n  - claiming to have reduced memory footprint by \"using a low dimensional encoding\" seems ambitious claim of novelty. While it may be novel, forcing the method to use a low dimensional parameterization of a normalizing flow seems incremental.\n\n- I was surprised that the discussion about \"faithful posteriors\" (Webb et. el. 2019) was in the SM, to me this felt like very closely related work and provides a method for building approximate posterior Bayesian networks,",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n- as above, I felt the arguments in the main paper can be made more concisely, I felt this was unnecessarily difficult for me to understand. The parts of the SM that I did read I felt were clear and thorough, and I wish the main paper were made more concise and explicit and many of the SM details may be moved to the main paper. (pseudocode, results and baselines for neuroimaging application)\n\n**Novelty**\n- I feel the main contributions seem to be the splitting of normalizing flow parameters into $\\phi_i$ shared across multuiple RVs and  RV specific parameters $E_{i,n}$, as well as the unbiased mini batching, which I feel are novel, though their significance may not be sufficient for acceptance.\n\n**Reproducibility**\n- the SM is extensive and contains many details and pseudocode, I feel the authors have been very thorough and, apart from a few misunderstanding I have, the method and toy experiments appear reproducible.\n",
            "summary_of_the_review": "- clarity of the main paper is very poor, verbose and possibly non-standard terminology for what appears to be standard Bayesian networks.\n- the technical contribution, parameterization of normalizing flows and mini batch training, seem novel but incremental.\n- the experiments are mostly toy, one non toy is only briefly considered.\n\nUPDATE: I have updated my score from 3 to 5 after reading through the responses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3912/Reviewer_d76j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3912/Reviewer_d76j"
        ]
    },
    {
        "id": "-H3o6H47eU",
        "original": null,
        "number": 2,
        "cdate": 1666654811338,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654811338,
        "tmdate": 1666654811338,
        "tddate": null,
        "forum": "hpr8KTZzz4W",
        "replyto": "hpr8KTZzz4W",
        "invitation": "ICLR.cc/2023/Conference/Paper3912/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work proposes an amortized VI scheme that leverages the hierarchical structure of a Bayesian model, using the latter's plate representation. Plates are used to instantiate subsets of the variables of interest, and share weights in the encoding of sets of variables used on a conditional flow architecture. The result is a plate-amortized proposal distributionI. This idea seeks to enable VI in a large scale setup. The methodology is validated empirically against the state-of-the-art, with one application with real data in neuroimaging context.\n",
            "strength_and_weaknesses": "## Strengths:\n* The methodology is generic and intuitive\n* Potential for large scale applications\n* The idea of embedding structure into the approximate inference scheme is potentially fruitful line of further research\n* The authors provide detailed discussion in the appendix, including some limitations regarding the approximation gap.\n* The empirical validation in a realistic large scale scenario.\n\n## Weakness:\n* Heavy notation makes the text hard to follow;\n* Limited novelty (e.g., the work is incremental when compared with ADAVI). It seems that the major difference between PAVI and ADAVI is the stochastic updates of variables;\n* I believe the gradient estimates for PAVI are biased;\n* Limited experimental campaing, limited to a small set of methods and models. Validation against multiple methods in smaller dataset could be beneficial.\n\n## Questions\n1. In the demonstration on Appendix A.2 it indicates that PAVI-F is unbiased by showing in A.12 that the ELBO term is equal the term with all the variables, however Section A.4 implies that there is a gap between vanilla VI, PAVI-F and PAVI-E: Gvanilla VI \u2264 GPAVI-F \u2264 GPAVI-E . This seems unintuitive and indicative that indeed PAVI-F might be biased, given that the term that differ in those approximation schemes is indeed the ELBO term. Are both of those results consistent?\n\n2. How does the proposed methodology would compared with MCMC/HMC? It could be beneficial given that a new approximation scheme is being proposed to compare it as well sampling methods. \n\n3. When it comes to the expressivity of the variational approximation. How does the predictive performance of the method compare with others variational inference proposals (empirically)? For example another family of expressive VI approximations are found in the works such as UIVI [1], SIVI [2] or VCD [3], which does not use flows and yet are generic and could be a better test bench and give the reader more confidence about the proposed method.\n\n4. In Eq. A.11, when two variables share the same descendants, hence leading to correlated E_{i,n} in the PAVI-E scheme, wouldn\u2019t in this case lead also to dependency between the indicator random variables and log q_{i,n} (given the dependency of log q on the encoding)?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Heavy notation makes the text hard to follow.\n\nQuality: I have doubts about the theoretical soundness, the experimental campaign is limited.\n\nNovelty: seems incremental compared to ADAVI.\n\nReproducibility: I have no concerns regarding reproducibility.",
            "summary_of_the_review": "Given 1) the limited novelty, 2) short experimental campaign, 3) and possible issues with theory, I recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3912/Reviewer_oFPk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3912/Reviewer_oFPk"
        ]
    },
    {
        "id": "E5JUFb_BNU",
        "original": null,
        "number": 3,
        "cdate": 1666768298127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666768298127,
        "tmdate": 1669732174821,
        "tddate": null,
        "forum": "hpr8KTZzz4W",
        "replyto": "hpr8KTZzz4W",
        "invitation": "ICLR.cc/2023/Conference/Paper3912/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Plate-Amortized Variational Inference (PAVI), an inference framework for hierarchical Bayesian models with amortization over the population (dataset) level implemented with normalizing flows with shared parameters. The main idea is to posit an expressive variational family parameterized with normalizing flows, and yet reduce the number of parameters to be trained by sharing part of the variational parameters across populations. Another important concept is stochastic training, where for each step, instead of training all the variables present in the graphical model, subsample a small number of variables and compute stochastic gradient with them. Thanks to amortization, PAVI could scale to large-scale population studies involving high-dimensional parameters.",
            "strength_and_weaknesses": "Strengths\n- The paper is well-written and easy to follow.\n- The description of amortization using the language of plate diagram is interesting.\n- The neuroimaging experiment seems interesting.\n\nWeaknesses\n- In my opinion, the notion of plate amortization is not entirely new, so I think the paper can be strengthened by discussing relevant existing works.\n- The experiments include a synthetic Gaussian model and neuroimaging experiments. The synthetic experiment looks promising, but it is of a small scale that does not particularly highlight the benefit of the proposed method. For the neuroimaging experiment, although it seems promising that only the proposed method was manageable to train, it is not clear whether the prediction given by the model is reasonable (at least for the non-experts like me).\n\nDetailed comments\n- The idea of plate amortization is indeed interesting. I guess the proposed framework virtually extends to hierarchical Bayesian models with an arbitrary level of hierarchies, but the main application would be the amortization across populations (datasets). Regarding amortization over datasets, there have been similar ideas (although motivated differently), especially in the context of meta-learning. Neural processes families, for instance, introduce an amortized inference network that can readily generalize to a new task (corresponding to the population) which is parameterized by the dataset encoding networks (deep sets or transformers); this would roughly correspond to PAVI-F, except that PAVI-F uses normalizing flows for the parameterization. Amortized Bayesian meta-learning (Ravi and Beatson, 2019) is also worth discussing in a similar sense. Depending on the application, neural processes can be considered as a baseline to be compared to the proposed method, since it can also scale to large population study due to its use of the amortized inference network.\n- The experiment section could be enhanced. I understand that the neuroimaging experiment can be compelling, but in the current form, I'm not entirely sure how well the proposed method is doing; the only result I could see from the paper is Figure 5, which I fail to interpret. It would be great to see the numbers, for instance, metrics could tell us how accurate PAVI is, or at least how well the predictions given by PAVI match the domain knowledge given by experts. Also, if the baselines are not trainable due to their limited scalability, one could consider comparing them on a smaller subsets of manageable sizes to see if PAVI would do better than baselines for real-world tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality: I think the paper is well-written in general, although there are some complex notations to digest which would be hard to fully grasp at a first glance.\nNovelty: as mentioned above, it is worth discussing the relevant works in meta-learning literature, and clarifying the novelty of the proposed method compared to them.\nReproducibility: I think the paper provides enough details to reproduce the experiments.",
            "summary_of_the_review": "Overall, the proposed method is an interesting contribution, but the paper could be strengthened by adding more discussions and hopefully more experimental results/interpretations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3912/Reviewer_pbEf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3912/Reviewer_pbEf"
        ]
    },
    {
        "id": "dVyBw_I03V",
        "original": null,
        "number": 4,
        "cdate": 1666934648081,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666934648081,
        "tmdate": 1666934648081,
        "tddate": null,
        "forum": "hpr8KTZzz4W",
        "replyto": "hpr8KTZzz4W",
        "invitation": "ICLR.cc/2023/Conference/Paper3912/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an approach for variational inference on large-scale hierarchical models which takes advantage of conditional independence in plate structures to factorise its variational approximations and better scale with problem dimensionality. The proposed framework is based on normalising flows taking shared parameters and data-encoding vectors. The resulting algorithm is equipped a stochastic optimisation scheme for training and is shown to converge faster than other algorithmic baselines in experiments.",
            "strength_and_weaknesses": "### Strengths\n* The proposed method achieves significant (orders of magnitude) performance speed-ups when compared to baselines in experiments.\n* Although the idea of using factorised variational posteriors for hierarchical models is not new, this paper implements it in a more computationally efficient way, which is also amenable for per high-performance computing with hardware accelerators.\n* Diagrams help understand the concepts in the approach.\n\n### Weaknesses\n\n* Related work: There are other flexible variational inference approaches for hierarchical models which also factorise the variational posterior. For example, Tran et al. (2017) introduced hierarchical implicit models for this task, and other relatively simple approaches for random effects models (as in the example experiment) are also available (e.q., Dao et al., 2022).\n\n* Experiments: There was only one benchmarking model (Eq. 6) and variations thereof for the comparisons. The other experiment (Fig. 5) did not provide comparisons against baselines due to its complexity. Another representative set of experiments would help assessing the improvements over baselines in corner cases where other methods can be applied but PAVI should in principle perform better.\n\n#### References\nTran, D., Ranganath, R., & Blei, D. M. (2017). Hierarchical Implicit Models and Likelihood-Free Variational Inference. 31st Conference on Neural Information Processing Systems (NIPS).\n\nDao, V. H., Gunawan, D., Tran, M., Kohn, R., Hawkins, G. E., & Brown, S. D. (2022). Efficient Selection Between Hierarchical Cognitive Models: Cross-Validation With Variational Bayes. Psychological Methods, (April).",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity:** The paper is quite dense to read. For instance, sections 3.1 and 3.2 seem to be repeating what is in Sec. 2.3 and 2.4 at times perhaps a bit too much. Something more interesting would be to expand on the unbiasedness of the approximation, which has been left for the appendix (A.2).\n\n* **Novelty:** I've found some of the methodology somewhat similar to that of ADAVI (Rouillard & Wassermann, 2022). The difference lies on the use of conditional independence structures between parameters to further factorise the posterior.\n\n* **Reproducibility:** Code is provided and the dataset is open access.\n",
            "summary_of_the_review": "The paper's contributions are somewhat novel, but it lacks in experimental comparisons against the baselines in other challenging scenarios.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3912/Reviewer_1MMm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3912/Reviewer_1MMm"
        ]
    }
]