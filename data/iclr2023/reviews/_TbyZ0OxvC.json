[
    {
        "id": "vGFW_Owd2S3",
        "original": null,
        "number": 1,
        "cdate": 1666591117454,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591117454,
        "tmdate": 1666591117454,
        "tddate": null,
        "forum": "_TbyZ0OxvC",
        "replyto": "_TbyZ0OxvC",
        "invitation": "ICLR.cc/2023/Conference/Paper5333/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Existing algorithms for the problem of PDA(Partial Domain Adaptation) are considered and evaluated. While most previous successes have relied on target labels during training(model selection strategy), the availability of target domain labels is unrealistic. \n\nWith a more realistic evaluation of the PDA methods, the authors claim that performance drops are significant without target labels. The finding urges the community to rethink the evaluation protocol for practical PDA methods. ",
            "strength_and_weaknesses": "As the study of PDA methods is relatively new to the community, the benchmark and evaluation protocol for the field still needs a careful study/investigation. The submitted paper has dealt with the problem of interest comprehensively and stated the PDA model selection checkpoints. This is an interesting and significant technical challenge that matters to the community. \n\nThe benchmark and relevant code are open-sourced for reproducibility. \n\nWhile the paper is well-written overall, some sentences are difficult to follow. Some examples are as follows: \np.2 protocal -> protocol, methods pairs -> method pairs, 100 random samples is -> 100 random samples are \np.3 that distinguish -> that distinguishes, a high confident model -> a highly confident model\np.4  transform ... in a vanilla -> transform ... into a vanilla, found than target -> found that target.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "While the tackled problem of interest, a realistic evaluation of PDA methods, is original, the novelty of the work overall is still in question. The assessment and the reproduction of the existing methods are relatively straightforward, and the most significant claim that the absence of target labels can hurt the performance of the finally chosen PDA model is not surprising. \n\nThe paper is challenging to follow and understand. More detailed analyses, explanations, and visualizations of the major claim and the pitfall of the existing algorithms would help the readers understand more clearly. \n\nThe code is open-sourced for reproducibility. \n",
            "summary_of_the_review": "While the paper is somewhat interesting, it does not suggest any interesting algorithm or an exciting finding. Most experiments are straightforward and there are not many insights the readers can learn from. \nThe paper is not clear and is difficult to comprehend. \nFor the aforementioned reasons(novelty, technical contribution, and clarity), I recommend rejecting it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5333/Reviewer_ZmSX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5333/Reviewer_ZmSX"
        ]
    },
    {
        "id": "9AXxe-Lw-R",
        "original": null,
        "number": 2,
        "cdate": 1666628722790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628722790,
        "tmdate": 1666628722790,
        "tddate": null,
        "forum": "_TbyZ0OxvC",
        "replyto": "_TbyZ0OxvC",
        "invitation": "ICLR.cc/2023/Conference/Paper5333/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a benchmark for partial domain adaptation (PDA) to evaluate the model selection strategies for different PDA approaches on two different real-world datasets. The experimental results show that 1) Target labels are critical for model selection strategies; 2) only one method and model selection pair perform well on both datasets; 3) The results of PDA methods are influenced by the rand seeds. The authors also give some recommendations for future works.",
            "strength_and_weaknesses": "Strength:\n1.\tThis paper studies an important domain adaptation problem, particularly for partial domain adaptation. Using the target label to select models violates the domain adaptation assumption. But there is no suitable and effective metric for model selection. This is why researchers still use target labels for model selection.\n2.\tThis paper evaluates 7 representative PDA algorithms on 2 different real-world datasets using 7 different model selection strategies. This benchmark will foster the research community.\n3.\tBased on the extensive experiments, the authors give some conclusions and suggestions for future works.\n\nWeakness:\n1.\tThis work raises an essential issue in partial domain adaptation and evaluates many PDA algorithms and model selection strategies. However, it does not present any solution to this problem.\n2.\tThe findings of the experiments are a bit trivial. No target label for model selection strategies will hurt the performance, and the random seed would influence the performance. They are commonsense in domain adaptation, even in deep learning.\n3.\tThe writing needs to improve. The Tables are referenced but always placed on different pages. For example, Table 2 is referred in page 4 but placed on page 3, making it hard to read. The paper also has many typos, e.g., \u2018that\u2019 instead of \u2018than\u2019 in section 3.\n4.\tMany abbreviations lack definition and cause confusion. \u2018AR\u2019 in Table 5 stands for domain adaptation tasks and algorithms.\n5.\tIn section 4.2, heuristic strategies for Hyper-parameter turning is not clearly described. And the author said, \u201cwe only consider the model at the end of training\u201d, but should we use the model selection strategies?\n6.\tIn section 5.2, part of Model Selection Strategies, the authors give a conclusion that seems to be wrong \u201conly the JUMBOT and SND pair performed reasonably well with respect to the JUMBOT and ORACLE pair on both datasets.\u201d In Table 6, the JUMBOT and SND pair performs worse than the JUMBOT and ORACLE pair by a large margin. For instance, on OFFICE-HOME, the JUMBOT and SND pair reaches 72.29 accuracies, while the JUMBOT and ORACLE pair achieves 77.15 accuracies.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and novelty are only marginally significant. For reproducibility, the authors provide the key resources to reproduce the results.",
            "summary_of_the_review": "The authors study an important issue in partial domain adaptation and conduct extensive experiments to benchmark different PDA methods and model selection strategies. Based on these experimental results, this paper shows some findings. However, this paper does not present solutions for this issue, and the findings are also obvious.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5333/Reviewer_unvz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5333/Reviewer_unvz"
        ]
    },
    {
        "id": "KfFelao4lzM",
        "original": null,
        "number": 3,
        "cdate": 1666835213081,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666835213081,
        "tmdate": 1666835213081,
        "tddate": null,
        "forum": "_TbyZ0OxvC",
        "replyto": "_TbyZ0OxvC",
        "invitation": "ICLR.cc/2023/Conference/Paper5333/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper raises an important question about reproducibility of results for partial domain adaptation. The authors show that the stopping criterion is key to the success of partial domain adaptation algorithms in the literature, and there can be massive variations for various stopping criterions commonly used in the literature. Some algorithms even produce wide variations in performance for different random seeds. ",
            "strength_and_weaknesses": "Strength\n- The paper have done considerable amount of experiments along various dimensions\n- The paper is well-written and easy to follow\n\nWeakness\n- While the paper does a lot of experiments, the paper does not establish a strong baseline. The main takeaway from the paper is that partial domain adaptation works in literature produce high variations in performance to stopping criterions and random seeds. \nThe expected takeaway from such papers should be rather a simple strong baseline which can be concluded from the experiments under a certain fair experimental protocol, which works well across stopping criterions and random seeds. This would then encourage researchers to follow that experimental protocol and report their results and compare against that baseline. \n\nThe paper is overall in a good direction and I highly encourage the authors to consider this to refine their work. ",
            "clarity,_quality,_novelty_and_reproducibility": "- In Table 1 it is not clear how the authors got the \"best\" and \"worst\" performances. Please clarify.\n\n- How are the seeds chosen? Randomly? \n\n- What effect can the optimizer and learning rate strategy have on the algorithms? I suppose they would also have some dependency on it.\n\n- ",
            "summary_of_the_review": "The paper raises an interesting question, presents various experiments to showcase the problems with the current methods. However, a simple strong baseline and a suggestion for a robust experimental protocol is missing, which should be the take-away from such works. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5333/Reviewer_PkyR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5333/Reviewer_PkyR"
        ]
    }
]