[
    {
        "id": "1bW-rnTcDs",
        "original": null,
        "number": 1,
        "cdate": 1666570221885,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666570221885,
        "tmdate": 1666570221885,
        "tddate": null,
        "forum": "Iuubb9W6Jtk",
        "replyto": "Iuubb9W6Jtk",
        "invitation": "ICLR.cc/2023/Conference/Paper1977/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a benchmarking framework for class-out-of-distribution detection with various levels of detection difficulty. This work benchmarks this technique's application to ImageNet with 525 publicly available pre-trained ImageNet-1K classifiers. Based on this benchmark, the authors identify several trends in out-of-distribution detection robustness and serve as a foothold for future research.",
            "strength_and_weaknesses": "Strengths:\n\nS1. This work provides a new benchmarking framework to detect class-out-of-distribution instances with different levels of difficulty. This opens new avenues for future OOD detection research.\n\nS2. The authors analyze the results of this benchmarking which lead to numerous interesting observations.\n\nS3. This paper presents a good viewpoint on designing large-scale objective benchmarks with different levels of detection difficulty for class-out-of-distribution, which is useful for real-world applications.\n\nWeaknesses:\n\nW1. It is not clear how and the intuition why a subset of ViTs achieves the best C-OOD detection performance.\n\nW2. The computational complexity is not discussed when analyzing baseline models, e.g., knowledge distillation, and language-vision CLIP.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper is good. The authors present a new benchmarking framework for class-out-of-distribution detection.",
            "summary_of_the_review": "This is a benchmarking paper focusing on an important problem. The contributions of the paper will have some impact to the OOD detection community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1977/Reviewer_3d28"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1977/Reviewer_3d28"
        ]
    },
    {
        "id": "P5V9yDpDnfd",
        "original": null,
        "number": 2,
        "cdate": 1666631965453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631965453,
        "tmdate": 1666631965453,
        "tddate": null,
        "forum": "Iuubb9W6Jtk",
        "replyto": "Iuubb9W6Jtk",
        "invitation": "ICLR.cc/2023/Conference/Paper1977/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a method for benchmarking models (combined with a dataset and a confidence function $\\kappa$) for their ability to recognize out-of-distribution examples at test time. In particular, the paper considers the OOD case of when the model is presented with an example with a label that that model had not seen during training. The paper uses the methodology to evaluate 525 different models as well as some different confidence functions to produce some observations about different types of models and about the OOD task generally.",
            "strength_and_weaknesses": "The paper well grounded in previous works is attacking a significant problem of interest to the community and has a very extensive empirical validation backing up its claims/observations (which are numerous). The most interesting aspect of the paper for me was the observations and claims about OOD detection, such as the performance of vision transformer models or how knowledge distillation helps with detecting OOD samples. I believe these types of observations are both important for practitioners and also as starting points for additional research.\n\nThe paper\u2019s only real weakness is in its clarity, and, possibly its correctness. There are some points in the paper that need further explanation. Specifically\n\n-\tIn section 2, how is AUC used to measure OOD performance? It seems like there is some cut-off score for $\\kappa$ below which the paper assigns the value of OOD versus ID, and then uses these assigned values to compute AUC. If so, what is the cut-off value? Is it different for different $\\kappa$\u2019s or models? \n\n-\tHow does one make apples-to-apples comparisons between models for the OOD performance? As is mentioned in the paper (section 5, point 6) OOD hardness is subjective and the proposed method will have different classes in different hardness ratings for different model + $\\kappa$ configurations. This would lead me to believe that one cannot really conclude that one model is better than another in OOD detection as data breakdowns between the two models are not the same. Is there a way to overcome this limitation or otherwise have a comparison between two models on their OOD performance where the only difference is the model itself? Or, is this not a problem for comparing models?\n\n-\tA minor point, but could you add the web addresses, probably as footnotes, for timm and torchvision for total clarity on the packages?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is high in quality and reproducibility. Then issues with its clarity are fixable and not serious (stated previously in limitations). In terms of the novelty of the paper, while the paper is not the first to address this idea of evaluating models for OOD performance, it does propose a significant advancement in this line of research.",
            "summary_of_the_review": "Sometimes the best papers are those papers that raise more questions than they answer. This is one of those papers. While the technical contributions are not especially monumental or flashy, the observations coming from the solid empirical work in the paper are very intriguing, especially from a practicing data scientist lens. As such, I believe this paper does merit acceptance and publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1977/Reviewer_gv9a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1977/Reviewer_gv9a"
        ]
    },
    {
        "id": "i_UKbH-Uj6",
        "original": null,
        "number": 3,
        "cdate": 1666633962835,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633962835,
        "tmdate": 1669480777866,
        "tddate": null,
        "forum": "Iuubb9W6Jtk",
        "replyto": "Iuubb9W6Jtk",
        "invitation": "ICLR.cc/2023/Conference/Paper1977/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a benchmark for out-of-distribution detection for ImageNet(-1K) models, using data from ImageNet-21k as out-of-distribution samples. The work presents a strategy for evaluating OOD detection, avoiding pitfalls encountered in prior work. Additionally, the authors evaluate a wide array of ImageNet models, and highlight findings on which models and confidence functions outperform others.",
            "strength_and_weaknesses": "Strength:\n\n- The authors point out a key issue in prior work on evaluating OOD detection: when OOD datasets are constructed by filtering to be difficult for one model, the bias in filtering results in other models naturally outperforming the original model. \n\n- The authors evaluate a large array of models on their data, in contrast to prior work, and find interesting conclusions about OOD detection (e.g., that CLIP models outperform models trained on ImageNet)\n\nWeaknesses:\n\n- The notion of severity levels needs to be motivated further. The paper states that the proposed method of computing severity levels is better than prior work \u2013 but why do we need severity levels? A plausible alternative metric is to simply compute AUROC on the entire OOD dataset, without the severity levels. Why is this metric insufficient? What conclusions would change if you were to use this metric?\n    - If the AUROC overall metric is sufficient, the severity levels can still be used as a diagnostic. It\u2019s unclear that one needs to look at AUROC at each level individually when comparing models.\n- While interesting, the result about CLIP\u2019s OOD detection needs further explanation and comparison. The paper claims it outperforms \u201c96%\u201d of other models. But this is a strange metric: the 96% presumably includes models like AlexNet. Using this metric, the ViT-L/32-382 outperforms \u201c99.99%\u201d of other models. It would be more interesting to compare a CLIP model head-to-head with a model with the same architecture, trained with different training strategies, such as training on ImageNet-1k (or pretraining on IN21k then finetuning on IN1K).\n- Notation could be more clear \u2013 it\u2019s still unclear to me exactly how AUROC is computed. \n    - In Eq (1), what is \\hat{y}? Is it the prediction of f on x1, or on x2? Is it the same when computing k(x1, \\hat{y} | f) as k(x2, \\hat{y} | f)? I assume not.\n    - In Section 3, k is changed from k(x1, \\hat{y} | f) to k(x | f). How is this defined?\n    - Is AUROC computed per OOD class, then averaged across classes within a severity level? Or are samples for all classes in a severity level merged together, and then AUROC is computed once for the severity?\n- How is ID AUROC defined?\n    - Do you compute AUROC per class, with samples from all other classes treated as negative, and then averaged?\n- Nits / questions:\n    - It would be useful to explain why ODIN, entropy and MC dropout specifically were chosen for evaluation in the work.\n    - Fig 6 \u2013 what severity level is this evaluated at?\n    - Sec 5, (1) \u2013 is this on all 525 models?\n    - Sec 5, (1) \u2013 any ideas why imagenet 21k training hurts OOD detection?\n    - Would it be possible to evaluate the effect of removing softmax at test time, and re-evaluating? Depending on how AUROC is defined (see my questions above), this may or may not change the results. This may also allow a more fair comparison between ImageNet models (which usually have a softmax) to CLIP models (which are just using cosine similarity without a softmax). Relatedly, evaluating the impact of the softmax temperature on OOD detection would be interesting. These experiments are not necessary for acceptance, but I would appreciate the authors\u2019 thoughts on the impact of the softmax \u2013 it will also help me understand the evaluation better.\n    - If possible, it would help to share the AUROC for all models at all severity levels in a table in the supplementary, or in a webpage\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: Please see weaknesses section. There are some key clarity questions that need to be resolved.\n- Quality / novelty: No concerns.\n- Reproducibility: Will the authors commit to publicly releasing code for evaluating all models, along with the AUROC per severity level for each model plotted in the paper? This will significantly help reproducibility.\n",
            "summary_of_the_review": "Overall, this is a good paper, with some issues in presentation and clarity. I have raised some questions about the presentation and clarity above, and particularly about the use of severity levels in the evaluation. Assuming these are addressed, I would vote to accept the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1977/Reviewer_CQ4c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1977/Reviewer_CQ4c"
        ]
    }
]