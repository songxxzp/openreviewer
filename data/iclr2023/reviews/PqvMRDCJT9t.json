[
    {
        "id": "7EiyxPxKb-S",
        "original": null,
        "number": 1,
        "cdate": 1666563954238,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666563954238,
        "tmdate": 1666563954238,
        "tddate": null,
        "forum": "PqvMRDCJT9t",
        "replyto": "PqvMRDCJT9t",
        "invitation": "ICLR.cc/2023/Conference/Paper3719/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces Flow Matching (FM), which provides a way to scale Continuous Normalizing Flows (CNFs) to very high dimensions such as ImageNet-128. Rather than using expensive simulation-based training approaches (as for conventional CNF training), the authors propose to regress the vector field learned by a neural network against the true vector field given by predefined conditional probability paths. This approach for leveraging conditional probability paths, and also using optimal transport (OT) displacement maps for Gaussian conditional probability paths, not only leads to computational gains, but also provides an alternative perspective on diffusion-based generative models that improves their performance and stability as well.",
            "strength_and_weaknesses": "Strengths:\n- The empirical results are very strong: CNFs trained with OT-based FM achieves SOTA on likelihood estimation relative to existing baselines (though some of these competing methods do not actually compute likelihoods on that scale) and sample quality in terms of FID on ImageNet-{32,64}. They are competitive with existing baselines on CIFAR-10.\n- OT-based FM also leads to more stable training, faster model convergence, and the simple/intuitive way in which the probability paths are defined leads to desirable properties such as \u201cfaster\u201d (more efficient) generation of the desired sample starting from random noise. This is in contrast to diffusion models, where the perceptual quality of the denoised (generated) sample increases drastically towards the latter timesteps.\n- Theoretically, the approach provides a nice perspective for understanding and unifying diffusion-based generative models. And by matching vector fields, this approach allows for the generalization of such generative models beyond the class of probability paths modeled by simple diffusions.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity/quality: The paper was very high quality with respect to both theoretical and empirical results, and also clear with nice technical exposition and explanations. \n- Novelty: The idea of matching vector fields is novel and interesting. The authors were also able to obtain impressive results.\n- Reproducibility: No code was included with the submission.\n",
            "summary_of_the_review": "Flow Matching is an interesting approach for scaling and improving the performance of not only CNFs, but also conventional diffusion-based generative models. The idea of matching vector fields and defining the generative process in terms of intuitive probability paths allows for not only empirical improvements in CNFs, but also ways to go beyond pre-defined diffusions and data modalities (e.g. this could be used for manifold data). This contribution is valuable and should be highlighted at the conference.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3719/Reviewer_SQzg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3719/Reviewer_SQzg"
        ]
    },
    {
        "id": "S3t8z29BhU",
        "original": null,
        "number": 2,
        "cdate": 1666675149582,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675149582,
        "tmdate": 1666675149582,
        "tddate": null,
        "forum": "PqvMRDCJT9t",
        "replyto": "PqvMRDCJT9t",
        "invitation": "ICLR.cc/2023/Conference/Paper3719/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new framework for training continuous normalizing flows, inspired by recent work on diffusion models. Originally, normalizing flows are trained by maximizing the likelihood of the pushforward density, which requires simulating the flow end-to-end for each gradient update and can thus be very costly. In this work, the proposed \"flow matching\" framework involves approximating the time-varying vector field of a desired ODE for each time t, much like the score matching algorithm in diffusion models. Thus training no longer requires end-to-end simulation of the flow, and simply involves a squared error loss for each time $t$.\n",
            "strength_and_weaknesses": "Strengths:\n- The work deepens the connections between diffusion models and normalizing flows originally proposed in [1], and allows for more types of probability flows, including non-diffusion flows.\n- Training involves a simple drop-in replacement of the loss in score-matching diffusion models.\n- Certain choices of the learned flow greatly improve sampling speed during inference.\n\nWeaknesses:\n- Models do not exhibit improved performance on CIFAR-10.\n- Authors claim state-of-the-art performance on ImageNet in terms of sample quality, however, FID metrics are much higher than that reported in [2] (see Table 5, for ImageNet 64x64). Can the authors comment on this?\n\n[1] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S. and Poole, B., 2020. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456.\n\n[2] Dhariwal, P. and Nichol, A., 2021. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34, pp.8780-8794.",
            "clarity,_quality,_novelty_and_reproducibility": "While [1] has already connected diffusion models to normalizing flows, this work further expands the class of learnable flows to include non-diffusion flows. In particular, it appears that OT flows are more efficient to sample from, which makes this extension more than a theoretical curiosity. Therefore, I believe the novelty is both significant and relevant.\n\nClarity:\nThe paper is generally well-written, though I found certain experimental results puzzling.\n\nBy my understanding, flow matching with diffusion flow (i.e. example I in Section 4.1) results in the same forward and reverse processes as diffusion models, except that, instead of just learning the score $\\log p_t(\\mathbf{x})$, flow matching models learn\n$d\\mathbf{x} = \\left[ \\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2} g(t)^2 \\nabla_x \\log p_t(\\mathbf{x}) \\right]dt$\ni.e., Eq. 13 in [1].\n\nHowever, according to Table 1, score matching and flow matching differ in performance, even when applied to the same diffusion process. Why is this the case? Shouldn't the learned ODE be identical between flow and score matching? In a similar vein, why would Flow Matching perform so much worse than score matching on CIFAR-10 in general?",
            "summary_of_the_review": "The paper is well-written and provides a straightforward generalization of the diffusion framework. Performance on ImageNet 64x64 is strong, and suggests that the theory translates well into practical improvements in image modeling.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3719/Reviewer_LJky"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3719/Reviewer_LJky"
        ]
    },
    {
        "id": "pmN0r3PRNZy",
        "original": null,
        "number": 3,
        "cdate": 1667144118330,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667144118330,
        "tmdate": 1667144118330,
        "tddate": null,
        "forum": "PqvMRDCJT9t",
        "replyto": "PqvMRDCJT9t",
        "invitation": "ICLR.cc/2023/Conference/Paper3719/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents flow matching, a method for training continuous-time normalizing flows by directly regressing the vector field of a chosen probability path. Flow matching only requires the ability to sample from the chosen probability path for training, and importantly does not require propagating gradients through ODE solver dynamics. The authors connect flow matching to optimal transport and existing diffusion methods for generative modeling. Empirical results suggest flow matching can give improved training and sampling efficiency over standard diffusion models. ",
            "strength_and_weaknesses": "**Strengths**\nThe fundamental idea proposed in the paper is excellent. Analogously to how denoising score-matching makes score-matching tractable in diffusion models, flow matching shows that the diffusion formulation is not needed at all, and regressing the target vector field defined by a simple probability path is possible directly. The authors then explicitly lay out the connection to existing diffusion models, as well as an alternative transport path. Both the flow matching objective and the OT path are ablated independently on toy and ImageNet-level tasks. These are all strong contributions, potentially simplifying the generative modeling landscape and making strong connections in the existing literature (continuous-time flows, diffusion models, optimal transport). \n\n**Weaknesses**\nA small constant sigma_min > 0 is used throughout the paper as the initial noise applied to the data distribution. That is, t = 1 corresponds to sigma =  sigma_min instead of sigma = 0. As far as I can see, the reason for introducing sigma_min is not stated explicitly. Is it there for practical/numerical or theoretical reasons? Why can't we have sigma = 0 at t = 1? This should be explained explicitly.    \n \nI think the biggest difficulty I have with the paper is the baseline diffusion model used for the empirical evaluation. A key claim of the paper is that the proposed method effectively gives practitioners a better way to fit the vector field of a probability path than through the diffusion formulation. I would therefore expect the implementation of the diffusion baseline to be treated with due care.\n\nFor example, appendix E.1 eq (41) states that the diffusion model regresses the score function directly. In practice, diffusion models often target a quantity related to the score, such as the clean data, or the Gaussian noise added to the clean data, from which the score is derived. The choice of parameterization can significantly impact the quality of the learned model (DDPM, Ho et al. 2020), since the magnitude of the score varies widely over time. Moreover, the ODE in eq. 42 has semi-linear structure (the sum of a linear and non-linear term). Existing work (DDIM, Song et al. 2020, DPM-solver, Lu et al. 2022) has shown that using black-box solvers which don't account for this structure (i.e. solving the linear part exactly) can lead to unnecessary errors in the solution, again impacting performance. \n\nIf it is an advantage of flow matching that the parameterization of the regression target and the choice of ODE solver do not need such careful consideration, this should be stated explicitly and taken into account for direct comparison, rather than comparing to a diffusion model implementation which is not as performant as it might be. \n\nNits:\n- 4: 'There is potentially an infinite number of vector fields that generate any particular probability path,\nbut the vast majority of these' -- what is meant by vast majority? Quantify this formally. \n- 4.1: 'we can set them to any reasonable function' -- what is a 'reasonable function'? Again be precise.\n- 6.1: 'we find that we can achieve very reasonable performance' -- what is 'very reasonable performance'? \n- 6.1: The BigGAN citation is for Lu\u010di\u0107 et al. 2019 but the canonical citation is Brock et al. 2018\n- 7: 'FM can be generalized to manifold data' -- perhaps, but you can't claim this without demonstrating it ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\nThe paper is clearly written and presented. The diagrams throughout are informative and helpful. \n\n**Novelty**\nAs far as I'm aware, the core method for directly and efficiently regressing the vector field corresponding to a probability path is novel, as are the connections to diffusion models and optimal transport. ",
            "summary_of_the_review": "While there are a couple of issues I would like to see addressed, I think this is a strong submission and would recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3719/Reviewer_tWC7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3719/Reviewer_tWC7"
        ]
    },
    {
        "id": "JKKKqJR_Tl",
        "original": null,
        "number": 4,
        "cdate": 1667584011797,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667584011797,
        "tmdate": 1667584011797,
        "tddate": null,
        "forum": "PqvMRDCJT9t",
        "replyto": "PqvMRDCJT9t",
        "invitation": "ICLR.cc/2023/Conference/Paper3719/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes flow matching, a method for training continuous normalizing flows without numerical ODE simulation. Key to flow matching is a framework for constructing suitable probability paths via a mixture of conditional (simpler) probability paths. Authors further draw connection to score-based diffusion models and optimal transport. Inspired by the connection with optimal transport, authors propose a new type of probability paths that are more efficient than existing ones in diffusion models. Experiments confirm that the proposed flow matching approach can obtain competitive image quality and likelihoods on real-world image datasets.",
            "strength_and_weaknesses": "## Strength\n1. Very clear writing.\n2. Proposed method is simple and effective.\n3. The method generalizes conventional score-based diffusion models without complicating the formulation.\n4. The optimal transport path stabilizes training and leads to more efficient sampling.\n\n## Weaknesses\n\n1. Novelty is limited. One central contribution of this work is a method to aggregate conditional probability paths to form a target probability path that connects a simple distribution $p_0$ to the data distribution $p_1$. This is actually a special case of the result in [1]. In fact, Theorem 1 in this paper can be derived from Theorem 1 in [1] by simply replacing SDEs with ODEs (setting the diffusion coefficient to zero). In addition, the conditional flow matching objective was already proposed in [1]. In light of [1], the contributions of this work are mostly experimental verifications, and using optimal transport to construct probability paths. It is disappointing to see that this work does not cite [1] and neither does it discuss the important connections.\n\n2. Experiments on ImageNet are likely wrong. It is surprising to see that Flow Matching has a significant lead over other approaches on ImageNet 32x32 and 64x64, while having mediocre results on CIFAR-10. Even the authors' reproduction of score matching models on ImageNet is significantly better than previously reported. I'm 99% sure that this is caused by using the wrong downsampled ImageNet dataset. In generative modeling, people use the downsampled ImageNet dataset from [2], not the one from [3]. The major difference is in downsampling algorithms: the former does not use anti-aliasing and is therefore significantly more difficult for maximum likelihood training compared to the latter. Authors need to re-run their experiments on ImageNet and report the correct results.\n\n3. Authors motivate Flow Matching as a simulation-free training method for continuous normalizing flows. It is important to mention that ScoreFlow in [4] was actually proposed earlier with the same goal. In fact, ScoreFlow is the first simulation-free training method for continuous normalizing flows that are based on diffusion processes. Compared to flow matching, it has the advantage of directly maximizing log-likelihood (see also [5]) and thus will outperform flow matching in terms of NLLs. This paper should compare with ScoreFlow more carefully, include more discussions with it, and highlight aspects where flow matching outperforms ScoreFlow.\n\n## References\n[1] Peluchetti, Stefano. \"Non-Denoising Forward-Time Diffusions.\" (2021).\n\n[2] Van Den Oord, A\u00e4ron, Nal Kalchbrenner, and Koray Kavukcuoglu. \"Pixel recurrent neural networks.\" International conference on machine learning. PMLR, 2016.\n\n[3] Chrabaszcz, Patryk, Ilya Loshchilov, and Frank Hutter. \"A downsampled variant of imagenet as an alternative to the cifar datasets.\" arXiv preprint arXiv:1707.08819 (2017).\n\n[4] Song, Yang, et al. \"Maximum likelihood training of score-based diffusion models.\" Advances in Neural Information Processing Systems 34 (2021): 1415-1428.\n\n[5] Lu, Cheng, et al. \"Maximum likelihood training for score-based diffusion odes by high order denoising score matching.\" International Conference on Machine Learning. PMLR, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "* Very clear writing. \n\n* Quality in experiments and overall novelty is less satisfying. See detailed explanation above.",
            "summary_of_the_review": "Flow matching is a nice and clean formulation for simulation-free training of CNFs, generalizing score-based diffusion models to non-Gaussian noise and optimal transport paths. However, the idea is not completely new, and experimental results on ImageNet are unreliable.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3719/Reviewer_AKwV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3719/Reviewer_AKwV"
        ]
    }
]