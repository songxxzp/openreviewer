[
    {
        "id": "1t56XCYomXJ",
        "original": null,
        "number": 1,
        "cdate": 1666558178886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558178886,
        "tmdate": 1670795254517,
        "tddate": null,
        "forum": "n0Pb9T5kmb",
        "replyto": "n0Pb9T5kmb",
        "invitation": "ICLR.cc/2023/Conference/Paper1742/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the topic of contrastive SSL. In particular, the paper studies whether using multiple data augmentations and choosing the one the model performs worst at is a more effective approach than Contrastive SSL. The paper calls this ArCL and also provides more experiments.",
            "strength_and_weaknesses": "# Strength \n* The paper studies the role of data augmentations in contrastive SSL in a rigorous manner. I wonder if it can be linked to the concept of expansion [3]. This can serve as a basis for more theoretical studies on the role of data-augmentations in various fields within ML.\n\n* The concept of ArCL is quite natural but showing how it might be necessary is an interesting take. \n\n# Cons\n\n* As I explain below, the experimental comparisons between ArCL and SImCLR might be unfair. Further, if the topic is distribution shift, I would recommend using more distribution shifts for example _Controllable Shift_s in [1].\n* The significance of the theoretical results are not immediately clear. Ofcourse, the accuracy increases with more augmentations , but other than that there does not seem to be any takeaway from the theoretical results. Also some terms, like $n$  and the absence of $N$ in Thm 5.2 is unclear.\n* Discussion of similar work like [2] is also missing. This particular work also discusses the same question but highlights the importance of the number of negative samples, which is completely ignored in this work.",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity** - The theoretical results are a bit opaque. Its implications need to be clarified.\n\n* **Reproducibility** - I do not have any concerns regarding reproducibility\n\n* ** Novelty** - The proposed regulariser ArCL is novel, as far as I am aware. However, the drawn conclusion about more augmentations helping SSL is not novel.",
            "summary_of_the_review": "* In Equation (2), the linear classifier is evaluated as the best linear classifier on the target domain. While this is usually not done in practice, some recent works (e.g. [1]) used this to empirically observe the robustness of SSL methods. The authors should discuss this and other similar works as they provide empirical  use cases of their setting. Perhaps, Lemma 3.1 indeed applies to the experimental setting in these papers.\n\n* Theorem 3.2 shows a kind of equivalence between SSL and Supervised for appropriately chosen regularisation term, This is similar to the recent work of bao et. al, which is also not cited in this paper. In addition Bao et. al. quantifies the effect of the number of negative samples in the regularisation term and presents a more realistic results as increasing number of negative examples work better in practice.\n\n* The example in Proposition 4.1 presents a multiplicative noise model. This can indeed lead to interesting theoretical examples showing that minimising expectation across domains is arbitrarily worse than  minimising supremum across domains. However, I am not sure if this is a realistic problem to address.\n\n* Theorem 5.2 has a dependence on m and n, but I don't see the dependence on N, the number of negative examples. I am also not sure what $n$ is here. It seems to be an application of Rademacher uniform convergence result, but $n$ is not defined here. Could the author discuss what this is ?\n\n* In the experiments, doing an ArCL with m views requires m forward passes with m augmentations and then selecting the worst one. A normal SimCLR uses just one random augmentation instead if I understand correctly. That is an unfair comparison. Could the authors do a fairer comparison either by multiplying the number of epochs with the number of views or doing an expectation over m views for SimCLR.\n\n* Could the authors also more extreme distribution shift experiments on more realistic/synthetic distribution shifts using the framework provided in [1] ?\n\n* How do the results change with varying $k$ as in [2] ?\n\n\n=========== Post Rebuttal =================\n\nAs the discussion below shows, we had an interesting discussion with the authors and due to the reasons stated below, I am of the position that I keep the score but do object acceptance if the other reviewers want to fight for it.\n\n\n[1] How robust are pre-trained models to distribution shift? https://arxiv.org/abs/2206.08871\n[2] Bao, Han, Yoshihiro Nagano, and Kento Nozawa. \"Sharp learning bounds for contrastive unsupervised representation learning.\" arXiv preprint arXiv:2110.02501 (2021).\n[3] Balcan, Maria-Florina, Avrim Blum, and Ke Yang. \"Co-training and expansion: Towards bridging theory and practice.\" Advances in neural information processing systems 17 (2004). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1742/Reviewer_jXHS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1742/Reviewer_jXHS"
        ]
    },
    {
        "id": "HYr3mhffnih",
        "original": null,
        "number": 2,
        "cdate": 1666639943664,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639943664,
        "tmdate": 1666639986945,
        "tddate": null,
        "forum": "n0Pb9T5kmb",
        "replyto": "n0Pb9T5kmb",
        "invitation": "ICLR.cc/2023/Conference/Paper1742/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work established a theoretical framework to analyze the OOD performance of contrastive learning based self-supervised learning and suggest that the effective of contrastive learning mainly comes from data augmentation. Further, they proposed augmentation-robust contrastive learning to show the better OOD performance.",
            "strength_and_weaknesses": "+ This work starts from a theoretical perspective to analyze the OOD performance of contrastive learning in self-supervised learning. The analysis reveals that the better performance of contrastive learning mainly comes from data augmentation.\n+ This work proposes a simple augmentation-robust contrastive learning to improve OOD generalization of SSL.\n- The empirical results seem not to be very extensive. Can authors provide the results of ArCL applied to both SimCLR and MoCo on all the three datasets rather than picking one model for each of these datasets?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and focuses on theoretical analysis of contrastive loss in self-supervised learning, followed with an empirical improvement.",
            "summary_of_the_review": "Since I do not have too much background in theoretical analysis, I could hardly measure the novelty and contribution of this work to the community. The proposed augmentation-robust contrastive learning (ArCL) seems to work well on two framework SimCLR and Moco although I am not sure why the authors do not provide the two frameworks on all the three datasets.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1742/Reviewer_y4QB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1742/Reviewer_y4QB"
        ]
    },
    {
        "id": "9iuXOVeOM_",
        "original": null,
        "number": 3,
        "cdate": 1666759803902,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666759803902,
        "tmdate": 1668842073786,
        "tddate": null,
        "forum": "n0Pb9T5kmb",
        "replyto": "n0Pb9T5kmb",
        "invitation": "ICLR.cc/2023/Conference/Paper1742/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the benefit/role of learning representations with contrastive learning for out-of-distribution (OOD) generalization.\nIt argues that one way contrastive learning could help with covariate shift is through the diversity in the augmentation distribution, even if the input distributions shift a lot.\nThis is justified with some theory to upper bound the supervised risk of a representation by the contrastive loss and some other \u201crepresentation diversity\u201d terms, under some clusterability assumptions on augmentations from prior work.\nHowever the paper finds that vanilla contrastive learning is not sufficient to learn *domain invariant representations* and thus might do poorly in OOD settings.\n\nViewing different augmentation transformations (like random cropping, gray scaling, brightness) as different \u201cdomains\u201d, the paper notes that contrastive learning only tries to make representations invariant to these on average over the augmentation distribution.\nInstead it proposes an augmentation-robust contrastive learning (ArCL) that encourages representation invariance for the worst case sets of augmentations, i.e. minimizing $\\sup\\_{A,A\u2019} ||f(A(x) - A\u2019(x)||^2$ instead of $\\mathbb{E}\\_{A,A\u2019} ||f(A(x)) - f(A\u2019(x))||^2$.\nThis ArCL alignment term (combined with MoCo) seems to do much better on linear probing and fine-tuning evaluation while transferring from ImageNet -> one of many downstream image classification tasks.",
            "strength_and_weaknesses": "**Strengths**\n\n- The new augmentation-robust contrastive learning algorithm is an interesting variant of contrastive learning that goes beyond \u201caveraging over augmentations\u201d. To my knowledge this is a novel idea and it also clearly learns better representations than standard contrastive learning, based on the experiments on ImageNet.\n- The paper is clearly written for the most part and easy to follow\n- Theoretical results are provided to justify the need for going beyond the average over augmentations setting, and also to upper bound the transfer risk for ArCL\n\n**Weakness**\n\n- One of the main concerns about this paper is the use of the term *OOD generalization* for the settings discussed in the paper. In the entire discourse, including experimental evaluations, a representation is learned using unlabeled data from a source domain (ImageNet) and this is evaluated with linear probe or fine-tuning by using **labeled data** from the target domain (CIFAR, Caltech101, etc.). While I am not an expert in OOD generalization, I believe that the standard OOD setting assumes no targets (and sometimes no inputs) from the target domain. I verified this with researchers who constantly publish OOD generalization papers and also checked some surveys (see Section 2.1.2 in [1]). In fact the setting discussed in this paper is much closer to something like \u201cself-supervised transfer learning\u201d. Even the original SimCLR paper [2] evaluate in this setting; see Table 8. OOD generalization is typically evaluated using benchmarks such as WILDS [3]. In light of this, a lot of the claims in the paper will need to be changed, including the title, or actual OOD evaluations need to be included.\n\n- The abstract makes the following claim: **recent work claims that contrastive learning learns more robust representations than supervised learning, our results suggest that this superiority mainly comes from the data augmentation used, i.e., more data are fed to the model.** However I do not think this is adequately justified in the paper. I could not find any experimental evidence to claim that the superiority of contrastive learning mainly comes from \u201cmore data fed to the model\u201d. One way to test this is the following: how well does *supervised learning* with the same augmentations do on OOD? If it does well, then one could believe that augmentations played a huge role. Otherwise it could be that contrastive learning somehow learns better features even with the same augmentations. Either the claim needs to be changed to *augmentations can play a role* (based on Section 3), or some experiments are required to justify *superiority mainly comes from \u2026*.\n\n- Section 3 is used to justify that augmentations play a huge role in the good OOD (actually transfer) performance, but the section seemed a little out of place after reading the full paper because it is not really related to the subsequent ideas. It just seems like an \u201cimproved upper bound\u201d for the performance of contrastive learning representations on a downstream task. In fact even the analysis from [4] shows an upper bound on supervised learning risk for the augmentation distribution (which they later convert to guarantees for the input distribution). I think the first equation in Theorem B.3 from the arxiv version of [4] shows this. So I think it is important to both, mention the bound in [4] and justify the section better.\n\n\nOther comments/questions:\n- Could it be that the superior performance of ArCL compared to CL is due to the fact that it gets to see more augmentations in total? If so, some ablation study to isolate this effect could be useful\n- Treating augmentations as domains for the results in Section 5  is a little weird, because they are not really domains of interest for any downstream evaluation. How does this idea (and Theorem 5.2) connect to the ImageNet experimental settings?\n- Is there any connection to ViewMaker network [5] that learn augmentations in an adversarial manner? Those augmentations are more robust to common corruptions in CIFAR. It would be a useful experiment to see if ArCL helps over those augmentations as well.\n- Proposition 5.1 $h_A \\in \\arg\\min$ might be more appropriate\n- In Theorem 5.2, it seems like the bound if non-vacuous only if $m$ (number of views used) is of the order of $1/c\\_{\\pi}$. However $c\\_{\\pi}$ will be at least $|\\mathcal{A}|^{-1}$ and so one would need $m$ to be as large as the total number of distinct transformations, which is unreasonably high. Of course this is just an upper bound, but it might be useful to discuss this a bit more. Maybe some covering number argument on $\\mathcal{A}$ might lead to a tighter bound since many $A$ might be close to each other.\n\n\n[1] Shen et al. Towards Out-Of-Distribution Generalization: A Survey\n\n[2] Chen et al. A Simple Framework for Contrastive Learning of Visual Representations\n\n[3] Koh et al. WILDS: A Benchmark of in-the-Wild Distribution Shifts\n\n[4] Haochen et al. Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss\n\n[5] Tamkin et al. Viewmaker Networks: Learning Views for Unsupervised Representation Learning",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly easy to follow and many of the findings, especially the use of worst case augmentation unalignment, are novel. Some of the claims are not well justified, as described above. Also the use of the term *OOD generalization* seems incorrect for the setting that is considered.",
            "summary_of_the_review": "Overall I think the paper makes interesting contributions to study the benefit of contrastive learning in the transfer setting. While the findings and results paper are quite promising, the issues with the incorrect usage of OOD generalization and some other issues about unjustified claims, I believe that the paper could use benefit from round of reviews. Thus for now I would assign a score of reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1742/Reviewer_QCLS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1742/Reviewer_QCLS"
        ]
    }
]