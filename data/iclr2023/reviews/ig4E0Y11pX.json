[
    {
        "id": "3_O-y2vDKE",
        "original": null,
        "number": 1,
        "cdate": 1665652482120,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665652482120,
        "tmdate": 1665652482120,
        "tddate": null,
        "forum": "ig4E0Y11pX",
        "replyto": "ig4E0Y11pX",
        "invitation": "ICLR.cc/2023/Conference/Paper2930/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "I informed the area chair that I cannot review this paper.",
            "strength_and_weaknesses": "I informed the area chair that I cannot review this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "I informed the area chair that I cannot review this paper.",
            "summary_of_the_review": "I informed the area chair that I cannot review this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_XNZ2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_XNZ2"
        ]
    },
    {
        "id": "XMPZh0kowde",
        "original": null,
        "number": 2,
        "cdate": 1666278748231,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666278748231,
        "tmdate": 1666278748231,
        "tddate": null,
        "forum": "ig4E0Y11pX",
        "replyto": "ig4E0Y11pX",
        "invitation": "ICLR.cc/2023/Conference/Paper2930/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proves a group imbalanced generalization result for a one-hidden-layer neural network under the Gaussian mixture model assuming that the labels are generated by a ground truth model of the same structure. With this result, the paper argues that the learning performance is the best when the spectral norm of the co-variance of each Gaussian component is at a medium regime (neither too big nor too small), and increasing the fraction of the minority group data does not improve the test performance if the spectral norm of its co-variance is large. The authors then conduct synthetic and realistic experiments to verify this result.",
            "strength_and_weaknesses": "Strength: The proof of the main result (Theorem 1 and Corollary 1) has some technical novelty, and the result leads to some interesting insights. For example, I think (P4) is a very interesting catch. The authors also validate the result with experiments.\n\nWeaknesses: The main concern I have is that the main result (Theorem 1) only works for a very limited setting, and the insights might not be generalizable to a more general setting. While it is understandable that such a theoretical result requires some strong assumptions, I feel that the assumptions made in Theorem 1 are way too strong, and some insights drawn from the result (especially (P5) and (P6)) are questionable.\n\nSpecifically, Theorem 1 assumes that:\n- The inputs are sampled from a specific Gaussian mixture model\n- The model is a specific 1-hidden-layer neural network\n- The labels are generated by a ground truth model of the exact same structure as the model used for training\n- $\\sigma_{min}$ is bounded away from zero, which is actually a very strong assumption: It is well known that in most tasks, the data lies on a low-dimensional manifold in a high-dimensional space, in which case $\\Sigma_l$ could even be non-invertible. And even if it is invertible, $\\sigma_{min}$ should be very close to zero.\n\nFurthermore, the main result (Theorem 1) only considers ERM, i.e. minimizing the average risk. It does not discuss any robust training method. One thing I believe the authors should add to the paper is ERM with importance weighting, i.e. minimizing the weighted average of the empirical risk. This should not be too different from ERM. Even better, the authors could also consider DRO, group DRO, etc.\n\nRegarding the insights, (P5) suggests that the performance would be the best if all Gaussian components have zero means, in which case the groups largely overlap with one another (provided that $\\tau = \\Theta(1)$). This seems rather vacuous. If all groups have the same mean and the same co-variance then of course the performance would be the best given that the groups share the same labeling function. However, in real tasks such as a fairness task, the groups are very different. Plus, I think the real reason why the groups need to have zero means is that the model does not have a bias term: It\u2019s $\\phi(w^{\\top} x)$ rather than $\\phi (w^{\\top} x+b)$.\n\n\nMoreover, I cannot see how (P3) and (P5) lead to (P6). In this paper\u2019s setting, batch normalization mixes the samples from different Gaussian components and potentially messes up the data structure. (P6) is only valid when all the samples in the batch come from the same component, or when the components largely overlap with each other as in (P5).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and easy to follow. There is some technical novelty in the proof of the main result.",
            "summary_of_the_review": "Overall, I think this paper provides some interesting insights (I especially like (P4)). However, the main result does make a lot of strong assumptions, and some insights drawn from the main result ((P5) and (P6)) are questionable, so I am concerned that these insights might not be generalizable to a more realistic setting. Moreover, it seems to me that all results and discussions in this paper are based on Theorem 1. While the proof of Theorem 1 is indeed not easy, I am concerned that the contributions might be insufficient for ICLR acceptance, especially given that Theorem 1 requires too many strong assumptions. I think the authors should also include robust training methods, such as importance weighting, in their result. I believe that this paper can benefit from another round of edition, so I recommend weak rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_n1W9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_n1W9"
        ]
    },
    {
        "id": "Wktbxbq0qR",
        "original": null,
        "number": 3,
        "cdate": 1666693591558,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666693591558,
        "tmdate": 1666693591558,
        "tddate": null,
        "forum": "ig4E0Y11pX",
        "replyto": "ig4E0Y11pX",
        "invitation": "ICLR.cc/2023/Conference/Paper2930/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors theoretically investigate the generalization performance of the one-hidden-layer neural network under a mixture of Gaussian. The main focus of this paper is to analyze the effect of group imbalance on the generalization ability. To simulate the group imbalance, their model can vary the group-wise Gaussian parameters. As a result, they demonstrate that the medium magnitude of the covariance achieves the highest generalization ability. This result also implies that increasing the minority group data does not always improve the generalization ability. The empirical evaluations also demonstrate the claim obtained in the theoretical result.",
            "strength_and_weaknesses": "Strength:\n-  Well-written and easy to follow.\n- The insights regarding the effects of the group-wise covariances help understand the effect of the group imbalance.\n- The introduced analysis techniques look novel \n\nWeakness:\n- Lack of empirical evaluations with a deeper model. ",
            "clarity,_quality,_novelty_and_reproducibility": "Group imbalance is a crucial issue in deep learning. Investigation of the effect of group imbalance on the generalization ability is interesting and is in high demand. The theoretical insights, particularly the dependency of the covariance norm on the generalization ability, are very interesting. More peculiarly, the insight of (iv) and (v) in Corollary 1 helps to understand the effect of the group imbalance. The analysis techniques also look novel. As claimed by the authors, analyzing the local convex region under the GMM model has a significant difficulty. \n\nGiven the results in this paper, a natural question arises do the results generalize for a deeper model? I understand the theoretical analysis for a deeper model is hard. However, empirical evaluations for a deeper model might be carried out easily. It is very helpful if the empirical analyses on when the findings generalize to a deeper model are included. ",
            "summary_of_the_review": "This paper is well-written and easy to follow. Investigation of group imbalance is in high motivation as group imbalance is a crucial issue in deep learning. The provided insights are interesting and helpful for understanding the effect of the group imbalance. I hence recommend the acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_LoT6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_LoT6"
        ]
    },
    {
        "id": "5dZ6Aq8HJ0n",
        "original": null,
        "number": 4,
        "cdate": 1666799931482,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799931482,
        "tmdate": 1666799931482,
        "tddate": null,
        "forum": "ig4E0Y11pX",
        "replyto": "ig4E0Y11pX",
        "invitation": "ICLR.cc/2023/Conference/Paper2930/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the convergence, average population risk, and average per-group population risk of empirical risk minimization (ERM) with gradient descent (GD). They study these phenomena under the assumptions that:\n\n* the input data features come from a Gaussian mixture model (GMM) with mixing coefficients equal to the fraction of the data collected for that group, \n* the input data labels are obtained with a neural network of the same architecture and some unknown weights,\n* the gradient descent is applied under a neural network with a single hidden layer, and\n* the task is a binary classification and the loss is the binary cross-entropy.\n\nTheir main result is Theorem 1, which provides some insights into when the performance is best:\n\n* having a group mean close to zero (in fact, low norm $\\lVert \\mu_l \\rVert$) is beneficial for training,\n* having the group co-variance close to the medium co-variance is beneficial for training, and\n* increasing the fraction of the minority group does not always increase the performance (it can degrade performance when the co-variance is far from the medium co-variance).\n\nAnother contribution that is not explicitly in the main text is new results on GMMs' concentration.",
            "strength_and_weaknesses": "**Strengths**\n\n* The paper tackles an important problem: convergence and generalization of GD with imbalanced data.\n\n* The insights from the main theorem are interesting and potentially useful to develop fairer learning algorithms.\n\n* Some new results regarding GMMs' concentration are given in the appendix.\n\n**Weaknesses**\n\n* The learning model assumption seems a little artificial and restrictive.\n\n  * It is mentioned that a one hidden layer model is assumed, but then we are provided with the model in (1), which is standard logistic regression (that is, a linear layer followed by a sigmoid activation function). \\\n  It is mentioned that the weights in the second layer are assumed to be fixed to facilitate the analysis. This is an artificial setting by itself, which even if other works have employed it, needs some justification as is quite uncommon and its utility is unclear to me. But also the second layer weights are not appearing in (1).\n\n  * The assumption that the real function from features to labels is also strong. Is it also assumed that they come from the arg-max after a linear layer?\n\n* There are some parts of the text where the claims seem incorrect or potentially incorrect:\n\n  * Why do you say that GD converges linearly in the introduction? In the Theorem 1 we see that it converges at a rate $\\mathcal{O}(\\sqrt{d \\log n / n} / K^{1.5})$. That's not linear in any parameter.\n\n  * Why does that group-level mean shifts from zero justify the pre-processing of making the data zero-mean? Note that this pre-processing will make the overall data have a mean of zero. Since it is assumed that the data comes from a mixture of distributions, it is not guaranteed it will have a beneficial effect on a minority group (indeed, it can be harmful if the mean of the group shifts away from zero after the average centering). \\\n  The same problem occurs with (P6). Unless the batch normalization is applied per group, which is not the case in this paper, it does not necessarily help as the means and variances are updated to normalize the whole batch considered as a unimodal distribution.\n\n  * In Section 3, it is said that $x$ follows a GMM $\\sum_{l=1}^L \\lambda_l \\mathcal{N}(\\mu_l, \\Sigma_l)$. This assumes that the fraction of the data from each group exactly matches the influence in the real mixture model. In practice, if one has a GMM $\\sum_{l=1}^L \\lambda_l \\mathcal{N}(\\mu_l, \\Sigma_l)$ one will observe $\\tilde{\\lambda}_l n$ samples of each group with $\\tilde{\\lambda}_l \\to \\lambda_l$ as $n \\to \\infty$. How does this affect your results?\n\n  * The assumption that $\\tau \\in \\Theta(1)$ seems strong and potentially not true in most cases.  I did some checks with random matrices $A \\in \\mathbb{R}^d$ generating potential covariances matrices $\\Sigma = A^T A$ and it seems that $\\tau \\in \\Theta(d)$. This can be problematic since in the proofs the dependence with $\\tau$ (and hence potentially $d$) can be as high as $\\tau^{12}$. \\\n  You could check how this condition holds up taking natural images of different sizes and assuming they are Gaussian. Compute their covariances per group and see if $\\tau$ grows or not with the dimension to see if this assumption would hold or not in practice.\n\n  * In Corollary 1, how do conditions (i) and (ii) be true simultaneously? Consider $\\Sigma_l^{(1)}$ and $\\Sigma_l^{(2)}$. Each is worse than the other since one has a spectral norm closer to zero (condition (i)) and the other has a larger spectral norm (condition (ii)). The idea is further explained in (P3), but needs a more formal statement here.\n\n  * In (P4) it is mentioned that $\\lVert \\Sigma_l \\rVert$ is the smallest among all groups, increasing $\\lambda_l$ improves the learning performance since the learning performance is enhanced at a medium regime of group-level co-variance. But if it is smallest, as in the case where it is largest, the medium-level covariance may change, thus also decreasing the performance. It would only increase performance when $\\lVert \\Sigma_l \\rVert$ is close to the medium level.\n\n  * In the appendix, you use that $\\nu_i$ ar i.i.d. zero-mean Gaussians but also assume they have bounded magnitude. How is this possible? I believe you can still recover some of your results by saying that this happens with a certain probability using some tail bounds on Gaussian random variables, but as of now, I do not understand how can this happen.\n\n  * When applying l'H\u00f4pital's  rule in (33) if I am not mistaken you are using that both $f(\\sigma) = 1 / (u_j^2 / \\sigma^2 + 1) \\to 0$ and $g(\\sigma) = \\beta_0(i, u/\\sigma, \\sigma) - \\alpha_0(i, u/\\sigma, \\sigma) \\to 0$ as $\\sigma \\to 0$. However, when applying the rule $\\partial f(\\sigma) / \\partial \\sigma = 2 u_j^2 \\sigma / (u_j^2 + \\sigma^2)^2$ and not $u_i^2 / (2 \\sigma)$. Hence, (33) ends up being a situation of $0 \\cdot \\infty$ and it is not determined. \\\n  This affects other properties like Property 3 (4) which builds upon the fact that $\\rho > 0$.\n\n  * I don't follow the equality after the inequality in (52). Where did the $e^{-|\\mu|^2 / 2}$ term go?\n\n  * Many of the lemmata leading to the main results are based on approximate inequalities ($\\gtrsim$). This is concerning since if the approximation error is not quantified it may have important effects on the final results.\n\n* There are some parts of the text that are not clear to me:\n\n  * In Figure 3 (c), it is not clear to me how the convergence rate is calculated.\n\n  * In Figure 4 (b) the test loss of the minority group is lower than the average risk. How is this possible? That would mean that the largest group has a larger loss.\n\n  * Why are the results in Figure 6 consistent with (P4)? In order to have this consistency it would be good to know which are $\\lVert \\Sigma_{\\text{male}} \\rVert$ and $\\lVert \\Sigma_{\\text{female}} \\rVert$, even if only empirically, and then observe how it is indeed the case that when the performance increases or decreases is for the reasons outlined in (P4).\n\n  * Similarly, the results with increasing $\\delta$ or $w$ are not clearly showing what they intended. We would need an estimation of $\\lVert \\Sigma_{\\text{male}} \\rVert$ and $\\lVert \\Sigma_{\\text{female}} \\rVert$ to see that effect. Similarly, one could also test doing a subtraction of the mean and increasing the mean of these means to see if the results are better when this happens and they get worse as they diverge. \n\n* Sometimes some important details are left:\n\n  * In (29) note that we know that (27) and (28) are positive due to Jensen's inequality. \n\n  * In the proof of Property 3 (3) note that we can use the assumption that the Gaussian behaves like the delta due to the Dominated Convergence Theorem (DCT).\n\n* The bound, though interesting, seems fairly vacuous.\n\n  * Note the potential dependence on $d$ of $\\tau$. In this case the number of samples required would explode with increasing dimensions. Even in the artificial case when $\\Sigma_1 = \\Sigma_2 = I$, for feature dimension 100, 3 units, and 2 classes, one needs more than 150k samples for the results to apply (Figure 1 a). Note that this is far less than the MNIST data samples (60k), where the feature dimension is 256.",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity**: The clarity of the paper could be improved. There are parts that are not obvious to me in the proofs and the main text. Also, some of the claims are under-justified or vague.\n\n* **Quality**: The quality of the paper is varying. I believe that it is a good piece of work in most parts. However there are some potential mistakes in some parts of the text (either fundamental, as in claims of this explaining a zero-mean preprocessing or batch normalization; or in some mathematical points).\n\n* **Novelty**: The paper seems novel. I have not seen a study under these conditions where the data is assumed to come from a mixture of Gaussians and the imbalance studied before.\n\n* **Reproducibility**: \\\n  *Theory*: I reproduced most of the proofs and could follow most of them. There were some parts that were not clear. \\\n  Overall, I believe that the amount of proofs is too long to have a proper review in the time available for a conference revision.\n\n  *Experiments*: There is no code to reproduce the experiments. Hence, I did not have time to replicate them. In this particular case, where a theoretical analysis is involved, having the code available is paramount, since the empirical match with the theory needs to be checked.",
            "summary_of_the_review": "This paper studies the convergence and generalization properties of ERM with GD on the expected population and expected per-group population risks for binary classification in logistic regression with the binary-cross entropy loss.\n\n They consider a rather artificial set-up where the input features data comes from a GMM with mixture representatives equal to the fraction of the observed group data and a balanced spectrum of the co-variances (i.e., $\\tau = 1$). Then, the labels come from a linear (or one hidden layer) network with a known number of units and unknown weights.\n\n Under this setup, they obtain convergence and generalization bounds that provide insights on which are the elements that help or don't help improve the training of unbalanced datasets. In particular, low norm means of each group and covariances close to the medium are key.\n\n Overall, the insights obtained from the paper are interesting but are very limited in the setting they consider. Unfortunately, as mentioned in the weaknesses, the paper does not successfully try to see if these insights carry on in practice in a convincing way. Also, one can not verify the available experiments with an available code. Moreover, there are some parts of the main text and proofs that seem to have some mistakes that need addressing or explaining. \n Therefore, for these reasons, I must incline towards rejection at the moment.\n \n On another note, not to the authors fault though, the paper and especially the proofs are very long and having a proper review and proof-check is complicated in the time window given for a conference, and maybe a journal would be a more appropriate venue for this work. \n\n\n**Some minor comments and nitpicks that did not impact the score of the review**\n\n* Many times you start sentences with a citation using citep, it should be citet.\n\n* In the introduction before the contribution. Please clarify: \"In these works, the input features are usually assumed...\"\n\n* In the notation, please, clarify \"The matrix spectral norm is $\\lVert Z \\rVert = \\delta_1(Z)\"$.\n\n* In Section 3. \"an unbalanced dataset\"\n\n* When you refer to works themselves and not the authors, you should youse citep instead of citet. For instance, right before Section 5.\n\n* The second and third paragraphs in Appendix A.1. are repeating themselves, don't they?\n\n* Before (12), you used one $\\times$ instead of $\\otimes$.\n\n* The $\\Gamma$-function $\\Gamma(\\Psi)$ should also contain a dependence on $W^*$.\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_MquM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_MquM"
        ]
    },
    {
        "id": "AfiTHjGdYp6",
        "original": null,
        "number": 5,
        "cdate": 1667230428589,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667230428589,
        "tmdate": 1667230428589,
        "tddate": null,
        "forum": "ig4E0Y11pX",
        "replyto": "ig4E0Y11pX",
        "invitation": "ICLR.cc/2023/Conference/Paper2930/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a Gaussian mixture model (GMM) for binary classification problems with imbalanced groups. The labels are generated according to a ground truth neural network. Authors mention a few observations according to the theoretical analysis and provided empirical evidence using CelebA and CIFAR datasets.",
            "strength_and_weaknesses": "Strength:\n\nThe paper is clearly written and overall easy to follow. I did not check the details of the proof but the theoretical results seem to be sound.\n\nWeakness:\n\nI think the theoretical model is quite restrictive and unnatural. The input distribution is assumed to follow a GMM, and the labels are generated using a true network, which is a strong assumption. The training algorithm involves a tensor initialization step which is not usually used in practice. These assumptions limit the significance of the theoretical contributions.\n\nThe empirical evidence supporting the theoretical results seems to be weak. All the test accuracy numbers in Figure 1(b) and Figure 6 are within 1~2 percentage points. The authors use a very small training dataset so it is likely that these numbers are all within confidence intervals, i.e., the results are not statistically significant. Moreover, the authors did not report error bars in their plots. Therefore, I think the practical implications of the results are also weak.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. There is some novelty in the theoretical model and analysis. The experiments seem to be simple so I think they should be reproducible.",
            "summary_of_the_review": "I think although this paper is well-written, there are some issues with its theoretical model and empirical evidence as mentioned above. Therefore, I don't think its contribution is significant enough to qualify as an ICLR paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_PYh3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2930/Reviewer_PYh3"
        ]
    }
]