[
    {
        "id": "_xuxec_HggN",
        "original": null,
        "number": 1,
        "cdate": 1666327101522,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666327101522,
        "tmdate": 1666327101522,
        "tddate": null,
        "forum": "Z4lOwCEJQ8Z",
        "replyto": "Z4lOwCEJQ8Z",
        "invitation": "ICLR.cc/2023/Conference/Paper4574/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper tries to tackle the problem of learning a generative model for sampling identically distributed but dependent data points from a distribution. Like previous normalizing flows, they try to learn a bijective map of data to an assumed latent space but unlike previous works which assume a diagonal Gaussian, they assume a correlation structure within the latent space as well. This makes the computation of probability density estimates in the latent space intractable in general but by restricting to specific classes of correlation structures the authors design a computation scheme that works for their case of dependent data. The authors test their method on toy and real world datasets.\n",
            "strength_and_weaknesses": "Strengths:\n\n- The authors chose an oft ignored area in generative modeling: that of dependent sampling which is interesting to analyze.\n- The paper is mostly well written and flows well.\n\nWeaknesses:\n- I am not quite sure why the specific covariance structures were chosen. As an aside do the authors think that adding a regularisation functional that minimises the norm of $\\rho$ will benefit the joint optimization strategy. In a way, would that choose the most decorrelating bijective map?\n- The authors should provide a compute budget difference (both time and memory) between the baseline and proposed methods. It seems the baseline is just a quadratic spline flow whereas the proposed method has additional parameters for the latent correlation structure. An aside: have the authors attempted to optimize the baseline latent mean and covariance?\n- The experiment section lacks many details:\n  -- In UKB: how is the relatedness used? Do you assume G is known?\n  -- In stocks: a few samples would help  understand what is log return referring to? Is it tracking per day movement of a pair? How is the temporal nature of stocks over time incorporated in the model?\n  -- ADNI: To my eyes all images in figure 3 look very similar. Does dependent modeling help in downstream tasks? If so, it would strengthen the author\u2019s case a lot if they could show something of that sort.\n\nOverall I feel the authors would greatly benefit in relegating some exp details to the appendix (around values in grid search, extra dataset details) and adding more results from each of the datasets. For ex. in UKB they say a quantile transformation makes statistical modeling hard. It would strongly benefit the authors to show that their way of modeling makes that easier.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The writing is mostly quite well done except for experiments (see weaknesses)\n- Adapting normalizing flows to dependent sampling is novel.\n- The authors provide code that will make it reproducible [I have not tried running their code.]\n",
            "summary_of_the_review": "I feel the authors need to strengthen the experiments section considerably in an attempt to justify that dependency modelling is actually useful [other than slight NLL benefits]. Other than that I think the paper is a good attempt at incorporating data dependency assumptions into the normalizing flow literature. I am willing to change my rating based on discussions on the forum.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4574/Reviewer_Krj4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4574/Reviewer_Krj4"
        ]
    },
    {
        "id": "1Xmq7iCaOb",
        "original": null,
        "number": 2,
        "cdate": 1666657125729,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657125729,
        "tmdate": 1666657125729,
        "tddate": null,
        "forum": "Z4lOwCEJQ8Z",
        "replyto": "Z4lOwCEJQ8Z",
        "invitation": "ICLR.cc/2023/Conference/Paper4574/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an extension of normalizing flows to non-IID data. This extension can be seen as the generalization of a Gaussian copula to vector-valued variables. The authors propose parameterizations of the resulting model and associated learning algorithms that yield efficient training.",
            "strength_and_weaknesses": "STRENGTHS:\n* The paper studies an important problem. Although the motivation describes it as non-IID data, this method also applies to high-dimensional correlated vectors, and may be useful for modeling IID data with interesting correlations among the features.\n* This work is novel, and to my knowledge this techniques of extending copula to higher-dimensional datasets is new.\n* The algorithmic ideas proposed in the work are novel, technically sound, and non-trivial. The level of technical depth required to derive the methods is significant.\n* The experiments are extensive and look at a wide range of interesting real-world problems.\n\nWEAKNESSES:\n* These are all relatively minor comments. I think it would have been interesting to describe the motivation and the various applications of the method early on (in the introduction). I only \"got\" the idea and why it's interesting after reading the whole paper. I didn't feel like the intro did a good job at selling the work.\n* While the experiments look at many domains, I didn't feel like the method achieved very strong improvements. Highlighting specific examples where this method really shines could make the paper stronger.\n* The paper mentions GWAS and ancestry-based confounding a lot, and it sounds like an interesting motivating application. It could help make the paper stronger to focus and commit on one application problem more deeply, use it as a clear consistent running example, show strong results on it, etc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. However, it could help make the paper stronger to focus and commit on one application problem more deeply, use it as a clear consistent running example, show strong results on it, etc. The results seem to be novel, high quality, and reproducible.",
            "summary_of_the_review": "I can see this paper being a useful addition to this line of literature, although I don't see it as completely ground-breaking, It could benefit from a more developed and more thoroughly studied use case.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4574/Reviewer_xwvU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4574/Reviewer_xwvU"
        ]
    },
    {
        "id": "n1Ip_vUYE2U",
        "original": null,
        "number": 3,
        "cdate": 1666796976001,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666796976001,
        "tmdate": 1666796976001,
        "tddate": null,
        "forum": "Z4lOwCEJQ8Z",
        "replyto": "Z4lOwCEJQ8Z",
        "invitation": "ICLR.cc/2023/Conference/Paper4574/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a modification to the standard normalizing flow training loss to account for certain dependencies in data. Specifically, the authors consider the special case where datapoints are dependent in latent space, but are transformed pointwise to the observed space. In addition, the authors specialize to the case where the correlation structure is either known up to a convex combination with the identity, or has a block-diagonal structure with known block assignment and constant positive correlation within blocks. The authors then propose training strategies for learning the parameters of these dependencies. The authors evaluate experimentally on simulated and real datasets, using NLL on held-out datasets as the core metric.",
            "strength_and_weaknesses": "Strengths:\n + Dependence between datapoints can be an important issue, especially in cases where one cares about being able to make joint or conditional predictions where the task depends on multiple datatpoints simultaneously.\n + The derivations are clearly explained. Although they are straightforward, they are not obfuscated to appear more complicated than they are.\n + Several practical issues in training are addressed.\n + Experiments explore degrees of freedom that were raised in the development of the method.\n\nWeaknesses:\n  - The primary weakness to me is that the method seems undermotivated. Although I know that there exist tasks where dependency between datapoints matter, it is not clear how accounting for dependencies between data translates to better solutions for important tasks. The authors raise several applications, including genomics, finance, and neuroimaging, but stop short of indicating what problems could be solved in these areas if only we modeled dependencies between examples better. NLL is not itself self-motivating; it measures the model's consistency with the data, but does not tell us what this extra fit buys in terms of performance on a real task. What is the unsolved problem in genomics, finance, or neuroimaging that can be solved with a good model of between-example dependence?\n - Along the same lines, when we consider dependent data, the actual prediction task needs to be specified with more care. What exactly is being measured by performance on the test data? Do we care about our having low expected prediction risk on the next datapoint? Low joint prediction risk on a set of $m$ new datapoints? Some kind of statistical inference? In the first case, dependency structure would not matter. In the second case, it might matter, but one would need to motivate why we care about $m$ datapoints simultaneously, and the composition of this test set would need to be motivated carefully (e.g., would this test set represent measurements from a completely new individual? from an individual we have observed before?). Also, we might need the test set to be composed of $n$ replicates of $m$ datapoints. In the third case, the form of statistical inference would need to be made clear (e.g., Are we doing anomaly prediction and constructing p-values? Are we using the normalizing flow to do covariance estimation for a downstream regression / GWAS task?).\n - For example, it is not clear what the goal of the task illustrated in Figure 1 is. If the goal were density estimation, it seems like the standard normalizing flow is doing the right thing. Meanwhile, it is not clear why we want the dependency-adjusted normalizing flow to match the distribution of data sampled independently. This comes back to the issue that when one has dependent data, the goals are not self-evident in the same way that they are when predicting with independent data. Goals and notions of bias need to be made explicit.\n - Given the note in the discussion that \"equal block sizes\" would be a case where the dependent data objective would produce no changes, did the authors consider the baseline of weighting datapoints by inverse block size? It seems that for some tasks this might address many issues and not require major modifications to training.\n - Many tricks for working with covariance matrices have been discussed before in literatures that do efficient ridge regression or work with Gaussian processes. I am not sure that the linear algebra tricks need to be rehashed in the main body of the paper, and this could make room for better discussion of tasks where dependence matters.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear in terms of the proposed approach. The specific tasks being solved require more motivation. To my knowledge, introducing dependency to normalizing flows novel, but I am not an expert in the normalizing flows literature. Many of the technical results are \"textbook\" decompositions of covariance matrices that are well-trod in the ridge regression and GP literatures (probably among others). The experiments seem easily reproducible.",
            "summary_of_the_review": "The proposed method seems fine in terms of specifying an objective that reflects certain forms of independence. However, the actual tasks being solved require far more explicit discussion, since there are many different tasks that one might try to solve in a dependent data context, and it is not clear which of these the method is designed to address. Because of this concern, the paper feels incomplete.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4574/Reviewer_gW39"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4574/Reviewer_gW39"
        ]
    }
]