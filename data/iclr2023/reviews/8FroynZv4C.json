[
    {
        "id": "IQxIYUiqUfa",
        "original": null,
        "number": 1,
        "cdate": 1666807842574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666807842574,
        "tmdate": 1666807842574,
        "tddate": null,
        "forum": "8FroynZv4C",
        "replyto": "8FroynZv4C",
        "invitation": "ICLR.cc/2023/Conference/Paper90/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies multi-agent general-sum Markov games with nonlinear function approximation. They focus on low-rank Markov games whose transition matrix admits a hidden low-rank structure. The authors provide a model-based and a model-free approach to learning the Nash equilibrium under such a transition model. The proposed algorithm achieves a sample complexity of $poly(H, d, A, 1/\\epsilon)$. They further consider Markov Games with a factorized transition structure.  The authors present the empirical results to verify their theoretical results.",
            "strength_and_weaknesses": "Strength:\n\n1. This paper provides novel theoretical results for representation learning in general-sum Markov games with a low-rank transition structure, which extends the previous single-agent MDP setting to the multi-agent general-sum Markov game setting. \n\n2. This paper presents a new result for low-rank representation learning in RL with a model-free learning algorithm, which is different from the existing model-based algorithm.\n\n2. This paper also provides an extension from the general transition case to the factored transition setting. \n\n4. The theoretical analysis is clear and this paper is well structured.\n\nWeaknesses:\n\n1. I feel that the proof of this paper largely depends on the recent work [Uehara et al., 2021]. Most of the proof can be directly modified from [Uehara et al., 2021] by setting the action a to $(a_1, a_2,...)$. To handle the proof with CCE, CE, and NE in the general sum game setting, one can also use the proof techniques in recent advances in general-sum Markov games. \n\n2. In Theorem 5.1, there is a $L^2$ in the exponential term, which does not appear in Theorem 4.1. In my understanding, this can be a problem in the upper bound. Can the authors provide a more detailed discussion on this? What is the major technical reason for obtaining this factor?\n\n3. The authors may need to further provide a detailed comparison between the results in Theorem 4.1 and Theorem 5.1, and show whether there are advantages of using the factored transition modeling.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the writing of this paper is clear to readers. The authors provide detailed proofs and codes in the Appendix which will help to reproduce the theoretical results and the experiment results. Part of the proof follows the techniques developed in the work [Uehara et al., 2021]. But the model-free algorithm and the associated analysis are novel to the topic of low-rank representation learning in RL.",
            "summary_of_the_review": "See the section of Strength And Weaknesses.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper90/Reviewer_oMoF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper90/Reviewer_oMoF"
        ]
    },
    {
        "id": "Fj96iYMWcp",
        "original": null,
        "number": 2,
        "cdate": 1667072592825,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667072592825,
        "tmdate": 1668916902213,
        "tddate": null,
        "forum": "8FroynZv4C",
        "replyto": "8FroynZv4C",
        "invitation": "ICLR.cc/2023/Conference/Paper90/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies representation learning for Markov games with low-rank structures. Under the low-rank assumption, the authors develop two representation learning methods to learn the underlying transitions: model-based and model-free representation learning and add the UCB bonus into the estimated Q-value functions for computing explorative policies. In theory, the authors show sample complexity PAC guarantees for two methods for obtaining near-optimal equilibria. Moreover, the authors study a special case of factored Markov games by showing a similar PAC guarantee while improving the exponential dependence on the number of players. Finally, the authors provide a series of experiments to compare the performance of the proposed method with DQN. ",
            "strength_and_weaknesses": "Strengths\n\n- The studied setting considers a general nonlinear representation of transition dynamics of Markov games. This setting is interesting and useful since it is often the case in practice that the model features are unknown, which brings the approach of representation learning to Markov games.\n\n- Under some assumptions on the model and the planning oracle, the provided representation learning methods enjoy polynomial sample complexities for learning equilibria, although it has an exponential dependence on the number of players. In the case of low-rank factored Markov games, the proposed method enjoys a polynomial dependence on the number of players.  \n\nWeaknesses\n\n- The application of representation learning to Markov games is more straightforward under assumptions on the realizability of model class or feature mapping and access to policy planning oracles. These assumptions could be too strong to hold, for example, exact solutions from policy planning oracles and real games do not satisfy your realizability assumptions. \n\n- The established PAC bounds are sub-optimal in terms of state/action space sizes, especially when we specialize it to the tabular case. It is important to compare it with some model-based MARL algorithms in the literature. Recovering the optimal sample complexity in the tabular case should be a sanity check of the efficiency of the proposed methods.\n\n -  The proposed methods are not decentralized, which prevents them to work in many real-world games that prohibit information sharing, especially policies. \n\n- The provided computational experiments only consider small-scale Markov games, which are not very sufficient to demonstrate the scalability of proposed methods under different low-rank structures. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The main results are clear except for some ambiguity. The motivation from real-world games seems to be a bit vague. It is important to point out real-world scenarios that fail existing provably sample-efficient algorithms for Markov games. In my view, the biggest reason is that we are mostly limited to the fully observable case, which could be a useful motivation for applying representation learning. The authors claim 'more computationally efficient solutions', which are not convincing without calibrating the computational complexity of proposed methods. In theory, it is less discussed the novelty in the statistical analysis of representation learning and the policy improvement analysis of Markov games. Some claims could be problematic, e.g., 'first algorithm that solves general-sum Markov games under function approximation', which can be controversial since the literature on Markov games with function approximation is evolving due to many recent works. \n\nOverall, the authors have shown the promising use of representation learning for Markov games. Viewing existing model-based MARL algorithms, representation learning methods are more or less expected to be applicable. Hence, the novelty of the proposed methods is questionable. In theory, sub-optimal PAC bounds seem to indicate a loose statistical analysis of representation learning, which I didn't check all proofs. However, it is worthy making more efforts to either tight bounds or demonstrate lower bounds. The authors also demonstrate the scalability of proposed methods without detailed evidence provided in experiments. Last but not least, computational efficiency is not discussed in the paper, which is a key factor when we consider how to apply them to real-world games.\n\nHere are some other questions for consideration:\n\n- What is MEL oracle?\n\n- What is 'more computationally efficient solutions' in Remark 2.1?\n\n- How do you solve policy planning oracles in Eqs. (2-4)? \n\n- Can you summarize the main idea of analysis of model-based and model-free cases? Why different dependence on problem parameters?\n\n- In experiments, can you increase the number of players to show the scalability in the number of players?\n",
            "summary_of_the_review": "The paper studies a class of Markov games with low-rank structure and provides representation learning methods for learning equilibria with PAC guarantees. Although the setting is useful, many assumptions in algorithm design and analysis need to be further justified. The novelty in the analysis needs to be discussed in detail. The experiments only consider small-scale Markov games that are not very sufficient to demonstrate the effectiveness of the proposed methods.\n\n\n============================\n\nPOST-REBUTTAL. Thank you for your response. I have read other reviews. I am inclined to recommend acceptance. The score has been increased.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper90/Reviewer_xksf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper90/Reviewer_xksf"
        ]
    },
    {
        "id": "k_KJ8y0w9_Y",
        "original": null,
        "number": 3,
        "cdate": 1667342016132,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667342016132,
        "tmdate": 1667342016132,
        "tddate": null,
        "forum": "8FroynZv4C",
        "replyto": "8FroynZv4C",
        "invitation": "ICLR.cc/2023/Conference/Paper90/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the low raw rank General sum Markov games, and provides two algorithms, model based and model free for finding the Nash equilibrium. In the model based algorithm, we sample state-action-next state tuples from the environment, and we estimate the transition probability directly from these tuples. In the model-free setting, however, we first estimate the feature vectors, and then we estimate the transition probability from these feature vectors. After estimating the transition probability, the algorithm performs the planning step, which is  conducted with a UCB style approach. \nAt the end, the authors provide the sample complexity of their algorithm in both model based and model free settings. \nFurthermore, the authors argue that their sample complexity for general MDP settings can be exponentially bad in terms of the number of agents. Hence, the authors provide the sample complexity of their algorithm for the Low-Rank Factored Markov Game. At the end the authors provide experimental results for proof-of-concept.",
            "strength_and_weaknesses": "Strength: \n- The authors study a general setting of Markov games fir linear MDPs. The problem formulation is novel and interesting, and the sample complexities are tight. \n- paper is well-written and easy to follow.\nWeakness:\n- In order to run the algorithm 1 and 2, we need to assume access to optimization oracles for calculating max and min-max-min. I was wondering if the authors can comment on the hardness of access to these oracles. \n- The definition of Low-Rank Factored Markov Game is slightly strong. I believe according to this definition, you are assuming that only L number of agents around each agent us can affect an agent. \n- Although the sample complexity is improved under Low-Rank Factored Markov Game setting, but the authors still require to save a large vector of parameters, in the size exponential in the number of agents. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The authors provide three definitions of optimal policy (policy we aim to evaluate), namely NE, CCE, and CE. Although the authors mention that NE \\subset CE \\subset CCE, the sample complexity of the algorithms to reach any of the optimal policies in the same. Can the authors comment on this? Is this sample complexity improvable?\n- What is the intuition behind the definition of CE? I would appreciate it if the authors could explain why such policy is of interest. \n- Can we reduce the memory requirement of the algorithm in the Low-Rank Factored Markov Game setting?\n- There are works such as\"Scalable Reinforcement Learning for Multiagent Networked Systems\" by Qu et al. where the correlation between the agents are controlled by a geometric factor. In particular, the agents which are far away from one another, have low correlation on each other. Can we adopt such notion instead of the restrictive Low-Rank Factored Markov Game?",
            "summary_of_the_review": "The paper is interesting and well written. It is comprehensive and studies a broad range of settings with providing tight sample complexity bounds for all of them. Beside some minor issues, I believe it is a good paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper90/Reviewer_H4wD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper90/Reviewer_H4wD"
        ]
    },
    {
        "id": "AZQjd7UkzU",
        "original": null,
        "number": 4,
        "cdate": 1667421010134,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667421010134,
        "tmdate": 1667421010134,
        "tddate": null,
        "forum": "8FroynZv4C",
        "replyto": "8FroynZv4C",
        "invitation": "ICLR.cc/2023/Conference/Paper90/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies general-sum Markov games where the probability transition admits a low-rank structure, and proposes algorithms that exploit the underlying structure via representation learning to efficiently learn the (approximate) equilibrium policy. Both model-based and model-free methods are provided and theoretically analyzed in a unified way. Quantitative regret bounds are provided for the general case and also the special case of factored Markov game, where for the latter case, the dependence on the number of players is shown to avoid the exponential scaling. Numerical experiments are also provided to show the advantage of the proposed approach over the DQN baseline.",
            "strength_and_weaknesses": "Strength:\n\n- Overall this paper is well-written and easy to follow. \n- The authors have clearly explained the goal of this paper and their contributions. The difference between the problem setting in the current paper and those in the existing literature is properly discussed.\n- The description of the proposed algorithm is clear and easy to understand.\n- The theoretical analysis is solid, and numerical simulations are provided in complement to the theoretical results.\n\nWeaknesses:\n\n- This is mainly a theory paper, and in the main context, the authors should discuss more the technical challenges and novelties compared with the existing works. It seems not clear to me what are the difficulties caused by the more general setting considered in this paper. A high-level description of the proof idea in Section 4 would also be appreciated. It would also be helpful to compare the results in Theorem 4.1 and 4.2 with existing results for Markov games when restricting the setting to two-player.\n- Some notations should be clarified in Section 2. For example, the definition of the function classes for the features should be provided. In line 6 of Algorithm 1, I guess $d_{P^*,h}^{\\pi^{(n-1)}}$ means the induced distribution on the state space and $U(\\mathcal{A})$ is the uniform distribution on the action space. Please define these notations (or let me know if I've missed anything).\n- When describing the components of the main algorithms, please provide references if a certain approach has been proposed in some related work. \n- It is remarked that the proposed algorithm is oracle-efficient, but it is unclear to me how to implement certain steps in Algorithm 1 and what are the computational costs. For example, line 6 of Algorithm 1 seems to require a certain simulator of the environment, instead of executing the policy to get the trajectory of an episode. I'd like the authors to provide more discussion about what oracles are needed and the corresponding implementations, especially since it is claimed that the proposed algorithm is scalable.\n- The organization of the experiment section is a bit weird. It seems that the description of the three different environments should be grouped together. Also, it should be clarified what Environment 1, 2 and 3 refer to.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is novel and original, and the results are solid. Overall this paper is well-written, while some notations should be clarified as mentioned above.",
            "summary_of_the_review": "I think this paper studies an important setting of multi-agent general-sum Markov games and the results are solid. This is a good submission as a complete work with both theoretical analysis and numerical simulations. Some aspects of the writing can be improved as I've discussed above.\n\n---\nOther minor problems:\n\n- In the fourth line under Algorithm 3, what's the meaning of 'M induced'?\n- In the last line of Remark 5.1, 'up to'.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper90/Reviewer_tHaH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper90/Reviewer_tHaH"
        ]
    }
]