[
    {
        "id": "0ICOgu-hgj",
        "original": null,
        "number": 1,
        "cdate": 1666678567010,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678567010,
        "tmdate": 1666734999050,
        "tddate": null,
        "forum": "sCrnllCtjoE",
        "replyto": "sCrnllCtjoE",
        "invitation": "ICLR.cc/2023/Conference/Paper2893/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduced a multi-scale forecasting training module to improve the forecasting performance of existing transformer models. The basic idea is laying on training scaling-shift aware fine-grind forecasting model. To achieve this goal, the authors introduce an across scale normalization module and a multi-resolution forecasting training structure. The result shows that the proposed module could improve the forecasting performance with various transformer deep learning models without adding additional burdens.\n",
            "strength_and_weaknesses": "Strength:\n1. the paper is well written and the technique is simple yet intuitive\n2. the contribution can contribute any transformer based forecasting model\n3. Extensive experiments are conducted to justify the claim\n\nWeakness\n1. Introducing Adaptive loss seems unrelated to the major contribution of this paper\n2. The multiscale forecasting is so-what similar to U-Net structure",
            "clarity,_quality,_novelty_and_reproducibility": "1. My major concern is the similarity between the proposed multi-scale forecasting structure vs. the existing U-Net based forecasting approach (e.g. [1]) which is not cited. The technique differents between the proposed method and such types of approaches should be discussed.\n\n2. Introducing adaptive loss seems unrelated to the major contribution of this paper. I personally think Table 6 in supplemental material is more important to justify the paper's claim.\n\n3.  Eq. 7 and 8 normalize the mean. Why not normalize the standard deviation? Any reason?\n\n4. Why solely using the proposed normalization cannot increase the performance of Autoformer? Any reason?\n\n[1] Yformer: U-Net Inspired Transformer Architecture for Far Horizon Time Series Forecasting, ECML-PKDD\n\n\n\n",
            "summary_of_the_review": "Overall, this is an interesting paper, and the proposed module can be widely used in existing time series forecasting framework. Therefore, I believe this is an accept paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2893/Reviewer_V4CM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2893/Reviewer_V4CM"
        ]
    },
    {
        "id": "N2O33ZTqgc",
        "original": null,
        "number": 2,
        "cdate": 1666693432259,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666693432259,
        "tmdate": 1666693432259,
        "tddate": null,
        "forum": "sCrnllCtjoE",
        "replyto": "sCrnllCtjoE",
        "invitation": "ICLR.cc/2023/Conference/Paper2893/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a transformer-based framework for time-series forecasting for multi-scale frequency components. The solution is inspired by two recent state-of-the-art transformer architectures used as backbones by iteratively refining a forecasted time series at increasingly fine-grained scales and introducing a normalization scheme that minimizes distribution shifts between scales.",
            "strength_and_weaknesses": "Strengths:\n* The novelty of the paper is interesting,\n* The idea of time-series forecasting for multiple scales, by minimizing the distribution shifts between scales and windows using cross-scale normalization, is an interesting contribution to the different domains: Neuroscience, Stock Market, Weather Forecasting, etc.\n* The paper is well written and is very thorough with the problem formulations (Both Architecture and its components are well explained with mathematical proofs).\n\nWeaknesses:\n* The Informer-MSA results are much closer to the actual time series compared to other methods in Figure 5; however, Fedformer and Autoformer display lower MSE and MAE in Tables 2 and 3. It is not clear to me why these models are performing poorly in the qualitative analysis compared to Informer-MSA.\n* Figure 4 requires standard error bars for better comparison across the baseline models. Also, the authors could perform a statistical significance test to compare the results. \n* While the authors focused more on comparing the model performances, whether authors have tried if we have frozen the layer of the Transformer and only train at the final layer? How do the frozen model effects approximate dynamic time-series forecasting?\n* It will be interesting if authors can present the layer-wise representations across the models and how these models learn the temporal aspect while forecasting the time series.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is well written and is very thorough with the problem formulations.\n* Most of the experiments are performed on standard time-series datasets. The framework needs to be validated on datasets like samples with chaotic time series, seasonality, and the time-series changes abruptly based on external factors.\n* Since the authors released the Github code; hence all the experiments are reproducible.",
            "summary_of_the_review": "Overall, the authors propose a novel framework Transformer-based model for time series forecasting for multiple scales. However, the paper requires much discussion on why one model is superior to the other and interprets the layer-wise representations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2893/Reviewer_VvHJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2893/Reviewer_VvHJ"
        ]
    },
    {
        "id": "BJjlQ72p-og",
        "original": null,
        "number": 3,
        "cdate": 1666702355195,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666702355195,
        "tmdate": 1666702355195,
        "tddate": null,
        "forum": "sCrnllCtjoE",
        "replyto": "sCrnllCtjoE",
        "invitation": "ICLR.cc/2023/Conference/Paper2893/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a general multi-scale framework for time series forecasting.",
            "strength_and_weaknesses": "Strengths\n1. SOTA methods are compared in the paper. \n2. Comprehensive experiments are conducted.\n\nWeaknesses:\n1. Technical contribution is incremental.",
            "clarity,_quality,_novelty_and_reproducibility": "The technical contribution of this paper is incremental. Detailed implementation details are shared. The overall presentation could be improved with more clarification on the model description and training process. ",
            "summary_of_the_review": "In this paper, the authors proposed a  multi-scale framework that can apply directly to the existing time series forecasting framework. Detailed comments are listed below:\n1. The technical contribution of this paper is a little bit incremental. The idea of leveraging multiple time resolutions of time series is not something new. The major contribution comes from cross-scale normalization.\n2. How is the model trained with learnable parameters in the loss function? Also, how are alpha and c learned during the training?\n3. Based on Figure 4, it seems that the adaptive loss doesn't have significant benefits until the multi-scale framework is used. Can the authors provide a move analysis and discussion?\n4. It's observed that, in many cases (Tables 2 and 3), a single-scale model with mean normalization performs significantly better than the multi-scale version without normalization. Does that mean a simple mean normalization is more important than the multi-scale information in the time series? Can the authors provide more insight analysis?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2893/Reviewer_N6mC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2893/Reviewer_N6mC"
        ]
    },
    {
        "id": "h0TEf8zCPKL",
        "original": null,
        "number": 4,
        "cdate": 1666717013004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717013004,
        "tmdate": 1666718128937,
        "tddate": null,
        "forum": "sCrnllCtjoE",
        "replyto": "sCrnllCtjoE",
        "invitation": "ICLR.cc/2023/Conference/Paper2893/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a Transformer-based framework that iteratively forecast time series at different scales with shared weights. In particular, it proposes normalizing the downsampled time series to avoid distribution shift and using adaptive loss to deal with outliers. Experiments show the multi-scale framework can significantly improve the performance of various transformer-based forecasting models.",
            "strength_and_weaknesses": "Strengths:\n+ The paper is well-organized and the essential ideas are easy to follow.\n+ Extensive experiments are conducted with impressive improvement in main results.\n\nWeakness\n- The introduction of multi-scale structure is not well motivated. It would be nicer if a good motivating example of using multi-scale forecasting is provided other than the ablation study.\n- The reason of using average only in cross-scale normalization is not well justified. Why would it be able to address distribution shift without standardization as well?\n- The adaptive loss is introduced for outliers but there is no such demonstration. And the improvement of adaptive loss seems marginal in the ablation study (figure 4)",
            "clarity,_quality,_novelty_and_reproducibility": "The methodology is presented clearly and the empirical studies are thorough, but the motivation is not quite clear and the novelty is marginal. The work is expected to be easy to be reproduce as source code is provided.",
            "summary_of_the_review": "Overall I believe this paper make incremental contribution to forecasting community with a multi-scale framework. The empirical results are inspiring, however, for future research.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2893/Reviewer_UakB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2893/Reviewer_UakB"
        ]
    }
]