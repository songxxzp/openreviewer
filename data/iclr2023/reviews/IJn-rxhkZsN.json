[
    {
        "id": "MJ-kOuPF1H",
        "original": null,
        "number": 1,
        "cdate": 1666648382693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648382693,
        "tmdate": 1666648382693,
        "tddate": null,
        "forum": "IJn-rxhkZsN",
        "replyto": "IJn-rxhkZsN",
        "invitation": "ICLR.cc/2023/Conference/Paper5397/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper applies VisionTransformer for Multivariate Time-Series Classification (VitMTSC) model that learns latent features from raw time-series data for classification tasks and is applicable to large-scale time-series data with millions of data samples of variable lengths. Their results show that it's can obtain near state-of-the-art results on public benchmark and obtain significant improvements on real-world commercial datasets.",
            "strength_and_weaknesses": "Strength\n* The idea is quite simple and cast time series as an \"Image\" and apply ViT to time series classification tasks.\n* Results on real-word commercial datasets are significantly better than baselines.\n\nWeaknesses\n* The idea is hardly novel and it's more like a pure application of Vision Transformer without any deep insight.\n* It's unclear why VitMTSC is significantly better than state-of-the-art baselines on commercial datasets. Authors don't discuss these insights in their papers and it's unclear when practitioners should utilize vision transformer for time series classification.\n* The accuracy is pretty high on PenDigits, SpokenArabicDigits and CharacterTrajectories. Is it due to data imbalance? Could authors provide other metric, e.g. F1?\n* This paper mainly explores binary classifications and multiple-class classification, however, is not fully considered. ",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is quite simple and obtain promising results on real-world commercial datasets. However, it's unclear why VitMTSC can outperform state-of-the-art methods in a large margin in real-world commercial datasets while slightly left behind on public benchmarks. Without further insights, it's hardly novel in term of both technical and empirical perspectives. In addition, code link seems to be empty although authors claim that it's publicly available.",
            "summary_of_the_review": "This paper applies vision transformer in Multivariate Time-Series Classification problems. Results show that it can outperform state-of-the-art on commercial datasets while slightly left behind on public benchmark datasets. It's unclear why this performance gap exist, making this paper less valuable. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5397/Reviewer_ivET"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5397/Reviewer_ivET"
        ]
    },
    {
        "id": "qYlL1mk2n-J",
        "original": null,
        "number": 2,
        "cdate": 1666674284740,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674284740,
        "tmdate": 1666674284740,
        "tddate": null,
        "forum": "IJn-rxhkZsN",
        "replyto": "IJn-rxhkZsN",
        "invitation": "ICLR.cc/2023/Conference/Paper5397/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed the `VitMTSC` model, a vision Transformer model that learns latent features from raw time-series data for classification tasks, and could be applied on large-scale time series data with variable lengths. The model can reach comparable performance on UEA datasets against previous state-of-the-art methods and outperform them by a large margin on real-world dataset.\n\nThe contributions of this paper are from proposing a VitTransformer based approach to model long-range contextual relationship in MTS data, and demonstrate its superior performance on real-world application data.\n",
            "strength_and_weaknesses": "This paper provides an interesting idea of using a Vision Transformer for a Multivariate Time-Series Classification problem. The strengths of the paper are:\n1. The figures are well-made and improve the clarity of the model description.\n2. Details on tokenization, feature encoding, padding and truncation are discussed in the paper, improving the paper's reproducibility.\n3. This paper did some interesting ablation study on layer normalization that change its position from after `MSA/FNN block + Residual connection` which is also called `POSTNORM` to before `MSA/FFN block` which is also called `PRENORM`. Experiments show it can improve applications' performance greatly on real-world dataset.\n4. The experiments section includes some real world dataset, including offline and online tests, which is interesting to see.\n\nOn the other hand, the weaknesses of the paper are:\n\n1. Even though the author(s) claimed this paper is based on Vision Transformer, there's no concept of \"patch\" applied in this feature extraction stage. The input was naturally given in different time stamps and features were extracted in a one-by-one fashion. After tokenization and feature encoding, there's only a linear projection applied onto each of the feature vectors. I tend to consider this process as some kind of feature engineering for the input of a Transformer. Can you please further explain how this is related to vision transformer's patch concept and whether the novelty comes from this process?\n2. Covariates (time features in this paper) like month, day, hour have proven to be very useful in time series, while using them as position encoding could help, have you tried combining it into the original feature vector before linear projection and combine with learnable position encoding?\n3. In the UEA dataset, the performance of the proposed model looks a bit worse than state-of-the-art models, but performance a lot better then these models on real world dataset. There seems to be not enough case study and analysis to further understand where the difference comes from and how we can mitigate the gap. I suggest the author(s) to provide more insights on the cause of this difference.\n4. The bolded numbers in tables are a bit confusing, for example in Table 3, VitMTSC's validation set results were bolded but other results are from the test set. In Table 5, some of the results in the same ablation study groups are bolded but some are not, e.g., the `padding mask` group where the dataset is `Large`. Can you please add some explanation on the bolded number meaning?\n5. The author(s) claimed to have `experimented with different aggregation strategies` but there were no conclusions or results mentioned. Can you please show the conclusion or performance with different aggregation strategies' impact?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and reproducibility of this paper is in OK level, most of the notations can match up, and the figures are informative. Details of this model are well discussed, including various aspects like feature encoding, padding, position embedding. The Algorithm in page 6 looks a bit confusing because there are some notations unintroduced. Experiment section includes both UEA dataset and real world application, where there's not enough in-depth analysis on the gap of analysis between them. The most concerning point comes from the design of this architecture, where the concept \"patch\" of Vision Transformer was nowhere to be seen. It seems the whole input processing and embedding process is just a feature engineering for Transformer input. The use of `PRENORM` seems interesting but it's already proposed in GPT-2 (Radford et al., 2019) and ViT (Dosovitskiy et al., 2020). Therefore, the paper seems to lack novelty and hard to find impact other than the performance on the real-world application dataset, which I'm not sure if it's gonna be open sourced. Therefore, the quality and novelty of this paper is low.\n",
            "summary_of_the_review": "To summarize, this paper introduced a Vision Transformer based model called `VitMTSC` to improve multivariate time series classification. However, the novelty in this paper seems to be limited and the results have a huge gap between open-source UEA dataset and real-world application dataset. Even though the performance on real-world dataset looks great and the model can handle large scale dataset, the engineering part seems out of the scope of this paper and there's a lot more to explain on the experiment results. Therefore, I recommend this paper to be rejected.\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5397/Reviewer_fdnF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5397/Reviewer_fdnF"
        ]
    },
    {
        "id": "XNcDlaS13o",
        "original": null,
        "number": 3,
        "cdate": 1666968838174,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666968838174,
        "tmdate": 1666968838174,
        "tddate": null,
        "forum": "IJn-rxhkZsN",
        "replyto": "IJn-rxhkZsN",
        "invitation": "ICLR.cc/2023/Conference/Paper5397/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the application of ViT models for the task of timeseries classification. ",
            "strength_and_weaknesses": "S\n\n- extensive experiments with multiple datasets and setups\n- straightforward application of ViT to timeseries\n\nW\n\n- lack of discussion wrt better results in baselines (TST, Rocket)\n- limited novelty compared to existing models (ViT)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, of high quality, and reproducible. The novelty is limited considering that it's applying an existing model.",
            "summary_of_the_review": "It's unclear whether this model improves against other sota models in the literature based on Table 3. It seems that baselines perform equally well or even better here. The proposed model seems to perform better only on the internal dataset (private?) which makes the comparison with public benchmarks really difficult. I would urge the authors to expand more on the public benchmark experimentation and discuss (and expand) Table 3.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5397/Reviewer_YY3h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5397/Reviewer_YY3h"
        ]
    }
]