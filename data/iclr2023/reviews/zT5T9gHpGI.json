[
    {
        "id": "m0r5xkpVmSb",
        "original": null,
        "number": 1,
        "cdate": 1666542307602,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542307602,
        "tmdate": 1666542307602,
        "tddate": null,
        "forum": "zT5T9gHpGI",
        "replyto": "zT5T9gHpGI",
        "invitation": "ICLR.cc/2023/Conference/Paper641/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors address the age old offline RL problem of counterfactual prediction, i.e. what would have happened if an unobserved action would have happened at an unobserved state. To do this, they use a weighted empirical risk minimization framework, which is extended with an adversarial framework to learn a model under a worst-case assumption. \n\nThey verify that their model makes accurate counterfactual predictions on unseen data by evaluating on D4RL and a propriety real-world food delivery task. ",
            "strength_and_weaknesses": "Strengths:\n- addresses an important problem\n- applies model based RL to offline RL, a promising avenue of research\n- applies adversarial worst-case ideas, which should in theory result in better \n\nMajor Concerns:\n\nClarity: It took a lot of reading and re-reading for me to understand what this paper is about. The main thing that made it hard for me to understand is that it's sold by the authors as a RL paper, when in fact their method is more a Domain Adaptation method, where they take data from one domain (observations of a transition function under one policy) and try to make predictions in another domain (behavior of a transition function under another policy). This is confusing, since what one is actually interested in is off policy evaluation, knowing how well a an new policy will work based on observation of an old policy.\n\nQuality: I do not thing the authors claims are supported by experimental evidence. The problem with the presented experimental evidence is three-fold: First, they don't compare to other state of the art domain adaptation methods, even though it's a domain adaption paper. Second, their experiments only talk about prediction accuracy of the transition function. It's totally possible to predict accurately enough an environment, but be very inaccurate at predicting a policy's performance in the environment. Last, the authors claim their method's \"policy significantly improves performance\" but only show it for a non-reproducible setup. In RL, the whole point is to improve a policies performance, and the authors can't demonstrate that their method indeed improves performance.\n\nTo be frank, since the authors were unable, in my view, to support their claims with evidence I didn't carefully study every mathematical detail of their method.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Reproducibility: As an experiment, the authors deploy their policy in a \"real-world large-scale food-delivery platform\", which is clearly not reproducible. ",
            "summary_of_the_review": "The authors fail to show their method is relevant for reinforcement learning, as the method merely learns a prediction of the transition function, but they don't show this leads to better policies or better off-policy evaluation.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper641/Reviewer_rQ79"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper641/Reviewer_rQ79"
        ]
    },
    {
        "id": "59hAmE1V70",
        "original": null,
        "number": 2,
        "cdate": 1666634784879,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634784879,
        "tmdate": 1666634784879,
        "tddate": null,
        "forum": "zT5T9gHpGI",
        "replyto": "zT5T9gHpGI",
        "invitation": "ICLR.cc/2023/Conference/Paper641/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces an approach for learning a model of the environment for model-based RL (MBRL) based on counterfactual data. This enables more stable model performance outside of the training distribution. The added generalization is driven by adversarial training to mitigate selection bias influencing the learned relationships between observations and subsequent actions.",
            "strength_and_weaknesses": "**Strengths**\n\nThis is a very complete and thoroughly written paper. The extensive efforts made by the authors to clearly justify the steps they took in the development of the proposed GALILEO is appreciated. I do wish there was a more detailed \"related work\" section with more concrete discussion about how the proposed approach builds or differs from prior work. However, this concern is mitigated mostly by how well cited the rest of the paper is. Each addition and concept throughout the paper is properly attributed.\n\nI found the step-by-step derivation of GALILEO through Section 4 to be really easy to follow. The writing was very clear in the development of the final use of AWRM. This was capped off with a great discussion about how the theoretical components of the method are implemented (Section 4.3). Unfortunately, this level of detail (and accessibility) was not carried through the experiments (more on that below). \n\nThe extent of the experiments provide some clear signals that the proposed method carries some \"general\" significance outside of the simple synthetic domains many papers use as a proof of concept. It's clear that the models learned with GALILEO are more accurate than those trained with other approaches.\n\n**Weaknesses**\n\nTo quickly jump to the empirical justification. It's disappointing that the improved models learned with GALILEO aren't shown to improve the overall task performance of the individual domains the models were learned to assist with. I can understand that this is not the major focus of the paper but the conceptual justification for needing generalizable models for MBRL is to provide more robust policy learning. As such the reported results feel only partial in nature. It would be great to see what the knock-on effects of an improved model are when learning a policy in the environment. Is there an improved nature of sample efficiency? How about the performance in more diverse test scenarios?\n\nIt's also disappointing that there aren't very thorough baseline comparisons to contemporary MBRL approaches in the accuracy of the learned models. Aside from the simple synthetic tasks, there are not really any baseline comparisons with \"competing\" approaches. I suppose that the SL baseline is better than nothing but the experiments overlook contributions made by the prior work in learning adequate models. Without a discussion about how these approaches devolve into SL, it's hard to place any significance on the experiments.\n\nAt times the discussion is cast in very formal language that distracts from the actual usage (or motivation) of the concepts. It's technically correct to cast \"error\" as \"risk\" (especially since the paper pursues \"risk minimization\") but it comes across as unnecessarily formal in places. Especially when MSE is used as a metric... This isn't a major concern but it does belie some aspects of where the writing in the paper could be improved to facilitate greater clarity. \n\nThe \"real world\" example is too vague in the initial sections of the paper. There is a lot of weight placed on the contributions of GALILEO working on a real domain but there's not sufficient framing or discussion about why this experiment works or is necessarily best solved by GALILEO (esp. when not compared to relevant baselines). \n\nThere are missing references to prior work using counterfactuals to improve environment modeling. In particular, I would interested to hear from the authors why they omit contributions from prior work such as Sontakke, et al (ICML 2021).\n\n```Sontakke, S. A., Mehrjou, A., Itti, L., & Sch\u00f6lkopf, B. (2021, July). Causal curiosity: RL agents discovering self-supervised experiments for causal representation learning. In International Conference on Machine Learning (pp. 9848-9858). PMLR.```\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity*: Great for the most part (detailed concerns above)\n\n*Quality*: Very Good\n\n*Novelty*: There are some minor concerns about significance since the experimental results barely touch on how the improved models contribute to better policies (and only for the \"real world\" example). The lack of discussion about relevant causal environment modeling papers is of some concern.\n\n*Reproducibility*: There is an extensive section in the Appendix that outlines how the methods were implemented with corresponding pseudocode. \n",
            "summary_of_the_review": "This is a good paper and one that warrants publication. I however reserve my enthusiastic recommendation because there are points where the discussion could be improved, specifically around the empirical justifications used to validate the development of GALILEO.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper641/Reviewer_j5p5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper641/Reviewer_j5p5"
        ]
    },
    {
        "id": "YO2JIEvW5X",
        "original": null,
        "number": 3,
        "cdate": 1666648078438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648078438,
        "tmdate": 1666648078438,
        "tddate": null,
        "forum": "zT5T9gHpGI",
        "replyto": "zT5T9gHpGI",
        "invitation": "ICLR.cc/2023/Conference/Paper641/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the model learning problem in sequential decision making. Traditional methods may learn a model that fails under counterfactual samples (unseen in offline training dataset). To imporve the model's generalizability, the authors introduce adversarial weighted empirical risk minimization (AWRM) which learns a model with adversarially selected test-time policies. Although the original formulation of AWRM is intractable, the paper proposes a surrogate objective with several approximations. The effectiveness of the proposed method is verified by experiments in a synthetic environment, 3 MuJoCo environments, as well as a real-world food-delivery platform.",
            "strength_and_weaknesses": "Strengths:\n\n1. The main idea of the paper are well-motivated and interesting.\n2. I am not sure how novel and original the relaxation of the adversarial objective is since I am not familiar with the related work. But the technical insights are sound.\n3. The paper is in general well-written with much details well-organized in the appendix.\n4. The experimental results are convincing in terms of showing the major messages.\n\nWeaknesses:\n\n1. It is not clear how the entire algorithm works and how many networks are learned in total. A high-level summarization or even an architecture figure would be helpful.\n2. Many implementation details are omitted in the main paper. How is $H_{M^*}(x,a)$ implemented?\n3. The presentation could be improved. The notations are sometimes not very clear. Are $\\kappa$ and $\\hat{\\mu}$ (in appendix) the same? The notation $a$ is used several times for different meanings. Also, the layout is too tight. It seems that a negative vspace is used very aggresively.\n4. The experiments only focus on the model learning performance in the form of prediction errors. However, it would be better to demonstrate how such a model can help model-based RL (learn a policy from the fitted model).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear. The idea and technical contributions look novel to me.",
            "summary_of_the_review": "This paper addresses the generalization challenge of model learning in sequential decision making problems. The proposed method is reasonable and sound, with theoretical analysis and empirical evidence. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper641/Reviewer_bcJS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper641/Reviewer_bcJS"
        ]
    }
]