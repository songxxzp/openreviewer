[
    {
        "id": "1wEp-gEJOeL",
        "original": null,
        "number": 1,
        "cdate": 1666459505188,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666459505188,
        "tmdate": 1666459505188,
        "tddate": null,
        "forum": "m7rFrsO0YWb",
        "replyto": "m7rFrsO0YWb",
        "invitation": "ICLR.cc/2023/Conference/Paper264/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a \u2018neural-symbolic recursive machine\u2019 (NSR) to learn compositional rules from limited data that can be applied to unseen combinations in various domains. SotA is reported to be achieved on SCAN, PCFG, and HINT, which suggests at good domain-generalizability.",
            "strength_and_weaknesses": "Strengths\n\n- NSR is evaluated on three benchmarks from different domains, which demonstrates a good degree of generalization, and appears to achieve SotA on all three.\n- The related work of Sec 4 is fairly comprehensive in topic, and clarity.\n- The visualization of Fig 3 is fairly convincing\n\nWeaknesses\n\n- Although this is relatively minor, there are more recent alternatives to Chen & Manning (2014) for dependency parsing, including Mrini et al (2020) and a few based on deep biaffine attention.\n- Also relatively minor, but it would be preferable to explore the space of program induction to a greater degree than in Sec 2.2, as this is a fairly active area of research.\n- The baselines are adequate, but other more recent permutations of some of these exist (and it it probably not necessary to include seq2seq at this point). For example, is it possible to compare NSR against the approach in Onta\u00f1\u00f3n et al (2022; arXiv:2108.04378v2), whom are cited?\n\nMinor\n\n- The forced correspondence to neuroscience always seems a bit naive, and may be removed? E.g., it\u2019s easy to find studies that show that syntax and semantics are linked anatomically in a way that may counter Miller et al (2001), and connectionism vs symbolism is somewhat of a forced dichotomy.\n- Please do another round of editing, as some grammatical errors exist throughout (e.g., \u201c\u2026 and are thus difficult to transfer such capability \u2026\u201d)\n- There should be a citation on the \u201chave been shown\u201d claim in Sec 2.3\n- Additional detail should be added to the lemmas of Sec 2.4. E.g., the broad claim of the existence of a neural network in Lemma 1 is not in doubt but either a citation should be added to \u2018provably\u2019 or the abstracted proof should be added, as there is already a precedent for repeating previously established facts or approaches, from Sec 2.3\n- It seems that \u2018Theorem 3\u2019 is meant to be \u2018Lemma 3\u2019 but, regardless, further explanation is advised.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- the work is very clearly described, but with a few gaps. The deduction-abduction algorithm specifically applied here provides much of the novelty.\n- With regards to reproducibility, standard tools like DreamCoder are used in some cases, and other approaches are fairly clear or very well-established (Sec 2.3). The appendices provide additional detail.",
            "summary_of_the_review": "\n- This is a valuable extension of other work in this area, although it is only somewhat hampered by some vagueness, and potential strawmen of baselines, as opposed to direct comparisons with specific work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper264/Reviewer_oDdz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper264/Reviewer_oDdz"
        ]
    },
    {
        "id": "YtyNfWyl40H",
        "original": null,
        "number": 2,
        "cdate": 1666640534217,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640534217,
        "tmdate": 1666640534217,
        "tddate": null,
        "forum": "m7rFrsO0YWb",
        "replyto": "m7rFrsO0YWb",
        "invitation": "ICLR.cc/2023/Conference/Paper264/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new model -- Neural-Symbolic Recursive Machine (NSR), that uses a Grounded Symbol System as its core mechanism. The model includes three components: a perception module, a parsing module, and a program induction module. The three modules are trained in an end-to-end fashion without intermediate supervision. The system benefits from the inductive bias of equivariance and recursiveness. It shows a strong compositional generalization ability on several standard benchmarks, including SCAN, PCFG, and HINT. ",
            "strength_and_weaknesses": "Strength:\n1. The proposed NSR achieve impressive generalization results on several datasets.\n2. The three-module design is intuitive and general enough to be applied to different tasks and modalities.\n3. Experiments on SCAN and PCFG show that NSR has good interpretability.\n\nWeaknesses\n1. The NSR is only tested on synthetic tasks. The limitation of applying NSR in real-world tasks is not discussed either.\n2. The paper is difficult to follow, important diagrams and figures are put in the appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "The method section is difficult to follow, important figures and algorithms that are essential for understanding the algorithm are put in the appendix. This also makes the model difficult to reproduce. ",
            "summary_of_the_review": "Overall, the paper proposed an interesting new framework for achieving compositional generalization. Experiment results confirm the effectiveness of the model in synthetic tasks. However, the proposed method is fairly complicated, which shed doubt on whether the method can be generalized to real-world tasks. The author didn't provide any proof or discussion on this question.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper264/Reviewer_j25h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper264/Reviewer_j25h"
        ]
    },
    {
        "id": "L9bG6fJdBn3",
        "original": null,
        "number": 3,
        "cdate": 1666848206349,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666848206349,
        "tmdate": 1669987110986,
        "tddate": null,
        "forum": "m7rFrsO0YWb",
        "replyto": "m7rFrsO0YWb",
        "invitation": "ICLR.cc/2023/Conference/Paper264/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes Neural-Symbolic Recursive Machine (NSR) as a means for neural networks to learn compositional rules through 3 modules, one for processing perception, syntactic parsing and semantic reasoning. These modules made up the Grounded Symbol System (GSS) and are jointly learned through a deduction-abduction algorithm. The authors also showed that the NSR modules confer equivariance and recursiveness to the network\u2019s representations. The experiments showed that NSR outperforms baselines on semantic parsing, string manipulation and arithmetic reasoning tasks.",
            "strength_and_weaknesses": "Strength:\nThe proposed idea is sound and shows superior performance vs some baselines.\nTheoretical analysis is done to show proposed NSR confers equivariance and recursiveness.\n\nWeaknesses:\nLack of comparison versus more recent baselines such as Ontan\u00f3n et al. (2022) \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and well organized. For a reader who is not very familiar with this field, it is not easy to tell how the proposed NSR is novel and different from existing work. This would be addressed by better contrasting NSR against prior work in the \u201cRelated Work\u201d section. The proposed design of NSR using 3 modules to process perception, syntactic parsing and semantic reasoning to confer systemic generalization is intuitive and the experimental results show superior performance versus baselines. The proposed deduction-abduction algorithm to address the model\u2019s training is sound and design choices are also well-explained.",
            "summary_of_the_review": "Overall, the paper proposed a sound and intuitive approach for a model\u2019s systemic generalization which is a timely contribution given how important generalization is for deep learning models. One area for improvement is more comparison against more recent baselines and prior work from the \u2018data augmentation\u2019 and \u2018symbolic scaffolding\u2019 categories. Though I am not an expert in this field, the contributions seem substantial enough for me to lean toward an accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper264/Reviewer_kopJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper264/Reviewer_kopJ"
        ]
    },
    {
        "id": "xJVfuaWCHp",
        "original": null,
        "number": 4,
        "cdate": 1667650586374,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667650586374,
        "tmdate": 1667651021520,
        "tddate": null,
        "forum": "m7rFrsO0YWb",
        "replyto": "m7rFrsO0YWb",
        "invitation": "ICLR.cc/2023/Conference/Paper264/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a new approach composed of multiple components for systematic generalization on language tasks. It achieves good results on multiple datasets including SCAN, PCFG, and a task for arithmetic reasoning. ",
            "strength_and_weaknesses": "**Strengths**: \n* **Domain**: the paper explores an important and growing domain of systematic generalization.\n* **Experiments and results**: The paper presents experiments in multiple complementary datasets covering parsing, semantic reasoning, compositional generalization and arithmetics. It achieves good results across multiple tasks, and compares to multiple suitable baselines. The descriptions in the experiments section of the tasks and the baselines are also clear, well-structured and easy to follow. \n* **Modeling section**: The discussion in the modeling section is interesting and gives solid motivation for the approach presented in the paper, and the section is structured in a good and clear way. The model is also discussed from a theoretical perspective, and formally defines its properties (equivariance and recursiveness). \n* **Related work, background and context**: the paper does a good job contextualizing the approach, providing the necessary general background, and covering the relevant related fields. I recommend moving the related work section to an earlier point in the paper (before the modeling section). \n    \n**Weaknesses**:\n* **Related work comparison to specific recent approaches**: More details on how the new approach compares to recent models on the explored tasks would be useful.\n* **Complicated approach**: The approach is a bit complicated and consists of multiple different modules that are responsible for different aspects of the problem. We know that models such as transformers do amazingly well on language tasks being more uniform, and so I recommend examining whether the model could be simplified without negatively and potentially also positively impacting its performance. \n* **Undiferentaible**: The NSR model isn\u2019t end-to-end differentiable, making training more challenging. Having multiple units that interact through hard predictions (without a gradient) could also lead to cascade errors, where early mistakes in early modules prevent the following ones to give the right predictions.I recommend exploring ways to make it differentiable, either by relaxing some of the constraints or using techniques such as gumbel-softmax etc. \n* **Synthetic data**: The model is evaluated on synthetic data only. I would highly recommend exploring e.g. logical problems in natural language such as word problems, logical questions (e.g. LSAT), etc etc, to strengthen the paper.\n* **Suggestion: focusing on the reasoning modules**: I also recommend exploring the parts of the model related to parsing and reasoning independently of the perceptual module since that is the central part of the paper, so would be good to highlight that, and also would broaden the range of datasets that could be explored.\n* **Evaluating each component independently**: Providing more details on the performance of each component independently and performing ablation studies is critical for models with multiple components, rather than only having results on downstream task performance.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The writing of the paper\u2019s abstract could be improved to increase the clarity, but the other sections\u2019 writing quality is good. The multiple diagrams and visualizations  are clear and helpful to convey the ideas of the paper, and the key concepts (e.g. the analogy among different problems having underlying tree structure, and the NSR high-level pipeline). I recommend moving some of the visualizations in the appendix to the main paper.\n\n**Novelty**: The proposed approach is new and the paper explores an important underexplored domain.\n\n**Reproducibility**: Adding more details such as hyperparams and finer modeling decisions for the different component would be useful to increase the paper\u2019s reproducibility.\n",
            "summary_of_the_review": "The paper explores an important direction and show nice experiments but I believe the main issue is the model that could be improved as well as some updates to the writing, and therefore overall at this point recommend weak rejection, but at the same time I wish to encourage the authors to work on improving the paper further to make it better!",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper264/Reviewer_w6P6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper264/Reviewer_w6P6"
        ]
    }
]