[
    {
        "id": "AVcdNNTgQ-",
        "original": null,
        "number": 1,
        "cdate": 1666392397337,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666392397337,
        "tmdate": 1666392397337,
        "tddate": null,
        "forum": "-pAV454n6mS",
        "replyto": "-pAV454n6mS",
        "invitation": "ICLR.cc/2023/Conference/Paper3758/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed Graphsensor, a graph attention network that generates signal segments and learns their internal relationships through a multi-head approach. Their model has 56% of the model parameters compared with the best multi-head baseline while producing a 13.8% improvement in accuracy compared with the best baseline in WISDM, an IMU dataset.",
            "strength_and_weaknesses": "Among the paper's strengths, I noticed that they apply a convolution encoder to every signal segment. Also, they enrich the feature representation with the convolution of multi-head features. Their convolution-based multi-head attention reduces the number of parameters. They show SOTA results for an IMU time series. \n\nOn the other hand, the approach compares slightly below AttnSleep for the SLEEP dataset, albeit without specifically declaring the targeting band..",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-presented and organized and provides enough detail for reproducibility, even though the code is available.  ",
            "summary_of_the_review": "The paper is well-written, illustrated, and described. It offers a novel architecture and SOTA results for an IMU time series dataset. The code is available, and the description is detailed. Yet, for the SLEEP dataset, it does not surpass the current SOTA method. However, Graphsensor does not need the targeting bands. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3758/Reviewer_Vp7E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3758/Reviewer_Vp7E"
        ]
    },
    {
        "id": "ZALtobOdVw2",
        "original": null,
        "number": 2,
        "cdate": 1666671033157,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671033157,
        "tmdate": 1666671033157,
        "tddate": null,
        "forum": "-pAV454n6mS",
        "replyto": "-pAV454n6mS",
        "invitation": "ICLR.cc/2023/Conference/Paper3758/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a graph attention network that is efficient requiring less parameters and training time that traditional approaches based on multi-headed attention. It also models the inter-sensor relationships using convolution encoder for feature representation.\n",
            "strength_and_weaknesses": "Strengths:\n- Good write-up\n- The paper does a good job of motivating the improvement over other attention based methods for time series sensor data, which is basically the improvement in complexity rather than performance.\n- Shows great performance improvement in IMU sensor data\n\n\nWeaknesses:\n- The paper misses some of the more recent baselines  on sleep staging which surpasses the methods the paper compares against.\n- [1] and [2] surpasses DeepSleepNet by more than 4% while this method surpasses it by 3%.\n- It is not that import for sleep staging to be online so complexity might have a lower effect than on a real-time use-case.\n- Other attention based methods perform as well as this for sleep staging.\n\n[1] Perslev, Mathias, et al. \"U-Sleep: resilient high-frequency sleep staging.\" NPJ digital medicine 4.1 (2021): 1-12.\n[2] Van Der Donckt, Jeroen, et al. \"Do Not Sleep on Linear Models: Simple and Interpretable Techniques Outperform Deep Learning for Sleep Scoring.\" arXiv preprint arXiv:2207.07753 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "All of these were good. The novelty aspect might be a little in doubt due to the prior work in sensor data based attention methods. The authors provided the code for reproducibility.",
            "summary_of_the_review": "I think the paper requires some work before it is ready for publication in ICLR. I am not very familiar with IMU sensor data but based on my experience in sleep staging, I can say it does not show any improvement over current baselines. Moreover it does not have any additional motivation which might be important for healthcare like explainability.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3758/Reviewer_vaNK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3758/Reviewer_vaNK"
        ]
    },
    {
        "id": "YIuu1PoqCuL",
        "original": null,
        "number": 3,
        "cdate": 1666695686846,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695686846,
        "tmdate": 1666695686846,
        "tddate": null,
        "forum": "-pAV454n6mS",
        "replyto": "-pAV454n6mS",
        "invitation": "ICLR.cc/2023/Conference/Paper3758/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces GRAPHSENSOR architecture to capture the internal relationships in time-series data. GRAPHSENSOR is composed of single segment representation module using convolutional encoders and relationship learning module using graph-based self multi-head attention. \nApplying GRAPHSENSOR to two datasets of Sleep-EDF-20 and WISDM Smartphone and Smartwatch Activity and Biometrics Dataset, and comparing with baseline methods of Transformer and GAT, it shows comparable performance to those baseline methods.\n",
            "strength_and_weaknesses": "Strength point is proposed architecture is clearly described.\nAbout weakness there are long history of researches about feature learning of audio signals. More deeper and broader description about related works other than Transformer and graph neural networks would be necessary. For example, WaveNet is one of them. \nMore experiments of variations of datasets would support claims.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly described. But more background of the problem and more evaluations would be necessary. The proposed architecture has novelty but not much novelty.\n",
            "summary_of_the_review": "This paper introduces GRAPHSENSOR architecture to capture the internal relationships in time-series data by combining  single segment representation module using convolutional encoders and relationship learning module using graph-based self multi-head attention. \nBut the novelty of the proposed methods is not much. About the objectives of the proposed method, more descriptions about backgrounds and related works would help. More experiments of more datasets and more related works would support claims.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3758/Reviewer_jWJx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3758/Reviewer_jWJx"
        ]
    }
]