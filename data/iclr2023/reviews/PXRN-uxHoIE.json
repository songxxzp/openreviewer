[
    {
        "id": "eehsTUVTxy",
        "original": null,
        "number": 1,
        "cdate": 1666535271751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535271751,
        "tmdate": 1666535271751,
        "tddate": null,
        "forum": "PXRN-uxHoIE",
        "replyto": "PXRN-uxHoIE",
        "invitation": "ICLR.cc/2023/Conference/Paper2298/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a new approach for online continual learning extending the OCM method introduced by Guo et al. (2022). The proposed approach consists in an optimisation problem amd loss function that not only enforces to learn holistic representations of the data (as in OCM) but also class-invariant features. The idea is to simulate images of objects in different environments (via data augmentation), which alter the background of the images but also the colour of the objects themselves.\nExperimental results show the effectiveness of the approach for different image classification benchmarks and two class-incremental learning protocols (disjoint and i-Blurry). The proposed method outperforms the state of the art in accuracy on these datasets.\n",
            "strength_and_weaknesses": "Strenghts:\n- Original approach with some theoretical justification and intuition\n- Experimental results are convincing\n\nWeaknesses:\n- The two data augmentations are somewhat \"handcrafted\" \n- Some parts are hard to follow (Section 3) without knowing OCM\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and presents an original contribution to the field of online continual learning with strong experimental validation.\nHowever, the paper lacks some clarity, especially in section 3, where some concepts are assumed known and not really explained (e.g. the memory buffer).\nAlso, it is not clear in what way the implemented augmentations relate to the more theoretical definition of invariance in section 3 (eq. 2).\nIn this regard, although the intuition of the two proposed augmentations seems clear, in practice, it is not convincing that this is the sort of invariance that may want to achieve. For example, randomly modifying the colour in the proposed way, may lead the model to just ignore colour in general and focus only on textural features.\nHowever, colour undoubtedly contains important discriminative information. Do you have some evidence or further intuition on why this is sensible? (Also concerning the lambda sampled from a Beta distribution.)\nConcerning the background augmentation (Aug_plus), this introduces some spurious noisy features around the \"seam\" or remove biases (related to the context or the scale) that are helpful for classification (or introduce new unrealistic and harmful biases).\n\n",
            "summary_of_the_review": "Overall, the paper proposes an interesting and original contribution to continual online learning with rather thourough experimental validation on several difficult benchmarks outperforming the state of the art.\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2298/Reviewer_jHRu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2298/Reviewer_jHRu"
        ]
    },
    {
        "id": "4ROsXBzswW",
        "original": null,
        "number": 2,
        "cdate": 1666605814780,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605814780,
        "tmdate": 1666605814780,
        "tddate": null,
        "forum": "PXRN-uxHoIE",
        "replyto": "PXRN-uxHoIE",
        "invitation": "ICLR.cc/2023/Conference/Paper2298/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors argue that learning only holistic representations is still insufficient. The learned representations should also be invariant and those features that are present in the data but are irrelevant to the class (e.g., the background information) should be ignored for better generalization across tasks. This new condition further boosts the performance significantly. This paper proposes several strategies and a loss to learn holistic and invariant representations and evaluates their effectiveness in online CL.",
            "strength_and_weaknesses": "Strengths :\n\n1. This paper analyze the problems of online continual learning from a novel perspective.\n2. The method is easy to follow, and the performance is improved to a certain extent.\n\nWeaknesses :\n\n1. The novelty of this paper is incremental. Data augmentation methods have been proposed a lot in continual learning, such as [1][2]. The authors should compare the proposed data augmentation methods with these methods.\n2. The idea of saving more samples proposed in the paper has already been proposed in continual learning, such as image compression [3], and the authors did not compare with the method.\n[1] Class-Incremental Learning via Dual Augmentation, NeurIPS 2021\n[2] Online Continual Learning Under Extreme Memory Constraints,ECCV2020\n[3] Memory Replay with Data Compression for Continual Learning, ICLR 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and the proposed method looks reproducible. The authors analyze the problems of online continual learning from a novel perspective. However, the novelty of this paper is incremental.",
            "summary_of_the_review": "The paper is well written and the authors analyze the problems of online continual learning from a novel perspective. However, the proposed method is not novel enough.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2298/Reviewer_K6JM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2298/Reviewer_K6JM"
        ]
    },
    {
        "id": "rdahmRtlQSX",
        "original": null,
        "number": 3,
        "cdate": 1666657516957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657516957,
        "tmdate": 1671343661484,
        "tddate": null,
        "forum": "PXRN-uxHoIE",
        "replyto": "PXRN-uxHoIE",
        "invitation": "ICLR.cc/2023/Conference/Paper2298/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis paper proposes a fix for the catastrophic forgetting problem (CF) in continual learning (CL) with neural networks. The authors propose an extension of OCM (Guo, 2022), a method that seeks to learn features which preserve mutual information with the original inputs, the outputs, and the previously learned representations over the course of continual learning.\n\nThe extension proposed in this paper is called \"IFO\" which stands for Invariant Feature learning for Online CL. IFO is presented as an add-on to OCM that aims to learn \"invariant\" features which are defined as being both discriminative and present across different many different environments. To do this, IFO incorporates additional image augmentations while training under the OCM loss. Specifically, more color augmentations are used, and an augmentation which superimposes backgrounds from other images is used to create a more diverse set of contexts for objects being classified.\n\nThe paper evaluates IFO within 2 continual learning paradigms: 1) task disjoint, and 2) incremental-blurry task continual learning.\nIn the task disjoint setting, IFO has increased accuracy over other the CL methods on 4 datasets image datasets -- MNIST, CIFAR10/100, TinyImageNet. IFO outperforms all other CL baselines in terms of accuracy, including OCM which it shares a loss function with. IFO also has a lower forgetting rate.  In the incremental-blurry setting, IFO modestly improves over the baseline method, CLIB.\n\nFinally, the paper presents some ablation experiments to determine which aspects of IFO improve performance most. In these experiments, other image augmentation strategies like CutMix and MixUp are used in place of the color and background augmentations used by IFO.\n",
            "strength_and_weaknesses": "The strength of this work is that IFO's empirical results show improved accuracy and decreased forgetting on the 4 image datasets tested on. And stable/time-invariant representation learning is an important problem in both continual learning and in information retrieval where representational shifts necessitate reindexing large datasets.\n\nThe weakness of this work is that makes several smaller contributions -- combining the loss function of OCM with a correlation-based loss to induce embedding stability, and adding a novel image augmentation scheme to create synthetic backgrounds -- rather than making one large contribution.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is mostly clear and can be followed with effort. Results look as though they could be reproduced if the code and datasets are made public, as the authors have promised to do upon publication. The level of novelty is low as the contributions are modest refinements to the existing OCM method.\n\nThe experimental results section can be improved by increasing the uniformity of methods compared. For instance, there are different baselines across the disjoint and blurry settings, it's not clear why the same baselines weren't used across all.\n\nMinor points regarding clarity:\n1) Page 7 - what is the \"ER\" baseline? It is not described as far as I could see\n2) Page 8 - in figure 2(a), what does \"CR\" stand for? Is it another name for the \"IFO\" method?\n3) Page 8 - the eigenvalues of IFO and OCM are claimed to be very different even though they look similar in the plot of appendix 4.\n",
            "summary_of_the_review": "The empirical results for IFO look promising, but the methodological novelty and theoretical motivations are not big enough to advocate for strong acceptance to ICLR.  However, I am weakly in favor of acceptance to ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2298/Reviewer_SrHu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2298/Reviewer_SrHu"
        ]
    },
    {
        "id": "2anekoRCCCV",
        "original": null,
        "number": 4,
        "cdate": 1666768060809,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666768060809,
        "tmdate": 1669121009445,
        "tddate": null,
        "forum": "PXRN-uxHoIE",
        "replyto": "PXRN-uxHoIE",
        "invitation": "ICLR.cc/2023/Conference/Paper2298/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper argues for invariant representations to enable continual learning. It proposes representations that generalize across environments to achieve invariance and shows that this helps reduce catastrophic forgetting.\n",
            "strength_and_weaknesses": "**Strengths**\n\n[S1] The proposed method of learning invariant features is important for continual learning. Improving invariance by constraining augmented views (environments) to have similar representations is sensible and empirically shown to improve generalization.\n\n[S2] The experiments clearly show better generalization to new tasks as well as lower forgetting of older tasks, all stemming from the proposed invariant feature learning. With clear benefits on both disjoint and blurry tasks, this technique should be the baseline to beat for newer works. \n\n[S3] The method shows large accuracy gains on multiple buffer sizes, showing that the invariance objective has better sample efficiency too.\n\n**Weaknesses**\n\n[W1] Since the choice of the augmentations (Aug color, Aug plus) are not clearly explained at the beginning, they seem arbitrary. Only in Sec 5.3, other schemes are discussed. I think the rationality for augmentation choices should be explained in Sec 4.1. \n\n[W2] The claim: \u2018invariance is not important for single/multitask learning due to i.i.d. assumption, is not entirely true. Single/multitask learning can have ood assumptions too, and there are multiple works already addressing invariant representation learning albeit in a non-continual learning setting e.g., IRM [1]\n\n[W3] Invariance is probably better tested when there is a distributional shift. The experiments do not tackle sub-population or domain shifts, it only tackles shift in terms of new classes. But realistically, continual data streams even from same class may change over time (e.g., due to seasonal changes, inclusion of more domains e.g., race, noise characteristics etc), but this is not studied in this work.\n\n[1] Arjovsky, Martin, et al. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019).",
            "clarity,_quality,_novelty_and_reproducibility": "**Awkward/incorrect parts**\n\nWhile the writing is clear overall, there are occasional grammatical/structural problems that require careful editing. Example:\n\nSec 5.2: E.g., \u201cHere we do not use the full IFO as our main contribution is the $L_{invariant}$\nloss and IFO has all components of OCM.\u201d\n\nAlso, shouldn't the main loss be:\n\n$ L_{all} = L_{holistic} + L_{invariant} $\n\ninstead of:\n\n$ L_{holistic} = L_{all} + L_{invariant}? $\n\n**Novelty**\n\nInvariance objectives have been explored for offline scenarios e.g., for distributional shifts, but application of invariance objective for continual learning seems novel. \n",
            "summary_of_the_review": "The paper clearly shows that adding the invariance term helps generalization to new classes while showing lower forgetting. This is an important contribution, so I am leaning towards acceptance. The main weakness would be not studying sub-population/domain shifts, however, I do not think this is critical for acceptance. Also, the paper needs to be carefully edited for clear acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2298/Reviewer_YuYN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2298/Reviewer_YuYN"
        ]
    }
]