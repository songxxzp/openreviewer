[
    {
        "id": "zjzG3bctoc",
        "original": null,
        "number": 1,
        "cdate": 1666695067640,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695067640,
        "tmdate": 1666695067640,
        "tddate": null,
        "forum": "COZDy0WYGg",
        "replyto": "COZDy0WYGg",
        "invitation": "ICLR.cc/2023/Conference/Paper2110/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work aims to improve the state space models (SSMs), which achieve state-of-the-art performance on the LRA benchmark but perform worse on language modeling and large-scale language pre-training. The authors identified two problems related to the model design through two synthetic tasks. Based on the observation, the authors further proposed some modifications which lead to strong LM performance. At the same time, the authors also leverage Flash to improve the efficiency of the inference.",
            "strength_and_weaknesses": "Pros:\n\n1. It is interesting to use synthetic tasks to investigate the drawback of the state-space model.\n2. It is important to improve S4 on large-scale language modelling tasks. This work make an essential step towards this goal\n3. The empirical results is adquante to support the advantage of the model\n\nCros:\n\nMajor questions:\n\n1.The authors proposed two synthetic tasks, Induction Head and Associative Recall. However, the relationship between these two tasks and the language model performance remains unknown. Why does the poor performance of the language model have a strong connection with these two tasks? This connection should be discussed clearly since it motivates the modification, which (as the authors stated) leads to better language model performance. \n\n2.The change improves the language modelling performance, but will the new model hurt the LRA performance. If I didn't miss anything, there is only an efficiency study on LRA without performance comparison. This work will be problematic if the model fails to work on LRA but only work on LMs.\n\nMinors:\n\n1. The authors claim that standard attention has the ability to complete Induction Head and Associative Recall. Is this a rigorous statement or not? If yes, I recommend the authors give formal proof of the statement. I am not that sure whether attention can succeed #for any length#\n\n2. The authors were motivated by linear attention and proposed an O(nlogn) architecture with FFT. The work \"Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding\" achieves the same efficiency with FFT based on linear attention. I believe the relationship should be discussed as a reference ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and the empirical study is strong.",
            "summary_of_the_review": "The authors solved an important problem for state-space models, and the empirical study is solid. I hope the authors can address my concerns, and currently, I give a weak acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2110/Reviewer_EkLp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2110/Reviewer_EkLp"
        ]
    },
    {
        "id": "pr5qJlPfla",
        "original": null,
        "number": 2,
        "cdate": 1666712862121,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666712862121,
        "tmdate": 1666712862121,
        "tddate": null,
        "forum": "COZDy0WYGg",
        "replyto": "COZDy0WYGg",
        "invitation": "ICLR.cc/2023/Conference/Paper2110/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new state space model (SSM) that achieves on par with transformer based language models while inferring up to 2 times faster. The authors first highlight the drawback of previous SSMs compared to attention -- SSMs can't recall earlier tokens in the sequence and can't compare tokens across the sequence. The authors propose an improvement that captures these features by using `shift` and `diagonal` SSM layers. Shift layer shifts dimensions in an input (or steps if the input is a sequence) and diagonal SSM is convenient for computing kernel in the convolution. By borrowing the structure of linear attention, the authors propose a new SSM layer, called H3, that utilizes recurrence structure and can scale to longer sequences better. They show that H3 improves previous SSMs and achieves on par with transformers on synthetic as well as real OpenWebText tasks. They further propose a mechanism, called FastFFTConv, for efficient estimation of convolution where the convolution is replaced with `iFFT (FFT(u) * FFT(f))`. They divide input into chunks -- each chunk fits into the GPU memory -- and keep end-state of each chunk for a more efficient convolution which results in up to 3x speed up against attention.",
            "strength_and_weaknesses": "**Strengths** I found the paper interesting and useful for thinking beyond typical transformer architectures. The paper proposes a simple SSM layer that can replace any self-attention module and broadly applicable. Experimental results are also promising and efficiency improvements should help other work in state space models.\n\n**Weaknesses**\n1. You mention that SSMs are capable of extrapolating to sequences longer than observed during training. I found no empirical results to support this claim in the paper.\n\n2. There are missing references in long-term dependency line of work, especially *Perceiver* style models. Given that one of the things that the paper claims is capturing long term dependencies, I think these need to be cited and compared against.\n\n3. How would you compare to other models in Table 2 is H3 is trained from data rather than by construction? Please also clarify how other models, excluding attention, are obtained.\n\n4. One thing that Transformers have shown is they scale with data and model sizes. While from 125M to 335M, there is a clear improvement in Table 4, I would like to see if there is a similar pattern for data size as in Table 6.\n\nGeneral-purpose, long-context autoregressive modeling with Perceiver AR. Curtis Hawthorne, Andrew Jaegle, C\u0103t\u0103lina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, Jo\u00e3o Carreira, Jesse Engel.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and H3 is a simple and novel extension of previous SSMs. While the implementation is not straightforward to derive from only the paper, the code is also open sourced.",
            "summary_of_the_review": "H3 is a simple yet novel extension of previous SSMs. It presents improvements compared to Transformers and can infer 2 times faster. FastFFTConv is useful for efficient training and can be applied to other SSM works.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2110/Reviewer_PnYM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2110/Reviewer_PnYM"
        ]
    },
    {
        "id": "xWn5efDnERm",
        "original": null,
        "number": 3,
        "cdate": 1666769391074,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666769391074,
        "tmdate": 1666769391074,
        "tddate": null,
        "forum": "COZDy0WYGg",
        "replyto": "COZDy0WYGg",
        "invitation": "ICLR.cc/2023/Conference/Paper2110/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed the H3 architecture to narrow the gap between state space models and attention models on language modeling. Basically, H3 uses a shift SSM and a diagonal SSM to replace the non-linear function in linear attention. The shift SSM is designed to remember tokens in the input sequence while the multiplication between the two SSM outputs for token comparison. \nTo further improve H3 efficiency on hardware the authors proposed FlashFFTConv, which uses fused block FFT algorithm. \n\nExperiments were conducted on training multiple variants of H3 on the PILE dataset, and compared with corresponding GPT2 checkpoints, on OpenWebText and WikiText103. Moreover, the authors also evaluate H3 on zero-shot and few-shot classification.\nAt last, they evaluate efficiency of the proposed FlashFFTConv algorithm on the LRA benchmark, achieving about 2 times speed up.",
            "strength_and_weaknesses": "Strengths: The design of H3 is novel and well-motivated and the paper writing is clear and easy to follow. The authors also clearly claimed their contributions.\n\nWeaknesses: The main concerns are from experiments. I found that the experimental setup on language models is not standard, which makes the comparisons unfair. It is difficult to position H3 among other neural models in language modeling. I elaborated my concerns in the following questions.",
            "clarity,_quality,_novelty_and_reproducibility": "The experimental setup in this paper make it difficult to fairly compare H3 with previous models:\n\n1. The authors trained H3 on the PILE data and calculated PPL on OpenWebText and WikiText103. But both the two datasets have their own training data, and several previous models have reported PPL on them. Why not following previous setup and directly comparing H3 with previous models, such as standard Transformer, Transformer-XL and other efficient Transformer architectures. It is strongly recommended to conducted language modeling experiments on standard benchmarks, such as WikiText103 and PG19, and closely follows previous settings for fair comparison.\n\n2. For the comparison in Table4, since GPT2 models are not trained on the same PILE dataset, the PPLs are not directly comparable. \n\n3. A lot model details are missed, making it hard to check or reproduce experimental results.\n\nOther questions about experiments:\n1. Does FlashFFTConv support half precision training?\n\n2. Have you evaluated H3 accuracy on LRA?",
            "summary_of_the_review": "To sum up, I am worried that the paper needs to re-organize its experiments and analysis before publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2110/Reviewer_UjQC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2110/Reviewer_UjQC"
        ]
    }
]