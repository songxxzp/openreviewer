[
    {
        "id": "_r6f54hPvL",
        "original": null,
        "number": 1,
        "cdate": 1665714866188,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665714866188,
        "tmdate": 1665714866188,
        "tddate": null,
        "forum": "NUBuJsAq1U",
        "replyto": "NUBuJsAq1U",
        "invitation": "ICLR.cc/2023/Conference/Paper1441/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the over-fitting issue caused by reducing intra-class distances in metric learning. Specifically, when the intra-class distances are excessively reduced, the Lipschitz constant of network embedding would be significantly enlarged. Then the decision boundary comes unsmooth and is not good for generalization performance. To address this issue, the authors propose a normalizing flow as a new network layer to regularize the learning. Experiments on popular datasets validate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "**Strengths:**\n\n(1). This paper is clearly written and easy to follow.\n\n(2). The high-level motivation is good. I agree with the authors that excessively reducing intra-class distance may incur over-fitting.\n\n(3). Theoretical analysis is provided.\n\n\n**Weaknesses:**\n\nThis paper has two critical flaws.\n\n(1). The authors claim that the L2 normalization is only available for linear layers, so they cannot be used in deep metric learning. I don't think so. Actually, there is a weight decay technique in deep learning, which also effectively reduces the Lipschitz constant of the neural network. In this case, the motivation for proposing a new regularization technique in this paper is not solid.\n\n(2). For pair-based loss functions such as contrastive similarity loss, I agree with the authors that they may aim to reduce the intra-class distance as much as possible. However, for many relative similarity loss functions, we only encourage that it is okay when inter-class distances are larger than intra-class distances. We would not excessively reduce the intra-class distances as much as possible.\n\n(3). Furthermore, I found the compared methods in experiments are before 2020, which means they cannot be regarded as sota.",
            "clarity,_quality,_novelty_and_reproducibility": "As I discussed above, the clarity of this paper is good. The idea of using normalizing flow to reduce the Lipschitz constant is also new to me. However, it seems that this paper has some incorrect claims, so I think its quality may not be that high. ",
            "summary_of_the_review": "Overall, this paper is well-written and the idea is novel, but I do not agree with some critical claims in this paper. Since these claims are rather critical to the motivation, I would like to vote for a \"reject\", and I think the authors should further clarify them in the new version.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1441/Reviewer_VbA6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1441/Reviewer_VbA6"
        ]
    },
    {
        "id": "v6YdpQ_0Yz",
        "original": null,
        "number": 2,
        "cdate": 1666313093132,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666313093132,
        "tmdate": 1666489492195,
        "tddate": null,
        "forum": "NUBuJsAq1U",
        "replyto": "NUBuJsAq1U",
        "invitation": "ICLR.cc/2023/Conference/Paper1441/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper is about deep metric learning for fine-grained image retrieval. The task consists in learning a parameterized embedding function modelled as a neural network to project images into a deep space where images from the same categories are similar and images from different categories are not similar. The authors propose to train their model with a regularization term minimizing the Lipschitz constant around a set of data points from the same class. They approximate the Lipschitz constant using a Normalizing-Flow network.",
            "strength_and_weaknesses": "Strengths:\n- The strength of this paper resides in the strong theoritical analysis of the effect the Lipschitz constant has on the embedding function modelled by the deep neural network.\n- The authors also show some improvement on the CUB-200-2011 and Cars-196 Dataset\n- The utilization of Normalizing flow network seems to be appropriate for approximating the Jacobian.\n\nWeaknesses:\n-  The main weakness of this paper is that it seemed to have been unfinished. They are a lot of typos and mistakes that degrades the overall quality of the paper. Most importantly the ablation study tables are missing.\n- Incremental contribution. The main contribution is not significant enough in terms of motivation and performance. While the regularization term shows improvement on 2 datasets, it does not on 2 others that are much larger and more complicated. The performances seems to be mostly achieved by the baseline.\n- The figure 2. is not informative, there is no disctinction between the 2 dueling branches. The captions are lacking information. What makes the top branch different from the bottom one?\n- The main contribution is the presentation of  the regularization term where the Lipschitz constant is approximated and minimized for similar sample (as stated end of section 4 and with Eq. 11). \n- In the conclusion the authors mention that other work focus on loss function design and not them. I would argue that design of regularization terms fall within the design of loss functions.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n- A lot of sentences are unclear. The paper requires polishing. I can refer to the second paragrpah of section 2 for instance or the first sentence of section 4.\n- Some mathematical terms are inconsistent or not introduced. d2 is used as output dimensionality in the notation and never used after. Similarly, proxies are mentipnned in the notation part but never used later. The Blank region is annotated by B by never clearly stated, we have to read a second time to understand.\n- Overall, the mathematical notation could be improved.\nQuality\n- The lack of ablation study tables showing the effect of different design choices is deteriotating the quality of the paper.\nReproducibility\nThe overall idea of the paper comes through and seems reproducible. The theory behind the manipulation and approximation of the Lipschitz constant seems grounded.",
            "summary_of_the_review": "I would not accept this paper because too many elements needs to be changed and the contribution is too incremental. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no Ethic concerns",
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1441/Reviewer_obNj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1441/Reviewer_obNj"
        ]
    },
    {
        "id": "nn629kLE9Ip",
        "original": null,
        "number": 3,
        "cdate": 1666620662043,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620662043,
        "tmdate": 1666620662043,
        "tddate": null,
        "forum": "NUBuJsAq1U",
        "replyto": "NUBuJsAq1U",
        "invitation": "ICLR.cc/2023/Conference/Paper1441/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the deep metric learning field. The motivation of this paper is that Current pair-based and proxy-based methods harm the generalization ability as shrink the distance between positive pairs. As a result, it adopts the structure of normalizing flow as the deep metric layer and calculates the determinant of the Jacobian matrix as a regularization term that helps in reducing the Lipschitz constant.",
            "strength_and_weaknesses": "Pros:\nThis paper focuses on a significant topic.\nThe organization of this paper is reasonable and the expression is clear, which make it easy to follow.\n\nCons.\nThe authors summarized four contributions of this paper, however, I did not find the first two.\nI do NOT think the motivation of this paper holds, which is the crucial concern from me about this paper. This paper claims that shrinking the feature space will increase the Lipschitz constant (LC), and give some deductions in Eq (2) and Eq(3). It\u2019s true you can find some x that satisfy Eq (3), but not for all x. Therefore, this can not prove the increasing or decreasing property of LC. Since the motivation does not hold, I do not see the meaning of the proposed method.\nThere are unclear statements, such as \u201cthe first/second deep metric learning layer\u201d, \u201cthe learnable parameter of the parameter of\u201d, \u201cmay be not a connected to the neighborhood of another class\u201d which should be \u201cmay be connected to the neighborhood of another class\u201d in my opinion.\nThe experimental part is very confusing. The proposed method compares with other methods based on the different backbone and embedding dimensions, which I don\u2019t think can be compared together. Since your main strategy is to modify the learning objection, by adding a new regularization term, I advise you to combine multiple loss functions with your regularization term, which will be more convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "I think this paper is good at clarity and reproducibility. The organization of this paper is reasonable and the expression is clear. Furthermore, the implementation details are very clear and those experimental results are supposed to be easy to be reproduced. The novelty and quality of this paper are ordinary, as the LC-based methods were studied to improve the robustness of neural networks before and the performance of this paper is not competitive.",
            "summary_of_the_review": "The motivation of this paper doesn\u2019t make sense to me, which is my main concern about it, hence I recommend rejection.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1441/Reviewer_n2tf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1441/Reviewer_n2tf"
        ]
    },
    {
        "id": "DQWEpZvuDn-",
        "original": null,
        "number": 4,
        "cdate": 1666698145510,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698145510,
        "tmdate": 1666698167558,
        "tddate": null,
        "forum": "NUBuJsAq1U",
        "replyto": "NUBuJsAq1U",
        "invitation": "ICLR.cc/2023/Conference/Paper1441/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper summarizes the traditional distance metric learning algorithms and classification-based distance metric learning algorithms together and solves the Lipschitz constant problem on distance metric learning. The authors design a deep neural network structure that allows us to minimize the Lipschitz constant of the deep neural network directly. They also minimize the determinant of the Jacobian matrix to reduce the Lipschitz constant of the deep neural network.",
            "strength_and_weaknesses": "Strength:\n- Clear motivation and identification of the problem in existing methods.\n- Proposed method is easy to understand and the description is clear.\n- Theoretical proof is sufficient and complete.\n\nWeaknesses:\n- Experiments are not convincing:\n1. An ablation experiment should be added to analyze the validity and necessity of each block. \n2. The effectiveness of sample augmentation is necessary to discuss. \n3. Visualization results should also be added.\n4. The performances of the proposed methods are not good enough.\n- Why Eq.(6) is better than Eq.(5)? \n",
            "clarity,_quality,_novelty_and_reproducibility": "My main concern is the performance of the proposed method.  There are many SOTA works [1][2][3] or more under the same setting and backbones that the authors do not list for comparison, which is better than the results of R@1 the authors propose.\n\n[1] Ebrahimpour et al. \"Multi-Head Deep Metric Learning Using Global and Local Representations.\" CVPR 2022\n\n[2] Zhang, Borui, et al. \"Attributable Visual Similarity Learning.\" CVPR 2022\n\n[3] Venkataramanan, Shashanka, et al. \"It takes two to tango: Mixup for deep metric learning.\" ICLR 2022",
            "summary_of_the_review": "The idea of this paper show some novelty, but the overall writing quality is limited. Also, the experimental result is not convincing and competitive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1441/Reviewer_F9Bh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1441/Reviewer_F9Bh"
        ]
    }
]