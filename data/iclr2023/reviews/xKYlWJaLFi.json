[
    {
        "id": "iDRRi3mgz7J",
        "original": null,
        "number": 1,
        "cdate": 1666710613427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710613427,
        "tmdate": 1669389280321,
        "tddate": null,
        "forum": "xKYlWJaLFi",
        "replyto": "xKYlWJaLFi",
        "invitation": "ICLR.cc/2023/Conference/Paper6135/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes to pre-train a dense retriever with multiple tasks (five in total), which are all related to masked language modelling (multi-decoder). Results on MS-Marco show moderate improvement with respect to baselines.\n",
            "strength_and_weaknesses": "Strengths:\n\n- Unifying/studying the pre-training of information retrieval \n- Improved performances with respect to the state-of-the-art.\n- The proposed pre-training is simple to set up, and might be a basis for future pre-training\n\nWeaknesses:\n\n- it is not obvious why the fact that using a more homogeneous task is better: There is a bold statement p.2: \"due to the divergences of input formats and learning objectives among different tasks, an arbitrary integration of these tasks is inappropriate, which may even cause detrimental gradient interference, leading to performance degradation\" - how is your setting more appropriate? Why having multiple tasks be detrimental - I checked the references and they do not provide any reason (but it is true that the magnitude of the respective losses has to be set as in ref. Kendall et al. 2018).\n- no comparison on the BEIR dataset (which would show how robust the pre-training + training procedure is)\n- apart from an ablation, there is no further analysis of the 5 proposed losses, although they are combined directly with no discussion on why they should have the same importance.\n- No code is provided\n- Using DocT5 (POR loss) shows very moderate improvement in comparison to the cost: it relies on a query generation model (trained on MS Marco which is used for the experiments here...) and document generation (which is costly). I fail to see why it justifies the 0.4 drop in MRR@10 (especially since the variance of the results is not really known)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear although some sentences are quite obscure (especially in section 4.2 where the loss is described). The paper seems to be reproducible (open source code would be a plus, but is not provided). Novelty-wise, the losses are (somehow) novel to the extent of my knowledge, being somehow inspired by Electra as acknowledged by the authors.\n\n\nSome points:\n- Results reported in table 5 show that losses are quite redundant: it would have been good to do ablations with just one loss to see the effect of each (and also to run experiments several time to get an idea of the variance of the results).\n\nMinor points:\n- p.5: \"and remains the masked ones unchanged\"? \"that can deduce its neighbouring relations with other passages.\"? \n- Figure 1: the \"complementary mask prediction\" figure part is wrong\n- Table 7: which dataset?\n- Figure 2: what is a step is not defined\n\n",
            "summary_of_the_review": "This paper proposes a new pre-training for IR - even if the authors frame it as \"multi-task\", it is still heavily based on a masked language modelling. Results are good but not so impressive when looking at the SimLM baseline (which shares this idea of corruption loss) and especially at the quite expensive pre-training procedure.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6135/Reviewer_3P7A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6135/Reviewer_3P7A"
        ]
    },
    {
        "id": "JNaMaG6kQL",
        "original": null,
        "number": 2,
        "cdate": 1666735579657,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735579657,
        "tmdate": 1669373084768,
        "tddate": null,
        "forum": "xKYlWJaLFi",
        "replyto": "xKYlWJaLFi",
        "invitation": "ICLR.cc/2023/Conference/Paper6135/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a multi-task pretraining objective for dense retrieval models. By leveraging the bottlenecked masked autoencoder architecture, the encoder produces a single-vector embedding, which is used to condition the decoder models on three different types of pretraining tasks, which are all trained in parallel: recovery of corrupted (input) passages, recovery of related passages, and recovery of the outputs from an external pretrained LM.\nThe model is tested on standard web search and open domain QA datasets, and achieves state-of-the-art performance.",
            "strength_and_weaknesses": "Strengths:\n\n* simple model that works well\n* paper is easy to understand\n\nWeaknesses (**UPDATED AFTER REBUTTAL**):\n* the ablation results are not convincing:\n    * the improvements from each pretraining objective are relatively small\n    * ~could it be that the results are simply better because you effectively train on more examples the more pretraining objectives you have? Figure 2 suggests that convergence might not have been reached, and hence more pretraining objectives would mean that you actually do more training steps (in the sense of training examples) than what the x-axis shows. When removing each of the 5 pretraining objectives, you could control for this by instead training for 1/5 = 20% longer.~\n    * there is no qualitative analysis that shows us that the different pretraining objectives actually teach the model distinct \"skills\". This could be done by training a model with only a single pretraining objective and then doing an error analysis for each.\n* ~in your comparisons to SimLM, how do you make sure that the small differences in performance are not actually due to factors of variation other than the pretraining objective? Are all the model details the same between the two models? How do you make sure that both models receive equal treatment in terms of hyperparameter tuning?~\n* the model is of limited novelty; neither the bottlenecked model architecture, nor the multi-task pretraining idea are novel. There is some novelty in the concrete pretraining tasks, but their effect is unclear.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, easy to understand and covers a lot of related work. The proposed pretraining tasks are somewhat novel, but are not sufficiently analyzed. The paper puts a lot of emphasis on its method surpassing the previous state-of-the-art. ~Reproducibility is limited, as most model hyperparameters are not explicitly provided, and no code is available.~",
            "summary_of_the_review": "The paper proposes a simple method that reaches state-of-the-art in dense retrieval. However, the ablations are not sufficient to convincingly show that the modeling contributions are actually responsible for this success. Hence, the paper would need a major revision before it can be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6135/Reviewer_jK4d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6135/Reviewer_jK4d"
        ]
    },
    {
        "id": "oVd4Tx-Yvu",
        "original": null,
        "number": 3,
        "cdate": 1666799664120,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799664120,
        "tmdate": 1666803306939,
        "tddate": null,
        "forum": "xKYlWJaLFi",
        "replyto": "xKYlWJaLFi",
        "invitation": "ICLR.cc/2023/Conference/Paper6135/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed multiple pre-training tasks to be used with the bottlenecked masked autoencoder architecture, designed specifically for dense retrieval. The three types of tasks are corrupted passages recovering, related passages recovering, and pre-trained language model (PLM) outputs recovering. The tasks are integrated by formulating each tasks in a unified text-to-text format. Each pre-training task has its own task-specific shallow decoder. The claimed benefits is that the Transformer encoder is forced to compress the information into a dense vector. The empirical results show the proposed method outperforms competitive baselines.",
            "strength_and_weaknesses": "Strengths\n1. The paper proposed integrating multiple pre-training tasks via a unified text-to-text format. The bottlenecked masked autoencoder also allows task-specific decoders.\n2. The results are competitive.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow.\n\nMinor suggestions:\n1. Section 4, \"Approach\". \"an approach to pre-training an effective ...\" -> \"pre-train\"?\n",
            "summary_of_the_review": "The paper proposed carefully designed pre-training tasks, a way to integrate multiple pre-training tasks and show competitive results empirically.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6135/Reviewer_JgYb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6135/Reviewer_JgYb"
        ]
    }
]