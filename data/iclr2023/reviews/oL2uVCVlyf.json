[
    {
        "id": "UPzPB8dFeN",
        "original": null,
        "number": 1,
        "cdate": 1666489088346,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666489088346,
        "tmdate": 1666489088346,
        "tddate": null,
        "forum": "oL2uVCVlyf",
        "replyto": "oL2uVCVlyf",
        "invitation": "ICLR.cc/2023/Conference/Paper6312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- This paper investigates the qualities of object-centric representations (OCR) in the context of reinforcement learning (RL) tasks. Does so by empirically benchmarking OCR and non-OCR methods (eg: end-to-end learning (E2E) and variational auto-encoding (VAE)) on 5 tasks from 2 domains (see Fig. 2). Each benchmark is either driven by an hypothesis or a question, which are clearly stated up-front.\n- The results from these empirical studies can be summarized as follows:\n    - Object-centric pretraining improves sample efficiency over VAE or E2E pretraining on object-centric tasks (Section 4.1.1). But it is slower in terms of wall-clock time (Section 4.2-Q2).\n    - Object-centric pretraining can learn relational properties of objects (eg: SLATE), which can enable reasoning (Section 4.1.1).\n    - OCR and E2E generalize to new objects in the observation but both fail when it comes to compositionality (Sections 4.1.2, 4.2-Q1). They also generalize better to environment that are more visually complex than the ones seen during training (Section 4.2-Q3).\n    - SLATE tends to outperform other OCR learning algorithms (Section 4.2-Q4) and MLP pooling degrades performance of SLATE (Section 4.2-Q5).",
            "strength_and_weaknesses": "- Strengths:\n    - This paper studies an important aspect of transfer in RL: what kind of representations are needed, and when? The empirical evidence makes a strong case for OCR, as they perform best on virtually all experimental testbeds (caveat: those are chosen to be object-centric, see below). I especially enjoyed the evidence from Figure 3 that OCR (ie: SLATE) can ease reasoning on relational tasks.\n    - The authors are very clear in their approach (driven by hypotheses and questions), and the experiments look carefully carried out. They also thoroughly describe their results, which makes me trust their conclusions on their experiments. Altogether, I found this paper easy to follow as it is always clear what aspect of OCR is being tested.\n- Weaknesses:\n    - The main limitation of this work is the scope hypotheses and questions: they all take the form \u201cdoes method A work better than method B in task C?\u201d. From the title and the introduction, I was hoping to gather new insights on the \u201chow\u201d and the \u201cwhy\u201d OCR could be useful in RL. But no hypothesis or question ever gets to these types of insights.\n        \n        In fact, I do not think the reader ever gets an answer to the title question; the first 2 bullet points in the conclusions suggests answers, but I don\u2019t see how those answers are supported by the analysis. For example, which experiments support that:\n        \n        - \u201cOCR can be slower for tasks where object-wise reasoning is not important\u201d? It looks like all testbeds require some form of object-wise reasoning.\n        - \u201cOCR provides disentangled representations for each object\u201d? First, this property is never tested. Second, it is never compellingly shown that it is the reason for the good performance of OCR.\n        \n        For those reasons, the main message I got from this paper strikes me as somewhat incremental: \u201cobject-centric representations work better for object-centric tasks\u201d.\n        \n    - I also have some concerns over the experimental designs.\n        - First, and as mentioned above, what about non object-centric tasks? How far do OCR lag behind non-OCR pretraining? This is important as it starts to provide some ground to answer the \u201cwhen does OCR work\u201d question.\n        - Second, are the E2E and VAE baselines trained on enough data and with enough variations? It would be help the paper if the authors could show that even with more data, VAE and E2E have already \u201cplateaued\u201d and so the benefits of OCR can\u2019t be make up for. Similarly, what if E2E and VAE were trained on much larger datasets (eg: ImageNet) where learning OCR becomes much more challenging (because of the lack of object-centric labels).\n        - Third, and a more minor point, the benchmarking testbeds look a little bit simplistic (see Fig. 2). Do the insights in the paper carry to more realistic tasks such as real-world embodied AI (Habitat/AI2Thor)? The most realistic task in this paper (object-reach) is only ever used once (Fig. 6).\n    - Some minor points:\n        - The background section could include more details for at least one of the OCR method (eg: SLATE) to refresh the reader\u2019s memory. It could also outline the main differences between all OCR methods.\n        - Figures are unreadable for color-blind readers or when printed black-and-white.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: good, the paper is clear and easy to follow.\n- Quality: ok, the presented experiments look sound but I wished the authors dove deeper into the properties of OCR.\n- Novelty: incremental, I wish the authors provided more profound insights than \u201cobject-centric representations work better for object-centric tasks\u201d.\n- Reproducibility: ok, I haven\u2019t look at the Supp. Mat. but believe the experiments are reproducible.",
            "summary_of_the_review": "- Strengths:\n    - The core question of the paper is important to the ICLR community.\n    - The experiments are clearly laid out, and their results discussed in details.\n- Weaknesses:\n    - The hypotheses tested in the paper are somewhat superficial and do not answer the core question of the paper \u2014 why and when are OCR required for RL?\n    - Some experimental design choices can be improved (choice of testbeds, training of baselines)?\n    - (Minor) Background could be more fleshed out and figures could be more legible.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6312/Reviewer_EbVV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6312/Reviewer_EbVV"
        ]
    },
    {
        "id": "VBeh9KkC4In",
        "original": null,
        "number": 2,
        "cdate": 1666721980967,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666721980967,
        "tmdate": 1666722123192,
        "tddate": null,
        "forum": "oL2uVCVlyf",
        "replyto": "oL2uVCVlyf",
        "invitation": "ICLR.cc/2023/Conference/Paper6312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a set of simple RL tasks meant to assess the benefits of using object-centric representations for RL. They compare the performance of 3 recent object-centric representation learning methods (IODINE, Slot Attention and SLATE) as features for a simple PPO agent solving these tasks, assessing details of where they help or not.",
            "strength_and_weaknesses": "1. The paper is clear and very well introduced. It makes its assumptions and targets clear, and the set of proposed tasks are clean and well chosen.\n   1. The Spriteworld tasks cover a good extension of capabilities, and if object-centric models fail on this dataset I am not sure how one would expect them to target more complex environments.\n   2. The Robotics task is a nice simple increase in complexity, but obviously one would prefer to have more of these (perhaps closer to the scope of MetaWorld [1]?), and more complex visuals.\n2. I think the models selected are good, and I found the baselines to be appropriate, but I was expecting the E2E CNN to be used more.\n   1. What is the performance of E2E CNN in Table 2b?\n   2. I am not entirely sure that CNN(VAE) is that useful, it is not really something that people use?\n   3. You could have used other types of \u201cend of convolutional stack as objects\u201d methods, like what\u2019s used in SPACE, RelationNets, or even use the Conv backbone used by Slot Attention directly?\n3. It might be important to discuss the effect of the training set used for the object-centric representations.\n   1. In the current tasks, a random policy might be sufficient, but this is usually not the case otherwise.\n   2. You could introduce more \u201con-policy\u201d datasets (akin to Dreamer), and see how this changes results.\n   3. This would also allow you to \u201ccount\u201d or not the samples used for this in your sample efficiency assessment (e.g. consider starting from an empty dataset and training the representation online like Planet/Dreamer. It might fail, but if it works the numbers would be better?)\n4. I really liked the presentation of Section 4. It is clear, hypothesis-driven and very well executed.\n   1. Figure 3 is clean and easy to follow.\n   2. Figure 4 somehow confused me, because I was trying to find the same columns as in Figure 3. Could you reorder/transpose it to match the structure of Figure 3 perhaps?\n5. The text in Section 4.2 describing Table 2 does not seem to correspond to the content of Table 2?\n   1. It feels like this is a leftover of a draft and should be updated. The table indicates better results than the main text.\n6. I also really liked the observations in Appendix B.3, might be nice if this could be brought into the main text more strongly?\n\n[1] https://meta-world.github.io/ \n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity is very high. It\u2019s really easy to follow and the choice of curves and results are great.\n* Quality is also high, it is small in scope but everything seemed very well balanced and executed to me.\n* Novelty is also good enough. The tasks aren\u2019t dramatically different from existing ones, but they cover a good set, and the exact model comparison performed is clearly novel and will be valuable to refer to in the future.\n",
            "summary_of_the_review": "I am quite torn about this paper, because even though I feel it is well executed and proposes something that would be quite useful to build upon and for the community to use, it is perhaps too limited in scope for ICLR (although there is past evidence that similar papers got accepted [1]). If that was NeurIPS, I would recommend submission to the Dataset and Benchmark track, where it would clearly shine and I would heartily support it.\n\n\nHowever, despite these current limitations, as a practitioner I would leverage this benchmark for my own work and I feel it would be a valuable paper to refer to or integrate into existing open-source projects. Hence, I would currently recommend to borderline accept, even though I understand this might not be possible.\n\n[1] https://openreview.net/forum?id=1W0z96MFEoH",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6312/Reviewer_BK2Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6312/Reviewer_BK2Z"
        ]
    },
    {
        "id": "gsFtr-cLyC",
        "original": null,
        "number": 3,
        "cdate": 1666756462783,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756462783,
        "tmdate": 1668803405956,
        "tddate": null,
        "forum": "oL2uVCVlyf",
        "replyto": "oL2uVCVlyf",
        "invitation": "ICLR.cc/2023/Conference/Paper6312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates multiple hypotheses regarding the use of object-centric representations (OCRs) for reinforcement learning. Specifically, they consider object-centric representations that are pre-trained using unlabeled interaction data in the environment before being frozen for use in RL. The hypotheses are that OCR pretraining improves RL sample efficiency (H1), that OCR pretraining improves performance on tasks involving relational learning (H2), and that OCR pretraining helps facilitate out-of-distribution generalization to new objects or combinations of objects (H3). To assess these hypotheses, the authors construct 4 2D interactive navigation tasks in Spriteworld and 1 robotic manipulation task in CausalWorld. The authors deliberately limit the complexity of the involved objects to assess OCR in a best-case setting. The authors consider the SLATE, IODINE, and Slot-Attention OCR encoders, and $\\beta$-VAE (using the VAE latent or CNN feature map), and end-to-end learned CNN as non-structured baselines. Finally, ground truth low-dimensional state is also compared to. The authors present results that largely support the 3 hypotheses, and also contribute results tackling 5 other research questions.",
            "strength_and_weaknesses": "Strengths\n\nThis paper sets out to empirically validate important motivating assumptions underlying the field of object-centric representation learning. The methods are clearly presented and the interpretation of the results is largely sensible.\n\nWeaknesses\n\nThe toy nature of the experiments restricts the significance of the study to environments with similarly limited complexity. That is, part of the answer to the \"When\" in the title must include \"when OCRs are essentially perfect\". At the very least, the claims made in the paper should be so qualified. And unfortunately, I'm not sure that there existed much doubt about the efficacy of OCRs in this setting. \n\nThe descriptions of the proposed tasks include a large number of plausible-sounding yet unsupported statements about necessary conditions for solving each task. For example, for Object Comparison, the agent does not necessarily need to compare all pairs of objects; it could succeed by simply learning to represent how much different colors appear in the image, and move towards the rarest color. Unfortunately, the inaccuracy in these statements propagates to the interpretation of the results, resulting in some overclaiming.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThe organization of the paper is logical and facilitates understanding. I would have liked for a bit more detail of the models to be present in the main text, e.g. the precise form of the OCR representations before pooling (presumably a sequence of slots/tokens).\n\nQuality\n\nThe execution of the work is largely good. The significance is somewhat limited, as argued above.\n\nNovelty\n\nThe main novelty of this work lies in the formulation of the study's research questions and the creation of the Spriteworld and CausalWorld tasks. There is no technical novelty otherwise.\n\nReproducibility\n\nThe methods are described in a reasonable level of detail. Source code is not provided or promised, however.",
            "summary_of_the_review": "Overall, this paper is well-written and well-executed, but as an empirical study of prior methods its significance is severely limited by its chosen scope. I currently recommend borderline rejection, but with low confidence since I could be swayed by more positive assessments of impact from other members of the reviewing team with more skin in the field of object-centric representation learning.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6312/Reviewer_YJeG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6312/Reviewer_YJeG"
        ]
    },
    {
        "id": "ku3GiEBpHom",
        "original": null,
        "number": 4,
        "cdate": 1666859868641,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666859868641,
        "tmdate": 1666859970488,
        "tddate": null,
        "forum": "oL2uVCVlyf",
        "replyto": "oL2uVCVlyf",
        "invitation": "ICLR.cc/2023/Conference/Paper6312/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides an empirical evaluation of whether object-centric representation pre-training is useful for RL learning. They find that OCR pre-training generally delivers better and more data-efficient model, also allowing generalization to unseen settings (e.g., an unseen number of objects).\n",
            "strength_and_weaknesses": "The paper presents a nice analysis on the environments and tasks it studies. It shows that OCR outperforms end-to-end distributed representations on relational tasks and scenes with many objects but not on simple object goal tasks, which aligns with our expectation of OCR.\n\nHowever, my main concern is that the experiments only concern two simple synthetic environments. I am not sure if this can be considered as a comprehensive study on whether OCR pre-training is effective for reinforcement learning.\n\nThe experiments are als similar to the experiments in COBRA (Watters et al.), which also uses Spriteworld. It would be good if the authors could highlight the difference.\n\nWatters et al. COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration\n\n---\n\nMinor question:\n\nWhy would OCR models outperform the GT model in second figure in Figure 3? I would image GT as an upper-bound.\n\nHow is the GT state embedded? What does the ground truth state include? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear. Most implementation details are provided.",
            "summary_of_the_review": "Overall, while I think the authors provide nice analysis on the studied environment. However, I am not sure if the experiments can be considered as comprehensive enough to support some of the rather general claims the authors made.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6312/Reviewer_JnXb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6312/Reviewer_JnXb"
        ]
    }
]