[
    {
        "id": "LvVR0lB3T10",
        "original": null,
        "number": 1,
        "cdate": 1666647266406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647266406,
        "tmdate": 1666647266406,
        "tddate": null,
        "forum": "Siln8xpTMrZ",
        "replyto": "Siln8xpTMrZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1632/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studied asynchronous decentralized SGD for strongly convex problems. The authors provide theoretical analysis for the convergence rate and conduct experiments to verify its performance. But the writing is not very clear. It is not easy to follow.",
            "strength_and_weaknesses": "Strength:\n1. Theoretical convergence rate is established.\n2. Studying the convergence from the continuous perspective.\n\n\nWeaknesses:\n1. The convergence rate in the abstract is wrong. \n2. Why is it necessary to study the convergence rate from a continuous perspective? Is there any benefit? For strongly convex problems, the traditional analysis can also establish the linear convergence rate. Moreover, in the appendix, I didn't find the authors use continuous tools to investigate convergence. Why do you introduce Eq.(3) and that in Section 3.3?\n3. What's the meaning of $\\chi_1$ and $\\chi_2$? How do they relate to the spectral gap?\n4. How does the communication latency affect the convergence rate?\n5. How does the spectral gap affect the convergence rate?\n6. Can the convergence rate achieve the linear speedup with respect to the number of devices?\n7. The experiment is too simple. More complicated models and datasets should be used to evaluate the performance. \n8. Asynchronous decentralized SGD has been studied before. But the authors missed some important literature, e.g., http://proceedings.mlr.press/v80/lian18a.html ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Poor\nQuality: Poor\nNovelty: Neutral\nReproducibility: N/A",
            "summary_of_the_review": "This paper studied asynchronous decentralized SGD for strongly convex problems. The authors provide theoretical analysis for the convergence rate and conduct experiments to verify its performance. But the writing is not very clear. It is not easy to follow.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1632/Reviewer_c2yc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1632/Reviewer_c2yc"
        ]
    },
    {
        "id": "zSnKW03ZOBo",
        "original": null,
        "number": 2,
        "cdate": 1666651975765,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651975765,
        "tmdate": 1666652293414,
        "tddate": null,
        "forum": "Siln8xpTMrZ",
        "replyto": "Siln8xpTMrZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1632/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposed a decentralized asynchronous stochastic first order algorithm to minimize a sum of L-smooth and \u00b5-strongly convex functions distributed over a time-varying connectivity network of size n. The authors model the local gradient updates and gossip communication procedures with separate independent Poisson Point Processes, to decouple the computation and communication steps. The proposed method is differ from the majority of works since it does not use a multi-consensus inner loop nor other ad-hoc mechanisms such as Error Feedback, Gradient Tracking, or a Proximal operator.\n",
            "strength_and_weaknesses": "strength: the proposed approach is differ from the majority of works since it model the local gradient updates and gossip communication procedures with separate independent Poisson Point Processes, to decouple the computation and communication steps. \n\nweakness: \n- the experiment setting with decentralized linear and logistic regression tasks is too simple to validate the effectiveness of the approach. \n- many importance factors of convergence rate are not clear, such as communication latency and linear speedup ",
            "clarity,_quality,_novelty_and_reproducibility": "writing is not very clear to follow",
            "summary_of_the_review": "The authors model the local gradient updates and gossip communication procedures with separate independent Poisson Point Processes, which differ from the majority of works. The experiment is too simple for verification. The convergence analysis is lacking some important factors.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1632/Reviewer_R4MY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1632/Reviewer_R4MY"
        ]
    },
    {
        "id": "TEI0vrEeo98",
        "original": null,
        "number": 3,
        "cdate": 1666669012071,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669012071,
        "tmdate": 1666669260139,
        "tddate": null,
        "forum": "Siln8xpTMrZ",
        "replyto": "Siln8xpTMrZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1632/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a decentralized asynchronous stochastic first-order algorithm DADAO to minimize the sum of strongly convex functions over time-varying decentralized networks. This paper develops the theoretical performance guarantee of a related ODE system and tests the empirical performance by numerical simulations. ",
            "strength_and_weaknesses": "Strength: \n\n1. The proposed algorithm achieves new performance guarantees in both communication edges and gradients.\n\nWeaknesses:\n\n1. Theorems 3.2 and Corollary 3.2.1 provide guarantees for the ODE system Eq.(3) and Eq.(8), respectively. However, because ODE considers a continuous-time regime, it is not obvious whether these results still hold in practical discrete-time implementations. The discrete-time performance guarantee has been shown in prior works (i.e., Theorem 4 of [1]). It would be beneficial if the performance guarantees of discrete-time GD and SGD algorithms can be provided, which is provided in prior work. \n\n2. Theorem 3.2 considers the case where $\\chi_1^* \\chi_2^* \\leq 0.5$. This assumption seems to be a bit restrictive because the constants $\\chi_1^*$ and $\\chi_2^*$ depend on the network itself. It looks that ADOM+ in [1] does not need such a restrictive assumption to ensure convergence. It would be helpful if an additional explanation can be provided to justify why such an assumption is required. It would also be beneficial to discuss whether similar assumptions are required to the mentioned benchmark algorithms. \n\n3. Theorem 3.2 shows the existence of the parameters $\\alpha,\\gamma$ but does not specify how these parameters should be chosen in real networks. It would be helpful to specify the choice of these parameters. Also, does  $\\gamma$ refer to the same quantity in Table 1? \n\n4. Corollary 3.2.1 introduces an additional bias term $\\frac{C_1}{\\sqrt{\\mu L}}$ to characterize the SGD system (8) in a continuous-time regime. It is claimed that \"$L$ allows to adjust the trace-off bias-variance of the descent\". This seems to be a bit confusing because $L$ represents the fixed Lipschitz constant. If one considers a larger $L$ to reduce this term, the decay rate of the term $C_0 e^{-ct\\sqrt{\\mu/L}}$ would deteriorate as well. It would be helpful if the authors could elaborate more on this. \n\n5. When comparing DADAO with ADOM+, it is claimed that ADOM+ has potentially substantially higher expected communication than DADAO. It would be beneficial if the paper can specify the referred scenarios. \n\n6. Various benchmark algorithms are tested in the numerical part. However, it seems that MSDA also has comparable performance with the proposed DADAO algorithm in most of the scenarios. Could the authors provide additional numerical experiments to demonstrate the efficiency of DADAO? It would be helpful if numerical experiments in some special cases such as star or complete networks can suggest DADAO's superior performance.\n\nMinor Issues: There are some typos and some quantities are undefined. \n\n1.  In the abstract, the $O(n \\sqrt{\\frac{L}{\\mu}} \\log \\epsilon)$ gradient complexity is confusing. Should it be  $O(n \\sqrt{\\frac{L}{\\mu}} \\log \\frac{ 1}{ \\epsilon} )$ where $\\epsilon$ is the precision? \n\n2. In Table 1, $\\gamma$ is undefined so that a fair comparison between $\\sqrt{\\chi_1\\chi_2} n $ and $\\chi_1 |\\mathcal{E}|$ is not obvious. \n\n3. When defining $\\chi_2(t)$ in page 4, the quantity $\\Lambda^+(t)$ is undefined. \n\nReferences: \n[1] Kovalev, D., Shulgin, E., Richt\u00e1rik, P., Rogozin, A., and Gasnikov, A. (2021). ADOM: Accelerated decentralized optimization method for time-varying networks. ",
            "clarity,_quality,_novelty_and_reproducibility": "The problem setting is clear but the writing is not very good because of typos and missing definitions. The work seems to be novel. \n\n\n\n",
            "summary_of_the_review": "This paper proposes an algorithm to solve decentralized strongly convex optimization and establishes some new complexity results. The results are developed for continuing-time frameworks, but the guarantees for implementable practical frameworks are absent. The theoretical claims are also only applicable to a restrictive class of problems. The overall contribution seems to be limited. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1632/Reviewer_gkNG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1632/Reviewer_gkNG"
        ]
    },
    {
        "id": "3265l6RWCs",
        "original": null,
        "number": 4,
        "cdate": 1667224775949,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667224775949,
        "tmdate": 1667224775949,
        "tddate": null,
        "forum": "Siln8xpTMrZ",
        "replyto": "Siln8xpTMrZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1632/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the decentralized asynchronous optimization problem on a time-varying network with n nodes where the main purpose is to minimize the sum of  L-smooth and \\mu-strongly convex functions distributed on the nodes. The authors decouple the computation and communication steps in the asynchronous optimization problem by modeling this problem as a union of two independent Poisson Point Processes and consider primal gradients in their proposed method. They show convergence results for this problem under the proper assumptions. They also present numerical results corroborating their findings in the convergence results.\n",
            "strength_and_weaknesses": "Strengths:\n-The authors can achieve communication rounds and convergence complexity results by decoupling optimization (computation) from communication via two independent Poisson processes.\n-The authors provide a clear statement of contributions and comparisons to the literature.\n-A novel analysis of asynchronous communications via is provided in Section 3.\n\nWeaknesses:\n-The optimization results in Subsection 3.3 do not contain information on bounds for hyperparameters in the proposed dynamic in Subsection 3.2.\n-The stochastic result (Corollary 3.2.1) shows a non-vanishing error in the rate. It is conventional to tweak the hyperparameters (e.g., a decreasing learning rate) to make this term vanish over time. It is unclear why this term is a constant and if it can be improved. The current format undermines stochastic optimization results.\n-Numerical comparisons are limited to the ADOM+ algorithm. I recommend further comparisons with other algorithms, such as Gradient Tracking.\n",
            "clarity,_quality,_novelty_and_reproducibility": "-Throughout the manuscript, underlying assumptions are clearly stated.\n-I recommend moving Algorithm 2 from the appendix to the main manuscript.\n-The clarity of convergence results requires some improvements. The main technical contribution of this paper lies in the gossip analysis. However, the optimization results are not presented and elaborated sufficiently. I recommend expanding the discussion in Subsection 3.3.\n",
            "summary_of_the_review": "A new study on decentralized asynchronous optimization on time-varying graphs is made in this paper. By modeling the problem via Poisson processes, the authors present communication and computation complexity for their proposed algorithm. Some improvements in Subsection 3.3 are required to clarify the findings for stochastic optimization.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1632/Reviewer_L3mL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1632/Reviewer_L3mL"
        ]
    }
]