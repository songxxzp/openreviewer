[
    {
        "id": "exXSXBk1gnn",
        "original": null,
        "number": 1,
        "cdate": 1666380298887,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666380298887,
        "tmdate": 1666380298887,
        "tddate": null,
        "forum": "O2cW5Q3bH_M",
        "replyto": "O2cW5Q3bH_M",
        "invitation": "ICLR.cc/2023/Conference/Paper4334/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study the evolution under gradient flow of the training and test errors in the setting of Gaussian covariate models, which encompasses several other well studied settings. Using tools from random matrix theory, in particular the linear pencil, they calculate the exact asymptotic behavior of several quantities in the high-dimensional limit. The authors show how multiple descent phenomena can arise as a function of both number of parameters and training time. The paper also includes results on real datasets and shows good agreement between simulations and theoretical predictions.",
            "strength_and_weaknesses": "**Strengths**\n\n- The setting of Gaussian covariates is nice and generalizes many other models.\n- Early stopping is important in practice. This theory gives some results on how to choose the stopping to avoid multiple-descent peaks.\n- The analysis connecting the peaks in Fig 2 to the spectrum in Fig 3 is nice.\n- The use of Dyson\u2019s Brownian motion to derive the self-consistent equation (276) is cool.\n\n\n**Weaknesses**\n\n- No significant new machine learning implications from the results are derived in the paper. This might limit the paper\u2019s interest to the wider ICLR community.\n- I couldn\u2019t find any details of how numerical predictions are generated from the equations of Result 2.1. In particular, how are the contour integrals calculated? Which contour is used? How do you avoid spurious solutions to the self-consistent equation?",
            "clarity,_quality,_novelty_and_reproducibility": "- It would help my review if the authors could comment on what is completely novel in this work. My current sense is that the main contribution is to generalize previous work, including Bodin and Macris, to the full Gaussian covariates setting.\n- Just after Eq (8), the assumption that U and V* commute is mentioned to provide a limiting self-consistent equation. Can this be avoided by introducing a joint spectral density like Assumption 1 in [1]?\n- In Sec. 1.1, the authors should consider citing [2] and [3] in relation to the analytic calculation of double-descent curves. Similarly [4] is a relevant citation from triple or even multiple descents for the generalization error.\n- I think Eq (9) has a typo since there is the matrix I in the denominator.\n\n[1] Tripuraneni, Nilesh, Ben Adlam, and Jeffrey Pennington. \"Overparameterization improves robustness to covariate shift in high dimensions.\" Advances in Neural Information Processing Systems 34 (2021): 13883-13897.\n\n[2] Lin, Licong, and Edgar Dobriban. \"What Causes the Test Error? Going Beyond Bias-Variance via ANOVA.\" J. Mach. Learn. Res. 22 (2021): 155-1.\n\n[3] Adlam, Ben, and Jeffrey Pennington. \"Understanding double descent requires a fine-grained bias-variance decomposition.\" Advances in neural information processing systems 33 (2020): 11022-11032.\n\n[4] Adlam, Ben, and Jeffrey Pennington. \"The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization.\" International Conference on Machine Learning. PMLR, 2020.",
            "summary_of_the_review": "The paper contains nice results that unify and extend previous work. The paper's main claims are supported by proofs and illustrative experiments. Overall the paper is clearly written and covers a lot of material. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4334/Reviewer_amF8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4334/Reviewer_amF8"
        ]
    },
    {
        "id": "jlEMGbNCED",
        "original": null,
        "number": 2,
        "cdate": 1666698945170,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698945170,
        "tmdate": 1666698945170,
        "tddate": null,
        "forum": "O2cW5Q3bH_M",
        "replyto": "O2cW5Q3bH_M",
        "invitation": "ICLR.cc/2023/Conference/Paper4334/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies gradient descent for linear regression under the high dimensional framework. Under certain conditions, the authors study the generalization error in such gradient descent, and show that there exists multiple descent in the generalization error of the linear model during training. The authors also conduct experiments on the random feature models and realistic data sets.",
            "strength_and_weaknesses": "Strengths:\n\n1.  Multiple descent and learning in high dimensions is an important and interesting topic. This paper proposes some timely and interesting results.\n\n2. The paper provides rigorous proof of the generalization error in the gradient descent flow, and also provides some simulations and experiments.\n\nWeaknesses: \n\nMy major concern is that there seem to be some confusing/vague statements. Below are some examples: \n\n1. Given existing analyses on double/multiple descent in linear models and the regularization effect of early stopping, the technical contribution of the paper may be limited. \n\n2. Recently, there are some papers related to multiple descent (arXiv:2204.10425, arXiv:2205.14846, arXiv:2208.09897). The connection between this paper and existing works is not discussed very thoroughly.\n\n3. The theory applies to the high dimensional setting, so setting $p=3$ in the experiments may not be very reasonable. The authors should provide simulation results with larger $p$.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the clarity of this paper can be improved if the authors give more discussion on the problem setting, intuition, and comparison to recent works. The novelty of this paper mainly lies in the observation of multiple descent with respect to the training epochs.",
            "summary_of_the_review": "In summary, this paper gives solid and interesting results. However, as discussed above, several parts of the paper can still be improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4334/Reviewer_utMN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4334/Reviewer_utMN"
        ]
    },
    {
        "id": "SCeBWj5joQ",
        "original": null,
        "number": 3,
        "cdate": 1666742458759,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666742458759,
        "tmdate": 1666742458759,
        "tddate": null,
        "forum": "O2cW5Q3bH_M",
        "replyto": "O2cW5Q3bH_M",
        "invitation": "ICLR.cc/2023/Conference/Paper4334/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper looks at the regularized linear regression where the data is generated using a Gaussian Covariate model and the regression targets are linear functions of the data. the paper starts by showing the that the setting is fairly general and captures many other set ups. In this set up, under some assumptions the paper then obtains a formula (albeit a complicated one) for the training and test error as time evolves. \n\nThen they specialize to specific instances and re-discover known results from prior work. \n\n",
            "strength_and_weaknesses": "**Strengths**\n---\n\n1) The set up is fairly general and captures many existing models that have been analyzed. \n2) The method provides the whole training curve and not just the limiting distribution. \n3) The paper re-captures known results/\n4) The idea of evolutions of the eigenvalues and the ``bulk'' in section 3.2 is interesting. \n\n**Weaknesses**\n---\n\n1) I think the results are very hard to parse specially Result 2.1 and result 2.4. Further in result 2.1 the terms depend on equations $\\tilde{f}$ which is very complicated term which itself terms on equation (8) which is a complicated implicit equation (both sides depend on the quantity). Hence while the formulas might be true, it is not clear how informative they are. **That is, do they actually tell us much more than $\\beta^\\lambda = (\\hat{X}^T\\hat{X} + \\lambda I )^{\u22121} \\hat{X}^T Y$ and then just writing down the test error.**\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n---\n\nWhile the writing of the paper is clear, the equations and the results are fairly opaque. \n\n**Novelty and Significance**\n---\n\nThe analysis techniques are not the most novel. However, the generality of the setting and then derivation of known results in this unified setting is significant. \n\n**Reproducibility**\n---\n\nThe paper should be reproducible. \n\n**Questions**\n---\n\n1) Can the authors provide some insight into the formulas in Result 2.1?",
            "summary_of_the_review": "In summary the paper is a nice theoretical contribution however, the insights that can be readily obtained seem lacking. I think significant work would be required to simply the formulas or some work should be done to help understand the formulas (at heuristically) would go a long way to improving the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4334/Reviewer_RBfi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4334/Reviewer_RBfi"
        ]
    },
    {
        "id": "_y1xVE5gzt",
        "original": null,
        "number": 4,
        "cdate": 1666771948330,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666771948330,
        "tmdate": 1666808308988,
        "tddate": null,
        "forum": "O2cW5Q3bH_M",
        "replyto": "O2cW5Q3bH_M",
        "invitation": "ICLR.cc/2023/Conference/Paper4334/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This submission computes the asymptotic training and test error of linear estimators optimized by gradient flow on the least squares objective, under the student-teacher setting with Gaussian covariates studied in [Loureiro et al. 2021]. The analysis generalizes the earlier linear pencil calculation for random features model in [Bodin and Macris 2021], and recovers the performance of the ridge regression estimator when the time $t$ is large. The authors present a few examples of solvable models, and also demonstrate empirically that the Gaussian covariate predictions can be accurate for certain real-world datasets. ",
            "strength_and_weaknesses": "## Strength\n\nThe studied problem is well motivated: most existing works on the precise asymptotics of training/test error only consider the empirical risk minimizer (instead of the entire gradient flow trajectory), and the time-evolution of the error reveals some interesting phenomena such as epoch-wise double descent. While the results are only derived for Gaussian features, we may expect wider applicability due to the Gaussian equivalence (universality) property. Moreover, the linear pencil construction and the Dyson Brownian motion derivation of the self-consistent equations could be useful in future random matrix-related research. \n\n## Weaknesses\n\nMy main concern is that many relevant prior results are not discussed.  \n\n- Asymptotics of the least squares estimator in Section 3.2 (anisotropic features, isotropic teacher coefficients) was first rigorously studied in [Dobriban and Wager 2018]. This result was then extended to general teacher models using the Stieltjes transform approach in [Wu and Xu 2020] [Richards et al. 2020], where the authors already showed that the multiple descent risk curve can be engineered using eigenvalues of different scales. Also note that the self-consistent equations in these works are identical to the infinite time-limit in this submission. \n\n- The discussion of the Gaussian equivalence principle in Section 3.3 is insufficient and non-rigorous. The cited [Peche 2019] only implies the (weak) equivalence in the kernel spectrum, which does not guarantee the equivalence in the training and test error. The authors should look into [Hu and Lu 2020] and references therein. \n\n- Asymptotics of the gradient flow dynamics can also be analyzed using alternative approaches such as dynamical mean-field theory. In this submission, the squared loss gives a closed-form expression of the trajectory which simplifies the computation, but it would still be nice to briefly discuss and compare these different method. \n\nDobriban and Wager 2018. High-dimensional asymptotics of prediction: ridge regression and classification.  \nWu and Xu 2020. On the optimal weighted $\\ell_2$ regularization in overparameterized linear regression.  \nRichards et al. 2020. Asymptotics of ridge (less) regression under general source condition.  \nHu and Lu 2020. Universality laws for high-dimensional learning with random features. \n\nIn addition, it is not entirely clear of how ML theory researchers can benefit from knowing these asymptotic formulae. Currently, the application seems to be limited to plotting good-looking risk curves; this does not provide quantitative characterization of the reported phenomena. For example, to rigorously establish epoch-wise double descent, the authors need to check the time derivative of the error to show non-monotonicity. Such analysis would strengthen the theoretical contribution of this submission. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity \\& Quality:** the writing is mostly clear. I have the following questions: \n\n- In the current manuscript there is no discussion around the assumptions, and it is not clear if they hold in the listed examples. For the random features example, I believe the activation function is zero-centered so that the spectral norm of the kernel matrix is asymptotically bounded. Can the authors confirm the conditions required for the activation? \n\n- In the Appendix, I would appreciate some explanations on how the large block matrices are constructed. Do we know if the linear pencil is minimal in terms of the size? \n\n- In Figure 4 (right), why are the experimental values only plotted until $t=10^2$? Does the test error start to deviate from the theoretical predictions after that? \n\n**Novelty:** see weaknesses above. \n\n**Reproducibility:** N/A. ",
            "summary_of_the_review": "My current evaluation is that this is a borderline submission: I believe the random matrix analysis is a solid contribution, but the authors need to thoroughly discuss prior works and present the results in a more accessible way for the ICLR community. I will consider updating my score if the authors can adequately address the above concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4334/Reviewer_FVzp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4334/Reviewer_FVzp"
        ]
    }
]