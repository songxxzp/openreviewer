[
    {
        "id": "S-iAh8aWG8",
        "original": null,
        "number": 1,
        "cdate": 1666501513231,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666501513231,
        "tmdate": 1666501541968,
        "tddate": null,
        "forum": "JdTnc9gjVfJ",
        "replyto": "JdTnc9gjVfJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3636/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates a staged approach to model-based RL with behaviour demonstrations, exploring the effect of each of these stages. Initially, demonstration data is used to train a behaviour cloning policy. This policy is then used with some exploration to gather data for model learning. Finally model and policy fine tuning takes place using environment interaction. Data augmentation is applied, alongside demonstration sampling, which is used to ensure the policy does not deviate significantly from the initial BC mode. Results show that this approach (MoDem) outperforms a number of other acronyms on a number of simulated benchmarks (dm control, meta world, adroit), and that exploration with the BC model seems to be key to this success. ",
            "strength_and_weaknesses": "Strengths:\n\nThe paper is clearly written with decent ablations, and the 3 stage approach proposed provides clear improvements over a number of methods (seemingly strong baselines) on relatively challenging benchmarks.\n\nWeaknesses:\n\nWhile I appreciate the exhaustive empirical study of the approach, the findings are not unexpected, in light of prior work. I see nothing technically wrong with this work, but nothing groundbreaking either.\n\nI think it would be valuable to reflect on the relationship of the proposed work to Dagger [1], which at its core follows a similar idea, albeit without the model-based rl aspects.\n\nSome minor comments. \n\nFigure 1. \"solves 21 hard robotics tasks...\" Could you define solve? How can we solve manipulation, pick and place or locomotion? Does a 53% success rate constitute \"solve\"? Perhaps a better, more precise description is needed. \n\nEQ 1 - can you define H as the planning horizon here? it is mentioned much later, but nice to have it here already.\n\nFig 4 - why is the oracle model from state so poor?\n\nRefs:\n[1] Ross, Gordon and Bagnell, A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning, AISTATS 2011\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, with good ablations and a seemingly reproducible approach.  \n\nThere has been prior work highlighting the benefits of pre-training policies with demonstration data [1], the importance of exploration in RL [3] and the need to balance policies against demos to avoid losing inductive biases [2], which limits the novelty and importance of the contributions made by this paper. At first glance the proposed approach seems somewhat obvious, but I appreciate the fact that the authors took the time to explore this question in detail. \n\n[1] Ross, Gordon and Bagnell, A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning, AISTATS 2011\n[2] Peters,  M\u00a8ulling and Altun, Relative entropy policy search AAAI 2012\n[3] Ecoffet,Huizinga, Lehman, Stanley, Clune, Go-Explore: a New Approach for Hard-Exploration Problems, ",
            "summary_of_the_review": "These results are potentially interesting to SOTA chasers in the environments tested and the paper describes a relatively clear and seemingly reproducible approach to do well in these settings. For this reason, I rate the paper as marginally above the acceptance threshold. I do not rate this higher, as I do not think the strength of contribution or novelty of this work warrants this.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3636/Reviewer_FmiB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3636/Reviewer_FmiB"
        ]
    },
    {
        "id": "I77962xpyKa",
        "original": null,
        "number": 2,
        "cdate": 1666588033083,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588033083,
        "tmdate": 1670211632884,
        "tddate": null,
        "forum": "JdTnc9gjVfJ",
        "replyto": "JdTnc9gjVfJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3636/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes several techniques to improve the sample efficiency of the model-based RL. Specifically, demonstrations are leveraged in a more clever way: first training the policy, then training the world models and the critic with pre-trained policy, finally training the policy in a model-based manner. Experiments are conducted on three task suites with necessary ablation study.",
            "strength_and_weaknesses": "Strength:\n\nThe paper has clear writing with necessary explanations about the motivations. A good summary of previous related works is provided. The experiments are solid and thorough in terms of the number of tasks. Ablation studies are also conducted to prove the effectiveness of the proposed techniques.\n\nWeakness:\n\nFig. 2(c) is unclear. The caption says the step (c) collects data using the model, however, the action $a$ is sent back to the environment. I assume this means interaction with the real environment also happens to collect data. So is the model only used for value estimation in TD-learning? If so, since TD-learning only requires the value of the next state, why are three steps plotted here? Please clearly indicate whether each $r$, $a$ and $z$ are from the prediction model or the rollout trajectories.\n\nWhat is the policy gradient? It seems a majority of the model updates in phase 2 and 3 are using TD-MPC method, however, the policy gradient as an important component in TD-MPC is not clearly explained. Only in Section 2 the paper briefly mentions that \u2018The policy \u2026 and is optimized to maximize temporally weighted Q-values\u2019. How specifically is the temporally weighted Q-values implemented and what\u2019s the explicit form of the policy gradient?\n\nWhy $\\pi$ at the seeding phase and $\\Pi$ at the third phase? It\u2019s not clear to me why the policies use different notations in the second and third phase. Please explain it in the paragraph.\n\nLack of experiments. For experiments, the most important baseline methods to compare for the proposed algorithm should be those falling in the category of model-based RL+IL setting as in Table 2. However, only the TD-MPC method is compared in the main results. The MWM and Dreamer-v2 should also be compared in the experiments across three task suites. The paper only reports partial results in Fig. 12. Moreover, given that the results in one (Meta-World) of the two environments show similar performances across three methods, it does not prove the strong performance gain of the proposed method. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Some details about the algorithm need to be explained well, as said in the Weakness part. \n\nThe quality of experiments and writing is good.\n\nI generally think the proposed algorithm is short in novelty. The oversampling of demonstration is a trivial technique. And the idea of training the critic and prediction models are quite straightforward. Moreover, there is a lack of theoretical insight into the algorithm.\n",
            "summary_of_the_review": "I think the paper proposes an effective method for improving the model-based RL, but the idea is not novel enough. Also, a lack of important experimental comparison is another major drawback. Details about the algorithm need to be modified and clarified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3636/Reviewer_9fcC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3636/Reviewer_9fcC"
        ]
    },
    {
        "id": "spa-vPrGQb",
        "original": null,
        "number": 3,
        "cdate": 1666675625448,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675625448,
        "tmdate": 1666675625448,
        "tddate": null,
        "forum": "JdTnc9gjVfJ",
        "replyto": "JdTnc9gjVfJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3636/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper demonstrates that a small number of expert demonstrations could assist model-based RL to learn faster. Model-based RL algorithms suffer from low sample efficiency, especially in complex environments due to exploration challenges. The proposed method alleviates this problem by continually reusing demonstrations to mix in desired state-action distributions. The proposed framework consists of three phases: (1) policy pretraining on a handful of demonstrations, (2) seeding phase where pretrained policy, with added exploration, is used to collect a dataset from the environment, in order to train world model and critic, (3) finetuning policy interactively with data from all phases. Evaluation on visual manipulation tasks with sparse rewards and locomotion tasks with dense rewards indicate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1. The motivation for improving sample efficiency is clearly explained.\n2. The paper presents experiments that indicated the effectiveness of the proposed method and provides extensive ablation studies on the importance of each designed phase.\n3. The paper is well-written and easy to follow.\nWeakness:\n1. The idea of combining expert demonstrations with RL lacks novelty, plenty of works have implemented this before.\n2. Under an unlimited sampling budget, will the proposed method and other baselines eventually converge at the same level?",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The proposed method and evaluation are clearly clarified.\n- Quality: The paper is well-written and structured.\n- Novelty And Reproducibility: The idea of incorporating expert demonstrations into RL is now general, and thus lacks novelty. This work can be easily reproduced with a public database.",
            "summary_of_the_review": "The evaluation of the proposed method is convincing. However, the main idea is not novel enough.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3636/Reviewer_6dVv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3636/Reviewer_6dVv"
        ]
    },
    {
        "id": "4Z2zQ9DcRM",
        "original": null,
        "number": 4,
        "cdate": 1666677336535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677336535,
        "tmdate": 1666684782979,
        "tddate": null,
        "forum": "JdTnc9gjVfJ",
        "replyto": "JdTnc9gjVfJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3636/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper identified three key ingredients to speed up the learning process for MBRL models. Specifically, the authors, building on TD-MPC, proposed a framework with policy pretraining, seeding, and finetuning with interactive learning, and showed great sample-efficiency improvement on a set of challenging visual controlling tasks.",
            "strength_and_weaknesses": "### Strength\n- The authors have provided a thorough comparison with prior work and have shown significant improvement on visuo-motor control tasks. Ablation studies also confirm the importance of each proposed training phase.\n\n### Weakness\n- minor\n    - Could the authors please elaborate more on your motivation? How is the expert demonstration helping the exploration bottleneck of model-based RL?\n    - TD-MPC uses real data for policy learning and conducts decision time planning. Meanwhile, other MBRL like Dreamer-V2 learns the policy in imagination rollouts, do you think the proposed framework can also benefit Dreamer-V2?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The results might be hard to reproduce.",
            "summary_of_the_review": "This paper identified three key ingredients to speed up the learning process for MBRL models. Experiments have shown significant improvement on a set of challenging control tasks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3636/Reviewer_Ycvo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3636/Reviewer_Ycvo"
        ]
    }
]