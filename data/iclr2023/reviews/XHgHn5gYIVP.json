[
    {
        "id": "lVrR4bKk6Y",
        "original": null,
        "number": 1,
        "cdate": 1666513041571,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513041571,
        "tmdate": 1666513041571,
        "tddate": null,
        "forum": "XHgHn5gYIVP",
        "replyto": "XHgHn5gYIVP",
        "invitation": "ICLR.cc/2023/Conference/Paper1200/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work aims at improving query-based video segmentation based on domain generalization. Specifically, this work focuses on solving the challenges of cross-modal settings, where domain shift exists at both the text and visual levels. To mitigate these challenges, this work proposes the query-guided feature augmentation method and uses AdaIN to modulate the text query.",
            "strength_and_weaknesses": "Strength\n+ A good attempt to improve domain generalization of the query-based video segmentation methods.\n+ The experimental results show that the proposed method outperforms the competing methods.\n\nWeaknesses\n+ This work proposes to augment training data with the proposed QFA. Only the background information is replaced during the augmentation. However, the foreground information also varies significantly for complex environments. For instance, as shown in Figure 1, the targets in the source and target domains have different scales. \n+ I am still confused about the detailed motivation for using AdaIN. It is originally used in style transfer, and this work seems to use it to mitigate the \u201cstyle\u201d of the query. However, AdaIN requires supervised training to present different styles, while the target domain data is not available during training for this task.\n+ The technical novelty of this paper is not up to the standard of ICLR. I believe that model performance is (marginally) improved but combining a data augmentation method and AdaIN and then applying them to the segmentation task is not appealing to me.\n+ The presentation should be improved. For instance, in the first sentence of the introduction, it seems that \u201cbyGavrilyuk\u201d should be separated into two worlds by spacing. Similarly, \u201cAdapINHuang\u201d in the last paragraph of the introduction.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Needs Improvement.\nQuality: Needs Improvement.\nNovelty: Somewhat novel.\nReproducibility: Reproducible.\n",
            "summary_of_the_review": "This work views query-based segmentation as a cross-domain generalization problem. It proposes a data augmentation scheme and uses AdaIN to improve model performance. The technical novelty of this paper is not up to the standard, and the motivations are not clear enough. In addition, the presentation needs further improvements.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1200/Reviewer_XKxN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1200/Reviewer_XKxN"
        ]
    },
    {
        "id": "snRREJwSZs6",
        "original": null,
        "number": 2,
        "cdate": 1666591558397,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591558397,
        "tmdate": 1669139812117,
        "tddate": null,
        "forum": "XHgHn5gYIVP",
        "replyto": "XHgHn5gYIVP",
        "invitation": "ICLR.cc/2023/Conference/Paper1200/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work addresses the task of multi-modal DG where each modality has to cope with its own domain shift. Specifically, the specific scenario of query-based video segmentation is studied to better advance the generalization ability of the model in the multi-modal situation. The authors observe that actions belonging to the same type of actors in different domains share similar representations in the query-video latent space, and the main reason for the performance degradation is that visual background and contextual complexity vary widely across different domains. The authors propose a Query-guided Feature Augmentation (QFA) module that uses the attention scores between query and video frames to distinguish query-related foreground regions from unrelated background regions for synthesizing novel visual features. AdapIN is adopted to remove the impact of style in attention map during training, and gradually introduce statistics from other samples to help the model learn robust cross-modal relationships. Extensive experiments validate that the proposed model has better generalization ability.",
            "strength_and_weaknesses": "Strength:\n1.\tThis work addresses multi-modal DG problem in query-based video segmentation task, which is still an untouched field. Multi-modal DG is meaningful and has its specific challenge. The authors\u2019 effort on handling the multi-modal DG problem is encouraged.\n2.\tThe authors observe that actions belonging to the same type of actors in different domains share similar representations in the query-video latent space, and the main reason for the performance degradation is that visual background and contextual complexity vary widely across different domains. Such observation is helpful.\n3.\tThe proposed QFA and AM-AdaIN are reasonable.\n4.\tExtensive experiments validate the effectiveness of the proposed model.\n\nWeakness:\n1.\tThe performances are over-claimed that \u201cFrom the results we can see that our method outperforms the baseline model by a large margin on all three generalization tasks, demonstrating the effectiveness of our proposed QFA and AM-AdaIN modules.\u201d In 4.3 Main result. The performance of A2R in Table I is just comparable to existing methods.\n2.\tThe proposed AM-AdaIN module is not illustrated in Fig. 2. It is hard to understand how it works in the proposed method.\n3.\tCan we replace the L_const into triplet loss, rather than Moco. Besides, the \\tau in L_const is not addressed.\n4.\tSome typos:\na)\tBold \u201cc \u201d in c_{hw}^k of \\sigma(C_k)\nb)\tC\u2019_K -> C\u2019_k in Eqn. 9\nc)\tInconsistence between A2R and A2D Sentences to RVOSpart in Table I and Table II.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. The novelty of this work is enough.",
            "summary_of_the_review": "Please see the Strength and Weakness.\n\n---------\n\nAfter reading the author response and other fellow reviewers' comments, I see that some concerns have been addressed, while others still remains unsolved. Based on the technical content and the author response, I decide to revise the score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1200/Reviewer_yPJV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1200/Reviewer_yPJV"
        ]
    },
    {
        "id": "bV2kOCsRXvP",
        "original": null,
        "number": 3,
        "cdate": 1666655202554,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655202554,
        "tmdate": 1666655202554,
        "tddate": null,
        "forum": "XHgHn5gYIVP",
        "replyto": "XHgHn5gYIVP",
        "invitation": "ICLR.cc/2023/Conference/Paper1200/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies cross-modal domain gereralization on the query-based video-segmentation task. Two modules, i.e., Query-guided Feature Augmentation (QFA) and attention map adaptive instance normalization (AM-AdaIN) are proposed for the multi-modal generalization task. Experiments are conducted on three domain generalization tasks for query-based video segmentation.",
            "strength_and_weaknesses": "Strengths:\n1. The studied domain generalization task, i.e., multi-modal generalization for query-based video segmentation, is novel and has not been investigated before. The idea is interesting.\n2. Two modules are designed to address this challenging task, i.e. QFA and AM-AdaIN. This method achieves better results than the compared baslines. Ablation studies seem effective and sufficient.\n3. The presentation is generally good.\n\nWeaknesses:\n1. Motivation is not clear enough. Some source-only (baseline) examples would better show the requirement of domain generalization for query-based video segmentation.\n2. The technical contribution is relatively limited. Most components, e.g. both loss functions, are based on existing techniques.\n3. I expected there are at leaset two source domains so that domain-invariant representations can be learned. I do not understand why only one source domain is used. This is also domain generalization but not the typical setting.\n4. Baselines and some implementation details are not clear enough. For example, the compared baselines are mainly single-modal based methods. How to use them for the multi-modal generalization task?\n5. The analysis on why the proposed method performs better in the experiment part is weak.\n6. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The task is novel, but the technical contribution is incremental, resulting limited novelty and originality. The proposed method is clearly presented in general. Some implementation details, especially how the baselines are implemented for multi-modal generalization setting, are not given. The code is not provided. I doubt exact reproducibility.",
            "summary_of_the_review": "Novel domain generalization task but limited technical contrbution, good experimental results but insufficient analysis and insights behind the results, and generally fluent presentation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1200/Reviewer_NKmv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1200/Reviewer_NKmv"
        ]
    },
    {
        "id": "eXEHtrThgK5",
        "original": null,
        "number": 4,
        "cdate": 1666665439642,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665439642,
        "tmdate": 1669056864168,
        "tddate": null,
        "forum": "XHgHn5gYIVP",
        "replyto": "XHgHn5gYIVP",
        "invitation": "ICLR.cc/2023/Conference/Paper1200/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "1. The paper attempts to introduce domain generalization to solve query-based video segmentation.\n2. The paper proposes QFA and AM-AdaIN modules to process query and video information, respectively.\n3. The authors conduct experiments on the A2D Sentences, Refer-Youtube-VOS (RVOS), and J-HMDB Sentences datasets.",
            "strength_and_weaknesses": "Strength:\n1. The research question is interesting.\n2. The author did some experimental comparisons, and the results show that the proposed method has improved on some baselines.\n\nWeaknesses:\n\n1 The paper experiment is not sufficient.\n\n-a. Does Table 1 compare other video segmentation methods? Does Table 1 compare with other domain adaption methods? Why does the title here compare with other generation methods? \n\n-b. From Table 2, the improvement of each module is minimal. Combining Tables 1 and 2, I doubt whether the motivation for introducing domain adaption is reasonable. I'm concerned that the author is simply doing a combination of methods and ignoring the rationale of the research motivation.\n\n2. Insufficient analysis of the rationality of domain adaption. \n\n-a. Does the paper have visualization results of domain adaption? I want to know what domain adaption means specifically for video segmentation tasks.\n\n3. The details of the method of the thesis are not clear. \n\n-a. How is Equation 1 trained? How does it relate to other loss functions (Equations 6 and 11)? \n\n-b. In Figure 2, what is the output of the augmented feature? Where did he enter the part of a? \n\n-c. Is the domain adaption of this paper merely from data augmentation?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity:\nSome details are not clear.\n2. Quality:\nIt can also be improved.\n3. Novelty:\nA bit innovative.\n4. Reproducibility:\nKind of difficult to reproduce.\n",
            "summary_of_the_review": "The paper lacks analysis and experimentation on research motivation. Some details of the method are unclear. I hope the author answers my question in the response. I'll change the score based on the author's reply.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There are no Ethics Concerns.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1200/Reviewer_HssY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1200/Reviewer_HssY"
        ]
    }
]