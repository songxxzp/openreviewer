[
    {
        "id": "pE422aNdpr",
        "original": null,
        "number": 1,
        "cdate": 1666365555683,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666365555683,
        "tmdate": 1669227178507,
        "tddate": null,
        "forum": "KaeYRGTaODt",
        "replyto": "KaeYRGTaODt",
        "invitation": "ICLR.cc/2023/Conference/Paper1950/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method for policy transfer in a multi-agent reinforcement learning setting. The method focuses on transferring to new population size. It learns task representations that capture relationships among source tasks. On an unseen task, a new representation is learned as a linear combination of the learned representations. The unseen task's representation is then plugged into the Q-learning population-invariant policy.\n\nThe method is evaluated on two multi-agent benchmarks. It outperforms baseline methods on unseen tasks. The task representation enables higher performance after fine-tuning the policy. It also effectively leverages the multi-task learning setting.",
            "strength_and_weaknesses": "**Strengths**\n* The paper is clearly written. The method is well-described. Experiment hypotheses are thoroughly constructed. \n* To my best knowledge, the method seems to be novel, although I am not too familiar with the multi-agent RL literature. It is also technically sound. \n\n**Weaknesses**\n\nMethod\n* The major weakness is that learning a new task representation requires collecting diverse trajectories on the new task. The paper uses the policy trained on source tasks to collect trajectories on a new task. The policy thus needs to generalize decently to the unseen tasks to generate interesting trajectories. I think this point needs to be emphasized in the paper. \n* Initializing the representations of the source tasks as orthogonal vectors ignores the relationships between them. \n* The learned forward model may be used for planning but only the task representation is used. \n* This is not really a weakness. But the method seems to solve more general problem than generalizing to new population size. Based on the current formulation, I believe it can also be applied to a single-agent setting. Is there any particular reason for formulating and showcasing it on an multi-agent setting? \n\nExperiments\n* Results on the MPE tasks are not convincing. The task representation makes very little difference (first 3 rows of tables 4 and 5). The authors incorrectly claim \"MATTAR...  significantly outperforms the baselines and ablations\". ",
            "clarity,_quality,_novelty_and_reproducibility": "Method\n* The notion of \"task\" is not well-defined in section 2. Source and target tasks in the experiments seem to only differ in the population size. The environment dynamic (for each agent) and the reward function remain the same.  \n* Why does backprop through the representation result in small-norm vectors?\n\nExperiments\n* Can you explain why the performance of MATTAR on MPE is not very different from \"0 task rep\" and \"w/o task rep\"?\n* Why does performance of MATTAR drop significantly on SMAC **source** tasks when zero-ing out task representation? Why does this phenomenon not happen on MPE?\n* Can you show the accuracy of the learned forward model? How good is it?",
            "summary_of_the_review": "Overall, I like the proposed method as it is very general. But some of results are not convincing and the authors make false claims instead of explaining the observed phenomenon. The paper is borderline and I am learning towards rejection. \n\n=======After rebuttal=======\n\nI thank the authors for giving very detailed and comprehensive responses. I decided to raise the score to 6, hoping that the authors will incorporate their responses into the final version, especially the explanation about the ineffectiveness of the learned task representation on MPE. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1950/Reviewer_oPef"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1950/Reviewer_oPef"
        ]
    },
    {
        "id": "rsbGe0Ccz87",
        "original": null,
        "number": 2,
        "cdate": 1666676195021,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676195021,
        "tmdate": 1666676195021,
        "tddate": null,
        "forum": "KaeYRGTaODt",
        "replyto": "KaeYRGTaODt",
        "invitation": "ICLR.cc/2023/Conference/Paper1950/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies transfer learning in multi-agent setting, where existing works based on graph neural networks or attention mechanisms achieves generalization across tasks implicitly through the generalization of neural network function approximations. The authors propose to explicitly model the task relationships by learning a task embedding and the resulting policies will depend on the task representations, which can be inferred for new tasks given a few trajectories. Experimental results demonstrates the effectiveness of the proposed framework.",
            "strength_and_weaknesses": "Strength\n- The proposed framework seems novel and reasonable to the problem considered.\n- Nice presentation with many useful figures and intuitive discussions so that the paper is easy to follow.\n- Promising experimental results.\n\nWeaknesses/Questions\n- Can you provide some theoretical characterization of the proposed framework? Transfer learning for multi-agent setting is very complicated and challenging, due to varying population size, input dimension, and even different action spaces. For example suppose source domain action space is a continuous interval [0, 1] while target domain action space is finite {0, 1}, then it is guaranteed that the two tasks are not transferable in some scenarios. For example, in single-agent case, which is simpler than multi-agent, [1] studies the alignment of two MDPs.\n- When do you expect the transfer to unseen tasks will succeed or fail? How many samples are needed to reliably learn a task representation for unseen task. It seems to achieve this kind of generalization across tasks, you may need datasets from a lot of different tasks, since the latent space for task representation is quite large and many values can be undefined.\n- Does the proposed framework also apply to competitive/mixed multi-agent system?\n\n[1] Domain Adaptive Imitation Learning",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above for detailed comments.",
            "summary_of_the_review": "Overall I think this is a well-written paper with a reasonable framework to capture the task relationships for multi-agent transfer learning. The paper could be improved with some theoretical characterizations of the problem and the proposed framework.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1950/Reviewer_AfTn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1950/Reviewer_AfTn"
        ]
    },
    {
        "id": "ezZXAB6d_B",
        "original": null,
        "number": 3,
        "cdate": 1667358379394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667358379394,
        "tmdate": 1667358379394,
        "tddate": null,
        "forum": "KaeYRGTaODt",
        "replyto": "KaeYRGTaODt",
        "invitation": "ICLR.cc/2023/Conference/Paper1950/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper puts forth a fully-cooperative multi-agent/task algorithm, MATTAR, that can be used to transfer knowledge learned from source tasks to improve learning unseen tasks at test time. MATTAR learns a high-level mapping from task representations to state, observation, and reward function parameters and combines this with a policy learning approach that allows for inputs and outputs of varying sizes. The paper evaluates MATTAR on multiple benchmarks against multiple baselines and shows that MATTAR generally attains superior performance in terms of win rate, sample efficiency, and feasibility (i.e., winning at all).",
            "strength_and_weaknesses": "Strengths\nThe paper motivates and addresses an important AI problem: fully-cooperative multi-agent learning, which can be and has often been denoted as just multi-task learning for several years (Parisotto et al., 2015). \n\nMATTAR is simple, easy-to-implement, and parametrizes a model that simultaneously outputs the next state, next observation, and global reward. This algorithm can handle varying input and output sizes, which is often the case across differing tasks, using a population-invariant network (PIN). However, the use of a high-level model that maps task representations to model parameters is not new nor is the use of PINs new, as the paper itself claims on page 4. In fact, it is unclear what the paper's contribution regarding PINs is, so the paper would benefit by clearly explaining and justifying this. Perhaps this is in Appendix G, but regardless, I apologize if this is clearly stated in the paper and I missed it.\n\nThe paper graciously provides a case study on a toy problem to help readers gain an intuitive understanding of how MATTAR works, but this case study doesn't include visualization of trajectories to task 3, even though it corresponds to one of the two non-zero learned coefficient components, and it omits the fact that from Figure 3(b), it is also typical behavior in task 4 for the agent to move left when justifying why the agent moves up first in moving towards the unseen task goal. I assume that the agent moves up instead of left first actually because unseen goal is further up than left from the initial agent region or because the trajectory from only one rollout or one policy is being shown. \n\nThe paper is easy to read, clear, and the Figures, Tables, etc. are great at capturing and explaining salient points the paper makes.  It does a great job of providing enough information on the experimental design to reproduce the experiments.\n\nThe paper evaluates MATTAR on multi-task settings on two benchmarks, SMAC and MPE, which have varying degrees of difficulty. Experiments are carried out with five random seeds, which in some cases is sufficient, but in certain cases here is insufficient. The paper claims that \"... the super hard map 3s5z_vs_3s6z and [sic] cannot be solved by learning from scratch,\" which is too strong of a claim generally, but especially if only 5 seeds were run.\n\nThe paper shows that MATTAR generally outperforms multiple baselines on multiple multi-task transfer learning setups. However, I'm unconvinced that the baselines are competitive, as UPDeT doesn't even aim to address the same problem setting as MATTAR does (single source task vs. multi-source task transfer) though the paper does aim to provide a fair comparison by introducing UPDeT-b. Regardless, there exist a plethora of multi-source task transfer learning algorithms (often i.e., fully-cooperative multi-agent learning algorithms) that may be better suited to compare against. The paper would also benefit by evaluating MATTAR on a set of more diverse benchmarks than just closed, game or toy-type settings, such as Starcraft (SMAC) and MPE, especially one or two real-world settings so that the research community has a better idea of how MATTAR performs generally.\n\n---\n\nParisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. \"Actor-mimic: Deep multitask and transfer reinforcement learning.\" arXiv preprint arXiv:1511.06342 (2015).",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and reproducibility are excellent. The quality of the algorithm is good, but the quality of the experimental design is lacking, as explained in Strengths and Weaknesses (S&W). There does not seem to be much novelty in the algorithm or in the experimental design, which is also explained in S&W.",
            "summary_of_the_review": "The paper clearly motivates and addresses an important AI problem. It proposes the algorithm MATTAR and shows that it generally outperforms what it believes to be competitive baselines on two game/simulation benchmarks extended to multi-task transfer learning settings.\n\nHowever, I believe that this paper falls far short of acceptance due to the lack of novelty, limited experimentation, poor experimental design, and lack of analysis of results.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1950/Reviewer_dhhQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1950/Reviewer_dhhQ"
        ]
    },
    {
        "id": "00Dt6bCJTyg",
        "original": null,
        "number": 4,
        "cdate": 1667427060885,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667427060885,
        "tmdate": 1667427060885,
        "tddate": null,
        "forum": "KaeYRGTaODt",
        "replyto": "KaeYRGTaODt",
        "invitation": "ICLR.cc/2023/Conference/Paper1950/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents MATTAR, which learns the common structure of tasks and transfers this knowledge to unseen tasks. Specifically, the framework first learns the representation explainer based on the pre-defined and mutual orthogonal task representation vectors. Then, during the transfer phase, a task representation vector for a new task is learned for adaptation. MATTAR also includes the task policy learning component that leverages PIN for addressing the varying number of agents across tasks. Experiments in the SMAC and MPE domains demonstrate the effectiveness of MATTAR's transfer capability. ",
            "strength_and_weaknesses": "**Strength:**\n1. The paper is generally well-written and conveys the main methods well. \n2. Experiments in Sections 4.1-4.3 show the benefits of MATTAR against competitive baselines. The ablation studies and related empirical analyses improve the understanding of MATTAR. \n\n**Weaknesses:**\n1. Novelty can be limited because 1) the idea of learning the small-size task parameters (i.e., the weight vector $\\mu$) for adaptation is similar to Zintgraf et al. (2021) and 2) the task policy learning component is based on PIN. \n2. I agree that the idea of pre-defining the task representation vectors as mutual orthogonal vectors and learning linear weight vectors is new and interesting. Because this idea is the main contribution, this paper can benefit from having a comparison and discussion (i.e., pros and cons) w.r.t. this idea. For instance, compared to Zintgraf et al. (2021), linear weight learning has advantages, including intuitive understanding and well-formed representation space. However, this linear weight learning can have disadvantages as mentioned in the conclusion (future work). Not necessary, but I wonder whether there can be a small experiment comparing the ideas between the linear weight learning and non-confined learning (similar to Zintgraf et al. (2021)). \n3. Related to the weakness #2, if all source tasks are set up to be identical, can the mutual orthogonal vector idea hurt the performance?\n4. MATTAR applies fine-tuning for unseen tasks. I wonder it would be more fair for baselines in Table 1-5 to apply fine-tuning. \n\n**Minor:**\n1. In Section 2, for completeness, source tasks $\\{S_i\\}$ and unseen tasks $\\{T_j\\}$ can be mathematically defined using $G$ (Dec-POMDP) with different transitions and reward functions.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The paper is generally well-written and conveys the main insights well.  \n**Novelty:** The idea of pre-defining the task representation vectors as mutual orthogonal vectors and learning linear weight vectors is new in the MARL context. However, as mentioned in weakness #1, some ideas overlap with prior works.  \n**Reproducibility:** The source code is provided in the supplementary material to reproduce the results.  ",
            "summary_of_the_review": "Overall, I have a positive evaluation of this paper. While the novelty can be a concern, the experimental results are solid and detailed, and the paper conveys the main findings clearly. Therefore, I initially vote for 6 (marginally above the acceptance threshold) and will make a final decision on the recommendation after the authors' response.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1950/Reviewer_U9UT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1950/Reviewer_U9UT"
        ]
    }
]