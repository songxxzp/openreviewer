[
    {
        "id": "4XJxxbPNCeh",
        "original": null,
        "number": 1,
        "cdate": 1666637069704,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637069704,
        "tmdate": 1666637069704,
        "tddate": null,
        "forum": "WgbcOQMNXB",
        "replyto": "WgbcOQMNXB",
        "invitation": "ICLR.cc/2023/Conference/Paper4745/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors evaluate pretrained large LMs on the binary implicature resolution. The results reveal that there is still a big gap between human performance and LM's on this task, indicating the challenge to interpret the language in context of the LMs. They also show that increasing the model size and prompt size (through few-shot prompting) is unlikely to close the gap, especially for the samples that require more context understanding.",
            "strength_and_weaknesses": "Strength:\nThe paper presents an interesting result of pretrained LMs performance on the binary implicature resolution task. While we are seeing promising performance with the large LMs, especially with the newest ones like OpenAI's, there is still a gap with human performance on this implicature resolution task.\n\nWeaknesses:\nThe claim that large LMs are not zero-shot communicators is a little strong because it is only based on a single dataset. There will be questions regarding whether this dataset is representative? the dataset size (600 samples) is enough or not? and even the quality of the samples/labels?\n\nFor the samples that require context understanding (\"Particularised\"), authors can show more case studies and analyze the predictions on these samples. Also, I think it is better to plot the absolute accuracy scores in Figure 3 to illustrate whether the performance on this type of samples will increase with the model size.\n\nThe claim that \"increasing the model size and prompt size is unlikely to close the gap\" seems not true for the recent model from OpenAI. Evidence in Figures 2 and 4 show that we are seeing performance gain when going to bigger model or having more few-shot examples in the prompt.\n\nFurthermore, the current paper does not have enough analysis to understand why the current LM models fail and hence suggest directions for improvement.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is clearly presented, and the paper is easy to read. The appendix section provides more details about the evaluation results. The result seems reproduceable without much effort.",
            "summary_of_the_review": "Although this work reveals an interesting result when evaluating LMs for conversation context understanding, this evaluation is on a single test set, hence it is not strong enough to make a claim on the general effectiveness LMs. It is also expected to see more analysis to understand why the current LMs model fail on this test and hence have some potential directions for improvements.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4745/Reviewer_kGh3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4745/Reviewer_kGh3"
        ]
    },
    {
        "id": "qAlorKykF_C",
        "original": null,
        "number": 2,
        "cdate": 1666641330303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641330303,
        "tmdate": 1666641330303,
        "tddate": null,
        "forum": "WgbcOQMNXB",
        "replyto": "WgbcOQMNXB",
        "invitation": "ICLR.cc/2023/Conference/Paper4745/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors construct a test to examine LLM's ability to understand certain types of conversational implicatures by asking the models to convert non-direct answer to yes/no questions to yes/no. They found that intractable GPT-3 models perform much better than other baselines they compared but still a lot worse than human performance. They also showed that scaling doesn't help on this task. In-context learning helps the task up to 5 examples. ",
            "strength_and_weaknesses": "# Strength:\n\n1. It is interesting to understand how LLM understand implicatures which is acquired through social learning for humans. The contrastive task design is neat and effective. \n2. The experiments make sense and the results are informative. \n\n# Weakness\n\n1. Given the small size of the dataset, some of the results for smaller categories may not be significant. It would be nice if the authors could calculate the uncertainty for Figure 3 & 4. \n2. The title is too eye-catching-oriented and a little disconnected to the main claim of this paper. I would suggest use \"Measuring implicature understanding\" or similar ones to more clearly state the purpose of this paper. Communication is a much broader concepts than implicature. Not being able to understand implicature than average humans doesn't mean LLMs cannot communicate. ",
            "clarity,_quality,_novelty_and_reproducibility": "This task and experiment design in this paper is very straightforward, and it is presented clearly. Given access to the models used in this paper, I think the results should be reproducible. ",
            "summary_of_the_review": "Good paper on analyzing implicature resolution of LLMs. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4745/Reviewer_J3vy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4745/Reviewer_J3vy"
        ]
    },
    {
        "id": "94TscTcOjb",
        "original": null,
        "number": 3,
        "cdate": 1666794852127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794852127,
        "tmdate": 1666794852127,
        "tddate": null,
        "forum": "WgbcOQMNXB",
        "replyto": "WgbcOQMNXB",
        "invitation": "ICLR.cc/2023/Conference/Paper4745/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper evaluates whether or not large language models understand implicature. They create (automatically) a dataset that allows them to evaluate pre-trained models\u2019 understanding of this phenomenon. They evaluate LLMs pre-trained on large text corpora and also ones that are fine-tuned with RL feedback to see the differences in models.\n",
            "strength_and_weaknesses": "Strengths\n\n1. This paper is well-motivated and the evaluation is formulated clearly and concretely.\n2. The authors spend time evaluating all models in different ways (e.g., relative accuracy, comparison across model sizes and classes, measuring the effect of in-context examples and so on) which is more comprehensive than most evaluations and helps understand model performance\n3. The authors report variance with the accuracy results which is good to see\u2014however it looks like the variance is exceptionally high? \n4. The authors also measure the effect of a larger number of samples (k), whether or not the wording of samples in the prompt affects performance, and they break down performance by example type which allows a nice and comprehensive analysis\n\nWeaknesses\n\n1. The framing of this paper (and the title) are misleading. This paper focuses on evaluating implicature, which is a very important phenomena for communication, however is not everything that is needed for (zero-shot) communication and the title implies that this paper focuses on all aspects of communication. There are other aspects of communication/intent (e.g., assertives, hedges, subjective words) and so on that are nuances aspects of communication, as well as other important pragmatic effects, and this paper does not touch upon any of those.\n2. The title also does not currently contain information/the word \u201cimplicature\u201d which is misleading, so I urge the authors to reframe/change it to remove \u201czero-shot communicators\u201d (since this is not something the paper tackles) and add \u201cevaluating implicatures\u201d (which is what the paper is largely about)\n3. There are several citations re: language models and implicature that are missing. They are: \u201cLinguistic models for analyzing and detecting biased language. Recasens, M., Danescu-Niculescu-Mizil, C., and Jurafsky, D. (2013)\u201d, \u201cMore than words: Syntactic packaging and implicit sentiment. \u200b\u200bGreene, S. and Resnik, P. (2009)\u201d \u201cWas it \u201csaid\u201d or was it \u201cclaimed\u201d? How linguistic bias affects generative language models. Patel and Pavlick (2021)\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and seems reproducible (assuming the dataset is released). Small clarification questions and typos are added below.\n\nClarification questions:\n\n1. Page 4. \u201c..the order in which these examples are presented matters.\u201d \u2190 can the authors clarify why this is the case, since there is no explanation given.\n2. On naming \u201cnatural\u201d vs. \u201cstructured\u201d prompts. This seems somewhat non-intuitive\u2013can the authors clarify this naming strategy?\n3. Why are the fine-tuned models called \u201cinstructable\u201d models?\n\nSmall typos:\n\nPage 4. \u201c.. change in protocol allows us to control for two two sources..\u201d\n\n",
            "summary_of_the_review": "This paper studies an interesting problem however the framing of the paper is misleading.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4745/Reviewer_uf2H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4745/Reviewer_uf2H"
        ]
    },
    {
        "id": "b9UjIWkqaS",
        "original": null,
        "number": 4,
        "cdate": 1666827645523,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666827645523,
        "tmdate": 1670885523782,
        "tddate": null,
        "forum": "WgbcOQMNXB",
        "replyto": "WgbcOQMNXB",
        "invitation": "ICLR.cc/2023/Conference/Paper4745/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Main contributions:\n1) Identify implicature understanding as a useful benchmark of LLM's communication ability.\n2) Design a protocol for comparing implicature understanding of LLMs versus that of humans.\n3) Roll-out the protocol on a host of existing LLMs. Providing a deeper analysis of (a) how different properties of LLMs (e.g., instrucability or size) affect their performance, (b) which types of implicatures are harder for LLMs to understand, and (c) the effects of multi-shot learning on performance.\n\nBroadly speaking, implicature refers to the act of saying one thing that implies another. The authors observe that any entity (or model) that is claimed to \"understand language\" or \"be able to communicate\" must, in particular, understand implicature. Continuing the wide range of works aimed at understanding and evaluating LLMs, the authors design a protocol for evaluating an LLM's ability to understand implicature.\n\nThe protocol entails prompting an LM (or human) with text of an implicature in the form of a question and answer; the task is to classify whether the answer means \"yes\" or \"no\" (logits of any token other than yes/no are masked, I think). Performing better-than-random (50%) at this task necessitates implicature understanding. The authors use six prompt templates, some *structured* (like script) and some *natural* (like prose).\n\nUsing this protocol, the authors compare the performance of various LLMs and benchmark these against humans. Overall, the authors find that the zero-shot performance of most LLMs is fairly close to random (around 60%), with the best models achieving around 72% accuracy -- compared to the mean 86% of humans. From this, the authors conclude that LLMs fail at zero-shot implicature understanding. Few-shot evaluation sees an increase to about 80%.\n\nIn addition to these main findings, the authors perform several refined anlyses:\n - breakdown by prompt type, showing that structured prompts are easier than natural prompts, but the overall ranking among models is not sensitive to prompt type\n - breakdown by implicatures type, showing that generalized implicature is easier than particularized implicature.\n - in *k*-shot evaluation, how performance scales with *k*, showing diminishing returns for $k \\geq 5$.\n - how performance scales with model size (the conclusion here was unclear to me).",
            "strength_and_weaknesses": "**Pros**\n - Rigorously evaluating implicature understanding is an interesting contribution to a timely and important area, namely, LLMs understanding of language and communication abilities.\n - The protocol is extremely simple. This might seem like a Con, but often times identifying simple ideas and executing them correctly requires much heavy lifting. The authors put in the work to present their idea clearly: the paper is well-written and concepts are clearly defined and demarcated.\n - Overall, the empirical evaluations are easy to understand and fairly complete (Cons 2-5, below, notwithstanding). The in-depth breakdowns shed light on the general claim that LLMs fail at zero-shot implicature understanding.\n - I commend the authors for identifying a subset of their experiments that can be run without access to paid APIs or large compute.\n - The authors survey existing literature on implicature.\n - Cons 6-8 aside, the findings of empirical evaluations are presented in a careful and complete manner. The experiments seem reproducible (though I do not have API access or compute to attempt a full reproduction).\n\n**Cons & suggestions for improvement**\nItems (6-8) refer to the code and reproducibility, and items (9-14) are minor comments.\n\n 1. The title is too general -- it's not necessary to replace the task of implicature understanding with the much more general task of \"communication\". If anything, you can drop \"Zero-Shot\". Concretely, I'd suggest something along the lines of \"Large Language Models Do Not Understand Implicature\" or, less accurate but more accessible, \"Large Language Models Do Not Understand Implication\".\n 2. The perfomance gaps of LLMs and humans are about 14% in zero-shot evaluation, which decreases to 6% in few-shot evaluation. Can you quantify what these gaps mean? How much better is 6% than 14%? Given that humans perform imperfectly in this task (86%), I imagine there is a some *x%* threshold past which perfomance is \"essentially\" human. How close is this threshold to $x=86$? This likely requires a new set of experiments, but seems an important step forward that I would suggest as future work. For example, a Turing test in which a distinguisher prompts a black box with an implicature (or sequence of these) and must distinguish an LLM from another human.\n 3. In Table 1, the differences between Davinci-001 and Davinci-002 are within each others' standard deviation. Does this affect the statistical significance / validity of the claim that Davinci-001 outperforms Davinci-002 (page 6)? Not much changes in the paper if you say that the Davincis are indistinguishable, but I find this glaring and worth a brief discussion in the paper.\n 4. The identity of the asker/answerer among the pairs (Esther,Juan), (Karen,William) and (Bob,Alice) may affect the performance of the models: for example, a question asked by a Karen may be interpreted differently than a question asked by a William. I suggest adding a small ablation permuting the order of the pairs to test this.\n 5. Why were human subjects only shown Prompt #2, rather than a mixture of all six prompts? As the authors find, structured prompts are easier for LLMs than natural ones. It is likely too late to test the effect of multiple vs. single prompt types on human performance, but I would expect to find an explicit discussion of this choice in the text.\n 6. **Code and reproducibility.** No Python version is specified in the readme, and the code is incompatible with any version other than the intended one. Which version should be used for reproducing the results? I managed to infer that it\u2019s 3.9 using binary search:\n    - Your use of the walrus operator (\u2019:=\u2019) implies Python\u22653.8.\n    - Your requirement of torch==1.10.2 implies Python<3.10.\n    - Your use of union (\u2019|\u2019) operator implies Python\u22653.9.\n 7. Even after finding the right Python version, the code does not run out of the box. Specifically, I had to manually downgrade protobuf to 3.20 (pip install -Iv protobuf==3.20). You may want to add this to your requirements.txt.\n 8. You should add a lightweight and easy-to-run sanity test. For example, an argument-less script that that trains a model on tiny data, validates on the same data, and asserts very high accuracy. While the small experiment from the readme did reproduce for me, it took several minutes to run; it's good to have something that fails faster than that in case things aren't working.\n 9. **Minor comments.** In the paragraph entitled \"On Prompting\" (page 8), the authors state that \"There is a narrative around large language models that if they fail a task, it might that the prompt was not the right one.\" This statement should be supported by citations (just a few examples would suffice).\n 10. A footnote next to a punctuation mark comes after the mark, rather than before it. In page 5, move footnote 3 to come after the period.\n 11. Although Wittgenstein is cited in the appendix, I expected to find him cited alongside Grice and Huang in page 1, supporting the statement that meaning is determined by contexts, beliefs and social institutions. Not a big deal, I'll leave this decision up to the authors.\n 12. Consider changing the title of Appendix B to \"Background on Implicatures\" or just \"Implicatures\".\n 13. On page 19, the authors write that Bach argues that \"conventional implicatures are [...] instances of something else\". Can you give an example of what this something might be?\n 14. It would be nice to conclude Appendix B with a paragraph explaining in what way this work is informed by the literature mentioned in this section. What is the connection between conventional/conversational implicatures (in the appendix) and generalized/particularized implicatures? If conventional/conversational implicatures do not make an appearance in the rest of the work, why the long discussion of them?",
            "clarity,_quality,_novelty_and_reproducibility": "These are all covered in the sections above and below.",
            "summary_of_the_review": "This paper offers a significant contribution to an important area of research. Overall, it is written well, its novelties are well-defined and explored, and the main claims are sufficiently supported by empirical findings. Do not take the many suggestions I made above as a serious negative sign; only a handful of them noticeably impact the paper. I strongly suggest the authors to make the experiments more easily reproducible by fixing Cons 6-8.\n\nEDIT: The authors' rebuttal adequately addresses my concerns (primarily reproducibility and the title) and I have increased my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4745/Reviewer_pKwN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4745/Reviewer_pKwN"
        ]
    }
]