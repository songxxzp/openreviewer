[
    {
        "id": "IAZ5yPPBeYb",
        "original": null,
        "number": 1,
        "cdate": 1666534617747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666534617747,
        "tmdate": 1668801371156,
        "tddate": null,
        "forum": "3VO1y5N7K1H",
        "replyto": "3VO1y5N7K1H",
        "invitation": "ICLR.cc/2023/Conference/Paper4191/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new Stable Doubly-Robust (SDR) method for learning a recommender system from Missing Not At Random (MNAR) feedback data. As compared to previous efforts on DR learning from MNAR data, current work provides theoretical guarantees of stability, specifically against very small propensities. Previous works like Inverse Propensity Scoring (IPS), and DR are prone to instability when certain items have very low propensities in the log data. Due to the inverse term, a very low propensity can cause very high variance. \n\nThe authors also propose a 'pipeline' for learning, where the imputation model, propensity model, and SDR model are trained serially/cyclically, to ensure stability in the learning process. Theoretical bounds of SDR learning guarantee robustness against small propensities, which is a downside of the existing methods, where small propensities can lead to larger generalization errors (as shown on Page 6).",
            "strength_and_weaknesses": "- The paper is well-written and easy to understand. The related works are covered in detail and are up-to-date with the latest work in learning from MNAR data. \n- Theorm 4 (Tail Bound of SDR) is convincing to show that the proposed method is robust to small propensities in the data. \n- Given that the overall learning is done in three stages, I was expecting a relatively larger difference in training time, but Table 2 suggests the proposed method has a runtime similar to existing methods, which is impressive, given the stability guarantees of the methods. \n\nSome weaknesses of the paper:\n\n- SDR is asymptotically unbiased (Theorm 1), whereas baselines like IPS are unbiased for any given data size. This means, for smaller datasets, there is no guarantee of unbiasedness. Although the experiments on small datasets like Coat demonstrate superior performance to baselines, there's still there's no guarantee of unbiasedness when the dataset is small. Any comments on that? \n- Since the SDR estimator is similar in form to the SNIPS estimator, it would be interesting to compare the bias-variance of both estimators. Since SNIPS performs some kind of normalization, it should also be robust to small propensities. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is very well-written, and it's very easy to understand.\n- The idea presented is novel, and the appendix has detailed proof of the tail bounds used in the paper. \n- Authors have provided the source code as supplementary material. ",
            "summary_of_the_review": "- The authors propose a new Stable Doubly-Robust (SDR) method for learning a recommender system from Missing Not At Random (MNAR) feedback data. \n- The method is proven to be robust under the presence of small propensities, a setting in which the existing methods might fail. \n- Empirical results on popular MNAR benchmarks (Coat and Yahoo! R3) demonstrate the efficacy of the proposed method against strong baselines.  \n\n-- Response to authors ---\n\nThe authors have addressed all the comments and some concerns I had initially. Overall I think this is a very interesting paper with extensive theoretical analysis of the generalization properties of the method. I think this paper would be very relevant to the ML & RecSys community. \n\n ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4191/Reviewer_HkXH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4191/Reviewer_HkXH"
        ]
    },
    {
        "id": "r2XCeUTuvYu",
        "original": null,
        "number": 2,
        "cdate": 1666568191027,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666568191027,
        "tmdate": 1666568191027,
        "tddate": null,
        "forum": "3VO1y5N7K1H",
        "replyto": "3VO1y5N7K1H",
        "invitation": "ICLR.cc/2023/Conference/Paper4191/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Existing doubly robust (DR) estimators have multiple limitations, e.g, having an unbounded bias, variance, and generalization bound when propensities are extremely small. To address these limitations, this paper proposes a stabilized doubly robust (StableDR) estimator which has a weaker reliance on extrapolation than the existing DR estimators. The paper demonstrate that the StableDR estimator has a bounded bias and a bounded generalization bound even when propensities are arbitrarily small. Moreover, the paper proposes a novel cycle learning approach for training a prediction model, a propensity model, and an imputation model based on the StableDR estimator. The key idea of the proposed cycle learning approach is cyclically employing different losses to update the parameters of the three models. The paper conducts extensive experiments on two widely-used datasets containing both Missing Not At Random (MNAT) and Missing At Random (MAR) ratings. The experimental results show that the proposed cycle learning approach based on the StableDR estimator significantly outperforms existing approaches in terms of rating prediction.",
            "strength_and_weaknesses": "W1: The paper argues that existing DR estimators have an unbounded bias, variance, and generalization bound given extremely small propensities, while the proposed StableDR estimator does not. It would be better to formulate this argument formally into some lemma or proposition. To argue that the StableDR estimator is more stable than existing DR estimators, it is crucial to demonstrate that the StableDR estimator has a smaller variance than the DR estimators, both theoretically and empirically.\n\nW2: I am concerned about the novelty of the proposed approach. First, the StableDR estimator has the same form as an existing Self-Normalized Inverse-Propensity-Scoring (SNIPS) estimator. The property of the StableDR estimator having a bounded bias, variance, and generalization bound mostly comes from the SNIPS estimator. Second, it is not clear why the paper proposes to train models in a cyclic order of prediction model, imputation model, and propensity model instead of another cyclic order or joint learning of all three models. It is important to add more explanations, justifications, and theoretical or empirical results regarding this aspect.\n\nW3: Looks like the property of double robustness in Theorem 1 does not have the exact same definition as that in existing DR estimators. It is recommended to call out the difference and provide some comparisons between these different definition of being double robust.\n\nS1: The experimental results are quite extensive. The paper uses two representative datasets for recommendation debiasing. The paper compares the proposed approach against a wide variety of existing approaches under various metrics. The paper also present some ablation studies to understand the impacts of different components of the proposed approach.\n\nS2: The presentation of the paper is good and the references by the paper are sufficient.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-presented and it is easy to follow the logic flow. The ideas proposed by the paper are marginally novel, though.",
            "summary_of_the_review": "I think this paper is marginally below the acceptance threshold due to limited novelty and some not well-supported arguments.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4191/Reviewer_f6Th"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4191/Reviewer_f6Th"
        ]
    },
    {
        "id": "8YIf0swHLJf",
        "original": null,
        "number": 3,
        "cdate": 1666676148806,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676148806,
        "tmdate": 1666676148806,
        "tddate": null,
        "forum": "3VO1y5N7K1H",
        "replyto": "3VO1y5N7K1H",
        "invitation": "ICLR.cc/2023/Conference/Paper4191/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "By applying a stabilizing constraint on propensity model learning given an error imputation model, the authors show that the result SNIPS estimator satisfies the double robustness property in Theorem 1. Hence this estimator is termed the SDR estimator. This naturally results in bounded bias (Theorem 2), a tail bound on the deviation (Theorem 4) and a generalization bound (Theorem 5), demonstrating mitigation of the effects caused by small propensities. \nMotivated by these theoretical results, for recommender systems, the authors propose iterative 3-step alternating optimization of the error imputation model, the propensity model and the prediction model. SDR and SMRDR outperform competitors on two public rating datasets using MF and NCF baselines for the prediction model.\n\n",
            "strength_and_weaknesses": "Strengths:\n\n1. The proposed approach in Algorithm 1 is simple, well-motivated and backed by strong theoretical results.\n\n2. The experimental section is strong. For instance, ablation studies on the stabilization constraint in Sec 5.3 demonstrate its efficacy despite the use of two different propensity estimators.\n\nWeaknesses:\n\n1. It is not clear why NCF is used as a baseline in Table 1. In \"Are we really making much progress? A worrying analysis of recent neural recommendation approaches\", RecSys 2019, it was shown that the SLIM baseline outperforms NCF.\n\n2. The authors ignore temporal variability in recommender systems and assume that features $x_{ui} $ for a user-item pair are static rather than temporally varying.\nRecurrent Recommender Networks, WSDM 2017, shows that this assumption often does not hold for several real world recommender systems.\n\n3. Typos exist, e.g., in the sentence before Theorem 3, \"when there exist an...\" should be \"when there exists an ... \"\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, the theoretical and experiment results are strong and there is no concern about reproducibility given the code in the supplementary file.\nTheorem 1 appears to be novel and the remainder of the paper follows from that simple insight.\n",
            "summary_of_the_review": "Despite the concern regarding general practical applicability mentioned under weaknesses, given the simple elegance of the proposed approach, the strong theoretical foundations and the convincing results, I recommend acceptance of this paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4191/Reviewer_si5q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4191/Reviewer_si5q"
        ]
    }
]