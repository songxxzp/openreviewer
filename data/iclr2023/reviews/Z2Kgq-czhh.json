[
    {
        "id": "pyOsweAVZR",
        "original": null,
        "number": 1,
        "cdate": 1666439139425,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666439139425,
        "tmdate": 1666439139425,
        "tddate": null,
        "forum": "Z2Kgq-czhh",
        "replyto": "Z2Kgq-czhh",
        "invitation": "ICLR.cc/2023/Conference/Paper4788/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission provided a TPP model estimation method based on a L2 loss by assuming parametric finite-support kernel, discretization and precomputation. The statistical and computational efficiency of the method is compared to a nonparametric method. Finally, the method is applied to the MEG data. ",
            "strength_and_weaknesses": "Strength: The paper is easy to follow and the derivation of inference seems correct but I did not check it carefully.\n\nWeakness: The submission is lacking in novelty and the experiment is not convincing. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is easy to follow.\n\nQuality: The derivation of inference seems correct.\n\nNovelty: The submission is lacking in novelty.\n\nReproducibility: I did not check the code. ",
            "summary_of_the_review": "The major concern of the submission is novelty. All key ideas of the proposed method: L2 loss, finite-support, discretization and precomputation, are all existing in previous works. There is nothing new in the methodology in this submission. Another conceren is that the experiment is not convincing. The submission compares the statistical and computational efficiency of the proposed method to a nonparametric method, and shows the proposed method is better w.r.t. accuracy and efficiency. On the one hand, the simulated data is from a parametric model and the proposed method also assumes the same parametric form for the kernel, of course the ground-truth parametric model would get a more accurate estimation with limited data; on the other hand, the efficiency of nonparametric methods are certainly less efficient than parametric methods in general. The experiment section does not provide any meaningful comparision. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_GU8n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_GU8n"
        ]
    },
    {
        "id": "qrJTxn0OQJZ",
        "original": null,
        "number": 2,
        "cdate": 1666639534024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639534024,
        "tmdate": 1666639534024,
        "tddate": null,
        "forum": "Z2Kgq-czhh",
        "replyto": "Z2Kgq-czhh",
        "invitation": "ICLR.cc/2023/Conference/Paper4788/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approximate version of the multivariate Hawkes process. The authors claim that the existing estimation procedure has significant limitations, especially in the computational cost. The authors propose a variant of the binned Hawkes process, where the continuous decay function is replaced by a discrete vector. \n",
            "strength_and_weaknesses": "**Strength**\n\n- Addresses the important issue of computational issue of the multivariate Hawkes process.\n- Developed a practically feasible procedure. \n- Provided a theoretical analysis on the L2 approach. \n\n**Weakness**\n\n- The theoretical basis is not very clear. Especially the justification of the main loss function is not clearly provided. Chapter 2 of Bompaire (2019) does not explicitly derive Eq.(2). The text looks misleading. \n\n- Introduciton misses a major part of the efforts that have been made so far to address the computational issues. Notably,  an EM-like iterative scheme, first proposed by Veen and Schoenberg, has been known as one of the standard approaches to address the issue. But apparently, little attention is paid to that thread. \n\t> Veen, Alejandro, and Frederic P. Schoenberg. \"Estimation of space\u2013time branching process models in seismology using an em\u2013type algorithm.\" Journal of the American Statistical Association 103.482 (2008): 614-624.\n\n\n\n- Also, I don't think it is true that existing multivariate Hawkes processes fail to estimate the decay parameter. In fact, many recent works such as the below seem to estimate the decay parameter without significant issues with the aid of the Veen-Schoenberg algorithm.\n\t>Id\u00e9, T., Kollias, G., Phan, D., & Abe, N. (2021). Cardinality-Regularized Hawkes-Granger Model. Advances in Neural Information Processing Systems, 34, 2682-2694.\n\n\n- Finally, it is not very clear how the proposed method is compared with the existing binned Hawkes process, in particular Kirchner's INAR(p) model. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The text is mostly clear and well-written.\n- However, technical clarity and quality is not clear because of the lack of detailed discussions on the theoretical basis and relative comparison with the existing binned Hawkes models. ",
            "summary_of_the_review": "The paper is overall well-written. However, the technical novelty is not clearly justified by the authors not only empirically but also methodologically. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_h6fy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_h6fy"
        ]
    },
    {
        "id": "BTORP9mulhO",
        "original": null,
        "number": 3,
        "cdate": 1666659108338,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659108338,
        "tmdate": 1666659851110,
        "tddate": null,
        "forum": "Z2Kgq-czhh",
        "replyto": "Z2Kgq-czhh",
        "invitation": "ICLR.cc/2023/Conference/Paper4788/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes discretizing and limiting kernel support for efficiently learning kernel parameters of a Hawkes process via gradient descent. Additionally, the paper provides theoretical insights measuring the discretization bias. Experimental results on synthetic and electrophysiology datasets show improved estimation of kernel parameters and computation time compared to baselines.",
            "strength_and_weaknesses": "**Strengths**\n- Theoretical analysis of discretization impact may be of interest to some researchers \n- The proposed approach improves computational efficiency \n- The proposed approach seems easy to implement\n\n**Weaknesses**\n\n*Technical limitations*\n- The proposed approach is limited to parametric self-exciting Hawkes kernels which are often violated in practice \n- The proposed approach could be sensitive to the selected discretization step-sizes $\\Delta$ and the length of kernel's support $W$\n- The novelty is quite limited. Discretizing and limiting support of existing parametric kernels seems straightforward\n\n*Lack of clarity*\n- In general, the writing and notation should be improved to increase clarity, e.g., it's not necessary to write kernel convolution with Dirac functions since it's the kernel evaluated at that location. \n\n*Experimental Limitations*\n- Figure 1: For easy comparisons, the paper should plot all methods on the same plot. As is,  the y-axis is on a different scale making it hard to make comparisons.\n- The paper should consider adding additional real-world datasets and recent state-of-the-art  baselines (*e.g.*, neural Hawkes [1] models flexible kernels with neural networks)\n- Figure 3:  Are the TG and RC intensity predictions from the proposed approach?\n- The performance improvements over baselines seem marginal\n- The paper should provide summary statistics of the synthetic and electrophysiology, *e.g.*, number of event types, sequence lengths, number of sequences, *etc.*\n- The paper should provide sensitivity analysis for step-sizes $\\Delta$ and length of kernel's support $W$\n\n**References**\n- [1] Mei et al., \"The neural hawkes process: A neurally self-modulating multivariate point process\", NeurIPS, 2017",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "The technical novelty is limited and experimental analysis is unconvincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_mV6v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_mV6v"
        ]
    },
    {
        "id": "2WROkFN1BKn",
        "original": null,
        "number": 4,
        "cdate": 1666953625084,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666953625084,
        "tmdate": 1666953625084,
        "tddate": null,
        "forum": "Z2Kgq-czhh",
        "replyto": "Z2Kgq-czhh",
        "invitation": "ICLR.cc/2023/Conference/Paper4788/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors study discrete Hawkes process to model temporal data, along with parametric kernels with fixed support. \nThey propose to use least squares loss to learn the model parameters (compared to the more popular log likelihood estimation). They propose to use gradient descent, where the gradients may be efficiently computed thanks to the discreteness of the process and the finite support of kernels. They also derive the results for the approximated solution to converge to the true solution as the discretization window approaches zero. Numerical results are presented on (a) simulated data to evaluate the kernel learning and to estimate the recovery error as a function of \\Delta (the bin width)  (b) MEG data illustrating the learnt latency. ",
            "strength_and_weaknesses": "Strengths\n- the paper is well written and easy to follow. \n- The impact of discretization is studied well, theoretically as well as experimentally\n\nWeakness / Questions / Comments\n- Discrete time Hawkes processes are also considered in some of the earlier works such as https://www.cs.princeton.edu/~rpa/pubs/linderman2015scalable.pdf, https://arxiv.org/pdf/2003.02810.pdf\nIt would be better to compare the methodologies in this submission agianst them, theoretically as well as in experiments. \n\n- The experiments section may be improved with more convincing claims. For instance, in kernel recovery comparisons, only a single kernel is used for generating the samples. For fair comparisons, more kernel types should be compared, assuming that FasDin is not aware of the true kernel. \n\n- In Figure 1, it is not clear how the recovery error compares across the models for all choices of T.\n\n- Section 4 can benefit from better description of the data and the setup. What are the choices of \\Delta tried? What Is the typical value of T  or G for the samples ? While the latency estimates are similar, what is the specific computational advantage in terms of the total time ?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is written well and easy to follow. The experimental section may be improved with more baselines and better clarity in the  claims. \nIn terms of the novelty, similar models have been proposed in the past (as mentioned above), it would be great to compare and contrast with them too. ",
            "summary_of_the_review": "Overall, the paper has good technical contributions, but may be improved significantly with convincing experimental claims. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_rbja"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_rbja"
        ]
    },
    {
        "id": "wTm_Q2zgNLv",
        "original": null,
        "number": 5,
        "cdate": 1667407552087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667407552087,
        "tmdate": 1667407552087,
        "tddate": null,
        "forum": "Z2Kgq-czhh",
        "replyto": "Z2Kgq-czhh",
        "invitation": "ICLR.cc/2023/Conference/Paper4788/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors deal with the problem of performin inference for multivariate hawkes processes with parametric kernels. They propose an efficient discretized inference scheme that aims to optimize the least square objective (rather than max likelihood for instance) which does not assume exact events but aggregated counts, hence the name discretized inference. Their approach covers parametric kernels and the efficiency of the compution relies on pre-computations and on finite supports. Finite supports means that they discard events in the long past as they claim these have low infleunce for the scope of their applications being to quantify the dependence between neural response and external stimuli.",
            "strength_and_weaknesses": "Strengths of the paper are firstly the efficient computation for multivariate hawkes processes which is a non-trivial task. To achieve this the authors make some assumptions and relaxations such as assuming aggregated rather than exact events and finite support. They choose to minimize a least square loss which they achieve efficiently with some pre-computations. The authors motivate well their proposal and the benefit of hawkes process efficient inference is clear especially withing the neuroscience context. The application is both relevant and interesting.\n\nThe paper disregards bayesian inference approaches to the problem - and there are quite a lot both in previous and in the recent literature covering both exact and aggregated approaches. Many authors have proposed different approaches to multivariate hawkes processes that in fact rely on less approximations than those proposed here. For instance when it comes to efficient computation of discretized hawkes processes approaches already exist (G. Mohler 2013). \nAdditionally when it comes to non-parametric kernel approaches, there are way more than the paper suggests (see papers by J. Rousseau). In the context of non-parametric vs parametric approaches, the authors claim throughout that parametric approaches are less prone to over-fitting in comparison to parametric ones and use this as an advantage of their parametric kernels. However this is not true. If the authors wanted to make a statements about the potential advantages of parametric approaches vs nonparametric ones, these could be around other topics such as model mismatch. Other claims that are not true are for instance that Hawkes processes are easy to fit among other temporal point processes. The contrary, considering homogeneous or other inhomogeneous poisson processes often used to explain temporal or spatial patterns, hawkes processes are not striaghtforward to fit.\nThe choice of the loss function is not well motivated nor justified and should be compared theoretically and empirically with max likelihood-based approaches, as well as bayesian ones ideally. The loss function choice should not be arbitrary (or merely serving computational purposes) but should be properly justified in regards to the model being used. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is in general well presented. However there are some typos and the writing is not always clear in terms of explanations or  how the notation is introduced. The assumptions and relaxations the authors make are reasonable but not well justified in all cases and statements which support these choice are not always correct (see above). This compromises the correctness of the work. \nThere is enough information on how the experiments were made and supplementary material.",
            "summary_of_the_review": "Overall this paper targets a known problem, that of computationally expensive inference for multivariate hawkes processes. The authors motivate their work within the context of neuroscience in which hawkes process can find great use and can bring interesting conclusions on neural activity. The assumptions the aithors make to achieve their computationally efficient inference are reasonable but the choice of loss function is not well motivated or justified and needs further support (theoretical guarantees, empirical examples on higher dimensions and comparisons). Additionally several claims supporting the proposal choices are incorrect. Overall although the authors make interesting suggestions for a known problem, the paper as it is does not have enough methodological support for a publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_EZux"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4788/Reviewer_EZux"
        ]
    }
]