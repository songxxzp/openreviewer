[
    {
        "id": "FKO0kfl6bPC",
        "original": null,
        "number": 1,
        "cdate": 1666047904023,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666047904023,
        "tmdate": 1666047904023,
        "tddate": null,
        "forum": "I7Mvqi0p9Xj",
        "replyto": "I7Mvqi0p9Xj",
        "invitation": "ICLR.cc/2023/Conference/Paper1877/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a formula bounding the test error using VC-dimension. By further considering linear models, the paper connects VC-dimension with the squared norm of weights, Eq.(3). The paper then empirically computes the weight norm square and observed that, for the linear models, the curve of weight norm square matches the double descent curve. ",
            "strength_and_weaknesses": "Strength:\n\nThe strength of the paper lies in that it empirically observes the perfect match of the curve of weight norm square and the double descent curve on several linear models. This suggests a deep connection between the weight norm square and the test error. \n\nWeaknesses:\n\n1: The paper only presents a few key formulas about the VC theory, but provides no theoretical derivation except a few references of prior works. As the VC-theory is the key of the paper, the authors should include a full analysis of all the details of the theory. The theory presented in the paper should be self-contained, such that readers with some background can understand the analysis/theory with minor or no reference to literature. \n\nWithout giving details of the VC-theory, I cannot judge the correctness of the paper. \n\n2: Explaining double descent includes two parts: 1, theoretically connecting test error with weight norm square, or VC-dimension; and 2, theoretically explaining the behavior of weight norm square and VC-dimension.\nThe paper does not theoretically explain the behavior of the weight norm square. Even assuming all the formulas in the paper are correct, one still cannot claim the double descent is explained. \n\n3: The formula of VC-dimension, in Eq.(3), only applies to linear models. However, the most interesting part of double descent is for non-linear models, e.g., over-parameterized neural networks. For the non-linear models, there is still no explanation.\n\n4: The values of $a_1$ and $a_2$ are not from theoretical analysis, but from empirical guesses. Then, I don\u2019t think the key equation, Eq.(2), is theoretical. \n",
            "clarity,_quality,_novelty_and_reproducibility": "[clarity] Most discussions are presented clearly. However, the main theory lacks many key details, making the correctness of the paper not judgable. \n\n[novelty] The empirical coincidence of the curves of weight norm square and double desent for linear model is novel.\n\n[reproducibility] The experimental plots are considered reproducible.",
            "summary_of_the_review": "The observation of the empirical coincidence of the curves of weight norm square and double desent for linear model is interesting and encouraging. However, the main VC-theory is absent in the paper, and the behavior of weight norm square is not theoretically explained. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1877/Reviewer_Uf6C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1877/Reviewer_Uf6C"
        ]
    },
    {
        "id": "WhBGSwHdHg5",
        "original": null,
        "number": 2,
        "cdate": 1666251047201,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666251047201,
        "tmdate": 1666332200053,
        "tddate": null,
        "forum": "I7Mvqi0p9Xj",
        "replyto": "I7Mvqi0p9Xj",
        "invitation": "ICLR.cc/2023/Conference/Paper1877/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This article makes the case that the double descent phenomenon in overparameterized neural networks and random feature models can be explained by considering the VC-dimension of spaces of such functions with L_2-bounded final layer weights. Moreover, the authors claim that this point has been missed in the existing literature.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is well-written and clearly makes a simple point regarding VC-dimension of neural networks and random feature models with L_2-bounded final layer weights\n\n- I agree with the authors that in some prior literature there are overly strong statements about how the extent to which VC-dimension bounds are useful in the overparameterized regime. So it is a valuable message to push back again this point of view to some extent.\n\nWeaknesses:\n- The main point raised in this paper: namely that VC dimension with norm bounds on network weights can explain generalization in deep networks is not new. In my view, before this article can be published, it is imperative that a thorough review of and comparison with prior literature is undertaken. Once this is done, I am not certain whether there will be anything truly novel about the article under consideration. A short list of some relevant prior articles includes:\n    - Bartlett\u2019s paper [1] that is among the earliest works directly about one hidden layer neural networks with bounds on the norm of the weights in second layer the giving rise to VC-dimension bounds. This article has over 1700 citations. \n    - Neyshabur et. al.\u2019s work [2]. They consider in Theorems 1 and 2 rademacher complexity neural networks with bounds on the distance weights changed from initialization. They also review in Table 1 a range of other norm-based complexity measures that can given VC or Rademacher bounds. This article has over 400 citations.\n    - Bartlett et. al.\u2019s article [3], which considers Rademacher complexity bounds on overparameterized neural networks with a variety of norms constraints on all layer weights (see e.g. Theorem 3.3). This article has over 800 citations. This paper was already cited in the article under review. However, its use of weight norms to compute bounds on covering numbers is not discussed. \n\n- The authors give the following as an example of an improperly held belief about the role of VC dimension in explaining generalization in overparameterized neural networks: \u201cAnother common view is that \u201cVC-dimension depends only on the model family and data distribution, and not on the training procedure used to find models\u201d (Nakkiran et al., 2021). In fact, VC-dimension does not depend on data distribution.\u201d I believe this assertion deserves a more nuanced treatment. Specifically, VC-dimension bounds on norm-constrained classes of predictors can only really be useful when the size of the norm-constraints depends on the data-distribution! For instance, consider a random feature model $f(x) = \\theta \\cdot x$ with a bound $||\\theta||_2 \\leq B$ on the model weights. If $y(x)$ is, say $\\pm 1$,  independent of x, then B will have to be much larger in order to fit the data and give non-vaucuous bounds, compared with the case when $E[y(x) | x]$ is a say a smooth function. Hence, this seems like an important example where VC-dimension should depend on the data distribution (via the choice of a priori constrains on the collection of functions whose VC-dimension is being computed). \n\n\n[1] Bartlett, Peter. \"For valid generalization the size of the weights is more important than the size of the network.\" Advances in neural information processing systems 9 (1996).\n\n[2] Neyshabur, Behnam, et al. \"Towards understanding the role of over-parametrization in generalization of neural networks.\" arXiv preprint arXiv:1805.12076 (2018).\n\n[3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" Advances in neural information processing systems 30 (2017).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. However, as explained above, the main contribution of this article is not original.",
            "summary_of_the_review": "This article proposes that VC-dimension bounds of classes of neural networks with bounds on the norm of the final layer weights can explain double descent. However, the article fails to differentiate its point of view from the majority of prior literature on this subject. As a result, I believe this article is not ready for publication. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1877/Reviewer_7rdX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1877/Reviewer_7rdX"
        ]
    },
    {
        "id": "ErXXocgUeLX",
        "original": null,
        "number": 3,
        "cdate": 1666346383696,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666346383696,
        "tmdate": 1668705677871,
        "tddate": null,
        "forum": "I7Mvqi0p9Xj",
        "replyto": "I7Mvqi0p9Xj",
        "invitation": "ICLR.cc/2023/Conference/Paper1877/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to provide an explanation of double descent using classical VC theory. The primary argument is that since the test error can be bounded by a function involving both the training error and the VC dimension, one may observe double descent if one minimizes the training error and then \u201cminimizes the VC dimension\u201d subject to having a small training error (e.g. zero). The authors argue that this explains double descent in modern neural networks. They conduct experiments in simple settings (mostly linear classification) to support their argument. \n",
            "strength_and_weaknesses": "The goal of the paper is certainly commendable; revisiting classical ideas may shed insights into recent empirical observations. \n\nHowever, I don\u2019t think that the authors apply VC theory correctly in their argument. First, it is important to keep in mind that one does *not* minimize a VC dimension because the VC dimension is a property of the hypothesis space that you consider during training. The only valid way to reduce the VC dimension is to change the hypothesis space *from the outset*, not during training. It seems that the authors allude to structural risk minimization (SRM), but SRM is different. In SRM, one has a nested set of hypothesis spaces and learning proceeds from the smallest to the largest hypothesis spaces. In this case, the VC dimension is the VC dimension of the largest set ever considered. Note that all hypotheses outside of the last set in SRM are *never* considered during training. In the arguments the authors make, one starts with a complex hypothesis space and then try to reduce it by picking a smaller set if any. This is not a correct application of VC theory.\n\nSecond, while authors focus on linear classification, the extension of their argument to deeper architectures is based on an unsupported hypothesis without any proof. The authors hypothesize that one can treat a deep neural network as a linear classifier on the pre-logit features and then proceed by stating that the norm of the weights on the classifier\u2019s head gives a measure of generalization. One problem with this argument is that when using activations like ReLU, we can always rescale the weights of the early layers and decrease the norm of the weights of the classifier head without changing the decision boundary. That's why in norm-based generalization bounds, such as in [1, 2, 3, 4], it is the *product* of the norms of all layers that count. In addition, since the product itself can be increased or decreased arbitrarily without changing the decision boundary, generalization bounds almost always normalize the product of the norms of the layers by the *margin* on the training examples.\n\nThird, the VC dimension is independent of the distribution of the data. If there is a claim that neural networks have a small VC dimension, then neural networks should generalize across *all* data distributions including those with random labels. But, clearly that\u2019s not the case.\n\n[1] Bartlett, P. L. The sample complexity of pattern classification with neural networks: the size of the weights is more\nimportant than the size of the network. IEEE Transactions on Information Theory, 44(2): 525\u2013536, 1998.\n\n[2] Bartlett, P. L.; Foster, D. J.; and Telgarsky, M. J. Spectrally-normalized margin bounds for neural networks. NeurIPS, 2017\n\n[3] Arora, S.; Ge, R.; Neyshabur, B.; and Zhang, Y.  Stronger generalization bounds for deep nets via a compression approach. ICML, 2018.\n\n[4] Neyshabur, B.; Tomioka, R.; and Srebro, N. Normbased capacity control in neural networks. COLT, 2015.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the experiments are described enough to be reproducible. ",
            "summary_of_the_review": "I do not believe VC theory is applied correctly in this paper. Also, the main hypothesis that the authors postulate for deep neural networks is not valid in my opinion since one can rescale the weights in the early layers and reduce the norm of the weights at the last layer without changing the decision boundary.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1877/Reviewer_J8jr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1877/Reviewer_J8jr"
        ]
    },
    {
        "id": "hB1U-_xbND",
        "original": null,
        "number": 4,
        "cdate": 1666645962478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645962478,
        "tmdate": 1666645962478,
        "tddate": null,
        "forum": "I7Mvqi0p9Xj",
        "replyto": "I7Mvqi0p9Xj",
        "invitation": "ICLR.cc/2023/Conference/Paper1877/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper argues that, contrary to a common belief, the \"double descent\" effect can be fully explained by classical VC-generalization bounds. The paper argues that this requires a proper form and interpretation of the bounds, which are discussed at length. The authors then perform an empirical study of several models on the MNIST data in which the VC bound curve can be plotted and shown to describe double descent. \n",
            "strength_and_weaknesses": "Strengths\n-------------\nThe main strength of the paper is that it makes a clear point challenging a common belief, and consistently argues in favor of this point. The paper is quite thoughtfully written. The conceptual problem addressed - practically deriving the complex dependence of model performance on model complexity and sample size - is important and hard. The experimental demonstrations in the case of shallow networks look convincing to me. I think the paper has a definite methodological value and can be useful to the readers.\n\nWeaknesses\n----------------\nThe main weakness that I see is that the main claim of the paper, that theoretical VC bounds can be used to describe double descent, is essentially only demonstrated for shallow models or in effectively shallow settings, when it is not that interesting because there are now other analytical methods applicable in these settings that can often provide more detailed information (e.g., explicit solutions based on random matrix theory). \nAlso, I'm not an expert in statistical learning theory, and it's hard for me to judge, but the paper contains no new technical results; its novelty seems to be limited to interpreting old VC bounds and performing an empirical study in a new context. The key idea on which the discussion of double descent rests in the paper is that VC dimension is primarily determined by the number of parameters in the underparameterized regime but rather by the weight norm in the overparameterized regime. In some form, this or closely related ideas seem to have already been around for a while in recent works on double descent (and are in a sense already present in bound (3)). However, this does not mean that I doubt the novelty of specific details of theoretical analysis and experimental setup in the present paper.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally quite well written.\n\nI'm not familiar with the VC bounds in the form (1), (3) playing a central role in the paper, and the paper only vaguely refers to the books of Vapnik and Cherkassky & Mulier as the sources. It would be helpful to see more specific references or maybe an appendix with exact formulations of relevant theorems.\n",
            "summary_of_the_review": "A solid well-written paper offering a comprehensive discussion of the very important topic of explaining double descent and related effects via classical VC dimension theory. However, I'm not fully convinced in the significance of the results, due to a lack of any analytical novelty and a relatively limited scope of demonstrated applications.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1877/Reviewer_4NbT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1877/Reviewer_4NbT"
        ]
    }
]