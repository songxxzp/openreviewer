[
    {
        "id": "IkEFS9M0lc",
        "original": null,
        "number": 1,
        "cdate": 1666583965576,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583965576,
        "tmdate": 1666583965576,
        "tddate": null,
        "forum": "x0BPR9iXc1",
        "replyto": "x0BPR9iXc1",
        "invitation": "ICLR.cc/2023/Conference/Paper2196/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a parameter-efficient contrastive vision-language method by leveraging a pre-trained vision encoder and a text encoder. The key idea is to align a well-trained vision encoder and a text encoder by updating a small percentage of their parameters on a small-scale pre-training dataset. The proposed method can achieve comparable performance as full-model training.  ",
            "strength_and_weaknesses": "Strength:\n1. Vision-Language Pre-training (VLP) requires a huge amount of computational resources, which is not affordable by a lot of researchers. Therefore, it is important to design a parameter-efficient training strategy without sacrificing the model performance.\n2. The proposed method is simple and can be easily applied to existing methods.\n\nWeakness:\n1. The proposed method is quite similar to [1]. Although [1] is designed for fine-tuning CLIP on downstream tasks, these two works share a lot of technical solutions (e.g., layerNorm tuning and adding adapters).\n2. This paper is more like an empirical study (combination of existing techniques). The technical novelty and contribution is limited, and new empirical findings are not significant enough.\n3. One contribution of this paper is \u201cwe show that contrastive vision-language models created with parameter-efficient transfer learning conserve useful existing knowledge from their initializations better than full model finetuning\u201d. This conclusion is obvious. Since parameter-efficient training freezes most of the parameters of the well-trained vision/text encoder, this training obviously will keep existing knowledge.\n\n[1] How to Adapt Your Large-Scale Vision-and-Language Model\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-organized and easy to follow. The quality is good but the novelty and technical contribution is quite limited. Since the proposed method is simple, it shouldn\u2019t be hard to reproduce the results.\n",
            "summary_of_the_review": "This paper proposes a simple parameter-efficient training method for contrative vision-language pre-training. Although the problem is important and some interesting finds are observed, the technical novelty and contribution are not significant enough.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2196/Reviewer_soUw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2196/Reviewer_soUw"
        ]
    },
    {
        "id": "sqbCTR4J2sg",
        "original": null,
        "number": 2,
        "cdate": 1666658387124,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658387124,
        "tmdate": 1666658387124,
        "tddate": null,
        "forum": "x0BPR9iXc1",
        "replyto": "x0BPR9iXc1",
        "invitation": "ICLR.cc/2023/Conference/Paper2196/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of efficient fine tuning vision language models. Specifically, the authors seek to answer the following question: \"given that strong, pre-trained vision and language models both exist, can we minimally update both of their parameters to align their representations?\". The authors adopted the following options 1) initializing the vision and language encoders from pre-trained ones, 2) locking their params, 3) unlocking some of the params and 4) insert a trainable modules (adapters). They showed that by fine tuning just 7% of the parameters, they were able to match the performance of the full fine tuning strategy on retrieval (Flickr) and zero-shot classification (ImageNet)",
            "strength_and_weaknesses": "[Strength] The authors conducted many experiments to answer various research questions such as multilingual adaptation and alignment effects. They also did extensive zero-shot experiments comparing with corresponding baselines.[Weakness] This paper is not technically novel and the ideas are simple. It is an incremental step from LiT and from fine tuning approaches that use adapters before. In addition, as stated in the papers, it has a limited set up, which include 1) only testing with COCO dataset, and later 1.5M image-text pairs. It might not work for other datasets, 2) only focusing on zero-shot classification and retrieval tasks. Their conclusions might not hold for other downstream problems, 3) only testing on transformer architecture, which in my opinion, is not a main limitation, however. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was written with clarity and it is easy to follow.  \nThe proposed idea and the experimental results are solid (with the understanding of the limitations stated above) \nThe approach has limited novelty since it is similar to LiT and to various fine tuning approaches proposed before in the literature.  \nIt should be easy to reproduce the approach and the results reported in this paper. ",
            "summary_of_the_review": "This paper had experimental insights to various interesting research questions. However, it has limited technical novelty and the testbed  is narrow.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2196/Reviewer_YyXF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2196/Reviewer_YyXF"
        ]
    },
    {
        "id": "UIC3ukeIzz",
        "original": null,
        "number": 3,
        "cdate": 1666922559505,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666922559505,
        "tmdate": 1666922559505,
        "tddate": null,
        "forum": "x0BPR9iXc1",
        "replyto": "x0BPR9iXc1",
        "invitation": "ICLR.cc/2023/Conference/Paper2196/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper shows the good efficiency and effect of parameter-efficient contrastive vision-language alignment through transfer learning. Specifically, contrastive vision-language models can be created by updating a small set of parameters of pretrained vision models(e.g. DeiT) and pretrained language models(e.g. SimCSE). ",
            "strength_and_weaknesses": "Strength\n\n[+] experiment results demonstrate contrastive vision-language models can be created by updates to a relatively small (<7%) set of parameters in pretrained vision and language models, which is a very practical idea.\n\n[+] empirical study illustrates contrastive vision-language models created with parameter-efficient transfer learning conserve useful existing knowledge from their initializations better than full model finetuning\n\nWeaknesses\n\n[-] As the authors claimed in the Limitation section, the conclusions of this work are limited to the datasets, model structure, and downstream tasks.\n\n[-] It is known that batch size plays a key role in contrastive learning. I am curious about the influence of different batch size on your conclusion.\n\n[-] parameter-efficient transfer learning is a hot but not a new topic. Except for Adapter and Unlocking, there are other methods such as Prompt-tuning. Not all parameter-efficient transfer learning methods work for contrastive alignment may be another limitation.\n\n[-] In the Abstract, \"vision-language models\" and \"vision and language model\" may make readers confused",
            "clarity,_quality,_novelty_and_reproducibility": "clarity: this paper is mostly understandable to me\n\nquality: this is a generally solid work\n\nnovelty: this work is creative, but some people could have put these ideas together\n\nreproducibility: Most people can reproduce the experiments in this work",
            "summary_of_the_review": "This work illustrates contrastive vision-language models can be created by updating a small percentage of unimodals by gradient descent. Comprehensive experiment results show adapter and unlocking scheme can create CLIP-like models for natural-language classification or image-text retrieval with little effort. Although there is no explanation for this phenomenon.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2196/Reviewer_LWVe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2196/Reviewer_LWVe"
        ]
    }
]