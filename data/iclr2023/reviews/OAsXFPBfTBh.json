[
    {
        "id": "zJX28T4RNJN",
        "original": null,
        "number": 1,
        "cdate": 1666372063140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666372063140,
        "tmdate": 1669973765038,
        "tddate": null,
        "forum": "OAsXFPBfTBh",
        "replyto": "OAsXFPBfTBh",
        "invitation": "ICLR.cc/2023/Conference/Paper4550/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates an autoregressive (AR) evaluation scheme for Conditional Neural Processes (CNP) and its variants. The work is motivated by the fact that standard CNP does not model correlations between prediction targets. Incorporating these correlations is tackled in prior work by extending the *model*, which typically leads to complex training procedures. In contrast, the proposed method changes the *evaluation procedure* by defining an autoregressive, non-factorized joint distribution over targets, while leaving the model/training procedure of CNPs untouched. In a range of experiments, the paper shows that AR-CNPs show strong performance.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper studies an interesting and relevant problem.\n- The paper is very well written and easy to follow.\n- The experimental section is very well fleshed out.\n\nWeaknesses: there are only minor open points that I encourage the authors to elaborate on, cf. below.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Novelty:** The paper fills a blank spot in the literature: as discussed in the paper, AR schemes have been used for visualization of CNP predictions (e.g., [1]), but there exists no thourough investigation of the theoretical and empirical properties of AR-induced predictive distributions.\n\n**Quality/Clarity:** The paper is very well structured and well written. The visualizations are of high quality and help the reader grasp the concepts. I did not check all the math in depth, but the theoretical exposition appears concise and correct. The empirical evaluation is extensive and contains comparisons on a range of synthetic and interesting real-world datasets with the relevant SOTA methods.\n\n**Reproducibility:** The paper contains detailed descriptions of the datasets and experimental settings. Nevertheless, I have some open questions regarding the evaluation methodology, cf. below. Unfortunately, the github links are not valid, so the source code cannot be accessed.\n\n**Open questions:** I ask the authors to elaborate on the following points:\n\n- As discussed in the paper, the AR scheme leads to predictive distributions that are not permutation invariant w.r.t. set of input locations. I would like to see an extended discussion of this point, as I feel that the order can be crucial for the quality of the AR predictive distribution. In particular, (i) how would, e.g., Fig. 3 look like if the targets are fed into the AR-scheme from *right to left* instead of from *left to right*?, (ii) which order was chosen in the experiments?, (iii) how to choose the order in practice?/how to determine an \"optimal\" order?, (iv) how sensitive is the predictive performance to the order (in the author\u2019s experience)?\n- If I understand Sec. E correctly, you sampled new tasks for each training epoch. While I know that this is a standard approach, I would be interested in a comparison between CNP and AR-CNP when trained on a meta dataset with *finitely* many tasks, as is done, e.g., in [1]. Preferably, I would be interested in results on a range of different meta data set sizes in terms of the number of tasks (results on a simple 1D experiment that can be quickly trained would be enough). I think this is an interesting evaluation, because it sheds light on the question whether a lot of training data is required for the resulting model to exhibit good performance when evaluated using the AR scheme.\n\n[1] Volpp et al., \"Bayesian Context Aggregation for Neural Processes\", ICLR 2021\n",
            "summary_of_the_review": "The paper studies an interesting way of constructing expressive predictive distributions for CNP models at test time which will be of interest for the community. Both the theoretical and empirical parts are sound and well fleshed out. I have some open questions that I ask the authors to elaborate on, so I might further increase my score after the discussion period. Nevertheless, I think the paper is ready for publication at ICLR, and, thus, I recommend acceptance. \n\n\n_________________________________________\n\n\nUpdate after rebuttal: I increase my score, cf. my comment below.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4550/Reviewer_WrGZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4550/Reviewer_WrGZ"
        ]
    },
    {
        "id": "r6-cx0wHEN",
        "original": null,
        "number": 2,
        "cdate": 1666740884355,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666740884355,
        "tmdate": 1670000924781,
        "tddate": null,
        "forum": "OAsXFPBfTBh",
        "replyto": "OAsXFPBfTBh",
        "invitation": "ICLR.cc/2023/Conference/Paper4550/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents Autoregressive Conditional Neural Processes, a way to use conditional neural processes in an autoregressive way, by feeding newly sampled points back into the CNP and re-evaluating. The paper vouches that using CNPs in this autoregressive way gives better predictive performance, since there are no independence assumptions. Furthermore, there are no changes that need to be done to the training or architecture, since this process only affects the inference routine.\n\nThe paper notes that this process does give up the property of consistency, since the predictions can depend on the different permutations chosen during inference. Also, sampling can be done in parallel using blocks of size N, to trade off conditional dependence and speed.\n",
            "strength_and_weaknesses": "*strengths*\n- related work section is comprehensive (maybe also worth including [this](https://arxiv.org/abs/2206.03992))\n- paper is overall well written, with thoughtful discussion\n- a variety of experimental results\n\n*weaknesses*\n- the idea of unrolling autoregressively seems to be already well known at this point, in many recent works, and in some of the works already cited (e.g. esp [[1]](https://arxiv.org/abs/2207.04179) cf C.1, and also [[2]](https://arxiv.org/abs/2102.04426) [[3]](https://arxiv.org/abs/2110.02037) [[4]](https://arxiv.org/abs/2205.13554)). Therefore, I'm not sure that the ideas of this paper are novel enough to be a useful contribution to the community.\n  - e.g. [[3]](https://arxiv.org/abs/2110.02037) Section 3.1 has a more in-depth discussion about parallelizing blocks, and presents a dynamic programming method from [[5]](https://arxiv.org/abs/2106.03802) for choosing how to parallelize. It's not in the stochastic process framework, but it does seem very similar.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is reasonable, and the results look very much reproducible.\nThe novelty is questionable, as discussed above.",
            "summary_of_the_review": "In summary, the paper proposes to augment conditional neural processes with an autoregressive inference procedure, by feeding samples back into the model. This method improves predictive performance without requiring retraining of the model.\n\nBased on my personal opinion, I think this (or similar) techniques have already been used in many recent works, so I'm not sure that it is novel enough to be of interest. However, I'm not against acceptance if other reviewers find the idea novel, since otherwise the paper is well written with reasonable experimentation.\n\n\n=============\n\nI have read the other reviews and the author's response.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4550/Reviewer_K56J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4550/Reviewer_K56J"
        ]
    },
    {
        "id": "JmagQb-8IVo",
        "original": null,
        "number": 3,
        "cdate": 1666887027227,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666887027227,
        "tmdate": 1666887486842,
        "tddate": null,
        "forum": "OAsXFPBfTBh",
        "replyto": "OAsXFPBfTBh",
        "invitation": "ICLR.cc/2023/Conference/Paper4550/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper shows that a standard CNP (trained with the diagonal covariance assumption) can be used, at test time, to sample target values capturing their non-diagonal covariance structure. This is done by sampling one target point at a time from the CNP and feeding that predicted target point back into the context set of the CNP. This is termed as running \u2018CNP in auto-regressive (AR) mode\u2019.",
            "strength_and_weaknesses": "### Pros\n\n1. Paper adopts the simplest class, CNP, to demonstrate their key idea \u2014 thus their main result seems to be free from the nuances of a complex architecture.\n2. Paper shows that CNPs in AR mode are a more flexible class than GNP.\n3. The paper clearly mentions the limitations such as the lack of consistency under marginalization and permutation. Also, care may be taken when feeding more points into the context set than shown during training. It is good for practitioners to know about these limitations and exercise caution when deploying it. \n4. Paper also proposes how one can trade off speed and expressiveness in CNPs in AR mode by sampling target points block-wise.\n5. Empirically, they show that CNP in AR mode outperforms the usual CNP that comes with diagonal covariance over target points. It also tends to do better/comparable to other baselines e.g. GNP or LNP. The fact it does so despite carrying the simplicity of CNPs seems like a good takeaway.\n\n### Weaknesses/Questions\n\n1. I do not fully understand the procedure for obtaining the smooth samples on the bottom part of page 4. \n2. Paper does not analyze how the performance is affected if we auto-regressively sample more target points than shown during training. How quickly does it get worse?\n3. Another important question not analyzed is whether TNP (a recent state-of-the-art) trained in diagonal-covariance-mode (i.e. TNP-D) would benefit similarly at test-time to what is shown in this paper for CNPs.\n4. When trying to sample $N$ target points in the AR model, we sample the points $1, 2, \\ldots, N$ sequentially. It would be good to show whether prediction error worsens going from point $1$ to point $N$ due to the accumulation of prediction errors in the context set.",
            "clarity,_quality,_novelty_and_reproducibility": "**Novelty.** The work seems to distinguish itself well from the prior works. It is motivated by the following limitations of the prior works: i) Standard CNPs assume a diagonal covariance among the target points \u2014 a limitation of CNP resolved by this work. ii) GNPs can model non-diagonal covariance structure but are restricted to Gaussian processes \u2014 a limitation of GNP but not of the proposed approach. iii) Latent-variable NPs (LNP) can also model non-diagonal covariance structure but are trained via an approximation of the likelihood objective \u2014 a limitation of LNP but not of the proposed approach.  \n\n**Reproducibility.** The work is reproducible since the CNP code is available and this work simply proposes sampling target points auto-regressively.\n\n**Clarity and Quality.** The writing seems error-free. But given that the idea is simple and perhaps impactful, the writing and the presentation of math seem to make the idea a bit inaccessible to the first-time reader. After my first reading of the paper, I missed catching the key idea (CNP in AR mode) that I was only able to realize after some re-reads.",
            "summary_of_the_review": "Several results (e.g., equation 7) rely on mathematical proofs which I think I (the reviewer) do not have the right expertise to review properly. Besides that, I think the paper is simple and clever. But due to my lack of expertise in judging the math and the possibility that there is some paper that already shows some part of what the paper shows (e.g. because it is so simple), I will exercise caution and rate it as 6 with confidence 2.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4550/Reviewer_AdCH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4550/Reviewer_AdCH"
        ]
    }
]