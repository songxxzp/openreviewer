[
    {
        "id": "iSy_AtO6Ohp",
        "original": null,
        "number": 1,
        "cdate": 1666595474841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595474841,
        "tmdate": 1670111763237,
        "tddate": null,
        "forum": "zKvm1ETDOq",
        "replyto": "zKvm1ETDOq",
        "invitation": "ICLR.cc/2023/Conference/Paper3166/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work the authors attack in models in the following adversarial threat model: \n* Problem considered: CIFAR-10 image classification task\n* Attacker threat model: can modify each training image by up to $\\epsilon=8/255$ in $\\ell_\\inf$ norm\n* Attacker goal: minimize clean test accuracy on models trained on this dataset \n\nThe authors develop a new algorithm, INF, for this kind of data poisoning task, specifically aimed at the defense of performing adversarial training at train time. The authors find that they can decrease accuracy by 13.31% (from 84.88% under no poisoning to 71.57% under INF).",
            "strength_and_weaknesses": "Strengths:\n* The method is novel and obtains state-of-the-art levels of poisoning success on CIFAR-10 adversarially trained models. The authors' attack is interesting: given a learning algorithm, it exploits the model class' idiosyncrasies to poison newly trained models.\n\nWeaknesses (mostly in writing and unnecessary claims):\n* Claims about the mechanism of INF (and of other attacks). Any claims about the claims need to be properly defined or removed. For example, from the introduction \"Different from our attack strategy, existing methods commonly inject perturbations as shortcuts, which ensures that the model wrongly learns the shortcuts rather than the actual image content, leading to low test accuracy (Segura et al., 2022a; Evtimov et al., 2021; Yu et al., 2022)\" - there is no definition of what a shortcut or indiscriminate feature here is. The authors should describe concretely in what sense the given attack is different from previous work. These poorly defined claims around shortcuts vs indescriminate features are throughout the paper and should be rectified (or removed) before the work is accepted.\n* Poor contextualization: Is adversarial training currently considered the best possible defense in the given $\\ell_p$-based threat model? It would be good to provide this kind of context.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is relatively well written other than the weaknesses above.",
            "summary_of_the_review": "The paper presents a new method that yields state-of-the-art poisoning on adversarially trained models, which were previously seen as an effective defense against $\\ell_p$-based data poisoning methods. The paper has some issues around claims and contextualization of work,. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3166/Reviewer_DCRE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3166/Reviewer_DCRE"
        ]
    },
    {
        "id": "tyaI1J1Fqaq",
        "original": null,
        "number": 2,
        "cdate": 1666625704416,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625704416,
        "tmdate": 1670305671735,
        "tddate": null,
        "forum": "zKvm1ETDOq",
        "replyto": "zKvm1ETDOq",
        "invitation": "ICLR.cc/2023/Conference/Paper3166/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper challenges the defense ability of adversarial training against clean-label availability poisoning attacks. It was believed that these attacks can hardly harm adversarially trained models. However, the proposed attack method in the paper substantially degrades the test accuracy of adversarially trained models from 84.88% to 71.57% on CIFAR-10, which is seven times better than existing methods, resulting in a new SOTA.\n",
            "strength_and_weaknesses": "Strengths:\n- The setting of $\\epsilon_{adv}\\ge \\epsilon_{poi}$ considered in this paper is indeed more reasonable than the setting of $\\epsilon_{poi}\\ge 2\\epsilon_{adv}$.\n- The paper points out an implicit assumption behind the defense ability of adversarial training against clean-label availability poisoning attacks: adversarial training is capable of minimizing the adversarial risk on the poisoned data distribution. When this assumption is broken by the proposed attack, adversarial training would fail.\n- Experimental results are strong. As far as I know, no previous attack can degrade the test accuracy of adversarially trained models by more than 2% on CIFAR-10. In contrast, the proposed attack can degrade the test accuracy by more than 10%. This is impressive.\n\n\nWeaknesses:\n\n- The authors claim that the proposed attack method makes some features become not useful even for adversarial training, and thus the assumption behind the defense ability of adversarial training is broken. This means that when the proposed method induces indiscriminative features in the training data, adversarial training becomes not capable of minimizing the adversarial risk on the poisoned data. Are there any experimental results that validate that adversarial training faces difficulties in minimizing the adversarial risk of such poisoned data?\n\n- It is interesting to see that in Table 3, when the poison budget $\\epsilon_{poi}=8/255$, adversarial training with $\\epsilon_{adv}=4/255$ performs better than adversarial training with $\\epsilon_{adv}=8/255$. Could you provide any insight about this?\n\n- The reference model adopted in this paper is adversarially trained. Figure 4 shows that this performs better than adopting naturally trained models. The effectiveness of the previous attack methods may also be improved by adopting an adversarially trained model as the reference model. Have the previous works tried this? Do you have any results to share?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-organized, and the proposed attack method is clearly introduced.\n- The proposed attack method is novel and sound.\n- The authors conduct various quantitative evaluations and qualitative visualizations, which provide insights for the proposed method.",
            "summary_of_the_review": "This paper establishes a new SOTA in degrading the test accuracy of adversarial training under clean-label availability attacks. Overall, the proposed method is adequately evaluated, and the results are very impressive. Thus, the paper successfully challenges the consensus that clean-label availability attacks can hardly harm adversarial training.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3166/Reviewer_ffZU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3166/Reviewer_ffZU"
        ]
    },
    {
        "id": "TY14AUQIRz",
        "original": null,
        "number": 3,
        "cdate": 1666725829706,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725829706,
        "tmdate": 1666725829706,
        "tddate": null,
        "forum": "zKvm1ETDOq",
        "replyto": "zKvm1ETDOq",
        "invitation": "ICLR.cc/2023/Conference/Paper3166/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new approach, indiscriminative features (INF), to the problem of poisoning a data set so as to make predictive models learned from the data less successful, even if those models are trained using adversarial training.  Typically, adversarial training has been a successful defense against indiscriminative data poisoning.  To overcome this, the paper introduces INF-push and INF-pull.  INF-push permutes the data points so their points' features in a learned model are maximally moved further from the centroid of their class's centroid, within a permutation budget.  In INF-pull, data points are permuted so their points' features are moved closer to the nearest centroid of an incorrect class.\n\nThe paper demonstrates these poisons effectively evade a defense of adversarial training, as well as several other reasonable defenses.",
            "strength_and_weaknesses": "**Strengths**\n\n- The paper is very clear and easy to read.\n- The approach is intuitive and effective.\n- The problem is relevant.\n- Visualizations are clear and informative.\n\n**Weaknesses**\n\nThe weaknesses are few. It's impact will not be world-changing, but it is a professionally written paper.  The description of Theorem 1 is awkward, as it uses notation and terms from another paper that is not introduced in this paper (for example, \"risk\" and $\\mathcal{R}$).  This should be easily remedied.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and well written.  It seems reducible, though I have not looked at the code.",
            "summary_of_the_review": "A very good and interesting paper, which moves the ball forward on indiscriminate poisoning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3166/Reviewer_VP8n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3166/Reviewer_VP8n"
        ]
    },
    {
        "id": "FVwtM64_YB",
        "original": null,
        "number": 4,
        "cdate": 1666918651150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666918651150,
        "tmdate": 1670616111153,
        "tddate": null,
        "forum": "zKvm1ETDOq",
        "replyto": "zKvm1ETDOq",
        "invitation": "ICLR.cc/2023/Conference/Paper3166/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a novel technique to poison training data by crafting perturbations to either repel or attract training data to the corresponding class' mean feature vector. The authors demonstrate that this poisoning technique is superior to known availability attacks when defended against with adversarial training.",
            "strength_and_weaknesses": "### Pros:\n1) The attack is intuitive, and the results appear promising.\n2) The paper is clear and well written.\n3) Experimentation is fairly thorough, and a large number of related works are compared to.\n\n### Cons/Random things to address:\n1) I strongly encourage the authors to re-name their method and rephrase its positioning. The authors claim that their poisoning method is successful because it perturbs \"indiscriminative features\". However, in my opinion, there is little basis for this claim. It is also an unnecessary claim as the paper's results ostensibly justify the proposed method. In fact, I would argue that the poisoning technique does in fact perturb discriminative features - especially in the INF pull case as the perturbations by construction center the features of the poisoned data around the class mean - a descriminative quantity... Observing the visualizations of your perturbations would seem to confirm this. To their credit, the authors do try to justify this claim (briefly) in section 6, but TSNE visualizations are not convincing evidence of this claim. It's also not clear from what models the plots in Figure 5 were generated. Are they the features according to the reference model or the victim model after poisoning?\n2) Table 1 presents very encouraging results for your method. However, from Table 3, it appears that your attack success is reduced by using a smaller adversarial training $\\varepsilon$-budget. To present a fair comparison, Table 1 should instead show the *maximum* adversarial training accuracy a practitioner could achieve against each method - a practitioner would not use a higher than necessary adversarial training budget if a smaller one produces better results.\n3) If I understand correctly, there is another slightly unfair aspect to the comparisons to previous works. Your method crafts the perturbations using an adversarially trained reference model, and you show the superiority of your method compared to other methods in the face of an adversarial training defense. Did you adapt the other attacks and use an adversarially trained model for those results? If not, Table 1 doesn't present an apples-to-apples comparison. As you state \"More specifically, modifying the robust (semantic) features is the key to poisoning the AT models\". Maybe this is key for other methods as well.\n4) While I understand your point that defenders might not have access to a clean-trained model, or a clean AT model, I think this is a bit of a stretch. What if a defender could use a general purpose clean-trained feature extractor (widely available) to defend against your attack?\n5) Potentially the biggest issue I have is Figure 4a. If I'm reading this correctly, if the defender simply trained  in a standard fashion (not AT), the results in Table 1 would jump well into the 80's? Why would the defender choose to adversarially train then? \n\nOverall, I think the paper is interesting, and I'm willing to raise my score if my concerns are sufficiently addressed.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear, the method is novel as far as I know.",
            "summary_of_the_review": "I find the work interesting and well motivated, but I have serious questions about the setting for comparisons. I am willing to raise my score if these questions are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3166/Reviewer_VPK4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3166/Reviewer_VPK4"
        ]
    }
]