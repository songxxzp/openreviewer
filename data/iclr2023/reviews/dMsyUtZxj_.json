[
    {
        "id": "HZ9ksgiq0R",
        "original": null,
        "number": 1,
        "cdate": 1666341245262,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666341245262,
        "tmdate": 1666341245262,
        "tddate": null,
        "forum": "dMsyUtZxj_",
        "replyto": "dMsyUtZxj_",
        "invitation": "ICLR.cc/2023/Conference/Paper5039/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a supernetwork training strategy for preserving the Pareto ranking between different subnetworks for handware-aware architecture search. To maintain a higher ranking correlation supernetwork, the authors propose to prune the operations based on the optimal Pareto front during the training of the supernetwork. Moreover, the authors define a new metric for better evaluating the architecture in a multi-objective context. Experimental results on three benchmarks demonstrate the effectiveness of the algorithm in some cases. However, the proposed method is not clearly described and the experiments can be further improved. My detailed comments are as follows.",
            "strength_and_weaknesses": "### Positive points:\n\n- The authors introduce the Pareto ranking as a novel metric for multi-objective architecture search. \n\n- The authors propose a supernetwork training strategy for preserving the Pareto ranking between different subnetworks.\n\n - The authors propose to prune the operations based on the optimal Pareto front while training the supernetwork.\n\n### Negative points:\n\n - In the Pareto rank-preserving training section, the details of the proposed method are missing. In other word, after reading this paper, I can not re-implement the proposed method. It would better for the authors to make it more clear and detailed.\n   - a)\tThe description of the listwise ranking loss is not clear. More explanations are needed.\n   - b)\tDetailed descriptions for obtaining the Pareto ranks (ground truth) for different subnetworks are needed. \n   - c)\tThe detail of the hypervolume improvement should be provided since it\u2019s an important metric for pruning the operations. Besides, the motivation for choosing HVI is unclear. Moreover, the number of sample networks n seems important for the proposed method. How to choose a good n?\n - Estimating architecture latency using a lookup table has been proposed and investigated in FBNet. In my opinion, this is not a good way to evaluate the latency of candidate architectures. In practice, the inference framework (e.g., TensorRT) would perform layer/operation fusion. For instance, TensorRT will fuse conv and batch norm layer into a single layer. In this case, the overall latency of the architecture is not equal to the sum of that of each layers. The reality is often more complicated than the above case. Thus, I think it would be difficult to accurately estimate the latency based on the lookup table.\n - In Eq. 1, the authors state $F_{k\u2019}$ is a set of sub-networks ranked k\u2019. I have no idea what is $F_{k\u2019}$ even aftering carefully reading it. More expalinations should be provide.\n - The ablation studies on \u201cthe number of sampled sub-network\u201d and \u201cthe number of epochs for Pareto-training\u201d are missing. It would be better for the authors to provide more ablation studies.\n - Some other multi-objective search algorithms [1][2] should also be compared in Tables 2 and 3.\n - The details of the experimental setting in Figures 3 and 5 are unclear. More explanations are needed.\n - The ablation study on the proposed pruning strategy is missing. More experiments are expected.\n\n### Minor issues:\n\n - In Section 3.2, \u201cAlgorithm 1 and figure 1 summarizes summarizes the training procedure\u201d should be \u201cAlgorithm 1 and figure 1 summarizes the training procedure\u201d.\n\n\n### Reference\n\n[1] Nsganetv2: Evolutionary multi-objective surrogate-assisted neural architecture search ECCV 2020.\n\n[2] Ponas: Progressive one-shot neural architecture search for very efficient deployment.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Lots of important details of the proposed methods are missing, I can not reproduce the proposed method based on the given manuscript.",
            "summary_of_the_review": "I vote for reject since the proposed method is not clearly described and several key ablations are missing in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5039/Reviewer_4mMw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5039/Reviewer_4mMw"
        ]
    },
    {
        "id": "WTvesyiyXXA",
        "original": null,
        "number": 2,
        "cdate": 1666597268307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597268307,
        "tmdate": 1669485052934,
        "tddate": null,
        "forum": "dMsyUtZxj_",
        "replyto": "dMsyUtZxj_",
        "invitation": "ICLR.cc/2023/Conference/Paper5039/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method for multi-objective Neural Architecture Search (NAS), i.e., finding the best NAs based both on task prediction and hardware-based metrics (e.g., latency).  The proposed method (HW-NAS, HW for \"hardware\") builds on single-stage supernetwork techniques (the union of possible architectural choices for the neural final architecture) by co-training the supernetwork's parameters and a \"Pareto\" parameter to find (and rank) the sub-models.   They propose to use Pareto-ranks as the primary search objective and NAS evaluation criteria.  They implemented their algorithm and compare the found rank of the models on the Pareto frontier with Pareto rank of the individually trained sub-models for multiple benchmarks.   In addition the trained Pareto parameter allows them to decrease supernetwork training time by pruning sub-networks not be on the Pareto front.    By doing so, they achieve a 97% Pareto front approximation (vs 87%) for the resulting sub-models. ",
            "strength_and_weaknesses": "Positive Points:\n+ Multi-objective NAS is an important area, and the proposed method (Pareto-based search training) appears novel\n+ Evaluation used 3 benchmarks/search spaces (DARTs, NAS-Bench-201, ProxylessNAS) and additionally compares to FairNAS and a recent one-shot GCN-based technique. \n+ Results are promising: optimal sub-networks often meet or improve accuracy/latency while also taking less time to train.  \n\nNegative Points\n- The paper is written as if optimizing for multiple objectives was a relatively new idea.  But searching for models on the Pareto front has been previously explored in the NAS area.  Related work doesn't describe much of the work in that multi-objective space.   \n- The description of the algorithm is light on details, specifically the Pareto loss function, the Pareto parameter updates, and how NAs are found.  This makes reproducibility difficult. \n- Given the above limited description of the operation of the Pareto-based training, it isn't clear how easy it is to extend to multiple objectives. \n- There is little to no evaluation of the Pareto parameter training in isolation to understand its behavior / performance.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this paper appears to propose a relatively novel take on discovering sub-networks on the Pareto frontier.   However, a lack of related work and detail around the algorithm muddy the presentation, ultimately making it difficult to assess novelty, reproducibility, and applicability. \n\nThe end of Sec 1 states that it achieves a \"97% near Pareto front approximation\" -- it might be straightforward but be very clear if it's a critical metric.  \n\nPerhaps the largest omission concerns Algorithm 1.   Section 3.2's \"steps\" do not seem to accurately reflect the algorithm.  In step 2 \"training to solve the task\", the bullet only states taking a mini-batch and choosing a subnetwork.  It does not describe training that subnetwork nor give intuition behind that action.   The next section \"Pareto-rank training\" starts with the sentence \"After completing the iteration\" but step 2 is part of the iteration?    At least the appendix could spell out the \"list wise CE loss\" function (and Algorithm 1 could refer to it as opposed to just \"Loss_{pr}\".    Is \\alpha just a single parameter (value) or something else (vector)?   What does \"Adjust \\alpha\" mean? \n\nIt's not clear whether you're training the sampled subnetworks from scratch.  I would assume this was the case, since the Pareto training compares the found Pareto ranks (so far) to \"ground truth.\"   However, part of the point of the work was to avoid expensive retraining of the sub-network.   It looks like the benchmarks provided trained networks and in one section you mentioned being able to use those for estimating the Pareto Kendall measure (sec 4.3).   So, does training time (GPU Hours) include that time? \n\nAnother gap is that the paper does not investigate the performance / convergence of the Pareto training.   How does the search proceed with N_p?  How many sub-networks should be sampled (n)?   It isn't clear what \"Hyper Volume\" is or why the reader should care.   It seems to have something to do with completeness of the frontier.   How often will this approach return only rank 1 models?  How often will it return the complete frontier?   How can we give the reader an intuition or proof of that behavior?   I.e., before we compare HW-NAS' final models, it wasn't clear how well the Pareto-training performed. \n\nI'm curious if there is a discussion to be had around the differences of Pareto ranks and performance ranks.   This mechanism will almost always return all models as rank 1 (sec 4.4), but performance ranks will likely have many fewer identical ranks.   Would it be possible to return a set of sub-networks scored rank of 1, but actually they all belong to the second frontier?  When you compute the \"ground truth\" rank, was it only Pareto ranking the sampled set?  In other words, if the ground truth set was scored in isolation, they would look like rank 1, and the correlation would be artificially high.  \n\nThe end of 3.1 says we'll only use Pareto ranks, but it would still be nice to repeat in Fig 5 caption.   \n\nRelated work should take into consideration other pieces of work that have looked at Pareto-based multi-objective formulations, such as \nElsken, Thomas, Metzen, Jan Hendrik, and Hutter, Frank. Efficient multi-objective neural architecture search via lamarckian evolution, ICLR 2019, and Efficient Forward Architecture Search, Hanzhang Hu, John Langford, Rich Caruana, Saurajit Mukherjee Eric Horvitz, Debadeepta Dey, Neurips 2019.    BTW, I got these by looking at prior openreview at ICLR.  ",
            "summary_of_the_review": "This looks like an interesting and fruitful direction of research - cotraining the supernetwork with a Pareto-based parameter to guide the search.  However, there are presentation, clarity, and background work issues that make it difficult to place the work into context, interpret the results, and understand the algorithm enough for reproducibility. \n\n[Post Author Response] The authors have addressed significant issues regarding the lack of details, including more detailed training steps, specifying the Pareto parameter's loss function, and including new results.   Many of these details should have been present in the original paper. but I've bumped the recommendation to marginal below in light of them.    ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5039/Reviewer_UeSf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5039/Reviewer_UeSf"
        ]
    },
    {
        "id": "2Kb5omOeR3",
        "original": null,
        "number": 3,
        "cdate": 1666628283108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628283108,
        "tmdate": 1666628283108,
        "tddate": null,
        "forum": "dMsyUtZxj_",
        "replyto": "dMsyUtZxj_",
        "invitation": "ICLR.cc/2023/Conference/Paper5039/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors introduce a methodology for pareto front sub-network identification from a supernetwork. This is done in a two step process, by first conducting fair subsampling of subnetworks for Nf epochs, followed by Pareto-Rank  training for Np epochs. Sub-networks that are furthest from the pareto front are also dropped to accelerate training. The proposed method achieves state of the art pareto front approximation (97%) in 2 GPU days. A new parameter, $\\alpha$ is introduced to measure which operation is critical, and pareto-ranking loss is utilized to adjust the $\\alpha$ for different paths. The sum of the selected operations alpha values is utilized as the pareto score and Cross Entropy Loss is taken between the ranking scores and pareto ranks (ground truth) to update $\\alpha$. ",
            "strength_and_weaknesses": "Strengths: \n\n- The idea is straightforward to implement on existing supernetwork based training schemes, and seems to be effective from the results.\n- The methodology is validated on NASBench201, DARTS and ImageNet. \n\nWeakness:\n\n- It is not clear how the second phase of training (argmax(a)) enforces fairness. Justification and implication of unfair sub-network sampling in this stage may be a useful discussion. \n- A table showing the pareto approximation effectiveness for each of the tested search spaces would be very useful.\n- The current results highlight the fact that PRP-NAS-BL finds a model with low latency, and PRP-NAS-BA finds a model with high accuracy. (As expected.) If possible, comparing with the ground truth optimal architectures in each case may add more value/context to the result. \n\nQuestions:\n\n- If argmax(a) selection scheme does not enforce fairness, it is natural to assume that the correlation between argmax(a) and kendall tau rank correlation would increase simply due to the nature of the CELoss minimization, with no bearing on the ground truth. How is the 'Truth' in the CELoss(Score, Truth) formulation calculated?\n- If Truth is a list of intermediate accuracies of the sub-networks, then naturally the kendall tau rank correlation will keep increasing, simply due to sampling bias. Comments on this issue would be appreciated. \n\nMinor comments:\n\n- Table 1 (Training hyperparameters) can be moved to the Appendix.\n- Fix 3.2 : 'Algorithm 1 and Figure 1 summarizes summarizes..' ",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "The paper is interesting but there are some concerns (see above).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5039/Reviewer_qrrY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5039/Reviewer_qrrY"
        ]
    },
    {
        "id": "sFwezOd3BS",
        "original": null,
        "number": 4,
        "cdate": 1666666429719,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666429719,
        "tmdate": 1670356912597,
        "tddate": null,
        "forum": "dMsyUtZxj_",
        "replyto": "dMsyUtZxj_",
        "invitation": "ICLR.cc/2023/Conference/Paper5039/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an hardware-aware NAS on a superset that preserves the Pareto ranking (accuracy and latency). It demonstrates better accuracy-latency tradeoff when compared to SOTA approaches.",
            "strength_and_weaknesses": "Strength\n* Good trade-off between latency and accuracy is achieved using the proposed approach comparing to the SOTA.\n\nWeakness\n* It is unclear how the alpha parameter is derived and how does the correctness of accuracy / latency modelling affect the quality of results.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n* This paper is generally well structured. Some details are missing and need to be clarified (see below).\n\nQuality\n* The approach is generally well described and is developed based on existing proven method. Some clarifications are needed to justify the correctness (see below).\n\nNovelty\n* This work appears to be an extension of superset training to include the proposed Pareto-rank. \n\nReproducibility\n* Cannot be evaluated based on the existing materials.",
            "summary_of_the_review": "* This paper presents an hardware-aware NAS on a superset that consider preservation of Pareto-ranking.\n* How is the operation score parameter calculated? Is this the same as the architecture parameter in DARTS? Is this score a good representation of the importance of operation?\n* Similarly, the line \u201cAdjust alpha\u201d in Algorithm 1 is not clearly explained.\n* The notations in Section 3.2 are slightly confusing, operation score (alpha), Pareto-rank score (alpha), Pareto score (P_s).\n* The latency model is based on the summation of layer-wise latency stored in a lookup table. Is this assuming sequential execution of layers in the target hardware? Is this assumption true for the target hardware?\n* Are the latency numbers reported in Table 2 and 3 based on real measurement or the latency model?\n* Can you provide more comparison to SOTA HW-aware NAS, e.g. BRP-NAS [1] and HELP [2] similar to Table 2 and 3?\n\nRef:\n[1] https://arxiv.org/abs/2007.08668\n[2] https://arxiv.org/abs/2106.08630",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5039/Reviewer_Z8Vf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5039/Reviewer_Z8Vf"
        ]
    }
]