[
    {
        "id": "fMFjkwkZuPi",
        "original": null,
        "number": 1,
        "cdate": 1666631664603,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631664603,
        "tmdate": 1666704373996,
        "tddate": null,
        "forum": "hfUJ4ShyDEU",
        "replyto": "hfUJ4ShyDEU",
        "invitation": "ICLR.cc/2023/Conference/Paper117/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the universal approximation problem of functions using neural networks. They showed that, the minimum width to approximate any functions in $L^p(\\mathcal K, \\mathbb{R}^{d_y})$ and $C(\\mathcal K, \\mathbb{R}^{d_y})$ is at least $w^*_{\\min} = \\max (d_x, d_y)$ for any activation functions. Furthermore, they showed that for some specific activation functions, such as Leaky-ReLU or ReLU+FLOOR, the minimal width can be achieved. ",
            "strength_and_weaknesses": "The related works are adequately cited. The main results in this paper will certainly help us have a better understating of the universal approximation property of deep neural networks from a theoretical way. I have checked the technique parts and found that the proofs are solid. The main result, which states that for any activation functions, the minimum width of a DNN to approximate any functions in $L^p(\\mathcal K, \\mathbb{R}^{d_y})$ and $C(\\mathcal K, \\mathbb{R}^{d_y})$ is at least $\\max (d_x, d_y)$, is an unexpected and elegant result, which is a non-trivial extension of previous results in this field. It would be more interesting if the authors could study the exact minimum width for more activation functions and more architectures used in practice. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is clearly written and well organized. I find it easy to follow.\nQuality: This paper is technically sound.\nNovelty: The novelty of this paper is high.\n",
            "summary_of_the_review": "The main result, which states that for any activation functions, the minimum width of a DNN to approximate any functions in $L^p(\\mathcal K, \\mathbb{R}^{d_y})$ and $C(\\mathcal K, \\mathbb{R}^{d_y})$ is at least $\\max (d_x, d_y)$, is an unexpected and elegant result, which is a non-trivial extension of previous results in this field. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper117/Reviewer_emi4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper117/Reviewer_emi4"
        ]
    },
    {
        "id": "cWCFmO_CWw2",
        "original": null,
        "number": 2,
        "cdate": 1666696603356,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696603356,
        "tmdate": 1669105091750,
        "tddate": null,
        "forum": "hfUJ4ShyDEU",
        "replyto": "hfUJ4ShyDEU",
        "invitation": "ICLR.cc/2023/Conference/Paper117/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The script studies the minimum width of neural networks to achieve universal approximation. Similar topics are also extensively studied in Park et al. 2022.\n",
            "strength_and_weaknesses": "The presentation needs to be improved. For instance, many terminologies such as C-UAP are not defined.",
            "clarity,_quality,_novelty_and_reproducibility": "See my comments below. ",
            "summary_of_the_review": "1. I doubt the significance of the script compared with the existing work Park et al. 2022. I compare the results as follows.\n\n**As for the lower bound:** the script provides a lower bound for arbitrary activation, which extends the results in Park et al which focuses on ReLU. \n\nPark et al. : w_min >= max (d_x +1 , d_y), ReLU\n\nThis script: w_min >= max (d_x , d_y), arbitrary activation\n\n\n**As for the upper bound:** both the script and Park et al. construct neural networks to match the lower bound. \n\nPark et al. : w_min < = max (d_x +2 , d_y+1 ), using general activation\n\nThis script: w_min < = max (d_x , d_y), using Leaky-ReLU+ ABS. \n\n\nFor both the upper and lower bound, the script improves the results in Park et al. only by constant 1 or 2. I doubt the significance of such improvement, please clarify the importance if there is any.\n\n\n\n2. The most important theoretical result is Theorem 2. However, the relevant analysis (the neuron ODE part) is mainly based on  Li et al. 22 and Duan et al. 22, please clarify the novelty if there is any. \n\n\n3. The definition of C-UAP and Lp_UAP is missing\n\n\n++++++++++++++++++ **POST REBUTTAL** ++++++++++++++++++++++\n\nThanks for the reply from the authors. Regarding my question 1, I am still not convinced about the importance of this result compared with the upper and lower bound of Parker et al.. I will keep my score. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper117/Reviewer_Vm8T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper117/Reviewer_Vm8T"
        ]
    },
    {
        "id": "l9mDdKIgAY",
        "original": null,
        "number": 3,
        "cdate": 1666702175534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666702175534,
        "tmdate": 1666702175534,
        "tddate": null,
        "forum": "hfUJ4ShyDEU",
        "replyto": "hfUJ4ShyDEU",
        "invitation": "ICLR.cc/2023/Conference/Paper117/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThis paper builds on certain recent results on Neural-ODE way of approximating functions and gets sharp estimates on the width required for feed-forward nets to approximate continuous and Lp spaces over compact domains. ",
            "strength_and_weaknesses": "The two highlights of their results is that (a) they are able to get a sharp estimate of the width requirement for space of continuous functions mapping between two unit cubes in different dimensions.  And (b) they have a very interesting approach to these width estimation problems via recent results in Neural-ODE. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "One of the key confusions I have here is about the rationale of proving theorems for the unit cube but claiming that the results hold for any compact domain. Why are the width requirements scale invariant? This claim seems to have been invoked many times but never really explained. \n\nThe readability of the paper is quite questionable. For eg. (a) the authors are assuming familiarity with recent results like the ``encoder-memorizer-decoder approach\" and hence are not seeming to define any of those terms. Very likely that only a small part of the audience will know of these immediately. This makes the paper feel quite incomplete. Maybe they could have given short descriptions of these in the appendix? (b) The $T_k$ matrices in equation $5$ are quite unclear. What is their definition? Equation 6 seems to give a definition but I am finding it quite hard to parse - where does this come from? ",
            "summary_of_the_review": "I think the paper needs some significant improvement in the presentation clarifying the issues raised above and my rating essentially reflects that. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper117/Reviewer_Mii1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper117/Reviewer_Mii1"
        ]
    },
    {
        "id": "7DYq_axa01",
        "original": null,
        "number": 4,
        "cdate": 1666883738150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666883738150,
        "tmdate": 1666883738150,
        "tddate": null,
        "forum": "hfUJ4ShyDEU",
        "replyto": "hfUJ4ShyDEU",
        "invitation": "ICLR.cc/2023/Conference/Paper117/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work tightens the upper bound from Park et al. (2021) while also proving a corresponding lower bound and prove that both $C-$UAP and $L^p-$UAP for functions on compact domains share a common minimum width $w_{min} = \\max(d_x, d_u)$. In fact, they use some tools from Neural ODEs to show that the critical width for $L^p-$UAP is achieved by leaky-ReLU networks.",
            "strength_and_weaknesses": "**Strengths:**\n- They prove matching upper and lower bounds, tightening the bound from Theorem 4 in Park et al. (2021) while also closing the problem of $L^p-$UAP.\n- The paper is well written with clear proofs.\n\n**Weaknesses:**\n1. I think Table 1 should also include Theorem 4 from Park et al. (2021) which shows that $w_{min} \\leq \\max(d_x+2, d_y+1)$ for continuous non-poly activations. If I understand correctly, the main contribution of this paper is tightening that bound and proving a corresponding lower bound which Park et al., were lacking. Please correct me if I am wrong.\n2. The definition of UOE functions should be stated (atleast intuitively) much earlier in the manuscript as it is mentioned several times.\n3. I would rephrase Lemma 11 as a corollary since it follows directly from Li et al., and Duan et al.'s results. That said, it is a nice connection that proves this result quite easily. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity, Quality, Novelty:**\n\nThe paper is well written and the proofs are easy to understand. I think it is a good contribution that improves the results from past work.",
            "summary_of_the_review": "I have a few minor concerns and clarifications as mentioned above. If they are resolved, I would recommend acceptance. I think it is a good contribution to a line of work which has a rich history.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper117/Reviewer_VMbe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper117/Reviewer_VMbe"
        ]
    }
]