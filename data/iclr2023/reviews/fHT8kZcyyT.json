[
    {
        "id": "lJ8u_ek0mX1",
        "original": null,
        "number": 1,
        "cdate": 1666393643853,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666393643853,
        "tmdate": 1666393643853,
        "tddate": null,
        "forum": "fHT8kZcyyT",
        "replyto": "fHT8kZcyyT",
        "invitation": "ICLR.cc/2023/Conference/Paper5185/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces CausalBench, a framework to benchmark gene regulatory inference methods using perturbational scRNAseq data. Although having access to perturbational scRNA data can help in discovering causal gene networks, generating large-scale perturbational data where every gene undergoes some genetic intervention (knockdown) is very cost prohibitive. There is a recent study that generates genome-wide perturbational scRNA data in only two cell types. The authors of CausalBench have used these data to benchmark some of the well-known network inference methods which use observational and interventional data. For the evaluation, they propose two approaches: biological and quantitative. For the biological evaluation, they use known gene-gene or protein-protein interaction data, and for the quantitative, they use some statistical tests to validate the effects of interventions. ",
            "strength_and_weaknesses": "Strength:\n\n- This paper tries to take the first step towards benchmarking the causal gene regulatory network inference methods using large-scale perturbational scRNA-seq data, which is interesting and important for causal discovery in biology.\n\nWeakness:\n\n- The idea of having a framework to test the causal network inference methods is great if the benchmarking metrics are powerful and interesting enough. I think the metrics proposed in this paper are not that much interesting. For the biological metrics, as mentioned by the authors, the well-known gene-gene or protein-protein interaction data are not specific for the cell types that they are working. The underlying gene regulation rules are very cell-type specific and the inferred networks could not be reliably evaluated by general networks which have been derived from many cell types. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to read and understand. The novelty is only limited to introducing a a benchmarking bed for causal gene network inference methods using perturbational scRNAseq data.",
            "summary_of_the_review": "Causal gene network inference (DAGs) are very important in understanding the mechanisms of gene expression. However, learning such DAGs using only observational data is not possible. Recent Perturb-seq data provide helpful interventional data by genetically perturbing the target genes and measuring the expression of other genes. This data empowers the causal DAG learning methods, however, they should be tested and benchmarked via the same metrics to measure their efficacy. \n\nThis paper takes the first step in the right direction by noticing the need for benchmarking causal network inference methods using Perturb-seq data. However, the benchmarking metrics are not comprehensive enough to be certain about the efficacy of such methods. My comments:\n\n- Biological metric does not make so much sense to me. The underlying DAG driving the gene expression programs are very cell-type specific. So, the general gene/protein networks cannot be a good and reasonable metric to evaluate the correctness of the learned DAGs in K562 and RPE1 cells. \n\n- As a qualitative metric, the authors can use held out perturbations and see how well different methods can predict the effect of each perturbation on various genes. This has also been used in Lopez et al. (2022) (cited in the paper). \n\n- Why haven't the authors used the biggest Perturb-seq (2 M cells) in K562 cells as published in (Replogle et al., 2022)? It would be interesting to see how well/fast each method works in a very large dataset.\n\n- Related to the previous comment, I think one another and yet interesting benchmarking metric would be how fast each method could be trained in each dataset. This kind of benchmarking has also been done in Lopez et al. (2022). By growing sequencing technology, it is not so far that we will have access to millions of cells and thousands of perturbations genome-wide, and the methods that could handle this huge computational barrier would be the winners. \n\nMinor commets:\n\nTypos:   - we we observed --> we observed\n             - in not large --> is not large",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5185/Reviewer_2A85"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5185/Reviewer_2A85"
        ]
    },
    {
        "id": "WPuxOLVtXz",
        "original": null,
        "number": 2,
        "cdate": 1666623845821,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623845821,
        "tmdate": 1669200430535,
        "tddate": null,
        "forum": "fHT8kZcyyT",
        "replyto": "fHT8kZcyyT",
        "invitation": "ICLR.cc/2023/Conference/Paper5185/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present CausalBench, a framework to benchmark causal gene regulatory network (GRN) inference methods on perturbational single-cell RNA sequencing data (scRNA-seq). It includes evaluation metrics, baseline implementations of relevant inference methods and access to perturbational scRNA-seq data. With CausalBench the authors evaluate the ability of recovering \u201csilver-standard\u201d ground truth networks and how they are affected by sample size.",
            "strength_and_weaknesses": "Strengths:\n- Authors are aware of and nicely describe the assumptions and limitations of this evaluation task and data.\n- Including both biological and statistical metrics for evaluation of method performance.\n- Evaluation frameworks allow to easily test if novel methods improve over baselines, motivating the development of improved methods.\n\nWeaknesses:\n- Authors propose novel evaluation metrics but did not implement classical ones such as the comparison against ChIP-seq derived networks, see (Pratapa et al. 2020).\n- Authors miss to evaluate classic methods, such as SCENIC (Aibar et al. 2017), that have been shown to be the top performers in other benchmarks (Pratapa et al. 2020). Although these methods are not causal, it would be beneficial to assess if the modeling of causality actually improves GRN inference.\n- While the authors provide different metrics, it is still not clear which methods perform better than others consistently. A \u201cconsensus\u201d score based on rankings would improve interpretation. Moreover, there is no discussion about which methods overperform the others and why.\n- Although the authors are aware of the limitation of perturbation experiments, no quality control assessment is performed to test whether the perturbation actually worked. Samples where the perturbed gene still shows high levels of gene expression should be removed or at least accounted for.\n- The partition of the data due to scalability issues raises concerns since, as the authors already mention, it breaks the no-latent-confounder assumption. Authors chose partition sizes such that the running time remains below 30 hours but do not mention anywhere the actual numbers for each method. In order to comply with this rule, some methods might use fractions of the feature space that are too small to generate valuable graphs.\n- In figure 3, authors mention \u201csignificant\u201d increase but do not perform any statistical test to corroborate it.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presented work is of good quality and is overall clearly explained, allowing reproducibility of the results. Regarding originality, some of the metrics that the authors are proposing are novel but others have been previously used. Moreover, while it is good to propose novel metrics, the authors miss to include some of the classical ones, which would make comparisons to previous work easier.\n",
            "summary_of_the_review": "The author's contributions to the GRN evaluation field are somewhat new but some aspects already exist in previous work. Moreover, the authors are missing some previous contributions, limiting the added cumulative value of it. On the other hand, the description of the problem, its assumptions and the description of the methods is overall clear and of good quality.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5185/Reviewer_vpJf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5185/Reviewer_vpJf"
        ]
    },
    {
        "id": "TXYbM4jnIy",
        "original": null,
        "number": 3,
        "cdate": 1666632486456,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632486456,
        "tmdate": 1669915676418,
        "tddate": null,
        "forum": "fHT8kZcyyT",
        "replyto": "fHT8kZcyyT",
        "invitation": "ICLR.cc/2023/Conference/Paper5185/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors of this paper propose a suite of tools for benchmarking network inference on both observational and interventional data single-cell gene expression data.  They run several network inference / causal discovery methods for recovering graphical relationships from scRNAseq data and evaluate the performance of the methods using existing putatative knowledge about specific gene-gene interactions as well as by comparing performance of observational and interventional data.  ",
            "strength_and_weaknesses": "- I think the paper tackles an important problem and provides a great service by colating these datasets and metrics.  \n- My main complaint is that the paper advertises it's contribution as providing a benchmark for network inference on large scale scRNA seq data although they admit that there are no gold standard datasets available from which to actually benchmark these methods.  This is a noted limitation, but my concern is that this is a really big limitation.  It's really hard to know whether the results are biologically meaningful, or really just noisy and reflect the difficulty of identifying useful benchmarks.\n- Along these lines, I think the \"Metrics\" section needs a lot more clarification.  The authors refer to equality tests on \"the two distributions\" as well as Wasserstein distance on \"the two empirical distributions\".  What are these distributions of? There could be a lot more detail here regarding the data / statistics used to create these distributions and how the tests were calculated.\n- While I understand some of the difficulties, it's hard to know how much to read into the TPR vs TP counts graphs.  The authors need to explicitly discuss the potential for false positives / false positive rates.  Are there any ways to identify putative false positives that can be used as a pseudo-benchmark?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The authors claim to have all code available in a github repository, but the repository seems to be empty.\n- Overall, I think the paper lacks technical precision needed for publication.  There are many cases where wordy but vague descriptions are used in place of more precise statements.  E.g. \"satisfactory results are only obtained on the RPE1 dataset\".  What counts as satisfactory?  What is the reason the other results are unsatisfactory? At the same there is additional and unneeded notation and technical details regarding SCM.  In my opinion, this doesn't add much, since you are not describing a new SCM-based method and it is never referenced again.  Thus the background on SCMs seems superflous.\n- Re-check for typos and language, e.g. \"appraoch\"\n",
            "summary_of_the_review": "Overall, I think the authors take on a challenging and important task of collating multiple datasets and implementing many causal discovery methods on these datasets.  Overall, I think the paper lacks clarity and justification around the statistical benchmarks employed.  As such, it's unclear how much value is actually added in the suite of benchmarks proposed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5185/Reviewer_WpyP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5185/Reviewer_WpyP"
        ]
    },
    {
        "id": "ojyuTFmbvI",
        "original": null,
        "number": 4,
        "cdate": 1666643200696,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643200696,
        "tmdate": 1669149735820,
        "tddate": null,
        "forum": "fHT8kZcyyT",
        "replyto": "fHT8kZcyyT",
        "invitation": "ICLR.cc/2023/Conference/Paper5185/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper aims to establish a dataset / benchmark for gene regulatory network inference from single-cell data.",
            "strength_and_weaknesses": "The paper is hard to follow for me. From the abstract and introduction, it seems it aims at providing a dataset / benchmark for GRN inference that would be very easy to understand and use by researchers from outside of computational biology or network inference fields - something like DREAM 3/4/5 GRN challenges from a decade ago. However, starting from Section 3, the authors introduce a very specific formal structure (SCM) that also does not necessarily reflect the underlying biology (e.g. the DAG assumption) - it is not clear what purpose this serves. The choice of benchmark evaluation metrics (Section 4) seems somewhat ad-hoc (why STRING and not other network repository, why use just true positives instead of metrics based on precision-recall and ROC used in DREAM). On the other hand, the paper lacks details about how to actually use the dataset in practice, whether there is a clear leaderboard etc. - details that would be essential in establishing CausalBench as the benchmark for the field. It does not help that the github repository mentioned in the introduction is empty.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper lacks clarity and novelty.",
            "summary_of_the_review": "The paper fails in achieving what a good benchmark/dataset paper should demonstrate: clarity, sound assumptions and design choices, and practicality. Reading it did not bring me any closer to being able to quickly, effortlessly evaluate a GRN inference method I may develop.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5185/Reviewer_QjTd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5185/Reviewer_QjTd"
        ]
    }
]