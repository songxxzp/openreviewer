[
    {
        "id": "c3z0qWpGcS7",
        "original": null,
        "number": 1,
        "cdate": 1666183702215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666183702215,
        "tmdate": 1666183702215,
        "tddate": null,
        "forum": "po-oqRst4Xm",
        "replyto": "po-oqRst4Xm",
        "invitation": "ICLR.cc/2023/Conference/Paper5548/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to predict solutions of PDE locally and unite them into a global solution. The proposed neural operator scales to large resolutions by leveraging local and global structures by decomposing both the input domain and the operator's parameter space. The proposed method can be trained with much fewer samples than previous approaches, outperforming the FNO when trained with just half the samples. ",
            "strength_and_weaknesses": "Strength\n\nMG-TFNO outperforms the FNO with a fraction of the parameters and memory complexity requirements.\n\nFigure 1 clearly illustrates the structure of the network.\n\nWeaknesses\n\nHow to handle the Fourier transform of non-periodic problems?\n\nWill the computational cost of the Fourier transform be large?\n\nHow is the differentiation of the Fourier transform implemented?\n\nCan this method make a 3D prediction?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method involves the Fourier transform, which can be complicated.",
            "summary_of_the_review": "The paper proposes a new approach but solves only a few simple problems. There are already many similar methods, and it is difficult to say which is better and which is more practical. Studying large-scale examples in three dimensions may be more critical.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5548/Reviewer_zgdt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5548/Reviewer_zgdt"
        ]
    },
    {
        "id": "p5EjKSlKFA",
        "original": null,
        "number": 2,
        "cdate": 1666345529438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666345529438,
        "tmdate": 1666345529438,
        "tddate": null,
        "forum": "po-oqRst4Xm",
        "replyto": "po-oqRst4Xm",
        "invitation": "ICLR.cc/2023/Conference/Paper5548/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a neural operator that scales to large resolutions by leveraging local and global structures through decomposition of both the input domain and the operator\u2019s parameter space.\nThis is achieved by writing an FNO operation with a single tensor, and subsequently using low-rank tensor factorization. \n",
            "strength_and_weaknesses": "Strengths:\n-\tThe paper presents a very interesting approach to a well-known problem: how to scale neural PDE surrogates efficiently to large resolutions\n-\tWriting the FNO basic block as single parameter tensor, and then applying matrix factorization on that is really neat. I think the idea has great potential.\n-\tThe multi-grid domain decomposition makes a lot of sense too.  \n\n\nWeakness:\n-\tThe biggest concern are the experiments. I cannot find any information on the grid, and any ablation on the baseline method. I could very well be that FNO with much less modes would perform equally well, which would make the achieved compression rates obsolete. It is really hard to judge the results without knowing the grid sizes and having some visual image of the data. Looking at Figure 3, the problems however don\u2019t really seem to have a very high resolution. What is the temporal resolution? Which solver is used to obtain the data? What are the boundary conditions, stating Reynolds numbers and not the boundary condition is problematic. \n-\tThere is no mentioning of other operator learning methods (e.g. DeepONet from Lu et al.) as well as a discussion of many other neural PDE surrogate methods. There is a huge body of neural PDE solvers/surrogates and omitting 95% of that is not good practice. I however admit that not everything is relevant for this paper. DeepOnet, Adaptive Fourier Neural Operators, multigrid methods, Vision Transformers are.\t    \n-\tConnecting to the previous point: I am also very curious how this relates to Vision Transformers or Adaptive Fourier Neural Operators. Also recently UNets get tremendous performances in high-resolution image generation tasks, how are they doing, or even more interestingly can this approach be applied to e.g. UNets/DeepONets as well?\n-\tWhat do I see in Figure 5 right? What is H1 loss there? Both y axes state L2 loss? That fact that there is such a gap between FNO train vs test can very well be due to the rather basic training scheme (no warmup, lr annealing can surely be improved) or can be a sign that better regularization for the baseline is needed.\n\nLu, Lu, Pengzhan Jin, and George Em Karniadakis. \"Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators.\" arXiv preprint arXiv:1910.03193 (2019).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly, up to the experiment section. There very interesting details are missing, which make it really hard to judge the impact. In the current version, the experiments are not reproducible",
            "summary_of_the_review": "The paper presents a strong idea, which is also practically very relevant. I highly encourage the authors to work on the experiment section and to work out the benefits of the methods as well as relation to other methods in more detail. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5548/Reviewer_u8oi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5548/Reviewer_u8oi"
        ]
    },
    {
        "id": "BiEGMBm_iuT",
        "original": null,
        "number": 3,
        "cdate": 1666636350027,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636350027,
        "tmdate": 1666636350027,
        "tddate": null,
        "forum": "po-oqRst4Xm",
        "replyto": "po-oqRst4Xm",
        "invitation": "ICLR.cc/2023/Conference/Paper5548/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on the problem of neural surrogates of high-resolution PDEs. To this end, it applies the principles of multi-grid domain decomposition to exploit the local structure in such data as well applies tensor factorization with low-rank regularaization to reduce the number of parameters. The method is evaluated on vorticity form of Navier-Stokes equations and shown to improve performance. Although it's unclear what was the resolution of data used in experiments.",
            "strength_and_weaknesses": "**Strengths**\n\nGiven GPU memory bottlenecks, different compute-memory tradeoff methods need to be explored as we keep increasing data sizes in deep learning. FNO based methods scale even worse with respect to memory when number of tracked modes increases. Proposed approach based on tensor decomposition methods makes sense for lower rank representation of model weights. \n\n**Weaknesses**\n\nWhether this is _hardware-efficient_ is not explored in this work. Something like Monarch [1] is the appropriate comparison for modern hardware. Matching derivatives during training makes the method even more expensive. Some timing comparisons would be useful.\nUsing % for reporting test error makes it harder to understand whether the underlying errors should be considered good. Similarly it's unclear what is \"high\" resolution in the paper because the experiments don't mention the spatial or temporal resolution. Moreover it would be useful to actually clarify if an external PDE solver was used or if the solver was hand written for datagen. \n\nSome of the claims about \"deep learning models being orders of magnitude faster than conventional solvers\" are based on incorrect comparisons; running on GPUs vs. CPUs, not using state-of-the-art PDE solvers rather comparing against hand written solvers, not taking into account approximation errors etc. \n\nIt's also a bit surprising that older neural operator methods like DeepONets [2, 3] are missing from related works.\n\n[1] https://arxiv.org/abs/2204.00595\n[2] https://arxiv.org/abs/1910.03193\n[3] https://arxiv.org/abs/2111.05512\n",
            "clarity,_quality,_novelty_and_reproducibility": "Many details are missing from the paper including the resolution of data used in the experiments. But many details can be more clear from the code and data release. The paper otherwise is easy to follow. The idea of using lower ranked factorization with alternative decomposition is common but specific applications to FFT based architectures is less explored and novel.",
            "summary_of_the_review": "There are important details missing from the experiments in the paper to make a clear judgement but the idea of using lower ranked tensor factorization and domain decomposition  makes sense for the PDE domain.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5548/Reviewer_J6V8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5548/Reviewer_J6V8"
        ]
    }
]