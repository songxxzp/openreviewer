[
    {
        "id": "dz_fsieoBg",
        "original": null,
        "number": 1,
        "cdate": 1666345929324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666345929324,
        "tmdate": 1666345929324,
        "tddate": null,
        "forum": "XZRmNjUMj0c",
        "replyto": "XZRmNjUMj0c",
        "invitation": "ICLR.cc/2023/Conference/Paper5107/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a method which accelerates the architecture choice step of certain one-shot NAS methods. The method greedily freezes the first block choices during the architecture optimization phase with an evolutionary algorithm. This allows for caching the intermediate feature maps and therefore reduce the computational effort. In an experimental study on ImageNet, they show that this method saves time at no reduction in predictive performance. Furthermore, they investigate the hyperparameter sensitivity.",
            "strength_and_weaknesses": "Strengths: important, under-explored aspect of NAS, clear description of the work\nWeaknesses: lack of baselines (see next section for actionable feedback), hyperparameter sensitivity (discuss seriousness, find ways to overcome it)",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written and it is an important problem. The presented idea is relatively simple but there has been little prior work in that direction making this quite original work.\n\nThere is a general lack of comparisons against reasonable baselines. First of all, a comparison against NASA is missing that was correctly identified as related work. However, other simpler ideas are not explored either:\n- Cache computations on disk\n- Reduce the number of generations in EA\n- Reduce the validation dataset size\n- Reduce the population size\n\nAdmittedly, each of these methods may have their own problems (additional hyperparameters, additional disk space, drop in predictive performance).\n\nThe hyperparameter sensitivity is a critical issue in my opinion. We neither have a theoretical nor empirical evidence that we would be able to choose the right hyperparameters for the presented method. Given that the method is only twice as fast as the hyperparameter-free baseline, retrying with a different set of hyperparameters makes this method pointless. I would like to see a strategy how to set the hyperparameters efficiently otherwise there is simply no point in using the method in the first place.\n\nTo me it is not clear why we need the importance sampling step. There is no need to keep all data in GPU memory. However, reducing the validation data itself will obviously reduce the overall time. This can be combined with the baseline as well. It is unclear, how much this contributes to the solution.",
            "summary_of_the_review": "In this well-written paper, the authors present an original idea to reduce the time for finding the architecture after a one-shot NAS. The proposed method reduces time by a factor of 2 but additionally it adds several sensitive hyperparameters. The authors study this sensitivity but provide no advice how to select stable hyperparameters. This raises the question whether we are able to reduce the time or we spend additional time searching for these hyperparameters. Finally, the only existing baseline is not considered for comparison as well as other simple baselines that could accomplish the same.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5107/Reviewer_zkYQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5107/Reviewer_zkYQ"
        ]
    },
    {
        "id": "rTisiEqao3",
        "original": null,
        "number": 2,
        "cdate": 1666653667576,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653667576,
        "tmdate": 1666653667576,
        "tddate": null,
        "forum": "XZRmNjUMj0c",
        "replyto": "XZRmNjUMj0c",
        "invitation": "ICLR.cc/2023/Conference/Paper5107/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces the idea of freezing layers within a One-Shot evolutionary NAS approach. As the lower layers quickly converge to a 'final' state the authors argue that freezing them and removing the calculations performed within them will reduce computation and hence energy. An approach is developed to identify when layers can be frozen and another approach to identify the 'significant' samples as pre-computing the computations (and re-using) can overwhelm the memory available within a GPU.",
            "strength_and_weaknesses": "Strengths:\n- The paper idea is intuitive and well worked out\n- The description of the idea is well presented making the idea clear to the reader\n\nWeaknesses:\n- The paper has no clear related work section and it is hence unclear how the authors feel their work relates to prior work save for the discussion of work they base themselves on.\n- The conclusion is very short and provides little in the way of insights\n- The only results are on ImageNet - it would be interesting to see how the approach works on datasets not over-evaluated with NAS approaches.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly presented and well written. For this reviewer it was easy to follow the though process in the work.\n\nThe proposed idea is quite novel, but not earth-shattering. \n\nIt would be possible to reproduce something similar to the work performed here, though some guesswork for parts would be needed.",
            "summary_of_the_review": "The paper is clear and concise. The idea seems reasonably novel and is supported by good evidence.\n\nSome more specific comments:\n\n- The review of one-shot NAS is interesting, but probably too long for this paper. More on related work would be better to present.\n\n- There is a bizarre blue line under the middle bottom graph in figure 2. \n\n- \"This provide a solid\" -> \"This provides a solid\"\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5107/Reviewer_cLeE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5107/Reviewer_cLeE"
        ]
    },
    {
        "id": "Cst9o8HJMA",
        "original": null,
        "number": 3,
        "cdate": 1666680986155,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680986155,
        "tmdate": 1666680986155,
        "tddate": null,
        "forum": "XZRmNjUMj0c",
        "replyto": "XZRmNjUMj0c",
        "invitation": "ICLR.cc/2023/Conference/Paper5107/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focused on accelerating one-shot neural architecture search (NAS). In particular, the authors aims to accelerate the architecture search phase based on evolutionary approach. The authors first discuss their observation that the evolutionary approaches tend to select the same shallow blocks from the early stage of the search. Then, an approach is proposed that freezes the selected blocks in shallow part of the super-network. This approach has an advantage that the features computed in all population in the evolutionary approach are the same because they share the same blocks, leading to saving some computational time. Moreover, to further speedup the search process, a mechanism to subsample validation dataset to approximate the validation accuracy with low computational time. The proposed approach, PCF-ES, have be compared with a baseline evolutionary approach and a variant of the proposed approach on three supernet variants on ImageNet. Without compromising the performance significantly, saving 50% of GPU hours and GPU energy are reported.",
            "strength_and_weaknesses": "# Strength\n\nEnergy saving is becoming a crucial recently. The proposed approach aims at contributing to this point.\n\n# Weaknesses\n\nSingle Dataset: Evaluation is only done on a single dataset. Its generality is not discussed.\n\nObservation: The observation in Section 3.1 seems to be observed on a single dataset as well. The experimental settings are not provided for this observation.\n\nComparison: The proposed approach is compared only with SPOS variants. It has not been compared with other one-shot NAS approaches such as DSNAS, which does the weight training and architecture search at once. Because of the lack of the evaluation it is not clear whether the proposed approach is useful among other one-show NAS approaches.\n\nStatistical Evaluation: No statistical evaluation has been performed. It is not even clear what are the numbers in Table 1 and Figures. Are they average values?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The algorithm is clearly described and well-motivated. The idea of progressively freezing choices may be a reasonable design choice. However, the detail of the experiments in Section 3.1 (observation) and Section 4 are not provided. Because of the lack of statistical evaluation, reproducibility is low.",
            "summary_of_the_review": "As mentioned in the strength and weaknesses section, numerical evaluation in this paper is not sufficient to support the claim of this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5107/Reviewer_Gnhw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5107/Reviewer_Gnhw"
        ]
    },
    {
        "id": "Qw-bwTCihL",
        "original": null,
        "number": 4,
        "cdate": 1667402044998,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667402044998,
        "tmdate": 1667402450857,
        "tddate": null,
        "forum": "XZRmNjUMj0c",
        "replyto": "XZRmNjUMj0c",
        "invitation": "ICLR.cc/2023/Conference/Paper5107/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an efficient one-shot neural architecture search (NAS) method that progressively freezes the architectures. The authors show that the first few blocks become similar among the candidate solutions during the evolutionary architecture search and then develop an acceleration method of architecture search by exploiting this phenomenon. In addition, the input sampling method is introduced to reduce memory consumption when reusing intermediate feature maps among candidate architectures. The experimental results demonstrate that the proposed progressive choice freezing evolutionary search (PCF-ES) can speed up the architecture search compared to existing one-shot NAS methods.",
            "strength_and_weaknesses": "**Strengths**\n* An interesting observation in one-shot NAS, the first few continuous blocks of candidate architectures become similar in the early search epoch, is presented.\n* The proposed PCF-ES can reduce the architecture search cost without significant performance deterioration on the ImageNet dataset.\n\n**Weaknesses**\n* The authors mention that the architecture searching cost is higher than the supernet training cost. It may be true when searching multiple architectures with different complexities by optimizing different objective functions. However, the proposed method only targets searching for a well-performed architecture. The reviewer suspects that the search cost reported in the experiment does not include supernet training. The search cost of both supernet training and architecture search phases, i.e., total search cost, should be reported in the experiment.\n* The authors treat the existing work of NASA (Ma et al. 2021) as related work on accelerating the architecture search process. However, an empirical comparison of NASA and the proposed method is not performed.\n* The experiment is conducted using only the ImageNet dataset. It is not clear whether the assumption that the first few blocks become similar among the candidate solutions will hold in other datasets and tasks. To evaluate the proposed method in various situations, it might be a possible choice to use NAS-Bench datasets for further performance evaluations.\n* The authors consider that the intermediate feature maps to be reused should be stored on the GPU memory. The reviewer thinks that it is possible to store the feature maps on the CPU memory, although the data transferring cost occurs. It is not clear the motivation for storing all intermediate feature maps on the GPU memory.\n* The detailed algorithm of importance sampling is unclear. How to calculate the probability distribution of $q(x)^*$ and the integral over $x$ in (2)? Also, the probability distribution of $p(x)$ is needed to use the equation (1). How to calculate $p(x)$ which is the distribution of input data x?\n* The effectiveness of importance sampling seems to be unclear. It would be better to compare the proposed sampling method to random sampling.\n* In the proposed PCF-ES, two techniques, freezing the shallow blocks and importance sampling-based input sample selection, are introduced. It is not clear which techniques mainly contribute to computational cost reduction. The ablation study should be conducted. What are the computational costs of the method only using the freezing technique and the method only using the importance sampling-based sample selection? If we store the intermediate feature maps on the CPU memory, we can evaluate the method only using the freezing technique.",
            "clarity,_quality,_novelty_and_reproducibility": "* The motivation and approach of the proposed method are reasonable.\n* The proposed method might be novel and is based on interesting observations. However, it is unclear whether it works well on other datasets and tasks because the experiment is conducted using only the ImageNet dataset.\n* The experimental evaluation is not enough to validate the effectiveness of the proposed PCF-ES.\n* Because the authors did not provide the code and the detailed experimental settings, it is hard to reproduce the experimental results.",
            "summary_of_the_review": "The authors treat an interesting topic in one-shot NAS. However, as described in the section on Strength And Weaknesses, the reviewer feels that the weaknesses of this paper outweigh the strength.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5107/Reviewer_Y3fv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5107/Reviewer_Y3fv"
        ]
    }
]