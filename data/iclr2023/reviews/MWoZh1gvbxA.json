[
    {
        "id": "go6g-Ye9B5Q",
        "original": null,
        "number": 1,
        "cdate": 1666100131365,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666100131365,
        "tmdate": 1666100131365,
        "tddate": null,
        "forum": "MWoZh1gvbxA",
        "replyto": "MWoZh1gvbxA",
        "invitation": "ICLR.cc/2023/Conference/Paper620/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors present a clean-label machine-unlearning-triggered poisoning attack that targets a specific test point. Given access to the victim model\u2019s architecture, the attacker crafts a poison set that makes the victim model misclassify the target sample and a camouflage set that neutralizes the poison set. Both sets are crafted by adding optimized noise (hard to detect by humans) to the images using the gradient-matching approach from previous work. The poisoning attack is then only triggered when the attacker requests to delete the camouflage set and the victim model unlearns by retraining. The authors show the method is effective on several models (SVMs and a few deep neural networks) and datasets (CIFAR-10, Imagenette, Imagewoof).\n",
            "strength_and_weaknesses": "Strengths:\n\n- Novel idea\n- The attack is only triggered when needed\n- Can be difficult to detect\n\nWeaknesses:\n\n- Unclear attack scenario\n- Strong attack assumption\n- Limited scalability\n\nComments for the authors:\n\nThe authors present a novel poisoning attack that combines the key intuitions from two previous works, Marchant et al. (2022) for using machine unlearning requests as the attack trigger and Geiping et al. (2021) for using gradient-matching as a clean-label poisoning approach. As a result, this poisoning attack is difficult to detect and the adversary can choose when to trigger the attack.\n\nHowever, I do have the following concerns.\n\n- The authors did not present a clear or realistic scenario where such a poisoning attack is necessary. While it adds an additional function compared to the traditional poisoning attack, the paper will benefit from some examples of scenarios where such a function is needed.\n\n- The added functionality also comes at a cost. On page 4, the authors claim the benefit of this attack is that the adversary can reveal whenever an unlearning request is submitted, while traditionally \u201cthe adversary simply plants the attack and must wait for the victim to train the model.\u201d However, the proposed attack also requires the adversary to \u201cwait for the victim to train the model\u201d and both the poison and camouflage sets (instead of just the poison set) need to be planted at the same time.\n\n- The proposed method also assumes strong adversary knowledge. The author mentioned in Section 2.1 that the attacker has access to the victim model\u2019s architecture and the ability to query gradients (as mentioned could be achieved by having access to the training dataset). The additional experiment results of transferability in Appendix B.6.1 further indicate that the attacker needs white-box victim model access (not just architecture and training data) to achieve the high attack performance presented in the main results since even when the brewer (model used to craft poison and camouflage sets) and victim models use the same architecture the attack performance still suffers.\n\n- Even under the strong attack assumption, the results show the efficacy still varies quite drastically across the 6 architectures tested, where only 3 architectures show optimal attack success rates. This implies the method might not generalize well to other architectures.\n\n- Given the training time provided by the authors, crafting each poison and camouflage set takes on average double the time needed for training the model (so 4 times in total for a pair of poison and camouflage sets). This significantly limits the scalability of the method, especially if the victim model uses more complex architectures.\n\n- The method also only limits to one single target sample. I believe a common setting for targeted poisoning attacks aims to misclassify a class of images (instead of a single data point) to another label. Combining the time cost mentioned above this limits the method\u2019s scalability. If the author can improve on the previous gradient-matching approach it would be a major contribution.\n",
            "clarity,_quality,_novelty_and_reproducibility": "good",
            "summary_of_the_review": "see strength and weakness",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper620/Reviewer_yC6u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper620/Reviewer_yC6u"
        ]
    },
    {
        "id": "Q9tARk0_ln",
        "original": null,
        "number": 2,
        "cdate": 1666471757068,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666471757068,
        "tmdate": 1666472234144,
        "tddate": null,
        "forum": "MWoZh1gvbxA",
        "replyto": "MWoZh1gvbxA",
        "invitation": "ICLR.cc/2023/Conference/Paper620/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The submission \"Hidden Poison: Machine unlearning enables camouflaged poisoning attacks \" describes a novel variant of targeted data poisoning attacks. The submission introduces the notion of \"camouflage\" data points that neutralize the effects of poisoned data samples. This essentially incorporates an \"off switch\" into targeted data poisoning attacks: camouflage data points can be created by similar techniques as existing data poisoning methods, but hide their effect. The actual poisoning attack is then triggered by an unlearning request of the camouflage data points, which removes the camouflage data points and activates the targeted data poisoning attack.",
            "strength_and_weaknesses": "Overall I like the idea put forward in this work. That a data unlearning request could trigger malicious effects already present in the data is clever and an interesting concept that would be surprising to readers interested in the security aspects of data unlearning. I see the the main strength of this work in the novelty of this concept.\n\nHowever, I do think that the evaluation and presentation of the attack could be improved. I want to highlight some areas that can be improved:\n* While the attack is conceptually interesting, what are the implications of this attack? Could the authors provide examples? In what scenarios would an attacker want to include an off-switch into their attack? \n* Relatedly, what is the trade-off that the attacker is making here? In most experiments in this work, the investigated threat model includes a doubled poisoning budget alloted to the attacker (e..g 0.6% for poisoning and 0.6% for camouflaging). In contrast, the attacker could have used an entire budget of 1.2% for an improved attack, potentially with additional targets. How much is the attacker giving up in capability to include this off-switch? Also, it is necessary to split poison and camouflage budget 1-to-1? It would be great to see ablations for all intermediate stages, e.g. 1.5% poisoning, .5% camouflaging, and so on, as a larger extension of the result shown in Fig3 b).\n* Further, what about robustness of the attack? In a data scraping scenario, it is generally unlikely that a victim would scrape all data points modified by an attacker. How robust is this attack to an attacker scraping random subsets of both poisoning and camouflage points?\n* Finally, this submission chooses targets from one class, data poisons from another, and camouflage points from the target class. Is this necessary? What happens when both poisons and camouflages are chosen from the same class? Or if camouflages are from a third class?\n\n* Aside from these specific points, more broadly: What about other types of unlearning? The submission only covers exact unlearning through retraining, but a range of approximate unlearning techniques have been proposed in the literature (as discussed also in the related work section). I think this submission could be majorly improved by evaluating the success of the proposed attack also in approximate unlearning scenarios.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some comments regarding clarity of experiements and writing:\n* Discussing gradient matching in both Sections 2.2. and 2.3 for poisoning and camouflaging respectively feels very repetitive. I think the methodology section could be improved by rewriting and possibly merging both sections.\n* Tables 1, 2, 6 describe camouflage and poison success separately, but what about joint success? If I understand these tables correctly, then they could be improved by including a column for joint success, denoting the fraction of attack runs where both camouflaging and poisoning were successful.\n* Concerning the experimental setup: The clean validation accuracy of most neural networks considered appears somewhat low. This might be related to the learning rate choice of 0.01 and lack of data augmentations. The submission discusses data augmentations in appendix B.6.2, but, for me, could be made much more convincing by incorporating data augmentations into the core experiments in Section 3.1.2. Also what about random crops?\n* Concerning the overall coverage of experiments, while I agree that ImageNet experiments are quite expensive,  what about poisoning scenarios that include finetuning of ImageNet models on poisoned datasets? This might circumvent some problem arising for models trained on ImageWoof where the overall poisoning budget is high (possibly because of the difficulty of training on these datasets with this budget).",
            "summary_of_the_review": "Overall, I like the idea put forth in this submission, but would love for the authors to improve the execution and evaluation of the proposed vulnerability,  discussing implications in exemplary use cases, evaluating the robustness of the attack and demonstrating its effect on approximate unlearning systems.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper620/Reviewer_XcYH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper620/Reviewer_XcYH"
        ]
    },
    {
        "id": "UXb84zDTf1_",
        "original": null,
        "number": 3,
        "cdate": 1666701101303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701101303,
        "tmdate": 1666701101303,
        "tddate": null,
        "forum": "MWoZh1gvbxA",
        "replyto": "MWoZh1gvbxA",
        "invitation": "ICLR.cc/2023/Conference/Paper620/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel attack, called camouflaged data poisoning attacks by leveraging a machine unlearning mechanism. This attack needs generate a pair of poisoning and camouflage datasets, which has less impact to the model on normal testing data. After triggering the machine unlearning procedure on the camouflage datasets, a retrained model could make target images to be classified as the target labels. ",
            "strength_and_weaknesses": "Positives:\n+ a novel attack is proposed by using machine unlearning techniques. \n\nNegatives:\n- It has less technical contribution since it just leverages existing machine unlearning techniques\n- only two datasets are used to evaluate the proposed method. ",
            "clarity,_quality,_novelty_and_reproducibility": "It is well written, and easy to follow\nThe proposed attack is a novel attack\n",
            "summary_of_the_review": "In this paper, a novel attack is proposed, which leverages machine unlearning techniques",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper620/Reviewer_xqqh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper620/Reviewer_xqqh"
        ]
    }
]