[
    {
        "id": "4XtPUmSKeT",
        "original": null,
        "number": 1,
        "cdate": 1666509719602,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666509719602,
        "tmdate": 1666509719602,
        "tddate": null,
        "forum": "o8fqVVKN3H",
        "replyto": "o8fqVVKN3H",
        "invitation": "ICLR.cc/2023/Conference/Paper647/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Summary of the paper\nThe paper proposes a new method for pretraining models for protein-protein contact prediction. The authors created pretraining data by collecting sequences and 3D data from the PDB database. \n\nThey break a monomer (a single chain) into a multimer with two chains at a random location in the sequence and train a model to predict the contacts of these two chains. The pseudo-contact ground truth between pseudo multimers is defined using the Euclidean distance between residue coordinates provided within the 3D data (residues within 5 Angstrom are considered as having a contact). \n\nThe pre-trained models are then fine-tuned further on protein-protein contact prediction datasets using supervised training approaches. \n\nThe authors demonstrated that although this is a simple pretraining approach, it provides a significant improvement over SOTA-supervised methods and the other methods that use pre-trained models for protein representation using language models such as the ProteinBert.\n\n\n\n",
            "strength_and_weaknesses": "Strengths\n\nInteresting idea of creating pretraining data.\n\nThe improvement over the SOTA-supervised learning approach is impressive.\n\n\nWeaknesses\n\nPotential leakage happens\nSince the SMP model was pretrained on the PDB dataset. There might be potential leakage happens if the test set of DIPS-Plus, DB5, and CASP-CAPRI  overlap with the data used for pretraining. Even though the objective of SMP which focuses on reconstructing the contacts of broken monomer and the objective of a multimer are different, seeing test instances during pretraining steps provides an unfair advantage over other methods that do not see the test data distribution during their learning. I suggest the authors check for overlapping instances between PDB and  DIPS-Plus, DB5, and CASP-CAPRI and remove them before pretraining to avoid leakage.\n\nComparison with pretrained protein models\nEven the authors have compared their methods to the pretrained language models ProteinBert with the results reported in Table 4, I would suggest the authors compare their approaches to the following pretrained models available at https://github.com/facebookresearch/esm:\n\n+ ESM-1b: this is a standard language model trained on the large database of  protein sequences, demonstrated very good results on contact prediction\n\n+ ESM-MSA-1b: leveraging MSA information for predicting better contact \n\nThis ESM pretrained model family was trained with very large data and demonstrated SOTA results on contact prediction.\n\nComparison with pretrained monomer contact prediction \nTo demonstrate that it is necessary to break the monomer into two chains and predict the merged contact with the Split and Merger proxy, the following experiments need to be done on the PDB sequences with 3D structure:\n\n+ Take the representation from ESM-1b as the representation of the protein sequence.\n\n+ Predict the contact matrix constructed by looking at the Euclidean distance between residues coordinates provided in the protein 3D data\n\n+ Use the new representation of ESM-1b pretrained on the monomer contact prediction for GeoTrans to further fine-tune for multimer contact prediction.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper source code is available and the experiment details are provided carefully in the paper. The results should be able easy to reproduce.\n\nThe paper was well written and all the technical points are described clearly.\n\nAlthough this is a simple pretraining approach, the idea is original and interesting. I have more concern on the evaluation methods (please see my comments on weaknesses).",
            "summary_of_the_review": "Summary of the comments\nThis is an interesting work that has a good impact on the application domain.\nMy main concerns are:\n\n+  potential leakage, please fix this and report new results, if the new results are still significant, I am happy to change my score\n\n+ additional comparisons when replacing ProteinBert with ESM-1b, ESM-MSA-1b are needed because these models demonstrated SOTA results on supervised/unsupervised contact prediction.\n\n+ additional comparisons with pretraining monomer contact prediction using PDB data and ESM-1b as back-bond features for proteins are needed to demonstrate that the Split and Merge Proxy is needed.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper647/Reviewer_oqAy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper647/Reviewer_oqAy"
        ]
    },
    {
        "id": "V4fX_GW1pkn",
        "original": null,
        "number": 2,
        "cdate": 1666574902995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574902995,
        "tmdate": 1666575048050,
        "tddate": null,
        "forum": "o8fqVVKN3H",
        "replyto": "o8fqVVKN3H",
        "invitation": "ICLR.cc/2023/Conference/Paper647/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new approach for protein-protein contact prediction that is based on monomer data (this has been used in related work) and particularly cuts monomers into two sub-parts  and pre-trains the model to merge them back. The results show huge performance gains compared to related works.\n",
            "strength_and_weaknesses": "(+) The paper is mostly straightforward and concise, the approach is reasonable.\n\n(+) The evaluation is extensive, contains various datasets and models, ablation studies, and more detailed analysis. \n\n(-) For someone coming from ML some parts are a bit unclear. For example:\np.1 \"the constraints between given protein sequences\" - what exactly is the form of these constraints?\n\n(-) Related work:\n- Overall the paper focuses heavily on the application. Pretraining has become a huge field for graph neural networks in very short time, and the paper is missing related work from there completely, which could be applied to the monomers. At a venue such as ICLR, that work should be mentioned. For example:\nPairwise Half-graph Discrimination: A Simple Graph-level Self-supervised Strategy for Pre-training Graph Neural Networks, IJCAI 21\n- \"ComplexContact Zeng et al. (2018) is the first work introducing the monomer data into the multimer contact prediction task.\" - This seems to be an important related work, how exactly do they use the data?\n\n(-) The results seem nearly a bit too good and I wonder what is the issue. It seems that the other models do not use pre-training, therefore I wonder why the authors do not compare to ComplexContact in more metrics, and don't apply some existing GNN pre-training approaches.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is rather clearly written. In terms of quality and novelty it seems to me to be ok, if the focus is restricted to reasoning over proteins. Code is provided.\n",
            "summary_of_the_review": "Altogether, to me this seems to be a solid contribution with some issues that could be corrected. I am no expert for reasoning over proteins, therefore other reviewers would have to confirm if the claims w.r.t. novelty and SOTA are true. \n\nMy main concern is that the work is very application-focused and the paper does not really represent new, more general ML technology. But this seems to be a more general question, how such papers are to be handled. I didn't find guidance for this from ICLR. I would find a more general consideration of GNN pretraining more insightful for the community.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper647/Reviewer_HQ2g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper647/Reviewer_HQ2g"
        ]
    },
    {
        "id": "-d9Nf4Naokn",
        "original": null,
        "number": 3,
        "cdate": 1666761558282,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666761558282,
        "tmdate": 1666761558282,
        "tddate": null,
        "forum": "o8fqVVKN3H",
        "replyto": "o8fqVVKN3H",
        "invitation": "ICLR.cc/2023/Conference/Paper647/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel self-supervised approach to pretrain models on protein-protein contact predictions. The approach, called Split and Merger Proxy (SMP), uses protein monomer samples by splitting them into two sub-parts as a proxy to protein-protein complex and pre-trains the model to predict its original \u2018merged\u2019 form. Through this approach, the models can break away from the limitations of the small dataset of labeled protein-protein contact. Experiments have shown that SMP improves the performance of current baselines to achieve state-of-the-art results in downstream protein-protein contact prediction.",
            "strength_and_weaknesses": "Strength:\nThe proposed SMP approach is a smart and novel trick to pretrain models without the use of labeled protein-protein contact dataset.\nStrong empirical results are shown with proposed approach.\n\nWeaknesses:\nImpact seems limited to protein-protein contact prediction\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized and clearly written. The approach is novel but the impact may be limited to protein-protein contact prediction.\nQuestion:\nWhat other domains/applications might the concept of SMP be applied to?\n",
            "summary_of_the_review": "The proposed approach is novel and a clever way to exploit the relatively larger dataset of protein monomers. The empirical performance gains are impressive. However, the impact might be limited to only protein-protein contact prediction.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper647/Reviewer_bLda"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper647/Reviewer_bLda"
        ]
    },
    {
        "id": "NgO-5zENq8",
        "original": null,
        "number": 4,
        "cdate": 1667578559990,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667578559990,
        "tmdate": 1668817241994,
        "tddate": null,
        "forum": "o8fqVVKN3H",
        "replyto": "o8fqVVKN3H",
        "invitation": "ICLR.cc/2023/Conference/Paper647/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes simple pre-training strategy for multi-mer contact prediction.  The strategy is shown to provide empirical benefit across several benchmark tasks.  However the paper does not compare to some of the most widely used methods for predicting contacts (AlphaFold and RosettaFold).",
            "strength_and_weaknesses": "Strengths:\n* The chief benefit of the suggested approach is it's simplicity.\n* The comparison of the performance of GeoTrans with and without SMP is good ablation for checking the extent to which the method improves performance.\n\nWeaknesses:\n* Discussion of prior work.  RoseTTAFold2 is explicitly geared towards interaction prediction, as is AF2 Multimer.  While these methods are primarily built for structure prediction (rather than phrased as for contact prediction) they may easily used for contact prediction.  For example using the predicted alignment error (PAE) output by AF2.\n*  Predicted alignment error as been shown to be effective in predicting existence and strength of contacts in the context of binder design (Bennet et al. 2022).   For RosettaFold a simple option could be pLDDT at the interacting residues.\n* How does pre-training compare to relying on generalization from structure prediction with large (e.g. 384 residue crops), as in AlphaFold?\n* An empirical comparison to AF2 or RosettaFold is crucial.  I will not consider changing my score if a comparison is not to be added.\n\nIn some cases, it seems that the pre-training strategy does not help (in particular, in table 2 in some comparisons to GeoTrans).  Could the authors comment on the situations in which they expect the pre-training will have the largest / smallest improvements?\n\n\nNit:\n* Check for typos: e.g. suppresses \u2014> surpasses?\n* What do you mean by \u201crich\u201d information in the title?  Would be helpful to clarify this vague language in the main text or choose alternative phrasing.\n\n\nReferences:\nBennett, Nathaniel, et al. \"Improving de novo protein binder design with deep learning.\"\u00a0bioRxiv\u00a0(2022).\nBaek, Minkyung, et al. \"Accurate prediction of protein structures and interactions using a three-track neural network.\" Science 373.6557 (2021): 871-876.",
            "clarity,_quality,_novelty_and_reproducibility": "The simple idea of the paper is clearly articulated.  Given the success of AlphaFold at predicting multimers it is not surprising that it works.\nHowever, I would not describe the idea as particularly novel, since AlphaFold is trained on monomers as well.",
            "summary_of_the_review": "A clear explanation and empirical demonstration of simple idea, but with insufficient demonstration of the novelty and comparison + discussion of existing work (namely AlphaFold and RosettaFold).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper647/Reviewer_2Wyz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper647/Reviewer_2Wyz"
        ]
    }
]