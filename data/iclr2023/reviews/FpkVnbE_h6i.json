[
    {
        "id": "Hs3r44NDFD",
        "original": null,
        "number": 1,
        "cdate": 1666521207742,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666521207742,
        "tmdate": 1666527828534,
        "tddate": null,
        "forum": "FpkVnbE_h6i",
        "replyto": "FpkVnbE_h6i",
        "invitation": "ICLR.cc/2023/Conference/Paper1755/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes to consider the generalization ability into the acquisition process of new labeled instances in active learning. The work selects instances with a high value of loss sharpness on pseudo-labels, which can be used for improving the generalization ability of the model based on the finding of Sharpness-Aware Minimization (SAM). Besides, the work theoretically gives the connection between the acquisition score and other scores, which are related to the generalization ability. In addition, the work shows consistent improvement over multiple datasets.",
            "strength_and_weaknesses": "\nStrength:\na. The approach is simple. It is interesting to see that selecting instances with a high value of loss sharpness improves the performance.\nb. The results look promising.\nc. The manuscript is easy to read\n\nWeaknesses\na.\tThe motivation may overlook the relationship of the selected instances and unselected ones. The work simply reduces the second term in Eq.5 by removing the unlabeled instances with highest loss sharpness. The challenges to compute sharpness should be further explained.\n\nb.\tThe method to estimate the loss sharpness of the unlabeled instances may be easy. And a wide lower bound in Theorem 3.1 may be not enough to guarantee the effect of the estimation.\nc.\tIn Experiment Setting, the work follows the settings of the prior works to a very low amount of allowed budget. But the setting is not practical in real world. It will be better if the work can provide the performance on larger budget.\nd.\tIn Baselines of 4.1, the work states it adopts k-means++ seeding algorithm as prior work. The ablation study on the effect of k-means++ seeding algorithm is needed.\ne.\tIn Quantitative Analysis, the work analyzes the time complexity. The description of the equipment is needed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The proposed method is simple and makes sense. The performance is promising. The proposed method is inspired based on SAM by taking the generalization ability into account for active learning. But it may need more innovations to improve the performance.\nClarity: The paper is well written. Easy to read and follow. \nOriginality: The work introduces the loss sharpness from SAM into active learning to improve the generalization ability. However, it doesn\u2019t provide more universal generalization improvement method that matches better with adaptive learning setting.\n",
            "summary_of_the_review": "This work proposes to consider the generalization ability into the acquisition process of new labeled instances in active learning. The work selects instances with a high value of loss sharpness on pseudo-labels, which can be used for improving the generalization ability of the model based on the finding of Sharpness-Aware Minimization (SAM). Besides, the work theoretically gives the connection between the acquisition score and other scores, which are related to the generalization ability. In addition, the work shows consistent improvement over multiple datasets. However, the proposed method seems extend the SAM to active learning setting.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1755/Reviewer_Xi2g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1755/Reviewer_Xi2g"
        ]
    },
    {
        "id": "6J8RmLMdVk",
        "original": null,
        "number": 2,
        "cdate": 1666592103917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592103917,
        "tmdate": 1666592436396,
        "tddate": null,
        "forum": "FpkVnbE_h6i",
        "replyto": "FpkVnbE_h6i",
        "invitation": "ICLR.cc/2023/Conference/Paper1755/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This manuscript describes a new active learning method considering generalization, which is very practical. The key insight is to incorporate the sharpness of loss space when designing the active acquisition function and select unlabeled samples with the maximal perturbed loss for labeling. The experimental results and theoretical evidence confirm the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "Strength:\n1. The paper is well written, the motivations for choices in the method are clear and the idea is simple yet effective, making full use of sharpness-aware minimization.\n2. The presentation of background in Section 2 is thorough and complete.\n3. The empirical evaluation is thorough and conducted on diverse vision-based tasks, including image classification and object detection.\n\nWeakness:\n\n[Motivation] The motivation is reasonable for me to consider the generalization ability in active learning. It might be better to give more examples to help understand why we need to select samples whose perturbed loss is maximum. For example, why should we apply SAM to active learning? Does SAM inherently have generalization properties?\n\n[Relevant works] The work aims to connect active learning and generalization ability. Recent works about active domain adaptation also concern generalization. Several relevant references are missing [a, b, c, d]. The similarities and dissimilarities should be discussed.\n\n[Experiment]\n- For more clear understanding, could you provide some qualitative and quantitative results for the loss landscape?\n- I would like to see why this work is not applied to the semantic segmentation problem and compared to works in that.\n\nRefs:\n[a] Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings.\n\n[b] Active Learning for Domain Adaptation: An Energy-Based Approach.\n\n[c] Discrepancy-Based Active Learning for Domain Adaptation.\n\n[d] Active Adversarial Domain Adaptation.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written nicely and is easy to follow.\n\nQuality & Novelty: As discussed above, sufficient novelty is contained in the proposed method. \n\nReproducibility lacks sometimes.\n\n",
            "summary_of_the_review": "Generally, I find this paper to touch on an interesting problem with a novel sharpness-aware minimization based active learning for generalization. Yet, it still requires clarification and some solid empirical support before warranting acceptance of this paper.\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1755/Reviewer_tmqo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1755/Reviewer_tmqo"
        ]
    },
    {
        "id": "8q1A1mFPyG2",
        "original": null,
        "number": 3,
        "cdate": 1666623419096,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623419096,
        "tmdate": 1666623419096,
        "tddate": null,
        "forum": "FpkVnbE_h6i",
        "replyto": "FpkVnbE_h6i",
        "invitation": "ICLR.cc/2023/Conference/Paper1755/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The study investigates the active learning problem with sharpness information in loss space. It proposes a method named sharpness-aware active learning to select unlabeled instances potentially having maximal perturbation loss. The method has a theoretical guarantee on the generalization error of active learning. Experiment results show the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "Strengths:\na. The proposed method takes full advantage of sharpness information of loss. \nb. Generalization error of active learning is upper bounded by the method.\nc. Experiments verify the performance of the method.\n\nWeakness:\na. More datasets should be considered in the experiments, especially for large-scale ones (e.g., Webvision, ImageNet).\nb. In Table 1, the proposed method does not beat the previous studies with a much margin. It seems that SAM optimizer plays a more important role in the improvement.\nc. The time complexity of the proposed method is much higher than that of the most of existing methods. Is it possible to accelerate the method without degradation?\nd. Is there a clear cut of value for \\rho to satisfy Proposition 3.2?\n",
            "clarity,_quality,_novelty_and_reproducibility": "a. The submission is clear and well-written. \nb. The usage of sharpness with theoretical guarantees in active learning is interesting and new.\nc. The submission does not provide source code. The experimental results are not guaranteed to have reproducibility.",
            "summary_of_the_review": "The work proposes a new method for active learning by adopting the sharpness information in loss space. The method is interesting with a design of minmax optimization. Experiments show the effectiveness of the method. However, the method does not outperform the existing methods, and the SAM optimizer affects more in the improvements. Also the proposed method is time consuming. Overall, the reviewer believes that the work is boarderline and needs further polishing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1755/Reviewer_3NDR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1755/Reviewer_3NDR"
        ]
    }
]