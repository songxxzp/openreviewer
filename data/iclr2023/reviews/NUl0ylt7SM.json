[
    {
        "id": "6DordS5eMDc",
        "original": null,
        "number": 1,
        "cdate": 1666585064369,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585064369,
        "tmdate": 1666585064369,
        "tddate": null,
        "forum": "NUl0ylt7SM",
        "replyto": "NUl0ylt7SM",
        "invitation": "ICLR.cc/2023/Conference/Paper5728/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the action representations emerged in multi-task policy training. The authors study a two-stream policy architecture for multi-task reinforcement learning. One input stream encodes the observation and the other input stream embeds the task ID. Finally, the two embedding vectors are concatenated and fed into the policy network. When proper normalization and noise injection are applied, the authors observed that the task ID embeddings space emerges where each embedding in this space encodes a meaning behavior. Embeddings in the emerged space can be interpolated or composed to generate interesting behaviors. The authors then further demonstrate that optimization in this embedding space can yield fast adaptation to downstream tasks. Visualization of the learned state embedding space and task embedding space provides more intuitive understanding of the learned structures.",
            "strength_and_weaknesses": "# Strength\n* Overall the paper is clearly written and easy to follow. A few flaws undermines the readability a bit, which are discussed below.\n* The finding of the emergence of the action embedding space is interesting. The proposed architecture is simple and straightforward. Yet this simple architecture yields interesting findings. The strong empirical performance against meta-RL baselines could inspire us to rethink the fast adaptation problem.\n\n# Weakness\n* The tasks in the experiments are relatively simple. Each task can be effectively described by one or two factors - which is nice for visualization and understanding. However, it is unclear if the same phenomenon would be observed in more complicated tasks such as object manipulation.\n* I find the experiment results in Section 4.6 rather confusing. I do not understand the purpose of this experiment. When the task IDs are one-hot encoded, the input layer essentially serves as an embedding layer. It does change the interaction between the state embedding and the task embedding from concatenation to addition, but a certain equivalence can be shown for these two different types of interaction [1]. The fact that this one-hot variant does not even work well during training seems rather confusing to me. I will appreciate if the authors could provide better clarity on this experiment.\n* One question I am interested in asking is the importance of embedding normalization and noise injection. I think it will be good to conduct an ablation study for this purpose.\n* The section paragraph of the related work section looks confusing to me. The authors list a few works that address multi-task RL. But I don't see their connections to this work that are worth highlighting.\n* Writing and presentation can be improved. For example, what are $\\theta_{1}$, $\\theta_{2}$, $\\bar{\\theta_{1}}$, $\\bar{\\theta_{2}}$ in Algorithm 1? The sampled action $a_{t}$ in the second for-loop is never used. Overall, the role of Algorithm 1 is unclear. It presents a standard multi-task policy training procedure which does not highlight LTE. Figure 3 and Figure 4 need more explanation in either the caption or the text. The screenshots are incomprehensible in their current form.\n\n# Questions and discussion\n* The second paragraph of Section 4.3 says \"The coefficient $\\beta$ is searched to better fit the target task\". Why is search needed here? The new task is a convex combination of two reference tasks. I would expect we can just interpolate the embeddings with the same coefficient. Did the authors try it? What did you observe?\n\n# Minor issues\n* In the 5th line in the second paragraph on page 5: \"The newly composed task is usually lie in a new modality...\" Please fix the grammar.\n* In the x-axis of Figure 5: what does \"steps\" mean? Does it mean episodes?\n* In the second paragraph in Section 5: reference to Jeon et al and Chandak et al should use \\citep{}.\n\n**References**\n1. Dumoulin _et al_ 2018, Feature-wise transformations, https://distill.pub/2018/feature-wise-transformations/ ",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: Fair. Suggestions for improvements on presentation are provided above.\n* Quality: Good, though I think an ablation study on embedding normalization and noise injection could provide better understanding.\n* Novelty: To the best of my knowledge, the findings presented in this paper are novel.\n* Reproducibility: Fair. Sourcecode is not provided. But the appendix provides enough implementation details.",
            "summary_of_the_review": "Overall I think the findings in this paper are interesting and could benefit the community. But the presentation can be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5728/Reviewer_YSuL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5728/Reviewer_YSuL"
        ]
    },
    {
        "id": "rFjjGKKzPF",
        "original": null,
        "number": 2,
        "cdate": 1666623077483,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623077483,
        "tmdate": 1666623077483,
        "tddate": null,
        "forum": "NUl0ylt7SM",
        "replyto": "NUl0ylt7SM",
        "invitation": "ICLR.cc/2023/Conference/Paper5728/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an architectures that separately encodes sensory information and the task id in a multi-task setting. These are then concatenated to processed by a neural network that produces the action. The paper imposes constraints on the task representation (or action representation in the paper): the representation is restricted to the unit sphere and trained with some noise. The paper then evaluates the proposed architecture on a number of simple locomotion domains. Empirically the methods compares favourably to multi-task and meta-RL baselines. The paper also demonstrates that it is possible to interpolate in the task embedding space both within the same task (e.g. different velocities) and between different locomotion modes.",
            "strength_and_weaknesses": "Strengths:\n* interesting results with a relatively simple methods\n\nWeaknesses:\n* From what I can tell the paper proposes essentially a well-regularized version of learned task embeddings. It would be nice to see an ablation of the two constraints (noise and restriction to the unit sphere). Are both of these necessary to achieve the results shown? Would other regularization ideas work as well (e.g. information bottleneck?). It is interesting to see that this methods appears to perform well but it is unclear to me what specifically makes it work.\n* relatively toy environments. I think the paper would be substantially stronger if the similar results were shown in more complex settings.\n* I think task representation might be a better term than action representation (since action already has a specific meaning in RL). At the moment I think readers might be confused. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and would be straightforward to reproduce. I think the novelty of the approach is somewhat limited (basically a well-regularized task embedding) but there are some intriguing empirical results. ",
            "summary_of_the_review": "Interesting paper showing that a well regularized task embedding is competitive with multi task and meta-RL baselines on a set of relatively toy locomotion domains. The embedding also allows some interesting interpolation. I would like to see some more analysis to understand what specifically makes this approach work. I would also like to see if it scales to less toy settings. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5728/Reviewer_VRfP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5728/Reviewer_VRfP"
        ]
    },
    {
        "id": "9P6SZSEniUI",
        "original": null,
        "number": 3,
        "cdate": 1666690536130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690536130,
        "tmdate": 1666690584357,
        "tddate": null,
        "forum": "NUl0ylt7SM",
        "replyto": "NUl0ylt7SM",
        "invitation": "ICLR.cc/2023/Conference/Paper5728/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a learning framework (Figure 1) for action abstraction in the scheme of multi-task reinforcement learning (RL). A latent action representation is assigned to each task, and the model tried to learn the tasks with state representation (shared among tasks) and action representations (specific to each task, task embedding) segregated. It was observed that the model self-developed interpretable and compositional action presentations via RL. The learned action representation can facilitate quick, gradient-free transfer learning. More interestingly, it was shown an interpolation in the latent action space led to an explainable \"interpolation\" in the behavior space. The main contribution is a proof of concept of a novel framework for action abstraction.",
            "strength_and_weaknesses": "## **[Strength]**\n\n- The methodology is simple and intuitive (no additional loss functions for action abstraction).\n- The idea and result are quite interesting.\n- Interpolation/composition in the latent action space surprisingly works.\n- The proposed framework provides novel insights on how to leverage the task distribution for learning high-level motor skills\n\n\n## **[Weakness]**\n\n### Related work is incomplete\nWhile the paper mentioned Representation learning in RL, multi-task/metaRL in Sec. 5, important pieces are missing. A clearly related batch of studies are skill discovery with deep RL, which is closely related. Some related references are:\n- Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019\n- Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2020.\n-  Kelvin Xu, Siddharth Verma, Chelsea Finn, and Sergey Levine. Continual learning of control primitives: Skill discovery via reset-games. In Advances in Neural Information Processing Systems, volume 33, 2020.\n\n### The proposed framework heavily relies on the task set\nTo achieve the proposed results, there are quite a few requirements: e.g., a distribution of uni/multi-modal tasks with well-defined reward functions, the tasks shared the same state space. By contrast, skill discovery methods can achieve similar results without a reward function. Furthermore, people and animals can often self-develop motor primitives in a single task.\n\n### Technical details are not clear\nFor example, model architecture, hyper-parameters, training schemes: Sec.A.3.2 is clearly not enough, not to say that source code was not provided. The authors should provide more details for reproducibility.\n\n### Statistical significance of major claims is not well addressed\nWhile the most interesting result --- interpretable interpolation and composition of LTEs, was well presented using an example agent (Figure 3, 4), it is unclear whether this result is common with different random seeds. That is to say, the authors did not demonstrate statistical reliability of the phenomenon showed in Figure 3,4, e.g., what is the percentage of random seeds that can achieve such a result?  More evaluations remain to be complete.\n\n### Lack of ablation studies\nMany design choices lack explanation. For example, the two techniques used to constrain the space, normalizing LTE and injecting random noise. Why normalizing LTE? What will happen if there is no such regularization? I guess these two techniques are essential to the success of the proposed framework, yet remains to be discussed.\n\n### The writing quality is to be improved.\nThe paper seems not well polished. Belows are some issues I found. Nonetheless, I think the paper need a careful and throughout proofreading. Also, reference should also be manually checked.\n- The first sentence in Sec.1: Deep RL can learn --> deep RL agents can learn\n- Page 5: Specifically, We perform ... --> Specifically, we perform\n- Page 5: We visualized the LSEs and and LTEs: duplicate \"and\"\n- Page 9: the end, incorrect citation style\n- Page 10: Duplicate citation of Chandak 2019\n- Page 11: {Bayes}\n- Page 14: control control\n\n\n### **[Questions]**\n1. Will the authors release source code?\n2. How to initialize $\\mathcal{Z}_\\mathcal{T}$ ?\n3. I am curious about the result of extrapolation in LTE space, e.g., if $\\beta < 0$ or $\\beta >1$, what will happen?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality is moderate but can be improved through proofreading. The authors did a good job on presenting the ideas and experimental results.  The key idea is clear and intuitive, while implementation details need to be clarified. The proposed framework is novel to my knowledge. ",
            "summary_of_the_review": "Overall, I appreciate this paper as it proposes a simple method to achieve interesting results, though it poses some demands to the task set. The goodness is the latent action representation showed self-organization via RL without additional, explicit loss functions for this purpose, which provides some insights for both machine learning and cognitive science about how motor primitives could be autonomously acquired and leveraged for transfer learning. The insights are, however, limited because there are a lot of points for the studies to improve as I mentioned above. Overall, my recommendation is around the borderline as for now, and my rating may increase given a constructive rebuttal.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5728/Reviewer_oCDb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5728/Reviewer_oCDb"
        ]
    },
    {
        "id": "R_Fj2XLxkZw",
        "original": null,
        "number": 4,
        "cdate": 1667132330946,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667132330946,
        "tmdate": 1667132330946,
        "tddate": null,
        "forum": "NUl0ylt7SM",
        "replyto": "NUl0ylt7SM",
        "invitation": "ICLR.cc/2023/Conference/Paper5728/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an action representation scheme for multi-task training, which is relatively simple in that the action representation is decoupled with the sensory represenation. The proposed method allows for emergent action composition as well as interpolation in the action space. The validity of the method is demonstrated on computational experiments in a Mujoco environment, compared against several other RL schemes. ",
            "strength_and_weaknesses": "Strengths: \n- This work is well-motivated. There is certainly a lack of effective task representation for complex multi-task environment. The proposed framework is addressing an timely and important problem. \n- The proposed idea is interesting and looks making sense in general. \n- The method is evaluated, compared agasint a reasonable choice of state-of-the-art \n\nWeaknesses: \n- The methodological details are not clearly described/analyzed. Perhaps the paper is to talk about the arguably simplea idea, but other than the framework in Figure 1, additional description/investigation is needed on what details in action representation would make impact on the performance. Would the MLP be the best choice for this purpose? \n- The use of terminology is a bit abused. It is not very clear the task composition in this work can be called \"emergent.\" \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is relatively well-written and easy to follow, but more detailed invetigation on the detailed choice of action representation network would be useful. \n- The approach looks technically sound in general, but details would be benefitical. \n- The paper addresses a timely and important problem and presents a simple but new idea. \n",
            "summary_of_the_review": "The reviewer thinks that this paper presents an interesting idea on an important problem. The paper will be benefited by adding analysis on the detailed algorithm choices/variations. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5728/Reviewer_MYUq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5728/Reviewer_MYUq"
        ]
    }
]