[
    {
        "id": "tAXl-WucD4C",
        "original": null,
        "number": 1,
        "cdate": 1666697739095,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697739095,
        "tmdate": 1669819554783,
        "tddate": null,
        "forum": "_j4ZUpoNO1e",
        "replyto": "_j4ZUpoNO1e",
        "invitation": "ICLR.cc/2023/Conference/Paper1341/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper describes the method for training the speech enhancements model using only real noisy speech (as opposed to the typical use of labeled clean and noisy speech). The paper describes the use of a Quality Predictor (Q) to guide the training of the speech enhancement model.\n\nThe main contributions of the paper are:\n(i) GAN-like training (Bi-level optimization) to train the quality predictor and speech enhancement model jointly which improves over the baseline.\n(ii) Use of adversarial robust quality predictor as a speech enhancement model. ",
            "strength_and_weaknesses": "Strength:\n(i) Paper clearly describes the use of quality predictor for speech enhancement. \n(ii) Empirical results demonstrate the usefulness of the proposed GAN-like training for speech enhancement models.\n(iii) Appendix contains additional studies which further strengthen the usefulness of GAN-like training.\n\nWeakness:\n(i) There are bigger datasets for speech enhancements such as DNS (https://www.microsoft.com/en-us/research/academic-program/deep-noise-suppression-challenge-icassp-2022/) which contains over 500 hours of clean speech. On the other hand, the Quality Predictor dataset (IUB) is around 45 hours.\n- Although the GAN-like training is useful, it would be interesting to see how much they help with really powerful SE baselines.\n(ii) Semi-supervised training = GAN-like training with a fully-supervised SE model.\n- The reverse should also be tried -> GAN-like training followed by fully-supervised training.\n(iii) Comparison with related methods such MetricGAN-U (Fu et al. 2022) are missing. \n\nQuestions:\n(i) Any particular reason metrics like PESQ, etc. were not evaluated. (Line 4 of Section 4.2.3 has a passing remark on PESQ of a prior-art). \n\n(ii) `Because the calculation of PESQ and training of NORESQA rely on two signal processing measures, Signal-to-Noise Ratio (SNR) and Scale-Invariant Signal to Distortion Ratio (SI-SDR) to compare the quality of the two inputs, synthetic data is needed to train the quality prediction model. However, in our proposed training method, it is not necessarily needed.`\n- The paper proposes a training of quality prediction model using a real dataset, why wouldn't that be used? What did I miss?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well and easy to follow. \n\nThe major ideas described in this paper are already described in the prior art.\n\n(i) `3.2.1 DeepDream-like Training` - described in Fu et al. 2019\n\n(ii) `3.2.2 GAN-like training` - described in Fu et al. 2022 (MetricGAN-U)\n\n(iii) `3.2.3 Robust Quality Prediction model` - As described in the experimental section (4.2.3), this method is inferior to GAN-like training.",
            "summary_of_the_review": "The paper describes a training framework for speech enhancement using real noisy speech, it lacks enough novel ideas. The most important ideas described in this paper are already tried. I really appreciate the thoroughness of the experiment section and appendix.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1341/Reviewer_zuxz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1341/Reviewer_zuxz"
        ]
    },
    {
        "id": "UihjX-GffXS",
        "original": null,
        "number": 2,
        "cdate": 1666703444907,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703444907,
        "tmdate": 1669845708602,
        "tddate": null,
        "forum": "_j4ZUpoNO1e",
        "replyto": "_j4ZUpoNO1e",
        "invitation": "ICLR.cc/2023/Conference/Paper1341/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper describes a method to train a speech enhancement system using a non-intrusive speech quality estimator. Many combinations of approaches, architectures, datasets, and training schedules are compared and measured with many metrics. The proposed approach does not do as well as the supervised or semi-supervised DPT-FSNet model, but does improve overall quality as estimated either through listening tests or DNSMOS p.835 compared to the original noisy speech.",
            "strength_and_weaknesses": "Strengths:\n* Important problem. There is a great deal of noisy speech out there, much more than the amount of clean speech. It is also much easier to acquire noisy speech that is matched to a particular application than to create it synthetically. Thus a speech enhancement system that could be trained from such noisy speech alone, without the need for a clean reference, would be very valuable.\n* Novelty: While other papers have trained speech enhancement systems using reference-based speech quality or intelligibility predictors, to my knowledge this is the first paper using a reference-free predictor.\n\nWeaknesses:\n* Clarity. I find the paper very hard to follow. Specifically, as mentioned in my summary, many many variations of systems and experiments are compared and it is difficult to confirm the claims that the paper makes with regards to these results or follow the chain of reasoning through the entire set of experiments. I am very familiar with the field in general, but just understanding which version of which approach was trained on which data and what that meant for the assumptions or hypotheses that were being evaluated in each experiment was very difficult to follow. \n* Proposed speech enhancement model is a bit out of date. The speech enhancement component of the system used a magnitude-only mask that was constrained to lie between 0 and 1, citing papers from 2018. This is ok, but since then, most approaches have shifted to approaches like TAS-Net or masking of complex signals. If the goal is to maximize quality, then it would be much more realistic to go back to the time domain from the enhancer before going into the quality estimator so that any phase artifacts or effects are properly accounted for (i.e., by showing up in the magnitude spectrogram after the resynthesis-reanalysis process).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity (see above as well):\n* One opportunity to simplify the experiments would be to fix the unobtrusive speech quality prediction model. I think that the inclusion of the training of the quality predictor as a factor under investigation muddied the water, since sometimes a separate speech quality predictor was used to train the speech quality predictor under investigation. Separating the data and models used for quality prediction and speech enhancement would significantly streamline the message of the paper.\n\nReproducibility:\n* The abstract states that code will be released after publication but it is not submitted as supplementary material.",
            "summary_of_the_review": "This paper approaches a significant problem with a potentially novel solution, but the clarity of the explanation makes it difficult to understand and to confirm.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1341/Reviewer_6yTX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1341/Reviewer_6yTX"
        ]
    },
    {
        "id": "HD3YBSHTTE",
        "original": null,
        "number": 3,
        "cdate": 1666703925526,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703925526,
        "tmdate": 1669994093886,
        "tddate": null,
        "forum": "_j4ZUpoNO1e",
        "replyto": "_j4ZUpoNO1e",
        "invitation": "ICLR.cc/2023/Conference/Paper1341/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors consider the problem of training speech enhancement (SE) models using only noisy speech utterances an their associated quality assessments, which are either annotated, or predicted by a quality prediction model (QPM). Several related approaches are considered, including 1) QPM score based SE optimization with a fixed QPM 2) joint GAN-like training of the SE and QPM, and 3) Enhancement based on an adversarially optimized QPM, by gradient-based noise removal of it's input features.\n\nThe approaches are evaluated primarily based on noisy testsets with no estimated clean reference (Vox2 and  DNS3), based on the DNSMOS P.835 external QPM model's scores, indicating small gains over MixIT and Modified MixIT (Their semi-supervised systems are trained on DNSMOS P.808 external QPM model scores, which, to the extent that P.808 and  P.835 are similar by virtue of training data and otherwise, may seriously affect the integrity of the semi-supervised results) . The approaches are also evaluated on 600 utterances of DNS1 by AMT workers, with small gains, modest gains, and degradation in overall quality on \"real\", \"noreverb\", and \"reverb\" subsets. Notably, the QPM model scores (tables 5,6,7) and AMT scores (tables 8,9,10) suggest the same performance trends, despite significant differences in their absolute scores, giving some credibility to utilizing external QPM models for evaluation.",
            "strength_and_weaknesses": "Strengths\n\n- Training with light supervision such as quality scores is an interesting and important problem in ML.\n- In general the problem and approaches taken are adequately explained.\n- As discussed above, the results suggest that the presented SE approaches lead to gains.\n\nLimitations\n\n- The results are solely based on the subjective quality of enhanced speech on noisy test sets with no clean reference. Moreover, most of these quality assessments are model generated, i.e. based on DNSMOS P.835 QPM scores. It is encouraging that the AMT and QPM scores follow the same trends, but traditional metrics such as SNR gain on datasets with parallel clean data should also be included.\n- As mentioned in the summary, their semi-supervised systems are trained on DNSMOS P.808 external QPM model scores, which, to the extent that P.808 and P.835 are similar by virtue of training data and otherwise, may seriously affect the integrity of the semi-supervised results.\n- Following up on the previous points, it is very difficult to gauge the signficance of these gains in quality score without audio examples, and the lack of traditional metrics such as SNR gain makes this judgement even more difficult.\n- In addition, the authors compare to only one recent SOTA approach for audio separation (MixIT, and Modified MixIT), which was not even formulated as an SE approach. This also makes the signficance of the results very difficult to assess.\n- While the presented methods may be novel to the SE application, they are already well established ML techniques, making the novelty and signficance of the paper for a method-focused ML conference low.\n- The grammar in the paper is slightly below par for an ICLR paper, which requires another revision.\n\nMinor Corrections:\n\nused to prevent enhanced magnitude spectrogram Y has T-F bins smaller than zero,\n->used to prevent the enhanced magnitude spectrogram Y from having T-F bins smaller than zero,\n\nTo utilize both benefits of synthetic clean/noisy speech pairs (easy to learn) and real noisy speech (large amount),\n->To realize the benefits of utilizing both synthetic clean/noisy speech pairs (supervised) and real noisy speech (plentiful) during training,\n\nThe quality predictor is then concatenated after a randomly-initialized SE model.\n->The quality predictor is then applied to the output of a randomly-initialized SE model.\n\nnon-intrusive (no clean reference is needed)\n->If you really want to use this terminology (it is counterintuitive), define it earlier in the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See S&W section.",
            "summary_of_the_review": "Overall I feel that audio demonstrations, results on parallel datasets with traditional metrics such as SNR gain, and additional comparisons with SOTA SE approaches are needed before the paper is ready for publication. I also feel that the paper is perhaps better suited for an application-focused conference as the ML novelty of the paper is low.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1341/Reviewer_kbKN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1341/Reviewer_kbKN"
        ]
    },
    {
        "id": "DZLSzXY6oS",
        "original": null,
        "number": 4,
        "cdate": 1666964180537,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666964180537,
        "tmdate": 1666964180537,
        "tddate": null,
        "forum": "_j4ZUpoNO1e",
        "replyto": "_j4ZUpoNO1e",
        "invitation": "ICLR.cc/2023/Conference/Paper1341/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors suggest an approach towards directly exploiting originally noisy speech w/o the need for a clean reference.\nThe algorithm is presented and code will be released. A combination with \"common\" denoising by having a reference available is possible.",
            "strength_and_weaknesses": "The method is simple, yet appears effective. It is well described and tested sufficiently.\nHaving more audio references would be a good addition.\nAlso, seeing more on ablation studies, comments on full reproducibility, additional comparative quantative result interpretetion and limitation considerations would make this an even stronger candidate.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very clearly.\nThe overall quality of presentation and writing is good.\nReproducibility is not fully clear in particular when it comes to the last details of data and hyperparameters.\nHowever, release of code as promised will make things clear, supposedly.",
            "summary_of_the_review": "A good paper in terms of writing and quality.\nWhat's more - the approach seems to be working and is rather a potential game changer in SE and beyond, as this could also be applied to other signal types in the noise.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1341/Reviewer_Pxpx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1341/Reviewer_Pxpx"
        ]
    }
]