[
    {
        "id": "aVwNqghxYN",
        "original": null,
        "number": 1,
        "cdate": 1666754957301,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666754957301,
        "tmdate": 1668645611192,
        "tddate": null,
        "forum": "hZRxiAZFJC",
        "replyto": "hZRxiAZFJC",
        "invitation": "ICLR.cc/2023/Conference/Paper3928/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors concentrate on variants of the fair classification problem. They unify some of the most popular fairness notions in a general representation framework by defining a quantity whose magnitude reveals (dis-)advantage to specific groups. By using the aforementioned quantity, they represent the fair classification optimization problem, derive a gradient descent (GD) algorithm to solve this problem and provide intuition on the \"weights\" that are used to update the GD iterations by bridging them with the protected groups.",
            "strength_and_weaknesses": "**Strength:** I believe the literature is summarized very well, and the paper is written well in general. The motivation is clear and the topic covered is relevant to the research our community is conducting. I really like equation (1), and the related proofs in Example 1 and Appendix B.\n\n**Weaknesses:** \n*I list my major and minor concerns that made me recommend a rejection. However, I still like some aspects of the paper, hence I hope that my comments would be found useful by the authors to improve the paper's quality. I am also going to stay active during the discussion period in case the authors have updates or questions regarding my review.*\n\n***Major Weaknesses/Questions***\n- I think this framework is just providing us with a higher-level notation for the fair classifier optimization problem, but unless I am not seeing something, we already know these gradient descent updates in each of the covered settings. For example, take the algorithm explained in 3.1, and replace $F_k(\\mathcal{T}, h_\\theta)$ with the specific definition of Accuracy Parity (AP). Then the resulting gradient descent algorithm is already known. Same for any other fairness notion covered in this work. In general, I do not think there is much advantage that comes with the new expression $F_k(\\mathcal{T}, h_\\theta)$ in addition to providing some intuition.\n- The optimization/duality/GD discussions are lacking assumptions, discussions, and citations. The convexity of the functions is never discussed, but duality is being used. Convergence to local solutions instead of global solutions is not 'warned'. The paper concentrates on the computational tools and numerical experiments that the theoretical arguments look weaker. \n- The proposed 'weighting' is obtained immediately in the alternating GD setting. There is no selection of these weights, rather everything is a corollary of using GD.\n- Lemma 1 looks redundant, as negative weights come automatically unless we rule them out.\n- Further questions and minor weaknesses that are enumerated next.\n\n***Minor Weaknesses/Questions***\n- Introduction: Maybe the authors can mention that the designing phase of the algorithms has access to the so-called sensitive attributes, as there is a new line of research where the optimizer is oblivious. See, e.g., Mozannar, H., Ohannessian, M., & Srebro, N. ICML (2020)\n- In the introduction, the 'reweighting scheme' is not clear. This is understood later.\n- Page 2: \"They are also often limited in the range of ...\" the word 'often' sounds a little subjective. Could the authors please consider adding citations to or deleting such sentences?\n- Page 2: \"gradient descent based methods\" -> do the authors mean gradient descent based solutions instead?\n- Page 2: \"update the weights of the examples\" is not clear yet.\n- python -> I think 'p' should be capitalized\n- Page 2: while discussing that the library will be very similar to PyTorch functions, could the authors please also discuss that the user needs to provide the sensitive attributes?\n- I personally think the notation abuse of $\\mathbb{P}(E)$ referring to both empirical and true probabilities is more than a slight notational abuse. For example, in equation (1), the term $\\mathbb{P}(h_\\theta(x) \\neq y | \\mathcal{T}_{k'})$ is hard to grasp.\n- Section 2.1: When the authors state $F_k$ as a representation of whether or not a group is advantaged, maybe they can highlight this with respect to a classifier. Sometimes in fairness literature \"advantaged\" groups mean those who have historically been associated with better/higher rewards.\n- Domain of $F_k$ includes the set of distributions, but $F_k(\\mathcal{T}, h_\\theta)$ takes a dataset. Probably the notation should be formalized with $\\mathcal{T}$ denoting the **empirical distribution** of the training set instead.\n- Maybe the authors can define $F_{(r)}$ notation uses $(r)$ as the group number $r$ refers to?\n- \"$0.01$ .. is a good rule of thumb\" -> why? This sounds subjective.\n- Section 3.4, \"$80\\%$ rule in the US\" -> similarly, there are no references here.\n-  Minor comment: In the appendix, the proof is re-sated as Lemma 2, maybe this can be fixed to Lemma 1 again?\n- The notation $[1,K]$ is confusing as $k \\in [1,K]$ is not continuous, hence we probably need the $k \\in [K]$ shorthand instead.\n- In the optimization problems `arg min` and `st.` are not aligned.\n- The gradient on top of page 4 is missing its function.\n\n***Optimization Related Issues/Questions***\n- Firstly, problem (2) is very general and there are lots of very strong techniques proposed to solve special cases of this problem. I think the use of GDs should be motivated further. \n- Shouldn't the Lagrangian dual problem flip the max and min operators (e.g., in (3))? Because the Lagrangian dual function first minimizes the Lagrangian function over the primal optimization parameters, and the Lagrangian dual problem maximizes the Lagrangian function over the dual variables $\\lambda$?\n- To use duality we need a lot of assumptions. Here nothing is mentioned. The compactness of the feasible sets, convexity, whether or not strong duality holds, whether the proposed algorithm converges to a global solution or not and whether it means the fairness is not satisfied due to the relaxations, are not discussed. **Update: The authors replied to this concern, hence I am marginally increasing my score.**\n- The optimization problems use $\\arg\\min$ instead of $\\min$. However, $\\arg\\min$ returns sets and the duality arguments are for optimal values instead. It is not very usual (even correct) to use $\\arg\\min$ in my view. Unless we try to address a specific optimal solution that belongs to the $\\arg\\min$ set, if we are referring to an optimization problem, the standard approach is to use $\\inf$ (or $\\min$ if existence is discussed).\n- Does the use of surrogates result in the promised fairness level not holding anymore? In general, the proposed method might give *a* solution, but is it feasible? Overall, I am not very convinced with the current discussions. \n- The values $\\omega_k$ seem to be coming immediately from the gradients, am I wrong? I think they are an immediate result of splitting the updates of the primal and dual variables.\n- When it comes to the proposed decomposition of alternatingly updating the primal and dual variables, there are many existing works already, hence I believe this approach is not new and looks like a less sophisticated version of the primal-dual methods that the literature uses.\n- Does \"Algorithm 1\" add anything to the paper? Maybe it should be clear from Section 3.1.",
            "clarity,_quality,_novelty_and_reproducibility": "For the details of these points, please see my review of \"Strength and Weakness\". However, to give a high-level answer:\n\n**Clarity:** The overall idea is clear. The notation may sometimes get confusing.\n\n**Quality:** The writing of the paper has a very good quality.\n\n**Novelty:** I believe the paper has some novelty in the general fairness definition (S2.1), however, the main idea and the solution technique look very standard.\n\n**Reproducibility:** The paper is reproducible because the steps used in the proposed algorithm are explained in detail and a Python package is provided.",
            "summary_of_the_review": "In general, the paper covers an interesting topic, provides a useful Python package, and has extensive numerical experiments; however, I think the work lacks novelty as the problems solved and the solution techniques are all known.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3928/Reviewer_BapZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3928/Reviewer_BapZ"
        ]
    },
    {
        "id": "qJ0wCw5fMx",
        "original": null,
        "number": 2,
        "cdate": 1666756327309,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756327309,
        "tmdate": 1666756327309,
        "tddate": null,
        "forum": "hZRxiAZFJC",
        "replyto": "hZRxiAZFJC",
        "invitation": "ICLR.cc/2023/Conference/Paper3928/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a python package and an approach called fairgrad. The goal of fairgrad is to learn model parameters that satisfy accuracy parity or its approximate equivalent across groups of a dataset. Fairgrad does this by formulating a constrained optimization problem, which is to minimize a model perfomance metric subject to accuracy parity constraints across demographic groups that partition the training dataset. This formulation can then be relaxed and put in a langrangian form where the lagrange multipliers are applied to the accuracy parity constraint for each group. This entire formulation can then be 'solved' via an alternating gradient descent formulation. The first component of the alternating gradient descent formulation applied gradient descent to the langrange multipliers. Given the current setting of the langrange multipliers, gradient descent is then applied again to the model parameters. This scheme is iterated until convergence or a pre-specified number of iterations. The Fairgrad package implements this scheme automatically. The empirical performance of the proposed scheme is demonstrated on a variety of benchmarks.",
            "strength_and_weaknesses": "### Strength\n\nIn my opinion, the primary strength of this work is the python package that is being released as part of this paper. Given the package, the proposed formulation can then be applied across a variety of different settings more easily. In my opinion, this alone, is an important enough contribution of this paper.\n\n\n### Weaknesses\n\n- Novelty in formulation: As a reviewer, I actually don't place any emphasis on the 'novelty' of a work, so I don't see this as a weakness. My only point here is that the formulation here is not different that those in [Jiang & Nachum, Cotter et al.] and others that all employ the Langrange multplier formulation. The key difference seems to be that this formulation allows for negative weights/multipliers which other formulations don't. I think the paper is written to introduce fairgrad as a new method, but in my opinion it is actually a generalization of existing methods. Perhaps a better pitch for this paper would be that these existing methods are generalized and implemented as part of clean python/pytorch package. In my opinion, that alone, is a worthy ICLR submission. The push to make Fairgrad a unique method  seems a stretch to me. I am willing to change my opinion if the authors believe that this is not the case.\n\n- The empirical results are helpful, but I would recommend that the authors jettison the traditional Adult dataset and use the newly proposed Adult dataset from the Neurips 2021 paper by Ding et. al. titled retiring adult. Results on that dataset would be more convincing. \n\n- In the experiments, reporting the average fairness level for all groups is slightly misleading since all the improvement might be in the dominant group. I think these should be broken into groups.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite readable and relatively clear. The Fairgrad approach builds on the previously proposed approaches related to Lagrangian formulations of fair classification.",
            "summary_of_the_review": "The paper proposed a formulation, FairGrad, that is presented in a python package that is then tested across a wide variety of datasets. Overall, the proposed approach can be easily applied by others in the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3928/Reviewer_chVG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3928/Reviewer_chVG"
        ]
    },
    {
        "id": "QBZIBkMzgZT",
        "original": null,
        "number": 3,
        "cdate": 1666853414184,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666853414184,
        "tmdate": 1666853414184,
        "tddate": null,
        "forum": "hZRxiAZFJC",
        "replyto": "hZRxiAZFJC",
        "invitation": "ICLR.cc/2023/Conference/Paper3928/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose group reweighting algorithms for achieving group fairness.",
            "strength_and_weaknesses": "+: \n\nThe performance of the algorithm seems to be better than the state of the arts.\n\nThe paper writing is clear.\n\n-:\n\nThe technical novelty seems limited. The difference between this work and the previous work [Agarwal et al., 2018] seems very marginal. The only difference here is that this work uses a gradient descent ascent algorithm while Agarwal et al. used the online convex optimization algorithm (exponentiated gradient) with the best response for one of the two players. \n\nFurthermore, the difference between this work and the previous work [Roh et al., 2020] also seems marginal -- (1) min max optimization vs bilevel optimization and (2) negative group weights are allowed. \n\nThe breadth of baseline algorithms is somewhat limited. Many new open-source frameworks implement a large number of fairness algorithms, and I believe that they will be helpful in running more extensive evaluations. \n\nAlso, many of the algorithms have tuning knobs, so it's more helpful if one compares the entire trade-offs achieved by each method.",
            "clarity,_quality,_novelty_and_reproducibility": "see my comments above.",
            "summary_of_the_review": "see my comments above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3928/Reviewer_93K5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3928/Reviewer_93K5"
        ]
    },
    {
        "id": "7dUje60R7g",
        "original": null,
        "number": 4,
        "cdate": 1667445456641,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667445456641,
        "tmdate": 1669826813092,
        "tddate": null,
        "forum": "hZRxiAZFJC",
        "replyto": "hZRxiAZFJC",
        "invitation": "ICLR.cc/2023/Conference/Paper3928/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a modification to Stochastic gradient descent where they alternatively learn the lagrange multipliers for the fairness constraint along with learning the parameters of the model during training. Fairness here is modelled as a constraint, and different data-points are associated with different weights (one weight per class). They empirically show the power of this method, and analyze the accuracy/fairness tradeoff obtained by this method and compare it against full constrained optimization.",
            "strength_and_weaknesses": "Strength\n\n+ Although the paper positions this as a problem of addressing fairness, this method can be applied more broadly to any domain where the dataset contains multiple distributions, and we would like the hypothesis to not reflect the same underlying distribution from which the dataset is generated and learn to generalize across the various classes equally well. Fairness is one such example, but there are many practical applications to this such as data coming from different distributions (e.g., dataset collected over many days from a slowly changing environment).\n\n+ The procedure is very simple to implement, and can be readily accommodated in a typical SGD training loop. Moreover, the additional overhead is very minimal since the aditional information the algorithm requires (i.e., average accuracy per group) is anyway computed in a typical SGD update step.\n\nWeakness\n\n+ The first weakness of the paper I find is that the paper does not compare to more baselines that are natural. First, since we know the groups already, one could in-principle first compute the ratios of prevalence of the various group in a random sample of the dataset and use an IPS estimate to correct for this bias (i.e., weight_i = 1/Pr[random sample belongs to class i]). The advantage with this method is that, this is direct and one does not need to worry about learning additional parameters. Second, one could try to learn the weights lambda outside of the SGD update using some form of non-differentiable method (such as BO) which are much more sample efficient. \n\n+ The second weakness of this paper is that the paper does not try this method on more modern datasets/architectures (e.g., GLUE/superGLUE and transformer). This may be needed, because even minor additional overhead during training will be more visible when training on large models on big datasets. I would recommend the authors to perform ablations on overhead more thoroughly and/or add the caveat that the overhead could be minimal but more experiments are needed.\n\n\n--- EDIT\n\nThe authors have run additional experiments to resolve both my main worries: scalability as a function of architecture complexity and natural baselines. So I am increasing my score.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\n+ The paper is clearly written and the concepts are explained well. The experiments are clear and the results are fairly represented for the most part.\n\nQuality\n\n+ The submission is of high quality. Barring some changes to the experiments, overall the paper is high-quality\n\nNovelty\n\n+ the paper considers an important problem and proposes a novel algorithmic solution to the problem.\n\nReproducibility\n\n+ the paper presents the psuedo code and for the most part the experiments seem reproduccible. The paper also links to the code. ",
            "summary_of_the_review": "Overall, the paper is well-written and the direction is important and promising. However, I would like the authors to address the above weakness in a meaningful form so that the submission can be made even more stronger. given a reasonable response to the above, I would be able to move my score based on that.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3928/Reviewer_xqvP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3928/Reviewer_xqvP"
        ]
    }
]