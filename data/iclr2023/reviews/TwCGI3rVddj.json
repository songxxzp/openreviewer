[
    {
        "id": "N0vv_lewsvF",
        "original": null,
        "number": 1,
        "cdate": 1666588567114,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588567114,
        "tmdate": 1666588567114,
        "tddate": null,
        "forum": "TwCGI3rVddj",
        "replyto": "TwCGI3rVddj",
        "invitation": "ICLR.cc/2023/Conference/Paper5295/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces FLGAME, a defense against adaptive backdoor attacks. It formulates the compromised client attack and server defense as a form of minimax optimization where the clients want to maximize their contribution to the model update and the server wants to minimize malicious contribution. This optimization centers around a genuine score that weights the model update during aggregation. The server computes this through a reverse engineered backdoor trigger and target class. ",
            "strength_and_weaknesses": "Strengths: \n\u2022\tFLGAME achieves relatively high model accuracy with a very low attack success rate compared to prior defense.\n\u2022\tThe dataset the server has does not need to be in-domain. This is a much easier requirement to achieve that prior work.\n\u2022\tThe introduction of using an adaptive defense is a good contribution.\n\nWeaknesses:\n\u2022\tThe experimental datasets were very simple. I would like to see more complex datasets, such as ImageNet/Tiny Imagenet. \n\u2022\tPlease expand on the contribution of the compromised clients in the model update. It\u2019s not clear whether the attack success rate is low because the compromised clients have a low genuine score or if their updates result in weak backdoor success.  \n\u2022\tIt is okay to include a few baseline comparisons. However, many of the defenses compared were not intended for backdoor attacks. Please show comparisons to other new defenses aimed for backdoor defense.\n\u2022\tThe paper relies on prior work for reverse engineering backdoor triggers and target class. I would like to see more about this. What limitations does this have? If the compromised clients use larger L1 norm triggers, does this fail? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The discussion of the attack/defense used was clear. The paper and experiments are reproducible.",
            "summary_of_the_review": "The method of using a minimax game and the use of an adaptive defense is a very interesting and important contribution. However, my main issue is the lack of depth in the empirical results. In particular, the datasets used and the analysis on compromised client participation can be made much stronger. Please refer to the weaknesses section for specifics.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5295/Reviewer_tQyp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5295/Reviewer_tQyp"
        ]
    },
    {
        "id": "CysM5ZbqesY",
        "original": null,
        "number": 2,
        "cdate": 1666649664954,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649664954,
        "tmdate": 1666664379222,
        "tddate": null,
        "forum": "TwCGI3rVddj",
        "replyto": "TwCGI3rVddj",
        "invitation": "ICLR.cc/2023/Conference/Paper5295/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to formulate the defense against backdoor attacks as a minimax game between the server (defender) and attackers in federated learning. The minmax problem is solved in three steps by 1) construct an auxilary global model; b) reverse-engineer backdoor samples with selected target classes; c) assign to each client a genuine score is used to weigh model update uploaded by the client. \n",
            "strength_and_weaknesses": "\nSorry that I don't see any strength in the paper.  \n\nThe proposed method is seriously flawed in my view for following reasons: \n\n- the assumption that the server is able to access and inspect local model updates poses privacy leakage risks on private data in the face of deep leakage type of gradient attacks [1];  this serious privacy risk defeats the purpose of federated learning in the first place;  authors are advised to reconsider the threat model and take into account various privacy-preserving mechanisms such as Differential privacy or Homomorphic Encryption; \n\n[1] Deep Leakage from Gradients, NeurIPS 2019; \n\n- the proposed method evaluates local model updates based on the agreement with the reverse-engineered trigger sets and target classes; nevertheless, the approach does not make sense at all since the target class selected by the reverse-engineering process is by no means the same as the target class might be selected by real backdoor attackers; the paper proposes to \u201cfind the backdoor trigger and target class such that the genuine scores for benign clients are large but they are small for compromised clients\u201d\uff0c however, the compromised clients are a prior unknown, and the proposed method cannot reliably estimate the target class and solve this chicken and egg problem; \n\n- the proposed method is inefficient in the evaluation of local model update (Line 4-5 in Algo. 1); this method cannot be efficiently applied to a federated learning scenario with large number of clients; that is probably why experiments reported in the paper only used 10 and 20 clients, which are far less than typical cross-devices use cases with millions of clients; ",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is easy to follow; the proposed method is seriously flawed in my view; ",
            "summary_of_the_review": "The proposed method is seriously flawed in my view for following reasons: \n\n- the assumption that the server is able to access and inspect local model udpates imposes privacy risks on private data in the face of deep leakage type of gradient attacks [1];  this serious privacy risk defeats the purpose of federated learning in the first place;  authors are advised to reconsider the threat model and take into account various privacy-preserving mechanisms such as Differential privacy or Homomorphic Encryption; \n\n[1] Deep Leakage from Gradients, NeurIPS 2019; \n\n- the proposed method evluates local model udpates based on the agreement with the reverse-engineered trigger sets and target classes; nevertheless, the approach does not make sense at all since the target class selected by the reverse-engineering process is by no means the same as the target class might be selected by real backdoor attackers; the paper proposes to \u201cfind the backdoor trigger and target class such that the genuine scores for benign clients are large but they are small for compromised clients\u201d\uff0c however, the compromised clients are a prior unknown, and the proposed method cannot reliably estimate the target class and solve this checken and egg problem; \n\n- the proposed method is inefficient in the evaluation of local model udpate (Line 4-5 in Algo. 1); this method cannot be efficiently applied to a federated learning scenario with large number of clients; that is probably why experiments reported in the paper only used 10 and 20 clients, which are far less than typcial cross-devices use cases with millions of clients; ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5295/Reviewer_Z2ep"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5295/Reviewer_Z2ep"
        ]
    },
    {
        "id": "3_liN7Z4Jcu",
        "original": null,
        "number": 3,
        "cdate": 1666657232677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657232677,
        "tmdate": 1666657232677,
        "tddate": null,
        "forum": "TwCGI3rVddj",
        "replyto": "TwCGI3rVddj",
        "invitation": "ICLR.cc/2023/Conference/Paper5295/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a minimax game model of backdoor attacks between attackers\nand defenders in the federated learning (FL) paradigm.  Based on the analysis\nof this model, it uses a reverse-engineering technique to defend against\nbackdoor attacks in the FL process. It provides theoretical analysis and\nexperimental results to present the effectiveness of its defense mechanism.\n",
            "strength_and_weaknesses": "It is an interesting angle to look at the federated learning backdoor problem.\nThe analysis is well presented.\n\nIts discussion is limited to strong assumptions about the adversary.\nThe theoretical analysis makes impractical assumptions about backdoor attacks.\nThe proposed defense has limited novelty.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the analysis of this paper is not very general. The paper restricts\nattackers and their capabilities to its designed minimax game model. This\npaper proposes a minimax game model between defenders and attackers, where\nboth optimize a genuine score of backdoored and clean models. However, in a\npractical scenario, the attack methods are unavailable, so attackers don't\nneed to optimize the genuine scores. So this game model considers a restricted\nassumption of attackers' attack methods, making its comparison with other\nbaselines less convincing. \n\nThe theoretical analysis to guarantee the robustness of its method is based on\nan impractical assumption. In backdoor attacks, the common triggers are not\nbounded by $L_p$ distances. This paper assumes that the Lipschitz continuous\ngradient will bound the backdoor updates of the loss function. It requires the\nbackdoor attacks in the FL model to be as strong as possible to make sure it\ncan be bounded. Existing works consider a successful backdoor attack (e.g.,\nattack success rate over 90%) rather than consider the strongest backdoor\nattack. Stronger attacks are more likely to be detected. So I think this\nassumption is not practical.\n\nThe proposed defense method has limited technical novelty as it mostly applies\na typical reverse-engineering method (Neural Cleanse). There are many existing\nbackdoor attacks like WaNet [3], the invisible attack [4], which can bypass\nNeural Cleanse. These attacks consider trigger patterns other than patches to\nbreak the assumptions of Neural Cleanse. So it is also important to consider\nthese attacks in this paper, which can help enhance the effectiveness of its\nmethod.\n\n[1] Gu et al. \"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\" IEEE Access 2019\n[2] Liu et al. \"Trojaning Attack on Neural Networks\" NDSS 2018\n[3] Nguyen et al. \"WaNet - Imperceptible Warping-based Backdoor Attack\" ICLR 2021\n[4] Li et al. \"Invisible Backdoor Attack with Sample-Specific Triggers\" ICCV 2021\n",
            "summary_of_the_review": "This paper makes strong assumptions about attackers, and it lacks technical novelty (proposed defense).\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5295/Reviewer_TbmV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5295/Reviewer_TbmV"
        ]
    },
    {
        "id": "caS5VQwWdJC",
        "original": null,
        "number": 4,
        "cdate": 1666657810649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657810649,
        "tmdate": 1670273343781,
        "tddate": null,
        "forum": "TwCGI3rVddj",
        "replyto": "TwCGI3rVddj",
        "invitation": "ICLR.cc/2023/Conference/Paper5295/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a dynamic backdoor defenses in federated learning, where both the defender and the attacker can dynamically adjust their strategies as the federated training continues. The defender reverse engineers the potential backdoor trigger so as to finally generate the benign score for each client, and downweights the clients with lower benign scores. Attackers aim to find the proper backdoor poisoning ratio such that a good tradeoff between backdoor effectiveness and benign scores of compromised clients can be reached. ",
            "strength_and_weaknesses": "Strength:\nThe paper is well written and the idea of defending against adaptive attackers is impressive. \nWeakness:\n1. the whole framework relies on the defender is able to reverse engineer useful backdoor triggers so as to effectively minimize the impact from compromised clients. However, this design may be exploited by the attacker [1]. Specifically, it is possible that attacker generate backdoors in a way that, either the reversed engineered backdoor samples are not effective or are similarly effective for both the compromised and benign clients. This assumption is the major weakness of this paper. \n2. it is unclear how well those theories (e.g., Theorem 1) imply in practice. For example, the assumption on $r_i^t=0$ seems to not hold in practice and so, the backdoor model cannot be the same the clean global model, even asymptotically as the training rounds continues.  \n3. the threat model makes too many restrictions on the attacker capability. For example, the attackers can only augment the training data with backdoored samples while in practice, they may choose to replace some samples with backdoored samples. The attackers are also assumed to be unaware of the benign scores of other clients while in practice, they may be able to simulate the benign scores (given the knowledge that the server is calculating benign scores for each client using reversed engineered triggers). In the worse case scenario, powerful attackers may also be able to get the actual benign scores. It is also assumed that attackers can only use its own backdoor trigger when deciding the proper poisoning ratio while in practice, attackers can also simulate the reverse engineering process of the defender. \n4. there are also some unrealistic experimental settings in the paper. For example, it is assumed that all clients in each round will be selected while in practice, defenders usually randomly selects subset of the clients. Also, the fraction of compromised clients are mostly evaluated under 60%, which, theoretically, should be hopeless to defend against. The lowest fraction of compromised clients is still 20%, which is quite a high number. I am curious to see the comparison among different defenses (not just FLTrust and the proposed method) under even lower compromised clients. \n5. some recent works are missing from the paper. For example, the most recent defense on backdoor attacks in federated learning [2]. \n[1] Veldanda et al., \"On Evaluating Neural Network Backdoor Defenses\".\n[2] Rieger et al., \"DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection\", NDSS 2022. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality is adequate and the paper is mostly easy to follow. However, it is worth mentioning in the paper, whether the backdoor attacks corresponding to different defenses are the same or not. For the baselines, I believe the same backdoor attacks can be leveraged. But for the proposed defense, since it involves dynamic game between the defender and attacker, the attacks are actually changing.\n\nAs for the originality, this work is not the first to propose defenses based on reputation scores of the clients. In addition, the reputation score computation itself vulnerable to adaptive attacks, and I am concerned that the contribution is limited. ",
            "summary_of_the_review": "The idea of defending against adaptive backdoor attacks in federated learning is interesting. However, the proposed framework did not consider this \"adaptive\" aspect thoroughly and hence, is not convincing. Also, the experimental settings are not practical and some recent baselines are missing. Considering all the weakness mentioned above, rejection is given.   ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5295/Reviewer_jdkR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5295/Reviewer_jdkR"
        ]
    }
]