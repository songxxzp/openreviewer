[
    {
        "id": "0cWcAklPve2",
        "original": null,
        "number": 1,
        "cdate": 1666532876309,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666532876309,
        "tmdate": 1670399332188,
        "tddate": null,
        "forum": "H0HGljkxQFN",
        "replyto": "H0HGljkxQFN",
        "invitation": "ICLR.cc/2023/Conference/Paper2998/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new way to incorporate MBconv in a transformers architecture. This in order to propose an efficient architecture for many tasks with good FLOPS- accuracy and parameters-accuracy trade-off.",
            "strength_and_weaknesses": "Strenghts:\n\n- The paper is well written and easy to follow.\n- The proposed method is simple and seems to give good results on the different settings considered. \n\n\nWeakness:\n\n-Comparison with concurent approaches: The paper states in the abstract \"The research community has thus attempted to combine the strengths from both architectures. Unlike the current works that simply stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block.\" However there is no analysis that compares the Moat block and other blocks proposed in the literature (CMT [1] , Yuan et al. [2], CoatNet [3])using the same architecture. In order to have a clear comparison of the Moat blocks and concurrent approaches.\n\n[1] Guo et al., CMT: Convolutional Neural Networks Meet Vision Transformers\n[2] Incorporating Convolution Designs into Visual Transformers\n[3] CoAtNet: Marrying Convolution and Attention for All Data Sizes\n\n-Comparison without EMEA:  According to Table 10 the paper use EMEA for models train on ImageNet-1k for a fairer comparison with the other approaches it would be interesting to also report the results without EMEA\n\n- Semantic segmentation: On ADE20k the UperNet evaluation setting is more popular than the one used in the paper. Currently the comparison is quite limited. It would be interesting to have a larger comparison with the upernet setting to see how the proposed architecture performs compared to competing approaches.\n\n- Missing metrics: There is only FLOPs and parameters that reported in the different tables. It is necessary to add the peak memory and the latency in order to better understand the different trade-offs. Indeed, MBconv are known to give good FLOPs accuracy/params-accuracy trade-offs but are not always very good for the other trade-offs.\n\n\n- Overfitting evaluations:There is no evaluation of overfitting but on ImageNet the evaluation is done on the validation set which can lead to overfitting. It is important to add evaluation on ImageNet-v2 [1] to evaluate the level of overfitting.\n\n[1] Recht et al., Do ImageNet Classifiers Generalize to ImageNet?\n\n\nOther comments:\n- FLOPs/ Parameters trade-off: Table 2 and Table 9 It is interesting to add all the EfficientNet-v2 models (S and M) because they have a good trade-off.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, there are many experiences to validate the proposed architecture. The architecture seems to be an adaptation of different already existing components. The code is promised so the results will be reproducible.\n\n",
            "summary_of_the_review": "The paper is well written and the proposed idea is interesting; however, it lacks results in order to better evaluate the significance of the results and to know the impact of the proposed method compared to the existing one in the literature.\n\n====Post Rebuttal====\nThe authors address some of my concerns in particular on the experimental part. However, the proposed architecture lacks novelty and clear motivation in its design in comparison to the literature, but the experimental part is quite extensive. So I will keep my original score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2998/Reviewer_BAq9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2998/Reviewer_BAq9"
        ]
    },
    {
        "id": "PXQQkS5I0bK",
        "original": null,
        "number": 2,
        "cdate": 1666620541693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620541693,
        "tmdate": 1669782433818,
        "tddate": null,
        "forum": "H0HGljkxQFN",
        "replyto": "H0HGljkxQFN",
        "invitation": "ICLR.cc/2023/Conference/Paper2998/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents a new transformer-based architecture for vision tasks, namely MOAT. \nThe key idea is to combine the convolutional priors, locality, and hierarchy into the pure transformer block design (self-attention, then MLP) and remove redundant operations. \nFirst, the locality is achieved by replacing the MLP block with a mobile convolution block (MBconv). In practice, the authors empirically verified that placing the MBconv before self-attention provides the best result. \nSecond, the hierarchy is achieved by introducing strided depth-wise convolution in the first block in each stage, and it is efficient compared to having additional patch embedding layers. \nFinally, the authors sweep the best macro-level block configurations and found that using MBconv blocks and MOAT blocks at the first and last two stages strikes the best efficiency-accuracy trade off.\nThe final model MOAT provides good basis for the model scaling (both up and downward) and achieves strong results in various vision tasks.\n\n\n",
            "strength_and_weaknesses": "**Strength**\n\n*+* The paper conducts extensive experiments. Especially having dense reference points on the broad regime of model sizes shows the strong scaling effect of the model and can allow the follow-up studies to compare it more thoroughly. I appreciate the authors for doing this.\n\n*+* The open-sourced code bases/checkpoints can provide broad visibility and high reproducibility to the community.\n\n*+* The MOAT achieves strong results in various vision benchmarks.\n\n**Weakness**\n\n*-* Given the recently presented MaxVIT, my main concern with this paper is ***limited architectural innovations***. The major observations and the resulting architectural changes are 1) MBconv + self-attention (to equip with the translation equivariance and locality) and 2) strided dwconv instead of extra patch-embedding layers (to equip with the hierarchy). However, both are already presented in the MaxViT. The remaining difference is the existence of SE in the MBConv block, which is a minor change (In fact, there is a recent trial of this in MaxViT by the 3rd group in https://github.com/rwightman/pytorch-image-models, i.e., MaxViT with ConvNeXt block, and works well).\n\n*-* Moreover, unlike MaxViT, which uses linear complexity self-attention, the MOAT directly adopts the quadratic complexity global self-attention. This makes the authors introduce inhomogeneous block configurations (i.e., using MBconv blocks at the first two stages) to reduce overeheads and perform post-hoc engineering (i.e., using window attention and setting the proper window size) to apply the model to high-resolution inputs. These reduce actual ***algorithmic simplicity*** overall.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe overall paper is well-written. However, the authors are bypassing the thorough discussions with the recent MaxViT paper. I encourage the authors to present the common observations, distinctions, and practical advantages over the MaxViT.\n\n**Quality** \n\nThe paper is well structured. Also, the extensive experiments and strong results are presented.\n\n**Novelty** \n\nThe technical novelty is quite limited, considering the MaxViT paper.\n\n**Reproducibility** \n\nThe authors open-sourced the code bases. This will help the community to easily reproduce the results.",
            "summary_of_the_review": "My main concern with this paper is the limited technical contribution. Given the MaxViT, the claimed technical contributions are not new. The MOAT can be regarded as an incremental variant of MaxViT. It works slightly better with proper post-hoc engineering (e.g., introduce pure MBconv blocks in early stages, use global attention and convert it to the window attention if test-time input resolution is large, etc.).\n\nOverall, I appreciate extensive experiments and engineering efforts. The empirical results, code, and model checkpoints will help the practitioners. However, this cannot be the basis for a new ICLR conference paper. I am willing to hear the author's view on comparing MOAT with the MaxViT in various aspects, including experiment-level comparisons in a broad model-size spectrum (e.g., in which model regime the MOAT outperforms/underperforms the MaxViT?) and thorough descriptions of key differences and their resulting practical dis-/advantages (e.g., what is the core that makes MOAT practically strong compared to MaxViT?).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I see no ethical issues in this paper.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2998/Reviewer_W4v5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2998/Reviewer_W4v5"
        ]
    },
    {
        "id": "7qpt1a41y1",
        "original": null,
        "number": 3,
        "cdate": 1666624265743,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624265743,
        "tmdate": 1669783486323,
        "tddate": null,
        "forum": "H0HGljkxQFN",
        "replyto": "H0HGljkxQFN",
        "invitation": "ICLR.cc/2023/Conference/Paper2998/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new building block, which combined MBConv and Attention, for neural network design and the authors show its efficiency and effectiveness on many visual tasks.",
            "strength_and_weaknesses": "Strength\n1. The authors demonstrate the efficiency and effectiveness on the proposed MOAT module.\n2. The ablation studies are comprehensive, especially the order of MBConv and Attention block.\n\nWeaknesses\n1. The third observation seems to be found by other works already, e.g. PiT and RegionViT both use depthwise convolution for the donwsampling and they found it is effective. ",
            "clarity,_quality,_novelty_and_reproducibility": "There are a few places that the text is unclear, which leads it is difficult to reproduce as the codes are not provided. E.g. \n1. at the first observation, if I understand correctly, the transformer block is not just MLP + Attention anymore, the authors added  depthwise convolution, normalization and activation, but I am not able to see how they are added or does the authors simply mean changing the MLP to MBConv? ",
            "summary_of_the_review": "Please see my comments in Section 1, 2 and 3. \n\nAnd I have other questions:\n1. In the abstract, authors mentioned that the generalization of transformer is worse than ConvNet, but is there any experiment to support this argument?\n\n2. In Table 8, this ablation study focus on the effects of downsampling layer, why the block composition is also varied in this case?\n\n3. Under tiny-MOAT setting (Table 5), what is the performance of tiny-MOAT if we would like to match the FLOPs to 0.05B from MobileFormer? \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2998/Reviewer_bFXs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2998/Reviewer_bFXs"
        ]
    },
    {
        "id": "yi5_uniTvK",
        "original": null,
        "number": 4,
        "cdate": 1666792689094,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666792689094,
        "tmdate": 1669749885816,
        "tddate": null,
        "forum": "H0HGljkxQFN",
        "replyto": "H0HGljkxQFN",
        "invitation": "ICLR.cc/2023/Conference/Paper2998/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a family of neural networks called MOAT, which combines Mobile ConvNet and Transformer. It studies how to build effective networks based on two observations from Mobile ConvNet and Transformer. The new proposed networks achieve good performance on different tasks.",
            "strength_and_weaknesses": "Strength: Paper writing is good and easy to follow; Idea is simple and effective; Experiments are sufficient.\n\nWeaknesses: The paper is more like a technical report. The novelty is weak by combing MobileConvet and Transformer. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: It's easy to follow. It presents its motivation based on two key observations which make sense to me.\nQuality: Writing is good and experiments are sufficient.\nNovelty: Novelty is a little weak for me. It is more technical by combining MobileConvet and Transformer. \nReproducibility: I think reproducibility is fine.",
            "summary_of_the_review": "Refer to the above sections.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2998/Reviewer_GsU5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2998/Reviewer_GsU5"
        ]
    }
]