[
    {
        "id": "ATs5t9xD1C",
        "original": null,
        "number": 1,
        "cdate": 1666048091783,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666048091783,
        "tmdate": 1666048091783,
        "tddate": null,
        "forum": "1Wo0vqaZ8WJ",
        "replyto": "1Wo0vqaZ8WJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6530/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper utilized *Normalizing Flows* models, which are trainable bijective mappings, composed of fully invertible layers. The core idea is to obtain a bounded support in the latent space by design via shifting the base distribution space (that of the latent variables) from a normal distribution to a bounded uniform one. The proposed method seems to be based on PLAS (Zhuo et al.), which constructs a latent space using VAE that maps to the actions which are well-supported by the training data. It then learns optimal policy in this space. ",
            "strength_and_weaknesses": "Strength:\n\n- The core idea of using NF is quite interesting and might be a good replacement for PLAS.\n\nWeakness:\n\n- The paper is too immature with numerous mistakes and quite poor presentation. In the current form it is far from acceptable. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: very poor.\n\nQuality: poor.\n\nNovelty: very good.\n\nReproducibility: Questionable, even if some of the hyper parameters are provided, but I do not see any statement about code release. ",
            "summary_of_the_review": "I had a hard time following the paper. It contains too many mistakes, inconsistency, and unclear/confusing parts. I would strongly recommend the authors should start fresh and completely rewrite the draft.\n\nHere are my main comments; hopefully they help:\n\n- The choice of $f(x) = tanh(x)$ as the activation needs to be discussed. In particular, $tanh$ has a strong saturation for when $x$ passes one. In other words, its behaviour is *almost* linear for when the advantage function is in $[-1,1]$ and somehow clipping $A(s,a)$ when falling outside unit interval (or $[-2,2]$).  Is it a limitation? Discussion needed. \n\n- In equation 2, $\\textbf{f}$ applies on the advantage function, while later $\\textbf{f}$ is defined over $(s,a)$. If by these you mean the same function, fix your definitions. If not, then use different letters. \n\n- Section 3 is quite unclear. Specially the paragraph before Fig. 3. Further, the range of data (from -1 to 1 on each axis) seems to be arbitrary with no explanation provided. \n\n- Page 4, end of first paragraph: \u201cone is still able to find latent vectors that would result in out-of-distribution points.\u201d  -> it is not inferred from the figures. Perhaps consider an example in the figures for this statement. \n\n- The idea of mapping the value function and training the Q network in the mapped space has also been studied in the recent literature. Though it might not be directly related, it could be beneficial to borrow some ideas, or proofs. See for example: https://arxiv.org/abs/2203.07171\n\n- Equation 6 is very confusing: sampling from $\\pi(\\cdot |s)$ gives rise to actual actions (not the z space).\n\n- Consider discussing equations 6-9 in much more details, as they are the core of your paper. \n\n\nOther comments:\n\n- Figure 1 seems to never have been referred to in the paper. Also, Figure 2 is referred after Figure 3. \n\n- When citing a paper indirectly (many times throughout the manuscript), use \u201c\\citep\u201d rather than \u201c\\cite\u201d to include parentheses outside both the names and the date. \n\n- Your definition of reward is indeed the expected reward (over next state). \n\n- You allow for $\\gamma = 1$ and at the same time you define return on an infinite horizon, which may become unbounded. Either remove the \u201cno discount\u201d scenario, or rather make your MDP episodic by introducing terminal states and necessitating termination. \n\n- Equation 1 should be the definition of advantage function (it is misplaced).\n\n- Your notation is not consistent. From what I see, you use CAP-BOLDFACE for sets, but then later you also use it for random variables X and Z. Similarly for the lower-case letters. Please consider a full overhaul of your notations. \n\n- Section \"Normalizing Flow\": X and Z are random variables, NOT distributions. \n\n- In section \u201cCritic Regularized Regression\u201d you used script-D for the dataset, but in the next part you used D. Again, I strongly recommend consistency in your notation to help your readers following the concepts better. \n\n- Figure 1 -> in the CNF graph, $z\\sim U(-1,1)^n$. Therefore, the color should be a solid blue instead of radial gradient.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6530/Reviewer_DZCL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6530/Reviewer_DZCL"
        ]
    },
    {
        "id": "KlVNIglq1x5",
        "original": null,
        "number": 2,
        "cdate": 1666634312006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634312006,
        "tmdate": 1666634312006,
        "tddate": null,
        "forum": "1Wo0vqaZ8WJ",
        "replyto": "1Wo0vqaZ8WJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6530/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method using normalizing flows for offline reinforcement learning (ORL).\nTo alleviate the difference between training and testing datasets, the proposed method maps uniform distributions to action spaces using the flow model.\nExperimental results present that the proposed method outperforms the state-of-the-art methods in many benchmark datasets.\n",
            "strength_and_weaknesses": "This paper is generally well-written and easy to follow.\nIt is reasonable that the uniform distribution is better than the normal distribution for the latent space in ORL.\n\nThe reviewer has two concerns as follows.\nFirst, the necessity of the flow model is not well supported.\nFigure 6 presents that the proposed model performs poor scores when using the normal distribution as the latent space.\nPlease report an ablation study for the pre-trained models (VAE decoder vs. flow model).\n\nSecond, the performance improvement by the latent space clipping is missing.\nThe latent space with the uniform distribution has clipped values in [-1,1], while the latent space with the normal distribution is unbounded.\nNormal distribution with value clipping in [-1,1] will provide fair comparisons.\nIn this regard, the reviewer wants to see the results of NF-uniform under the experiment setting in Figure 3.\nIf the results are the same as in Figure 2(b) regardless of the value of amplitude, the proposed method does not address \"extrapolation\".\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "If the major performance improvement is latent space clipping ([-1,1]), the novelty of the proposed method is very limited.\n\n",
            "summary_of_the_review": "The proposed method is interesting and the paper is well-written.\nHowever, some ablation studies and clarifications of the performance improvements are missing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6530/Reviewer_gFEy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6530/Reviewer_gFEy"
        ]
    },
    {
        "id": "oGZikrrPHB-",
        "original": null,
        "number": 3,
        "cdate": 1666643560212,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643560212,
        "tmdate": 1666643560212,
        "tddate": null,
        "forum": "1Wo0vqaZ8WJ",
        "replyto": "1Wo0vqaZ8WJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6530/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces Conservative Normalizing Flow (CNF) which encourages the final offline RL policy to remain withing the data distribution while being able to run policy optimization over an unconstrained latent space. They achieve this by pre-training the latent space to be a uniform distribution using normalizing flows that learns bijective mappings using fully invertible layers. The key novelty here is the use of bounded functions such as tanh at the last layer of normalizing flow such that the latent distribution may be learned as a uniform distribution between -1 and +1. This allows us to model/sample the behavioral action distribution using the entire latent space. (no clipping required.). Experimental results show that they are competitive with other SOTA offline RL methods in many domains.",
            "strength_and_weaknesses": "Overall the paper is written well and is intuitive. The use of normalizing flows to construct a latent space that leads to conservative agents by design is unique yet neat. The ability to learn better action-spaces that allows to model the state-action coverage of the dataset by default can be very useful for a many downstream and RL learning problems.\n\nIt would be wonderful to see some experimental analysis on the qualitative or quantitative differences between the action sampling portion of the current model and other methods in terms of being able to sample state action pairs that are well covered in the data set along with if and when these models are prone to over-fitting as discussed in the paper. Some empirical evaluation of different action generating functions in real wold datasets would be really helpful to push the community towards using Normalizing Flows with uniform latent distribution as a principled way of sampling actions in offline RL settings.\n\nRegarding the experimental results, while it is competitive with the SOTA methods in many cases, it is not trivial why it fails to perform well when it does not. For example it is surprising that the method does not work as well for domains like maze2d umaze and medium expert tasks. (It is expecially surprising that it is performing worse in hopper medium expert as compared to hopper medium replay. This is an anomaly as generally \u00a0most offline RL algorithms tend to perform better with expert dataset. I would love to know more about what authors think about this result. Moreover it would be wonderful to have results on more involved domains such as antmaze to further bolsterd the claims made in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "-",
            "summary_of_the_review": "Overall the paper provides a novel approach on sampling well covered actions for the downstream task of offline RL, However the experimental results show some anomalies that I would like to be been elaborated further by the authors. I would be willing to change my score with some explanations regarding the anomalies in the scores and some additional experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6530/Reviewer_17pi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6530/Reviewer_17pi"
        ]
    },
    {
        "id": "dPtKvBv5Ex",
        "original": null,
        "number": 4,
        "cdate": 1666644758300,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644758300,
        "tmdate": 1666644758300,
        "tddate": null,
        "forum": "1Wo0vqaZ8WJ",
        "replyto": "1Wo0vqaZ8WJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6530/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Offline RL is challenging when the learned RL policy drifts too far from the support of the dataset. Thus, many offline RL methods use some form of constrained or conservative policy update to ensure that the RL policy remains close to the behavior policy of the dataset. One method for doing this is to train a generative model (e.g. a VAE) on the data, and learn an RL policy that picks embedding vectors which are decoded into actions by the VAE decoder. This paper proposes a slight tweak on that prior approach: rather than learning a VAE on the dataset, what if we learned a model using normalizing flows? Using this Conservative Normalizing Flows approach, the authors transform the prior into a Uniform distribution rather than a Normal distribution, which ensures that the RL policy can choose samples anywhere within the support of the Uniform distribution without generated OOD samples, unlike with the VAE. The results show some improvement over existing methods on a sample of tasks from the D4RL offline RL benchmark. ",
            "strength_and_weaknesses": "A strength of the paper is that it precisely targets a clear hypothesis: whether NF can improve the ability for an offline RL agent to learn to control a pretrained model vs. a VAE. The motivation for the paper is clear, as is the distinction from prior work.\n\nA weakness of the paper is its significance, or impact. It makes a relatively minor tweak to the approach proposed by Zhou et al. 2020, and while the results demonstrate gains on 67% of environments sampled for the paper (8/12), it also leads to dramatically worse performance in some environments. Thus, the novelty and effectiveness of the approach is limited. \n\nOne possible avenue for future work that the authors could consider in order to increase the significance of their results, is to think about applying this approach to learning to control large language models with RL. Currently, it's difficult to train LLMs with RL due to the complexity of LLM infrastructure and training with RL on such a large scale (i.e. fine-tuning all the parameters could be prohibitively expensive). One approach would be to impose a bottleneck embedding layer within an LLM (see e.g. https://arxiv.org/abs/2209.06792), and then train an RL agent to pick embedding vectors to optimize some language objective. The issue is that this can be quite difficult to get working. The linked vec2text paper suggests that using a VAE doesn't work well in this context. So perhaps normalizing flows could be useful there as well. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\nFor the most part, the paper is written clearly. In particular, the abstract and intro motivate the problem well, and clearly outline the paper's contributions. \n- Figure 1 nicely explains both the approach and its difference from prior work. \n- The toy example in Figure 3 helps build intuition for the problem.\n\nHowever, the paper becomes less clear in the methods section. More details for Section 3.3 would be helpful. For example, providing an explanation of algorithm 2 in the text would be more clear. There is far too much detail of low-level hyperparameter settings in Section 4.1 that could be moved to the Appendix, in order to spend more time in Section 3 explaining the methodological choices and the actual algorithm. Other issues with the methods clarity:\n- The ordering of presentation of the equations seems unclear. A critic is needed to estimate Eq 7 but hasn't been introduced yet\n- Why use two critic networks as in Fujimoto? This should be motivated better. \n- There appears to be a typo in step 7 of algorithm 2, where it takes A(A(s,a)). What is the advantage of the advantage? \n\nMinor clarity issues:\n- \"Supervisely\" pre-trained models in Figure 1\n- Why use the normalizing flow conditioning scheme from PARROT? Why do you hypothesize it will be useful here / how is it relevant? \n\n**Quality:**\n- This paper criticizes prior work for having to manually bound the RL agent's action space with the VAE approach, but also takes the approach of manually bounding the agent's actions to be within the (-1,+1) interval of support of the uniform distribution. So it would be better to tone down or rephrase those claims. For example, the related work talks about how CNF \"makes agents conservative by design, rather than squeezing policy outputs\". But in effect you are squeezing policy outputs also. \n- It appears the paper tests on a subset of the D4RL environments. Why were these chosen vs. others? This should be justified better. \n- As mentioned above, according to the results CNF helps in 67% of environments, but it hurts badly in others. Are these results of significant interest to the community? \n- The ablation studies such as those of Figure 6 are a great thing to include and help illustrate why the approach is useful. However, the top paragraph of p. 8 states that the ablation using a Normal latent space was carried out with the best hyperparameters found for the Uniform latent space. So how do we know that the difference in performance can't be attributed to doing better hyperparameter tuning for the proposed approach? \n\n**Originality:**\nThe idea of pre-training a conditional latent model on data and then using an RL agent to pick embedding vectors was proposed by Zhou et al. (2020). The difference with this work is that while Zhou used a VAE, this work proposed to use normalizing flows. This is a relatively minor change from Zhou et al. \n\nThe paper is missing a relevant citation to https://arxiv.org/abs/2010.05848, which is a conservative Offline RL method that minimizes divergence from the behavior policy using KL-control. ",
            "summary_of_the_review": "In summary, the paper clearly and precisely investigates a specific hypothesis related to using normalizing flows rather than a VAE for a particular type of Offline RL approach. It is benchmarked against relevant Offline RL baselines and shows an improvement in 67% of environments, and a strong detriment in others. Relevant ablations are conducted to give intuition as to why the proposed approach is important. The novelty above prior work is somewhat limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6530/Reviewer_iyr1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6530/Reviewer_iyr1"
        ]
    }
]