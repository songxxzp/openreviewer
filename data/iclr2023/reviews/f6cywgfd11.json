[
    {
        "id": "-MTlli1A-0",
        "original": null,
        "number": 1,
        "cdate": 1665780640178,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665780640178,
        "tmdate": 1665780640178,
        "tddate": null,
        "forum": "f6cywgfd11",
        "replyto": "f6cywgfd11",
        "invitation": "ICLR.cc/2023/Conference/Paper2986/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a unified collection of 11 vision (and vision-and-language) benchmarks, called G-VUE. One of the messages of this paper is that there are a lot of different evaluations of vision models these days, and they often cover very disjoint things. The paper suggests that vision models be evaluated on a variety of different tasks, exposing different capacities. Some of the tasks are a bit simplified (in terms of their format) to make evaluation easier (e.g. VCR is just the question answering part of that benchmark).\n\nThe paper evaluates ResNets and Vision Transformers, pretrained on both imagenet, web images, and egocentric videos, and with objectives spanning supervised and self-supervised learning. These are often treated as underlying encoders in the literature, and here it seems that the self-supervised approaches do best -- CLIP is better for grounding/reasoning, and MAE is better for raw perception tasks like depth estimation. The paper also evaluates recent large-scale vision-language models proposed in the last year (OFA and UnifiedIO) and finds significant headroom on many of of the proposed tasks.\n\nThe G-VUE benchmark will have an associated leaderboard for other researchers to use.",
            "strength_and_weaknesses": "Strengths:\n* Benchmarks are important, and to this reviewer the overall message of the paper (models ought to be evaluated on a variety of vision tasks, not just low-level vision for instance), seems reasonable.\n* Making this leaderboard public will help other researchers compare to past work.\n* The analysis of low-level encoders (ResNet50 versus ViT-B); with different pretraining mechanisms / data sources is interesting. I'm not quite sure how much it extends to much larger models, though, or models that use language through other means (besides just a source of \"labels\" like CLIP).\n\nOne overarching question to this reviewer is the purpose of this benchmark collection -- are researchers supposed to finetune models on it, evaluate zero-shot, or do something else (like linear probing)? In hindsight, looking at what happened in the text domain like GLUE, to this reviewer it seems like the zero-shot evaluation paradigm won out as many of the GLUE subtasks were quite noisy and easy to overfit to if the right finetuning recipe was applied. I'd be curious as to the thoughts of the authors here.\n\nWeaknesses:\n* The benchmark could be improved -- and the results in this paper more insightful -- if more models were chosen. I'd like to see how well OFA / UIO do on a finetuned setting, if that's what the benchmark is about (since the underlying image encoders are finetuned). If the message is that e.g. OFA can't do depth perception out-of-the-box (because it wasn't trained on that) -- I'd be curious as to how it would do in a linear-probe setting. I'd also like to see other vision-language models included to see the benefits of training on other types of data -- like perhaps FLAVA (Singh et al 2021 https://arxiv.org/abs/2112.04482 ) which is another visual-bert like model trained on slightly different data versus OFA, or MERLOT Reserve (Zellers et al 2022 https://arxiv.org/abs/2201.02639) trained on videos. Finally, I'd be curious as to whether the results are different with size (e.g. is UIO-XL better than a smaller Unified-IO model?)\n* An issue with this collection of benchmarks, at least to this reviewer, is that they all have different formats (different inputs, outputs, metrics); different training distributions / validation sets, etc. For that reason, I'm not quite sure whether this benchmark collection will catch on. Though not as important for my overall score, I think it would be neat if some code was released that made it easy for different e.g. image encoders to be finetuned on the various tasks. However, I'm still not sure as to whether that code would be used -- as it would presume a model structure that might not be what's ideal for these various tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "Seems clear and original at least to this reviewer. A similar benchmark collection was proposed for videos (Liu et al 2020; https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Violin_A_Large-Scale_Dataset_for_Video-and-Language_Inference_CVPR_2020_paper.pdf) -- but I'm not sure if something has caught on covering the proposed selection of tasks.",
            "summary_of_the_review": "To this reviewer, the introduction of a leaderboard that encourages evaluating on multiple tasks seems promising. However, it's not clear to this reviewer what the role of this leaderboard ought to be, and the analysis in the paper (i.e. a proxy of what we can learn from the leaderboard) could be improved by incorporating different classes of models.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2986/Reviewer_U7v5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2986/Reviewer_U7v5"
        ]
    },
    {
        "id": "88a8Ril_6S",
        "original": null,
        "number": 2,
        "cdate": 1666034383483,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666034383483,
        "tmdate": 1666034383483,
        "tddate": null,
        "forum": "f6cywgfd11",
        "replyto": "f6cywgfd11",
        "invitation": "ICLR.cc/2023/Conference/Paper2986/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper combines 11 tasks and benchmarks into a new benchmark for general-purpose visual representation learning. Four kinds of visual tasks are considered, i.e., perception, grounding, reasoning, and action. Then, by choosing different pre-trained backbones, a unified experiment is conducted to compare the learned representations from these backbones. Discussion and analysis are provided according to the results of the above investigation.",
            "strength_and_weaknesses": "Pros:\n+ Pursuing the general purpose visual representation learning is important for our community.\n\n+ Some results and discussions are interesting and verify some conclusions of previous works, e.g. the comparison between CNN and Transformer, and the superiority of CLIP.\n\nCons:\n- There is no deep analysis of the relationship between the selected 11 tasks, only a performance correlation matrix is given. This is weird as the analysis of tasks is the most important thing to build a real unified benchmark, instead of simply compositing all existing sole datasets with domain gaps, different raw data, definition gaps, and annotation quality.\n\n- Few new insights and discussions are provided, except for sec. 5.3. Moreover, \"camera pose estimation and 3D reconstruction show fewer connections to other tasks\", this seems weird, camera extrinsic parameter estimation matters most in 3D reconstruction in tasks like structure from motion. Please give a discussion.\n\n- The claim about the proposed model is confusing. We these pre-trained encoders and fine-tuned decoders for these tasks in evaluation. But if we want to propose a new general model, what is the point of this model's existence? I think an experiment is suitable for this contribution instead of a model structure.\n\n- Some discussion about Pathway from Google?\n\n- ViT-B/32 MAE?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the writing is clear and detailed. Fig 3, the overview of the proposed model lies in the appendix, which is weird. The words in fig.1 are too small.\n\nQuality: Some claims about the contribution, especially the model and benchmarks seem not suitable.\n\nNovelty: weak. There are not many new insights and contributions of the general learning model, or benchmarks (metrics, data collection, annotation, etc.). Only some superficial conclusions and analyses are given.\n\nReproducibility: the authors have given the details of the proposed model and implementation, and all the datasets adopted are commonly-used, thus I think the reproducibility is OK. ",
            "summary_of_the_review": "Overall, the experiments are extensive and implementations are detailed. However, besides the composition of previous datasets and tools cannot result in a \"new benchmark\", at least not enough. New insights into the benchmark design, the relation between previous datasets and tasks, and how to design a new general model are all lacking. I appreciate the efforts of this work. However, deeper discussions are essential. Moreover, the contribution claim about the model is also not suitable. I encourage the authors to make a major revision to make the analysis and discussion deeper and more inspiring. Thus, I think this paper is not ready yet currently for ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2986/Reviewer_xbAo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2986/Reviewer_xbAo"
        ]
    },
    {
        "id": "KOoVSQZYOnV",
        "original": null,
        "number": 3,
        "cdate": 1666654983951,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654983951,
        "tmdate": 1666654983951,
        "tddate": null,
        "forum": "f6cywgfd11",
        "replyto": "f6cywgfd11",
        "invitation": "ICLR.cc/2023/Conference/Paper2986/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a General-purpose Visual Understanding Evaluation (G-VUE ) benchmark that covers 11 tasks in four functional\ndomains (Perceive, Ground, Reason, and Act). The benchmark is accompanied by several decoders that allow the evaluation of arbitrary visual representation models on these tasks. ",
            "strength_and_weaknesses": "Strengths:\n- The benchmark will be made easily accessible through an evaluation platform and a leaderboard.\n- The task correlation analysis and the decoders that can be used for evaluating any arbitrary model are nice pluses.\n\nWeaknesses:\n- It seems that the tasks are a composition of existing datasets with existing evaluation metrics. As such, the contribution seems limited. \n- It is unclear whether the tasks in each functional domain are necessary and sufficient for evaluating general-purpose human-like vision systems, let alone if evaluating each functional domain separately is the best way to test human-like visual cognitive abilities. \n- In Table 3, it is really difficult to extract from the summary score that ViT-16-CLIP is the best general-purpose model because, for some tasks, models have large deviations (such as I-T Retr.) while others do not. Would the underlying assumption of a summary average be sufficient, or should an overall evaluation metric consider task difficulty? Please also clarify the sentence \"To make domains more balanced, we augment each task in Act with weight at 1.5\".\n- It is unclear how extensible the benchmark is, e.g., how easy it is in the future to include more tasks as newer datasets are constantly being constructed.\n- Most evaluation does not consider multi-task models, but rather self-supervised or pretrained visual representations. It is a bit of a stretch to refer to this as a general-purpose vision benchmark in the conclusion and introduction (list of contributions). In addition, it would be great to have a direct comparison of a multitask model with the ones shown in Table 3. Adding this would demonstrate how far away self-supervised visual representation models are from finetuned multitask models.\n\nA few additional questions:\n1. In terms of the claim, that unified architectures are \"still limited to the conventional vision-language domain, lacking the general capabilities for low-level geometric understanding and high-level reasoning and acting\", what about GATO [1]?\n2. Also, why are the unified architectures on Table 4 only evaluated on tasks they can solve, and not the full pool of benchmark tasks? Aren't the decoders described in this work specifically proposed for facilitating general evaluation purposes? \n3. If the language encoder can be substituted with a CLIP language encoder, then what is the reason for not using this  CLIP language encoder for all models, and evaluating these models with improved cross-modal understanding capabilities?\n4. How do the custom licenses for some of the datasets affect benchmark license and usage?\n\n[1] Reed, Scott, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez et. al. \"A generalist agent.\" arXiv preprint arXiv:2205.06175 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "Well-structured related work and clear paper presentation, however, the current section #3 reads as a set of separate tasks, while it would be great to focus on the alignment between visual cognitive abilities and the tasks for each functional domain, such that each task is better motivated. For example, why semantic segmentation is covered, but not scene-graph generation? Why visual QA and not embodied QA? There also seems to be limited (or none at all) tasks on compositional zero-shot learning and affordances, which are also fundamental human abilities. The Appendix covers *some* of these questions but it does not seem to be the right place for including the motivation for selecting specific tasks and datasets. \n\nFigure 1 could be improved. It is unclear what the arrow between \"Visual Representation\" and the tasks represents.\n\nSmall grammar and typos here and there, e.g.,\n\"Architectures with too many complex ...\", \"Such findings further motivates the question ...\"",
            "summary_of_the_review": "This paper aims to create a benchmark that can holistically evaluate the cognitive abilities of visual systems. This is work in the right direction, with functional domains properly articulated and motivated. However, the current implementation is a compilation of existing datasets, each with its own evaluation metrics. Improving the overall evaluation, for example by considering task difficulty, or a curriculum based on these tasks, or a unified evaluation metric and dataset, would make the realized work more aligned with the overarching goal of human-like general-purpose vision systems.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2986/Reviewer_7KYb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2986/Reviewer_7KYb"
        ]
    },
    {
        "id": "0FdhDE2agd",
        "original": null,
        "number": 4,
        "cdate": 1666768758889,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666768758889,
        "tmdate": 1669232212784,
        "tddate": null,
        "forum": "f6cywgfd11",
        "replyto": "f6cywgfd11",
        "invitation": "ICLR.cc/2023/Conference/Paper2986/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a general purposed visual understanding benchmark (G-VUE), which covers four different abilities -- Perceive, Ground, Reason, and Act. The authors chose 11 existing tasks in this benchmark. The authors also provide a general encoder-decoder framework with pre-trained representations and tests on the benchmark. ",
            "strength_and_weaknesses": "[Strength]\n\n- The proposed benchmark involves four main functions, which are perceive, ground, reason, and act. Seems like a good combination of skills to test the unified model. \n\n- The author also proposes to use pre-trained representations to test the benchmark and evaluate two existing methods -- OFA and Unified-IO. \n\n[Weakness]\n\n- Creating a new benchmark is not simply aggregating existing datasets/skills. The more important for unified benchmark creation is what is the connection between each dataset, are there any shared concepts? what do we expect if the model performs well on one task and on others? Although the paper proposed interesting combinations of skills, there is no in-depth discussion on this. \n\n-  The reason for choosing the dataset in each skill is not clear. For example, In question answering, the authors choose GQA dataset for this benchmark. It is known that GQA is created by question templates and a lack of language diversities. VQA might be a better option. For an evaluation benchmark, the best option would be to use a mixed of existing benchmarks for the testing set. \n\n- The proposed framework uses frozen features and fine-tune the decoder on each task. The performance is far behind state-of-the-art results. Table 3 also doesn't have any STOA numbers on these tasks. My biggest concern is the trends shown in this table can not transfer or hint at other approaches. \n\n- In table 7, even with the fine-tuning encoder, the performance is still behind the state-of-the-art model. By freezing the encoder and having a separate decoder for each skill, the proposed framework is not a unified model anymore. \n\n- Table 4 shows two existing method results, OFA-HUGE and UIO-XL. These are interesting results, but many details are missing. For example, Is OFA-HUGE a single model or a model fine-tuned on each task? UIO-XL model didn't train on GQA datasets, this might be the main reason why the gap between OFA and UIO on VQA is huge. However, they may not reflect the general QA abilities since UIO also performed well on VQA datasets. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper overall is clear and well-written. Some of the work in supplementary should move into the main paper. ",
            "summary_of_the_review": "The paper proposed an interesting benchmark to evaluate more unified models. However, there are multiple issues related to the connection between different tasks/datasets, and dataset selection for each skill. The problem of the proposed encoder-decoder model etc. Please check the weakness session for details. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2986/Reviewer_9fAp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2986/Reviewer_9fAp"
        ]
    }
]