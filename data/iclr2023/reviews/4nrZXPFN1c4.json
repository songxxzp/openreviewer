[
    {
        "id": "pjD3Ze3n0i",
        "original": null,
        "number": 1,
        "cdate": 1666627159308,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627159308,
        "tmdate": 1670053731504,
        "tddate": null,
        "forum": "4nrZXPFN1c4",
        "replyto": "4nrZXPFN1c4",
        "invitation": "ICLR.cc/2023/Conference/Paper2834/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a transformer architecture called Energy Transformer that replaces the sequence of feedforward transformer blocks with a single large Associative Memory model. The sequence of the energy transformer layers is designed to minimize an energy function in charge of representing the relationships between the tokens. Experiments show that Energy Transformer achieves good performances on image completion and graph anomaly detection.  ",
            "strength_and_weaknesses": "[Strengths]\n+ The transformer is a prominent attention mechanism in machine learning, and it is interesting to design the Transformer mechanism from energy attention and the Hopfield network.\n\n[Weaknesses]\n1) The connection between Hopfield Networks, energy function, and Transformer is mentioned in Ramsauer et al. (2020). It is not clear the new insight of the proposed Energy Transformer.\n2) The effect of designing the proposed ET block is not apparent; it is better to conduct an ablation study experiment to assess each component within such a designed block.\n3) The detailed structure of the Energy Transformer is not explicitly depicted. A network architecture helps the reader to understand the ET implementation.\n4) Since the Energy Transformer requires recurrently updating, it is better to show the cost of training and testing compared with the vanilla Transformer.\n5) The experiments are weak. The quantitative result has merely the graph anomaly detection. However, the results of BWGNN (Hetero) and [A], which shows good performance, are not included. The on-par performance of the Energy Transformer does not make it impressive to the reader.\n\nRelated paper:\n[A] Derek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim: New Benchmarks for Learning on Non-Homophilous Graphs. Workshop on Graph Learning Benchmarks, WWW 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Unclear descriptions reduce clarity and reproducibility.\n\nTechnical novelty should be clarified.",
            "summary_of_the_review": "The primary concern of this paper is its weak experiments, which do not sufficiently demonstrate the strength of the proposed Energy Transformer.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_VTZR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_VTZR"
        ]
    },
    {
        "id": "fJIrZsXf66p",
        "original": null,
        "number": 2,
        "cdate": 1666646707909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646707909,
        "tmdate": 1670812865247,
        "tddate": null,
        "forum": "4nrZXPFN1c4",
        "replyto": "4nrZXPFN1c4",
        "invitation": "ICLR.cc/2023/Conference/Paper2834/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a new transformer architecture called energy transformer (ET). ET is designed to minimize a special handcrafted energy function. This function contains two terms. The attention term aligns the keys and queries between neighbor token vectors and the hopefield network term matches the token vectors to some memory vectors. The ET layer is applied recurrently to imitate a continuous time differential equation that minized the designed energy terms. The authors try to demonstrate the ability of their model through image completion task and graph anomaly detection task.",
            "strength_and_weaknesses": "Strength:\nIn general, I like the idea of this paper:\n1. This paper propose an specially designed energy-term that may be helpful for understanding the transformer structure.\n2. It derives a new transformer structure based on this handcrafted energy-term\n3. The authors show that this structure outperforms baselines in the graph anomaly detection task.\n\nPossible concerns:\n1. While the authors showcase experiments on image completion and graph anomaly detection task. I think these are not enough for me to draw the conclusion that this newly designed arctecture does have advantage over the previous one. The transformer structure is famous because it performs well in various task in computer vision(CV) and netural language processing(NLP). To really demonstrate the ability of a new structure, the authors need to show competitive results on those widely accepted tasks like transfer learning to ImageNet in CV or results on GLUE [1] benchmark in NLP. Currently, the authors only showcase two tasks that are well-suited for their model (and they only show quantative results in one of them), thus the general ability of this model is still unclear to me.\n\n2. Some designs of the model are not fully justified experimentally. For example, unlike traditional transformer architecture which uses different weights for different layers, ET apply a single block recurrently and the MLPs in their hopfiled network also share weights . The authors may find theoretical explanation for this design according to their energy design. However, whether this helps or hurts the performance is unclear. Empirically speaking, recurrent model or model with shared weight may be hard to optimize due to the vanishing gradient problem. Thus, ablation study may be needed to justify the these design choices.\n\n3. Questions for the graph anomaly detection task: How are the edges used by the model? Does the attention layer only compute attention over the nodes that have edges between them? Also, can the model be also used on directed graph? Or it is only limited to undirected graph?\n\n4. For the image completion task, the authors assumes the image patches can be represented as fully masked and fully open area. But what if the masked area is not divisible by the patches? Then do you need to re-design the patch size (which may influence the full structure of the model)? This might not be a problem if one just treat image completion as a self-supervised task to learn representations and then applied these learned features to downstream tasks. But if the image completion task itself is the target and its performance is used to showcase the ability of the model. Then possible limitation may need to be considered.  \n\n[1] GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING\n\n=====================================================================================================\n\n**Post rebuttal:**\n\nFor me, although in the new responses, the authors say that they are not trying to replace the original transformer architecture, what the paper mainly about is proposing a new transformer architecture based on modification of the original one. Thus, a natural question to ask is how this new design performs comparing with the original one. Although the authors show that their design principle is based on the Hopfield Networks, whether this principle does benefit the transformer architecture design needs further justification. There can be many interesting principles in modifying the designs, but whether they do make senses need to be supported by comprehensive and convincing experimental results. A valid modification to the transformer architecture needs to demonstrate its advantage over the original one. The current results, however, are far from that. The current results are mainly based in the graph domain. Solid experimental results in areas that transformers are mainly used in, like CV or NLP, are missing. I can not agree with the authors that since they expertise in graph domain, they only pick experiments in this area and miss others. As I said, if the paper narrows its objective into proposing a new technique to achieve STOA results in graph domain, then the current results are fair enough. However, if they want to make a valid and general design principle for transformer architecture, they should compare their new architecture in the areas that transformer architecture are widely used. As a reviewer, I'm not convinced by the current results and thus I still lean to reject. I would recommend the authors to polish their work with more convincing results in different areas or limit their objective to a technique that specialized in graph domain.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is in general well written and introduce its idea clearly. ",
            "summary_of_the_review": "I think this paper proposes some interesting ideas like designing a energy function and deriving a transformer structure based on this energy function. However, I think the experiments of this paper may not be strong enough to demonstrate that the new designed structure does have advantage over the original one. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_v3Fm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_v3Fm"
        ]
    },
    {
        "id": "PueTw2UFKAr",
        "original": null,
        "number": 3,
        "cdate": 1666651714530,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651714530,
        "tmdate": 1666651714530,
        "tddate": null,
        "forum": "4nrZXPFN1c4",
        "replyto": "4nrZXPFN1c4",
        "invitation": "ICLR.cc/2023/Conference/Paper2834/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThe paper puts forward a novel sequence-to-sequence neural architecture named Energy Transformer (ET) that builds\non recently introduced generalized Hopfield networks and their connection to transformer layers. \nThe architecture is composed by a single block that combines two subsystems,\ndefined by energy functions corresponding to two different types of generalized hopfield networks, \npreceded by a layer normalization operator. \nOne subsystem is in charge of pooling to each position the information coming from the other\npositions, while the other enforces the consistency of learned representations.\nThe system dynamics is given by a differential equation driven by the gradient\nof the energy function with respect to the renormalized features. The total\nenergy is proven to decrease along the trajectory. \nFor practical purposes, the dynamics is discretized and the block applied recurrently.\nThe architecture is validated qualitatively on an image occlusion task and quantitatively\non a node anomaly detection task.",
            "strength_and_weaknesses": "\nThe manuscript is an enjoyable read and contains a good degree of novelty. \nThe output of the network is given by the final outcome of an energy minimization process.\nAlthough this perspective is not new (see Yang et al '22), it has not been quite explored yet\nand I find the particular instantiation given in the paper quite inspiring. \nI think this could trigger additional theoretical and applicative results.\nThe resulting architecture is also quite simple and has a good degree of interpretability.\nThe paper is well organized, although some details given in the appendix could be moved to the main text.\nThe results on the node outlier detection tasks are very good, outperforming concurrent methods in\nmost cases.\n\nMain comments:\n\n- Why the authors didn't attempt to benchmark the performance on a standard image classification task? \n  This should be doable by pooling and putting an head on top of the final outputs, as commonly done in ViT.\n\n- Related to previous questions, is not clear if the ET can be considered as a drop-in replacement to self-attention\n  transformer blocks for arbitrary task, or if its usage should be confined to some specific settings.\n  I would like the authors to comment on the limitations of the model.\n\n- It is not clear how important the HN component is and how would perform the ET without it.\n\n- Could the author motivate why the attention module excludes the current the considere position from the input. \nWhat happens if you remove the restriction from the sum?\n\n- Did the authors explor stacking multiple ET modules in a feeforward fashion?\n\nMinor comments:\n- While it is clear in the appendix, I think also the main text should mention that the DE is discretized.\n\n- eq. 2, ignored the \\bar{x} dependence on x_i when computing the derivative. Should be fixed by inserting a factor 1 / (1 - 1/D) in the definition of L\n\n- \"is then minimized to train the whole network. Above, \u03c3 is the ratio of anomalous labels (lA = 1) to\nthe regular label.\" Maybe the other way around, that is regular labels / anomalous labels?\n\n- The main text should mention that for the graph task a neighborhood softmax (similar to GAT) is used instead of standard attention\n\n- The authors could consider numbering more equations for readibility.",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript is overall quite clear and contains novel and interesting results.\nThe experimental settings are fully detailed and the code will be publicly released.",
            "summary_of_the_review": "I think this is a good paper that deserves acceptance.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_UVr7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_UVr7"
        ]
    },
    {
        "id": "DxFrdU9TsGw",
        "original": null,
        "number": 4,
        "cdate": 1666688698264,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688698264,
        "tmdate": 1666688698264,
        "tddate": null,
        "forum": "4nrZXPFN1c4",
        "replyto": "4nrZXPFN1c4",
        "invitation": "ICLR.cc/2023/Conference/Paper2834/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new transformer architecture that replaces the sequence of transformer blocks with a single associative memory model. Specifically, the model is designed to minimize a specific energy function that represents the relations between tokens. The model is evaluated on image completion and graph anomaly detection, and experiments show encouraging results.",
            "strength_and_weaknesses": "Strength\n- Despite the great performance achieved by the Transformer model, its architecture is designed empirically. This work proposes a Transformer architecture from a purely theoretical perspective.\n- This work provides a deep insight into the relationship between Transforms and associative memory models. Based on the relationship, it designs a new energy function and a corresponding Transformer architecture that minimizes the energy function.\n\nWeakness\n- To show the effectiveness of ET, it is better to evaluate it on more mainstream tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "good\n",
            "summary_of_the_review": "It is an interesting work toward understanding the Transformer models. To show its effectiveness, more experiments have to be conducted on more challenging tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_zHgn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_zHgn"
        ]
    },
    {
        "id": "TghgcpJcrYw",
        "original": null,
        "number": 5,
        "cdate": 1667012008657,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667012008657,
        "tmdate": 1670298524348,
        "tddate": null,
        "forum": "4nrZXPFN1c4",
        "replyto": "4nrZXPFN1c4",
        "invitation": "ICLR.cc/2023/Conference/Paper2834/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the energy transformer, a transformer architecture that uses a recurrent energy transformer block. The energy transformer block updates its input in accordance to minimizing two energy functions. Experiments on image reconstruction and graph anomaly detection demonstrate the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "Strength: The proposed method is interesting in the sense that it reformulates transformer architecture in a way to combine Hopefield networks and attention networks, which are recurrently optimized using two carefully designed energy functions. The empirical results look promising. \n\nWeakness: I am concerned about the clarity of the paper. For example, in the explanation of equation (3), \"The log-sum energy function (3) is minimal when for every patch in the image its queries are aligned with the keys of a small number of other patches connected by the attention map,\" It is not clear what does this mean in a mathematical sense. There is also no detailed algorithmic description about how the model is trained. For example, for the image reconstruction task, how does minimizing the energy functions correspond to reconstruction? It would be helpful if the authors include a formal algorithmic description of the proposed method. \n\nI am also concerned about some of the experiment settings. For example, for image reconstruction task, Figure 3 only shows some of the examples. However, this does not represent its general performance. One way to make this experiment more complete may be to include the MSE of the test datasets, the variance of the MSE, and perhaps reconstructed images corresponding to the best MSE and the worst MSE. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Although the proposed method seems novel, the methodology of the proposed transformer architecture is not clear enough to me. The experimental results are also not very satisfying, as detailed in the previous section. ",
            "summary_of_the_review": "Although the paper is interesting to read, I have two major concerns as detailed in the weakness section. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_iuLg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2834/Reviewer_iuLg"
        ]
    }
]