[
    {
        "id": "-0PQMuQxQwd",
        "original": null,
        "number": 1,
        "cdate": 1666281494393,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666281494393,
        "tmdate": 1669021060279,
        "tddate": null,
        "forum": "kPLzOfPfA2l",
        "replyto": "kPLzOfPfA2l",
        "invitation": "ICLR.cc/2023/Conference/Paper2743/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a method to rotate the weight space in a way that newly acquired knowledge does not interfere with the previous one. In the rotated space, only few dimensions are important for previous tasks and thus, they can be frozen to encode new knowledge in the remaining weights. The proposed method outperforms all the considered the state-of-the-art baselines.",
            "strength_and_weaknesses": "Strengths\n=======\n* The proposed method is sound and generalizes the methods of Saha et al., 2021 and Wang et al., 2021.\n* Results are state of the art\n* The authors provide ablations and code, which makes their work more reproducible\n\nWeaknesses\n=========\n* Given the generality of the method, it is not clear why it is only tested on class-incremental learning\n* Only papers from 2021 are considered for comparison, missing works like [A]\n* I found the notation complex and I had to read the paper multiple times to understand all the equations. I still have trouble with the relationship between Hilbert spaces and equation 4.\n\n[A] Hersche, Michael, et al. \"Constrained Few-shot Class-incremental Learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n=====\n* For the most part the paper is clear. However, I found the description of the method complex and it looks like it could be simplified.\n\nQuality\n=====\n* The quality of the related work could be improved. For example [B] presents a method to rotate the weights of a neural network to perform EWC, which is related to your work.\n\nNovelty\n======\n* The proposed method is incrementally based on the work of Saha et al., 2021 and Wang et al., 2021, which the authors acknowledge (which is a positive thing).\n\nReproducibility\n===========\n* The authors provide all the necessary tools to reproduce their method.\n\n[B] Liu, Xialei, et al. \"Rotate your networks: Better weight consolidation and less catastrophic forgetting.\" 2018 24th International Conference on Pattern Recognition (ICPR). IEEE, 2018. ",
            "summary_of_the_review": "The proposed method is sound and achieves good performance. My main concerns are about comparison with recent papers like [A], generality (why few-shot incremental and not class incremental, for instance), and the amount of novelty. Overall I think this is a good paper and I would be willing to raise my score after discussion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2743/Reviewer_hFPG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2743/Reviewer_hFPG"
        ]
    },
    {
        "id": "-pzRiaOn5Sn",
        "original": null,
        "number": 2,
        "cdate": 1666582489430,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582489430,
        "tmdate": 1669087382707,
        "tddate": null,
        "forum": "kPLzOfPfA2l",
        "replyto": "kPLzOfPfA2l",
        "invitation": "ICLR.cc/2023/Conference/Paper2743/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper introduces weight space rotation process into the few-shot class-incremental learning. Specifically, the proposed method can identify the important parameters and freeze them in the following sessions.",
            "strength_and_weaknesses": "Strengths: \n\nThe paper is well-written. The motivation is clear and the method is technically correct. \n\nWeaknesses:\n\n1). In experiments, the comparisons with the previous methods are not enough. The following papers are not compared, but they seem to outperform the proposed method:\n\n[1] Few-shot class incremental learning by sampling multi-phase tasks. (TPAMI)\n\n[2] Subspace regularizers for few-shot class incremental learning. (ICLR \u201822)\n\n[3] Metafscil: A meta-learning approach for few-shot class incremental learning. (CVPR \u201822)\n\n[4] Constrained few-shot class-incremental learning. (CVPR \u201822)\n\n[5] Few-shot class incremental learning via entropy-regularized data-free replay. (ECCV \u201822)\n\n[6] Few-shot class-incremental learning from an open-set perspective. (ECCV \u201822)\n\nI notice that the authors give references for [2] (introduction) and [1] (related work), but I do not find a reasons to ignore them in the comparison in experiments.\n\n2). Fixing part of the network, though seems to be novel in FSCIL, but has been studied in the very similar task CIL (Adaptive Aggregation Networks for Class-Incremental Learning, CVPR \u201821). The motivations are very similar. It would be better to have a discussion instead of claiming \u201cfirst work that makes changes to the parameter space itself\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is with a good clarity.",
            "summary_of_the_review": "The paper is well-written and has a clear motivation. But the experiments do not support the advantage of the proposed method. Comparison is not enough. Necessary discussion is lacking to support the novelty. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2743/Reviewer_AjwX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2743/Reviewer_AjwX"
        ]
    },
    {
        "id": "O1TMszFRH_F",
        "original": null,
        "number": 3,
        "cdate": 1666663450265,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663450265,
        "tmdate": 1666663450265,
        "tddate": null,
        "forum": "kPLzOfPfA2l",
        "replyto": "kPLzOfPfA2l",
        "invitation": "ICLR.cc/2023/Conference/Paper2743/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors propose an algorithm to efficiently learn new classes in an incremental setup with a few-shot dataset at every iteration. Towards this, they propose WaRP - which first changes the basis of the parameter/weight space of the base network and then uses SVD to find parameters with less importance in that space. At every round, a fraction of parameters which are found to be unimportant (with a metric defined in this paper) are fine-tuned to learn new classes while the important parameters are kept fixed. Important parameters found after each iteration are added to the original set of important parameters. On standard few-shot class incremental learning (FSCIL) benchmarks, WaRP outperforms existing methods. ",
            "strength_and_weaknesses": "**Strengths**\n* The narrative of the paper is crisp, easy to follow and enjoyable to read. Authors clearly motivate why changing the basis is important (instead of staying in the original parameter space) with an explanation in the two-dimensional space. After that, authors use the existing knowledge that activations reside in a low-dimensional sub-space to find unimportant parameters in this new basis and updates only those parameters for a given task while keeping the important parameters fixed. Every major component of the proposed algorithm is well motivated, clearly explained and intuitively makes sense. \n* Apart from only providing a more theoretical overview of the proposed method, authors also provide details on how the weight-space change of basis (re-parameterization) can be implemented for a fully-connected and convolutional layer. Authors also improve the computational cost of the method by organizing the parameters in a way such that standard automatic differentiation packages can be used.\n* Experimental results are performed on multiple FSCIL benchmarks and the proposed method outperforms existing methods (on-average, ~1 point over the SOTA method F2M). Authors also provide reasonable ablation studies to showcase the importance of various components including weight space re-parameterization, amount of forgetting with the amount of parameters modified etc. - overall, the empirical evaluations properly justify the efficacy of the algorithm.\n\n**Weaknesses/Clarifications needed**\n* For me, a major missing piece in this paper is the lack of comparison against methods from the domain of neural network pruning. Although the authors compare against existing methods from the literature, but for me, an important baseline to compare this method would be to take a pruning algorithm e.g. any channel pruning method for CNN, use that to identify which channels are unimportant after every round and only fine-tune those channels for a given task. Apart from channel pruning, other methods like layer pruning or structured pruning are also relevant to this discussion.\n* Modifying the basis by re-parameterization requires non-trivial amount of work specific to each neural network operator. Authors provide guidance on how to do it for Conv/FFN, but I'd like to understand if this process can be generalized and applied to other kind of operators - multi-head attention from a Transformers network architecture or a transposed convolution operator for example. This can potentially hinder the adaptability of this method to other type of tasks or architectures.\n* I'd like to see a plot on how many parameters are staying unimportant after every round - this will help us understand how does this method scale to more number of tasks.\n* Finally, some details on the overall computational cost increase (at the beginning to change the basis and then for every iteration) compared to doing a standard fine-tuning will help us understand the cost/performance trade-off better.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe narrative of the paper is clear and straightforward. Although the proposed method is somewhat complicated, authors do a great job in motivating each piece and explaining the underlying details of each component carefully. I appreciate the extra attention authors have paid to explain the overall procedure in a succinct manner (sec 3.3).\n\n**Quality & Novelty**\n\nI find this paper to be meeting the quality bar for a top-tier ML conference. The writing is crisp and pleasure to read, proposed method is well motivated and explained and finally, sufficient empirical results are provided to validate the hypothesis. The idea of moving the weight to a different basis and performing weight update in that space is unique and as a whole, there is enough novel contribution in this work.\n\n**Reproducibility**\n\nAuthors have provided enough details pertaining to their proposed algorithm, backbone and other hyperparameters. My only minor concern regarding reproducibility is the difficulty in terms of extending this method to other neural network operators in future.\n\n\n",
            "summary_of_the_review": "Overall, I found the paper to be well written, clearly motivating the proposed method along with explaining each component in details and finally showcasing the efficacy of the method on standard benchmarks against current SOTA methods and via additional ablation studies. At this point, based on my opinion alone, I am leaning towards accepting this paper. I'd still like the authors to respond to my comments - especially regarding the channel pruning style baselines.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2743/Reviewer_1hev"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2743/Reviewer_1hev"
        ]
    },
    {
        "id": "hP3N0uTyt5z",
        "original": null,
        "number": 4,
        "cdate": 1666732870968,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666732870968,
        "tmdate": 1670526274648,
        "tddate": null,
        "forum": "kPLzOfPfA2l",
        "replyto": "kPLzOfPfA2l",
        "invitation": "ICLR.cc/2023/Conference/Paper2743/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to mitigate catastrophic forgetting in class-incremental few-shot learning (CIFSL). The key observation is that there may be many directions in the weight space that are flat with respect to the loss. If these directions can be identified, then updates may be freely made in these directions without affecting performance on the previous classes. In order to do this, a method called WaRP is proposed that utilizes the SVD of the network activations to construct a rotation such that the majority of directions are flat. For each subsequent incremental learning session, only a subset of the parameters with respect to the new basis are updated. The subset of parameters to be learned in each learning session are chosen based on a score criterion that relies on the magnitude of the gradients. Experiments on several standard CIFSL benchmarks demonstrate the improvements of the proposed WaRP method relative to baselines.",
            "strength_and_weaknesses": "Strengths\n- Writing is clear and contains several interesting insights distributed throughout the paper.\n- The method makes intuitive sense and should be easy to implement.\n- The experiments are complete and contain a set of ablations justifying design decisions.\n- The proposed WaRP method could have impact in a variety of transfer learning scenarios beyond CIFSL. Any setting where performance on previous tasks should be preserved can take advantage of the ideas behind WaRP.\n\nWeaknesses\n- The method seems to be primarily focused on preserving accuracy with respect to the base classes rather than the incrementally learned novel classes. In particular, the basis is computed only once after training on base classes, rather than updating after each session.\n- After many learning sessions, it seems plausible that there could be a shortage of trainable parameters.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is excellent. In particular, the introduction and Figure 1 motivate the problem well. The quality is also good, with complete experiments and ablations. With respect to novelty, the paper does not have a large amount of strictly novel material per se but rather pieces together various bits of knowledge into an elegant solution for catastrophic forgetting. In my view, this is a sufficient amount of novelty. Reproducibility also is good and code is included in the supplementary materials.\n\nSome other comments:\n- For the ablations over alpha, I found it slightly disorienting for the plots to focus on the end sessions only. Perhaps showing the performance increase or decrease relative to a baseline alpha might make the plots easier to parse?\n- One aspect of the submission that could be improved is an analysis of the learned basis and masks. For example, are parameters masked evenly throughout the network, or are they concentrated in either the lower or upper parts of the network? If the latter, then this matches the intuition that lower-level representations can be fixed with fine-tuning performed on top. WaRP offers data-driven masking and thus is in a unique position to be able to answer these questions.\n",
            "summary_of_the_review": "Update post-rebuttal: The authors have addressed my remaining concerns with additional results and ablations. I will maintain my original rating.\n\n---\n\nOverall, this is a strong submission. It addresses a meaningful problem and provides an effective and elegant solution to tackle it. Experiments show the proposed WaRP method provides consistent accuracy improvements relative to baselines.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2743/Reviewer_Z3CK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2743/Reviewer_Z3CK"
        ]
    }
]