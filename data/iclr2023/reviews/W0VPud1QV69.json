[
    {
        "id": "GPpUUssizu",
        "original": null,
        "number": 1,
        "cdate": 1666525932477,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666525932477,
        "tmdate": 1670143118923,
        "tddate": null,
        "forum": "W0VPud1QV69",
        "replyto": "W0VPud1QV69",
        "invitation": "ICLR.cc/2023/Conference/Paper3046/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A popular pre-training GNN method is to mask out some edges and GNN is trained to recover them. However, there exists graph mismatch can mislead the performance. To handle this problem, the authors propose DiP-GNN which can better match the original graph. Specifically, the generator is trained to recover identities of the masked edges. Simultaneously, a discriminator is trained to distinguish the generated edges. Finally, the discriminator is chosen to accomplish the downstream tasks.",
            "strength_and_weaknesses": "### Strength:\nThe authors introduce generative mechanism into graph pre-training and propose a DiP-GNN (Discriminative Pre-training of Graph Neural Network) framework. Meanwhile, compared with generative pre-training, the proposed reconstructed graph fed to the discriminator will better match the original graph.\n\n### Weakness:\n1) Lack of analysis about the generated graph. As shown in Fig. 1, the different masked-edge impacts the performance. Hope the authors can clarify how to choose the masked edge and analysis the influence of the masked edge.\n2) Some statements are unclear and mislead the reader. For example, in section 1, the authors state that \u201cwe find that missing edges hurt more than wrong ones\u201d, which is contrary to common sense but interesting. Although the authors utilize the experiment results to verify this point, I hope the author can explain and analyze it in more detail.\n3) Lack of some explanation of equations. For example, the loss function in section 3 lacks an explanation and formula label.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The statement of the paper is relatively clear and logical. \n\nQuality: The presentation of the paper is relatively clearly.\n\nNovelty: The authors propose a novel graph pre-training strategy which introduces the generative strategy. Compared with the current model, they fed the discriminator with the reconstructed graph to better match the original graph.\n\nReproducibility: The model framework is relatively easy to reproduce.\n",
            "summary_of_the_review": "To handle the graph mismatch in graph generative pre-training, this paper proposes an interesting DiP-GNN (Discriminative Pre-training of Graph Neural Network) pre-training framework. Specifically, the generator is trained to recover identities of the masked edges. Simultaneously, a discriminator is trained to distinguish the generated edges. \nThe major strength is that the proposed reconstructed graph fed to the discriminator will better match the original graph, compared with generative pre-training.\nThe major weakness is that there lacks some detail explanation about the statement and the generated graph. Apart from that, some equations lack explanation. Meanwhile, some typo errors need to be rectified such as adding the equation label.\nApart from that, I have an interesting question. In section 3.1, the authors mask some edges not nodes. So, if all adjacent edges of the node are masked, how to handle this node in training.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3046/Reviewer_K6Uv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3046/Reviewer_K6Uv"
        ]
    },
    {
        "id": "WFvGtAuyLOa",
        "original": null,
        "number": 2,
        "cdate": 1666540010044,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540010044,
        "tmdate": 1666540010044,
        "tddate": null,
        "forum": "W0VPud1QV69",
        "replyto": "W0VPud1QV69",
        "invitation": "ICLR.cc/2023/Conference/Paper3046/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, a pre-training method of GNN is proposed. The whole framework consists of generator and discriminator, and the discriminator is used as the pre-trained model. Therefore, the proposed method could be regarded as a variant of GAN on GNN, though there are significant differences between them. The authors also discuss the generation/discrimination of both edges and node features. ",
            "strength_and_weaknesses": "### Strength\n- The paper is well-written and easy to follow. \n- The discussions of both edges and node features are sufficient. \n- The method is proposed to be applied to the pre-training of GNN which is different from the existing GNN setings\n\n### Weakness\n- Although the authors claim that \"*our setting is different from conventional self-supervised learning settings, namely we pre-train and fine-tune on two separate graphs*\", I still have some questions about the experiments: \n    - Could it show the superiority of DipGNN on transfer learning of GNN, since the authors claim \" This meets the practical need of transfer learning\"? \n    - The compared methods seems a little confusing. If the competitors are mainly the self-supervised/unsupervised methods, the methods proposed by Kaveh Hassani et al. (ICML 2020) and SGC (Felix Wu et al. ICML 2019) are missing. \n    - As DipGNN consists of pre-training and fine-tuning, do the all methods reported in Tables 1 and 2 have these two phases as well? \n    - As the pre-training technique is usually used in large-scale datasets, some experiments on OGB datasets may be convincing.\n\n- Although the details of generator and discriminator are different, the novelty seems a little common compared with the existing methods. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. \n\nThe novelty of the core idea/motivation seems a little ordinary though the details are different from the existing methods. \n\nI have some questions about reproducibility mentioned in \"Strength And Weakness\" which requires the feedback of the authors. ",
            "summary_of_the_review": "The paper is well-written and propose a method applied to the pre-training of GNN which is different from the existing GNN setings. \n\nHowever, I have some concerns about the experimental settings and the originality.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3046/Reviewer_S9Gb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3046/Reviewer_S9Gb"
        ]
    },
    {
        "id": "urbdSBxvDr",
        "original": null,
        "number": 3,
        "cdate": 1667182257592,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667182257592,
        "tmdate": 1667182257592,
        "tddate": null,
        "forum": "W0VPud1QV69",
        "replyto": "W0VPud1QV69",
        "invitation": "ICLR.cc/2023/Conference/Paper3046/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a discriminative method for pre-training Graph Neural Networks. The main idea is to simultaneously train a generator to recover identities of the masked edges, and train a discriminator to distinguish the generated edges from the original graph\u2019s edges. \n",
            "strength_and_weaknesses": "Strength\n- The proposed method makes sense.\n- The empirical results show the improved performance brought by the proposed methods on a number of  benchmarks.\n\nWeaknesses\n- There is no mentioning of n_1^hat or generated edges e_g in the loss of the generator. How should I understand that the generated edges are not captured in the generator loss?\n\n- Also, it is said that the discriminator is to distinguish edges that are from the original graph and edges that are generated. It would make sense to contrast the masked edges vs the generated edges, especially the ones that are different. However, the training loss for the discriminator seems to still focus on the unmasked edges vs generated edges. I don\u2019t quite understand the intuition.\n\n- A masked edge has two end nodes n_1 and n_2. How do the authors decide which end node to predict?\n\n- Why is the same lambda shared between edge loss and feature loss, given these two losses are so different?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is unclear in various aspects, as explained in above comments.",
            "summary_of_the_review": "Overall, the proposed method makes sense. However, I feel the authors are selling the methodology wrong. Instead of saying that discriminative pre-training is better than generative training, the authors should present it as a method for applying generative and discriminative pre-training jointly. Otherwise, why don\u2019t you just rely solely on discriminative pre-training?\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3046/Reviewer_FMbA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3046/Reviewer_FMbA"
        ]
    }
]