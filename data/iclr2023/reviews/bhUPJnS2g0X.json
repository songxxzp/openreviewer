[
    {
        "id": "RPgz4v5y2uM",
        "original": null,
        "number": 1,
        "cdate": 1666541549076,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541549076,
        "tmdate": 1666541549076,
        "tddate": null,
        "forum": "bhUPJnS2g0X",
        "replyto": "bhUPJnS2g0X",
        "invitation": "ICLR.cc/2023/Conference/Paper5912/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a simple strategy to prompt language models: Ask Me Anything. From the findings that open-ended questions outperform restrictive prompts, AMA first encourages the LMs to generate open-ended questions, which is a scalable approach. From a few open-ended questions generated by the model, the LM answers the question for each prompt chain. From a set of different predictions, AMA uses weak supervision to aggregate the noisy answers. The result shows that AMA applied on 6B GPT-Neo outperforms 175B GPT-3.",
            "strength_and_weaknesses": "Strength:\n1. The improvement of AMA is significant (GPT Neo 6B outperforming GPT-3 175B). Also, the experiments are extensive, evaluating on various model families such as OPT and BLOOM. This shows the effectiveness of AMA is universal.\n2. This approach is scalable since AMA uses LLM itself to generate open-ended questions.\n\nWeakness:\n1. To apply AMA in real use cases, AMA needs 1000 training examples from the training set of the target task (specified in Appendix A.3.). This is an unrealistic setting because only a few training instances might be present depending on the task. What if only a few training examples are present for a task (similar to few-shot setting)? Also, we need to train the probabilistic graphical model for each task. This introduces additional latency for task adaptation.\n2. For the AMA prompts on Appendix H, it seems that some tasks (DROP, SST, BoolQ, COPA) only do either prompt() or answer() stage, not both. How can we decide \"automatically\" when to do only prompt() or answer() or both depending on the target task?\n3. This paper is quite unclear on how the predictions are mapped to the output space. Are the predictions mapped to the output space using a predefined rule? (ex) no -> False for Figure 1) \n4. Are there any results on a large-scale LM evaluation (175 OPT, GPT, BLOOM, etc,) comparing using AMA or not on the evaluation setting of Table 1 and 2?\n2. The structure of Section 3.2 and Section3.4 is divided into observation and solution (AMA). However, the observation experiments are not extensive enough. Is there a reason for selecting 3 tasks (CB, WSC, RTE)? Observing the findings on more tasks would make the findings more general and the motivation of AMA stronger. \n5. I think that StoryCloze should be included in Natural Language Understanding instead of NLI for Table 1 and 2.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is novel in that it is effective and scalable. However, this paper contains some unclearness, especially on weak supervision and how the predictions are mapped to the output space. See Weakness part for details.",
            "summary_of_the_review": "This paper proposes Ask Me Anything (AMA), a simple prompting method that outperforms LMs that are 30x times bigger. By making the LMs generate open-ended questions and aggregating predictions based on weak supervision, AMA is scalable and shows improved performance. Although I recommend this paper be accepted, the authors should answer and clarify the questions on the weakness part (especially Question 1 and 2).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5912/Reviewer_SsWm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5912/Reviewer_SsWm"
        ]
    },
    {
        "id": "qw1i8LvWaAb",
        "original": null,
        "number": 3,
        "cdate": 1666571985796,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666571985796,
        "tmdate": 1666620051511,
        "tddate": null,
        "forum": "bhUPJnS2g0X",
        "replyto": "bhUPJnS2g0X",
        "invitation": "ICLR.cc/2023/Conference/Paper5912/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a technique for aggregating predictions from multiple prompts inspired by weak supervision. They exploit the idea that there is a dependency graph labels, errors, and prompts, and use this to train a mapper from raw prompt output to a final predictions competing mostly against a simple voting baseline.\n\nFor the most part, their technique for aggregating prompt predictions is treated as a black box. Also, there is another aspect of their technique: they transform input examples into a special format for their prompt. Presumably, this transformation works better for aggregation.\n\nTo me, this is primarily an empirical study, and the main contribution is strong results showing their approach enables a 6B param model to outperform a less advanced version of a 175B param model.",
            "strength_and_weaknesses": "# Strengths\n\n- Consistent improvements over the base model when using the new aggregation. \n\n- There is useful analysis: the observations related to dependencies between prompts, the diagnostic, etc. Including many details throughout the methods section motivating the approach.\n\n- The extensive results and appendix will be valuable to others using prompting.\n\n# Weaknesses\n\n- I found the story line a little confusing. The aggregation based on weak supervision is touted as main accomplishment, but this actually only gives a small improvement in Table 1 (but this small push does give edge over GPT-3 sometimes). In many of the tasks the 6B model already beats the 175B without weak supervision based aggregation. Also this table is not really apples-to-apples comparison making it hard to interpret. Should make it more clear what is the goal in this table --- is GPT-3 simply used more like a reference than a baseline? Also, is it fair to compare against GPT-3 like this when so many other prompting techniques have been developed since release?\n\n- To explain more previous weakness, I was not clear when reading if it is enough to do prompt re-formatting or aggregation is also necessary. I think still I am a little not sure, also about whether re-formatting is necessary for aggregation to work well. In general, it could help to tailor overall story as being primarily about prompt re-formating or aggregation.\n\n- It is well known that aggregating can be effective for prompting, and there are existing techniques like (self-consistency combined with chain of thought) that typically yield good improvements. Perhaps worth using those as baselines, or mentioning why they weren't used. Maybe CoT is not effective for smaller models?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Comments\n\nIt would be helpful to have finetuning baselines. For example Brown et al mention such baselines for BoolQ (91), CB (96.9), etc. in their Table 3.8. Others: DROP (89.1), NQ (44.5), WebQS (45.5). Sometimes the improvement over GPT-3 is great but still far behind finetuning.\n\n# Related Work\n\nSelf-consistency is a very crucial baseline / related work. Unlike the text says, it does not require additional training. It is an insightful innovation that plurality vote is effective when you have different reasoning paths, which runs counterintuitive for how samples from a standard prompt behave.\n\nAnother related work with diverse prompting is Li et al. On the Advance of Making Language Models Better Reasoners. https://arxiv.org/abs/2206.02336\n\nThere is a lot of other works that describe strategies for exemplar selection. One particular relevant example is Shi et al. (https://arxiv.org/pdf/2204.11454), which found aggregating 5 prompts (3 distinct exemplars each) is better than single prompt with 15 exemplars.\n\nThere are other works that do sequential prompting like least-to-most and SeqZero.\n\n# Minor Comments\n\nThe analysis / diagnostic in Fig 5 is great, and builds confidence that sub-tasks are related to final performance.\n\nIt was not always clear to me what is the source of weak supervision... Are you using the labels from the task, and if so, isn't this standard supervision? Are the prompts themselves considered the source of noise?\n\nIt was not immediately clear to me what was meant by \"highly-correlated outputs\".\n\nnit: Are you taking majority vote or plurality vote?\n\nnit: The framing of perfect vs. imperfect prompts is not the strongest. Perhaps it is sufficient to reframe and say you are introducing an efficient and effective ensembling method?",
            "summary_of_the_review": "To me, the work is valuable because it provides some useful results and extensive analysis in a popular topic, and probably more so because it is with an open source model. Simultaneously, it almost seems like a combination of a technique from weak supervision and prompting, and does not provide a surprising insight (since others have done something similar before regarding prompt reformatting and aggregation, and the aggregation method is not new), although perhaps I have missed some key point about why this is particularly relevant for the included models. I wish the story was more clean with respect to whether the paper is about analysis alone, presenting a new aggregation method, or prompt reformatting. Or maybe the bigger point is that we do not need big models and can simply get the same performance by fixing small models. Also, I wish there was more analysis about dependencies between prompts that inspired the aggregation method --- this did not give as big an improvement in Table 1 as prompt reformatting AFAICT, but I found this to be an interesting point of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5912/Reviewer_CQxp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5912/Reviewer_CQxp"
        ]
    },
    {
        "id": "-8QB5TXERm5",
        "original": null,
        "number": 4,
        "cdate": 1666639075955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639075955,
        "tmdate": 1670945166810,
        "tddate": null,
        "forum": "bhUPJnS2g0X",
        "replyto": "bhUPJnS2g0X",
        "invitation": "ICLR.cc/2023/Conference/Paper5912/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on zero-shot setting and looks into generating prompts that will improve zero shot performance. It has been known that LLMs behave very differently given even slightly different prompts, and a lot of effort can be spent finding the perfect prompt. Authors propose to, given the input, generate several \"imperfect\" prompts for this input and then aggregate the results. To generate the imperfect prompts, authors come up with a number of templates (question template and answer templates) that teach model to, given an input, generate the questions for the input and then to answer these questions. The final prediction is an aggregation of prediction from the answer step. The aggregation happens via weak supervision (the model for this is also learnt). The experiments show impressive improvements  for smaller models, allowing them to match that of larger models few shot performance.\u00a0\u00a0\nAdditionally, authors provide some insights as to what makes the good prompt. For example, they find that open ended prompts do better than prompts with suggested restricted output\n",
            "strength_and_weaknesses": "Pros:\n-  impressive results (zero shot performance of smaller models is comparable or better than few shot of larger) on context dependent tasks\n- the idea is really neat \ncons:\n- not flashed out enough to allow me to reproduce/code it up (without looking at authors' code)\n- clarity (please see more detailed comments)\n- less impressive results on closed book tasks\n- not sure how much effort/tuning goes into prompt genereation and WS",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: ok\nI really found this paper hard to follow. After reading the main paper the reader is left wondering as to what was actually proposed. The intro should really focus more on Figure 1 and highlight the overall algorithm better (generate the questions, generate the answers for these generated questions, aggregate the output).  I feel that Figure 1+ Appendix F should form a good chunk of the main paper, showing what happens, why you believe that questions and answers that were generated by AMA are of good quality. \nAdditionally, apart from \"insights\" as to what makes good questions, authors don't provide enough details of what is being generated. How many templates do you end up using? Is it task dependent? Were these templates selected for each dataset manually? \nFinally, the weak supervision should be also described in the main paper, as it is an integral part of the approach. \n\nNovelty: after having (finally) understood the approach, I do find it witty. Authors are generating a number of prompts that hopefully highlight different angles of the input, and combine the outputs of the model based on these prompts to come up with a final solution. \n\nReproducibility: see my comments on clarity. As far as I understand authors open source their code (I have not seen where???) so this should help reproducibility, but overall just by reading the paper, I don't think I would have enough details to reproduce the approach.",
            "summary_of_the_review": "Update: in light of additional comparison with prompt tuning, i am raising my score. I appreciate all the additional experiments authors ran. I still encourage to improve the flow/the story/the presentation, but the method deserves to be seem\n\nUpdate: i am keeping my score of marginally above. I do think the narrative can be improved, but it is also an interesting peace of work and it is pretty witty to make the model generate the questions and answers to improve the prompting. \n\nOverall It is a very good idea of forcing the model to write the prompts that it is able to respond to. I do think the main paper should be reworked to include more examples/explanations of what is being proposed plus WS part.\n\nAdditional questions\n1) Are all the models you tested are next token (perfix) trained, not span corruption trained correct?\n2) With respect to having less of improvement on closed book tasks - do you think it is because that the answers you show in the \"anwers\" part of the chain don't draw on closed book knowledge? I assume your \"answers\" templates are the same for various tasks so they don't include that closed book knowledge?\n3) What is the complexity (time and space) of weak supervision?\n4) You say in Table 1 that prompts can abstain from the predictions. What does it mean? Don't you get an output for each of your prompt chains?\n5) Are questions prompts and answer prompts task specific? How did you create them\nHow many chains do you create for each input during inference? \nBuilding on this\nIsn't it the case that AMA performance will depend greatly on question() step? If the questions are not well done then when you create answer prompts, the model will not produce the result you are looking for\n6) This seems to work well on simpler tasks (yes or no, question answering), but i am struggling to understand how the combining of the results will work for more generative tasks like summary tasks where variance of the predictions will be much higher\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5912/Reviewer_Pytd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5912/Reviewer_Pytd"
        ]
    },
    {
        "id": "m4jFIitnjuP",
        "original": null,
        "number": 5,
        "cdate": 1666804129089,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666804129089,
        "tmdate": 1669611967647,
        "tddate": null,
        "forum": "bhUPJnS2g0X",
        "replyto": "bhUPJnS2g0X",
        "invitation": "ICLR.cc/2023/Conference/Paper5912/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a new \"meta\" prompting method to achieve better accuracy in few-shot prompting. This is based on the observation that certain types of questions tend to be more amenable to few-shot prompting (specifically, QA-type prompts). This prompting method (AMA), includes:\n- Creating question() prompts, which transfer the original task into an open ended question\n- Creating answer() prompts, which transfer the result to the actual answer\nThey then aggregate a range of answers for a given prompt into a final answer, and find a significant performance lift",
            "strength_and_weaknesses": "Strengths:\n- This paper has impressive empirical results in that it up-levels the 6B parameter model to the accuracy of the 175B parameter model for most of the tasks\n- In general, the observation that question-based tasks are much more likely to be answered correctly than other formats of tasks is really interesting. This is exactly the sort of quantitative/qualitative evaluation of benchmark datasets that the field needs more of: we often take benchmark datasets for granted, and even though we know that they're flawed in various ways, we still use them to define SOTA. \n- Similarly, thinking about what sorts of text features and distributions work \"well\" with LLMs  (e.g., formatting a task as a natural language question rather than templating it in less natural ways) is often overlooked, even though it has significant impact on the accuracy.\n- The error analysis section in the appendix is good-- much more useful for future work than just presenting a raw accuracy number.\n\nWeaknesses:\n- My main concern is that I'm not sure this is enough for a full paper. This is a useful analysis and prompt engineering strategy, but I would expect either a deeper analysis of *why* formulating things as Q/A works so much better (e.g., analysis of the training data), or \n- I'm confused about the question() prompt. I thought these contained \"task-agnostic examples of how to transform statements to various questions\", but it seems like the format is not the same for all tasks? (Appendix H)",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and seems to be reproducible given that the authors release the prompts and code. See Strengths and Weaknesses section for comments on novelty.",
            "summary_of_the_review": "At a high level, this paper is starting to push back against the idea that LLMs are agnostic to prompt structure (ie, that prompt structure is a unimportant implementation detail), which is an important step for the field. That being said, it feels a bit more like an analysis of single prompt engineering strategy than a full paper, and for that reason, I am on the fence for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5912/Reviewer_XWv2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5912/Reviewer_XWv2"
        ]
    }
]