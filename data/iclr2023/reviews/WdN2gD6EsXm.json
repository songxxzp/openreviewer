[
    {
        "id": "yKCfp_eelE",
        "original": null,
        "number": 1,
        "cdate": 1666550204410,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666550204410,
        "tmdate": 1666551154613,
        "tddate": null,
        "forum": "WdN2gD6EsXm",
        "replyto": "WdN2gD6EsXm",
        "invitation": "ICLR.cc/2023/Conference/Paper2064/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed an upper bound to overlap index to measure the distributional distance between two distributions in a distribution-free setting, i.e. when we are only given access to finite samples from the underlying distributions. The authors first present a theoretical result to approximate the accuracy of a model with their upper bound on the overlap index.  Second, the authors employ this bound to study the distribution membership of given examples. Experimental results on out-of-distribution and novelty detection show that the proposed bound outperform existing heuristics on average in the AUROC metric. ",
            "strength_and_weaknesses": "**Strengths** \n-  The paper presents an interesting yet simple upper bound of the overlap index. The upper bound is distribution-free and can be computed only with samples. \n-  An interesting application of the bound on one-class classification is presented in the paper.  Experimental evidence for novelty detection, out-of-distribution detection, and backdoor detection shows the efficacy of the method. Moreover, the proposed one-way classifier does not need to train any parameters. \n\n\n\n**Weaknesses** \n- In Theorem 2, authors present an approximation to the accuracy of a model under distribution shift. However, it is unclear if this bound is practically useful. Moreover, there are no experiments highlighting the efficacy or practicality of the bound. Specifically, this may happen because we can not compute the term q in the bound. \n- Abstract and introduction of the paper should be toned down with respect to the above contribution. In particular, it should be made clear that the application to accuracy estimation is only of theoretical interest.  \n- Several crucial experimental details and design choices are missing in the paper. Please refer to the response below for details. \n",
            "clarity,_quality,_novelty_and_reproducibility": "In addition to several weaknesses highlighted above, it would be useful for a reader for reproduciblity and clarity if the following points could be clearly discussed in the paper: \n\n- More discussion should be added on choices of the conditional function g. Currently, authors use indication function $I(||x|| < r)$. Can authors present some motivation behind this choice and other choices that can be worth exploring in the future? \n- In the abstract authors mention that their one-way classifier \"requires only a small number of in-class samples to be accurate.\" However, this argument is not substantiated with any theoretical or concrete empirical justification. In my opinion, this statement should be toned down to precisely reflect the experimental finding. Moreover, in general, empirical experiments can not show that the proposed bound requires only a small number of samples to be accurate. \n- More details about the baseline methods compared in Section 4.4 should be included in the paper. Are there hyperparameter choices in the baseline methods that impact the final results? If yes, how did the authors choose those hyperparameters in their comparison?\n- For high-dimensional benchmark datasets, authors use WideResNets to extract features that I assume to act as the input $x$. Can authors provide more details on how this Resnet is trained? Moreover, the experimental setup for Section 4.4.2 is missing. More details should be added to justify how the comparisons made in Tables 2 and 3 are fair with other baseline methods. \n\n**Other Writing suggestion**:\n- Language in Theorem 2 should be significantly improved. In its current form, it is very confusing. While it is not hard to infer the real meaning using parenthesis like in \"If a model is trained on D with p accuracy on D (including the overlap area between D and D\u2217)\" doesn't make it clear that p is the accuracy on D which is restricted to the overlap region. It would be great if authors can formally define these quantities. Moreover, the last phrase \"is upper bounded due to (14)\" is unclear. What is this quantity upper bound on? Is it the overlap index?\n- (Very minor) For Definitions 1, 2, and 3, authors may consider defining P and Q distribution in the text before these definitions to avoid redundancy\n\n\nI believe that all the suggestions made above can be taken into account during the rebuttal phase, and I look forward to the author's response and will be happy to change my score if these questions are answered satisfactorily. ",
            "summary_of_the_review": "Overall, the paper presents an interesting yet simple upper bound of the overlap index. The upper bound is distribution-free and can be computed only with samples. An interesting application for one-class classification is discussed in the paper. However, the section with the application of accuracy estimation under distribution shift is a bit skim, and currently, it is unclear if the proposed upper bound on the overlap index can be leveraged in any non-trivial manner. Moreover, the paper lacks some crucial experimental and baseline details.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2064/Reviewer_yR91"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2064/Reviewer_yR91"
        ]
    },
    {
        "id": "faA_habTGgJ",
        "original": null,
        "number": 2,
        "cdate": 1666671834915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671834915,
        "tmdate": 1666671834915,
        "tddate": null,
        "forum": "WdN2gD6EsXm",
        "replyto": "WdN2gD6EsXm",
        "invitation": "ICLR.cc/2023/Conference/Paper2064/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors define an upper bound for the overlap index and then by converting it into confidence score they can train a one-class classifier. Since the bound is estimated on unknown distribution then the classifier is also distribution free. ",
            "strength_and_weaknesses": "Strength: \nInteresting idea\nWeakness:\nMinor improvement in results AUPR is not better than others or not that much increased. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and well structured",
            "summary_of_the_review": "Given the comments, I vote for borderline reject. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2064/Reviewer_S8uA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2064/Reviewer_S8uA"
        ]
    },
    {
        "id": "qClidicIT2",
        "original": null,
        "number": 3,
        "cdate": 1667400203296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667400203296,
        "tmdate": 1667400203296,
        "tddate": null,
        "forum": "WdN2gD6EsXm",
        "replyto": "WdN2gD6EsXm",
        "invitation": "ICLR.cc/2023/Conference/Paper2064/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method to compute the distance between two distributions. \nThe authors show how this can be used for novelty detection, out-of-distribution classification, and the like. The authors present results primarily focusing on these tasks, and compare them with traditional novelty detection methods.",
            "strength_and_weaknesses": "Strengths\n- The paper is nicely written\n\nWeakness\n- The results on novelty class detection are not convincing. Traditional methods perform almost similarly to the proposed method\n- The performance for out-of-distribution detection is also not convincing. There is no clear winner, and the proposed method is only good on half of the datasets. Similar comment for Table 3.\n- The authors do not compare with Deep One-Class Classification, which is a relevant work.\n- Figure 3 is not well presented. The captions should be self-explanatory.\n- Overall it is not clear what is the utility of this work compared to other works in the literature, as it hardly introduces a new concept, findings, or better performance. \n \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "At a high level, the algorithm for Compute Bound is very similar to other methods of distribution distance which uses statistics of distribution to compute the distance between them. I would suggest the authors compare it theoretically with other methods, where it can work, and where it will fail.\n\nWhile the authors talk about how their method can be useful for distribution shifts, there are no experiments in the paper regarding that. How shall we validate that Theorem 2 actually holds in practice?\n\nWhen we say the distance between two distributions, isn't it supposed to be at the input space itself? when we compute the distance on the extracted feature space, does that really allow for a good distance metric? because then the distance is dependent on the model. \n",
            "summary_of_the_review": "The paper tries to attack an important problem of measuring the distance between two distributions. However, the technique that the paper proposes is quite similar to the statistics-based method for distribution distance computation. The results section of the paper is missing a comparison with some commonly used techniques, and the proposed method does not stand out as the clear winner in any of the settings. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2064/Reviewer_NVdx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2064/Reviewer_NVdx"
        ]
    }
]