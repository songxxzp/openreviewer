[
    {
        "id": "pMH0ZGg8eK",
        "original": null,
        "number": 1,
        "cdate": 1666003560663,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666003560663,
        "tmdate": 1666003560663,
        "tddate": null,
        "forum": "sWOsRj4nT1n",
        "replyto": "sWOsRj4nT1n",
        "invitation": "ICLR.cc/2023/Conference/Paper3056/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the issue of temporal drift. The proposed methodology consists of a recurrent neural network modelling the parameters of the classification/regression model, assuming constant topology of this later. This recurrent model aims to encapsulate the latent change in parameters through time. The parameters of the network at a new time point are then drawn from this latent state that encodes the concept drift. The model is compared to multiple strategies on a set of both synthetic and real datasets for both regression and classification tasks.\n",
            "strength_and_weaknesses": "The paper approaches the important problem of temporal concept drift with an elegant solution. The draft is quite dense but remains clear. The experimentation is thorough with useful ablation studies and interesting discussions and limitations.\n\nThe theoretical results are interesting and show the importance of modelling the concept drift, but do not quite show why the proposed approach should outperform the other state-of-the-art strategies. Importantly, two key claims are unclear:\n- Why should the variance of the union of domains always be larger than the last domain? For instance, if each domain at time $t$ is a gaussian with mean $0$ and variance $t$ (the variance increases with time, meaning that the union has a lower variance than the last observed distribution)\n- Why does the posterior always have a lower variance than the prior? I think this claim only stands in expectation.\nFinally, the formalisation of the concept drift under which the methodology would perform better would be particularly valuable. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is dense but remains quite clear. The authors tackle an important problem with an intuitive and elegant novel solution.\n\nMiscellaneous:\n- Is \u201cIncfinetune\u201d the same as \u201cTemporal\u201d in 4c ? \n- Do you have an intuition why \u201cCIDA utilizes the adversarial training technique to solve the domain adaptation, yet the predicted decision boundary in Figure 4e is less stable than other state-of-the-art methods due to its model complexity.\u201d is not reflected in the variance of the performance in Table 1?\n- Table 2 line one does not correspond to the performance of IncFinetune but offline (so I think there is a typo somewhere)\n",
            "summary_of_the_review": "The paper studies an interesting problem and proposes a novel and efficient solution. However, the theoretical results do not strengthen considerably the paper and I am not sure of their correctness. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3056/Reviewer_NDvd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3056/Reviewer_NDvd"
        ]
    },
    {
        "id": "e2aLHIUVHI",
        "original": null,
        "number": 2,
        "cdate": 1666641009448,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641009448,
        "tmdate": 1666641009448,
        "tddate": null,
        "forum": "sWOsRj4nT1n",
        "replyto": "sWOsRj4nT1n",
        "invitation": "ICLR.cc/2023/Conference/Paper3056/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the problem of domain generalization where the domain index is across time. The authors proposed a Bayesian framework to explicitly model the concept drift over time, i.e., predicting the updated model on the future domain based on historical domains. A recurrent neural network is employed to learn the evolving dynamics of model parameter distribution based on how data distribution evolves across the domains. To achieve maximal expressiveness, the authors proposed to model the neural network as a dynamic graph which allows all connections between the neurons to adapt temporally. The authors also introduced a technique based on skip-connection to mitigate catastrophic forgetting. Theoretical analyses showed smaller margin of error and uncertainty in prediction of the proposed method compared with two baselines. The proposed method achieved excellent performances on several classification and regression datasets compared with several state-of-the-art domain generalization methods.",
            "strength_and_weaknesses": "Strengths:\n\n1.\tThe originality of this paper is great. The problem is clearly defined.\n\n2.\tThe topic of temporal domain generalization is interesting and prompt. \n\n3.\tThe proposed method is both theoretically and technically sound to me.\n\n4.\tHow the authors capture the temporal drift of model parameter distributions by considering the dynamic graph over a recurrent structure is intuitive. This paper makes a non-trivial exploration of the temporal domain generalization problem.\n\n5.\tExtensive theoretical and empirical results round up the good work.\n\nWeaknesses:\n\n1.\tCould the authors elaborate more on the reason of using skip-connection in Sec 3.2.4? In other words, why providing the LSTM unit with previous domain\u2019s information will mitigate the catastrophic forgetting on the future domain?\n\n2.\tSome details are not clear enough. For example, what is the shape of the parameter $\\omega$ into the recurrent unit? In addition, what initialization is used for the model parameters in this paper?\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n[originality] As far as I know, this paper delivered the first attempt to consider neural networks as dynamic graphs over a recurrent structure to capture the temporal drift of model parameter distributions. I found the proposed method interesting and intuitive, and believe it is non-trivial in tackling the problem of temporal domain generalization. \n\n[quality] I found the proposed method theoretically and empirically sound. Extensive theoretical analyses are shown for both prediction error and uncertainty quantification.  Experiment results are quite comprehensive and demonstrate the proposed method\u2019s effectiveness. Furthermore, the visualization of decision boundaries provided deep insights beyond the values in the tables.\n\n[clarity] In general, I found this paper well-written and easy to follow.\n\n[Reproducibility] Source code and formal proofs of this paper are provided by the authors. As far as I checked, I did not find mathematical mistakes in the proof.\n",
            "summary_of_the_review": "Overall, this paper makes a non-trivial and meaningful exploration of temporal domain generalization. Both theoretical and empirical results are provided with great performance. The paper is well-written and quite easy to follow. Hence, I would recommend accept of this paper. I am open to hearing from the authors if I made any misunderstandings.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I did not find any ethics concerns in this paper.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3056/Reviewer_WCLC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3056/Reviewer_WCLC"
        ]
    },
    {
        "id": "UD31XBrRH1",
        "original": null,
        "number": 3,
        "cdate": 1666799959859,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799959859,
        "tmdate": 1670836388020,
        "tddate": null,
        "forum": "sWOsRj4nT1n",
        "replyto": "sWOsRj4nT1n",
        "invitation": "ICLR.cc/2023/Conference/Paper3056/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The Paper studies temporal domain generalization with dynamic neural networks. \nFor a discrete set of datasets: D1, ..., D_T at different timestamps: t_1, ..., t_T, goal is to build a model that captures concept drift. \nIn particular, model learns how to predicts parameters of NN for timestamp T+1 for the mapping X_{T+1}-> T_{T+1}.\n\nAuthors, propose a Bayesian probabilistic framework, that is based on the law of total probability where integration over inference part P(w_{T+1}| w{1:T}, D{1:T}) and training part is done P(w{1:T}| D_{1:T}), in the end2end manner. \n\nThe authors provide two main theorems about uncertainty quantification and generalization error. \n\nAuthors, show on on 5 datasets for classification and 2 dtasets for regressions that they have very competitive performance w.r.t. baselines: offline, lastdomain, CDOT and others. \n\n\n\n",
            "strength_and_weaknesses": "Strength:\n- Interesting problem setting\n- Interesting approach with solid results.\n\nWeaknesses:\n- Paper does not provide enough information to understand the full details (e.g. see comments on integrals and proofs)\n- Proofs and theorems are not fully clear\n",
            "clarity,_quality,_novelty_and_reproducibility": "Few remarks on clarity:\n\n1. Authors use special type of integral, without fully describing what kind of integration they really do.\n.e.g. what they write in eq 4. is in standard mathematical literature known as Line/Contour integral.\nIf that is the case, which parametrization are you using? Please provide more details on the integration part. \n\n2. Can you provide empirical evidences that your Theorem 2 holds\n\n3. Regarding assumption 1 for the concept drift is Gaussian. Can you try to quantify this empirically?\n\n4. Proof 2\n- Eq 16 you add one more assumption on the expected L2 error for your model. Why is this assumption not in the statement of the Therem?\n- Can you clarify part on the mild assumptions for the Lipschitz continuity?\n\n",
            "summary_of_the_review": "Interesting paper but needs more clarity w.r.t. theoretical guarantees. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3056/Reviewer_vJXs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3056/Reviewer_vJXs"
        ]
    }
]