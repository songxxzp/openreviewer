[
    {
        "id": "4mCaBy6wXEz",
        "original": null,
        "number": 1,
        "cdate": 1666678540237,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678540237,
        "tmdate": 1666679493503,
        "tddate": null,
        "forum": "xIWfWvKM7aQ",
        "replyto": "xIWfWvKM7aQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5917/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a saliency based detector TextShield to identify adversarial samples in NLP. They use existing methods (vanilla gradient, guided backpropagation, layerwise relevance propagation, and integration gradient) to obtain saliency maps and then combine the outputs of all the methods to produce a final prediction of whether a sample is adversarial. The authors also introduce a corrector that corrects that modifies adversarial samples such that they are correctly classified. ",
            "strength_and_weaknesses": "Strengths:\n* The authors present a new method based on saliency maps. \n* On a few datasets and attacks, the authors show that they can successfully detect and defend against adversarial samples. \n\n\nWeaknesses:\n* Some baseline comparisons are missing, for instance \"Defense against Synonym Substitution-based Adversarial Attacks\nvia Dirichlet Neighborhood Ensemble\" (Zhou et al., 2021) present DNE. From looking at the numbers in the DNE paper, it also seems like some cases performance of DNE is better. \n* The authors mention that one of the weaknesses for adversarial training is that the robustness of the model is determined by the diversity of adversarial examples used in adversarial training and that you cannot defend against unknown attacks. Isn't the same thing true for TextShield as well because the detector needs to be trained with adversarial examples? Does TextShield generalize well to unknown attacks or to attacks that haven't been used during the training of the TextShield detector?\n* The authors state that \"there remains a significant gap between successfully detecting adversarial sentences and correctly classifying adversarial sentences for a specific application.\" This claim is confusing because defense methods are specifically designed for this. ",
            "clarity,_quality,_novelty_and_reproducibility": "There are a few grammatical errors and the writing can be improved. ",
            "summary_of_the_review": "I'm not entirely convinced about the empirical evaluation because there are a few missing baselines. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5917/Reviewer_Yn8V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5917/Reviewer_Yn8V"
        ]
    },
    {
        "id": "RSO4FPx0gC",
        "original": null,
        "number": 2,
        "cdate": 1666921862127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666921862127,
        "tmdate": 1666921862127,
        "tddate": null,
        "forum": "xIWfWvKM7aQ",
        "replyto": "xIWfWvKM7aQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5917/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a detection-correction paradigm to defend against word-level attacks on the text classification task.\nTheir adversarial sentence detector leverages adaptive word importance (AWI) based on saliency computation. If a sentence is detected as an adversarial one, the corrector will act on the sentence by refining the word with the highest saliency score. Then, the victim model takes the corrected sentence as input to make the final prediction. Thus, their model is like a\u00a0shield in front of the victim classifiers.",
            "strength_and_weaknesses": "Strengths:\n1. This paper is well written, with a clear description and exact formula.\n2. This paper aims to not only successfully detect adversarial sentences but also correctly classify adversarial sentences that previous attacking work rarely pays attention to. In order to enhance the robustness of victim models, previous work generally mixes the detected adversarial sentences and original training data to retrain the victim model. This work provides a new way to explore how to get correct predictions of those adversarial samples.\n3. The experimental results on adversarial sentences produced by four widely-used text attacks in the NLP field achieve comparable performance to previous defense methods.\n\nWeakness:\n1. Some attackers get adversarial sentences by adding an imperceptible perturbation into benign sentences. Ideally, the adversarial sentence's target remains the same as the benign one. But the perturbation process can not promise the unchangeable targets of the adversarial sentences. The automatic evaluation can only take the targets of the benign sentences as the true targets of the adversarial sentences, but maybe they are not. So it is better to add a human evaluation on the part of samples to reflect the precision of Table 1.\n2. This method is somewhat\u00a0opportunistic. By adding a \"shield\",\u00a0the\u00a0victim model could be more likely to make correct predictions of adversarial samples, but there still have been some concerns. The first one is the speed of inference. Calculating saliency should go through a forward and a backward process of the victim model. So, I wonder about the speed of the detector and the corrector. Another is that the detector is trained on the adversarial sentences generated by the\u00a0GA, IGA, PWWS, and TextFooler. The reported final experiment results are also on the adversarial sentences\u00a0generated by the\u00a0GA, IGA, PWWS, and TextFooler. So I wonder if the adversarial sample is \"out of domain\" (for example, train detector and test the\u00a0\"shield\" using two different \"styles\" attacker), the performance of the detector. Such a situation is more realistic because an attacker may generate adversarial samples with the same \"style\", which can not cover all potential adversarial sentences.\n3. The experiments have compared the saliency detector with previous detectors by adding the designed corrector to previous detector-based methods, with the results demonstrating the corrector's efficiency.\u00a0I'd like to know how much of a role the saliency played in the corrector. Specifically, how will the model behave if I do not choose the replaced word according to the saliency but something else, like some entities? It is better to have some case studies to explicitly show the saliency scores and their importance.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to understand. The authors have found a different way to defend against attacks.\u00a0",
            "summary_of_the_review": "This paper proposed an interesting way of defense by adding a\u00a0\"shield\" of the victim model. But there are some concerns like the inference speed, the generalization of the detector and the corrector. I will reconsider this paper if the authors can give some explanation and evidence of these problems.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5917/Reviewer_iR5J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5917/Reviewer_iR5J"
        ]
    },
    {
        "id": "e8eOSe348G-",
        "original": null,
        "number": 3,
        "cdate": 1666922033575,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666922033575,
        "tmdate": 1666922033575,
        "tddate": null,
        "forum": "xIWfWvKM7aQ",
        "replyto": "xIWfWvKM7aQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5917/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method called TextShield to handle a problem setup within the field of adversarial attacks, where the goal is to first detect an adversarial sentence and then correctly classify it. The system detects adversarial sentences using a saliency-based methodology and subsequently corrects them to benign ones. TextShield surpasses existing approaches on multiple metrics in experiments on standard NLP datasets.",
            "strength_and_weaknesses": "Strengths\n\n- The authors present strong motivation in Section 4 about the connection between saliency and robustness \n- The authors present a concise, clear description of adversarial text attacks and saliency computation.\n- I like that TextShield has both a detector and a corrector for adversarial sentences which seems to be novel\n- The proposed adaptive word importance (AWI) is interesting in that it is a simple, efficient method for identifying which words are the most important\n-TextShield outperforms existing methods in adversarial detection metrics on popular datasets\n\nWeaknesses\n- Table 1 could be better presented if TS had (ours) in front of it to make it easy to distinguish between the proposed method and the rest. For the rest of the methods, it would be nice to have citations in front of them to make it easy to identify where they come from. \n- Multiple runs of the experiments in Table 1 are missing as the variance based on the random seed would give us a good idea about whether the reported results are significant. It is not clear whether TS significantly outperforms others.\n- No code is provided to reproduce the results and get a deeper understanding of the control flow.\n- While this work doesn't seem entirely novel as the methods are based on popular computer vision algorithms related to saliency: the authors used saliency at the word level compared to the pixel level used in computer vision, which is very similar. However,  I like this work as it provides a simple yet efficient way of achieving strong performance compared to existing methods for adversarial detection and correction.",
            "clarity,_quality,_novelty_and_reproducibility": "please see the Strength And Weaknesses section.",
            "summary_of_the_review": "please see the Strength And Weaknesses section. While the method is not entirely novel, the method TextShield is an easy, efficient way to detect and correct adversarial attacks while achieving competitive results for NLP type of datasets.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5917/Reviewer_xPPy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5917/Reviewer_xPPy"
        ]
    },
    {
        "id": "M_uE36hBEG",
        "original": null,
        "number": 4,
        "cdate": 1667083210376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667083210376,
        "tmdate": 1667083210376,
        "tddate": null,
        "forum": "xIWfWvKM7aQ",
        "replyto": "xIWfWvKM7aQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5917/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a detection-based defence strategy called TextShield against word-level text attacks, incorporating a detector and a corrector. Four saliency computation methods, vanilla gradient (VG), guided backpropagation (GBP), layerwise relevance propagation (LRP), and integration gradient (IG), are selected as sub-detectors. Adaptive word importance (AWI) is designed using the vanilla gradient method. Then a saliency-based corrector aims to correct adversarial sentences to benign ones based on AWI. Comprehensive experiments demonstrate that TextShield is superior to previous defence methods on both generalization and robustness.",
            "strength_and_weaknesses": "Strengths:\n\n(1)\tThe illustration structure for intuitions and explanations for TextShield is stated clearly with the help of Figure 1-2. \n\n(2)\tThe experimental part has conducted comprehensive ablation study on considering the impacts of the hyperparameter, multiple sub-detectors and the number of adversarial examples.\n\n\n\nWeaknesses:\n\n(1)\tOverclaim in Section 2 (Related Work): \"However, because of the extreme time cost in the training stage, certified robustness is difficult to be applied to complex models. For example, IBP performed catastrophically on large pre-trained language models (e.g., BERT)\". Certified robustness of recent work can be applied to complex models like BERT [1]. \n\n(2)\tIn Subsection 5.3 (Corrector): \"For a suspect w_{i}, the corrector substitutes w_{i} with the most frequent word w^{'} \u2208 S(w_{i}), where S(w_{i}) is w_{i} 's synonyms [4]\". Here the synonym candidate set is selected from NLTK toolkit. However, the adoption of synonym sets is different in research on adversarial attacks, and the defence method assumes that the defenders are aware of the strategy for adversaries to create synonyms. Because we cannot limit a malicious attacker's use of synonym tables, this scenario is not realistic.\n\n(3)\tIn the experimental part, TL, PWWS, GA and IGA are selected as word-level attacks for generating adversarial examples. However, some SoTA attacks like TextFooler [2], and Bert-Attack [3] are not considered as baselines to test the effectiveness of the proposed defence method.\n\n(4)\tThe computation runtime and efficiency of the proposed defence strategy are not discussed in the experimental part.\n\n\nTypos:\n\n(1)\t\"Given sentence X with label y_{j} predicted by text classifier F, the adaptive word importance (AWI) of word w_{i} in the sentence, R_{ij}, \u2026\" in Subsection 4.2 lacks articles. Such a problem also appears in several places in this paper.\n\n(2)\t\"Takings VG as an example\" in Subsection 5.2 (Detector).\n\n\nReference:\n\n[1] Ye, Mao, Chengyue Gong, and Qiang Liu. \"SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions.\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.\n\n[2] Jin, Di, et al. \"Is bert really robust? a strong baseline for natural language attack on text classification and entailment.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 05. 2020.\n\n[3] Li, Linyang, et al. \"BERT-ATTACK: Adversarial Attack Against BERT Using BERT.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.\n\n[4] Loper, Edward, and Steven Bird. \"Nltk: The natural language toolkit.\" arXiv preprint cs/0205028 (2002).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Pls see Sec. Strength And Weaknesses\n",
            "summary_of_the_review": "Pls see Sec. Strength And Weaknesses\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5917/Reviewer_V9s1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5917/Reviewer_V9s1"
        ]
    }
]