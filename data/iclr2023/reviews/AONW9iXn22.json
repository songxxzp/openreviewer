[
    {
        "id": "cTZ8hzw8Xbp",
        "original": null,
        "number": 1,
        "cdate": 1666520639382,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666520639382,
        "tmdate": 1666520639382,
        "tddate": null,
        "forum": "AONW9iXn22",
        "replyto": "AONW9iXn22",
        "invitation": "ICLR.cc/2023/Conference/Paper1641/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes a learning and inference algorithm for deep Gaussian processes based on operator variational inference (OVI). \n\nParticularly, OVI (Ranganath et al., 2016, Hu et al. 2018, Grathwohl et al. 2020) generalizes Kernelized Stein Discrepancy (KSD) to situations where the RKHS space the smooth function (of the Langevin-Stein Operator) is replaced by the L2 space. Key to both OVI and KSD is the computation of the Stein Discrepancy (SD) which is parameterized by both the smooth operator and a surrogate of the true posterior that we wish to sample from. \n\nIn KSD, we can solve for SD in closed form which eases the optimization of the surrogate. In OVI, we parameterize the smooth function by a deep neural net and the SD can be estimated via sampling with re-parameterization & solving a minimax optimization task. Both OVI & SGD can be considered generic inference strategy for deep generative learning (including DGP). This paper extends OVI by also parameterizing the posterior with a neural net generator, which appears to be using the re-parameterization trick (Kingma & Welling, 2013). \n\nThis is demonstrated on deep Gaussian processes applied to 4 UCI regression datasets + 3 classification benchmark datasets (MNIST, Fashion-MNIST and CIFAR-10) showing marginal improvement over two baselines DSVI (Salimbeni & Deisenroth, 2017) and SGHMC (Havasi et al., 2018).",
            "strength_and_weaknesses": "Strengths:\n\n+ This paper is well-organized and has a good review section on important background of deep GP, Stein Discrepancy and SD-based inference techniques for generic deep Bayesian models such as KSD and OVI\n\n+ There are theoretical results characterizing the behavior of the optimal loss when the Stein Identity is satisfied; and an upper bound on the bias of the DGP predictor using the model learned via OVI.\n\nWeaknesses:\n\n- The contribution stance of this paper is somewhat problematic. It is in fact not clear to me what are the new algorithmic insights that the proposed algorithm (NOVI) provides in addition to what were found in OVI and KSD -- I will elaborate more on this in the Novelty section.\n\n- There is a lack of comparison with other relevant inference work for deep GP.\n\n- Despite the well-organized structure, this paper has quite a no. of typos -- some of which is quite problematic as it changes the semantic of the sentence into a contribution claim of an existing work -- more on this later.\n     ",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty & Quality:\n\nAs stated in the paper summary, my take of this paper is that its main contribution is a re-parameterization of the surrogate posterior q(.) in the formulation of OVI -- this is highlighted in Section 4.1 but this is exactly the same as the re-parameterization trick. \n\nMoreover, such re-parameterization has been indeed incorporated in the original KSD work of (Liu & Wang, 2016) -- see the text after Eq. (4) in https://arxiv.org/pdf/1608.04471.pdf\n\nAs such, NOVI is not a new variant of OVI but it is instead an application to DGP with the choice of re-parameterized surrogate put on the distribution of inducing inputs. \n\nI view this as an empirical report advocating for the use of OVI in DGP but even in this regard, this is still lacking in comparison with other related works. The field of DGP contains more than the two baselines the authors compare with. \n\nExperiments:\n\nFor example, deep GP learning & inference can also be made via expectation propagation -- https://arxiv.org/abs/1602.04133; the Bayesian treatment of the inducing inputs has also been explored -- see http://web.mit.edu/~jaillet/www/general/neurips2019_main.pdf\n\nFor the latter work, isn't the part below Eq. (3) the same as what the authors proposed in 4.1 (which is the only new content added to OVI) & shouldn't it be compared with given that it is more recent and also compared with the two baselines used in this paper on almost the same set of datasets?\n\nThere is also another work in the line of Stein Discrepancy: https://arxiv.org/pdf/2009.12141.pdf which is also much more recent.\n\nNote that I wouldn't request comparison with algorithms of other families such as the above if the core of this paper is about a new insight on expanding OVI (even if it is incremental) but in this case, it is a direct application of OVI to DGP and this entire work reads to me as an advocation of using OVI for learning DGP. This is also a fine angle but it has to be compared with other families of algorithms.\n\nSuggestion for Future Improvement:\n\nIn fact, given the core components of this paper, I'd suggest the authors to at least revise the stance of this paper towards an empirical comparison of inference algorithms for DGPs and analysis of their pros and cons from both theoretical & experimental perspectives. Then, even if there is no particularly new algorithmic component, the insights from such experiments would still be useful to the community which is meritable. Otherwise, presenting it as an extension of OVI is not convincing.\n\n--\n\nThere are also interesting theoretical facts established by this paper but they do not contribute to the algorithm designs. I believe these  are good-to-know facts but do not impact the algorithm design or shed important insights on its numerical behavior.   \n\nClarity:\n\nThe paper is clear on the high-level flow but there are a lot of typos, e.g. the 3rd last sentence of the abstract is only half a sentence; \"distrbution\" -- before Eq. (7); \"objictive\" -- 2nd sentence after Eq. (12)\n\nThere is also particularly problematic typo -- in 4.2 the authors stated that \"In Section 3, we have developed OVI\" which is not true given the cited reference in Section 3. An appropriate statement would be \"we have reviewed OVI\".\n\nFig 1. can be further improved in quality and clarity, e.g. annotations and labels could be added to make it clearer what the notations stand for (even though those are mentioned in the text narrative) -- the caption can also be more informative.\n\nCitations for the mean-field assumption should be mentioned as soon as the assumption was mentioned in the Introduction. Also, in the first paragraph of the Introduction, there is a statement about the constructed probability distribution (in GP) is far from posterior contraction -- please do consider citing references that explore & demonstrate this.\n\nBelow Eq. (11): it is not immediately clear why the authors mention the non-integrability of neural network. It seems to be a condition that needs to be enforced by including a regularizer on the space of neural network generators; so please make it clear the connection between this statement & the construction in Eq. (12) -- why such regularizer helps encourage the neural net generator to meet the L2 constraint.\n\nReproducibility:\n\nThe code is released and the presentation of the algorithm is clear so I believe the reproducibility of this work.",
            "summary_of_the_review": "Currently, my take of this paper is that it is an application of OVI to learning DGP. Despite the claim that it develops NOVI which is an extension of OVI via the neural net generator of the posterior in Section 4.1, this is not new. First, it is on high-level the same as re-parameterization which has been incorporated into KSD (a preliminary version of SD-based VI which OVI was built on). Second, even if we want to discuss it specifically in the context of stochastic inducing input, the same perspective has also been explored in a recent NeurIPS-19 paper -- http://web.mit.edu/~jaillet/www/general/neurips2019_main.pdf.\n\nGiven that no new algorithmic insight was introduced, I do not believe the current position of this paper merits an acceptance. But I do suggest another angle for this work (see my specific suggestions above) which is more meritable but that requires more extensive experiments, involving comparison with other families of inference algorithm for DGPs as well as detailed analysis of their modeling pros & cons (demonstrating explicitly deficiencies of existing approaches in the specific context of DGP; and showing how those could be remedied by using OVI). \n\n---\n\nThese are my preliminary assessment of this paper. I am of course happy to discuss if the authors disagree.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1641/Reviewer_GwnC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1641/Reviewer_GwnC"
        ]
    },
    {
        "id": "pqoL6Jy7FJJ",
        "original": null,
        "number": 2,
        "cdate": 1666634787505,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634787505,
        "tmdate": 1666634809318,
        "tddate": null,
        "forum": "AONW9iXn22",
        "replyto": "AONW9iXn22",
        "invitation": "ICLR.cc/2023/Conference/Paper1641/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a novel framework to conduct approximate variational inference in deep Gaussian processes. It is based on minimizing the regularized Stein discrepancy between the approximate distribution and the true posterior through the newly proposed Neural Operator Variational Inference algorithm for DGPs, which iteratively updates a generator and the associated discriminator to estimate more accurately the target discrepancy. This leads to a better final approximation than the ones obtained through the regular mean-field Gaussian assumption usually employed in DGPs. Authors support this claimg by extensive regression experiments, in which they show also that the method presents important properties such as robustness to overfitting. ",
            "strength_and_weaknesses": "# Strengths: \n\n* The method is an interesting addition to the literature in Deep Gaussian Processes, a promising line of research that has layed out very interesting results for the past few years. \n\n* The approach is fairly easy to understand. Even though there are important details that must be covered in detail, for the most part it is pretty straight-forward. \n\n* The authors provide extensive experiments, accounting also for the scalability of the method and its applicability in different problems and datasets. This strenghtens the submission, as well as the convergence guarantees and the extra information provided in the supplementary material.\n\n* The paper is mostly well written and easily understandable, covering the most important points on previous works and providing a good description of the method's development.  \n\n\n# Weaknesses:\n\n* My main concern here is the novelty of the method, which I deem to be a bit lacking. Overall, I see the contribution as interesting to the community, and also potentially very useful. However, as far as I understand it, this method results from the combination of previously well-known previous contributions in a very direct manner. I may be mistaken, and in that case I am entirely open to change my opinion on this matter, but as it is I think the contribution may lack sufficient novelty.\n\n* I also suggest the authors to emphasize in a stronger note the differences between this method and previously existing contributions. This may help with the previous point as well.\n\n* The UCI experiments could be much more robust if other metrics were reported asides from the RMSE. Common choices are the negative log-likelihood or the continously ranked probability score, as a proper scoring rule. I suggest the authors to include some other metrics that help better assess the quality of the predictive distribution, since RMSE does not necessarily provide that information.\n\n* I would appreciate a more extensive experiment with bigger datasets than Energy (e.g. Taxi, Airlines or others similar to these ones). Also, it would help if the authors shed some light on the expected performance of the method in cases with high dimensionality.\n\n* (_minor_) The presentation of the tabulated results in the supplementary material should be revised.\n\n* (_minor_) The \"Related Work\" section seems limited, since there are several other approximate inference methods strongly related to this one. As an example, the NOVI setup proposed in section 4.2  strongly ressembles the construct made in articles like [5,6] that deal with implicit distributions for approximate inference. Moreover, formulation in the equation of section 4.1 makes me wish there was a discussion between the relation between these methods and function-space inference with implicit stochastic processes, since the two approaches actually have a lot in common. As suggestions, I see strong relationship with works such as [1,2,3,4]. \n\n* (_minor_) Figure 1 should be re-thought, its quality leaves some improvement to be desired to help understanding the content of the graphical models. \n\n\n## Other comments: \n\n* The language employed in section 4.2 makes it seem sometimes that the authors contribute here to the development of previous techniques, e.g. OVI (Ranganath et al. 2016). I suggest the authors to be clarify the language here to state what contributions are genuinely theirs.  \n\n* Please, clarify further the statement before Eq. 14 regarding the fact that the two distributions must be \"equivalent\"\n\n\n\n## References: \n\n\n[1] Ma, C., Li, Y., and Hern\u00e1ndez-Lobato, J. M. (2019). \u201cVariational implicit processes\u201d.\nIn: International Conference on Machine Learning, pp. 4222\u20134233.\n\n[2] Sun, S., Zhang, G., Shi, J., and Grosse, R. (2019). \u201cFunctional variational Bayesian neural networks\u201d. In: International Conference on Learning Representations.\n\n[3] Rodr\u0131\u0301guez Santana, S., Zaldivar, B., & Hernandez-Lobato, D. (2022). Function-space Inference with Sparse Implicit Processes. In International Conference on Machine Learning (pp. 18723-18740). PMLR.\n \n[4] Ma, C., & Hern\u00e1ndez-Lobato, J. M. (2021). Functional variational inference based on stochastic process generators. Advances in Neural Information Processing Systems, 34, 21795-21807.\n\n[5] Mescheder, Lars, Sebastian Nowozin, and Andreas Geiger. \"Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks.\" International Conference on Machine Learning. PMLR, 2017.\n\n[6] Santana, S. R., & Hern\u00e1ndez-Lobato, D. (2022). Adversarial \u03b1-divergence minimization for Bayesian approximate inference. Neurocomputing, 471, 260-274.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n# Clarity\n\nThe paper is well-written, covering previous contributions and the derivations needed to understand the proposed method. \n\nPlease, check for typos. Some examples are: \n  * \"framwork\" (first contribution bullet point, page 2)\n  * \"objictive\" (three lines below Eq. 12)\n  * \"gemoetry\" (second line of Section 4.2)\n\n\n# Quality\n\nThe paper is of good quality, although some details should be polished to make justice to the amount of work behind this contribution. More specifically, providing better plots and graphical models would help, as well as taking care of the mishaps of the tabulated data in the supplementary material.\n\n# Novelty\n\nThe ingredients for the method already exist before the proposal, so the contribution is mainly centered on a particular combination of these well-known concepts in a new manner that results beneficial. \n\n\n# Reproducibility\n\nCode is already publicly available and the text is clear. Without attempting to reproduce it myself, I deem the contribution reproducible.\n",
            "summary_of_the_review": "\nThe contribution seems correct and interesting to the research community both due to its implications in DGPs and VI. However, it may lack sufficient novelty to merit a higher score. The authors should make an effort to highlight in a stronger fashion how does this contribution differenciate from previous works. Some extensions to the current experimental setup could help the case here as well (it is fine already, but some simple additions would make it even stronger).      \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1641/Reviewer_vt6c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1641/Reviewer_vt6c"
        ]
    },
    {
        "id": "dhyx5wAHg76",
        "original": null,
        "number": 3,
        "cdate": 1666698382980,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698382980,
        "tmdate": 1666871717443,
        "tddate": null,
        "forum": "AONW9iXn22",
        "replyto": "AONW9iXn22",
        "invitation": "ICLR.cc/2023/Conference/Paper1641/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new method for inference of the intractable posterior for deep Gaussian processes (DGPs). The idea is to minimize the Stein discrepancy between the approximated posterior distribution which is demonstrated by a generator and the true posterior distribution. The algorithm utilizes a discriminator to estimate the discrepancy between two distributions. It is claimed that a better approximation to the true posterior is recovered. To support the claim, extensive regression and classification experiments are conducted. ",
            "strength_and_weaknesses": "In fact, this paper demonstrates a high resemblance to Implicit Posterior Variational Inference for Deep Gaussian Processes [1]. The difference is that this work decides to minimize the Stein discrepancy while the work of [1] minimizes KL divergence. The design of the generator and discriminator, the iterative optimization procedure, and even the design to represent the inducing variables. \n\nThese two methods are not even compared in this paper, given all these resemblances. From my point of view, this comparison is compulsory. I am happy to raise my score if the authors can provide convincing reasons. \n\nIn the context of the NOVI algorithm, this paper is still far from satisfactory. The convergence guarantee is provided in Theorem 2 and Theorem 3 and yet no analysis or illustrations to prove this theorem. How would this method perform given a known true posterior distribution? This kind of synthetic experiment will make the claims more convincing than RMSE or classification accuracy. \n\n\n[1] https://arxiv.org/abs/1910.11998",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity: The paper is well-written and relatively easy to follow.\nQuality & originality: The contribution of this paper is relatively incremental\nReproducibility: The code is provided with the paper, I believe the reproducibility of this work.",
            "summary_of_the_review": "Given that the novelty and contribution of this paper are relatively incremental, I am inclined to reject this paper. Moreover, I would like to suggest the authors polish this paper in the direction of comparison of different inference methods for DGPs, the pros, and cons for each. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable\n",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1641/Reviewer_C7ZF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1641/Reviewer_C7ZF"
        ]
    },
    {
        "id": "WZ0d25XCo4L",
        "original": null,
        "number": 4,
        "cdate": 1666744007096,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666744007096,
        "tmdate": 1666744007096,
        "tddate": null,
        "forum": "AONW9iXn22",
        "replyto": "AONW9iXn22",
        "invitation": "ICLR.cc/2023/Conference/Paper1641/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method for approximating the posterior of deep gaussian processes, to allow for non-Gaussian posteriors.  The main idea is to train a generator network that approximates the posterior, while training a discriminator that learns a regularized version of the Stein discrepancy.  They also optimize hyperparameters and inducing point locations via SGD.  The authors also prove some theorems in the (unrealistic, but still interesting) regime of having infinitely expressive neural networks than can be perfectly optimized.",
            "strength_and_weaknesses": "Strengths:\n* The approach is interesting, and the ability to learn complex posteriors in the DGP framework is promising.\n\nWeaknesses:\n* (minor) there are a number of typos throughout the manuscript. This did not interfere with my understanding, but the paper could use some thorough copy eding.   For example there are numerous citation typos (particularly missing spaces before citations in the text).  Also, e.g., \"In particular, according to Bayesian formula\" --> \"In particular, according to Bayes' Rule\";  \"a quick introduction to these concepts that forms the foundation of our method\" --> \"a quick introduction to these concepts that form the foundation of our method\"; and so on.\n* The theorems assume an infinitely expressive neural network architecture, but the role of network architecture on the empirical results is relatively unexplored (e.g., only number of layers, but not number of hidden units, convolutional architectures, etc...)\n* In table 1 in the appendix, why does the runtime increase only slightly with the number of inducing points?  If the runtime increases so slightly and accuracy increases substantially, then why not use substantially more inducing points.  E.g., spending an additional 0.011s (presumably per iteration?) on Concrete to go from 50 to 400 inducing points, but going from an RMSE of 0.28 to 0.19 seems like a very good tradeoff.  Why not go on to more inducing points?  When does accuracy plateau or when does the runtime become infeasible?\n* In equation (16), a non-random quantity on the left hand side is claimed to be equal to a random quantity on the right hand side.  This equality does not even hold in expectation (i.e., the gradient will be biased) as can be seen by applying Jensen's inequality to Equation (6) in the appendix.  And the authors are certainly aware that this is not a strict equality given the $\\approx$ in Equation (6) in the supplement.  As such, I think that calling this a \"theorem\" with a strict equality is somewhat disingenous. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents an interesting, novel approach.  The writing and exposition could certainly be made clearer.  Theorem 1 is presented a theorem when in reality it is more like a proposed scheme for approximating gradients.",
            "summary_of_the_review": "The paper presents an interesting approach, but the key assumptions of its theorems are not met in practice (infinitely expressive neural networks, perfect optimization).  The impact of neural network architecture in particular, is not sufficiently explored.  I also found some parts of the paper to be presented in a way to appear rigorous, but are actually sort of pseudo-rigorous, at the expense of both clarity and accuracy (e.g., Theorem 1; repeated claims that neural networks with a finite number of hidden units are expressive enough to model any distribution).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1641/Reviewer_wts5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1641/Reviewer_wts5"
        ]
    }
]