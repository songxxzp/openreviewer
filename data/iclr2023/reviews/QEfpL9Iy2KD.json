[
    {
        "id": "xfpoRn1YQj",
        "original": null,
        "number": 1,
        "cdate": 1666101417811,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666101417811,
        "tmdate": 1666258180230,
        "tddate": null,
        "forum": "QEfpL9Iy2KD",
        "replyto": "QEfpL9Iy2KD",
        "invitation": "ICLR.cc/2023/Conference/Paper740/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper points out problems with traditional metrics for evaluating conditional natural language generation,\nand proposes a novel paradigm for multi-candidate evaluation,\nas they are not appropriate for domains such as visual description or summarization where are semantically diverse.\nIn this work, authors introduce metrics to measure deviations between samples from candidate and reference distributions.\nThey are designed to leverage already strong pairwise tools and alleviate the sensitivity issues induced by introducing simple augmentations to existing pairwise metrics.\nAuthors demonstrate that existing metrics for semantic similarity can be seamlessly extended to this framework and their paradigm can capture nuances through a case study of visual description.\n",
            "strength_and_weaknesses": "Strength\\\n*Issues are clear and convincing:  An issue in the evaluation of conditional text generation models from multiple sampled texts.\\\n*Proposals for new metrics are important and challenging:  Reuse existing text semantic distance functions and increase the sensitivity of these measures when faced with multiple candidates and references, rather than rebuild the entire set of distributional measures.\\\n*Prior research is sufficient: Discuss the advantage and disadvantages of existing metrics and extend the existing pairwise metrics.\n\nWeaknesses\\\n*Term definition: For example, many distribution  appear to appear many times in different ways (density or not) many times (e.g., Figure1 and KERNEL-BASED METRICS), but how is this term defined in this paper?\\\n*Leaning a bit too empirically: Sensitivity to data, reference text, especially the ground-truth and task, is likely to change the evaluation of the indicator, although the Table 3 and its discussion.\nCould you apply this framework to other tasks such summarization or information extraction?\\\n*No human judgments: While metrics such as ROUGE have been pointed out for their proximity to human judgment, the reason why there is no human judgment in the proposed metrics based on traditional metrics is weak.\nCan you explain the case study?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality, and Novelty is enough.\nReproducibility of the indicator is high, but experimental reproducibility appears to be not so high due to data dependence.",
            "summary_of_the_review": "This paper proposes a novel paradigm for multi-candidate evaluation of conditional language generation models.\nIt is designed to leverage already strong pairwise tools and alleviate the sensitivity issues induced by introducing simple augmentations to existing pairwise metrics.\nAuthors propose both Kernel-Based Metrics (KBMs) and Triangle-Rank Metrics (TRMs) as the basis framework,\ncompare them in their benefits,\nand that existing metrics for semantic similarity can be seamlessly extended to this framework.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper740/Reviewer_URmg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper740/Reviewer_URmg"
        ]
    },
    {
        "id": "jQlbBZbSCk",
        "original": null,
        "number": 2,
        "cdate": 1666299475402,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666299475402,
        "tmdate": 1668862117163,
        "tddate": null,
        "forum": "QEfpL9Iy2KD",
        "replyto": "QEfpL9Iy2KD",
        "invitation": "ICLR.cc/2023/Conference/Paper740/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper argues that current metrics relying on a single candidate text and a single reference are inappropriate to evaluate the quality of conditional language generation models and proposes two families of metrics to evaluate models in a multi-candidate, multi-reference setting: triangle-rank metrics (TRM) and kernel-based metrics (KBM). The experimental results show that both are better than naive aggregation (over multiple candidates) at discriminating between the model and the ground truth distributions.",
            "strength_and_weaknesses": "Strengths:\n- The motivation of the paper is interesting, especially for the problem of generating visual descriptions, which is studied in the experimental section.\n- Although the novelty of KBMs is incremental, they are appropriate for the problem in-hands.\n- The experimental section is appropriate. In particular, the experiment with VLP vs. CLIPCap is particularly enlightening.\n\nWeaknesses:\n1. The authors should elaborate more on the intuition of the proposed TRMs (eq (4)). It is clear that a triangle where all edges have the same length will produce a score of 0, but apparently $Q(C,R)$ cannot discriminate between a situation where $I_0 = 1$ and a situation where $I_2 = 1$. Intuitively, a model that can generate a candidate that is closer to the two references than the two references are of each other ($I_0=1$) is better than another model where the candidate is far apart from the references ($I_2=1$). In addition, isn't always $\\mathcal{I}(C,R) = |T|$?\n2. The authors motivate the need for having multiple candidates by the importance of evaluating diversity in the generated samples. However, it is not clear either how the proposed TRMs take diversity into account.\n3. It is unclear what hypotheses testing method is being employed throughout the experimental section to compare if the two distributions are the same. The authors should state clearly which kind of test they have adopted.\n\nTypos:\nEq. (4) - It should be $t_i$ instead of $T_i$ inside the summations.\nSection 5, Paragraph about perplexity -\" On the other hand, van den Oord & Dambre (2015) demonstrates that even in situations where the perplexity is *low*, models may not generate high-quality test samples.\"",
            "clarity,_quality,_novelty_and_reproducibility": "The major problem of the paper is the very laconic intuition provided about its major contribution TRMs. In addition, the paper is in general well written and the discussion of the presented problem and experimental results are adequate. Apart from the proposed TRMs, the novelty of the paper is limited.",
            "summary_of_the_review": "Although some findings of the paper are interesting, its novel contribution is rather small and poorly explained, so at the moment I feel it is more appropriate to recommend rejection of the paper.\n\n**Update after rebuttal:**\nThe authors have greatly improved the explanation of the proposed TRM metric and now everything makes much more sense. The proposed method is simple and not technically groundbreaking, but the results show that it can be effective. Its applicability is limited to settings where multiple references are available, though. For all these reasons, I decided to increase my score slightly. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper740/Reviewer_ZEw9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper740/Reviewer_ZEw9"
        ]
    },
    {
        "id": "YHWFwrm8e3l",
        "original": null,
        "number": 3,
        "cdate": 1667051677732,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667051677732,
        "tmdate": 1667051677732,
        "tddate": null,
        "forum": "QEfpL9Iy2KD",
        "replyto": "QEfpL9Iy2KD",
        "invitation": "ICLR.cc/2023/Conference/Paper740/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents new diversity and quality evaluation mechanisms for natural language generation models, especially multiple candidates and multiple reference items. The main idea of the methods is making triangles; those nodes are sentences in the candidate set, and the reference set and edges are existing semantic similarity methods such as BLEU and ROUGE. And the authors assume that two sentences in the same set are closed rather than another sentence. The authors also suggest kernel metrics by using pre-trained large language models to compute the similarity between sentences like a BERTScore. The authors choose to generate descriptions from visual datasets as a case study since the datasets have multiple references for a given data. To show the performance of metrics, the authors suggest using a p-value that shows the probability of rejecting the null hypothesis - samples from the candidate set and reference set are drawn from the same distribution. Experiments show that suggested methods capture the diversity of candidates when the number of candidates are increased and the temperature is changed by showing the decreasing p-value.",
            "strength_and_weaknesses": "The main strength of this paper is suggesting an interesting idea to evaluate generated texts, especially measuring the diversity of candidates and the quality of them together. I like the idea - the triangle-rank approach since it is a meta-approach that can use other evaluation metrics. And the idea is also simple, so other researchers can adopt and extend the idea for their own research work.\n\nWhat concerns me most is the lack of experiments on human evaluation. Yes, the authors mentioned that comparing with human evaluations is hard and I agree with that. Human hard to figure out the diversity among items, especially sentences. Even though it is hard to do experiments, it would be better to show the human correlation since this paper suggests new evaluation methods. One way is inserting a wrong candidate with well-generated candidates and asking what the wrong sentence is to humans and suggested methods. Human annotators find it hard to answer the score of the diversity of items, but they easily figure out the weird item [1, 2]. I assume that the authors did a similar experiment in Table 3, but I am not sure.\n\nAnd I have more questions about this paper. I hope to listen to the author's response and discuss how to improve the paper.\n\n- I am not sure if I understand the meaning of the results (p-value) correctly. The p-value is the probability of chance to see the test statistics if the null hypothesis is true. So, intuitively, we can reject the null hypothesis with a low p-value (i.e., 0.01). Here, the null hypothesis is that samples from the candidate set and reference set are drawn from the same distribution. Then, using BLEU@4 on the MSR-VTT dataset (Table 1), we can say that one ground-truth sentence (Human) has a higher probability that supports the null hypothesis than sentences from machine learning models (TVT and O2NA).\n- The authors insist that existing single-ground truth comparison is not sensitive enough. That\u2019s because existing metrics are not good? Or is it the limitation of single-ground truth comparison? If the first assumption is correct, what are the ideal values in Table 1?\n- Can the suggested method consider the recall in terms of reference samples? In Figure 1, there are no model samples for left-bottom ground truth reference. Then, the recall value is low because of the non-existence.\n- What is BERT in Table 1? BERTScore?\n\n[1] Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. \"Reading tea leaves: How humans interpret topic models.\" Advances in neural information processing systems 22 (2009).\n[2] Li, Margaret, Jason Weston, and Stephen Roller. \"Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons.\" arXiv preprint arXiv:1909.03087 (2019).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Originality is good, but I still have some questions about the experiment settings and results.\n",
            "summary_of_the_review": "I would like to discuss this paper with the authors to make the final decision.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper740/Reviewer_niHx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper740/Reviewer_niHx"
        ]
    }
]