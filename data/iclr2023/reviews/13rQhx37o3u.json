[
    {
        "id": "nwPc1tG3Od",
        "original": null,
        "number": 1,
        "cdate": 1666589711088,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589711088,
        "tmdate": 1666589711088,
        "tddate": null,
        "forum": "13rQhx37o3u",
        "replyto": "13rQhx37o3u",
        "invitation": "ICLR.cc/2023/Conference/Paper2304/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes DeepTime, a deep time-index model trained via meta-learning, with the motivations for stronger smoothness prior, avoiding the problem of covariate shift, and having better sample efficiency. Experimentation on various time series benchmarks are shown.",
            "strength_and_weaknesses": "Strengths\n\n- Forecasting as meta learning framework is an interesting and canonical idea. \n\n- Extensive ablation analyses\n\nWeaknesses\n\n- There is not sufficient explanation on what non-stationarity is. \n\n- The method is proposed for non-stationary time-series, but the experiments are not specifically obtained for non-stationary time-series data. The synthetic datasets are simple functions and most of the real-world benchmarks are not from applications where non-stationary dynamics are prominent, maybe except Exchange, on which the results are weaker. \n\n- It is unclear why Fourier features would be a good idea for non-stationary time series, as opposed to time-varying frequency representations. \n\n- There is no analysis on how much different datasets contain covariate or distribution shifts. \n\n- Some relevant baselines are skipped:\nhttps://arxiv.org/abs/2205.14415\nhttps://arxiv.org/abs/2205.13504\nhttps://arxiv.org/abs/1912.09363\n\n- For computational efficiency, the impact of parallelization is ignored.\n\n- The impact of lookback length is not sufficiently analyzed. \n\n- For a non-stationary dataset, training and validation distributions would have differences. How would it affect the hparam selection? This aspect is ignored. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper clarity is okay. The methods section could be improved. I suggest including the pseudocode for training and inference. \n\nE.g. for INR, \"We make use a of them as they are a natural fit for time-index models,\" - such motivations are not well supported. \n\nThe quality is mediocre. Some key aspects have issues as mentioned. \n\nThe idea is novel in time-series. \n\nReproducibility is concerning, as not all details are included. ",
            "summary_of_the_review": "Because of the issues above, mainly lack of convincing empirical validation appropriate for non-stationarity, the paper cannot be accepted in its current form in my opinion.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2304/Reviewer_comQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2304/Reviewer_comQ"
        ]
    },
    {
        "id": "5VC8O8bHPb9",
        "original": null,
        "number": 2,
        "cdate": 1666600822529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600822529,
        "tmdate": 1669068700279,
        "tddate": null,
        "forum": "13rQhx37o3u",
        "replyto": "13rQhx37o3u",
        "invitation": "ICLR.cc/2023/Conference/Paper2304/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new deep learning model for forecasting under non-stationary conditions \u2013 utilising a network that builds global representations of random Fourier features and projects it using meta-learning framework that recalibrates the output layer with each window.",
            "strength_and_weaknesses": "Strengths\n---\n\nAs neural networks notoriously underperform in the presence of covariate shifts \u2013 which are common in non-stationary time series datasets \u2013 the creation of methods that allow neural networks to adapt to changing regimes is extremely important. The use of meta-learning here is an interesting approach, and the out-performance of other state-of-the-art transformers makes it a promising method.\n\nWeaknesses\n---\n\nHowever, I do have several concerns regarding experimental evaluation, in particular:\n\n1.\tLack of hyperparameter optimisation \u2013 while the paper does show out-performance across benchmarks, we note that all the hyperparameters of DeepTime and benchmarks have been pre-specified based on \u201csuggested\u201d settings. Given that optimal hyperparameters can vary largely across time-series datasets \u2013 for example smaller models in data-limited regimes vs larger models where data is abundant \u2013 the lack of hyperparameter optimisation does beg the question of whether the performance differences here are due to improperly selected hyperparams and not model improvements.\n\n2.\tAre improvements due to differences in features used? \u2013 While the use of random Fourier features helps to make the time index range bound (which would otherwise introduce its own train-test mismatch), the final output of the INR would essentially be a set of non-linear seasonal features which repeat over time. Out-performance hence could be due to these seasonal relationships being more prevalent in the datasets used. As such, it would be good to see 1) how properly tuned transformer/tabular models perform when given the same random Fourier features + observations, and 2) whether a rolling linear regression on random Fourier features is sufficient for performance.\n\n3.\tDoes the meta learner handle non-stationarity? \u2013 while promising in theory, it is unclear whether there are in fact concept drifts in the time series datasets used for benchmarking. Tests on simulated data that incorporates regime changes over time could be useful here, with performance changes/decays across regimes documented for all models.\n",
            "clarity,_quality,_novelty_and_reproducibility": "On the whole, the paper was clearly presented and the motivations were very intuitive as well. A few questions remain, however, which would be very helpful with reproducibility.\n\nQuestions\n---\n1.\tIs any feature engineering applied to inputs for benchmarks?\n2.\tHow are lookback windows determined?\n3.\tHow much data was used for training in each case?\n",
            "summary_of_the_review": "I do think the idea and intuition behind the model is compelling, but would require improvements in experiments and benchmarks to be more convincing for me.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2304/Reviewer_3pqc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2304/Reviewer_3pqc"
        ]
    },
    {
        "id": "WWK33a1m0NA",
        "original": null,
        "number": 3,
        "cdate": 1666661873092,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661873092,
        "tmdate": 1666661873092,
        "tddate": null,
        "forum": "13rQhx37o3u",
        "replyto": "13rQhx37o3u",
        "invitation": "ICLR.cc/2023/Conference/Paper2304/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a meta-learning formulation for learning to adapt a deep time-index model to the look-back window. INR was the choice of time-index models, and meta-learning was formulated by using samples from look-back window as context set and forecasting horizon as query set. The goal of the meta-learning was mainly motivated for dealing with non-stationary time series. Experiments were conducted on synthetic as well as real datasets demonstrating performance improvement of forecasting over selected baselines.\n",
            "strength_and_weaknesses": "Strengths:\n\n1. Addressing non-stationary time-series by learning to adapt the time-series model is an important and interesting direction of research. \n2. The use of INR for the deep time-index model is interesting. \n3. The results demonstrated improvement over a good number of baselines used.\n\n\nWeakness:\n\n1. The presented work is heavily related to switching state-space models [1] and sequential neural processes [2]. In particular, it seems that it can be formulated as a special case of the SNP for learning y = f(t) where Bayesian meta-learning is learned to adapt f to observed frames from the lookback window for prediction in the forecasting horizon. Neither of these two works were discussed in the paper. Relations with these two works, and comparisons in terms of performance, should be provided. \n\n2. By formulating the meta-learning with optimization-based approaches, the model needs to be optimized to the query set before being used for forecasting. To mitigate this, the paper opted to restrict the optimization to only the last layer of the INR. The effect of this restriction should be demonstrated, empirically at least. In an ideal setting (where resource or time is not a constraint) where the full INR can be optimized to the lookback window each time, how does the performance look like, and how does it compare with the assumption of restricting the optimization to the last layer? Note that, with SNP, this was avoided by formulating the meta-learning with feedforward based approaches. This again stresses the need to compare with SNP.\n\n3. Details on meta-training is needed. It is not clear how the current tasks are defined -- by treating each pair of lookback window and forecasting horizon as a task, it means that the meta-training is looking at a large number of tasks and each task has only one set of context and query sets? Is the task boundary known assumed to be known ahead of time? If it is, it seems to be an unrealistic assumption. It it is not, this seems to be an unconventional setting of meta-learning -- Does the training follow the typical episodic training then? Further, how does the method apply if either the look-back window or the forecasting horizon falls within the transition of tasks boundaries? \n\n4. Experimental details are missing. It is not clear how the baseline models are trained. Since most of them do not use any meta-learning formulation, are they trained on the meta-training set? In that case, the comparison may not be fair as the presented model -- at meta-test time, actually is optimized to the look-back window (while the baseline models are simply applied to the look-back window without optimization). A fair comparison would be to fine-tune the baseline models (after training on the meta-training set) to the same samples (look-back window) used at meta-test time. If the argument is that these models cannot be fine-tuned this way (since they are history-value based models), at least in ablation study, the \"-RR\" version need to be fine-tuned at meta-test time.\n\n5. Since the look-back window represents context set in meta-learning, its size may have an important effect in \"optimizing\" the base model. The value of L in synthetic experiments seems to be large, and was not specified in real-data. Please add such details, and provide experimental evidence about the effect of the size of the context set L on meta-learning.\n\n6. While the paper was heavily motivated for better learning non-stationary time series, the experimental evaluation is limited in demonstrating how or whether the proposed solutions achieved the stated goal. In synthetic experiments, there lack details on how many tasks were used to meta-train the model and how many tasks were used in meta-testing. Since tasks/segments are generated with random sampling of the parameters, it'd be good to get a sense of the distance between the meta-train and meta-testing tasks. Finally, it was stated that \"A total of 400 time steps are sampled, with a lookback window length of 200 and forecast horizon of 200.\" In Appendix D, it was then stated that \"each function/task consists of 400 evenly spaced points\". So assuming each context-target set pairs are 400 time points, it was not clear how many such samples of length 400 were used. It was also not clear how does the forecasting work in such segment of 400. Does the model take 200 context samples and forecast for 200 context samples, and then it moves to the next window of 400? i.e., the task boundary is assumed to be known?\n\nIn real data experiments, it is understandable that the truth about the \"non-stationary\" nature of the data is not always available, but to the extent it's possible, it'd be desirable to say some level of analyses that link the model performance with the \"non-stationary\" nature of the underlying time series. \n\n\n\n7. The methodology is presented in a general fashion for forecasting m-dimensional observations. It'd be good to understand to what extend the value of m could be, i.e., what types of observations can be modeled by the presented method. Are we looking at multivariate data with relatively lower number of dimensions, or are we looking at image series?\n\n\n\n\n\n\n[1] Variational Learning for Switching State-Space Models, Zoubin Ghahramani and Geoffrey E. Hinton, Neural Computation 12, 831\u2013864 (2000)\n[2] Sequential Neural Processes, Gautam Singh, Jaesik Yoon, Youngsung Son, Sungjin Ahn, NeurIPS 2019\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing was overall clear and relatively easy to follow, although the methodology and experiment descriptions lack many critical details (as detailed above) that make it difficult to assess or reproduce the method. I have questions on the novelty of the work due to its unclear and not-discussed relation with switching systems and more importantly, SNP. \n\nCode is submitted and will be released for reproducibility.",
            "summary_of_the_review": "This paper tackles an important problem of time-series forecasting (i..e, non-stationary series) with an interesting solution (meta-learning of time-index models). The novelty however is unclear with respect to some existing works especially sequential neural processes. The choice or benefit of meta-learning method and the simplification of optimizing only the last layer of the INR, in comparison to alternative meta-learning method such as feed-forward model based method used in SNP that can bypass such simplifications, is not clear. The writing lacks many critical details on methodology and experiments, such as how are tasks handled during meta-training, whether the task boundary needs to be known, and how to address lookback window or forecasting horizon that includes task boundaries. It is also not clear how large a L is needed as the context set, and how baseline models utilized these context data at meta-test time. Overall it is an interesting idea, but can be improved in clarification of novelty and many methodological/experimental details. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2304/Reviewer_i4vc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2304/Reviewer_i4vc"
        ]
    },
    {
        "id": "ME-L0GQTkz3",
        "original": null,
        "number": 4,
        "cdate": 1667000869667,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667000869667,
        "tmdate": 1667000869667,
        "tddate": null,
        "forum": "13rQhx37o3u",
        "replyto": "13rQhx37o3u",
        "invitation": "ICLR.cc/2023/Conference/Paper2304/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work developed light-weighted ts modeling, time-index meta learning scheme for time series forecasting.  ",
            "strength_and_weaknesses": "## Strength\n- enhanced deep-time class of method via INR and \n- extensive multivariate real data experiments and ablation study\n\n\n## Weakness\n- clearly define what class of non-stationrity thei work target. It just generally mention covariate shift, conditional distribution shift, concept drift without formal definition. The figure 1 is not sufficient, except visuallizing locally-stationary (as high-level concept)\n- need to provide more direct evidence on 'whereas existing neural forecasting methods, which are historical-value models, are unable to take full advantage of this formulation,' in the introduction\n- need to clarify 'include having a stronger smoothness prior' in the abstract\n- In figure 2, need to specify the method of naive deep-time method. At the same time, usually, historical-value based model can capture such a seonsonalty well.\n- In section 2, what is the use of forecaster $h$?. the $\\mathcal{L}$ is poorly defined, seems to $R^m \\rightarrow R^m$ in eq (1) , which is different in problem formultation. \n- What is $\\Theta, \\Phi$? how do the author decide? is it depending on model types? any rule of thumb? it is too general and vague with too much degree of freedom and little intuition.\n- meta learning framework seesm to be nothing but a fitting some part of parameters like hyperaparemters in additional validation datasets. why this is specifically robust to non-stationary data?\n- why only applied for long-term and multivariate forecaster? what about medium-term forecasting and univariate (global) forecasters  like DeepAR, MQ-RNN, ConvTrans, etc.\n- lack of literature study on distribution shift, non-stationarity context. For example AdaRNN https://arxiv.org/abs/2108.04443",
            "clarity,_quality,_novelty_and_reproducibility": "Lack of clarity on motivation on why it is strong under non-strationtity and historical-based models are not sufficient. Poor mathematical description on problem formulation. Marginal novelty in chosing specific INR and hyperparameter for meta learning. only limited to multivarate modeling, not global modeling modeling even with its modeling capacity, where the baseline are known to be very poor.",
            "summary_of_the_review": "Need to better highlight the motivation on why Deep-time class is considered over historical based classes, and why meta learning helps with theoretical analysis or strong intuition. The writing can be enhanced significally especially in problem formulation. More solid experiments beyond multivariate modeling can be done to demonstrate the need of this method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2304/Reviewer_NCFZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2304/Reviewer_NCFZ"
        ]
    }
]