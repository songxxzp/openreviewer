[
    {
        "id": "kh6ulGm9NO",
        "original": null,
        "number": 1,
        "cdate": 1666642045072,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642045072,
        "tmdate": 1666642045072,
        "tddate": null,
        "forum": "ynD_LAMwar2",
        "replyto": "ynD_LAMwar2",
        "invitation": "ICLR.cc/2023/Conference/Paper5924/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work presents a method for identifying temporal logic rules.\nThis method uses reinforcement learning to select rules and separately optimizes the weights of these rules.\n\nThis method is evaluated on finding rules on benchmark tasks, in a transfer setting, and on a real-world problem.",
            "strength_and_weaknesses": "The method is an intuitive improvement over the existing enumeration approach.\nMy understanding of the method is that the selection of rules is done in a greedy way.\nAdditionally, further rules are not added when the next greedy rule is no longer useful.\nAs a result, this method is akin to performing prioritized enumeration along with a stopping condition.\n\nMy understanding is that the state only includes information about the tokens for the current rule (i.e., no information is included about existing rules).\nAs a result, the typical MDP properties do not hold in this setting.\nThe choice of an RL formulation is therefore a bit odd, but the sequential nature does make it a better fit than alternatives with which I am familiar.\n\nThis work seeks to also perform transfer. Transfer in RL is a non-trivial problem.\nThe assumption of \"same or similar ground truth rules\" substantially simplifies the problem. This assumption is akin to assuming similar optimal policies, at which point policy reuse (without adaptation) is expected to substantially improve early performance.\nBenefiting from past knowledge when able while not hurting performance in other cases is difficult. This aspect is stripped away in the performed transfer experiments as transfer is known to be helpful.\nAdditionally, this assumption cannot be made in real-world applications of this method as ground truth rules cannot be known prior to applying methods.\n\nSection 5.2 with human experts has experts looking at rules and determining if these rules seem reasonable.\nThis approach to evaluation is only considering one side: do doctors generally agree with the rules identified.\nThe other side is missing: do doctors disagree with the rules not selected by the system.\nSince the system is supposed to distinguish between useful and non-useful rules, this categorization should be evaluated by doctors.\nIdeally, this should be done close to the decision boundary, so comparisons should be made between the last included rules and the first ones not included (rather than comparing random rules to the top rules, for example).\nAdditionally, the goal of explanations is to provide trust in _performant_ systems.\nEvaluation can be repeated with rules generated by a faulty model, and doctors should be able to distinguish between the properly functioning and faulty models based on the produced explanations.",
            "clarity,_quality,_novelty_and_reproducibility": "This work notes that enumeration is intractable for long temporal logic rules.\nTractability of their methods was successfully demonstrated.\n\nSection 5.1 would have benefited from results for additional methods within the main body for comparison.\nLikewise, the baselines mentioned in 5.2 are not used for earlier comparisons.\n\nMinor comments:\n- Several times, the authors use a phrase akin to \"We make the temporal logic rule search subproblem differentiable.\" This is not entirely true. The RL policy is differentiable, but the search problem itself is not differentiable. Only the problem of finding logic rules _with this given policy_ is differentiable.\n- Parenthetical and non-parenthetical citations are each sometimes used when the other would be more appropriate. Another editing pass for this would be helpful.\n- 4.2.2: \"wihch\" -> \"which\"\n- 5.2: \"studies suggests\" -> \"studies suggest\"\n\nThe results in Table 1 (OURMETHOD) are prior to doctor intervention, right?\nWhat is the performance after the doctors have made modifications?",
            "summary_of_the_review": "The new method is motivated well and appears to perform better than existing alternatives.\nEvaluation could be improved, though. For example:\nFor the real-world study, by only considering the useful rules found by a successful system, the usefulness of explanations is not evaluated.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5924/Reviewer_Snuq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5924/Reviewer_Snuq"
        ]
    },
    {
        "id": "YDUjO-J7l-J",
        "original": null,
        "number": 2,
        "cdate": 1666673467846,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673467846,
        "tmdate": 1666673467846,
        "tddate": null,
        "forum": "ynD_LAMwar2",
        "replyto": "ynD_LAMwar2",
        "invitation": "ICLR.cc/2023/Conference/Paper5924/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to learn a set of temporal logic rules to build a probabilistic model that maximizes the likelihood of the observed data that come in the form of temporal event sequences. Compared to brute-force search in the space of logic rules that are enormous, the proposed approach leverages a neural policy to generate logic rules guided by heuristic reward signals to maximize the likelihood of the probabilistic model that can be obtained using the new rules. Empirically, the paper demonstrates that the proposed approach can improve upon prior baselines that utilize brute force for logic rule generations on a set of synthetic event data proposed by the authors. The proposed approach also outperforms prior methods on the MIMIC-III health care dataset by achieving lower prediction errors. ",
            "strength_and_weaknesses": "*Strength*\n\n- The idea of formulating the logic rule generation problem as a reinforcement learning problem is interesting. \n\n*Weaknesses*\n\n- The paper has a lot of incorrect statements and confusing sentences which make the technical content hard to understand. \n- The empirical section seems to only contain results generated using one single run of the algorithm, which makes it very difficult to judge the effectiveness of the proposed algorithm. \n- Many details of the baseline algorithms are missing in Table 1 (e.g., Transformer architecture, network size, how the data are fed into the transformer models) which could pose major reproducibility issues. ",
            "clarity,_quality,_novelty_and_reproducibility": "In general, I found that many statements in the paper are either inaccurate or poorly justified:\n- Section 1 - last paragraph above the contributions -- \"1) We make the temporal logic rule search subproblem differentiable\" -- I am not sure if this statement is accurate. The sentence seems to imply that there are gradients that can be computed through the search procedure, but I think only policy gradient is computed for optimization as far as what I understand.\n- Why does $w_f$ need to be positive in Equation (4, 5)? \n- Section 4.1 - \"For example, we can formulate $\\Omega(\\mathbf{w}) = \\lambda\\_0 \\sum\\_{f \\in \\bar{\\mathcal{F}}} c_f w_f$ where $c_f$ is the rule length\". I am confused by this regularization function. If $w_f$ is a vector of parameters and $c_f$ is a scalar, how can the multiplication results in a scalar?\n- Should the minimization be over $f$ rather than $\\phi\\_f$ in Equation (7)? It does not make much sense to minimize the logic-informed function as the function itself (from my understand) remains fixed (when conditioned on $f$). Although I could be misunderstanding it as it was not very clear in the setup whether this feature function is fixed. \n- Section 5.1 accuracy and scalability - \"1) whether our reinforcement temporal\nlogic learning algorithm can truly uncover the rules from the noisy variable set\" -- what is the noisy variable set referring to? How is the noise being introduced?\n\nThere are also many typos and grammar issues through out the paper that makes the paper hard to read and understand. To name a few:\n- Page 3 above Equation 1 -- \"By some simple proof Rasmussen (2018)\"\n- Page 4 Section 3.2 Temporal Logic Rule -- \"Add a temporal dimension to the predicates.\": incomplete sentence. \n- Page 4 Section 3.3 Temporal Logical Point Process -- \"Introduce a logic function $g_f(\\cdot)$ to check ...\": incomplete sentence. Also, is $g_f(\\cdot)$ a learnable function? What does it output? \n\nMany notations are not introduced properly:\n- What is $X_u(t_u)$ in Equation (2)? \n- How is $g_f(\\cdot)$ defined in Equation (3)? The paper mentions that it is a logic function that checks the body conditions of $f$, but this is very vague and it is unclear what the output of the function really is.\n",
            "summary_of_the_review": "In general, I found the paper to be a bit difficult to understand. I can roughly get the main idea of the propose method, but I might have misunderstood many technical details due to the writing issues. In addition, I found that the empirical evaluation of the paper to be a bit lacking and the details of some baselines are missing. These altogether make it hard to evaluate the significance of the results. Because of these reasons, I would not recommend acceptance of the paper at its current state. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5924/Reviewer_Z32A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5924/Reviewer_Z32A"
        ]
    },
    {
        "id": "mjIeOXWaau",
        "original": null,
        "number": 3,
        "cdate": 1666699446579,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699446579,
        "tmdate": 1666699446579,
        "tddate": null,
        "forum": "ynD_LAMwar2",
        "replyto": "ynD_LAMwar2",
        "invitation": "ICLR.cc/2023/Conference/Paper5924/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the problem of learning the intensity functions for temporal point processes (TPP) to model the occurrence of events in irregular time intervals. Specifically, the authors consider the representation of the intensity functions as parameterized by an exponentiated weighted sum of kernels switched by temporal logic rules. Given this parameterization, the overall objective is to find a set of rules and their corresponding weights to maximize the likelihood of data.\n\nTo optimize this objective, the authors break the MLE problem into two steps, a master step and a substep. In the master step, the weights are optimized via convex optimization. In the substep, the logic rules are selected. In order to optimize in the combinatorial space of logic rules, the authors formulate the problem as an RL problem, where the policy outputs a token of logic rules at a time and receives a reward when the sequence of tokens is completed. The authors then apply the risk seeking policy gradient method to solve this problem.\n\nThe paper includes empirical evaluations of the proposed method on synthetic and real datasets, and the results suggest that the proposed method achieves good likelihood and absolute error.\n",
            "strength_and_weaknesses": "Overall I think the paper presents an interesting idea of applying RL to solve combinatorial optimization problems in temporal logic rule learning. I\u2019m not familiar with the field of temporal logic point process (TLPP) and the related works in this field, so I will focus on the optimization method part in this review.\n\n### Pro\n\nThe paper is well presented and the proposed method is easy to follow. The authors include a detailed introduction about the preliminaries and make it easy for readers to understand the problem. Although I\u2019m not familiar with TLPP, I am able to quickly understand its formulation and the optimization problem associated with learning TLPP thanks to the high quality introduction of this paper.\n\n\nThe empirical results of the proposed method seem strong. The proposed method outperforms many baselines in real datasets.\n\n\n### Con\n\nWhile the problem of searching for logic rules can certainly be framed as an RL problem, I\u2019m not convinced that this is a good formulation, and I want to argue that formulating it as an RL problem here makes it unnecessarily more difficult. First of all, the problem has no inherent Markov state transition structure of an MDP because all tokens are directly generated by the agent, unlike in an MDP, where the states can only be influenced by the actions of the agent rather than directly chosen by the agent. Moreover, the reward here is the result of an entire sequence rather than the result of a single time step. Combining these two characteristics, I believe the problem is better formulated as a contextual bandit problem, and solved using a bandit algorithm instead of an RL algorithm. Using a bandit algorithm will directly remove the long horizon credit assignment difficulty of RL and likely result in much stabler training and better performance. \n\n\nGiven that the optimization problem is a standard back box optimization problem, it would be important to compare to many widely used back box optimizer baselines. Some examples of these algorithms include cross-entropy methods, genetic algorithms and simulated annealing methods. There are many well-developed software packages for these blackbox optimization algorithms that can be easily integrated into the specific problem, so it would be important to try them out and include them in baseline comparisons.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the proposed method is easy to follow. Because I\u2019m not familiar with the field of temporal logic point process, I cannot evaluate the novelty and reproducibility of this work.\n",
            "summary_of_the_review": "While this paper presents an interesting method of applying RL to optimize the log likelihood of TLPP, I\u2019m not really convinced that RL is the right approach and I believe that simpler approaches could work better for this problem. Hence, I cannot recommend acceptance of this paper in its current state. I highly encourage the authors to try applying bandit methods and including some black box optimization baselines during the author response phase.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5924/Reviewer_XGxF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5924/Reviewer_XGxF"
        ]
    },
    {
        "id": "ngNuUNivZj",
        "original": null,
        "number": 4,
        "cdate": 1667034546778,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667034546778,
        "tmdate": 1667034546778,
        "tddate": null,
        "forum": "ynD_LAMwar2",
        "replyto": "ynD_LAMwar2",
        "invitation": "ICLR.cc/2023/Conference/Paper5924/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presents a reinforcement temporal logic rule learning\nalgorithm to jointly learn temporal logic rules (before-like kind of\nrules) and their weights from event data. The proposed learning\nalgorithm alternates between a rule generator stage and a rule\nevaluator stage, where a neural search policy is learned by\nrisk-seeking gradient descent to discover new rules in the rule\ngenerator stage.",
            "strength_and_weaknesses": "The work is quite interesting and shows good results in the\napplication of predicting sepsis from the MIMIMC-III database by\ndistinguishing time predictions for LowUrine from NormalUrine rules. Various state of the\nart systems were compared against the method proposed in this work.\n\nI wonder if classical methods based on (Probabilistic) Inductive Logic\nProgramming wouldn't work as well as your proposed method. Why isn't a\nmethod like that compared in your work? Is it very different? TTE\nproblems may be also modeled using ILP and variants that are\nneurosymbolic.\n\nWhy did you use only one application? How well would your method work\nif you used other examples?\n\nWhy do you concentrate on grounded terms and not on first order logic terms with a logical variable?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: good\nReproducibility: authors give enough materials in the paper itself and in complementary material.  It may be possible to reproduce, but I haven't checked",
            "summary_of_the_review": "This work presents a reinforcement temporal logic rule learning\nalgorithm to jointly learn temporal logic rules (before-like kind of\nrules) and their weights from event data. The proposed learning\nalgorithm alternates between a rule generator stage and a rule\nevaluator stage, where a neural search policy is learned by\nrisk-seeking gradient descent to discover new rules in the rule\ngenerator stage.\n\nThe work is quite interesting and shows good results in the\napplication of predicting sepsis from the MIMIMC-III database by\ndistinguishing LowUrine from NormalUrine rules. Various state of the\nart systems were compared against the method proposed in this work.\n\nI wonder if classical methods based on (Probabilistic) Inductive Logic\nProgramming wouldn't work as well as your proposed method. Why isn't a\nmethod like that compared in your work? Is it very different? TTE\nproblems may be also modeled using ILP and variants that are\nneurosymbolic.\n\nWhy did you use only one application? How well would your method work\nif you used other examples?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5924/Reviewer_AuUW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5924/Reviewer_AuUW"
        ]
    }
]