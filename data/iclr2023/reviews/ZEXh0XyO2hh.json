[
    {
        "id": "7bk5Ad86ctS",
        "original": null,
        "number": 1,
        "cdate": 1666086009342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666086009342,
        "tmdate": 1668825079758,
        "tddate": null,
        "forum": "ZEXh0XyO2hh",
        "replyto": "ZEXh0XyO2hh",
        "invitation": "ICLR.cc/2023/Conference/Paper3792/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies long-tailed learning for binary networks. This is a novel setting. To address this, this paper proposes a new CANDLE method, which is based on three main strategies: (1) Calibrate and Distill, (2) adversarial learned balancing, (3) multi-resolution training. Promising empirical results demonstrate its effectiveness.",
            "strength_and_weaknesses": "Strengths:\n1. Learning binary networks on long-tailed data is a novel and practical task. Overall, I like this idea.\n2. The analysis is interesting, which shows the differences between learning binary networks and learning float-point networks on long-tailed data.\n3. Empirical results are encouraging. \n\nWeaknesses:\n1. Figure 2 finds that the classifier weight norms increase at the tail classes on both float-point networks and binary networks. However, this is contrary to several classic methods, like decoupling (Kang, ICLR 2020). They find classifier weight norms of float-point networks are positively related to long-tailed class distribution with standard training.  The authors explain that conventional FP methods use SGD and weight decay but binary networks use ADAM with zero weight decay. It makes sense, but it will be better to provide a baseline that binary networks use SGD and weight decay to demonstrate this.\n2. Moreover, it is interesting to know the size of feature space per class. In conventional FP networks, head classes usually have large feature space than tail classes. How about binary networks? \n3. Furthermore, many long-tailed approaches use the cosine classifier since its classifier weights are normalized. It is good to show how the proposed method performs with the cosine classifier.\n4. Why do you use MiSLAS to calibrate the classifier? Efficiency seems not a superiority of MiSLAS.\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: this paper is easy to understand.\nQuality: the quality is high but there are several questions that can be resolved.\nNovelty: the novelty is high.\nReproducibility: authors are expected to release the source code to the public.\n",
            "summary_of_the_review": "Despite a few questions that can be resolved, I like this paper as it explores a new and practical task, i.e., learning binary networks on long-tailed datasets, and provides a few insights. I tend to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3792/Reviewer_2EyL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3792/Reviewer_2EyL"
        ]
    },
    {
        "id": "7L3XmIZCBNc",
        "original": null,
        "number": 2,
        "cdate": 1666546127965,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666546127965,
        "tmdate": 1669398673514,
        "tddate": null,
        "forum": "ZEXh0XyO2hh",
        "replyto": "ZEXh0XyO2hh",
        "invitation": "ICLR.cc/2023/Conference/Paper3792/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents CANDLE, a binary-based network trained using a teacher-student framework in which a learned weight selects the largest loss between two loss terms in the optimization problem. In this way, CANDLE ensures the minimization of the maximum loss at every iteration. The motivation of using a binary network is to ensure a simple but efficient network that can be used in long-tail scenarios with efficiency in mind. Also, the paper adds a multi-resolution training schema in order to boost performance. The paper presents experiments on synthetically long-tailed and natural long-tail dataset demonstrating that the average accuracy improves considerably.",
            "strength_and_weaknesses": "Strengths:\n1. The motivation of the paper has a practical impact. Tackling long-tail scenarios is an important problem in real applications, and especially, considering the computational efficiency aspect is an important problem.\n2. The idea of introducing a learned weight (\\lambda) that controls the \u201cfocus\u201d between the two loss-terms in order to guarantee that the maximum loss at every iteration is minimized is interesting. I think the learning formulation in the teacher-student framework is a new idea.\n3. Overall, the clarity of the intuition and high-level ideas are explained well. However, there are some concerns I have with details and motivation; see the section below.\n\nWeaknesses:\n1. Assumptions are not well justified, and the paper is not self-contained: \n- The main assumption from the paper is that a neural-network-based image classifier suffers from performance issues due to the weight-norm variances. This already assumes that the networks use a linear-based classifier. However, previous work has shown that cosine-based classifiers also improve the performance of image classifiers in the long-tail scenario; see references A and B. In theory, these cosine-based classifiers should be more robust to the weight-norm variance because only angles are the main discriminative statistic.\n- The paper is not self-contained because it lacks a full discussion about the bad impact the weight-norm variance has when learning from long-tailed datasets. The reader has to dig in deeper and read extra to understand this claim. \n- Moreover, the narrative in the introduction projects to the reader that the main cause of the bad performance when learning from long-tailed datasets is the weight-norm variance. However, as shown in previous work (see references A and B for example), the lack of data from tail classes is the main cause of the problem. Thus, over sampling methods also alleviate the bad performance effect when learning from a long-tailed dataset. I think the introduction needs a revision in which it acknowledges previous work in a more structured and inclusive manner of previous work.\n\n2. The paper lacks clarity:\n- It is unclear from the introduction and Section 3.2 how the teacher network is pre-trained on non-LT data. What does this mean? Does it mean the teacher network used a balanced dataset (e.g., CIFAR 100), and then the teacher transfers its knowledge to the student network? I could not find a clear explanation about this.\n- In section 3.1, the paper states that it is believed that the \u201cmodel is likely to exhibit high [weight-norm] variance as it fits to the few training samples in the tail classes\u201d. I think this statement needs to be backed up with an experiment.\n\n3. Insufficient experiments: In the long-tail context, the mean average accuracy is not very informative. This is because it is unclear if the head classes are improving and thus moving the average accuracy metric up. I think a plot showing accuracy performance per class can be more informative (see Fig. 4 in A). It is unclear if the method favors more head or medium-shot classes. Note that this also applies to the ablation study.\n\n4. While CANDLE can improve efficiency during the inference stage due to the use of binary-based networks, the training of these networks is quite elaborate. In my (humble) opinion, this decreases the practical benefit as it requires training a teacher network first, then training a student network using CANDLE again.\n\nReferences:\n\nA. Kozerawski, et al.. BLT: Balancing Long-Tailed Datasets with Adversarially-Perturbed Images. ACCV 2020.\n\nB. Park, et al.. The Majority Can Help the Minority: Context-Rich Minority Oversampling for Long-Tailed Classification. CVPR 2022\n\n\n\n================= Post-Discussion =================\n\nAfter engaging in a discussion with the authors, I will downgrade my recommendation to 3 - Reject, not good enough. The reason for this is the following: the proposed approach is not practical. To me, the proposed approach has a fundamental problem. The proposed approach assumes that a teacher model (i.e., a non-LT model) exists and can be used to train a student model that will effectively deal w/ the long-tail dataset. In a realistic scenario and as an example, if I want to train a classifier using CANDLE and the Google Open Images dataset, which is a long-tailed one, it would be very hard to get a non-LT teacher model. Therefore, I cannot use CANDLE in large scale and real long-tailed dataset. Therefore, I question the approach as a viable solution to the long-tailed problem. Unfortunately, I could not get a satisfactory answer to this concern.",
            "clarity,_quality,_novelty_and_reproducibility": "\nI think the clarity can be improved, and consequently this can improve the reproducibility of the paper. However, I think the idea of using an adversarial learning framework is interesting. ",
            "summary_of_the_review": "Overall, I think the paper lacks clarity, better justifications of their algorithmic design choices, and more informative experiments showing the performance on head, medium-shot, and tail classes. However, I am intrigued about the adversarial learning framework applied to the teacher and student paradigm.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3792/Reviewer_ah6w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3792/Reviewer_ah6w"
        ]
    },
    {
        "id": "z7BmFnLgXN",
        "original": null,
        "number": 3,
        "cdate": 1666688338026,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688338026,
        "tmdate": 1666688338026,
        "tddate": null,
        "forum": "ZEXh0XyO2hh",
        "replyto": "ZEXh0XyO2hh",
        "invitation": "ICLR.cc/2023/Conference/Paper3792/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors proposed a binary network for long tailed data. They transfer knowledge from a calibrated teacher network to the binary network, and utilize adversarial learning for learning the hyperparameters. Experimental results demonstrate the effectiveness of their method.",
            "strength_and_weaknesses": "Strength:\nThe authors proposed a bianry network for long tailed data, which is interesting. The presentation is good, and easy to follow. \n\nWeaknesses:\n1. The authors claimed high variance in binary network can influence the performance of the model, and attempted to lower the high variance. Is there theoretical analysis on this? This seems to be claimed only based on some empirical observation. \n\n2. The proposed method utilized the traditional knowledge distillation for learning a binary network, leading to the limited novelty.\n\n3. In Figure 2, the authors claimed \"The classifier weight norms increase at the tail classes, implying high variance\". why?\n\n4. Why learn the balance bwtween two losses can be realized by an adversarial learning manner? It is expected to give more explanations.",
            "clarity,_quality,_novelty_and_reproducibility": "The authors are expected to give more clarity.",
            "summary_of_the_review": "I incline to reject this paper based on the current issues. However, I will be happy to change my score if the authors can solve my questions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3792/Reviewer_xZC3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3792/Reviewer_xZC3"
        ]
    },
    {
        "id": "GmWt2_0nHJ",
        "original": null,
        "number": 4,
        "cdate": 1666738621908,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666738621908,
        "tmdate": 1669653622987,
        "tddate": null,
        "forum": "ZEXh0XyO2hh",
        "replyto": "ZEXh0XyO2hh",
        "invitation": "ICLR.cc/2023/Conference/Paper3792/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the task of classifying data with long-tailed distributions of class samples under resource constraints. The resource constraints are addressed by using binary neural networks. The authors propose to use floating point networks pretrained on standard benchmarks and to retrain the classifier layer on the long-tailed data (here called _calibration_). The proposed objective is a linear combination of the loss of the binary network and the retrained floating point network, and the floating point and binary encoders. The weight of this linear combination is learned in an adversarial way, such that the part of the objective with a higher loss gets a higher weight. In addition, a mult-resolution module is added. The experiments indicate high accuracy of the proposed method in comparison to competitors on various datasets for which a long-tail class distribution has been created. ",
            "strength_and_weaknesses": "# Strengths\n* I like that the weight of the linear combination of losses is optimized\n* Figures and plots are used to illustrate various points\n* Dataset statistics are given in the appendix\n* The considered task of long-tail class distributions under resource constraints is interesting and possibly impactful for many applications\n# Weaknesses\n* The paper is not clear. Many terms are never properly defined, there is a lack of mathematical definitions, big chunks of the method are a black-box to me, I don't understand most Figures and the reasoning is also often unmotivated. \n* The writing is often repetitive in what the authors are going to do without actually explaining it, which makes for a frustrating read\n* The related work just lists related papers without actually pointing out how the existing work builds the fundament for benchmarks.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and reproducibility of the paper are very low and it's the main weakness.\nI am missing proper definitions of major relevant terms, such as\n* long-tailed data distribution\n* classifier weight norms \n* imbalance ratio\n* variance at classes\n* multi-resolution input\n* encoders\n* $Atten_\\phi$\n\nFurther, the issue of optimization is largely glossed over. The training of binary networks can be done in multiple ways and is non-trivial. Here, it's unclear how the binary weights are trained. Further, the authors propose a min-max optimization, which typically also requires some finesse. I read somewhere that Adam is used but I doubt that you can just through an optimizer on this objective and get decent results. How is the step size set? Some choices for the objective are unmotivated, for example, the choice for the cosine distance loss for the encoders. How are hyperparameters tuned? I didn't understand at all how the multi-resolution input is integrated and how this is handled in the training.\n\nI am not familiar enough with the field that I could assess the novelty. Quality is also quite unclear due to the clarity issue.",
            "summary_of_the_review": "Possibly a good paper that is not acceptable at this point in my view, because of its unclarity.\n\n# After Rebuttal Thoughts\nI see that the authors revised the paper carefully and that they provided more clarity regarding the terminology and methodology. However, in order to assess the revised version, I would have to re-read the whole paper. Unfortunately, I don't have time to do so, hence, I'll decerease my confidence score and give other reviewers the floor.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3792/Reviewer_F3k8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3792/Reviewer_F3k8"
        ]
    }
]