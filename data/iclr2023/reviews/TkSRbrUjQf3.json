[
    {
        "id": "QExUp6qmFQe",
        "original": null,
        "number": 1,
        "cdate": 1666528617886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666528617886,
        "tmdate": 1666528617886,
        "tddate": null,
        "forum": "TkSRbrUjQf3",
        "replyto": "TkSRbrUjQf3",
        "invitation": "ICLR.cc/2023/Conference/Paper2227/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies initialization method that is based on tree embedding for k-median clustering, with differential privacy guarantee. By initialization, it means a light-weight approximation algorithm that finds k initial center points, to be used with iterative approximation algorithms such as local search. Since it is only an initialization, the approximation ratio does not need to be heavily optimized, and it is the efficiency that is important.\n\nThis paper focuses on the general metric space case. The main technical idea is to impose a tree embedding, which is O(log n) distortion to the true distance, and find a set of representative centers directly on the tree. The overall running time is \\tilde{O}(nd), which is independent of k. The differential privacy can also be guaranteed, with a slight modification to the non-private version, by adding noise to some intermediate variables. The differentially private version, if combined with a previous work Gupta et al., can obtain a slightly improved additive error bound.\n\nThe experiments have been conducted to validate the performance of the new initialization method, including the widely-used k-means++. The new method demonstrates a better performance overall.\n",
            "strength_and_weaknesses": "# Strength:\n\nWhile there are many recent works on the topic, I find the general metric case less studied, and this paper fills in this gap which is timely. The fact that the running time is independent of k can be crucial for some applications, and this is something k-means++ method cannot achieve. The tree embedding method is easy-to-implement, and is generally applicable, which is an advantage. \n\n# Weakness:\n\nThe claimed improvement in ratio/error seems to be minor (for instance, log(\\min{\\Delta, k}) v.s. k-means++\u2019s log k, and a log n -> log log n improvement in the number of iterations of local search, where there is already/still a factor of k^2).\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity:\n\nThe clarify is fine overall, but I have the following detailed comments.\n\n1. Tree embedding in clustering-like problems have also been considered before, in e.g., \u201cFacility location problems in differential privacy model revisited\u201d, by Esencayi et al. Please add a comparison/discussion. \n2. Also, I find \u201cDifferentially Private Clustering: Tight Approximation Ratios\u201d by Ghazi et al. relevant but is not cited.\n3. At the end of page one, you mentioned \u201ck-median++\u201d. It\u2019s fine to call it \u201ck-median++\u201d, but it is actually \u201ck-means++\u201d, so one needs to clarify at least once, that k-median++ is the k-means++ adapted to the k-median case.\n4. It seems you use \\Delta as the diameter of the dataset. However, this makes sense only when you normalize the minimum distance between every pair of distinct point. Unfortunately, I didn\u2019t see this mentioned (I might have missed it).\n5. In the first bullet of page 2, you mentioned that \\Delta = O(d) is the typical case of bounded data \u2014 I don\u2019t agree, and in my opinion the bounded case should be \\Delta = poly(n).\n6. The definition of (2) is confusing, since the expression only replaces U with D. A suggestion is to avoid writing this (2) again, but simply say (1) with a differential privacy constraint is the DP k-median.\n7. In the experiments, your refer to \u201cleft\u201d and \u201cright\u201d column in Fig 2 and Fig 3. However, I only see one column. Maybe you can use sub-captions?\n\n# Quality:\n\nThe major concern is the experiments.\n\n1. The data is mostly from simulated sources, even though some of the simulation is based on real datasets. The suggestion is to experiment on more real datasets to make it more convincing. For instance, for the graph data, what about the road network data, such as OpenStreetMap? For Euclidean data, MNIST seems to be a small dataset, and experimenting a dataset of higher dimension and bigger size is helpful.\n\n2. The running time comparison, which is an important aspect for initialization/seeding algorithms, is not provided.\n\n# Originality:\n\nThe study of the general metric case is timely, but the techniques are somewhat standard, and I wouldn\u2019t consider the result particularly novel since it is an immediate application of tree embedding, especially provided that similar ideas have been used in differential privacy (see e.g., \u201cFacility location problems in differential privacy model revisited\u201d, by Esencayi et al.).",
            "summary_of_the_review": "I would suggest a weak reject because of the limited technical novelty, and result-wise, the improvement over existing methods does not seem to be very significant. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2227/Reviewer_2CeM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2227/Reviewer_2CeM"
        ]
    },
    {
        "id": "BnCjvWvGJx",
        "original": null,
        "number": 2,
        "cdate": 1666703613037,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703613037,
        "tmdate": 1666703613037,
        "tddate": null,
        "forum": "TkSRbrUjQf3",
        "replyto": "TkSRbrUjQf3",
        "invitation": "ICLR.cc/2023/Conference/Paper2227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work focuses on k-median clustering in metric space with privacy. The paper presents a new algorithm based on a HST and compares it with baselines in experiments. \n\n1- The author presents a k-median clustering initialization with $O(\\log \\min{k,d})$ approximation guarantee.\n2- They propose a differentially private algorithm with constant approximation guarantee and additive error $O(\\frac{1}{\\epsilon} \\Delta k^2 \\log^2 n)$.\n\nMoreover the authors provide experiments for different datasets.\n",
            "strength_and_weaknesses": "non-private algorithm:\n - Strength:  The algorithm is well explained and the selection of centers in novel (to the best of my knowledge).\n - Weaknesses:\n    - The approximation ratio of the algorithm is comparable with the HST based algorithm but significantly more than the best known algorithm.\n    - There are almost linear time algorithms that find the optimum solution on a HST, the algorithm proposed in this work is significantly weaker (e.g., Parallel and Efficient Hierarchical k-Median Clustering). These works are not mentioned and compared at all in this paper.\n    - The running time for metric space is not presented clearly, but it is presented for euclidean space.\n\nPrivate algorithm:\n - Strength: the algorithm improves the additive error by a factor $O(\\log n / \\log \\log n)$.\n - Weaknesses: the improvement in additive error is marginal given that there is more than a factor $k$ gap to the lower-bound.\n \nExperiments:\n - The experiments consider only small size instances.\n - Experiments lack fast and state of the art k-median algorithms.\n - Experiments does ignore greedy k-median++ which is known to outperform k-median++. \n - HST is slower than k-median++ even for very small datasets, and seems to be worse as they grow.\n - There are few datasets considered. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Most parts of the paper is well-written and the algorithm that solves the HST instance is novel.\n\nThe comparison with previous works is questionable in this paper. Most of the state of the art algorithms for k-median are not presented (for instance well-known publications based on LP roundings which result in the best approximation guarantee for k-median). Also more advanced algorithms are developed for solving an HST which is also not mentioned in this paper. Arthur and Vassilvitskii  paper in 2007 does not introduce k-median++ as well, it only focuses on k-means++.\n",
            "summary_of_the_review": "The algorithm is nice but it is not clear if it has any advantage in non-private settings and only a small factor improvement in the additive error in the differential private setting. This paper requires significant changes in the experimental section and comparison with previous work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2227/Reviewer_SXQm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2227/Reviewer_SXQm"
        ]
    },
    {
        "id": "vvEWGd689j",
        "original": null,
        "number": 3,
        "cdate": 1667032473159,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667032473159,
        "tmdate": 1667032473159,
        "tddate": null,
        "forum": "TkSRbrUjQf3",
        "replyto": "TkSRbrUjQf3",
        "invitation": "ICLR.cc/2023/Conference/Paper2227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides a seeding technique for the local search algorithm for k-median clustering in general metric spaces. Their algorithm is based on a tree embedding of the data. They also provide a version of their algorithm which can be used for differentially private $k$-median clustering.",
            "strength_and_weaknesses": "Strength: The paper is overall nicely written and studies important problems.\n\nWeaknesses: Although their algorithm is interesting academically, I don't find either their approximation bounds or their experimental results to be earth shattering.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Overall pretty clear. \n\nBut I encountered one irritating issue when reading the paper. The authors refer to $k$-medians++ as if it is a standard name for the technique they are using as the baseline. But AFAIK, this naming is very non-standard and I haven't seen such usage in the literature elsewhere. $k$-means++ gets its name because it is literally an augmented version of an algorithm which is referred to as the \"$k$-means\" algorithm. But the $k$-medians algorithm is not the local search algorithm, it refers to an algorithm similar to Lloyd's ($k$-means) but instead of computing the mean in the alternative minimization step, it computes the median of the group. See https://en.wikipedia.org/wiki/K-medians_clustering . Instead, I would refer to your baseline seeding algorithm as $D^1$-sampling following Wei (2016) : Wei, Dennis. \"A constant-factor bi-criteria approximation guarantee for k-means++.\" Advances in Neural Information Processing Systems 29 (2016).\n\nA Constant-Factor Bi-Criteria Approximation Guarantee for k-means++\n\nAnother issue was that it was a bit hard to compare the plots. They were too crowded. I suggest providing plots for the non-DP algorithms and the DP-algorithms separately in the appendix.\n\nQuality: The submission is technically sound. All claims are well-supported with proofs and detailed experiments.\n\nNovelty might be the Achilles' heel for this paper.\n\nReproducibility: Proofs are provided. The code is not provided but their algorithms and experiments are detailed enough that it would not be too hard to reproduce their results. But I encourage the authors to provide the code as well (or open-source it).\n\n----------------------------------------------\nTypos/minor issues:\n\nLine just before the inequality in the introduction: \u201ca clustering algorithms\u201d\n\nIn section 2.1, you claim that Arya et al. (2004) showed that cost(F) $\\leq$ 5OPT for the Algorithm 1 you describe. But that is only true when $\\alpha = 0$ i.e. if the final solution is a locally-optimal solution. There should be a term dependent on alpha. Something like cost(F) $\\leq 5(1+\\alpha)$ OPT but I am not entirely sure about the constant (might be $5(1+2\\alpha)$ or something like that). I wouldn't bother too much with this. You can just provide a note. If you are enthusiastic enough and want to compute the exact constant, I would Williamson-Shmoys chapter 9 over Arya et al. \n\nExperiments 5.1 \u201cDiscrete Euclidean space. Following previous work .,\u201d has ., ",
            "summary_of_the_review": "Clean and easy paper to accept, but nothing groundbreaking.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2227/Reviewer_RZVN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2227/Reviewer_RZVN"
        ]
    },
    {
        "id": "AL44CvkTXE",
        "original": null,
        "number": 4,
        "cdate": 1667592235226,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667592235226,
        "tmdate": 1667592235226,
        "tddate": null,
        "forum": "TkSRbrUjQf3",
        "replyto": "TkSRbrUjQf3",
        "invitation": "ICLR.cc/2023/Conference/Paper2227/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to seed Lloyd's iteration for computing k-means clustering, in the context of finite metric spaces, using the approximation the underlying metric by a convex combination of hierarchically separated tree metrics. This is claimed to achieve better approximation guarantees (for the seeding step) than current conventions.\n\nAlso, the same idea is used to generate differentially private seeding.",
            "strength_and_weaknesses": "The claims are somewhat dubious, because the paper introduces a new parameter (the diameter of the metric space). The bound becomes better if this parameter is small, but it is not clear that previous analyses of kmeans++ variants cannot be proven to do better if this parameter is taken into account. Moreover, part of the attractiveness of the older seeding schemes does not lie in their worst case performance, but rather in their excellent performance in stable instances, under various notions of stability.\n\nThe paper has some empirical evidence that their method is good. It's beyond my expertise to judge the empirical part.\n\nAlso, I cannot evaluate the significance of the differential privacy result.\n\nThe survey of relevant literature is lacking.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly.",
            "summary_of_the_review": "The paper needs to address the comparison with previous methods more seriously.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2227/Reviewer_it5w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2227/Reviewer_it5w"
        ]
    }
]