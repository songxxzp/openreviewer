[
    {
        "id": "oNWnW8crfPb",
        "original": null,
        "number": 1,
        "cdate": 1665975127156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665975127156,
        "tmdate": 1668684113012,
        "tddate": null,
        "forum": "FI5IysDR8pG",
        "replyto": "FI5IysDR8pG",
        "invitation": "ICLR.cc/2023/Conference/Paper1938/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents an interesting approach to improving DETR-based models across multiple tasks. The proposed approach mainly consists of the following three steps: \n- learn image-adaptive dynamic coefficients for each of the original grouped learned queries,\n- perform convex combination, i.e., aggregate, each group of learned queries according to coefficients, thus producing a small set of clustered queries, and\n- apply Hungarian matching over the clustered queries and the original queries.\n\nLast, the proposed approach also achieves relatively WEAK improvements across multiple benchmarks.\nIn general, the proposed approach seems to be capable of\n\n- modeling the co-occurring relations between different objects, thus I guess the proposed approach\nmainly benefits from explicitly modeling the interactions between different objects. However, the authors still use the original single-object distribution to\nact as the training signal instead of constructing a set of multi-object distributions, which makes the proposed approach less reasonable.\n\n- increasing more positive samples during matching. If my understanding is correct,\nanother key is that **the proposed approach also benefits from adding more positive samples as each matched modulated query essentially corresponds to r=4 original queries, thus bringing 4x more positive samples**, which is also verified in the recent approaches[1][2].\n\n[1] DETRs with Hybrid Matching, arXiv:2207.13080\n\n[2] Group-DETR: Fast DETR Training with Group-Wise One-to-Many Assignment, arXiv:2207.13085",
            "strength_and_weaknesses": "> Strengths\n\n\u2705 The presented idea is simple and easy to follow.\n\n\u2705 The overall writing is satisfying.\n\n\u2705 The proposed method can be used to improve performance across multiple detection and segmentation tasks.\n\n> Weaknesses\n\n\u274e According to Table 2, the proposed method only slightly (+0.8) improves Deformable-DETR but improves DAB-Deformable-DETR significantly (+1.6), which is very STRANGE.\nThe authors explain why. Besides, the authors fail to compare their approach with a very important baseline, i.e., DN-DETR[1]. In fact, according to\nthe official implementation[2] of DN-DETR, DN-DAB-Deformable-DETR already achieves 49.5 while the proposed approach achieves 49.7 based on DAB-Deformable-DETR.\nAlthough the proposed approach seems relatively different from DN-DETR/Group-DETR/HDETR, the authors are still encouraged to discuss the differences between the proposed approach with DN-DETR,\nGroup-DETR[3], and HDETR[4], which already verify that the key insight is to increase more positive samples during matching. If my understanding is correct,\nthe key is that **the proposed approach also benefits from adding more positive samples as each matched modulated query essentially corresponds to r=4 original queries, thus bringing 4x more positive samples**.\n\n[1] DN-DETR: Accelerate detr training by introducing query denoising, CVPR2022\n\n[2] https://github.com/IDEA-Research/DN-DETR\n\n[3] Group-DETR: Fast DETR Training with Group-Wise One-to-Many Assignment, arXiv:2207.13085\n\n[4] DETRs with Hybrid Matching, arXiv:2207.13080\n\n\u274e The experimental results are slightly WEAK and far from convincing as the authors only report improvements over the VERY weak results of Mask2Former and SeqFormer.\nFor example, the Mask2Former reports mAP=43.7/PQ=51.9 with R50 as the backbone while the authors only conduct experiments with a baseline of mAP=42.4/PQ=50.4 under a\nshort learning schedule. According to my experience, the proposed approach might become less useful under a longer training schedule. In summary, the experiments are WEAK and the authors should include more comparisons with results under the original learning schedule of Mask2Former or\nSeqFormer. Besides, the authors should also report the comparison results with stronger backbones such as Swin-L to verify whether the proposed method generalizes well. \n\n\u274e Figure 1 is misleading to some degree considering the overall GFLOPs of the transformer decoder are much smaller compared to the transformer encoder\nand backbone. In fact, the GFLOPs of the transformer encoder might be more than 10x larger if we take Deformable-DETR as an example. Therefore, the proposed\napproach only brings VERY slight efficiency improvements.\n\n\u274e Some important details are missing:\n\n- The authors should justify how to divide the basic queries into multiple groups. Especially, how to divide the basic queries seems non-trivial for a two-stage approach that selects the most confident queries to initialize the basic queries.\n\n- **The authors are required to clarify how many queries are used during training indeed to avoid misunderstanding.**   As mentioned by the other reviewers, the authors seem to try to hide some very important details.",
            "clarity,_quality,_novelty_and_reproducibility": "> Clarity\n\nGood.\n\n> Quality\n\nGood.\n\n> Novelty\n\nRelatively weak considering the recent HDETR and Group-DETR.\n\n> Reproducibility\n\nNo code is available.",
            "summary_of_the_review": "The authors' efforts on pushing the frontier of object detection and segmentation performance along the DETR path are highly encouraging.\nThe overall idea is interesting and the results are encouraging but relatively WEAK.\nPlease carefully address the above-listed weaknesses. I will increase the ratings if the authors could well address these concerns, especially to verify the proposed method over strong backbones under longer training schedules following Mask2Former and SeqFormer.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No further concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1938/Reviewer_ykWm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1938/Reviewer_ykWm"
        ]
    },
    {
        "id": "DxMA5y8kuM",
        "original": null,
        "number": 2,
        "cdate": 1666577647361,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577647361,
        "tmdate": 1667502614565,
        "tddate": null,
        "forum": "FI5IysDR8pG",
        "replyto": "FI5IysDR8pG",
        "invitation": "ICLR.cc/2023/Conference/Paper1938/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an attentioinal plug-and-play module, namely, Dynamic Query (DQ), to better generate object queries for set-based transformer detector and segmenter. The proposed method can consistently help improve different kinds of frameworks for both object detection, instance segmentation and panoptic segmentation.",
            "strength_and_weaknesses": "### Strength\n\n**1. The idea is simple and sweet, which should be easy to follow.** The proposed method is essentially an attentional plug-in, which should be able to be implemented in just a few lines of code. Such simplicity will attract people to follow.\n\n**2. The experiments are extensive and comprehensive.** The authors evaluate the proposed component into about 10 different frameworks for set-based object detection, instance and panoptic segmentation. \n\n### Weaknesses\n\n**1. The novelty is limited.** The proposed method is too similar to other attentional modules proposed in previous works [1, 2, 3]. The group attention design seems to be related to ResNeSt [4] but it is not discussed in the paper. Although these works did not evaluate their performance on object detection and instance segmentation, the overall structures between these modules and the one that this paper proposed are pretty similar.\n\n**2. Though the improvement is consistent for different frameworks and tasks, the relative gains are not very strong.** For most of the baselines, the proposed methods can only achieve just about 1% gain on a relative small backbone ResNet-50. As the proposed method introduces global pooling into its structure, it might be easy to improve a relatively small backbone since it is with a smaller receptive field. I suspect whether the proposed method still works well on large backbone models like Swin-B or Swin-L.\n\n**3. Some of the baseline results do not matched with their original paper.** I roughly checked the original Mask2former paper but the performance reported in this paper is much lower than the one reported in the original Mask2former paper. For example, for panoptic segmentation, Mask2former reported 51.9 but in this paper it's 50.4, and the AP for instance segmentation reported in the original paper is 43.7 but here what reported is 42.4.\n\nMeanwhile, there are some missing references about panoptic segmentation that should be included in this paper [5, 6].\n\n### Reference\n\n[1] Chen, Yunpeng, et al. \"A^ 2-nets: Double attention networks.\" NeurIPS 2018.\n\n[2] Cao, Yue, et al. \"Gcnet: Non-local networks meet squeeze-excitation networks and beyond.\" T-PAMI 2020\n\n[3] Yinpeng Chen, et al. Dynamic convolution: Attention over convolution kernels. CVPR 2020.\n\n[4] Zhang, Hang, et al. \"Resnest: Split-attention networks.\" CVPR workshop 2022.\n\n[5] Zhang, Wenwei, et al. \"K-net: Towards unified image segmentation.\" Advances in Neural Information Processing Systems 34 (2021): 10326-10338.\n\n[6] Wang, Huiyu, et al. \"Max-deeplab: End-to-end panoptic segmentation with mask transformers.\" CVPR 2021",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed above, the illustration of this paper is clear and the overall quality for writing is acceptable. However, the originality of this paper might be a bit limited and its reproducibility cannot be guaranteed since the authors do not promise to release the code publicly in the future.",
            "summary_of_the_review": "Most of my concerns lie in the novelty issue and also the non-impressive performance. So I am inclined to reject this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1938/Reviewer_QkjX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1938/Reviewer_QkjX"
        ]
    },
    {
        "id": "YfeelSdyrWS",
        "original": null,
        "number": 3,
        "cdate": 1666625139964,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625139964,
        "tmdate": 1668659559396,
        "tddate": null,
        "forum": "FI5IysDR8pG",
        "replyto": "FI5IysDR8pG",
        "invitation": "ICLR.cc/2023/Conference/Paper1938/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a general design improvement over models from the DETR family. Specifically, it introduces an additional set of *modulated queries* which are convex combinations of the regular queries. The weights used for the combination are predicted by a small MLP operating on the pooled representation of the image.\nThen:\n- At train time, both regular queries and modulated queries are trained, being fed independently to the decoder, and with a separate hungarian matching\n- At test time, only the modulated queries are used.\n\nThe paper demonstrates the applicability of the method on a wide range of DETR variants as well as tasks (detection, instance segmentation, panoptic segmentation)",
            "strength_and_weaknesses": "Overall, the paper is clearly motivated, and shows improvements for a variety of models and tasks, suggesting the general applicability of the method.\n\n## Non-modulated ablation\n\nTo me, the main weakness of the paper is that the ablations don't prove the usefulness of the *modulation* in itself.\nIn my opinion, here is the ablation that would demonstrate it convincingly:\nInstead of adding $n/r$ *modulated* queries to the model (as is currently done), one could add the same number of *basic* queries. The rest of the design would be the same: this additional set of queries would be fed separately to the decoder and a separate matching would be computed for it.\n\nThe reason why this ablation is important is because it is known (see [1]) that adding extra groups of queries at training time with a separate matching helps with performance. Thus, the ablation above is required to demonstrate that the gains indeed come from the modulation component as claimed, and not from the same effect uncovered by [1].\n\n\n\n[1] \"Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment\", Chen et al",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. The idea is novel as far as I know. I believe enough details are provided to allow reproduction.\n\n\nQuestions:\n## Inference speed\n\nLet's take an example to make sure I understand correctly. Let's say we start from a DeformableDETR model with 300 base queries, and we use r=4 as is done in the experiments. This will create 300/4 = 75 modulated queries. At train time, there are two forwards through the decoder, which naturally incurs a slightly higher cost. However, at inference time, only the modulated queries are used. So effectively the run-time is equivalent to a DeformableDETR with 75 queries plus the cost of computing the modulation coefficient. I would expect it to be faster because of the reduced computation in the decoder (quadratic self attention). However the paper seems to indicate that it's not really the case? Is it because computing the modulation coefficient is costly? Reporting some inference speeds would help here (eg Deformable DETR 300q vs Deformable DETR 75q vs DQ Deformable DETR 300q)\n\nTypos:\n- \"visualization examples are proviede\" (section 4)\n\n",
            "summary_of_the_review": "Without the result of the ablation I mentioned, it is not possible for me to conclude whether the reported improvements come from previously discovered phenomenon (ie using several groups of queries help performance) or from the claimed contribution of this paper.\nAs such, I recommend rejection for now, but depending on the result of the ablation I'm willing to increase my score.\n\n\nPOST REBUTTAL:\nAuthors have provided an ablation that better demonstrate their points. I'm increasing my score 5->8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1938/Reviewer_LvX2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1938/Reviewer_LvX2"
        ]
    },
    {
        "id": "e8NKM6V8MPm",
        "original": null,
        "number": 4,
        "cdate": 1666648385667,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648385667,
        "tmdate": 1669118331720,
        "tddate": null,
        "forum": "FI5IysDR8pG",
        "replyto": "FI5IysDR8pG",
        "invitation": "ICLR.cc/2023/Conference/Paper1938/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a learnable convex combination of the learned object queries. High-level semantics are used to generate dynamic coefficients for query combination. Experiments are performed on a wide range of detr-based methods. Consistent improvements are observed on coco, cityscapes and ytvis.",
            "strength_and_weaknesses": "strength:\n1. extensive experiments on various of tasks, showing effect of the proposed method.\n2. the idea of convex combination of object queries is interesting, and the paper is with good motivation.\n3. paper writing is good with clear structure. \n\n\nweakness:\n\n1. When dividing n queries into m groups, is the dividing process fully random? Is there better grouping strategy to discuss?\n2. What kind of objects will the modulated queries correspond to during training and testing? What kind of objects are being improved? Is there visualization on the dynamic attention weights to help explain the improvement? Is there explanation on why non-convex combination is better than convex one?\n3. The improvement in Table 3 and 4 is very limited.\n4. why mapping one query to one group is necessary? can one query correspond to multiple modulated queries?",
            "clarity,_quality,_novelty_and_reproducibility": "Good writing and interesting idea.",
            "summary_of_the_review": "Exploring the queries properties is meaningful; the experiment improvement shown on various datasets; the analysis can be further improved (see weakness).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1938/Reviewer_tEAE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1938/Reviewer_tEAE"
        ]
    }
]