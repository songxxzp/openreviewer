[
    {
        "id": "mkTZGFQJkX",
        "original": null,
        "number": 1,
        "cdate": 1666352136783,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666352136783,
        "tmdate": 1670913276408,
        "tddate": null,
        "forum": "d77RVuVg-Mf",
        "replyto": "d77RVuVg-Mf",
        "invitation": "ICLR.cc/2023/Conference/Paper2570/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes UniFormerV2, which could arm the readily available and well-pretrained image ViT with efficient Uniformer designs. Extensive experiments demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Pros:\n1. The idea of arming rich well-pretrained image ViTs with efficient Uniformer design for video performance improvement is novel and useful. \n2. The proposed method achieves state-of-the-art results on many popular video benchmarks.\n\nCons:\n1. Lack of analysis of limitations of the proposed method\n2. What is the video pretrain method used in Table 1? Is it supervised or self-supervised? And how many epochs does it take?\n3. Why not use video pretrain in Table 4 for SSV2? Is there any special consideration?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The idea of using pretrained image ViT to boost video transformers is novel",
            "summary_of_the_review": "Overall, this paper proposes an effective way of using well-pretrained image ViTs to facilitate the performance of video ViTs. The proposed method achieves state-of-the-art results on 8 popular video benchmarks. \n\n**After rebuttal**: I carefully read the discussion between Reviwer ikef and the authors. I agree that some contributions are overclaimed since they are not entirely new and bear some similarities with other architectures. As mentioned by AC, this may need a re-writing of the manuscript and a new round of review.  But I think this paper gives us some good insights and practices on designing competitive video backbones. I lean to weekly accept this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2570/Reviewer_GtFh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2570/Reviewer_GtFh"
        ]
    },
    {
        "id": "DUrnpPr-e9",
        "original": null,
        "number": 2,
        "cdate": 1666489881773,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666489881773,
        "tmdate": 1666489881773,
        "tddate": null,
        "forum": "d77RVuVg-Mf",
        "replyto": "d77RVuVg-Mf",
        "invitation": "ICLR.cc/2023/Conference/Paper2570/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new video transformer backbone, which is named UniFormerV2. Compared to the previous UniFormer, this paper improves the original local Multi-Head Relation Aggregator (MHRA) with one local MHRA temporal and one global MHRA spatial blocks. In which, the global MHRA spatial block can leverage the pretrained ViT model. Extensive experimental results demonstrate the new state-of-the-art performance in various video benchmarks.\n",
            "strength_and_weaknesses": "Strength:\n\nThis paper is well-written. The discussion of new architecture design and empirical study are mostly well done. The motivation is well justified and the proposed algorithm is easy to follow.\n \nThe major contribution of this paper can be summarized as:\n\n1. it decouple the previous local uniblock with one local MHRA temporal and one global MHRA spatial blocks, which not only reduce the local temporal redundancy, but also inherit the effective image pretraining of ViT architecture.\n\n2. This work provides extensive empirical study of multi-stage fusion methods, pre training pipelines, training cost and intermediate feature locations of UniFormer. This provides insights of how each component works in the UniFormer.\n\n3. This paper demonstrates strong performance in various video benchmarks, outperforming previous state-of-the-art performance. I believe the pre-trained models of this work are able to contribute to the community in various downstream video-based tasks.\n \nFrom Figure one, it can be seen that the UniFormerV2 achieves good accuracy-FLOPs balance. Compared to other methods, UniFormerV2 achieves better performance with lower cost.\n \nThe implementation details and ablation set-up are well introduced, which provides much convenience for other researchers to re-implement the proposed algorithm in this paper.\n\nWeakness & Questions:\n\nCompared to the Local UniBlock in the UniFormerV1, the current version looks like a hybrid block of MHRA and MHSA. It would be interesting to see the performance of swapping local MHRA temporal with the temporal MHSA. In this case, the Local UniBlock will be like a divided Space-time attention introduced in the TimsFormer[1]\n \nWhen leveraging the MHSA from image pretrained ViT, how to deal with the gap between ViT features and UniBlock features. The input to Global MHRA Spatial in UniFormer and the one to ViT MHSA should be very different.\n \nThanks authors for doing lots of experiments in various datasets. However, from the limited number of apple-to-apple comparisons, the improvement of UniFormerV2 is not significant.(for example compared to the MViTv2)\n\n[1] Is Space-Time Attention All You Need for Video Understanding? ICML, 2021",
            "clarity,_quality,_novelty_and_reproducibility": "This paper clearly demonstrates its motivation and proposed method with sufficient details for reproducibility. As an extension work of UniFormer, this paper demonstrates a couple of novel designs. ",
            "summary_of_the_review": "Overall, I would vote weakly accept for this paper. The main reasons are: 1. The new design of UniFormer is well introduced and verified. 2. Extensive experimental results in 8 popular video benchmarks demonstrate its effectiveness and will potentially contribute to the community by releasing the pre-trained model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2570/Reviewer_DTkU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2570/Reviewer_DTkU"
        ]
    },
    {
        "id": "EUOvpBRpqN",
        "original": null,
        "number": 3,
        "cdate": 1666545195888,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666545195888,
        "tmdate": 1670280592310,
        "tddate": null,
        "forum": "d77RVuVg-Mf",
        "replyto": "d77RVuVg-Mf",
        "invitation": "ICLR.cc/2023/Conference/Paper2570/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes a new improved architecture for video recognition based on Uniformer (ICLR 2022), dubbed Uniformer-V2. To this end some of the techniques proposed in Uniformer are re-used and extended to create a video architecture that can benefit from existing pre-trained ViT models (CLIP in particular). The architecture is  evaluated on a number of video/action recognition benchmarks where the method is shown to be among the best reported method.",
            "strength_and_weaknesses": "Strengths:\n+ CLIP adaptation methods is a reasonable direction for improving video recognition although the best models are really huge.\n+ The architecture is evaluated on a large number of benchmarks where the method is shown to be among the most accurate ones. \n\nWeaknesses:\n- The method proposed in this paper is primarily an adaptation of CLIP for video recognition and it should have been presented as such. On the contrary the authors have tried to present Uniformer-V2 as a \"generic paradigm to build a powerful family of video networks\". \n- The novelty of the proposed local and global uni-blocks seems thin: The local uni-block resembles a temporal depthwise convolution (can the authors make this clear as in the uniformer paper?) followed by a spatial transformer block as in the image encoder of CLIP. Hence, this block is basically a simple temporal adapter that is put on top of CLIP's ViT (something similar I believe has been proposed in Pan 2022). The global block resembles a perceiver block (ICML 2021) and/or DETR decoder (ECCV 2020) and in a similar fashion has been used in Flamingo (Neurips 2022). I think the authors need to discuss the similarities with these works.  \n- The experimental results are the paper's strongest point but they are not so impressive as they look from a first glance: (1) by using CLIP, there's a huge advantage over all other methods (which dont use CLIP), but that comes to no surprise. (2) When CLIP is not used, as in Table 1 (upper part), Uniformer-V2 is not better than Uniformer (which is trained on IN-1K). (3) When CLIP is used, the best results are reported by finetuning on their K710 (can you please clarify why this is needed? and explain in relation to table 9e) (4) On Something-Something V2, Uniformer outperforms Uniformer-V2 for the same comp. budget.\n\n**After rebuttal:**\nUnfortunately, the rebuttal does not address my concerns as also explained in my responses to the authors' responses. I still believe that the proposed blocks (local and global) must be described in a more clear manner to fully understand the differences to previous work. As it stands the proposed contributions seem thin. Moreover, as demonstrated in my review, the results are not so significant as claimed. For example on SS-V2, their method is outperformed by previous work and on K400 to surpass EVL their method needs additional pre-training.  Overall the paper shows that if CLIP is used one can achieve very high results on video recognition which is not a surprising result at all. Hence, I would like to keep my original score.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think clarity could have been improved if the authors chose to present this work as a CLIP adaptation for video recognition emphasizing the novel parts of their method. ",
            "summary_of_the_review": "The paper has some merits but it's confusingly written. Moreover, the contributions in my opinion are not so strong as claimed. Some of the experimental results appear strong, but a closer look reveals that, in many cases, they are not so impressive as might have been thought from a first glance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2570/Reviewer_ikef"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2570/Reviewer_ikef"
        ]
    },
    {
        "id": "thdtNte9zZA",
        "original": null,
        "number": 4,
        "cdate": 1666723633587,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723633587,
        "tmdate": 1670916724535,
        "tddate": null,
        "forum": "d77RVuVg-Mf",
        "replyto": "d77RVuVg-Mf",
        "invitation": "ICLR.cc/2023/Conference/Paper2570/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new video Transformer architecture, termed UniformerV2, that extends the pre-trained ViTs (on images) to the video action recognition tasks. Specifically, UniformerV2 introduces local temporal aggregation, global temporal modeling and multi-stage fusion to enhance the original image-based ViTs to capture temporal information. Although some of the modules are inspired by the design in Uniformer, the paper proposes better and more efficient design choices for local / global temporal aggregation, and presents extensive ablation studies to verify their effectiveness. The paper reports experiment results on multiple video recognition benchmarks and UniformerV2 achieves state-of-the-art results.",
            "strength_and_weaknesses": "Strengths\n1. The paper presents strong experimental results on multiple video recognition datasets, including those requires stronger capacity on temporal modeling (e.g., Something-something) and long-range modeling (e.g., ActivityNet). The paper also provides extensive ablation study to present the impact of each component of the model and different design choices. These design choices are good practices for video understanding research and could be valuable for the community.\n\n\nMinor questions:\n1. How is the 3D convolution for patch embedding initialized? Is it inflated from patch embedding weights in the ViTs? \n2. The four multi-stage fusion strategies share similar results in the ablation study. Which one would the author prefer to use in this case? Other than recognition accuracy, how about the comparison of computational cost / running time among them?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well organized and well written. The description of UniformerV2 is clear and easy to follow, and the model should be able to reproduced. The originality of the work is not that significant because many of the modules are not new (e.g., the affinity operation in local temporal aggregation, the learnable-query-based attention in global temporal attention, and the multi-scale fusion designs). Nevertheless, the contribution of this paper is that it provides detailed ablation to these design choices and verify the effectiveness of these modules.",
            "summary_of_the_review": "This paper proposes a good design choice for extending pretrained ViTs to video recognition task. The proposed method achieves superior results on multiple video recognition benchmarks and the effectiveness of the model is verified by extensive ablation study. In summary, I suggest \"accept\" to this paper.\n\n*After reviewers' discussion*: I agree with the other reviewers that the novelty of the proposed local & global block is not as significant as claimed in the paper. However, I do feel that this paper provides some good practice and insights for making use of pre-trained image-based ViTs for spatiotemporal modeling in video recognition. Although the individual modules are not novel, the whole system presented in this paper still differs from prior work on extending image ViTs to video modeling (e.g., ST-Adapter, X-CLIP) and achieves better results (or comparable results with better efficiency). By reconsidering the strength and weakness of this paper, I'd update my rating to \"weak accept\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2570/Reviewer_AXo7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2570/Reviewer_AXo7"
        ]
    }
]