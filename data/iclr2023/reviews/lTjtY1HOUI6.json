[
    {
        "id": "FV6ZAvyck1-",
        "original": null,
        "number": 1,
        "cdate": 1665918374004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665918374004,
        "tmdate": 1665918458386,
        "tddate": null,
        "forum": "lTjtY1HOUI6",
        "replyto": "lTjtY1HOUI6",
        "invitation": "ICLR.cc/2023/Conference/Paper3219/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes an adaptive parametric prototype learning approach for one specific setting of the cross-domain few-shot learning (CDFSL) classification. \n\nTechnical contributions: (1) the authors proposed the prototype calculator network (PCN) to learn new prototypes for the support set with meta-training; (2) a weighted-moving-average (WMA) self-training strategy was proposed and applied to the query images at meta-test time in a transductive manner. \n\nExperiments: the proposed algorithm is evaluated on both Guo et al. (2020) and FWT (Tseng et al., 2020) settings for CDFSL. Higher performances are achieved on the part of the eight investigated datasets with different improvements. An ablation study and parameter analysis are conducted. ",
            "strength_and_weaknesses": "Strength:\n- This paper proposed the Adaptive Parametric Prototype Learning (APPL) method, which includes PCN and WMA submodules designed for the CDFSL problem. \n- Experiments are conducted on eight datasets, and slightly improved results are obtained on some datasets. \n\nWeaknesses: \n- Technically, the proposed modules (PCN and WMA) provide little new knowledge for the few-shot and cross-domain few-shot learning field. A series of improvements for Prototypical Networks is proposed, e.g., [R1-R3] to name a few. Learning from the concatenated support features is similar to set-to-set representation [R4]. WMA explored the idea of transduction with moving average updates, which was also discovered in previous work [R5, R6]. \n- Experimentally, the performance improvements are marginal or negative on most dataets, e.g., in Table 1, the proposed method underperforms baselines on three datasets, overlaps on three datasets, and only outperforms with a significant margin on two datasets. Similar results are in Table 2. \n- This paper builds on the cross-domain setting of Guo et al. (2020). However, as a cross-domain few-shot learning paper, it should also discuss other cross-domain settings, at least in the related work section, e.g., [R2, R7-R10]. \n- In Table 1 and Table 2, it is better to indicate the transductive/inductive setting of each method.   \n- Typos. In Sec 2.2., (1) \"... methods. for cross-domain few-shot learning . The FWT ...\", (2) \"... across largely different domains .\", (3) in Figure 1, \"(a)\" and \"b)\" are inconsistent.\n\n\n\n\n\n[R1] Yang, Boyu, et al. \"Prototype mixture models for few-shot semantic segmentation.\" European Conference on Computer Vision. Springer, Cham, 2020.   \n[R2] Bateni, Peyman, et al. \"Improved few-shot visual classification.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.   \n[R3] Jiang, Ruijin, and Zhaohui Cheng. \"Mixture Gaussian prototypes for few-shot learning.\" 2021 International Conference on Data Mining Workshops (ICDMW). IEEE, 2021.   \n[R4] Ye, Han-Jia, et al. \"Few-shot learning via embedding adaptation with set-to-set functions.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.   \n[R5] Chen, Wentao, et al. \"Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images.\" arXiv preprint arXiv:2105.11874 (2021).   \n[R6] Islam, Ashraful, et al. \"Dynamic distillation network for cross-domain few-shot recognition with unlabeled data.\" Advances in Neural Information Processing Systems 34 (2021): 3584-3595.   \n[R7] Triantafillou, Eleni, et al. \"Meta-dataset: A dataset of datasets for learning to learn from few examples.\" arXiv preprint arXiv:1903.03096 (2019).   \n[R8] Liu, Yanbin, et al. \"A multi-mode modulator for multi-domain few-shot classification.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. \n[R9] Bateni, Peyman, et al. \"Improving few-shot visual classification with unlabelled examples.\" (2020).   \n[R10] Doersch, Carl, Ankush Gupta, and Andrew Zisserman. \"Crosstransformers: spatially-aware few-shot transfer.\" Advances in Neural Information Processing Systems 33 (2020): 21981-21993.   \n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall novelty and quality of this paper do not surpass the ICLR acceptance bar.   \n\nTechnical contributions are insufficient, and experimental improvements are also marginal (detailed above).   \n\n",
            "summary_of_the_review": "The technical contributions are insufficient for an ICLR paper, the experimental verification is also less evident.   \n\nI think this paper is not ready for publishing in ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3219/Reviewer_9xQ2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3219/Reviewer_9xQ2"
        ]
    },
    {
        "id": "WnqqKzMXREW",
        "original": null,
        "number": 2,
        "cdate": 1666524775079,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666524775079,
        "tmdate": 1666524775079,
        "tddate": null,
        "forum": "lTjtY1HOUI6",
        "replyto": "lTjtY1HOUI6",
        "invitation": "ICLR.cc/2023/Conference/Paper3219/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the problem of cross-domain few-shot classification, in which novel classes and base classes belong to different data distribution. The authors propose a novel Adaptive Parametric Prototype Learning method based on prototypical network. For meta training, class prototypes are learned by enforcing both inter-class discriminability and intra-class cohesion. For finetuning approach, weight-moving-average self-training is applied. Experiments are conducted on a single source dataset and eight target datasets. Performance of the proposed method is competitive against other few-shot or cross-domain few-shot learning methods.",
            "strength_and_weaknesses": "Strength:\n+ The studied cross-domain few-shot problem is more challenge than simple few-shot classification problem. Methods for CDFS are more robust towards real scenarios.\n+ Experiments are conducted on eight target domain datasets. It\u2019s more complete than some previous works. And performance of the proposed method is competitive.\n\nWeaknesses:\n- The proposed method combines some losses and self-training method which is widely used in domain adaptation with prototypical networks. Novelty of this work is a bit limited. \n- Authors didn\u2019t summarize their contributions and the superiority of this work over previous few-shot and cross-domain few-shot methods, since this paper is not the first to tackle cross-domain few-shot classification problem. \n- There are too many losses and hyperparameters in the proposed method, searching for the best combination of hyperparameters may be hard.\n- A few mistakes in writing. E.g. \u03bb in Fig.1. or L ? Quotation marks in section 4.3 are incorrect. Chinese quotation marks are used by the authors.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity of this work is not very good. Motivations and contributions are not well discussed. In the section of methods, statements, formulations and algorithms are not well organized.\nQuality of this work is at borderline level and novelty of this paper is a bit limited as discussed above.\nReproducibility: hope the author can release code of the proposed method for reproducibility if accepted.\n",
            "summary_of_the_review": "This paper studies a challenge problem of cross-domain few-shot classification. However, the problem is studied by many previous works. Methods proposed by this work can achieve better performance than most of previous works, but the contribution and novelty towards CDFS problem is limited. In addition, this work has some problem with writing and clarity. So from my perspective, it is below the acceptance threshold. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3219/Reviewer_BfM8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3219/Reviewer_BfM8"
        ]
    },
    {
        "id": "-97jgkPo4sH",
        "original": null,
        "number": 3,
        "cdate": 1666555009471,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555009471,
        "tmdate": 1666555009471,
        "tddate": null,
        "forum": "lTjtY1HOUI6",
        "replyto": "lTjtY1HOUI6",
        "invitation": "ICLR.cc/2023/Conference/Paper3219/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a novel Adaptive Parametric Prototype Learning for cross-domain few-shot learning. The idea is to learn class prototypes from a parametric prototype calculator network. The class prototypes  are learnt from concatenated feature vectors of the support instances to ensure inter-class discriminability and intra-class cohesion. The proposed approach is shown to be effective for cross-domain few-shot learning. ",
            "strength_and_weaknesses": "Strength:\n1. The problem of cross-domain few-shot learning is important and the authors propose to improve ProtoNet for cross-domain few-shot learning. \n2.  The authors conducted extensive experiments to show the benefits of the proposed approach. \n\n\nWeaknesses:\n1. Many design choices lack motivation. For example,\ni. What's the benefits of using the concatenated feature vectors for computing the prototypes? What's the issue of using the feature from the last layer?\nii. Why use the unlabeled target can reduce the domain shift?\niii. What's the motivation of using a weighted-moving-avarage?\n\n2. The proposed method has many hyperparameters which it hard to use the method for other applications. \n\n3. What is the time complexity of the proposed approach compared with the baselines? The introduction of the adaptive prototype calculator network is an additional overhead. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is fair and the organization needs improvement, especially section 3.2.2. The quality of the paper is OK. The idea is somewhat novel but the introduction of many hyperparameters makes the method not applicable in real-world applications. The paper seems not easy to reproduce. ",
            "summary_of_the_review": " The authors propose a novel Adaptive Parametric Prototype Learning for cross-domain few-shot learning which has some novel ideas. However, the motivations are not clearly stated and it is hard to understand where is the improvement comes from. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3219/Reviewer_EF7b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3219/Reviewer_EF7b"
        ]
    },
    {
        "id": "jmg6XCw01Ep",
        "original": null,
        "number": 4,
        "cdate": 1666990964263,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666990964263,
        "tmdate": 1666990964263,
        "tddate": null,
        "forum": "lTjtY1HOUI6",
        "replyto": "lTjtY1HOUI6",
        "invitation": "ICLR.cc/2023/Conference/Paper3219/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to perform few-shot recognition, and introduces two innovations. The first is that instead of averaging support image features to get a prototype, the proposed approach meta-learns a model that produces a prototype from concatenated features. The second innovation is to perform self-training with weighted moving averaging on the target domain. ",
            "strength_and_weaknesses": "Strengths:\n- The proposed approach is novel.\n- The performance of the approach is quite good.\n- The ablations are very informative.\n\nWeaknesses\n- The method seems general enough to apply to all few-shot learning systems. Why restrict to the cross-domain setting?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The approach seems fairly clear and original.",
            "summary_of_the_review": "I think this is a reasonable paper and worth accepting, but I would like to see how it does on the traditional FSL benchmarks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3219/Reviewer_TtNB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3219/Reviewer_TtNB"
        ]
    }
]