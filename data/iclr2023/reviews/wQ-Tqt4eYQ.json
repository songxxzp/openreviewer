[
    {
        "id": "IG2FcPTjSwg",
        "original": null,
        "number": 1,
        "cdate": 1666606702363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606702363,
        "tmdate": 1669397163288,
        "tddate": null,
        "forum": "wQ-Tqt4eYQ",
        "replyto": "wQ-Tqt4eYQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3508/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose ASGNN, a robust GNN architecture. ASGNN learns a \"cleaned\" adjacency matrix as well as feature matrix per message passing layer with the goal of removing adversarial perturbations from the graph. The message passing procedure is framed as an optimization problem, and the individual message passing steps are proximal gradient steps. The authors present experimental results on well-known datasets from the literature.",
            "strength_and_weaknesses": "Strengths:\n* Framing the structure learning as an optimization approximately solved via the message-passing procedure itself appears novel and is interesting\n* The presented results suggest that the method is effective at defending against existing attack methods.\n* The presentation of the work is clear and easy to follow.\n\n\nWeaknesses:\n* The authors do not evaluate their defense against adaptive attacks designed to circumvent their defense\n* The comparison is missing an important baseline with GNNGuard [Zhang and Zitnik, 2020].\n* The authors only evaluate their method on small datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: very good and easy to follow.\n\n**Quality**: There are some important weaknesses of the paper and the experiments:\n* The authors do not compare to GNNGuard [Zhang and Zitnik, 2020], which is an important baseline.\n* The authors do not evaluate their defense on adaptive attacks, i.e., attacks designed to overcome the specific defense the authors propose. This is very important, as it is always easier to defend against existing attacks, but often an attacker can break novel defenses by adapting the strategy. This is, of course, different from certifiable defenses where we get a mathematical guarantee of robustness.\n* There is no information about the computational complexity of the method and/ or runtime analysis. Given that the authors only report on small datasets, I expect the method to be quite computationally expensive.\n* From the description in the text it is unclear whether the authors repeated all experiments ten times on one single split of the data or whether they evaluate on ten different splits. If they only evaluate on a single split, this is a problem because GNN performance has been shown to be highly variable depending on the individual split [Shchur et al., 2018].\n* It would be interesting to see some qualitative insight into what kind of graphs the method learns. Does it remove adversarial edges? How sparse are the resulting graphs?\n* There are no hyperparameter sensitivity of ablation study results. There are five terms in the loss function: are all of them required, and how sensitive is the model to different choices of hyperparameters?\n\n**Novelty**: While the loss function is very similar to the one in Pro GNN [Jin et al. 2020] and the idea of framing message passing as gradient steps is not new, their combination is novel, interesting and non-trivial.\n\n**Reproducibility**: Okay; while the description of the method is sufficiently clear to implement it, in order to exactly reproduce the results some information is missing: the authors did not provide an implementation of their method, did not provide details which of the hyperparameters in their search space was selected in the end, it is unclear which exact split of the data was used, and the hyperparameters of the MLP of the method are not provided.\n\n\nReferences:\nShchur, O., Mumme, M., Bojchevski, A., & G\u00fcnnemann, S. (2018). Pitfalls of graph neural network evaluation. Relational Representation Learning Workshop, NeurIPS 2018.",
            "summary_of_the_review": "The paper is a solid piece of work developing a robust GNN architecture. The presentation is clear and easy to follow. There are some key limitations of the work that prevent me from recommending acceptance (see details above):\n* Missing the GNNGuard baseline\n* Lack of adaptive attacks.\n* Only reports results on small datasets, no runtime/ complexity information or ablation study results provided.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3508/Reviewer_Qd9P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3508/Reviewer_Qd9P"
        ]
    },
    {
        "id": "BRYhZfBIN20",
        "original": null,
        "number": 2,
        "cdate": 1666683617985,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683617985,
        "tmdate": 1666685512532,
        "tddate": null,
        "forum": "wQ-Tqt4eYQ",
        "replyto": "wQ-Tqt4eYQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3508/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The major contribution is the message passing scheme with the adaptive structure that enables learning different propagation structures for different GNN layers. The proposed method ASGNN is one of the graph structure learning methods and the improvement of this work in terms of motivation is marginal. \n",
            "strength_and_weaknesses": "Strength:\n1. It\u2019s a generalized framework, achieving conduct message passing over different graph structures at different layers.\n2. In most experiments, it achieves the best results. \n3. The proposed update scheme has good theoretical support.\n\nWeaknesses:\n1. The authors claim that the message passing scheme of ASGNN is \u2018interpretable\u2019. But relevant experimental results are not given. \n2. This paper lacks a complexity analysis of the proposed model, considering the proposed ASGNN is trained to learn different propagation structures on each GNN layer.\n3. The challenges and innovation of introducing the adaptive structure are not clear.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Limited innovation.\n2. The code is not open, and some setting of hyperparemeters like $\\mu_1$, $\\mu_2$ is unknown.\n",
            "summary_of_the_review": "The motivation of this work is not quite novel. And the adaptive structure assumption increases the complexity of the proposed model. In a word, the proposed ASGNN brings performance improvements, but the complexity of this model also makes it hard to apply to large-scale graph data.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3508/Reviewer_sBNe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3508/Reviewer_sBNe"
        ]
    },
    {
        "id": "gDOMqHFCIaC",
        "original": null,
        "number": 3,
        "cdate": 1667404599096,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667404599096,
        "tmdate": 1667404599096,
        "tddate": null,
        "forum": "wQ-Tqt4eYQ",
        "replyto": "wQ-Tqt4eYQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3508/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper deals with building robust graph learning methods not susceptible to adversarial attacks. Existing methods learn the structure of graph either by preprocessing the graph structure or by parametrically learning the graph adjacency. In this paper, the authors propose to simultaneously learn the graph structure as well as to use it for message passing iteratively. They call it message passing with adaptive structure (ASMP), which is shown as a way of doing proximal gradient descent for simultaneous denoising of graph signal and graph structure. Experimental results are provided to show the usefulness of the approach.",
            "strength_and_weaknesses": "### Strengths:\n1. The paper is well-motivated to handle the noise in graph structure with simultaneous learning of graph structure with features.\n1. The idea of adaptively learning the graph structure iteratively is appealing.\n1. Experimental results show that the method is promising. However, improvements on most datasets is only marginal.\n\n### Weaknesses:\n1. I do not understand the usefulness of having \u2018different structure\u2019 in each layer. More plausible to me is that the with adaptive graph structure, it gets progressively better and not just different. \n1. Why do authors use $L_{rw}$ and not $L_{sym}$. Is there any reason which makes it better? Experimental analysis to study the difference would be helpful as well.\n1. The paper can also be strengthened with further ablations. What are the learnt values of ASGNN coefficients which give the best results? This would be helpful to see the terms affecting the performance.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. I do not know much of the related work. However, it seems the main idea is novel and well-developed.",
            "summary_of_the_review": "The overall idea of adaptively learning graph structure with message passing seems nice and well-developed. However, I am not fully aware of the related works and hence, not clear on the degree of novelty and significance of the results. Hence, I'm less confident. Therefore, I'm going with weak acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3508/Reviewer_tSZb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3508/Reviewer_tSZb"
        ]
    }
]