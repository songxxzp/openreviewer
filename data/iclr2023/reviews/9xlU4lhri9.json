[
    {
        "id": "UnybUuu4ez",
        "original": null,
        "number": 1,
        "cdate": 1666186157678,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666186157678,
        "tmdate": 1666186271409,
        "tddate": null,
        "forum": "9xlU4lhri9",
        "replyto": "9xlU4lhri9",
        "invitation": "ICLR.cc/2023/Conference/Paper2161/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors pick up a controversy on the nature of the stochastic gradient noise, and aim at clarifying it by distinguishing among iteration-wise and dimension-wise noise. The iteration-wise noise looks separately at a single component throughout the whole dynamics, and it is more Gaussian distributed. The dimension-wise noise looks at all components at fixed time, and looks more power-law-distributed.\nThen, they show empirically that the covariance spectra follow a power law (this seems to hold in different settings), and emphasize the connections/differences between them and the hessian of the loss.\n\n",
            "strength_and_weaknesses": "Strengths: \n- The distinction between two different kinds of stochastic gradient noise allows to provide a clearer picture of the phenomenon\n- The power law of the covariance spectrum was not observed before and seems to be very stable across different situations\n\n\nWeaknesses: \n- Presentation: Though the paper is well-written, several things were not clear to me (see next section).\n- Averaging: Curves don't seem averaged over several instances\n- Stages of dynamics: There is no emphasis on different stages of the dynamics, but we know that at least three distinct phases of learning can be identified (see e.g. arXiv:1803.06969 or arXiv:2111.07167). This might as well help explaining why the Gaussian and Power-Law rates are far from 100%. For example, arXiv:1910.09626 notices that there might be differences along training.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and reproducibility seem good. \n\nAs for the novelty, perhaps the authors could emphasize the advances with respect to the works belonging to Research Line 2 (Xie et al 2020, Xie et al 2022b, Li et al 2019).\n\nThough the paper seems generally well-written, there are many places where I am not sure about what was done or about what a specific message meant. This makes it harder for me to assess the paper.\n\n1) Section 2, when defining the KS test, the authors say:\n\"If the KS distance $d_{ks}$ is smaller than the critical distance $d_c$ , the KS test will reject the power-law hypothesis.\"\nImmediately after I read:\n\"The smaller $d_{ks}$ is, the better the goodness-of-power-law is.\"\nThis is confusing.\n\n2) Section 2, when defining the $\\chi^2$ test. \n\"If the estimated $p$-value is less than 0.05, the $\\chi^2$ test will reject the Gaussian hypothesis.\"\nImmediately after I read:\n\"The smaller $p$-value is, the better the goodness-of-Gaussianity is.\"\nThis is confusing too.\n\n3) I am not sure about Figure 1 (and others). The dimension-wise gradients should have n components (the # of weights). Since the model is a LeNet on cifar-10, I would expect there to be 62000 points on the x axis, but there are only 2000. As for the iteration-wise gradients, the run has 5000 iterations (App.A), so again it looks like not the full curves are shown.\n\n4) In Fig.1 it is not clear at what time the Dimension-wise curve is taken. Given that there are so few points, I don't think it comes from more than one time. Is it at initialization?\n\n5) In Fig.2 there is a red curve for the power law rate on the legend, but the curve is not visible. Is it at y=0? I would make it visible.\n\n6) Some times the main text is not self explanatory, and it is necessary to go to the appendix to understand the tables/figures. For example, column 2 of table 1 (Training) is not explained in the main text.\n\n7) Though the names might sound intuitive, I often got confused between the two kinds of noise. I would suggest a table similar to the following:\n\n------------------------------------------------\nIteration-wise     | Fixed component, All the dynamics   | Gaussian\n------------------------------------------------\nDimension-wise | All components, Fixed time                | Power law\n------------------------------------------------\n\n8) I am not sure I understand why the authors say that the true SGN is the iteration-wise (e.g. \"the heavy-tail property describes dimension-wise gradients (not SGN)\"). As said in other parts of the text, the heavy-tailed dimension-wise noise (i.e. the fact that at a fixed step the gradient feels some strong kicks toward specific directions) is what is actually interesting for implicit regularization. This is also my understanding of what is treated by Simsekli et al 2019, and if I were to focus on one of the two kinds of noise, it would be dimension-wise.\nSo I am not sure about the conclusions of section 3, and I am not sure I understand the request to Simsekli and coauthors to clarify this.\n\n9) Figure 3 is not mentioned in Sec.3, but only in the introductory part.\n\n10) Is Fig.4 (and the others) before running the dynamics, or are they near a minimum? Is there a difference in the spectra at the beginning of the dynamics and at initialization? It is hard to interpret the results of sec.4 without being sure about this.\n\n11) It would be great if the figures in page 7 were made a little bit bigger or more readable (e.g. Fig.5, the legend could have bigger dots, and the curves could have more different colors)\n\n12) Also, Eq.3 shows that the batch size is important in the correspondence between the spectra of Hessian and Covariance, but it is not reported in Fig.5. Further, why don't they rescale the eigenvalues by the batch size, since that's the quantity of interest?\n\n13) Why do the authors emphasize that the highest eigenvalues of the hessian are different when looking at Hessian and Covariance? The full spectra seem different (including the power law tails in Fig5a), not only the largest eigenvalues. If the intention is to emphasize that the largest eigenvalues are different, this is already well-established (e.g. arXiv:1812.04754 or arXiv:1706.04454).\n",
            "summary_of_the_review": "The paper is simple but it passes a couple of ideas worth disseminating. My main concerns are about the presentation, because some of the results were not too clear to me and it was therefore hard to assess them. My second concern is that there often does not seem to be a distinction between different phases of learning.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2161/Reviewer_cbEm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2161/Reviewer_cbEm"
        ]
    },
    {
        "id": "6D7x_QBpZun",
        "original": null,
        "number": 2,
        "cdate": 1666659215733,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659215733,
        "tmdate": 1666659215733,
        "tddate": null,
        "forum": "9xlU4lhri9",
        "replyto": "9xlU4lhri9",
        "invitation": "ICLR.cc/2023/Conference/Paper2161/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tests the distribution of stochastic gradient across both parameters and iterations. They show that dimension-wise gradients have power-law heavy tails. However, iteration-wise gradients and stochastic gradient noise caused by minibatch training have Gaussian tails. More interestingly, they show that the gradient covariance has power-law tails.",
            "strength_and_weaknesses": "## Strength\n- This work delivers thorough simulations to test the distribution of the stochastic gradients across both dimension and iteration. This is very important since, in the previous literature, there are two contradictory views about the distribution of stochastic gradients. The author first clarifies the difference between these two distinct views and put them into a consistent framework. Moreover, they do thorough simulations on many datasets, demonstrating the universality of their observations. \n\n- The observation that gradient covariance has power-law tails seems very interesting, which might be able to explain the success of stochastic gradient methods. \n\n## Weaknesses \n- The main concern is that this work only provides some interesting observations, from which we cannot easily derive any informative implications. Hence, this work is more like a technical report instead of a paper. For example, they show that the gradient covariance has power-law tails in many neural network settings and they argue that this might be able to explain the success of stochastic gradient. It is not clear at all that why this special structure can help the generalization.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The results are clearly presented. The writing is good. The novelty seems to be marginal, the details can be found in the weaknesses part. ",
            "summary_of_the_review": "Please see the strength and weaknesses section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2161/Reviewer_ZG6J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2161/Reviewer_ZG6J"
        ]
    },
    {
        "id": "2kJjKhc5dhu",
        "original": null,
        "number": 3,
        "cdate": 1667072852943,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667072852943,
        "tmdate": 1667072852943,
        "tddate": null,
        "forum": "9xlU4lhri9",
        "replyto": "9xlU4lhri9",
        "invitation": "ICLR.cc/2023/Conference/Paper2161/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The effectiveness of Stochastic Gradient Descent optimization for deep learning is well known. The paper aims to study the nature of the probability distribution of stochastic gradient noise (SGN) via statistical tests that check for power law behavior or gaussian behavior of dimension-wise gradients, iteration-wise gradients, with mini-batch training.  The study uses MNIST, CIFAR10 datasets and LeNet, FCN, and ResNet18 architectures. The formal statistical tests reveal that dimension-wise gradients usually exhibit power-law heavy tails, while iteration-wise gradients and stochastic gradient noise caused by minibatch training usually do not exhibit power-law heavy tails. Power law distributions are reported for the covariance spectra of stochastic gradients.  The paper then speculates that this structure may be the reason for effectiveness of SGD in deep learning.",
            "strength_and_weaknesses": "Strengths:\nThe paper addresses an important problem. The experimentation with formal statistical tests and conclusions are interesting. \n\nWeaknesses:\nApart from the fact that the authors conduct formal statistical tests (KS for power law and Chisquare test for Gaussianity) provide a little more rigor in the analysis, the central question of why these distributions arise is not addressed.  Indeed the authors point this out as a limitation and leave theoretical explorations to future work.  There are a number of points raised in the discussion section that are not completely resolved. For instance, why is the proportionality relation between the hessian and the covariance very weak?  The surprising discovery of invariance of the structure of the power law structure for SGN distribution is raised, but not analyzed.  I would have liked to see more of a concrete theoretical analysis of the problem with a simulated dataset involving a particular manifold structure and then studying the SGN structure thus allowing explicit correspondence between the nature of the dataset and the SGN distribution form.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:  The paper is written well and clear.\nQuality:   The quality is good. \nOriginality:   Builds upon past work and reconciles/clarifies conclusions that are deviating in past literature may be due to the incoherent definition of SGN.  \nReproducibility:  Adequate details are provided in the appendix. However, I am not absolutely sure about the speculations and quantitative claims in section 4 and 5.  ",
            "summary_of_the_review": "The paper is addressing a very important problem to unravel why SGD methods offer success in deep learning. Formal statistical tests for the distribution form of SGN and the empirical observation that the spectra of the covariance has a power law structure is interesting. However, the main difficulty I have with the paper is that I am unable to judge the empirical conclusions as there is limited explanation for the results and surprises. Moreover,  the authors themselves allude to the fact that the theoretical analysis is left for future work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2161/Reviewer_2nic"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2161/Reviewer_2nic"
        ]
    }
]