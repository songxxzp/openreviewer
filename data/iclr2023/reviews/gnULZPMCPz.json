[
    {
        "id": "lwoiJxIwZny",
        "original": null,
        "number": 1,
        "cdate": 1666580325507,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580325507,
        "tmdate": 1666580325507,
        "tddate": null,
        "forum": "gnULZPMCPz",
        "replyto": "gnULZPMCPz",
        "invitation": "ICLR.cc/2023/Conference/Paper2137/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Two interesting machine learning models that are simultaneously of theoretical and practical interest are Deep Equilibrium Models (DEQs) and well-behaved infinite-width neural networks trained using gradient flow (Neural tangent kernel, NTK). The first is theoretically interesting, because it can *sometimes* be used to understand very deep weight-tied networks, and the second is interesting because it can *sometimes* be used to understand finite-width neural networks trained using gradient descent. This paper combines elements from DEQs and NTKs, showing how the NTK of a DEQ can be computed in a certain regime. ",
            "strength_and_weaknesses": "**Strengths:**\n- Studying the NTK of a DEQ is an interesting mathematical problem, and will help connect the two communities of implicit neural networks and infinite width neural networks.\n\n**Weaknesses**\n- A very critical part of the proof and intuition for this paper is that the NTK of a weigh-tied network is the same as the NTK of a corresponding network with independently sampled weights in each layer. This appears to be an existing result of Yang. However, as this a very central part of the paper (first equations in section 3), it needs to be thoroughly unpacked and explained in the main text. Currently, the way the analysis is presented is backwards (the unweight-tied version is introduced first), and remark 3.2 gives no hint of the machinery behind the statement. The proof of theorem 3.1 in the appendix seems to also not be helpful in this issue. As such, at the moment I can only see that this paper discusses infinitely deep networks with independently sampled weights in each layer, not DEQs. What are the conditions and/or assumptions required for the equivalence of weight tied and not weight tied networks?\n- Theorem 3.1 and 3.3 seem to give no conditions that ensure that that the fixed point is unique, or even that exists. In contrast, the condition in Theorem 3.5 that the standard deviation is less than 1/8 seems to be a contraction condition that ensures that a unique fixed point exists. I do not think Theorem 3.1 and 3.3 can be correct in general without some assumption ensuring that a fixed point is unique and/or is unique. But at the moment, the text preceding the theorem is not precise enough to even allow the theorem to be expressed in terms of such conditions or assumptions. Can you add the required assumptions and conditions into the relevant theorems?\n- There are some further issues around clarity (detailed below).\n- As far as I can tell (please correct me if I am wrong), the paper makes no attempt to argue that the derived NTK-DEQ is a practically useful model, only that it is a mathematical curiosity. Given the toy evaluation, it is unlikely that such a model will be useful in the broader context of machine learning.\n\n\n**Mathematical ambiguities and correctness:**\n- Section 3. In this equation block, this infinite depth limit does not always converge to the unique fixed point. Since this seems to be the main mathematical section of the paper, it is necessary to have some more precision in this section.\n- First equation in section 3. This is not a weight-tied network, so the text \"Define the ... DEQ as the following:\" is not appropriate. You mention later that DEQs require weight-tied networks and that the limit of this un weight-tied network will be the same as the weight-tied. If this is the case, why not introduce the DEQ with weight tying and then prove that this limit is the same as with weight-tying? As is, the sentence before the equation is not accurate.\n- In order to invoke the result of Yang, you mention that \"The neural architecture needs to satisfy a gradient independent assumption.\" Can you write down what this assumption is?\n- Proof of Theorem 3.3. For equations (17) and (18), it might be more appropriate to cite Cho and Saul than Daniely, since Cho and Saul were first.\n- Theorem 3.5. Please give a definition of \"with high probability\".\n\n**Experiments:**\n- Section 5.2. What is the model? I assume some kind of kernel method? Kernel logistic regression? SVM? In the text it says \"After obtaining the NTK matrix, we apply kernel regressions\", but these problems look like classification problems?\n- \"Notice these root finding problems are one-dimensional, hence can be quickily solved.\" Which problems are \"these\", and why are they one-dimensional? It is not obvious to me as currently written. For a dataset of size $n$, each data point will appear in the kernel matrix $n$ times (granted, it is a symmetric matrix). There is a typo in quickly.\n- Section 5.1. Are these $W$ trained in an NTK regime? (Non-stochastic) gradient descent? What is the data and task that the network was trained on?\n- Figure 5. Which nonlinearities are used in the figure on the right?\n\n**Minor:**\n- First sentence of introduction. There are also optimisation-based layers (Deep Declarative Networks: A New Hope), which implicitly define the output of a layer as the solution to a (possibly constrained) optimisation problem. Under sufficient regularity, this is equivalent to finding a root of the gradient. But not always. These are also mentioned in the implicit layer NeurIPS tutorial.\n- \"Bai et al. (2019) proposed the DEQ model, which is equivalent to running an infinite-depth FCNN-IJ, but updated in a more clever way\". This is not precise. A DEQ model *can* be equivalent to an infinite-depth FCNN-IJ. For example, if the mapping involved in each iteration is a contraction, the infinite-depth recursion converges to the unique fixed point. But a DEQ model is not necessarily *always* equivalent to an infinite-depth FCNN-IJ. You mention existence and uniqueness at the end of the paragraph, but this should come first.\n- Two periods on \"bias terms..\" in section 3.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\nUnfortunately, there are issues around clarity. See detailed comments above.\n\n**Novelty:**\nAs mentioned above, the key ingredient of this paper this paper is writing the first equation of section 3 as a network without weight-tying, which appears to be an existing result of Yang. Once this can be written, standard techniques apply to analyse the NTK. The reader is not given any assistance in understanding if or how the result of Yang can be applied.\n\n**Reproducibility:**\nThere are some minor issues in the experiments section.",
            "summary_of_the_review": "This paper analyses DEQs in the context of the NTK. My biggest concern is that as currently written, it is not clear that the weight-tied structure of DEQs is faithfully analysed (see section 3 first equation and remark 3.2). If this structure is indeed not faithfully analysed, the analysis reduces to a trivial known result. The writing in section 3 could generally be improved (the lack of precision of the assumptions and conditions in section 3 arguably makes them incorrect), as could the details of the experiments. The experiments do not seem to argue that the derived model is practically useful (which would not be an issue with the paper in itself, if its other merits could stand on their own).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2137/Reviewer_TcTa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2137/Reviewer_TcTa"
        ]
    },
    {
        "id": "USdAuNrCn-",
        "original": null,
        "number": 2,
        "cdate": 1666780679752,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666780679752,
        "tmdate": 1672230262838,
        "tddate": null,
        "forum": "gnULZPMCPz",
        "replyto": "gnULZPMCPz",
        "invitation": "ICLR.cc/2023/Conference/Paper2137/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper showed that contrarily a DEQ model enjoys a deterministic NTK despite its width and depth going to infinity at the same time under mild conditions. The deterministic NTK can be found efficiently via root-finding.",
            "strength_and_weaknesses": "Strength:\n\n+ The paper showed that unlike the infinite depth limit of NTK to FCNN, the DEQ-of-NTK does not converge to a degenerate kernel. This non-trivial kernel can be computed efficiently using root-finding.\n\n+ The NTK-of-DEQ coincides with the DEQ-of-NTK under mild conditions. The paper showed numerically that reasonably large networks converge to roughly the same quantities as predicted by theory.\n\n+ The paper showed the NTK-of-DEQ matches the performances of other NTKs on real-world datasets.\n\nWeaknesses:\n\n- Only simulation experiments are conducted to demonstrate the performance of the NTK-of-DEQ.\n\n- The paper is presented in a rather dense mannar. It is better to give a high-level overview of the proof pipeline.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper writing is clear in general while it could be further improved.\n\nQuality & Novelty: The proof builds upon exisiting method, the real novelty should be made clear further. \n\nReproducibility: Given the dense mathematics and some minor issues, it is not very easy to reproduce the method.",
            "summary_of_the_review": "The paper showed that contrarily a DEQ model enjoys a deterministic NTK despite its width and depth going to infinity at the same time under mild conditions. The deterministic NTK can be found efficiently via root-finding. The main contributions of the paper lie in the theoretical aspects and simulations have demonstrated the performance. Some of the key proof builds upon existing work, the real new contributions should be made clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2137/Reviewer_fAKE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2137/Reviewer_fAKE"
        ]
    },
    {
        "id": "PckAFTc6Dg",
        "original": null,
        "number": 3,
        "cdate": 1666900545960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666900545960,
        "tmdate": 1666900545960,
        "tddate": null,
        "forum": "gnULZPMCPz",
        "replyto": "gnULZPMCPz",
        "invitation": "ICLR.cc/2023/Conference/Paper2137/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the NTK of the dee equilibrium model.  Contrast to the NTK of FCNN which can be stochastic if its width and depth both tend to infinity simultaneously, a DEQ model will have a deterministic NTK in this case under some mild conditions. Also this deterministic NTK can be found efficiently via root-finding.",
            "strength_and_weaknesses": "Strength\n- This paper shows that unlike the infinite depth limiting case of NTK to FCNN, the DEQ-of-NTK does not converge to a degenerate kernel, and the non-trivial kernel can be computed efficiently using root-finding.\n- The forms of the final main theorems are compact, intuitive and easy to understand.\n\nWeakness and Questions\n- What is the significance or any application of the conclusion that the DEQ-of-NTK and NTK-of-DEQ are deterministic? I understand this is a theoretical work, but I think it is better to mention the usage or theoretical significance for the existence of a deterministic NTK-of-DEQ as I'm not familiar with the idea of DEQ.\n- What is the accurate meaning of \"high probability\" in Theorem 3.5. Is it somehow related to the distance between $\\sigma_w^2$ and 1/8?\n- I'm a bit confused about the definition of $\\sigma_W$ and the initialization process of $W,U,b,v$. The author mentioned in the beginning of Section 3.1 that pick $\\sigma_W,\\sigma_U,\\sigma_b$ arbitrarily . But it seems there is no constraint for $\\sigma_W$ in the beginning of Section 3.1. What is the difference between this one and that defined in Theorem 3.1.\n- What will happen if $\\sigma_W^2>1/8$ ? Will the NTK-of-DEQ definitely diverges or there be some possibility that both NTK-of-DEQ and DEQ-of-NTK converges but at different limits?",
            "clarity,_quality,_novelty_and_reproducibility": "The main structure and logistics of this paper are quite clear and easy to follow. But some notations are not defined unambiguously enough.\nThe major technical innovative results of this paper is to prove a nested limits could be exchanged in Theorem 3.5. With this holding true, one can conclude the existence of deterministic NTK-of-DEQ.",
            "summary_of_the_review": "I'm not familiar with DEQ model, but in my perspective this paper made some theoretical, especially technical contribution for proving the existence of NTK-to-DEQ by proving an exchangeable nested limit. However I'm not sure what further theoretical or application results can be induced from this conclusion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2137/Reviewer_4dFG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2137/Reviewer_4dFG"
        ]
    }
]