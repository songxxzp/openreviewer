[
    {
        "id": "YffN7k4gmao",
        "original": null,
        "number": 1,
        "cdate": 1666109627110,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666109627110,
        "tmdate": 1669550085900,
        "tddate": null,
        "forum": "7D5EECbOaf9",
        "replyto": "7D5EECbOaf9",
        "invitation": "ICLR.cc/2023/Conference/Paper3454/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a novel method for reducing the dataset size by selecting the data that is close to the median error of the projection of each sample against the mean projection of all samples that belongs to the same class. The authors evaluate their results in three datasets, obtaining state-of-the-art results in almost all of them. ",
            "strength_and_weaknesses": "Strength:\n* The algorithm is easy to understand and to implement\n* Both computational and spatial costs are minimal\n* The experimental results are promising\n\nWeaknesses\n* Only tested in one neural network architecture",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n* Although the idea is easy to follow and to implement, I suggest the authors to encapsulate the idea into one algorithm environment. The intuition behind the idea clearly stated.\n\nQuality:\n* The solution is very well presented. The experimental results are quite promising. However, I would like to see how the algorithm behaves when different architectures are provided. The algorithm was only tested using a resnet solution.\n\nNovelty:\n* Although the idea is somehow simple, its novelty is clear.\n\nReproducibility:\n* The algorithm is easy to understand and to reproduce. The model hyper-parameters are included in the paper.",
            "summary_of_the_review": "Overall, I think it is a very interesting idea. It is very simple, efficient and easy to implement. I am lowering my score because of the lack of multiple NN architectures in the experimental section. As a future remark, I would like to see if a combination between this approach and the forgetting algorithm could combine the benefits of both methods. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3454/Reviewer_Ergu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3454/Reviewer_Ergu"
        ]
    },
    {
        "id": "9KIp8Fiw-Q",
        "original": null,
        "number": 2,
        "cdate": 1666574921643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574921643,
        "tmdate": 1666574921643,
        "tddate": null,
        "forum": "7D5EECbOaf9",
        "replyto": "7D5EECbOaf9",
        "invitation": "ICLR.cc/2023/Conference/Paper3454/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper argues that corset should be selected  from near the score median. The authors argue that corset examples that are closer to the median are expected to generalize better to different scenarios than those  that pick examples based on highest loss. The authors point out that in prior work, where outliers are expected to be significant, data points with smaller losses are favored. Their approach seems similar but instead of picking examples based on loss, it picks examples that are closer to the mean class-specific center in feature space, where closeness is measured as as ||z  - zj ||_2, where zj is the class center.\n\nThe abstract and intro does not make it apparent what the scenarios that authors have in mind. When reading the paper, I was thinking they  are going to present examples of distribution shifts (e.g., different weightage of unrepresented categories). But, later, in the empirical evaluations, the focus was on data corruption -- mislabeled examples or corrupted images (e.g., fog, motion blur, random occlusion). Unsurprisingly, these are also the scenarios where one can expect techniques that prefer high loss and those close to decision boundary to underperform So, I am not entirely surprised that their scheme did well with increased data corruption or mislabeling compared to most schemes. But, perhaps unsurprisingly, they underperform in the absence of corruption and mislabeling, especially if the corset is large.\n\nThe authors tested on adversarial inputs that attack an original classification pipeline, but one weakness in that testing is that the adversarial inputs do not seem adaptive to their technique.\n\n\n\n\n\n",
            "strength_and_weaknesses": "Pros: \n-- The empirical results seem consistent with the intuition presented. \n-- Overall, the paper is intuitive. \n\nCons: \n-- Isn't Herding also a form of moderate corset strategy? Assuming that is the case, the paper could do a better job explaining the distinction and why that matters.\n-- The paper claims that computing distances on hidden representation is more efficient. Any data to substantiate that? Any timing numbers?\n -- What is the intuition, if any, for outperforming Herding?  It may be good to discuss that further. \n -- Could you elaborate more on how you trained the model when corruptness  or mislabeling was injected in your experiments? For instance, when you had 20% mislabeled examples, what are the curves that are similar to Figure 5? I didn't find them in the Appendix. \nDid the models overfit to the corrupted or mislabeled data?\n\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Generally, the paper seems clear.  \n\nThe results are not surprising, but that may be OK. I think the novelty over Herding  (and possibly random) needs to be better argued and supported. Downsides need to be better presented. ",
            "summary_of_the_review": "The paper makes a cogent argument as to why a coreset that has examples from near the center of the each class may be more robust to outliers or mislabeled data. But, Herding  (Welling 2009) also does something similar. So, that by itself does not make the paper novel. The authors could do a better job articulating why Herding is not the solution they are looking for. If the main argument is the proposed scheme is more performance-efficient in terms of selecting a corset on very large datasets, then the entire set of experiments should have been geared to supporting that argument.  So, overall, I think interesting ideas are possibly there in the paper. But, they need to be better teased out and better supported.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3454/Reviewer_vgUK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3454/Reviewer_vgUK"
        ]
    },
    {
        "id": "GUELtMX4UxK",
        "original": null,
        "number": 3,
        "cdate": 1666648742719,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648742719,
        "tmdate": 1667444919849,
        "tddate": null,
        "forum": "7D5EECbOaf9",
        "replyto": "7D5EECbOaf9",
        "invitation": "ICLR.cc/2023/Conference/Paper3454/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a data selection method for data-efficient deep learning. To make the model robust under various conditions, such as different levels of corruption and label noise, it propose the moderest coreset. Specifically, it first computes the class centers by averaging features of the corresponding samples, and then define the coreset as the samples that have median distances to the center. Experiments show that the method achieves overall good performance for various conditions, including different levels of image corruption and label noise, and adversarial attacks.",
            "strength_and_weaknesses": "Strength\n\n- The method performs more robustly compared to other baseline data selection methods under various conditions, including image corruption, label noise, and adversarial attacks.\n- The method is simple and easy to implement.\n- The paper is clearly written and easy to follow.\n\nWeakness\n\n- Although the overall accuracy is good, the improvement from the baselines looks a bit marginal. Also, the improvement from the baseline seems smaller for higher levels of corruption and label noises.\n- I am not sure if the simple average of the features can be a robust representation of the class. If there are outlier samples, then simple averaging will be significantly affected and can be apart from the actual class center of when there are no outliers. Since the proposed moderest coreset is selected based on the distance computed from the centers, which are the simple average of features, it can also be affected.\n\nOther things\n\n- Is the good data selection strategy common among different model architectures? Or is it a property of data itself? What will be the effect of different choices of \"well-trained deep model\" for representation extraction? How will the median samples and the model performance change?\n- Is the good data selection strategy common among different tasks? Or the good coresets differ across tasks? It would be nice if a discussion on the potential extension to general tasks beyond classification could be included.\n- How will the method perform for different backbone architectures (e.g. transformer)?\n- Another potential advantage of the proposed method could be a better generalization to unseen domains. It would be interesting to see cross-domain evaluation.\n- Additional analysis can be done by varying the selection range, e.g. Q1, Q3, in addition to the current Q2.\n- In Figure 2, how does the method work for a smaller selection ratio under 60%?\n- In Figure 5, how does the method work for more severe corruption (>20%)\n- In Table 3, how does the method work for more severe label noise? (>30%)\n- I am not sure about the purpose of Figure 4. I guess a comparison with other sampling strategies (Figure 3) with different score computation methods (GraNd-score, EL2N-score) will show the effect of the proposed median-based selection. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The motivation for the moderate coreset is reasonable, and the proposed method achieves overall good accuracy. As far as I know, the method to select median distances from the class center is novel. As the method is simple and easy to implement, I expect the results can be reproduced.",
            "summary_of_the_review": "In summary, the proposed method is simple but works robustly against various scenarios. However, I think the improvement from the baseline is a bit marginal, and not sure about the significance of the contribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3454/Reviewer_GiR4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3454/Reviewer_GiR4"
        ]
    },
    {
        "id": "Xhcdth4sJa",
        "original": null,
        "number": 4,
        "cdate": 1666665223643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665223643,
        "tmdate": 1666666053943,
        "tddate": null,
        "forum": "7D5EECbOaf9",
        "replyto": "7D5EECbOaf9",
        "invitation": "ICLR.cc/2023/Conference/Paper3454/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies data selection to construct a subset of full data, which is an important research topic in data-efficient deep learning. The authors piont out that prior works on data selection are always specially designed for certain cases, which makes it hard to apply them in practice, since realistic scenes are ever-changing and mismatch pre-defined ones. To address the issue, the concept of moderate sets and a new algorithm are proposed, where the data located in the middle of the data distribution is selected. Extensive experiments on multiple tasks demonstrate the effectiveness of the proposed concept and method. A strong baseline is created for future research. ",
            "strength_and_weaknesses": "Pros:\n- The motivation of this paper is clear and strong. Although data selection methods have been widely studied in different research topics, there may be no method that is designed to cope with a variety of scenarios at the same time. The paper also gives careful analyses of the vulnerability of existing methods to scenario changes. \n- The proposed method is simple but effective. The method Moderate-DS works with extracted deep representations, which avoids model retraining and access to network structures. The advantages may make it easier to apply in practice. \n- The experimental results are promising. Although the proposed method does not perform the best in all cases, the results show that it can achieve the best performance in most cases and is competitive in others. Moreover, the proposed concept can be applied to other methods to bring practical improvements.\n- The writing of this paper is overall great. \n\nCons:\n- Technical contributions of this paper seems trivial. Although this paper provides insights into the research community, the method implementation is not very technical. \n- More comparison methods can be added to enhance this paper. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity. There are some unclear points in this paper, which need to be addressed carefully. \n\n(1) The proposed moderate sets exploit the data points with scores close to the score median. It may be more common that we regard the mean/location as a proxy of a distribution. Why did this paper exploit the median? There is no enough discussion on this problem. \n\n(2) The baseline \u201cSelf-sup.-selection\u201d has a similar claim that we should select easy examples for small datasets and hard examples for large datasets, which could be mentioned in the discussions in Section 2.2.\n\n(3) The mutual information estimator is critical for the justification. One suggestion is to provide more technical details about it. \n\n(4) More baselines in data selection, e.g., [1-3], can be added to make the results more convincing. \n\n[1] Cody Coleman et al. Selection via Proxy: Efficient Data Selection for Deep Learning. ICLR 2020. \n[2] Kristof Meding et al. Trivial or Impossible\u2014Dichotomous Data Difficulty Masks Model Differences (on ImageNet and Beyond). ICLR 2022. \n[3] Vitaly Feldman and Chiyuan Zhang. What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation. NeurIPS 2020. \n\n(5) In practice, a dataset always is corrupted by various sources at the same time. Although the paper has presented the proposed method works well in the cases of clean data, corrupted images, and adversarial examples respectively. It will be interesting to see how different methods work when all these factors simultaneously exist. The setting may be more realistic. \n\n(6) Apart from the baseline Herding, the other baseline methods mainly stress that those \u201cdifficult\u201d examples are more helpful. For example, Forgetting selects the examples that are difficult to be memorized. In fact, these methods can also be revised to select \u201ceasy\u201d examples. It is interesting to see how these methods work after such revisions. \n\n(7) Figure 4 shows that the baselines EL2N and Grand can be improved with the concept of moderate sets. Could the paper provide more such evidence to strengthen the contributions of this paper? \n\nQuality. The quality of this paper is great. Overall, the writing is good. The descriptions of the motivation, research problem, and solutions are clear, following convincing experimental results.\n\nNovelty. The idea is novel to me and is much potential for future research. \n\nReproducibility. The descriptions of experimental settings are detailed. The reproducibility is satisfactory.\n",
            "summary_of_the_review": "This paper focuses on data selection to boost data-efficient learning. The motivation of this paper is strong. To address the issues of prior work, the concept of moderate, and a simple and effective method are proposed. Although the paper does not provide theoretical proofs for the method, both justifications from representation learning/information bottleneck and empirical evidence are provided. Besides, the proposed method is simple but with impressive results. The reviewer appreciates the inspiration this paper provides to the research community and its value in practical applications. Therefore, it is recommended for acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3454/Reviewer_vuQC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3454/Reviewer_vuQC"
        ]
    }
]