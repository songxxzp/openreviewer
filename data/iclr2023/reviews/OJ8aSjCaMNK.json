[
    {
        "id": "BEUf6gLt_s",
        "original": null,
        "number": 1,
        "cdate": 1666636148919,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636148919,
        "tmdate": 1668799750001,
        "tddate": null,
        "forum": "OJ8aSjCaMNK",
        "replyto": "OJ8aSjCaMNK",
        "invitation": "ICLR.cc/2023/Conference/Paper2076/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers $\\beta$-VAEs and treats the weight $\\beta$ of the KL-divergence term as a hyper-parameter. The authors develop a hypernetwork model which learns the optimal VAE parameters (i.e. weights of the encoder and decoder) simultaneously for all $\\beta$ values within a pre-specified range. The approach is experimentally tested on different datasets like MNIST, SVHN and others as well as for different VAE models like standard VAEs and hierarchical VAEs.",
            "strength_and_weaknesses": "Paper strengths:\nThe authors start from simple log-linear Gaussian VAEs and prove theoretically that a simple affine transform of the encoder/decoder weights parametrised by $\\beta$ provides the dependence of the optimal wights on the hyper-parameter $\\beta$. They conjecture that the hyper-network for complex, non-linear Gaussian VAEs is obtained by applying such affine transforms of the base weights in each layer of the encoder/decoder pair. The experiments compare achievable data likelihoods for VAEs (with  fixed architecture), when learned with fixed betas (aka response curve), with the one obtained from learning the hypernetwork. It is striking to see how good the corresponding rate curves are matched by the hypernetwork for different VAE variants.\n\nPaper weaknesses:\nThere are a few conceptual choices and issues which remain unclear for me:\nI was not able to fully follow the proofs given in the supplementary material for the log-linear Gaussian VAEs:\n- Step from (18) to (19,20): Taking the expectation over the data and maximising it w.r.t. to $\\mu$ will give a $\\mu_{MLE}$ that depends on the other model parameters. It remains unclear how to derive the gradients of the objective w.r.t. the latter.\n- Theorem 1: It remains unclear to me, why it is necessary to represent the parameter matrices of the model as products of \nmatrices? Why are then the affine transforms applied directly to the VAE weights for the non-linear models in (8,9)?\n\nIt remains unclear to me, why applying the affine transform in all layers of the encoder/decoder of non-linear VAEs is sufficient for obtaining the required hypernetwork. Is this may be related to the known rescaling properties of ReLU networks? Would this work also for other non-linearities?. And more general, can this approach be used also for non-Gaussian VAEs with discrete latent variables?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organised and clearly written in most parts. However, the technical material given in the supplementary material is indispensable for understanding the approach. Even considering this, there are quite some open questions remaining (see above).",
            "summary_of_the_review": "The paper proposes an interesting hypernetwork approach for estimating the optimal weights of $\\beta$-VAEs simultaneously for all $\\beta$ values within a pre-specified range for given encoder/decoder architectures. The technical part (especially in  the supplementary material) is sometimes hard to follow and there are a few questions and issues remaining open. My current recommendation is a lower bound. The final recommendation will depend on the clarification of these questions in the discussion phase.\n\nAfter rebuttal discussion: The clarifications provided by the authors answered almost all my questions and are convincing. I am raising my recommendation to 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2076/Reviewer_MZus"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2076/Reviewer_MZus"
        ]
    },
    {
        "id": "x0YZym1XQUy",
        "original": null,
        "number": 2,
        "cdate": 1666648473296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648473296,
        "tmdate": 1666648473296,
        "tddate": null,
        "forum": "OJ8aSjCaMNK",
        "replyto": "OJ8aSjCaMNK",
        "invitation": "ICLR.cc/2023/Conference/Paper2076/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes MR-VAE, an algorithm to train a VAE that corresponds to an interval of $\\beta$ values in a single training run.  The proposed method trains a hypernetwork that modifies the weights of a regular VAE at inference time for a given value of $\\beta$, thus eliminating the need to retrain a VAE for each value of $\\beta$.  The choice of hyperparameters for the hypernetwork is based on the analysis on linear VAEs, and the empirical results demonstrate that the method generalizes well to modern VAE architectures.",
            "strength_and_weaknesses": "**Strengths**\n* Very well-written paper with a clear motivation.\n* Strong empirical results on diverse datasets.  In particular, the paper addresses the important questions that naturally arise: (1) does MR-VAE introduce performance degradation?  (2) does MR-VAE generalize to larger, complicated architectures?  (3) how sensitive is MR-VAE to the choice of its (only) additional hyperparameter $(a, b)$?  The experiments convincingly and positively answer all these questions.\n\n**Weaknesses**\n* I did not find any particular weakness.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarify & Quality**\nThe paper is very well-written, and the quality of the exposition is high.\n\n**Novelty**\nWhile the question studied in the paper is not very novel, the paper provides a clean algorithm with good justification that also works well in practice.  The analysis of optimal response functions on linear VAEs was insightful.",
            "summary_of_the_review": "This paper provides an answer to a somewhat obvious but practically important question of whether it's possible to train a VAE that can operate on a continuum of rate-distortion points.  The presentation is clear, with clear experimental results to back up the usability (i.e. R-D performance, computational/memory cost, stability to hyperparameters).  I believe this is a valuable contribution to the field.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2076/Reviewer_47TW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2076/Reviewer_47TW"
        ]
    },
    {
        "id": "aHsAekpXnqS",
        "original": null,
        "number": 3,
        "cdate": 1667111881893,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667111881893,
        "tmdate": 1667111881893,
        "tddate": null,
        "forum": "OJ8aSjCaMNK",
        "replyto": "OJ8aSjCaMNK",
        "invitation": "ICLR.cc/2023/Conference/Paper2076/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces Multi-Rate VAE, which is a hyper-network that outputs the parameters of a $\\beta$-VAE network, when given a value of $\\beta$. The rate-distortion curve created using the the networks computed from this hyper-network matches the one that is obtained by repeatedly training $\\beta$-VAEs for different values of $\\beta$. This hyper-network also has similar number of parameters as that of the $\\beta$-VAE network it outputs, and has similar training time. The construction of the hyper-network is theoretically principled, because the paper proves that the hyper-network can output the optimal $\\beta$-VAE when the VAE is linear.",
            "strength_and_weaknesses": "Strengths:\n1. The proposed hyper-network construction seems to be theoretically principled. This is because for linear VAEs, the paper provides a theorem that proves that indeed the optimized hyper-network can output parameters for the optimal linear $\\beta$-VAE.\n2. The hyper-network is of practical importance since it bypasses the need to retrain $\\beta$-VAEs for different values of $\\beta$, while fine tuning for optimal $\\beta$.\n3. The hyper-network has similar size and training time as the $\\beta$-VAE it outputs.\n4. The results have been empirically verified on MNIST, OMNIGLOT, CIFAR10 and CelebA datasets, where it is shown that the rate-distortion curve created using the the networks computed from this hyper-network matches the one that is obtained by repeatedly training $\\beta$-VAEs for different values of $\\beta$.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and the idea is novel.",
            "summary_of_the_review": "I think the idea of hyper-networks outputting $\\beta$-VAE for different values of $\\beta$ without the need for retraining from scratch for each $\\beta$, is of practical importance. Furthermore, the proposed framework is both theoretically principled and empirically verified to work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2076/Reviewer_fEL1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2076/Reviewer_fEL1"
        ]
    }
]