[
    {
        "id": "bkfpbw1x_39",
        "original": null,
        "number": 1,
        "cdate": 1666506558272,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666506558272,
        "tmdate": 1666506558272,
        "tddate": null,
        "forum": "ooqH4D9Xys",
        "replyto": "ooqH4D9Xys",
        "invitation": "ICLR.cc/2023/Conference/Paper1344/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new framework for data augmentation that generalizes other existing data augmentation methods by incorporating Bayesian inference methods. Empirical performances on common benchmark datasets suggest that the LatentAugment methods outperforms other Adversarial benchmarks not only as measured by test accuracy in classification tasks, but also by training cost.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper is well-organized, with each section clearly stating what it does. The contribution of the paper has been clearly outlined.\n\n2. Experiments look sound and justify the main claim of the paper.\n\nWeaknesses:\n\n1. The level of novelty is relatively low. On a high-level, LatentAugment appears to be a straightforward application of the EM algorithm over the logarithmic losses defined with respect to conditional probability. The intuition behind the choice of loss function is also a bit vague. It would be helpful to explain why softmax is used and how the conditional probability relates to the goal of the LatentAugment algorithm.\n\n2. Except for the classification task, no other task has been used to test the performance of LatentAugment. It would be helpful to incorporate other relevant tasks where data augmentation is helpful (e.g. inpainting, image generation) to see how LatentAugment is useful in general.\n\n3. CIFAR, SVHN, and ImageNet are public datesets very commonly used across different image tasks. Has there been any tests on other less commonly used datasets where LatentAugment might be helpful? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very clearly. However, the originality of the work should be more elaborated. ",
            "summary_of_the_review": "My personal evaluation would be a 5. I'd be happy to adjust my score, if the authors respond to the concerns accordingly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1344/Reviewer_AEKr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1344/Reviewer_AEKr"
        ]
    },
    {
        "id": "LaRDYLsEtQ1",
        "original": null,
        "number": 2,
        "cdate": 1666512865353,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666512865353,
        "tmdate": 1666512918566,
        "tddate": null,
        "forum": "ooqH4D9Xys",
        "replyto": "ooqH4D9Xys",
        "invitation": "ICLR.cc/2023/Conference/Paper1344/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper addresses the learn optimal policy for data augmentation on image classification task.  Learning the optimal augmentation policy, which is the latent variable, interesting research problem. The paper proposes a simple via EM-based method (call LatentAugment) to estimate the probability of optimal policy via conditional probability of the policy customized for given input and network model. It also provides the theorical analysis to show some exisitng methods (AdvAA and UBS) as the special cases. Experiments are on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets using different network architectures showing the encouraging results that it outperforms most of existing methods.  ",
            "strength_and_weaknesses": "[Strength]\n\nThe paper is well-written and easy to follows. \n\nNovelty and with theoretical analysis provided.  \n\nThe model looks simple and computational efficiency, but would be good if with evidence supported.  \n\nThe results are encouraging with ablation study provided.  \n\n[Weakness]\n\nThe claim on computational efficiency needs to be supported with theoretical or empirical results. For example, it would improve a paper if the authors can provide the computational comparison to other methods.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the idea is novel to me.  Code is provided in the supplementary (but I do not have chance to test the code). ",
            "summary_of_the_review": "Overall, I think the method is novel to me and most of claims are well-supported except one in the weakness. The paper is in the form can be published.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1344/Reviewer_M9Bp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1344/Reviewer_M9Bp"
        ]
    },
    {
        "id": "kr9gC3rPxu",
        "original": null,
        "number": 3,
        "cdate": 1666663148997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663148997,
        "tmdate": 1666663148997,
        "tddate": null,
        "forum": "ooqH4D9Xys",
        "replyto": "ooqH4D9Xys",
        "invitation": "ICLR.cc/2023/Conference/Paper1344/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a simple yet effective data augmentation method, which is called LatentAugment.",
            "strength_and_weaknesses": "Strength:\n\nThe proposed LatentAugment is straightforward and reasonable;\n\nCompared to some previous methods, the proposed method is efficient in terms of computation cost;\n\nWeakness:\n\nThe performance gain is relatively small. According to Table 1, UBS is also an efficient method, while the proposed method is only slightly better than UBS with more computation cost. Given the results, I'm curious about result of K=1 for Table 1, from which we may better compare the training cost and performance of proposed method and UBS.\n\nGiven that the results are slightly better than previous method, more experiments are needed. For example, instead of vanilla fully-supervised learning, how will the proposed method benefit some settings where data augmentations might be really important, including few-shot learning, transfer learning, meta learning, etc. \n\nSome presentation needs to be improved. For instance, Figure 2 looks incomplete because some bars seems to be missing without clear explanation. I would suggest the authors either compared all the related methods under all the same settings, or explain clearly about the figure/results.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the proposed method has good reproducibility and some novelty, while the quality of the paper needs to be improved. Specifically, I believe more experiments are needed, and the presentation needs to be polished.",
            "summary_of_the_review": "Although the paper propose a simple and effective method, I believe the paper needs improvement before getting accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1344/Reviewer_AC1v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1344/Reviewer_AC1v"
        ]
    },
    {
        "id": "xvRNCU0neix",
        "original": null,
        "number": 4,
        "cdate": 1666668588898,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668588898,
        "tmdate": 1666668588898,
        "tddate": null,
        "forum": "ooqH4D9Xys",
        "replyto": "ooqH4D9Xys",
        "invitation": "ICLR.cc/2023/Conference/Paper1344/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new algorithm for data augmentation based on the same transformations used for autoaugment and follows-up papers. The algorithm considers policies that are composed of two sequential transformations selected from 16. Thus in total there are 256 different policies that can be selected. The probabilities of selecting those policies are latent variables that are estimated during training with a expectation maximization approach. In addition the algorithm can estimate also the probabilities of a policy conditioned to the given sample by normalizing the corresponding losses. Results show that the proposed algorithm outperforms previous approaches in most of the cases.",
            "strength_and_weaknesses": "\\+ The proposed approach seems to generate more meaningful augmentations than previous approaches as shown in the experimental results\n\n\\- Different parts of the paper are disconnected and difficult to follow. See clarity for more details.\n\n\\- The differences between this work and adversarial autoaugment (advAA) seem minimal. As shown by theorem 2.1, with uniform unconditional probabilities and $\\sigma \\rightarrow \\infty$ the proposed approach is advAA. As shown in table 3, the contribution of considering variable unconditional probabilities is quite low and in the order of the std. Not sure about the difference between $\\sigma = 1$ and a high value. It would be interesting to see table 3.\n\n\\- Differences in classification accuracy are relatively small. How can we verify that those differences are not due to a larger batch sizes or other implementation details",
            "clarity,_quality,_novelty_and_reproducibility": "\\- Clarity: the presentation of the paper should be improved in several aspects.\nIn the introduction I consider that the authors should not limit the presentation of previous methods to AutoAugment, even if the other approaches are mentioned in related work. I do not see many similarities between the proposed work and the Bayesian augmentations of Tran et al. (2017). Fig. 1 has a disconnect with the proposed formulation. For instance in Fig. 1 the E-step is applied on the multiplication of the conditional probabilities of the policies and the losses, while in Equ. 4 the conditional probabilities are multiplied by $P(y|o_z(x),\\theta)$. In method, in the third line the authors mention the use of random augmentations, but then $\\pi_z$ is used. In 2.1 first equation is presented as Bayes' rule, but in my understanding is just probability normalization. Algorithm 1 contains too much text and some definitions of variables are inaccurate. Equ. 3 seems a bit strange as it normalizes $h_z$ which is based on already normalized probabilities, thus twice exponentiation which might not be ideal for gradient propagation. \n\n\\-Quality: the method seems to outperform previous approaches, however the differences are relatively small. It would be important to provide the code in order to make sure that the improvements are due to the algorithm and not different and better hyperparameters.\n\n\\- Novelty: as mentioned previously, the paper seems very similar to advAA, thus the novelty is limited. Authors should explicitly present the improvements with respect advAA in related work as well with experiments.\n\n\\- Reproducibility: authors provide the hyperparameters for the trained model. However, it would be important also to provide the code and make sure that the obtained results are due to only the algorithm and not different hyper-parameters than previous work.",
            "summary_of_the_review": "I consider this paper a valuable contribution for ICLR. However there are several points that the authors should improve for acceptance.\n\n\\- the writing the paper seems rushed, with some parts that are disconnected and other that should be improved and clarified (see clarity).\n\\- the differences with advAA seems minor. Authors should present and evaluate the differences with advAA.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1344/Reviewer_LaAK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1344/Reviewer_LaAK"
        ]
    }
]