[
    {
        "id": "_StZphCD9n7",
        "original": null,
        "number": 1,
        "cdate": 1666382184300,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666382184300,
        "tmdate": 1668535409399,
        "tddate": null,
        "forum": "8CJrjp73sfk",
        "replyto": "8CJrjp73sfk",
        "invitation": "ICLR.cc/2023/Conference/Paper4065/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method for reducing the memory needs of the backward pass of a neural network. It does so by quantizing the gradients of the activation functions meaning that they now only need a few bits to store as opposed to the standard 32-bit floating point representation.\n\nThe few bit representations are computed to minimize the l2 distance from the exact gradients. The paper proposes an algorithm based on dynamic programming to compute them. This algorithm only needs to be run once for each bit count/activation function pairing.\n\nThe experiments confirm that these few-bit representation do not significantly affect the training dynamics so the memory reduction has no downside.",
            "strength_and_weaknesses": "The algorithm for finding the quantized representation is quite clever. Its makes good use of the dynamic programming principle. I wonder, however,\n* Is l2 loss the right metric to optimize? The goal is to minimize the bias that we are adding to the gradient updates and perhaps a different metric, maybe one that is optimized to minimize this bias would perform better.\n* Is dynamic programming needed here? Could we simply optimize the values and the beginnings/endings of the segments using a simple optimizer?\n* What is the resolution of t used in the experiments (and what interval [A, B])?\n\nThe paper makes a convincing case that the proposed method indeed reduces the memory footprint of the gradients and that this reduction does not significantly impact the training dynamics of the network. It would be nice to show concrete benefits of this memory reduction.\n* As stated in the paper, the memory reduction does not result in faster backward passes. Instead, it enables training with larger batch sizes. Does training with larger batch sizes then result in a speedup?\n* In the Abstract, it is stated that \"Memory footprint is one of the main limiting factors for large neural network training.\". Does it enable the training of larger networks that have better accuracy?\n* In the conclusion, it is stated that the method results in a 20% reduction in memory usage. How is this 20% computed?\n* In Appendix B, we see the potential increase in batch size. Is this table generated by measuring the size of the model running on a GPU or is it calculated according to the formula in Section 2? There may be significant differences in the theoretical memory footprints and the real memory footprints of these models.\n* The paper should also compare against low-precision neural networks, for example 16-bit floating point networks. It is claimed that the savings are `complementary and can be used together', but this is not fully true because the savings don't combine multiplicatively. The savings in a low-precision network are a smaller proportion of the overall memory cost than in a high-precision network.",
            "clarity,_quality,_novelty_and_reproducibility": "Clairty:\nThe paper is well-written. The method and the contributions are clear. Perhaps the description of the quantization algorithm could be improved. Especially equations 5-7 are could be clearer.\n\nQuality:\nThe work is good quality and the reasoning is sound. The experiments could be improved as mentioned above.\n\nNovelty:\nThe idea of quantizing the activations in the backward pass is novel to my knowledge.\n\nReproducibility:\nCode is supplied along with the submission. I did not try to run the code, but I think it shouldn't be difficult to reproduce the results.",
            "summary_of_the_review": "The paper is well written and good quality. My main issue is the thoroughness of the experiments. I would like to see the total memory reduction measured at runtime and it would also be good to see comparison against low-precision approaches (that use low-precision throughout the network not only the gradients of the non-linearities).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4065/Reviewer_1fcj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4065/Reviewer_1fcj"
        ]
    },
    {
        "id": "GQaTRw0V6F",
        "original": null,
        "number": 2,
        "cdate": 1666541949637,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541949637,
        "tmdate": 1666541949637,
        "tddate": null,
        "forum": "8CJrjp73sfk",
        "replyto": "8CJrjp73sfk",
        "invitation": "ICLR.cc/2023/Conference/Paper4065/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to use point-wise activations to reduce the memory cost of back-propagation. An analysis is proposed to determine quantization levels minimizing a quantization MSE metric. Experimental results using GeLU and similar activations using several bitwidths are cometitve.",
            "strength_and_weaknesses": "The idea is interesting however, I have a couple of concerns with the analysis.\n\n1) The presented analysis in eq. (3) - (7) is not novel. It is simply re-deriving the famous Lloyd-Max algorithm [1].\n[1] Lloyd, S. P. \"Least square quantization in PCM. Bell Telephone Laboratories Paper. Published in journal much later: Lloyd, SP: Least squares quantization in PCM.\" IEEE Trans. Inform. Theor.(1957/1982) 18 (1957): 5.\n\n2) How does the method compare to low-precision integer quantization, which can also be optimized (see [2])? Unlike the proposed codebook quantization scheme, low-precision integer representation can use ultra-fast tensor cores on GPUs. Can the authors provide some comparisons taking that into account?\n[2] Sakr, Charbel, et al. \"Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training.\" International Conference on Machine Learning. PMLR, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The method is clear. I appreciate the authors spending some time explaining the implementation details. I have some concerns about novelty that I listed above.",
            "summary_of_the_review": "The presented work is an interesting approach to code-book quantization of activations. The paper is well written and has a satisfactory amount of details. However, I have some concerns regarding the novelty of the proposed quantization scheme.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4065/Reviewer_KDUc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4065/Reviewer_KDUc"
        ]
    },
    {
        "id": "kpsmDWGxHG7",
        "original": null,
        "number": 3,
        "cdate": 1666587778962,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587778962,
        "tmdate": 1666587870340,
        "tddate": null,
        "forum": "8CJrjp73sfk",
        "replyto": "8CJrjp73sfk",
        "invitation": "ICLR.cc/2023/Conference/Paper4065/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a method to perform activation compressed training (ACT) for pointwise non-linear activation functions. Instead of storing a quantized version of the input x, the proposed approach stores a quantized version of the activation function's gradient f'(x), which is multiplied with the gradient in the backward phase. The paper proposes a nonlinear quantization method for f'(x) with a dynamic programming method to determine the optimal quantized approximation. Quantization can be reduced to table lookup then. The proposed method is evaluated on language model pretraining, text-to-image generation, and image classification tasks, while the proposed method is slightly better than existing ACT methods under the same bitwidth.",
            "strength_and_weaknesses": "Strength:\n- Reducing training memory footprint is an important problem.\n- The proposed method technically sounds, and should be the correct way to deal with nonlinearities. \n\nWeaknesses:\n- The paper still needs polishing. For example, the introduction is too brief.\n- The improvement is incremental.\n- It is somewhat unclear whether the comparison with ActNN is fair. ActNN compresses both the linear and nonlinear layers. Does the proposed method also do so? (i.e., combine the proposed method for nonlinear layer and ActNN for linear layer. I think the combined result is what of practical interest.)\n- Table lookup can be very expensive. The time consumption is not reported. I'm not sure if the proposed nonlinear quantization is practical.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clearly written and reproducible. Novelty might be somewhat thin.",
            "summary_of_the_review": "The paper deals with an important problem in a reasonable manner. However, the paper might require more work to fully demonstrate its significance before publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4065/Reviewer_Wx7m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4065/Reviewer_Wx7m"
        ]
    }
]