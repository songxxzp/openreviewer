[
    {
        "id": "16TtOdvaY-n",
        "original": null,
        "number": 1,
        "cdate": 1666505605166,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666505605166,
        "tmdate": 1670561773330,
        "tddate": null,
        "forum": "H4Ncs5jhTCu",
        "replyto": "H4Ncs5jhTCu",
        "invitation": "ICLR.cc/2023/Conference/Paper2735/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of how model accuracy relates to sample efficiency a class of model-based reinforcement learning algorithms, that is, model-based value expansion algorithms. The paper conducts experiments to show that the improvement of using the oracle dynamics model over a learned dynamics model is not too much, and there is a phenomenon of diminishing return for both, which means short horizons suffice to achieve the best sample efficiency, while model-free baselines are only slightly worse. Thus, the authors conclude that model-based value expansion algorithms are not limited by the accuracy of the learned dynamics model. ",
            "strength_and_weaknesses": "Strength:\n\n- The preliminary section covers many components and greatly helps the reader to understand the background. The paper is well organized. \n- The problem this paper studies is indeed important. Diagonizing the bottleneck for model-based reinforcement learning algorithms can be quite difficult. \n\nWeakness:\n\n- There are a lot of missing details in Section 2, which makes it difficult to understand the exact algorithm this paper studies:\n  - How is $V$-function in Eq (3) evaluated? Is it a different network than $Q$, as in [1], or simply sampling an action $a' \\sim \\pi(\\cdot| s')$ and using $Q^\\pi(s', a') - \\alpha \\log \\pi(a' | s')$ as the result, as in [2]? More importantly, how is $V$-function evaluated in Eq (6)? \n  - The Retrace target $Q^H_\\text{retrace}$ doesn't encompass Equation (3): it doesn't take the entropy of the policy into consideration. \n  - If $\\lambda \\neq 1$, $Q^H_\\text{retrace} \\neq Q^H$ and the paper doesn't provide the used $\\lambda$. \n- The paper only rules out the possibility of compounding error, but there are a lot of other factors which might affect the efficiency of model-based expansion. An important topic in estimating the value functions is how the bias-variance trade-off is taken into consideration. As I didn't see the paper makes effort to reduce the variance of $Q$ value estimation, I assume this paper only considers Monte-Carlo estimation, which is known to suffer from high variance. Retrace, which utilizes $V$-function to reduce variance and serves as a baseline in this paper, is another method to estimate the $Q$ values. One even simpler baseline is to average the $Q$ values over a few trajectories starting from the same $(s, a)$, given that the samples from the model are \"free\". Consider an extreme scenario, where we use the exact expectation of $Q^{\\pi, H}(s, a)$ (so that the variance is reduced to 0) to do action-expansion or value-expansion. If the phenomenon of diminishing returns still exists in this scenario, I'll be convinced. The conclusion that I can draw from the paper is that with Monte-Carlo estimation of $Q$ values, an oracle dynamics model with a longer horizon might hurt. \n- Even in the case we're using an oracle dynamics model, the diminishing return phenomenon is not surprising either. One can expect that the distribution of $Q^H(s, a)$ converges to $Q^\\infty(s, a)$ at some rate when $H$ goes to infinity. (Indeed, this is what Figure 2 wants to show.) So even with a perfect $Q$ estimator with zero variance, the diminishing return may still exist. It will surprise me if the performance will drop, though. \n- Maybe experiments show that 100 particles suffice to evaluate the Wasserstein distance between $Q^H(s, a)$ and $Q^\\infty(s, a)$. Please also report the mean and variance of $Q^\\infty(s, a)$ and $Q^H(s, a)$ so that the readers can get a sense of what the Wasserstein distance means. \n- The author \"conclude that the limitation of model-based value expansion methods is not so much the model accuracy of the learned models.\" However, in Figure 1.a,  SAC-CE (oracle model) performs significantly better than SAC-CE (NN model) in HalfCheetah, so the accuracy of the learned models **can** be a problem and can be an even greater problem than the choice of horizon under certain scenarios. What I can agree with is that it's not the only limitation. \n- This paper doesn't have any theory and doesn't find the root causes for the phenomenon either. \n\n\n\n[1]: Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, Haarnoja. etc.\n[2]: Soft Actor-Critic Algorithms and Applications, Haarnoja. etc. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Writing: \n\n- The paper is well-written in many places, but unfortunately one of the key parts, Section (2), is confusing. \n- Notions are not very consistent: It defines $Q^{\\pi, H}$ but never uses it. Eq (4) uses $V_t$ and $Q_t$ which are not defined. \n- Please define the gradient mean and gradient variance in Section 3.3.\n\nTypos:\n\n- Page 4: \"from real the environment\" -> \"from the real environment\"\n\nReproducibility: The authors promise to release the code upon publication so I don't think it will be a concern. \n\nNovelty: As far as I can see, this paper is novel. I'm not aware of existing work on analyzing how model accuracy affects sample efficiency. However, this paper might not be the first to reveal the phenomenon of diminishing returns, at least not for learned dynamics models. \n\n",
            "summary_of_the_review": "My major concerns with this paper are that the variance of the $Q$ value estimation is not taken into consideration. The writing can be more precise and less confusing. I don't recommend acceptance in the current form but am glad to increase my score if my concerns can be addressed. \n\n--- \nUpdate: As the authors provide more experiments on the bias-variance trade-off, addressing my major concern, I'd like to increase the score to 6. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2735/Reviewer_kpea"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2735/Reviewer_kpea"
        ]
    },
    {
        "id": "FD_5IZeTpF",
        "original": null,
        "number": 2,
        "cdate": 1666601661306,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601661306,
        "tmdate": 1666601661306,
        "tddate": null,
        "forum": "H4Ncs5jhTCu",
        "replyto": "H4Ncs5jhTCu",
        "invitation": "ICLR.cc/2023/Conference/Paper2735/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyses model-based RL methods and studies the impact of the model-based rollout horizon H. The main finding of the paper is that longer horizon do not guarantee better results (they have diminishing returns, and may even be detrimental). This is even true for oracle models (perfect models!), so it is not caused by compounding model errors.\n\nHalf the experiments focus on identifying the above effect (larger H is not always better), and the other half of the experiments try to find why longer horizons do not always perform well. No answer is found for that last question, and the paper concludes that future research needs to focus on why longer horizons do not benefit model-based RL.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper contains information that clearly needs to be known by the RL community. Recent algorithms, such as Dreamer and MuZero, leverage models and do planning/rollouts in them. It is important to know that there is possibly a problem in RL that prevents these models, as good as they may be, from benefiting the agent.\n\nSmall weakness:\n\n- Some directions have been pursued as to why longer horizons are not always beneficial, but these directions did not bear fruit. The paper therefore identifies a problem without a direction for solving it.\n- The experiments are based on SAC and DDPG, with their losses modified to include model-based rollouts (in ways explained in the paper). It would have been very interesting to also see experiments with existing implementations of SOTA algorithms, such as DreamerV2, and with plugging the oracle model in them. This would have allowed to be certain that the diminishing return of H does not come from an implementation error in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "- Quality: medium-high: the quality is overall good, but the weakness above regarding existing implementations of algorithms raises the question of \"how can we be certain that there is no bug in the experiments?\"\n- Clarity: high. The paper does an amazing job at explaining model-based RL and its variants, and where rollouts can be used. This is material to be used in a classroom. The rest of the paper flows well and the results are presented in a clear way. Small remark: the figures would benefit from one extra sentence that states what we have to see in them. Instead of \"these are the variances of the losses\", it is useful to have \"this are the variances of the losses. They seem normal and longer horizons do not appear to have an effect of them.\"\n- Originality: high. This work questions an aspect of model-based RL that was not questioned before, and found a problem.\n- Reproducibility: medium-high. It would be important to have the source-code used in the paper to inspect it for correctness.",
            "summary_of_the_review": "Clear information that needs to be available to the RL community. There is only a minor problem of whether model-based RL is limited in some way, or there is a bug somewhere.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2735/Reviewer_5jBe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2735/Reviewer_5jBe"
        ]
    },
    {
        "id": "qixPH65RhJ",
        "original": null,
        "number": 3,
        "cdate": 1666668785889,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668785889,
        "tmdate": 1669947774314,
        "tddate": null,
        "forum": "H4Ncs5jhTCu",
        "replyto": "H4Ncs5jhTCu",
        "invitation": "ICLR.cc/2023/Conference/Paper2735/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors conduct an extensive empirical study using simulated control tasks on a family of model-based RL algorithms to answer the following question: how much more sample efficiency can be gained by improving the learned dynamics models. The results from the experiments are contrary to the common belief that regards the accuracy of the dynamics model as a limitation of model-based approaches. The experimental results show that learned models provide on-par improvements compared to perfect models and the improvements diminishes with increasing horizon. Furthermore, the off-policy method RETRACE that does not rely on dynamics models and thus has better computational complexity archives similar sample efficiency.  \n",
            "strength_and_weaknesses": "The results from the paper are extensive and are relevant to the model-based RL community. And the observations that a learned model achieves efficiency similar to a perfect model and RETRACE is a strong baseline for model-based algorithms are interesting. \n\nHowever, I think the empirical results are kind of expected. The authors focused on Q function estimation using a single trajectory trajectory (Eq (3)), which is known to suffer from large variance when horizon increases. Many existing methods are designed to specifically tackle this problem [1][2]. In the experiments, the authors can explore this direction by reducing the variance in the true and learned dynamics model and using these variance reduction methods under the value expansion framework. Furthermore, the phenomenon of diminishing returns can be simply attributed to the exponential decay due to the reward discount factor.\n\n\nWould the authors please clarify the sentence on In page 7, \u201cwe show that the critic gradients are probably not the source of the diminishing returns.\u201d?\n\nTypo: second line in Introduction, samples one -> sample on\n\n[1] Grathwohl, Will, et al. \"Backpropagation through the void: Optimizing control variates for black-box gradient estimation.\" arXiv preprint arXiv:1711.00123 (2017).\n\n[2] Cheng, Ching-An, Xinyan Yan, and Byron Boots. \"Trajectory-wise control variates for variance reduction in policy gradient methods.\" Conference on Robot Learning. PMLR, 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. The experimental results are extensive and seem to be solid. ",
            "summary_of_the_review": "Due to the slight lack in significance as discussed above, ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2735/Reviewer_vr7K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2735/Reviewer_vr7K"
        ]
    },
    {
        "id": "9ILi2Q74EUD",
        "original": null,
        "number": 4,
        "cdate": 1667214551312,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667214551312,
        "tmdate": 1670232174960,
        "tddate": null,
        "forum": "H4Ncs5jhTCu",
        "replyto": "H4Ncs5jhTCu",
        "invitation": "ICLR.cc/2023/Conference/Paper2735/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper has a clear focus, aiming to address the question \"is model accuracy a bottleneck to value-expansion methods in RL?\" The paper approaches this by studying the performance of two algorithm families, SAC and DDPG, in several continuous control tasks, using both a learnt dynamics model, and an oracle (exact) dynamics model.\n\n\n\n--- POST-REBUTTAL\n\nI thank the authors for their detailed response.\n\nMy initial review mentioned that this is a potentially impactful contribution that aims to address an important question in model-based reinforcement learning. I highlighted several reservations with the initial draft, including: (i) lack of architecture descriptions/hyperparameters/general concerns around reproducibility, (ii) the potential for further experiments to strengthen the evidence for the main hypothesis claimed in the paper, and (iii) the specificity of the findings to the agents/environments investigated in the paper.\n\nIn their response, the authors have included more qualifications in the paper regarding (iii).\n\nThe inclusion of the further DDPG experiments is a good step towards checking the bias-variance explanation of their findings with regard to (ii) as well; this definitely strengthens the paper in my view. I think there is still scope for further experimentation here, but this shouldn't be a barrier to acceptance in my view. In particular, the authors mentioned that evaluating the bias-variance trade-off further, or the impact of an inaccurate value function, by extending the roll-out horizon beyond H=30, was computationally intractable. The inclusion of the run-time table in the appendix makes this clear, and I think adds useful context to the experiments carried out in the main paper. However, using a smaller discount factor would make H=30 sufficient to essentially eliminate the bias from the inaccurate value function, and this could have been an interesting additional experiment to run.\n\nRegarding (i), the authors have included more details regarding agent hyperparameters, and the neural-network-based dynamics model. I think this is now at the level of detail where reproducibility is realistic. The paper could still be strengthened by including more details about any hyperparameter sweeps carried out e.g. in training the NN dynamics model.\n\nAside from these larger points, the authors' response also clarified many of the more minor queries I initially raised.\n\nBased on the factors above, I have increased my rating for the paper, and would argue for it be accepted.\n\nI think there is still scope for the clarity of the paper to further improved. One aspect in particular that came up in discussions with other reviewers and the meta-reviewer is that the messaging of the core take-aways of the paper are perhaps a bit unclear. In some sense, it is arguably not surprising that there are diminishing returns from increasing the rollout horizon (once H >> 1/(1 - gamma), for example, the bias of the value function has essentially been eliminated). In contrast, the observation that moving from a learnt model to an oracle model has limited impact on algorithm performance seems like a much more impactful take-away.",
            "strength_and_weaknesses": "This is an empirical paper, which seeks to address a common belief in model-based RL: that model accuracy is a bottleneck to performance in value-expansion methods in RL, prohibiting us from doing longer-horizon expansions. The singularity of the paper's focus is a strength, and I found the work to be clearly presented. Papers such as this have the potential to be highly useful to the community, in carefully examining commonly-held beliefs, and impacting the practice of model-based RL in the future. However, I do have several comments on areas where I believe the paper currently falls short, which is the basis for my current rating. I would be open to revising this rating based on the authors' response.\n\n_Scope_\n\nI appreciated the inclusion of several different agents, and several environments. While it is always possible to include more comparisons in a paper such as this, and a line needs to be drawn somewhere, I think there are a few important comparisons which, even if not included in the paper, should be discussed.\n\nDeterministic policies. Although the paper considers the DDPG agent in Figure 1, this agent does not seem to be investigated further in the paper. However, I think analyzing DDPG gradients in a similar way to the SAC analysis would be an interesting additional data point for the paper. If the DDPG policy and model are deterministic, this removes all stochasticity from the gradients, and therefore potentially provides a more nuanced answer to the question: \"do diminishing returns stem from increased gradient variance\", which is not currently answered conclusively in the paper.\n\nError in bootstrapped value function. As far as I can tell, all bootstrapped value functions used are those learnt by the SAC agents. Since one of the main hypotheses as to what drives the change in performance as a function H is a bias/variance trade-off for gradient estimation, it would have been interesting to see the effects of more/less accurate bootstrap value estimates on performance.\n\nOther types of environment. The paper focuses on deterministic, continuous control tasks. I would not necessarily expect experiments beyond this in a conference paper, but some discussion as to whether the authors expect the findings to generalise beyond this would be very valuable. For example, could stochastic transitions affect the bias/variance trade-off and lead to very different performance as a function of H.\n\nFurther values of H. The largest value of H considered in the paper is 30. With a discount factor of 0.99, this still attaches a weight of 0.99^30 = 0.74 to the bootstrapped value function. It would have been interesting to see results with value expansion closer to Monte Carlo simulation (i.e., with a value of H so that gamma^H is reasonably close to 0).\n\nTo emphasize, I don't think it's necessary to run further experiments relating to each of these comparisons (although doing so would obviously increase the impact of the paper, and the weight of any conclusions drawn), but I think discussion, and possible qualification, of the paper's findings with these comparisons in mind would strengthen the paper, and make it more useful for the community in general.\n\n\n\nSome additional comments for each section are provided below\n\n_Section 2_\n\nIn the definition of Markov decision processes given here, some non-canonical choices are made: state/action spaces as subsets of R^n, transitions having probability density functions (which is actually not satisfied by the (deterministic) continuous control tasks considered later in the paper). How important are these assumptions?\n\nEqn (2): Should there be a stop gradient around V^pi(s') in this case?\n\nJust below Eqn (6), the authors claim that when mu = pi, the RETRACE estimate reduces to the H-step action-value target. This is not true, because c_{t-1}V_t - c_t Q_t, which appears in Eqn (6), does not evaluate to 0.\n\n_Section 3.2_\n\nI am unsure exactly what is intended to be taken away from the Wasserstein comparison. It is clear that the distance should go to 0 as H increases, but it is not clear to me why these Wasserstein distances are of direct importance to studying the source of diminishing returns.\n\nThe comparison of estimator variance as H varies feels more related to the topic of the paper, but further discussion would be valuable here: while variance increases with H, is the increase in variance correlated with the extent of diminishing returns observed?\n\n_Section 3.3_\n\nI found the results of this section very difficult to interpret. As far as I can see, the authors do not define the mean and standard deviation of the gradients here (is this on a per-coordinate basis?) Additionally, the scale for the gradient means the y-axis seems too large, so that the curves essentially look like the line y=0 in most panels.\n\nThe authors reach the conclusion that gradient variance may contribute to the poor performance of large horizons in actor-expansion methods, but is not likely to be the cause of diminishing returns in critic-expansion methods. More discussion of these results would strengthen the paper (what makes the variance so catastrophic in some environments like HalfCheetah, and not in others)? If gradient variance doesn't explain diminishing returns for critic-expansion methods, what alternative hypotheses should be investigated further?\n\n_Model-free vs. model-based comparison_\n\nOne of the main takeaways that the authors present is that the model-free expansion based on RETRACE is competitive to model-based methods. I agree with this conclusion when the number of learner steps is taken to be equal in both cases, but presumably one of the main advantages of using model-based expansion is that if the computational budget is available, more updates can be performed by sampling new experience from the model (while RETRACE is constrained just to use collected trajectories). This aspect of the comparison seems to be absent from the paper currently, and I think should at least be mentioned as another possible reason to prefer model-based value expansion over model-free methods.\n\n_Other comments_\n\np1: \"samples one\" -> \"samples from\"\nThere are a few instances of citations to recent papers for well established concepts e.g. Deisenroth et al. (2013) for model-based RL, and (Peyr\u00e9 & Cuturi) (2019) for the Wasserstein distance.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I appreciated the paper's aim to thoroughly investigate a single, well defined question. I generally found the writing to be clear. I think the question studied here is highly relevant to the RL community, and I think careful empirical studies such as these into such questions are potentially highly valuable. However, I believe there currently some issues with reproducibility, which I hope the authors can address.\n\nThe authors commit to open-sourcing the code. However, there is very little discussion of hyperparameter settings and other experimental details in the paper. In particular, no hyperparameter settings are described for the SAC and DDPG agents, or for the RETRACE baseline (e.g. lambda), or for the neural network dynamics model, as far as I am aware. These details will be crucial for reproducibility of the paper, and just as importantly, the interpretation of the results presented in the paper.\n\nAs an example, it is difficult to compare the RETRACE baseline with the model-based approaches without knowing what kinds of hyperparameter sweeps were carried out in the course of the research.\n\nIn addition, there don't appear to be any details of the neural network dynamics model used in the paper, but details on this (is it a deterministic or stochastic model) are crucial for understanding the results.\n\nOther important experimental details \"in practice, we use a large rollout horizon based on the discount factor \u03b3\" are also currently omitted, and should be included in the appendix.",
            "summary_of_the_review": "In my view, this is an interesting paper that tackles an important question. I think it has the potential to be impactful in the RL community, but I currently see several issues that prevent me from recommending acceptance, including (i) the lack of clarity in the way the results are communicated in Section 3.3, which are crucial to the main findings of the paper, (ii) a discussion of the generality of the findings, and (iii) full experimental details. I would be open to potentially adjusting my rating in response to the authors' rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2735/Reviewer_Q2WM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2735/Reviewer_Q2WM"
        ]
    }
]