[
    {
        "id": "Gf2oYcO97Ve",
        "original": null,
        "number": 1,
        "cdate": 1666219032098,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666219032098,
        "tmdate": 1666224169846,
        "tddate": null,
        "forum": "bNPth9YMqZ",
        "replyto": "bNPth9YMqZ",
        "invitation": "ICLR.cc/2023/Conference/Paper3113/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a new approach called Hetero-SSFL, to conduct federated learning under self-supervised tasks with heterogeneous clients. Specifically, besides local training, all clients learn to align the lower dimensional representations on a common dataset. The paper provides a convergence guarantee theoretically and demonstrate the improved performance empirically.",
            "strength_and_weaknesses": "Strengths:\n- The modification by adding an objective to make the representation on a common dataset aligned is intuitive and clear. \n- Authors gave a convergence guarantee for the proposed method, making the approach more thorough and convincing from the theoretical side.\n\nWeaknesses:\n- Experiments were not very comprehensive. Some important ablation studies were missing. For example, how will the coefficient $\\mu$ will affect the performance? And what is the influence of the size of the common dataset RAD? It is not clear whether the performance gain was due to the additional data.\n- It was not clear how the server utilized the representations from different local clients, given that the dimensions were not the same. In Eq. (1), the second term was the distance of two representations, while in Eq. (4), it was changed to two kernels. It seems that the paper did not mention how to aggregate representations.\n- Comparison of communication efficiency in federated learning is necessary in terms of metrics such as the size of transmitted data in one round.\n- Some implementation details were not complete. For example, how to obtain the RAD dataset? The authors also stated that weight vector $[w_1, w_2, \\dots, w_N]$ can be learned, but how to achieve it? Will learning the weight vector require the communication between the server and clients just as FedAvg to average weights from different clients?\n- There is a lack of discussion about privacy in federated learning and the authors should talk about potential privacy problems and whether the method can preserve differential privacy.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is easy to follow but as mentioned in weaknesses, some details were missing, which might cause confusion to the audience. The technical contributions of the paper were quite marginal despite its superior performance compared with baseline methods. As for reproducibility, it is only possible when the RAD dataset is available.",
            "summary_of_the_review": "The paper has some important details missing and the experimental part was not comprehensive enough. The current version is not ready for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "There is a lack of discussion about privacy in federated learning and the authors should talk about potential privacy problems and whether the method can preserve differential privacy.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3113/Reviewer_X5at"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3113/Reviewer_X5at"
        ]
    },
    {
        "id": "uj1q05GYwn",
        "original": null,
        "number": 2,
        "cdate": 1666608948826,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666608948826,
        "tmdate": 1666608948826,
        "tddate": null,
        "forum": "bNPth9YMqZ",
        "replyto": "bNPth9YMqZ",
        "invitation": "ICLR.cc/2023/Conference/Paper3113/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents Hetero-SSFL, a framework for SSL in FL that enables diverse clients (heterogeneous in terms of their compute capabilities and/or data distribution) to collaboratively learn a good SSL model. At the end of N rounds, the SSL model gets personalised (via linear evaluation protocol) to each client's data. Different from the majority of previous FL works, is the fact that clients in Hetero-SSFL do not communicate their locally-trained model with the server. Instead, what clients send are the embeddings outputted by the local model when using a common (i.e. same in all clients) dataset. These representations are then aggregated in the server and used in the following round to align (via a distance loss term) the representations each client learns during SSL. Hetero-SSFL is primarily evaluated on vision classification tasks but also text classification.",
            "strength_and_weaknesses": "### Strengths\n\n*    The paper is fairly well written, although it would benefit if Figure 1 was presented earlier in the text (maybe at the top of page 4?) Making it more self-contained would be good too.\n\n*    The results in Tables 1,2,3 look super good compared to the other baselines but. Could the Authors explain why the figures in Table 2 are lower than in Table 1? It seems like all the baselines do better in the IID case (as you'd normally expect) for CIFAR-10 but Hetero-SSFL does worse... But then for CIFAR-100 in Table 2 (IID) all methods do worse than in Table 1 (non-IID). Could the Authors elaborate? Maybe the numbers got mixed up adding them to the table?\n\n### Weaknesses\nSome weaknesses and clarifications needed:\n\n*    The Authors present this work as enabling collaboratively SSL in FL when clients are of different compute capabilities. However, only two types of networks ResNet-18/34 are considered and both train with batch size 200. That leaves the setting to just two types of clients. If the Authors considered a few more models to cover a wider spectrum of system capabilities (specially towards more lightweight models -- e.g mobilenet, efficientnet or similar?) and different batch sizes (which directly impacts on memory peak at training time), then I would be convinced that Hetero-SSFL is indeed addressing the client device heterogeneity problem.\n\n*    The RAD is one of the key ingredients in Hetero-SSFL but the Authors do not provide details about it. Is it also a vision dataset? which one? How does the quality of the SSL change with different RADs?\n\n*    The Authors state that the RAD \"doesn't posses any special properties\", could this be elaborated? I'm inclined to size and diversity in the RAD would be important, but to what extent? is MNIST a good RAD?\n\n*    How the SSL training on the clients happens is clear. However, how is the linear evaluation protocol implemented in this work? Is there labelled \"validation\" set to train the classifier and then the evaluation is done on a, yet another, sub-set of data that the client has (i.e. a labelled test set)? Do these validation and test sets follow the same label distribution as the (unlabelled) training set each client has? \n\nIn Table 3, the Authors present some results on how Hetere-SSFL extrapolates from the cross-silo (5 total clients and 5 clients per round -- as considered for Table1 and, I believe, Table 2 also) to a cross-device setting. I understand the cross-device setting is more challenging, specially when it comes to ensuring the alignment of representations helps the training process. \n*    Could the Authors comment upon why SSL suffers such a sharp drop in performance in cross-device settings?\n*    It would be good to include in Table 3 the standard (non-SSL) FL baseline, which should be something in the high 50%s, given that FedAdam(https://arxiv.org/abs/2003.00295) achieves 52.5% in a setup with 500 clients (sampling 10 per round I believe). Adding that extra row is good for putting the results in context: SSL is really important and clearly the way to go for FL, but we aren't there yet.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and addresses a known problem in an original way. ",
            "summary_of_the_review": "I am happy with this work but there are some open questions that need to be addressed ( see both the Strengths and Weaknesses lists) specially in regards to the RAD and results in Table1&2. I think the Authors can address these during the rebuttal period easily.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3113/Reviewer_N6gv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3113/Reviewer_N6gv"
        ]
    },
    {
        "id": "qGKNTtq9qG",
        "original": null,
        "number": 3,
        "cdate": 1666613822502,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613822502,
        "tmdate": 1666613822502,
        "tddate": null,
        "forum": "bNPth9YMqZ",
        "replyto": "bNPth9YMqZ",
        "invitation": "ICLR.cc/2023/Conference/Paper3113/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new method for federated self-supervised learning with clients who may train on local tasks using heterogeneous model architectures. The authors adopt a Siamese structure for local self-supervised learning, implemented to BYOL or SimSiam, and validate it using both image and text classification tasks. To allow semantic aggregation of local clients' representation, they introduce a public dataset named Representation Alignment Dataset (RAD) that all local models utilize for implicitly transferring representations across clients by minimizing the CKA distance of RAD representation. Therefore, the proposed model doesn't need direct weight aggregation and allows communication with heterogeneous clients.\n",
            "strength_and_weaknesses": "**strength**\n- propose a new method to transfer the knowledge across clients without direct weight aggregation, so that allows participation in heterogeneous self-supervised models \n- validate the model over text- and image-based classification problems\n- provide the convergence analysis for the proposed model\n\n**weakness**\n- Missing communication cost analysis.\n- Missing analyses of the RAD size and the construction of the RAD. To avoid the data privacy issue, the distribution of the RAD should be different from the distributions of local datasets (i.e., Non-IID). Therefore, the construction of the RAD can highly impact the model performance.\n- Only two ResNet-variants are used for architecture heterogeneity. More diverse architectures are required for backbones, such as Transformers, simple CNNs, and MLPs.\n- Performance gap with baselines decreases for larger participating clients at each round.\n\n----\n+ local clients from baselines do not train on the samples constructed to RAD?",
            "clarity,_quality,_novelty_and_reproducibility": "The construction of RAD is not clearly described. Otherwise, the paper is easy to read, and motivation is intuitive.",
            "summary_of_the_review": "The paper proposes a practical framework allowing model heterogeneity during federated self-supervised learning. The method is reasonable and provides convergence analysis, but there is a lack of analyses to understand the behavior of the proposed methods and the issue of RAD construction.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3113/Reviewer_GXVZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3113/Reviewer_GXVZ"
        ]
    },
    {
        "id": "LuMHpl_nTZ",
        "original": null,
        "number": 4,
        "cdate": 1666767038961,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666767038961,
        "tmdate": 1669261763274,
        "tddate": null,
        "forum": "bNPth9YMqZ",
        "replyto": "bNPth9YMqZ",
        "invitation": "ICLR.cc/2023/Conference/Paper3113/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Summary: This work proposes an FL paradigm for SSL under client heterogeneity. The proposed method, Hetero-SSFL allows each client to train personalized and different self-supervised models. It then enables joint learning across clients by aligning the lower dimensional representation with those of a common dataset assumed to be available on the server. The work is an extension of Makhija et al., 2022 by changing the supervised learning task to SSL. Also, Herero-SSFL has the impractical assumption of having a common dataset on the server, which may limit its application in some real settings. The author compared Hetero-SSFL with other FL+SSL methods on benchmark datasets.",
            "strength_and_weaknesses": "Pros: Overall, the idea of allowing different network architecture across clients in FL for SSL is very interesting. The selected technique foundation (Makhija et al., 2022) is reasonable. \n\nCons: \n1.Limited methodology novelty. Despite of the different learning paradigm, Hetero-SSL is very similar to Makhija et al., 2022 (for supervised learning tasks). The core techniques discussed in Sec 3, including general formulation, RAD, CKA, etc., seems directly borrow from Makhija et al., 2022. The authors may want to highlight the differences if there is any or give sufficient credits to the previous work. \n\n2.The assumption of requiring a common public dataset available in the server will raise many practical concerns. For example, what kind of data can be considered as the common dataset, how many data points need to be used, is it easy to find the privacy-free public data to serve this purpose, etc.\n\n3.What do a and b mean in Assumption 4.1. I assume they are two iterations. Then don\u2019t you want a,b > 0?\n\n4.Lemma 4.5 is similar to Tan et al.  The authors should give proper reference here.\n\n5.Are the results in Table 3 from IID or non-IID setting?\n\n6.How to select the common (global) data is missing in Section 5.\n\n7.Grammar and format issues should be corrected.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: okay\nQuality:  the paper written is not well-prepared and it requires proofread\nNovelty: limited as core parts are similar to previous work\nReproducibility: okay",
            "summary_of_the_review": "The paper aims to address the interesting question, but the novelty and the written quality downgrade it from the other submissions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3113/Reviewer_9iyo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3113/Reviewer_9iyo"
        ]
    }
]