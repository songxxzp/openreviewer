[
    {
        "id": "8X_MufkppNi",
        "original": null,
        "number": 1,
        "cdate": 1666649536053,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649536053,
        "tmdate": 1666649572049,
        "tddate": null,
        "forum": "U45w87vFQ3",
        "replyto": "U45w87vFQ3",
        "invitation": "ICLR.cc/2023/Conference/Paper3699/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to quantize ( two values)  the updates of the adam optimizer. The approach leverages the bounded nature of adam updates to design a unbiased quantization scheme - it uses the value of the update to define a bernoulli distribution (+1, -1). They show improvements in speed in several experiments. \n\nSome questions /  comments, \n1. What is \\mathcal{M} on page two . It is not defined. \n2. what does \"random gradient of f\" mean? Also i believe g_t is a vector . so what is g_t / |g_t|  .. Do you mean elementwise division?\n3. Page 4, \"Adam do not have this appealing property\" can you explain this point further. The point you are making claims that adam does not converge at local minimas but keeps oscillating. \n4. Theorem 1 \n      a. statement about \"spectral norm of b_i\" is bounded. Are we assuming b_i to be a matrix?\n5. lot of grammatical errors in the paper. like neede, Lagre, \"cost become a bottleneck\",  \"be friendly to efficient primitive communication primitives\" and a lot more.",
            "strength_and_weaknesses": "Strength:\n1. novel quantization scheme which is easy to implement \n2. Provable convergence as update is same in expectation\n3. strong empirical results\n\n\nWeakness\n1. writing needs a lot of work - see some confusions / point outs above\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is hampered a bit as the paper is riddled with writing issues. See above",
            "summary_of_the_review": "The paper seems to be making a novel contribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3699/Reviewer_YNiH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3699/Reviewer_YNiH"
        ]
    },
    {
        "id": "ONVlgz2ktx",
        "original": null,
        "number": 2,
        "cdate": 1666756972605,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756972605,
        "tmdate": 1666756972605,
        "tddate": null,
        "forum": "U45w87vFQ3",
        "replyto": "U45w87vFQ3",
        "invitation": "ICLR.cc/2023/Conference/Paper3699/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed a new quantized distributed stochastic gradient algorithm which resembles Adam.\n\nThey prove the new algorithm BinSGDM bears the same convergence rate as Adam, and BinSGDM is more communication efficient than Adam. They also compare the new algorithm with other algorithms, including ADAMW, 1-bit Adam and SGDM to show that the BinSGDM implementation is more performant than other algorithms' implementations.",
            "strength_and_weaknesses": "There are four key contributions listed by the authors\n\n1. The first algorithm that quantizes the entire update of an adaptive optimizer (which seems to me that it translates to less quantization steps) and does not need warm-up.\n  - This one looks novel and simplifies the usage of this algorithm.\n2. BinSGDM has the same convergence rate as the full-precision Adam\n  -  This one is based on strong assumptions such as the gradients are bounded by a constant, and it doesn't say much since the convergence rate is also the same as SGD's 1/\\sqrt{T} rate. A simple quantized SGD can achieve the same rate.\n3. Hierarchical 1-bit All-Reduce\n  - This is a standard system implementation, basically it treats a node as a single worker in distributed training. Already implemented in frameworks such as DDP, Horovod, Bagua, etc.\n4. First work to be consistently faster than DDP\n  - This is highly dependent on the system implementation and computer network. And also not true, Existing works like Bagua and HiPress already show significant speedup over DDP\n\nTherefore the novelty seems to be 1, but since the algorithm is completely different from the traditional Adam (the second-moment stat is also replaced by first-moment of the gradient absolute value), more comprehensive experiments are needed to confirm the uncompressed version of the algorithm matches the convergence of Adam on most tasks.\n\nAlso since the paper claims to have speedup in real-world settings, it should use more practical settings for experiments. Most people don't use A100-80G GPUs with only 10Gbps ethernet connection.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to understand. The algorithm looks novel.",
            "summary_of_the_review": "The algorithm looks novel, but there remains some work to justify the effectiveness of the algorithm.\n\n1. The experiments need to be done in more realistic settings (for example A100 with RDMA connections)\n2. Since the algorithm is based on a different update rule than the original Adam, to justify the new algorithm is as good as Adam (as one of the key contributions states) more comprehensive experiments are needed to confirm the uncompressed version of the algorithm matches the convergence of Adam on most tasks.\n\nAlso some claims need to be adjusted according to the \"Strength And Weaknesses\" section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3699/Reviewer_C7p2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3699/Reviewer_C7p2"
        ]
    },
    {
        "id": "QtFiQBNVJl",
        "original": null,
        "number": 3,
        "cdate": 1666856989681,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666856989681,
        "tmdate": 1670352615261,
        "tddate": null,
        "forum": "U45w87vFQ3",
        "replyto": "U45w87vFQ3",
        "invitation": "ICLR.cc/2023/Conference/Paper3699/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A major bottleneck in large-scale distributed deep learning is the communication bottleneck. The computational cost of the majority of existing compression algorithms to reduce the communication cost is too high. The authors proposed a new distributed optimization algorithm, Binary SGD-Momentum which 1- compresses the updates of the model's parameters at each worker, instead of the gradients commonly done in most other distributed DL methods, and 2-stochastically quantizes the values to +1 or -1. To aggregate the binary quantized values, they devise hierarchical 1-bit All-Reduce to take advantage of the 1-bit quantized values, and inter-node/intra-node communications. Finally, they theoretically analyzed their algorithm and evaluated it performance using numerous experiments.\n",
            "strength_and_weaknesses": "**Strengths:**\n\nThe authors have considered all aspects of distributed DL in developing their algorithm, such as computational cost, communication, and aggregation. This paper is among the few works that have considered compressing model updates instead of the SGs, and the simulation results show that the training with Bin-SGDM is consistently faster (or at least as fast as) uncompressed optimizers (even with high-speed intra-node connections among multiple GPUs).\n\n**Weaknesses:**\n\nIn some parts, the paper is hard to read and the intent of the authors of some sentences/paragraphs are not clear. Moreover, some notations are not defined, and the reader has to search the appendix for the definition of some of the concepts. Moreover, the theoretical analysis seems to be flawed at some points.\n\n1. There is a disparity between algorithm 1 and BinSGDM equation (4). Algorithm 1, line 7, keeps the maximum of $b_t^{(i)}$ and uses it to compute the parameter update. The authors do not explain or motivate this seemingly critical point in distributed BinSGDM, and it is not clear whether equation 4 or Alg. 1 is used for simulations. Practically, since the magnitude of SGs are usually larger at the first few training epochs, using the maximum value ($b_t^{(i)}$) is similar to scaling the SGs with a large constant value to map them to [-1, 1]. In other words, after few epochs, Alg. 1 applies (almost) fixed scaling factors to the computed SGs before binarizing them.\n2. Theorem 1, what are the constants $\\rho$ and $I$? Also, $z_1$ should be defined in the theorem.\n3. The analysis and proofs in the appendix either have some flaws or unclear:\n- Equation 10, note that although $g_t=\\frac{1}{n}\\sum_i g_t^{(i)}$, but $|g_t| \\neq \\frac{1}{n}\\sum_i |g_t^{(i)}|$. Hence, the RHS equation is incorrect. I am not sure how this affects the validity of the proofs.\n- Equation 34, the reason for 3rd and 4th inequalities are not clear.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The algorithm is clearly presented (except the confusion between equation 4 and Alg. 1), and well-motivated, especially, the connection to the existing sign-SGD and Adam.\nMoreover, all aspects of the experiments (HW, used software and libraries, architecture of the models, hyper-parameters, datasets, ...) are clearly stated with enough details.\nHowever, as mentioned in the weaknesses, some parts of the paper are hard to understand or contain seemingly contradicting statements (e.g., first few sentences of Appendix B- Discussion). The writing of the paper needs some improvements.\n",
            "summary_of_the_review": "The paper has presented an interesting and somehow novel compression algorithm for the distributed DL.The authors have provided both theoretical analysis and to some extent complete simulation results.\nI believe that the paper needs some minor improvements, such as double-checking the proofs of theorems and fixing some mistakes, ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3699/Reviewer_kGZr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3699/Reviewer_kGZr"
        ]
    }
]