[
    {
        "id": "Orc5KI4H0gV",
        "original": null,
        "number": 1,
        "cdate": 1666515461330,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666515461330,
        "tmdate": 1666515461330,
        "tddate": null,
        "forum": "0L8tuglXJaW",
        "replyto": "0L8tuglXJaW",
        "invitation": "ICLR.cc/2023/Conference/Paper1297/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose an SNN model with one timestep only. The key to the model is the use of the Hoyer extremum as a spiking threshold. The Hoyer regularization is a technique previously used for DNN regularization. The proposed SNN corresponds to a DNN with binary activation functions. The backprop pipeline includes the gradient of the threshold function (nondifferentiable), which is approximated to a simple boxcar function with a sufficient width (0-2) to cover the range of inputs. The performance of the proposed model was evaluated in terms of classification accuracy (on CIFAR 10/ImageNet and VOC2007) and computational efficiency. Given the use of binary activation functions, accuracy degradation was inevitable compared with the DNN counterpart. Instead, the authors place emphasis on computational efficiency that may be enhanced given the use of binary activations that may avoid FP multiplications, this is very arguable though. ",
            "strength_and_weaknesses": "Strength:\\\nThe proposed model was verified on various datasets (CIFAR10, ImageNet, VOC2007) and compared with previous models including DNNs, BNNs, SNNs in a systematic manner.\n\nWeaknesses:\n\n1. SNN comes into its own when encoding time-varying data, so that it should be able to extract spatiotemporal patterns of event stream through time. Therefore, SNN should be defined in a time domain. The authors proposed time-independent SNNs given the use of a single timestep, which are not SNNs as a matter of fact given the key to SNNs, i.e., dynamic models. Therefore, the advantages of the time-independent SNN over DNNs are not sure. In fact, the model used in this work is a DNN with binary activations rather than SNN. \n\n2. The CE comparison between the proposed model and DNN in Eq. 14 is not fair. The proposed model includes additional operations that are not addressed in the CE calculation. They include the division in Eq. 3 and comparison with the threshold in FP in Eq. 4. Note that the comparison with the FP number is much more expansive than the comparison in ReLU, which can simply be done by seeing the sign bit of an input. Moreover, I wonder if FP multiplications are avoided by using binary activations (0/1 or 0.0000\u2026/1.0000\u2026) because this depends on hardware used. I recommend the authors to specify the hardware considered for the proposed model.\n\n3. Where is the multiplier $\\lambda_H$ in Eq.6?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well organized and clearly written.\n\nReproducibility: it cannot be evaluated since no code is provided. Also, no hyperparameters are given in the manuscript.\n\nQuality and Novelty: As for the weaknesses mentioned in the above section, I found that the significance and novelty are marginal. ",
            "summary_of_the_review": "The proposed model (SNN with one timestep) corresponds to a DNN with binary activations. Therefore, the model cannot leverage the rich dynamics inherently given to SNNs, and thus the proposed model is not able to process time-varying event stream. Given this reason, I found marginal significance in this work. As well, Novelty is limited given that the Hoyer regularization concept was proposed previously. Regarding technical aspects of the proposed model, the loss in accuracy due to the use of binary activations is obvious, and the computational efficiency estimation is also very arguable.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_YzPx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_YzPx"
        ]
    },
    {
        "id": "s54Lne8iaJX",
        "original": null,
        "number": 2,
        "cdate": 1666600396684,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600396684,
        "tmdate": 1668775651055,
        "tddate": null,
        "forum": "0L8tuglXJaW",
        "replyto": "0L8tuglXJaW",
        "invitation": "ICLR.cc/2023/Conference/Paper1297/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new method to train spiking neural networks (SNNs) with only one timestep. The method is based on a new regularization term (\"Hoyer\"), and a new threshold adjustment method (\"Hoyer spike\", Eq 4).\n\n",
            "strength_and_weaknesses": "STRENGTHS:\n\nThe accuracy they reach with only one timestep is impressive, better than the previous state-of-the-art.\n\nWEAKNESSES:\n\nThe theory is unclear:\n\n* \"we estimate the value of the Hoyer extremum as He(ul) =\"\nI think what follows\u00a0is not the value of H but the value of the element of u_l at the extremum.\n\n* a SNN with one timestep is actually a somewhat degenerated case because there is no temporal integration. It boils down to a vanilla feed-forward artificial neural network, with Heaviside as the activation\u00a0function. This also corresponds to the first neuron model proposed by\u00a0McCulloch and Pitts and 1943, also known as threshold\u00a0gates. This should be discussed.\n\n* according to Table 5 what matters is the Hoyer spike, not the Hoyer regularization. This raises a question: could Hoyer spike be used alone, without the Hoyer regularization?\n\n* does the\u00a0approach extend to multistep SNNs?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Will the authors share their code?\n\n* Eq 1 is misleading, there is only one type step here.\n\n* |u_l| is described in the text before it's actually used in the equations\n\n* what are the blue areas on Fig 1?\n\n* what is the dataset in Table 1?\n\n*\u00a0https://arxiv.org/abs/2102.04159 and\u00a0https://arxiv.org/abs/2007.05785 should be included in Table 4\n\n",
            "summary_of_the_review": "A potentially promising method, but some aspects are unclear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_qbCE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_qbCE"
        ]
    },
    {
        "id": "CzH2QePYgui",
        "original": null,
        "number": 3,
        "cdate": 1666668014371,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668014371,
        "tmdate": 1670074226531,
        "tddate": null,
        "forum": "0L8tuglXJaW",
        "replyto": "0L8tuglXJaW",
        "invitation": "ICLR.cc/2023/Conference/Paper1297/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a training framework (from scratch) for one- time-step SNNs that uses a novel variant of the recently proposed Hoyer regularizer. By estimating the threshold of each SNN layer as the Hoyer extremum of a clipped version of its activation map, the approach not only downscales the value of the trainable threshold, but also shifts the membrane potential values away from the threshold. The approach outperforms existing spiking, binary, and adder neural networks in terms of accuracy-FLOPs trade-off for complex image recognition tasks.\n\n\n%%%%%%%%%%%%%\n\nUpdate:\nThe authors have addressed part of my initial concern about bio-plausibility and limitation of BNN. Yet, I don't think the proposed method is that novel. So I change my score to 5. ",
            "strength_and_weaknesses": "Strength\n\n1. The paper is well-written and easy to follow.\n2. The experiments show the method with low-latency.\n\nWeaknesses\n1. The problem importance is unclear. One-time-step SNNs is simply binary network. The lack of dynamics make it less biologically plausible.\n2. Limited novelty. The proposed method is simply training with HOYER regularizer, while similar method has been proposed in [1]\n\n\n[1]Huanrui Yang, Wei Wen, and Hai Li. Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures. In International Conference on Learning Representa- tions, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. The quality is fair. The novelty is limited. There is no code provided, so it is not reproducible.",
            "summary_of_the_review": "The paper fail to clarify the problem importance, and the proposed method is limited novel. So I vote for a reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_ujG1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_ujG1"
        ]
    },
    {
        "id": "bfrkSi9ghtL",
        "original": null,
        "number": 4,
        "cdate": 1667285121440,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667285121440,
        "tmdate": 1668780784092,
        "tddate": null,
        "forum": "0L8tuglXJaW",
        "replyto": "0L8tuglXJaW",
        "invitation": "ICLR.cc/2023/Conference/Paper1297/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to train one-time-step SNNs with a Hoyer regularizer and Hoyer spike layer. The proposed Hoyer spike layer uses adaptive threshold based on the Hoyer extremum of membrane potentials, and a Hoyer regularization for membrane potentials is added in the loss function. Experiments on static image classification and object detection tasks demonstrate competitive performance and energy efficiency.",
            "strength_and_weaknesses": "Strengths:\n\n1. This paper conducts extensive experiments on static image tasks, including classification and detection, with different network structures, and considers many aspects such as accuracy, energy efficiency, and training and inference time. The effect of quantization of weights is also considered.\n\n2. This paper considers comparisons with AddNNs and BNNs and reports promising results.\n\nWeakness:\n\n1. The \u201cone-time-step SNN\u201d in the paper is actually an ANN with binary activations. It does not consider temporal dynamics of neuron models, so it should not belong to \u201cspatio-temporal computing paradigm\u201d as mentioned in the first sentence of the abstract. It cannot handle temporal inputs such as speech or DVS data, which may possibly be more suitable for SNN with temporal processing ability. So this paper is more close to binary or quantized ANNs compared with general SNNs. The introduction, related work, and comparisons should have more focus on those ANNs than SNNs, and some introduction such as the dynamics of the LIF neuron model in Section 2.1 almost has no relation with models in this paper.\n\n2. The motivation and the effect to introduce Hoyer regularizer and Hoyer spike layer are not clear enough. As introduced in Section 2.2, Hoyer regularizer is first introduced to induce sparsity. Then adding this regularizer for membrane potentials will encourage membrane potentials towards 0, implying that spikes are encouraged to be sparser. However, in Section 3.1, it is said that \u201cit is crucial to reduce the value of the threshold to generate enough spikes for better network convergence\u201d, which is *contradictory* to the regularizer that encourages fewer spikes. Why to combine these two opposite components and why it can work? Besides, there is no explanation for why \u201cHoyer extremum\u201d is introduced to adjust the threshold and why it can work to balance the threshold and include proper data distribution as shown in Figure 1(a), why Eq. (4) considers clip function, why combining this \u201cHoyer extremum\u201d with the trainable threshold can be better than the single trainable threshold? There lack significant explanations for the methods.\n\n3. Some descriptions are imprecise. For example, in Section 3.1, \u201cThis is because if a neuron does not emit a spike, the pre-synaptic weight connected to it cannot be updated\u201d. What does \u201cpre-synaptic weight\u201d mean? I suppose the authors mean that the synaptic weight will not be updated if the pre-synaptic neuron does not emit a spike when gradients of $w_{ij}$ from neuron $i$ to $j$ are calculated by $g_{u_j}*o_i$, where $g_{u_j}$ is the gradient for $u_j$. \nAnd in Section 3.3, \u201cNote that unlike existing multi-time-step SNN architectures that avoid BN due to difficulty in convergence\u201d. This is not up-to-date since most recent SNN works use BN to achieve high performance [1,2,3,4].\n\n4. The energy results in Table 6 are strange. As introduced in the \u201cEffect on Quantization\u201d paragraph, transforming the full-precision ACs to 2-6 bit ACs will lead to 4.8-15.2 reduction in compute energy. Why in Table 6, there are more than 400-1000 times reduction? Not to mention that only convolutional layers are quantized as said in the paragraph.\n\n5. The comparison with BNN and AddNN in Table 7 is not clear enough. The reported accuracy of the method is based on full-precision (also note that the full-precision results in Table 6 is inconsistent with Table 4 and Table 7) but the reported energy is based on 2-bits quantization, which is inconsistent. And how is the energy of BNN and AddNN calculated? Is AddNN also quantized? As for the proposed methods, even with quantization, it is unclear to me why the energy can be much lower than BNN with only pop-count operations. I think pop-count operations are much more efficient than ACs. These details should be reported and explained.\n\n6. The reference style in the paper is with poor readability. The authors had better distinguish parenthetical and narrative types, i.e. \\citep and \\citet in the latex.\n\n[1] Zheng, H., Wu, Y., Deng, L., Hu, Y., & Li, G. (2021). Going deeper with directly-trained larger spiking neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 12, pp. 11062-11070).\n\n[2] Li, Y., Guo, Y., Zhang, S., Deng, S., Hai, Y., & Gu, S. (2021). Differentiable spike: Rethinking gradient-descent for training spiking neural networks. Advances in Neural Information Processing Systems, 34, 23426-23439.\n\n[3] Deng, S., Li, Y., Zhang, S., & Gu, S. (2021). Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting. In International Conference on Learning Representations.\n\n[4] Meng, Q., Xiao, M., Yan, S., Wang, Y., Lin, Z., & Luo, Z. Q. (2022). Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12444-12453).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & Quality: The introduction of the method details is clear, but there lacks a reasonable explanation for the motivation and effects of the method. Also, there are some questions regarding the experimental results and comparisons. (see above weakness)\n\nNovelty: The Hoyer regularizer is introduced by previous works. The Hoyer spiking layer is new.\n\nReproducibility: Good.\n",
            "summary_of_the_review": "In summary, this paper lacks some significant explanations for the methods despite the good performance, and there are some questions about the experiments that should be tackled. Besides, the paper should have more focus on the more similar binary or quantized neural networks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_pFTU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_pFTU"
        ]
    },
    {
        "id": "9O0zQcem5Rs",
        "original": null,
        "number": 5,
        "cdate": 1667295063105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667295063105,
        "tmdate": 1667295063105,
        "tddate": null,
        "forum": "0L8tuglXJaW",
        "replyto": "0L8tuglXJaW",
        "invitation": "ICLR.cc/2023/Conference/Paper1297/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents an interesting training algorithm for training SNNs from scratch with only one timestep.",
            "strength_and_weaknesses": "Very comprehensive results\n\nSimple yet effective idea\n\nSince the authors use a regularization technique, I am wondering if the authors can shed light on how their method differs from previous temporal BN methods proposed by prior works that have shown accuracy improvement while decreasing the total timesteps [1, 2]. \n\n[1]Kim, Y., & Panda, P. (2020). Revisiting batch normalization for training low-latency deep spiking neural networks from scratch. Frontiers in neuroscience, 1638. \n\n[2] Zheng, Hanle, et al. \"Going deeper with directly-trained larger spiking neural networks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.\n\nThere is a plethora of works today on SNN algorithmic training- precisely talking about how we can get improved accuracy with less timesteps. But, I am more concerned by the fact that in such large-scale settings, are SNNs going to be actually advantageous? The authors show some simplistic energy estimation results which is grossly approximate. For true energy estimation, they have to consider memory access and data access energy which turns out to expend a lot of computations in SNNs given their repeated time-wise computation. In a recent work [3], true energy estimation on a systolic accelerator precisely shows that SNNs are not very advantageous over ANNs because repeated timestep computations will lead to redundant access of weights and membrane potentials is going to further add to energy unless we really improve the sparsity rate. Can the authors kindly comment on this - and it may be worthwhile for authors to include a discussion n the relevance of using more mainstream tools for energy estimation rather than just doing analytical modeling of FLOPS count?\n\n [3] Yin, Ruokai, et al. \"SATA: Sparsity-Aware Training Accelerator for Spiking Neural Networks.\" arXiv preprint arXiv:2204.05422 (2022).\n\nComing to my next point, there is a recent work [4] that explores sparse SNNs using lottery ticket hypothesis or NAS [R5]  to truly take advantage of SNNs energy efficiency over ANNs. Can the authors comment on how their model in terms of parameter count compares to these sparse SNN models which in fact show SOTA accuracy on CIFAR10,100 with very low timestep count? \n\n[4] Kim, Youngeun, et al. \"Lottery Ticket Hypothesis for Spiking Neural Networks.\" arXiv preprint arXiv:2207.01382 (2022).\n\n[R5] Kim, Youngeun, et al. \"Neural architecture search for spiking neural networks.\" arXiv preprint arXiv:2201.10355 (2022).\n\nFinally, I think it is well known that SNNs will be more suited to DVS or event based tasks as compared to standard digital camera recognition models. Recent works have shown superiority of SNNs over ANNs on these neuromorphic datasets [5, 6, 7]. Can the authors run their model on one of these datasets and compare to [5,6,7]? \n\n[5] Li, Yuhang, et al. \"Neuromorphic Data Augmentation for Training Spiking Neural Networks.\" arXiv preprint arXiv:2203.06145 (2022).\n\n[6] Kim, Youngeun, and Priyadarshini Panda. \"Optimizing deeper spiking neural networks for dynamic vision sensing.\" Neural Networks 144 (2021): 686-698. \n\n[7] Kim, Y., Chough, J., & Panda, P. (2022). Beyond classification: directly training spiking neural networks for semantic segmentation. Neuromorphic Computing and Engineering.",
            "clarity,_quality,_novelty_and_reproducibility": "he contributions are clear and the results are good. I am not sure about novelty since it is a mix of methods that have existed in SNN/ANN literature and putting together seem to make the model better. -I am concerned about whether this is truly advanategous SNN framework as I have raised questions around SNN implementation and the sparsity in weakness.",
            "summary_of_the_review": "I am not very convinced by the novelty and true efficiency advantage of this method since many comparisons and experiments are limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_hEby"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1297/Reviewer_hEby"
        ]
    }
]