[
    {
        "id": "TPnkPq9K5Q",
        "original": null,
        "number": 1,
        "cdate": 1666355697375,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666355697375,
        "tmdate": 1666445281648,
        "tddate": null,
        "forum": "VB75Pi89p7",
        "replyto": "VB75Pi89p7",
        "invitation": "ICLR.cc/2023/Conference/Paper1587/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new masked image modeling method(BEIT v2). A new Vector-Quantized Knowledge Distillation helps the BEIT v2 explore the high-level semantics. Meanwhile, this paper introduces a patch aggregation strategy to enhance global semantic representation. Experiments on image classification and semantic segmentation show the good performances of BEIT v2.\n",
            "strength_and_weaknesses": "# Strength\n## 1. VQ-KD\nThe VQ-KD follows the [1] using the l2 normalization and VITs to get codebook embeddings. To explore the high-level semantics, the VQ-KD reconstructs the feature of DINO or CLIP. The high-level semantics help the BEIT v2 have better performance.\n[1] 1.  Yu, Jiahui, et al. \"Vector-quantized image modeling with improved vqgan.\" arXiv preprint arXiv:2110.04627 (2021).\n## 2. Global semantic representation\nThe BEIT v2 tries to use the [CLS] token to learn the global semantic representation. Combining the global representation, the BEIT v2 may have the ability to understand the global information of the image, which is essential for image classification or other tasks.\n## 3. Performance\nThe performances in image classification and semantic segmentation are good.\n\n# Weakness\n\n## 1. VQ-KD\n###  First question \nHow to explain the BEIT v2 beyond the teacher models as shown in Table [6]. I think the upper limit of BEIT v2 is teacher models, since the reconstruction loss in VQ-KD can not be zero, there has a gap between the feature provided by the VQ-KD and the teacher.\n\n###  Second question \nThe authors may try RQ-VAE [2] as your basic model in VQ-KD. The paper extends the capability of codebook in channel dim, which may help the BEITv2 performance better.\n\n[2]. Lee, Doyup, et al. \"Autoregressive Image Generation using Residual Quantization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n## 2. Global semantic representation\nIt is hard to say the [CLS] token to learn the global semantic representation in your experiments. As shown in Table [5], selecting the suitable $l$-th layer is essential to the performance. I think this is because the model not only learns the [CLS] token but also refines the other tokens in the patch aggregation strategy. In this situation, the patch aggregation strategy will let the layers($0$-th~$l$-th) predict better tokens (not the [CLS] token), and the [CLS] token is not important. The patch aggregation strategy seems like a kind of multi-scale loss and add the MIM loss in the selected layers.\n\nSo if the authors want to let the [CLS] token learn the global information better, I think you can stop the gradient computation of other tokens and just refine the [CLS] token.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: good.\nClarity: good.\nOriginality: fair.",
            "summary_of_the_review": "The BEIT v2 has good performance in image classification and segmentation. But I still have some questions as mentioned above, and I am glad to see the responses of the authors. If the authors can resolve my questions, I will increase my rating.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1587/Reviewer_3zyP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1587/Reviewer_3zyP"
        ]
    },
    {
        "id": "VvcZb_2RpBK",
        "original": null,
        "number": 2,
        "cdate": 1666445037394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666445037394,
        "tmdate": 1666445037394,
        "tddate": null,
        "forum": "VB75Pi89p7",
        "replyto": "VB75Pi89p7",
        "invitation": "ICLR.cc/2023/Conference/Paper1587/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper improves BEIT by introducing the Vector-Quantized Knowledge Distillation (VQ-KD) algorithm for better visual tokenizer training. A patch aggregation strategy is also introduced into the Masked-Image-Modeling (MIM) pretraining framework. Experimental results show that BEITv2 (this work) significantly outperforms other MIM pretraining methods.",
            "strength_and_weaknesses": "Strengths\n- The idea of introducing a semantic-rich visual tokenizer into the BEIT framework is reasonable and interesting.\n- This paper is clearly written and easy to follow.\n\nWeaknesses\n- Using a teacher model for supervising the visual tokenizer training makes the whole pretraining procedure more complex and time consuming. Two additional training stages (teacher model training and tokenizer training) are required before the final masked image modeling pretraining when compared to other one-stage pretraining methods, which weakens the significance of this work. A detailed comparision of the GPU hours (number of used GPUs and training Days) of different training stages of BEITv2 and other competitors may help clarify this issue.\n- The final pretraining performance (evaluated with imagenet finetuning) strongly relies on the teacher model. As shown in Table 6, better teacher model (CLIP vs DINO) can consitently bring better performance of BEITv2. While, BEITv2 can improve DINO from 83.6 to 84.4, but can only improve CLIP from 84.9 to 85.0, which indicates that with stronger teacher, the performance gain of BEITv2 will rapidly decrease. When Patch Aggregation strategy is not used, the performance of BEITv2 even drops to 84.7 (Table 4), lower than the CLIP teacher, which further weakens the significance of this work.\n- The novelty of of introducing Patch Aggregation strategy is somewhat limited as it shows little difference from Condenser Head in Gao & Callan 's work (Condenser: a Pre-training Architecture for Dense Retrieval, 2021). The architecture and the usage (used for pretraining but dropped during fine-tuning) are all similar.",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper is  well written with sufficient details, so it may not be diffcult to reproduce.",
            "summary_of_the_review": "I mainly worry about the significance of the propsed method, considering the strong teacher required and more complex training stages. The novelty of the Patch Aggregation strategy is also limited. The authors are encouraged to address the above issues.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1587/Reviewer_d8pL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1587/Reviewer_d8pL"
        ]
    },
    {
        "id": "iXYM7tPXwg",
        "original": null,
        "number": 3,
        "cdate": 1666641075167,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641075167,
        "tmdate": 1671136471166,
        "tddate": null,
        "forum": "VB75Pi89p7",
        "replyto": "VB75Pi89p7",
        "invitation": "ICLR.cc/2023/Conference/Paper1587/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an improved version of the BEiT method for\nself-supervised representation learning by predicting masked image\npatches. The improvements rest mainly in two incorporations:\n1) Encoding image patches using a ViT-based tokenizer that distillis the\nsemantic information from a CLIP model into the encoded image tokens.\n2) Adding a patch aggregation stage that makes class token aggregate\n   global information from the patch encodings.\n\nExtensive experiments show the effectiveness of the improvements in\nstandard ImageNet pre-training benchmarks for classification and\nADE-20K for semantic segmentation.\n",
            "strength_and_weaknesses": "**Strengths**\n- The two contributions are well justified and the extensive\nexperimental results strongly validate the design decisions.\n\n- The method outperforms compared pre-training methods by \n  significant margins across diverse tasks.\n\n- The obtained representations are significantly more robust compared\n  to the state-of-the-art pretraining method MAE.\n  \n**Weaknesses/Questions**\n- Unless I missed it, it would be nice to explain how the projection\n  for reducing the code dimension for lookup is obtained.\n- In (2), there is no $\\beta$ multiplying the commitment term as in the\n  standard VQ-VAE, but isn't $\\beta=1$ equivalent to\n  directly optimizing the distance without stop-gradient? If\n  $\\beta$ was used, please specify its value.\n- I'm curious about how the clusters in Figure 4 where found. Was it\n  by visual inspection of occurrences of all (or many) of the codebook\n  elements? Have you compared this type of semantic association with the\n  baseline tokenizer?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, high quality, and has significant novelty. The authors included the code as supplementary material for reproducibility.",
            "summary_of_the_review": "The paper proposes methodologic contributions that significantly\nadvance the state-of-the-art  in large-scale pretraining for computer\nvision tasks.  The contributions are well justified and the paper present \nsufficient empirical evidence through extensive experiments.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1587/Reviewer_jP3q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1587/Reviewer_jP3q"
        ]
    },
    {
        "id": "MVQVru7o6I",
        "original": null,
        "number": 4,
        "cdate": 1666659761948,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659761948,
        "tmdate": 1666659761948,
        "tddate": null,
        "forum": "VB75Pi89p7",
        "replyto": "VB75Pi89p7",
        "invitation": "ICLR.cc/2023/Conference/Paper1587/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": " - This work extends the study of BEiT, i.e. BERT-style pre-training of image transformers. Specifically, this work proposes to adopt a semantic-rich visual tokenizer distilled from the semantically-rich CLIP model, promoting the MIM process to focus more on semantic-level.\n\n - In order to learn better and compact codebook space, this work also exploits other practices adopted in previous vector-quantized modeling paper, such as factorized code, and L2 norm. Meanwhile, a patch aggregation strategy is introduced in MIM to enhance global semantic space.\n\n - Extensive experiments on image classification and semantic segmentation are conducted, which demonstrates the effectiveness of this method.",
            "strength_and_weaknesses": "#### **Strength**\n\n - Masked Image modeling and pre-training has been an important topic in computer vision. Different from predicting pixels directly (MAE-style), this work promotes the development of token prediction (BEiT/BERT-style) pre-training. \n\n - The authors conduct a variety of experimental studies. With the introduced changes, promising results on image classification and semantic segmentation are shown.\n\n - Linear probing has been a relatively weak point of MIM-based pre-training, compared to contrastive and self-distillation methods. However, this work upgrades the model's linear performance.\n\n#### **Weakness**\n\n - Albeit with promising performance, the major issue in this work is about the use of CLIP features. With VQ-KD training, the encoder outputs could be viewed as distillations from CLIP, which is further leveraged to guide the MIM training. This paradigm results in unfair comparison with previous works. (See summary)\n\n - The vision encoder of CLIP already shows a 76.2 zero-shot performance and 84.9(reported in Table 6) fine-tuned accuracy. Considering the significance of its raw features, it remains questionable of the value about such modified pre-training targets. With additional efforts and computation spent on VQ-KD and MIM pre-training, the final accuracy is only improved marginally. Given the context that proper knowledge distillation could further improve fine-tuned accuracy [1], the benefits of MIM pre-training are hindered.\n\n - Several tricks in ViT-VQGAN are adopted in this work. While it's non-trivial to exploit advances in generative modeling, there lacks discussion over these modifications such as L2 norm, and factorize codes (e.g. from 256-dim to 32-dim). Since ViT-VQGAN have already conducted studies on representation learning and shows improvement in linear probing. The technical contribution seems to be limited here.\n\n - As expressed in the paper, the patch aggregation strategy is designed to obtain enhanced global representations.  However, from the comparison in Table 1 and Table 5, the design choice seems to be adhoc and most performance elevation comes from the CLIP-distilled codebook. The role of patch aggregation lacks further investigation. \n\n[1] Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation.",
            "clarity,_quality,_novelty_and_reproducibility": " - Major clarity and presentation is clear, where as many typos exist: e.g. in Table 5: l-th Layer denotes **path** tokens.\n\n - While some interesting techniques are adopted in this work, it's unclear where the actual improvement comes from. Therefore, the novelty is still questionable.",
            "summary_of_the_review": " - As promising results are provided in this work, it's also valuable to design better vector quantization modules for representation learning. However, the unfair comparisons and marginal improvement (comparing to directly using CLIP ) suspect the contribution of this work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1587/Reviewer_Y8Gd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1587/Reviewer_Y8Gd"
        ]
    }
]