[
    {
        "id": "y6J47K9kHS",
        "original": null,
        "number": 1,
        "cdate": 1666574806928,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574806928,
        "tmdate": 1666575267667,
        "tddate": null,
        "forum": "MuWgF-FVzON",
        "replyto": "MuWgF-FVzON",
        "invitation": "ICLR.cc/2023/Conference/Paper1422/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the influence of warm-start in actor-critic algorithms. The authors provide a finite-time analysis of the impact of the approximation error and the sub-optimality of the initial policy. ",
            "strength_and_weaknesses": "### Strength:\n\n- This paper is overall well-written and easy to follow\n- Treating actor-critic algorithms as a perturbed Newton method is novel and provides many helpful insights\n\n### Weakness\n\n- The proof of Theorem 2 is similar to the proof in policy gradient methods leaving some control of approximation error. The authors is expected to highlight why this analysis is novel given the existing analysis in PG.\n- The impact of warm-start is not fully addressed in the theorems. In detail, in Theorem 2 and Theorem 4, the sub-optimality $h(\\omega, \\theta_t^*) - h(\\omega, \\theta_{t-1})$ is exponentially decreasing, which is similar with conventional analysis in optimization. I wonder why the author emphasizes the term `warm start' in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written. The author might want to highlight the difference of Theorem 2 compared with the policy gradient analysis to better show the novelty.",
            "summary_of_the_review": "I don't have any other major comments besides the comments above. Here're some specific questions. \n\n- What's the rate of $\\prod_{i=0}^t H_{t-i}$ in Theorem 5? Should we expect it to be decreasing exponentially fast w.r.t. t?\n- What does the `warm start' exactly mean? Does it mean by eq. (8) we could get a closed-form solution of the critic update instead of a one-step update? If that's the case, the analysis should not be claimed as a single-time scale analysis since the critic is updated completely until the actor update.\n- Again, compared with the closed-form update of the critic, can we use the two-time scale [1] update to provide more efficiency?\n- In the proof of Theorem 4, why does the first inequality after `plugging Eqn. 27 into Eqn 26 holds? How do we control the $\\mathcal E_t$ by $\\mathcal B(t)$ and $\\mathcal N(t)$?\n- The authors mentioned the actor-critic algorithm could be treated as a perturbed Newton method. Is there any relationship between this understanding and the natural actor-critic algorithm where [2] provides some theoretical analysis? It would be helpful to give some brief discussion.\n\nAfter all, I found this paper valuable and would suggest acceptance\n\n[1] Wu, Yue Frank, et al. \"A finite-time analysis of two time-scale actor-critic methods.\" Advances in Neural Information Processing Systems 33 (2020): 17617-17628.\n\n[2] Khodadadian, Sajad, et al. \"Finite sample analysis of two-time-scale natural actor-critic algorithm.\" IEEE Transactions on Automatic Control (2022).\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1422/Reviewer_gGbf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1422/Reviewer_gGbf"
        ]
    },
    {
        "id": "KF--YrESpe",
        "original": null,
        "number": 2,
        "cdate": 1666599599915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599599915,
        "tmdate": 1666599599915,
        "tddate": null,
        "forum": "MuWgF-FVzON",
        "replyto": "MuWgF-FVzON",
        "invitation": "ICLR.cc/2023/Conference/Paper1422/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors take a finite-time analysis approach to quantify the impact of approximation errors on the learning performance ofWarm-Start A-C method with a given prior policy. By delving into the\nintricate coupling between the updates of the Actor and the Critic, the paper first provides upper bounds on the approximation errors in both the Critic update and Actor update of online adaptation, respectively,\nwhere the recent advances on Bernstein\u2019s Inequality are leveraged to deal with the sample correlation therein.",
            "strength_and_weaknesses": "\nThe paper seems to present interesting results. \nIn terms of presentation, there seem exist some room for further improvements. \nIn section 2.2, the authors explain AC method, which is a stochastic RL method. \nHowever, when explaining how the error propagates, the derivation process uses a perfectly deterministic manner. \nTherefore, it is rather confuzing. \nThe same points apply to section 3. \nThe AC method is expressed in a deterministic manner with exact expectations. \nTherefore, the title of sections need to be changed and the discussions need to be changed. \n\nIn eq (10), it is not clear how hard it is to project parameters into the parameter space. Some discussions may be needed.\n\nIn Theorem 1, it is not clear if the result is for the deterministic update in eq (8) or it is for the batch-based stochastic AC in eq (10).\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper seems to contain somewhat new results. \nAll the presentations need to be improved following the sense of my points given in the previous comments. \n",
            "summary_of_the_review": "The paper seems to contain interesting results. \nThe overall presentation needs to be improved further. \nMoreover, the authors need to discuss more the advantage of the proposed analysis compared to existing ones.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1422/Reviewer_528u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1422/Reviewer_528u"
        ]
    },
    {
        "id": "oa12CFT9sJ",
        "original": null,
        "number": 3,
        "cdate": 1666653870613,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653870613,
        "tmdate": 1666654201930,
        "tddate": null,
        "forum": "MuWgF-FVzON",
        "replyto": "MuWgF-FVzON",
        "invitation": "ICLR.cc/2023/Conference/Paper1422/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a theoretical understanding to the advantage of using warm-start in Actor-Critic (AC) type RL algorithms. The authors achieve this by establishing finite-time bounds of warm-start AC algorithms, which explicitly capture the impact of using warm-start. In terms of technical approach, the authors cast the AC algorithm as a Newton\u2019s method with perturbation and bound the critic error and the actor error separately.",
            "strength_and_weaknesses": "Major Comments:\n\n(1) The AC type algorithm the authors study is a stochastic approximation algorithm for maximizing the cumulative reward. In stochastic approximation, usually the finite-time bound consists of two terms, one is called bias (or optimization error), and the other is called variance (or statistical error) [1,2,3]. The bias captures how much away in expectation the iterates are from the desired limit, and the variance captures the noise error. It is well-known in stochastic approximation literature that the bias depends on the distance between the initial condition (e.g., the warm start policy in the case of AC) and the limit, and the variance depends on the batch sample size. While the terminology is different, It seems that the authors rediscovered this phenomenon in the special case of AC, which is not surprising. \n\n[1] Srikant, R., & Ying, L. (2019, June). Finite-time error bounds for linear stochastic approximation andtd learning. In Conference on Learning Theory (pp. 2803-2830). PMLR.\n\n[2] Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. Siam Review, 60(2), 223-311.\n\n[3] Agarwal, A., Kakade, S. M., Lee, J. D., & Mahajan, G. (2021). On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift. J. Mach. Learn. Res., 22(98), 1-76.\n\n(2) Some assumptions are too strong and can never be satisfied. For example, in Assumption 2, assuming the smallest eigenvalue of $E_\\rho[\\phi(s,a)\\phi(s,a)^\\top]$ is bounded below by $\\sigma^*$ is only possible for a specific $\\rho$ but not for all $\\rho$. Consider a $\\rho$ with an entry that is close to one and all other entries close to zero, the corresponding smallest eigenvalue of $E_\\rho[\\phi(s,a)\\phi(s,a)^\\top]$ can be made arbitrarily close to zero. Also, Assumption 3 (2) does not hold if deterministic policies are considered.\n\nMinor Comments:\n\n(1) Section 2 paragraph 1: I don't think the analysis allows for $\\gamma=1$. Is this a typo?\n\n(2) The paragraph after Eq. (4), saying that \"F can be viewed as the gradient of an unknown function.\" is not correct. Since the second derivative of $F$ (provided it is twice differentiable) may not be symmetric, F in general cannot be viewed as a gradient. It is well-known that value-based algorithms such as Q-learning are not stochastic gradient descent/ascent algorithms.\n\n(3) Eq. (5) is not clear. I need to go back and read (Grand-Cle \u0301ment, 2021) to understand this equation. Probably more details are needed.\n\n(4) Eq. (8) is confusing. The parameter $w_{t+1}$ is on both left and right of the equation.\n\n(5) In the statement of Theorem 1, on the left-hand side, since $Q$ is a vector, I believe the authors should use a suitable norm instead of an absolute value.\n\n(6) In the paragraph after Theorem 1, more steps of rollout can only partially decrease the error. In order to make the error arbitrarily small, one has to increase the sample size. Therefore, the \"either increase N or increase m\" statement is scientifically inaccurate. \n\n(7) In Theorem 2, why is there an expectation in a high probability bound? Also, where is $p$ in the bound?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing needs improvement as there are many typos. The result is not surprising as similar results are well-known in stochastic approximation literature.",
            "summary_of_the_review": "While the authors use different terminology, the paper essentially shows that the initial condition shows up in the bias term (or optimization error term) in AC algorithm, which is a stochastic approximation algorithm, and hence using warn start can reduce that bias term. This does not seem surprising to me as similar results were well-established in stochastic approximation literature, with applications in SGD, RL algorithms such as TD-learning, and policy-gradient type algorithms. In addition, some assumptions are too strong. In view of these two points, I feel this paper in its current form is not publishable at ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1422/Reviewer_7KE9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1422/Reviewer_7KE9"
        ]
    },
    {
        "id": "kSqj0LMWb2",
        "original": null,
        "number": 4,
        "cdate": 1667169190473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667169190473,
        "tmdate": 1671010960956,
        "tddate": null,
        "forum": "MuWgF-FVzON",
        "replyto": "MuWgF-FVzON",
        "invitation": "ICLR.cc/2023/Conference/Paper1422/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a finite-time analysis of Actor-Critic (A-C) methods for reinforcement learning. First, the paper provides an upper bound to the error of the critic update due to function approximation and sample estimates. Then, it provides an upper bound to the error of the actor update due to approximating the greedy step with several steps of policy gradient, and due to the bias and noise coming from the approximation error of the critic update. Finally, the paper studies the error propagation through subsequent iterations of the A-C method, providing both a lower bound and an upper bound that relates the error to the value of the initial (warm-start) policy and an additive bias term.",
            "strength_and_weaknesses": "After Discussion\n\nI want to thank again the authors for their clarifications and remarkable effort in improving the paper following reviewers' suggestions. Unfortunately, the discussion revealed some important concern over the result in Theorem 2: The upper bound still includes random variables, which means that convergence rate and sample complexity cannot be straightforwardly derived. Thus, I am updating my score to a slightly negative evaluation. However, I still believe that this paper will be a nice contribution with an additional effort to make Theorem 2 more informative.\n\n----\n\n\n*Strengths*\n- Providing a deeper understanding of the properties of A-C methods is paramount, given that it is widely used in practice;\n- The analysis consider the dependence between samples drawn from the MDP instead of the common i.i.d. assumption;\n- The main result directly connects the error with the value of the initial policy.\n\n*Weaknesses*\n- The paper is rather technical, tweaking the presentation to make it accessible to a broader audience could benefit its impact;\n- The warm-start setting does not seem to be crucial for the analysis;\n- The analysis consider expectations estimated through samples averages, but it does not specify how the samples are collected.\n\n*Comments*\n\n(Warm-start setting) From the title, abstract, and introduction, the reader is inclined to believe that the warm-start setting will play a crucial role in the analysis. Instead, from my understanding, the warm-start policy only affects the results through the value $v^{\\pi_0}$ in the Theorem 5. It is not clear to me whether it is essential for $\\pi_0$ to be warm-started, or the result would hold for a randomly initialized policy as well. Are the properties of the warm-start policy crucial for the analysis? E.g., does it matter that the policy is coming from its own optimization process, or perhaps that it displays good coverage properties as in (Xie et al., 2021)?\n\n(Samples averages and Markovian samples) The paper claims to introduce refined results that account for the dependence between the samples (called Markovian samples) rather than assuming i.i.d. samples, which is unattainable in practice without a generative model. However, it is not clear to me how the samples are actually collected. Do they come from a batch of episodes or a single long trajectory? I guess the former setting would further complicate the analysis, inducing additional bias and noise coming from the sampling procedure. For the latter setting instead: Why the divergence between the initial state distribution and the stationary distribution does not appear in the bounds (see Theorem 12 in Fan, Jiang, Sun (2021))? Are we assuming that the samples are directly coming from the stationary distribution here, so that we can neglect mixing considerations? ",
            "clarity,_quality,_novelty_and_reproducibility": "I am not familiar with the related literature, so I cannot judge the novelty of this paper. The presentation is rather technical, and it does not make the paper easily accessible from researchers outside the narrow field of error propagation in A-C methods. To the best of my judgement, the results look reasonable. However, I did not check the proofs, which makes it hard to evaluate reproducibility and correctness.",
            "summary_of_the_review": "Since I am not familiar with the previous works in the finite-time analysis of A-C methods, it is hard for me to judge whether the contribution is substantial. Nevertheless, I followed the main steps of the error propagation argument, and both the assumptions and the obtained upper bounds seem pretty reasonable to me. Speculating on the novelty of the Markovian samples analysis and the explicit dependence of the final bound with the value of the initial policy, I am leaning towards a positive evaluation for this paper, but I provide a borderline score that reflects my limited confidence rather than clear weaknesses in the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1422/Reviewer_uKPa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1422/Reviewer_uKPa"
        ]
    }
]