[
    {
        "id": "0Heor5-kdPw",
        "original": null,
        "number": 1,
        "cdate": 1666031135972,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666031135972,
        "tmdate": 1668749826571,
        "tddate": null,
        "forum": "OpC-9aBBVJe",
        "replyto": "OpC-9aBBVJe",
        "invitation": "ICLR.cc/2023/Conference/Paper3149/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors empirically study how combining replay ratio increases with network parameter resets can greatly improve sample efficiency of off-policy methods. Their experiments demonstrate superior performance on existing benchmarks with simple modifications to existing methods.",
            "strength_and_weaknesses": "## Strengths\n\n**Clarity:** The paper is written clearly and generally easy to understand.\n\n********************************************Framing/Contribution:******************************************** The paper\u2019s contribution is clear to understand.\n\n**Experiments/Results:** The authors perform extensive experiments that are convincing of the usefulness of high replay ratios when combined with periodically fully/partially resetting network parameters.\n\n## Weaknesses\n\n**Clarity:** \n\n- In the related works section, first paragraph, many italicized terms are used but not explained. I was familiar with all terms except ************primacy bias************, which I had to look up separately while reading. But in general, these terms should be quickly defined to make the paper easier to read.\n- The first paragraph of 5.1.2 is written poorly and hard to read, please fix it.\n\n**Experiments:** \n\n- IQMs are listed (which is great) but the authors should probably explain IQM and the \u201cFraction of runs with score > $\\tau$\u201d statistics for readers unfamiliar with the benchmarking paper that introduced these metrics.\n- While the provided analysis on different algorithm design choices in light of high replay ratios is useful, I\u2019m not sure that what the authors analyzed is the most sensible thing to analyze. The paper is about replay ratio scaling through resetting network parameters. Therefore, there should be analysis on two things: 1) different replay ratios, and 2) how/when to reset network parameters. The authors provide analysis of 1) through trying different replay ratios, but insufficient analysis of 2) as in Section 4 they define their reset strategies and just stick with it. I think the paper really should have this analysis to be more useful as an empirical study to inform design choices for other researchers.\n- In combination with the experiments in Section 5, it would be useful to have an experiment with an offline RL algorithm (perhaps IQL or CQL). Offline RL is increasingly popular and also being deployed on real robots, so the authors should study their replay ratio increases with an offline algorithm to make the paper more useful for researchers.\n\n****************************Minor details:****************************\n\n- \u201cThe number of agent updates per environment step is usually called ************replay ratio************, and most standard algorithms were designed to have values around or below 1\u201d \u2192 not sure if they were ********designed******** to have values around or below 1, I think this statement would be more accurate if it was something like \u201cmost standard algorithms are trained with a replay ratio around 1.\u201d\n- \u201cunder tasks switches\u201d \u2192 \u201cunder task switches\u201d\n- \u201cwith a same algorithm\u201d \u2192 \u201cwith the same algorithm\u201d\n- \u201cfor evaluation and comparisons, we follow the protocol suggested by  Agarwal et a. (2021)\u201d missing a period",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and generally high quality. Reproducibility is likely somewhat easy, but authors don't list hyperparameters. The work is original, though it is somewhat of an extension of prior work.",
            "summary_of_the_review": "I think this paper is useful and should be accepted to this conference, however, I have some suggestions for improvements with the experiments. I do think that the authors are missing one key set of analysis, which is why I am recommending a weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3149/Reviewer_X4J7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3149/Reviewer_X4J7"
        ]
    },
    {
        "id": "IW_1ejvU8M",
        "original": null,
        "number": 2,
        "cdate": 1666325884225,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666325884225,
        "tmdate": 1668522030849,
        "tddate": null,
        "forum": "OpC-9aBBVJe",
        "replyto": "OpC-9aBBVJe",
        "invitation": "ICLR.cc/2023/Conference/Paper3149/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Overall, the paper provides deeper analyses of the reset method proposed by Nikishin et al. [1].  \nThe paper under review shows that resetting the agent after every certain number of updates prevents performance degradation even at large replay ratio settings.  \nIn addition, several experiments and analyses that had not been done in [1] were made:  \n1. comparison with existing sample-efficient methods such as REDQ, 2. evaluation of the reset method in iterated offline and tandem settings, 3. analysis of implementation-level design decision in discrete control settings, and 4. evaluation of trade-offs between computational resources and performance.  \n\n[1] Nikishin, Evgenii and Schwarzer, Max and D'Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron, The Primacy Bias in Deep Reinforcement Learning, Proceedings of the 39th International Conference on Machine Learning, 2022 \n",
            "strength_and_weaknesses": "**Strengths:**  \n[Clarity & Quality] The paper is basically well-written and easy to read.  \n[Novelty] Analyses to better understand the reset method [1] are conducted.  \n * In the original reset paper [1], the reset method was not compared to methods with a sample efficient method, such as REDQ. The experiments in the paper under review show that the reset method is generally more efficient than the existing sample-efficient method. \n * Experiments in tandem and iterated offline settings are conducted.  In addition, the effect of combining online updates with offline updates is evaluated. This combination approach addresses the temporal performance degradation of resetting, which was one of the main limitations of the reset method. \n * Implementation decisions for discrete control problems and the trade-off between computational resources and performance are also analysed. \n * The experiment results and findings should be beneficial especially to practitioners. Also, as a reset method for RL is a relatively new one and experiments about it have not been stacked so far, the experiment results provided in the  paper under review will be beneficial for a future advance of the reset method.) \n\n**Weaknesses**  \n[Novelty] Not so much progress has been made from [1].  \n * There is only a few technical difference from the original reset method: for continuous control, the proposed reset method is based on \"updates\" steps rather than \"environment steps\", and for discrete control, the Shrink and perturb style encoder update is used.  \n * As with [1], the reset interval is treated as a hyperparameter, \nand it is still unclear the optimal timing for reset (i.e., we still need hyper-parameter tuning for it).  \n\n[Clarity ] It is unclear how much performance is sensitive to the hyperparameter of the reset interval.  \n * For experiments for both discrete and continuous environments, reset intervals are set to magic numbers. No detailed discussion about how this hyperparameter affects overall performance is provided. This makes understanding how the insights observed in the experiments are generalizable a bit difficult. \n\n\n\n**Minor comments**  \n> Recent approaches in continuous control leveraged high replay ratios as a strategy to improve sample efficiency through the use of ensembles of value functions (Chen et al., 2021) or normalization strategies (Smith et al., 2022). \n\nThe following approaches also use ensemble and regularization in high replay ratio settings. \n\n1. Takuya Hiraoka and Takahisa Imagawa and Taisei Hashimoto and Takashi Onishi and Yoshimasa Tsuruoka, Dropout Q-Functions for Doubly Efficient Reinforcement Learning, International Conference on Learning Representations, 2022\n2. Wu, Yanqiu and Chen, Xinyue and Wang, Che and Zhang, Yiming and Zhou, Zijian and Ross, Keith W, Aggressive Q-Learning with Ensembles: Achieving Both High Sample Efficiency and High Asymptotic Performance, arXiv preprint arXiv:2111.09159, 2021\n\n> To answer this question, we resort to what we call iterated offline RL, ...\n\nIs this the same setting as the following paper?\n\nMatsushima, Tatsuya and Furuta, Hiroki and Matsuo, Yutaka and Nachum, Ofir and Gu, Shixiang, Deployment-efficient reinforcement learning via model-based offline optimization, arXiv preprint arXiv:2006.03647, 2020\n\n\n> Replay Ratio Scaling: Change in an agent\u2019s performance caused by doing more updates for a fixed number of environment interactions.\n\nRegarding SR-SAC, does replay ratio here mean, critic update ratio, actor update ratio, or both?  In the REDQ paper, replay (UTD) ratio means critic update ratio (actor is updated only once per environment step). \n\n\n> The progressive loss of ability to learn and generalize of neural networks and its interaction with RL was only one of the recent empirical discoveries about deep RL.  \n\nThis sounds like an exaggeration. Why \"only\" one? \n",
            "clarity,_quality,_novelty_and_reproducibility": "See my comments in Strengths and Weaknesses. \n",
            "summary_of_the_review": "Overall, I lean somewhat toward Accept.\nThe main weakness of the paper is the lack of novelty, i.e., the methods are not very different from the existing one, and the reset interval is still treated as a hyperparameter (and the analysis of how much it affects the results is not provided in the paper under review).\nOn the other hand, the various experiments conducted to gain a deeper understanding of the reset method are useful and this is the main strength of the paper. \nI think that the results/insights from these experiments will accelerate the development of the reset method in the future.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3149/Reviewer_MFRL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3149/Reviewer_MFRL"
        ]
    },
    {
        "id": "lww1JxpdGg",
        "original": null,
        "number": 3,
        "cdate": 1666532170341,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666532170341,
        "tmdate": 1666532170341,
        "tddate": null,
        "forum": "OpC-9aBBVJe",
        "replyto": "OpC-9aBBVJe",
        "invitation": "ICLR.cc/2023/Conference/Paper3149/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents that we can significantly improve the sample-efficient of prior deep RL approaches by increasing the number of updates per environment steps, and shows that resetting all parameters or part of parameters is critical. The paper shows improved performance on a variety of benchmark tasks, and provides interesting analysis and observations that can be related to the offline RL literature.",
            "strength_and_weaknesses": "Strengths\n- Well written, intuitive approach\n- Strong performance with a very simple idea\n- Exhaustive experiments and interesting analysis and discussions\n\nWeaknesses\n- Given the recent surge of leveraging pre-trained representations for vision-based RL, it would be an interesting investigation to see what would happen when considering such a pre-trained perception module.\n- Investigating the role of replay ratio scaling for model-based approaches, especially the effect on the world models and corresponding policies obtained from the models, would be an interesting addition to the paper, but I don't think this is a necessary and could be a future work.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality.**\n- There's a typo in Section 5.2: ` achieving robust replay ratio scaling for SR-SPR is requires due to its shorter training period and more complex function approximation.`\n- In Figure 3 and Table 2, it could be more helpful for clarity if baselines are introduced with acronyms along with their references.\n- For self-containedness, it would be nice to include the formulation or more detailed explanation on SPR\n\n**Novelty.**\n- Interesting and novel empirical observations\n\n**Reproducibility.**\n- It seems that the approach is not difficult to reproduce; but it would be nice to include source codes for this",
            "summary_of_the_review": "The paper consists of an intuitive approach, nice explanation, strong performance, well supported claims, and interesting discussions. I recommend the paper to be accepted and has no major concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3149/Reviewer_qf9b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3149/Reviewer_qf9b"
        ]
    }
]