[
    {
        "id": "XeuXjjkkwC",
        "original": null,
        "number": 1,
        "cdate": 1666623407245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623407245,
        "tmdate": 1666623407245,
        "tddate": null,
        "forum": "xjb563TH-GH",
        "replyto": "xjb563TH-GH",
        "invitation": "ICLR.cc/2023/Conference/Paper4310/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission entitled \u201cRepresentational dissimilarity metric spaces for stochastic neural networks\u201d proposed a new method to characterize the representational similarity of neural networks, taking into account the noise characteristics. The paper considered metrics over the stochastic shape using p-Wasserstein distance and some variations. Overall, this is a well-written paper. I enjoyed reading it. ",
            "strength_and_weaknesses": "\nStrength: \nNice combination of theory and experiments. \nThe numerical experiments are extensive. \nThe theoretical framework is elegant and will be useful in some applications.\n\nA few concerns:\n\nAlthough the numerical experiments covered several examples,  the insights revealed were quite limited. Some of the results could be obtained by using fairly standard methods to study the mean and the noise covariance structure (e.g., Section 3.2).\n\nThe Gaussian assumption needs to be better justified, perhaps through more simulation experiments.  Having a small number of trials in practice is not a good *justification* of the Gaussian assumption. The maximum entropy argument is also problematic as the maximum entropy distribution depends on the domain being studied. For variables that are non-negative, the maximum entropy distribution will not be Gaussian. \n\nThe innovation on the \u201cstochastic representation\u201d needs to be toned down. This is not a new concept. Every general model with noise can be naturally treated as a \u201cstochastic representation\u201d. The classic RSA approaches can also deal with stochastic presentations, under well-defined noise assumptions (e.g., reviewed recently in Kriegeskorte & Wei, Nature Reviews Neuroscience, 2021). \n\nThe following reference should be cited and discussed, as it is closely related to the current work:\nShahbazi, Mahdiyar, et al. \"Using distance on the Riemannian manifold to compare representations in brain and in models.\" NeuroImage 239 (2021): 118271.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is generally clear.\nThe novelty is moderate.",
            "summary_of_the_review": "Solid paper. The study is well executed with moderate innovation. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4310/Reviewer_pofg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4310/Reviewer_pofg"
        ]
    },
    {
        "id": "Hj6fuLuufK",
        "original": null,
        "number": 2,
        "cdate": 1666832350414,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666832350414,
        "tmdate": 1669853360070,
        "tddate": null,
        "forum": "xjb563TH-GH",
        "replyto": "xjb563TH-GH",
        "invitation": "ICLR.cc/2023/Conference/Paper4310/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces proper distance metrics for quantifying the similarity of representations in stochastic networks, either biological or artificial. The paper builds on a recent article by Williams, which described metrics for quantifying similarity in deterministic networks by allowing for a set of preprocessing steps (e.g., whitening) and alignment operations (e.g., rotation) before applying a \u201cground metric\u201d (e.g., Euclidean norm). This paper uses the same framework but applies it to a set of probabilistic ground metrics that were chosen to converge to the Euclidean norm in cases where the network is deterministic. The paper focuses on a Gaussian approximation to one of these metrics (Wassertein distance). The Gaussian approximation results in a metric that has two terms, one based on the similarity of the means, which might be thought of as the deterministic component, and one based on the similarity of the covariances after rotational alignment. They also describe a meta-metric that linearly interpolates between these terms in order to emphasize either the means or covariances. \n\nThe authors apply their framework to a set of toy networks that differ in their covariance scale and correlational structure and report that their metrics can uncover the latent structure (Fig 2C). They then apply their approach to neural recordings from mouse primary visual cortex to gratings and natural scenes (Fig 3). They find that their similarity metric is dominated by the noise covariance structure for static gratings but not for natural scenes, presumably because natural scenes drive more variation in the mean response. The authors then apply their method to a large collection of 1800 VAEs, and report that their distance metrics can be used to decode a variety of network properties with varying degrees of accuracy (Fig 4). Finally, the apply their model to a Patch-Gaussian network trained with varying amounts and sizes of Gaussian noise, and report that their metrics can recover this structure to some degree (Fig 5). \n",
            "strength_and_weaknesses": "Strengths\n\nThe paper addresses an interesting and important topic: how to deal with stochasticity when quantifying large-scale similarity measures between biological or artificial networks. As noted, virtually all prior work has focused on the deterministic case, with comparatively superficial treatments of noise covariance structure (e.g., mahalanobis-based correction metrics). \n\nI found the writing clear, logical, and easy to follow. \n\nApplying their method to both biological and artificial networks is a strength in my opinion. \n\nThe method seems like it scales well to large datasets and many networks though not much information is reported about computation time as far as I can tell. \n\nWeaknesses\n\nAll the metrics are motivated by the desire to reproduce a sensible Euclidean metric in the deterministic limit. This seems like a potentially desirable property, but it doesn\u2019t say anything about how metric behaves in the stochastic case, which is the whole point. In Figure 1, they illustrate how the covariance structure of a set of responses can impact discriminability, but it is not obvious how their metric relates to the stimulus discriminability. This strikes me as a major shortcoming, since it makes it hard to understand what the metric tells you. \n\nTheir Gaussian-approximated Wasserstein metric allows for a rotation of the covariance structure. This rotation appears to be separate from any rotation allowed to align the two networks. It is not clear to me why this is appropriate or desirable. Is this partly why the mean-insensitive version of their metric does not produce the desired result in their toy dataset?\n\nThe experiments are useful demonstrations of the method, but don\u2019t seem to shed new light on brain or network function. The fact that stimulus-driven variation is greater for natural stimuli doesn\u2019t seem particularly surprising and presumably explains the findings in Figure 3B/3C. It is also not clear that their analysis tells us much about noise structure. In Figure 3F, what is each datapoint? A session? Why would we expect distances to covary across sessions between natural scenes and gratings? Do the neurons differ across sessions?\n\nOne thing that would be interesting is to know is the extent to which the noise covariance structure of a visual region (e.g., V1) is stereotyped, in the sense of being similar across animals but distinct across regions, and whether this structure is predictable from the stimulus-driven component. This seems like the type of question their method might be able to answer. \n\nThe authors seem to make a big deal out of whether distances are above or below the diagonal when plotting the covariance- and mean-insensitive measures against each other (e.g., Fig 3B/C and 4B). It\u2019s not clear to me that this has any particular significance. Why are squared distances between the means comparable to the Frobenius norm of the covariances?\n\nI can\u2019t tell if the fact that you can decode network properties from their measures is notable (e.g., Fig 4). There is no baseline comparison against any alternative model. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The extension of representational similarity analyses to covariance structure is novel and important in my opinion. I don\u2019t think there are any particularly important or novel findings from their analyses applying this method. \n\nI found the paper clear and easy to follow.\n\nThe paper generally seems of high quality, and I don\u2019t have any particular concerns about reproducibility. \n\nI did not have the appropriate expertise to check the appendix proofs. \n",
            "summary_of_the_review": "The paper develops a framework to extend representational similarity to the stochastic regime, which I feel is an important contribution. The metric is solely motivated by how it behaves in the deterministic case which is a notable limitation in my opinion. There are no major novel findings from their analyses applying their method, but the analyses do serve to illustrate and validate the approach. The paper generally seems of high quality. \n\nPost-rebuttal comments:\n\nI apologize for my slow response. I thank the authors for taking so much time to consider and respond to my questions and comments. I was impressed with all of their responses including to other reviewers. I have upgraded my score from a 6 to an 8. I hope to see this paper accepted. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4310/Reviewer_Moyn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4310/Reviewer_Moyn"
        ]
    },
    {
        "id": "kmbL40Ss_E3",
        "original": null,
        "number": 3,
        "cdate": 1667530437571,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667530437571,
        "tmdate": 1667531563439,
        "tddate": null,
        "forum": "xjb563TH-GH",
        "replyto": "xjb563TH-GH",
        "invitation": "ICLR.cc/2023/Conference/Paper4310/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a novel class of stochastic shape metrics, which are suitable for assessing similarities in the representations of both biological and artificial stochastic networks.  The paper presents a thorough analysis of the formal properties of their class of metrics, showing they satisfy the metric axioms, that they have other desirable formal properties, and their relationship to certain previous metrics.  In their experimentation, they focus on a generalized form of 2-Wasserstein distance, which allows a fine-tuning of its sensitivity to mean vs covariance structure, and has a tractable analytic estimator.  They provide a series of case studies demonstrating the use of this metric on synthetic, biological and artificial networks, leading to some intriguing observations. ",
            "strength_and_weaknesses": "Strengths:\n\n- The problem of measuring distances between representations in stochastic networks is an important one, both in neuroscience and machine learning.  The formal properties investigated by the authors are well motivated, such as invariance to network permutations.  The authors offer a clean and general solution to this problem.\n- Some interesting observations are made in the experimentation, which I'm not aware of having being observed before.  Particularly, the observation that the response of biological networks (using Allen mouse brain observatory data) to different classes of responses can generate representations primarily focused on the mean or the covariance (noise) structure, and that this is mirrored in artificial networks pre and post training.  Also, the observation that many artificial network training properties, such as random seed, reconstruction loss and regularization can be predicted from a network's noise structure is unexpected, and may be relevant for instance in predicting which networks will generalize well (e.g. in statistical learning theory).\n\nWeaknesses:\n\n- It is unclear whether the 'minimization' operation over the group of nuisance transformations is necessarily the right way of assessing distances between stochastic representations (which is used since the work generalizes previous work on deterministic shape metrics).  Potentially, some form of uncertainty about the best alignment should also be preserved over the group of nuisance transformations.\n- The empirical work is somewhat preliminary, in the sense that many of the observations could be followed up in more detail with more substantial studies.  It is also limited in the sense that only covariance (pairwise) structure is considered when comparing 'noise' distributions empirically, while the framework would allow for higher-order properties of the distributions to be compared.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, quality, novelty and reproducibility are all good.",
            "summary_of_the_review": "I recommend acceptance of the paper, since it offers a general solution to an important problem that will be of interest both to the neuroscience and machine learning communities.  Further, the results will be of interest to both communities (although, as noted above, several of the observations could perhaps be followed up in more detail in making the case for the use of generalized forms of stochastic metrics).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4310/Reviewer_yN72"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4310/Reviewer_yN72"
        ]
    }
]