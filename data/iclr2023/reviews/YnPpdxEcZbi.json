[
    {
        "id": "kaHUz9HXNM",
        "original": null,
        "number": 1,
        "cdate": 1666326919050,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666326919050,
        "tmdate": 1666326919050,
        "tddate": null,
        "forum": "YnPpdxEcZbi",
        "replyto": "YnPpdxEcZbi",
        "invitation": "ICLR.cc/2023/Conference/Paper5281/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose TCSR; they perform experiments to show that it improves upon EfficientZero and achieves SOTA performance on the Atari100K benchmark.\n",
            "strength_and_weaknesses": "This is an interesting and well-motivated paper studying an important topic.\n\nHowever, the contribution is entirely empirical (the contribution boils down to the proposal of TCSR, and the claim that TCSR \u201cachieves SoTA performance\u201d), and the experiments are deeply flawed.  The results are based on 3 runs, which is unacceptable in RL, due to the high variance between runs.  This topic has been repeatedly covered in publications, talks, etc. in recent years, so I will not elaborate further.  It is interesting to notice, however, the fact that the \u201creported EfficientZero\u201d score greatly outperforms the TCSR score.  The large discrepancy between the \u201creported EfficientZero\u201d score and the \u201csource code re-run EfficientZero\u201d score likely comes from the low number of runs used.  This discrepancy is an excellent illustration of the problem with only using 3 runs.\n\nClarity issues:\n- The loss functions that are in the definitions of the MuZero loss are not defined (nor is it easy to infer what they are).\n- Similarly, the SimSiam loss is not defined.\n- g_{state} is not defined.\n- The difference between s^ and s~ (defined at the end of section 3) is not clear.  They \u201ceach represent an augmentation of a parameter\u201d.  So there are two parameters to augment?  (I do not think that\u2019s correct, but I do not know how else to read it.)\n\nMinor edits:\n- \u201creward is sparse(Shelhamer et al., 2016)\u201d (missing a space)\n- \u201cThe training pipeline is as shown in Figure 3\u201d (missing period)",
            "clarity,_quality,_novelty_and_reproducibility": "The quality is poor, primarily due to the flaw with the main contribution discussed above.  The novelty seems to be slightly limited, but is adequate.  Regarding clarity: the paper is well-written in general, but has some severe clarity issues in a few places, including undefined terms (see above).",
            "summary_of_the_review": "This is an interesting and well-motivated paper studying an important topic.  However, the contribution is entirely empirical, and the experiments are deeply flawed.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5281/Reviewer_1Xom"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5281/Reviewer_1Xom"
        ]
    },
    {
        "id": "Vvoe5hbT-H",
        "original": null,
        "number": 2,
        "cdate": 1666618341979,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666618341979,
        "tmdate": 1666618341979,
        "tddate": null,
        "forum": "YnPpdxEcZbi",
        "replyto": "YnPpdxEcZbi",
        "invitation": "ICLR.cc/2023/Conference/Paper5281/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new auxiliary loss for learning representations for reinforcement learning from images. The loss penalizes the difference between the change of two subsequent latent states for one image augmentation and the change of two subsequent states for a different augmentation. By integrating this loss into \"EfficientZero\" (Ye et al. 2021) the performance on Atari benchmarks good be improved.\n",
            "strength_and_weaknesses": "Strength:\n- The paper presents a novel and reasonable auxiliary, that achieves good empirical results.\n\nWeaknesses:\n- The contribution is rather small, as the modification of the previous method is only incremental and not theoretically justified.\n- The presentation is sloppy.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- Although the content of the paper is relatively small, it is difficult to understand the proposed method without knowing the related work in detail, since the paper not at all self-contained. For example, not even the actual loss (the main contribution) is shown in the paper; instead, the reader is assumed to know $\\mathcal{L}_{\\text{SimSiam}}$. Furthermore, the difference between the policy $\\pi$ and $p$ is not clear as they have not been properly defined. The paper should discuss EfficientZero and the SimSiam loss in more detail, which could have been done in the current submission, given that it only used a bit more than seven pages.\n- The paper is not well-structured. For example, Fig. 1 is mentioned in the Appendix, but shows a comparison with EfficientZero which has not been mentioned at that point. The actual Figure is only described in the experiments section at the end of the paper.\n- Also the writing is quite sloppy and even contains errors that could have been automatically found with a spell check (\"represnetation\"). \nSeveral sentences even have multiple mistakes.\n\nQuality and Reproducibility:\n- The paper contains little information on how the experimental study has been conducted. The paper does not come with any appendix nor source code and is therefore not at all reproducible. It is not reported which hyperparameters and network architectures have been used and how they have been tuned.\n- For many environments, the proposed method performed worse than the previously reported results of EfficientZero. While the paper argues that the comparison is fair by using the same implementation (of the original authors of EfficientZero) for evaluating the new method and for rerunning EfficientZero, it seems like the authors of the current submission did not even try to reproduce the reported EfficientZero results beyond running the code on their own machine. \n\nNovelty:\n- The novelty of the paper lies given only by a new loss term, which was not well motivated.\n ",
            "summary_of_the_review": "The contribution of the paper is incremental, but could be somewhat interesting.\nHowever, the communication of the work is substandard, and the paper would require a major revision by:\n- describing the proposed method in a more self-contained way,\n- providing an in-depth description on the experimental procedure,\n- improving the structure of the paper,\n- and, ideally, investigating the discrepancy between the reported results of EfficientZero, and the results obtained in the current experiment (I recommend contacting the authors).\nFurthermore, providing the source code for reproducing the experiments would be highly appreciated.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5281/Reviewer_JdZQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5281/Reviewer_JdZQ"
        ]
    },
    {
        "id": "0jES9RkJeT",
        "original": null,
        "number": 3,
        "cdate": 1666662951557,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662951557,
        "tmdate": 1666662951557,
        "tddate": null,
        "forum": "YnPpdxEcZbi",
        "replyto": "YnPpdxEcZbi",
        "invitation": "ICLR.cc/2023/Conference/Paper5281/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes Temporal Change Sensitive Representation, a self-supervised representation algorithm that seeks to capture the temporal representation changes in RL. Experiments on different datasets suggest that the proposed algorithm does outperform benchmark SOTAs in the Atari100k environment. ",
            "strength_and_weaknesses": "Strength:\n\nThe paper is well-organized. Each section does what it aims to do. The algorithms are described very clearly.\n\nWeaknesses:\n\n1. The extent of novelty is relatively low. On a high level, the paper appears to be a combination of the existing EfficientZero algorithm(a temporal-aware variant of the MuZero algorithm over non supervised contents, with data augmentation and minor adjustments over similarity losses. More elaboration on novelty would be helpful.\n\n2. The motivation should be stated more clearly. While the experiments are adequate, there should be more explanation as to how/why each set of experiments correspond to the context/setting of the problem. Additionally, it would be helpful to provide some visual illustrations(or other qualitative demonstrations) as to why the algorithm works, as opposed to simply listing the performances SOTA and conclude accordingly. \n\n3. Would it be possible to incorporate experiments on other datasets? Empirical results solely on Atari-based datasets seem a bit inadequate, especially when the algorithm seeks to address representational learning for reinforcement learning problems in general.   ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly, and the results/experiments contain adequate details to be convincing.  ",
            "summary_of_the_review": "My rating would be a 3, though I'm happy to adjust my score if the authors address my concern. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5281/Reviewer_yZGc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5281/Reviewer_yZGc"
        ]
    },
    {
        "id": "TqKPGtoknSD",
        "original": null,
        "number": 4,
        "cdate": 1666680576932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680576932,
        "tmdate": 1666680710250,
        "tddate": null,
        "forum": "YnPpdxEcZbi",
        "replyto": "YnPpdxEcZbi",
        "invitation": "ICLR.cc/2023/Conference/Paper5281/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a method named TCSR which is based on EfficientZero. TCSR helps a reinforcement learning agent to learn better latent representations in a self supervised manner.",
            "strength_and_weaknesses": "Weaknesses: \nThe paper has several weakness in writing, results, explanation of the approach. See below: \n\nResults: \n- The paper does not evaluate the proposed method with significant experiments. Only Table 1 shows results on Atart100K which shows improvement in 11 out of 26 games as compared to the author's claim of 17 out of 26. These results are also not consistent with Figure 2. The table results are also not explained. \n- The visualization shown Figure 1 is highly basic and any encoder should be able to encode the representations. Why many method are failing to encode it is not explained anywhere in the paper. \n- Training details are not provided, the results seems not reproducible with any implementation details. \n- The author's claim self-supervision. I didn't find any information which help TCSR to train in a self-supervised manner. \n\nMethod: \n- The approach is not detailed. The reader cannot understand the proposed method as many details are missing or not explained. For example, Figure 3 is never explained and it is the training pipeline. I am not able to infer what is the training part in this figure. \n- The notations are not clear. In s_t^k, s is state at time t but what is k?\n- The authors gave a good emphasis on consistent loss but it is not explained. an equation mentioned it in sec 3.2 but what is the loss? The equations are not numbered. \n- The TCSR loss in sec 4.1 is not clear. Why t+k time is the target for state at time t?\n\nWriting: \n- Sec 3.1, first paragraph third line, encodes a observation\n- Sec 3.2, represnetation\n- Full stop in last line of page 5, many places in sec 4.1. \n- Table:1 can be written as Table 1.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not clear at many places and the proposed method is not detailed enough. No implementation details are provided which makes the paper not reproducible. ",
            "summary_of_the_review": "The paper has several weaknesses and it is not ready for publication. See weaknesses for justification.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5281/Reviewer_VxAy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5281/Reviewer_VxAy"
        ]
    }
]