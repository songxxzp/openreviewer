[
    {
        "id": "0ow0s2EUt1-",
        "original": null,
        "number": 1,
        "cdate": 1666579165880,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579165880,
        "tmdate": 1666579165880,
        "tddate": null,
        "forum": "OhUAblg27z",
        "replyto": "OhUAblg27z",
        "invitation": "ICLR.cc/2023/Conference/Paper2177/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper is trying to solve the problem that when there exists a scenario in the offline RL dataset: Mostly low return trajectory, few high return trajectory, indicating the mixed policies lying behind, the results could be not ideal.\n\nBy introducing the RPSV, the variance of the positive case and negative cases are well captured in the original dataset and is helping to infer the distribution of high-return and low-return policies.\n\nAnd the author base on the variance of PSV proposed two weighting methods. Abundant experiment on CQL, IQL, TD3+BC, sole BC, etc. from various environments like classic control has justified the effectiveness of different methods. Detailed explanations of experiments and deductions are shown in the appendix.\n",
            "strength_and_weaknesses": "Strength:\n\n- The novelty is clear. The author is trying to analyze the mixed dataset\u2019s efficiency in a theoretical way, representing and capturing the tendency of PSV.\n\n- The author proposed the two methods in a rigorous logic, return-weighting and advantage weighting are straightforward by the definition. \n\n- The sufficient experiments are evidently showing that the proposed two methods on different baseline methods, could better capture the dataset\u2019s trajectory original feature, and by reweighting the importance of the high-return trajectory, the method\u2019s effectiveness is well improved.\n\nWeakness:\n\n- It\u2019s noticeable from Figure2 that the two weighting methods sometimes have a different appearance, it would be better if the author could analyze the different suitable situations for the above different two methods.\n\n- A little confusing part may be the introduction of negative-sided variance(NSV), because the majority part of the paper is mainly focusing on the RPSV, is NAV the NSV used in the implementation part of two re-weighting methods? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is of good quality, overall clear in the explanation. The example code was not provided, but the instructions on environment and realization details are well presented.",
            "summary_of_the_review": "The author is trying to use novel ways to analyze the distribution of offline training datasets. Which helped to improve the performance of the offline RL methods and also justify the applicability to other methods. The paper is presented in an explicit way, also with necessary details from the appendix part. \n\nIt\u2019s meaningful work to help offline RL algorithms improve the exploitation in the limited amount of high return trajectories.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2177/Reviewer_Z9su"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2177/Reviewer_Z9su"
        ]
    },
    {
        "id": "WMx6UuDNpx",
        "original": null,
        "number": 2,
        "cdate": 1666642989932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642989932,
        "tmdate": 1669597781858,
        "tddate": null,
        "forum": "OhUAblg27z",
        "replyto": "OhUAblg27z",
        "invitation": "ICLR.cc/2023/Conference/Paper2177/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores a particular setting of low sparse high reward trajectories in an offline RL datasets. Offline RL algorithms that explicitly do Behavioral Cloning (BC) have a hard time learning good policies in these settings. The authors propose a simple reweighing scheme to induce an artificial dataset with a higher performing behavioral policy. They also show theoretical analysis regarding the bias of this estimate for deterministic settings and also introduce Return Positive Sided Variance as a measure of quantifying the variance in the returns of trajectories among the dataset. Finally the experimental results show that the weighted sampling of the dataset significantly helps in the artificially generated datasets with varying RPSV however the performance gains on true D4RL tasks are marginal especially for methods without explicit BC like CQL and IQL.",
            "strength_and_weaknesses": "Strengths:\n\n- The weighted sampling helps combating the need for a near expert dataset for approaches with explicit behavioral cloning terms.\n    \n- The weighted sampling approach has been demonstrated to not hurt the performance even for well balance datasets across various modern offline rl approaches (CQL and IQL).\n    \n\nWeakness:\n\n- The results while impressive is not surprising. \u00a0While the gains are signficant for BC and TD3+BC, it is well established that these methods will suffer in sparse high reward trajectory settings. The gains over CQL and IQL are marginal for the current D4RL benchmark.\n    \n- While the authors introduce RPSV as a measure of tracking the variance in the dataset, it does not answer the question of \u201cWhen is it necessary to do weighted sampling?\u201d. i.e. The RPSV metric follows a inverted U curve as we increase sigma from 1% to 100% in the artifical datasets that were created.\n    \n- For this approach to be applicable, the data needs to be collected such that the trajectories are complete; i.e. The behavioral policies have to start from the starting state distribution. While this is not that hard of a constraint it is to be noted that this approach does not make sense if the collected policies start from different parts of the state-space. (i.e. trajectory stitching settings.)\n    \n- Probability of improvement over uniform sampling 3(b) is very skewed compared to the actual amount of improvement we see in figure 3(a). Here it seems that our approach is equally valid for both IQL and TD3+BC according to fig 3(b) whereas 3(a) tells a different story. overall I feel fig 3(b) does not add much value.\n\nMinor points.\n\n- \u201c\u201cHowever, offline IL requires expert and random data to be separated while we neither assume separated datasets nor access to expert data.\u201d ([pdf](zotero://open-pdf/library/items/BFZJEQLU?page=9)) \u201d This is not fully accurate as we are using the rewards to rescale sampling we are assuming proxies to both a \u201cseparate dataset\u201d and access to \u201cexpert data\u201d offline IL does not require a expert data, its just that it wont work without one, as with the case with this approach.- \u00a0\n    \n- sigma has different scale in page 14 and 15. (0.01 and 1 )\n    \n- The formatting /Bolding has errors in the tables in appendices. \u00a0\n    \n- It would be wonderful to include plans to release the sampling code snippets for the public.\n    \n- IQL numbers for hopper is not matching with the author provided numbers, It might be good idea to doublecheck them.",
            "clarity,_quality,_novelty_and_reproducibility": "-",
            "summary_of_the_review": "Overall the paper presents a relative simple weighted sampling method which can be a valuable tool in an offline-rl toolkit, however, the metric RPSV presented in the paper fails to capture the scenario where the method can/should be applied. This can be an interesting paper with a better metric / analysis on when the approach should be considered during offline RL, however, under the current version, I am leaning toward a rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2177/Reviewer_BiNt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2177/Reviewer_BiNt"
        ]
    },
    {
        "id": "zABxdzVCs",
        "original": null,
        "number": 3,
        "cdate": 1666670885058,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670885058,
        "tmdate": 1668883810786,
        "tddate": null,
        "forum": "OhUAblg27z",
        "replyto": "OhUAblg27z",
        "invitation": "ICLR.cc/2023/Conference/Paper2177/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to reweight episodes in the replay by their advantage values. The paper claims that doing so is especially useful when there are many more low return episodes in the replay buffer compared to high return episodes. ",
            "strength_and_weaknesses": "Strength\n\nThe proposed idea is intuitive and the claim that the idea is useful when there are many more low return episodes than high return episodes makes sense. \n\nWeaknesses\n\nLack of comparison to simple baseline. The issue that the paper studies is well-known, and a common technique to tackle it is to ensure each batch of episodes used to update the networks contain half successful, and half failed episodes. I would have liked to see the comparison to this simple and common baselines.\n\nParts of the writing are not clear. For example:\n\n- \"cold start performance boost\" in the Introduction.\n\n- \"Most offline RL algorithms are anchored to the behavior policy\". What are anchors mean, and where is the evidence that support this claims?\n\n- How should I interpret figure 2? The caption states that the proposed methods outperform baselines, but how is this conclusion supported by the figure?",
            "clarity,_quality,_novelty_and_reproducibility": "The idea proposed in the paper is novel, as far as I know. But the paper lacks comparison to simple baseline that is commonly used to tackle this problem. The caption of the figure does not clearly explain the reasoning as to why the information shown demonstrates that the proposed method outperforms baselines.",
            "summary_of_the_review": "I look forward to the rebuttal. The idea is simple, and may therefore be used widely. But as it currently stands, it is unclear if the method outperforms simple and common baselines.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2177/Reviewer_C84m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2177/Reviewer_C84m"
        ]
    },
    {
        "id": "AnyEinotOIg",
        "original": null,
        "number": 4,
        "cdate": 1667082897618,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667082897618,
        "tmdate": 1667082897618,
        "tddate": null,
        "forum": "OhUAblg27z",
        "replyto": "OhUAblg27z",
        "invitation": "ICLR.cc/2023/Conference/Paper2177/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors tackle a long-known problem in offline learning where the performance of the RL agent is highly dependent on the distribution of high and low return trajectories. It follows that in datasets that contain a high percentage of low-returning trajectories and a low percentage of high-returning trajectories. The Agent will be overly restrained by the low-return trajectories. \n\nThe authors tackle this problem by implementing trajectory weighting, where different weights are assigned to different trajectories of different importance, naturally a more important trajectory would have a more important weight. After assigning weights to each trajectory, the authors add an entropy regularization term.",
            "strength_and_weaknesses": "Main concern: \n- How prevalent is this problem in offline RL? I see the table in A.3, but it does not give the whole image of this problem. Is this problem seen in other mainstream datasets?\n- How does this method compare to the other sampling strategy methods? Why should this method be used over the other ones?\n- Would making the model learn the policy on offline datasets make the model overfit?\n- Is this method robust under different distributions of dataset? For example in Figure 1, what if the dataset has two density spikes, on 0.0, and 0.8.\n- In the result part, there is no comparison between the two methods, the authors only gave experiments on their method vs the vanilla method of no weighted distributions.",
            "clarity,_quality,_novelty_and_reproducibility": "About Novelty: \n- This problem itself is not novel, as a lot of work in the past has been about solving the distribution skew in offline datasets. But their main approaches are by skewing the sampling methods. Thus the idea of introducing a weight in sampled data is novel. \n\nAbout Organization: \n- I feel that the paper\u2019s structure is very clear and concise, from preliminary to results. From what the problem is, and why the problem needs to be addressed, all the way to the author\u2019s methods.\n- Section 3 has a lot of reasons for the motivation of the work, especially in figure 1, the analysis is clear and very insightful.",
            "summary_of_the_review": "About Challenge: \n- Without collecting additional data, how can the performance of the policy be improved? Previous works have tackled this by skewing their sampling.\n\nAbout Contribution: \n- The authors reweight the transitions in the dataset, and regularize it by adding an entropy regularization term. Then they further change the reweighting term to equation 11 in the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2177/Reviewer_56VB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2177/Reviewer_56VB"
        ]
    }
]