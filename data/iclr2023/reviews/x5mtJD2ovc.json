[
    {
        "id": "B5GtRoJPZCX",
        "original": null,
        "number": 1,
        "cdate": 1666468912633,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666468912633,
        "tmdate": 1670403162682,
        "tddate": null,
        "forum": "x5mtJD2ovc",
        "replyto": "x5mtJD2ovc",
        "invitation": "ICLR.cc/2023/Conference/Paper224/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to train a conditional diffusion models by removing the necessary of annotation pairs. More detailed, they utilize the kNN retrieved image embedding as condition signal, and can further utilize caption embedding due to the almost-perfect joint distribution between text and images.  The framework also facilitates the generation of out-of-distribution sampling. One extra bonus is they can remove the necessity of mask annotation when conducting local semantic manipulations. \n\nExperimental results demonstrate effectiveness of the proposed method on zero-shot text-to-image generation, the metric is FID and user study around image quality and text-alignment.",
            "strength_and_weaknesses": "Pros:\n\n-  The method framework is well motivated to sample from the out-of-distribution images.\n-  Competitive results compared with several baselines.\n-  Related works are quite thorough for new beginners.\n\nCons:\n\n- Writing is a little rush, the preliminary knowledge of diffusion model is missing, which makes the method part is very unfriendly to novice of this area.  \n\n- Why KNN can solve text-to-image models annotation problem in abstract? It seems that your method is condition on retrieved features instead of text/caption.\n- Figure 3 is too rush, what is MMMM, 9143? I am totally missed about what you are trying to express in this figure. The data flow is full of confusion.\n- Evaluation details are missing, how to generate condition in your sampling process? Where do those conditioned text/image come from?  Where is the reference set from? How many samples are used to calculate the FID in Table 1 and 2? \n- Training and network details are missing, how many parameters in your method, is the parameter number almost the same between your method and the proposed baselines. How many GPUs did you use? Learning rate? optimizer? batch size? epoch number? training time? weight decay? \n\n\nMinor comments:\n\n- The full comparison is available in the supplement.\n- The dataset intro should go to experiments part.\n- Figure 6,9 is saved as vector image, pdf.\n- The font size of Figure 6 is too small.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: 8/10\n\nClarity: 8/10\n\nNovelty: 5/10\n\nReproducibility: 8/10",
            "summary_of_the_review": "The motivation is well consolidated, while too many details about training/inference/preliminary knowledge are missing. Also, the paper is not easy to follow. I suggest the authors to spend more time about the writing of this paper.\n\n=======\n\npost-review update:\n\nThe paper has been improved a lot based on my concerns about description, preliminary background, training details, etc. I think the quality of current version is enough for ICLR. I also share similar concern about the marginal novelty of the proposed methods with other reviewers.  Since advantages outweigh disadvantages, I upgraded my result.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper224/Reviewer_o44L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper224/Reviewer_o44L"
        ]
    },
    {
        "id": "DTzuBDDOs_4",
        "original": null,
        "number": 2,
        "cdate": 1666619597627,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619597627,
        "tmdate": 1666619597627,
        "tddate": null,
        "forum": "x5mtJD2ovc",
        "replyto": "x5mtJD2ovc",
        "invitation": "ICLR.cc/2023/Conference/Paper224/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an approach to improve the training performance and overall quality of the recently introduced text-to-image generative framework. By leveraging a kNN-based retrieval method, the proposed method 1) adds more flexibility to the training data, 2) greatly reduces the computational cost. Extensive experiments and ablation studies are conducted to prove the efficacy and the efficiency of the model. \nThe description of the technical approach is confusing and vague. In Section 3, the authors firstly criticize the use of CLIP embedding since it alone cannot accurately bridge the gap between the text and image distributions. However, the authors still choose to use the CLIP pretrained model to do the job without any modifications. Without any proper justifications on how the proposed retrieval model could solve the problem, the practical motivation of the paper is lacking.",
            "strength_and_weaknesses": "Strengths:\n1.\tThe paper is generally well-written and easy to follow.\n2.\tThe paper introduces an easy approach to facilitate the training of text-free text-to-image generation.\n3.\tThe experiments are comprehensive, it demonstrates the strength of the proposed method from different perspectives.\nWeakness:\n1.\tThe technical novelty might be limited, its model is conceptually similar to several previous works (e.g., CLIP, CogView, and LAFITE). The main contribution lies in the training strategy (kNN Retrieval) rather than in the techniques.\n2.\tThe Retrieval Model (Section 3) is described very vaguely. The provided details are not sufficient to understand the module and re-implement it.\n3.\tIt would be also helpful to release the code and pre-trained model for subsequent research.",
            "clarity,_quality,_novelty_and_reproducibility": "Given the limited motivation and technical novelty of the work, I don\u2019t think the current version meets the standard of the ICLR. ",
            "summary_of_the_review": "This is a good paper with promising experimental results, with some aspects of the presentation that should be improved and clarified. Also, I have the following questions regarding to the paper.\n\nQuestions to Authors:\n1.\tCould the authors provide more justifications on how could adding the retrieval part bridge the gap between the text and image distributions?\n2.\tHow does the encoders map text descriptions and image samples to a joint feature space? Do you merely use the pretrained models?\n3.\tThe subsection of \u201cImage generation network\u201d also lacks sufficient details. How does the generation network generate new images with conditioned on other similar instances? The equations in the subsection don\u2019t indicate any signals that this is a conditional generation framework. \n4.\tHow does the kNN search avoid the problem of \u201ccurse of dimensionality\u201d?\n5.\tWithout any details of the parameter setting and providing model details, the experimental merit is also hard to evaluate.\n6.\tThe scalability of the model is highlighted as one of the primary contributions. However, the corresponding scalability analysis experiment is not found in the paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper224/Reviewer_myk8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper224/Reviewer_myk8"
        ]
    },
    {
        "id": "PYJZmJCqnD",
        "original": null,
        "number": 3,
        "cdate": 1666634205974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634205974,
        "tmdate": 1669435503681,
        "tddate": null,
        "forum": "x5mtJD2ovc",
        "replyto": "x5mtJD2ovc",
        "invitation": "ICLR.cc/2023/Conference/Paper224/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method for generating images from text with an addition of a retrieval component. Authors use pretrained text and image encoders and train diffusion-based generative models that use an image embedding and a set of image embeddings closest to the target one to reconstruct the image. During inference, instead of the image embedding, a text embedding is used (which comes from the frozen clip encoder, pretrained on image-text pairs). Authors demonstrate that the retrieval component is crucial for image quality both in online and in human studies and also extend the functionality for global image manipulation, which looks rather convincing.",
            "strength_and_weaknesses": "Strengths:\n1) The paper presentation is overall good. Illustrations make sense, most of the ideas are clearly expressed.\n2) The results look good, both FID and user studies. I was a bit surprised that having just embeddings as input (without the access to raw images) is enough to make a big difference. I guess the neighbors provide the hint of how far the query is from the real images, so it allows the model to prefer directions to the real embeddings.\n\nWeaknesses:\n1) A big red flag for me is the claim that no text pairs were used during the training. To reproduce the model from data one would have to pretrain the encoders on the paired data, so this claim is misleading. I highly recommend to put it into correct context to avoid reader confusion.\n2) There are some details missing. E.g. I could not find the value of K during training, which should help interpret Fig. 9 (maybe the optimal number of neighbors is just the train one?)\n3) Not clear whether the solution is going to be open-sourced.\n\n\nMisc: Sometimes there is kNN in the text/figures, sometimes it is KNN, sometimes knn. Sometimes it is k, sometimes K, sometimes it is kNN. It also makes sense to explicitly state that Image quality and Text-alignment are the human evaluation metrics (e.g. it is mentioned for table 2, but not for table 1). ",
            "clarity,_quality,_novelty_and_reproducibility": "From novelty point of view, it seems that most of the building blocks of the solution were known before (the retrieval part, the generation part). Maybe the image manipulation method stands out; at least I never saw a similar idea. \n\nI also would recommend discussing paper on semi-parametric image generation which used raw pixels as input, like Qi, Xiaojuan, et al. \"Semi-parametric image synthesis.\" CVPR2018 or Iskakov, Karim. \"Semi-parametric image inpainting.\" arXiv preprint arXiv:1807.02855 (2018). ",
            "summary_of_the_review": "Overall, I think this is a decent paper. I my main criticism is about claim \"training ... model without any text\" which might very misleading. I believe it has to be fixed.\n\nUpdate after the discussion - my comments were address during the rebuttal period, so I updating the rating (given the code will be open-sourced).\nAlso I suggest the authors to update Figure 3 - the kNN part there is hard to understand (both what it does and what is the output)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper224/Reviewer_5XrX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper224/Reviewer_5XrX"
        ]
    },
    {
        "id": "rxLzgStTwB",
        "original": null,
        "number": 4,
        "cdate": 1666653216900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653216900,
        "tmdate": 1666653216900,
        "tddate": null,
        "forum": "x5mtJD2ovc",
        "replyto": "x5mtJD2ovc",
        "invitation": "ICLR.cc/2023/Conference/Paper224/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to apply retrieval in text-to-image generation based on an encoder-decoder architecture. K-nearest neighbors of some image database is computed on top of the encoder output, and a transformer-based diffusion network is applied on these neighbors to produce the final output image.\n\nThe main idea of the paper is that by leveraging the retrieval process, smaller training dataset can be used, and even out-of-distribution image generation is possible by choosing appropriate database from which to draw the k-nearest neighbors.\n\nIn settings controlled for backbones author show significant improvements on common image generation metrics.",
            "strength_and_weaknesses": "Strengths\n- [S1] The application of retrieval is interesting and tackling limited training data is a practical problem.\n- [S2] The controlled settings show strong performance.\n\nWeaknesses\n- [W1] Benchmark seems weak. Not a lot of baselines, and main results are only zero-shot but it's not clear that this is the focus of the paper.\n- [W2] Some drawbacks of retrieval-based methods are not well discussed. For example, the choice of k is highly dataset specific, how would someone select this in practice? The proposed method also do not scale well to larger dataset if every query involves a retrieval against the whole dataset.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow. \nQuality is okay, it could use more experiments. \nThe method is not novel but the application is pretty interesting.",
            "summary_of_the_review": "I recommend weak accept. The experimentation could be stronger, but the idea and the application is quite interesting. The problem this paper tackles is relevant.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper224/Reviewer_yR24"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper224/Reviewer_yR24"
        ]
    }
]