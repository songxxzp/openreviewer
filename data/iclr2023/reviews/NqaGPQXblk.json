[
    {
        "id": "X0faCx3Znm0",
        "original": null,
        "number": 1,
        "cdate": 1666506026751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666506026751,
        "tmdate": 1668993637954,
        "tddate": null,
        "forum": "NqaGPQXblk",
        "replyto": "NqaGPQXblk",
        "invitation": "ICLR.cc/2023/Conference/Paper1398/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Summary:\n- The paper proposes a new task called Visual Transformation Telling (VTT), which is defined as predicting the intermediate steps ('steps' are referred to as \u2018transformations\u2019 in the paper) in forms of natural language given the start and end state of each step. \n- Two existing instructional video understanding datasets, CrossTask and COIN, are combined to form the VTT benchmark. \n- A Transformation Telling Net (TTNet), consisting of the CLIP image encoder, a transformer based context encoder and a transformer based decoder for text generation, is proposed to tackle the VTT benchmark problem. \n- Three strategies are further proposed to address the challenges of VTT and thus allow the TTNet to be difference sensitive, context aware and context consistent. \n- In-depth experiments demonstrate the effectiveness of the proposed model and strategies. \n",
            "strength_and_weaknesses": "Strength:\n1. Experiments and analysis are quite thorough with ablation studies clearly demonstrating the impact of each proposed component of the paper.\n2. The proposed model TTNet significantly outperforms the baselines and the proposed strategies are intuitive, reasonable and effective.\n3. Interesting qualitative results were presented. \n4. Failure cases were shown and discussed.\n5. The paper is well-written and easy to follow. \n\nWeakness:\n1. The necessity or importance of the proposed new VTT task is not clear. The concerns are:\n-  There are similar tasks such as Procedure Planning [1], Walkthrough Planning [1], and Assistant Writer [2] which is an abstract visual storytelling task. \n    * Comparison between the VTT task and these tasks should be discussed. \n    * The proposed VTT task seems to be a simplified version of the Procedure Planning task. Procedure Planning requires a model to predict the intermediate steps given only the start and end state of a *video*, whereas the VTT task sort of asks a model to predict the step given the start and end state of each *step*. Therefore, Procedure Planning seems to be more complex than VTT. In addition, the application-wise impact of the Procedure Planning task is quite obvious - an agent that is learned to perform Procedure Planning basically learned how to achieve a certain goal/task through some ordered steps. The application-wise importance of the VTT task is currently not clear to me. \n2. Whether the current way of constructing the VTT benchmark is sensible is not clear. The reasons are:\n    * According to the paper, the first frame of the first transformation (i.e., step) is used as the first state and the last frame of all transformations are used as the remaining states of the video. However, can the start frame and the end frame really display the start state and end state of a step/transformation? Based on my experience with the CrossTask and COIN datasets, very often that the first frame and the end frame of a step cannot truly show the start state and end state of a step. For example, unlike what\u2019s shown in Figure 8, the last frame of the step \u201cDig a pit with proper size\u201d could be *not* actually displaying the digged pit - the outcome of the transformation, but rather to be a frame just showing the narrator, some tool, etc. There are existing works like \"Look for the Change\" [3] that aims to find out which 3 frames in the video correspond to the time depicting the object initial state, the state-modifying action and the object end state. Using the method in [3] or designing a method to identify the frames that truly showcase the start and end state of a step, in my opinion, is more sensible.  Otherwise, I\u2019m concerned about how clean the start and end states are in the current VTT benchmark, and how this kind of noise would affect the benchmark. \n    * Does the current VTT benchmark really require sophisticated generalizability and reasoning skills from a model? How challenging is the benchmark really? Right now the train/val/test splits of the benchmark were randomly splitted. It is not really clear whether there are any types of biases in the current benchmark, and whether a model can leverage that potential bias to bypass reasoning skills (this is especially important because the authors claimed the proposed benchmark is a reasoning benchmark). Also because the dataset splits were randomly generated, it is not clear to me if the natural language generation of the transformation is necessary, or how important it is for the TTNet to have the natural language generation capability. Since the TTNEt has the natural language generation capability, this means that TTNet should be able to generate unseen verb-noun combinations, unseen order of steps for seen or unseen tasks. After checking the qualitative results, I\u2019m curious about whether the ordered list of transformation descriptions appears in training and whether it is possible that the model simply learns to memorize these descriptions. If the set of transformation descriptions is the same in training and testing, then maybe there is no need to define the VTT task to output the natural language of the transformations; using the step classes like the traditional approaches would be sufficient. \n    * One possible improvement to the current VTT benchmark might be providing annotations for a small fraction of the videos from the \u201cRelated Task\u201d of the CrossTask dataset and use these videos as testing videos (I\u2019m assuming only videos from the \u201cPrimary Task\u201d of the CrossTask dataset were used to form the VTT benchmark since only those videos were provided temporal step boundary annotations). In this way, the VTT training data will have a step transformation like \u201cpour water\u201d, and a step transformation like \u201cpour coffee into glass\u201d will be absent in training but present in testing. Such a testing set would allow one to demonstrate the power of the natural language generation complement. \n\n[1] Chang, Chien-Yi, et al. \"Procedure planning in instructional videos.\" European Conference on Computer Vision. Springer, Cham, 2020.\n\n[2] Ravi, Hareesh, et al. \"AESOP: Abstract Encoding of Stories, Objects, and Pictures.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[3] Sou\u010dek, Tom\u00e1\u0161, et al. \"Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: the paper is well-written. \n- Quality: the quality of the paper is good.\n- Novelty: the authors may need to justify the novelty as well as the application-wise impact of the proposed task.\n- Reproducibility: the proposed model and strategies in this paper should be easy to reproduce. In addition, the authors have provided their code in Supplementary Materials but I did not really check. \n",
            "summary_of_the_review": "The paper is really well-written, and the performance of the proposed methods is well-supported by experiments. However, the authors may need to further justify the VTT task and how they construct the VTT benchmark. \n\n*After rebuttal*: I appreciate the new analysis added by the authors which to some extent addressed my major concerns on whether the current VTT benchmark really requires generalizability from models and whether the VTT benchmark was reasonably constructed. Therefore I raised the score from 5 to 6. However, there are indeed several aspects of this manuscript that can be improved, e.g., technical novelty of the proposed model - as the authors admitted, the generalizability of the proposed model is not satisfactory. Overall, I think the idea/direction of the paper is worth pursuing, so I encourage the authors to continue improving the paper no matter what the final outcome is.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1398/Reviewer_tWf2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1398/Reviewer_tWf2"
        ]
    },
    {
        "id": "CXqGxmMuLR",
        "original": null,
        "number": 2,
        "cdate": 1666512731342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666512731342,
        "tmdate": 1669874838812,
        "tddate": null,
        "forum": "NqaGPQXblk",
        "replyto": "NqaGPQXblk",
        "invitation": "ICLR.cc/2023/Conference/Paper1398/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new visual reasoning task, Visual Transformation Telling (VTT), to evaluate models' understanding on action-state associations. The authors leveraged instructional videos in existing datasets to form image sequences and ask models to predict the corresponding transformation text in a captioning manner. The authors also proposed an end-to-end transformer-based model for solving this task and show compelling performance results with captioning baselines.",
            "strength_and_weaknesses": "[+] The proposed visual transformation telling task, sharing the same spirit with text-guided visual prediction or dynamics learning, studies important problems including action grounding and world dynamics learning. However, evaluating the description of transformation can be a more direct evaluation compared with image generation metrics since it could be made more structuralized in a quantitative evaluation setting. Therefore I believe, this task could be a good evaluation approach (or checklist) to test models' capabilities on fine-grained world dynamics learning and activity understanding.\n\n[+] The authors provided sufficient ablative studies showing the limitation and strengths of current models and also their proposed TTNet. This could provide new insights into the model design (e.g. masking for autoregressive decoding), incorporating state difference features as context for better transformation learning, etc.\n\n[-] One key concern of this paper also lies in the task design. With the authors evaluating the final transformation as a captioning task, I do not feel that the clarity of text for a direct evaluation has been fully utilized. More specifically, the authors argued that the ultimate goal of the transformation-telling task is to provide a logically consistent explanation for a sequence of observations (i.e. images), however, I do not see any evaluation regarding this part. Can the captioning scores reflect the quality of transformation telling generated by the models? or to what extent should the model elaborate to have a good enough transformation description (e.g., should it be a simple action verb? verb+noun phrase? phrase with adverbs, or what?). Therefore, I do feel that the current task does not suffice as a thorough evaluation benchmark for fine-grained action grounding or action-state association understanding, limiting its significance.\n\n[-] Next, I do have questions regarding the data. The authors select start and end images as states and chain all such states into an image sequence to form the input data for transformation telling given an instructional video. However, I do feel that there are concepts that can not be simply represented by a single image (e.g. reflected by temporal patterns); then did the authors check if there exists (or could be visually distinguished) transformation between each two images so that the transformation could be reasoned out? As I believe the current data sources CrowdTask and COIN contain various instructional activities, I do feel that this could potentially be a problem. Following the same though, several end states could be the result of several previous steps and not only a single step (e.g., soup being red not only because of adding tomato but also adding ketchup), the transformation might not necessarily follow a chain structure, causing ambiguous states for visual transformation telling. These all contribute to the concerns of task quality.\n\n[-] Finally, despite the good performance of the proposed TTNet, I feel that its significance is somewhat limited as there is no specific representation bound to the transformation and therefore incapable of being used as a transformation representation for forward dynamics simulation. The authors could potentially connect with text-guided generation to see if this loop from transformation to text and text to transformation could be closed.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written and clear despite some difficulties in reading (e.g. TVR in abstract without full illustration, Figure 3 could be equipped with better captions and in-figure illustrations, the current one is vague for the autoregressive part though could be understood with text). The proposed Dataset and task are novel but incremental to existing works regarding understanding action-state transitions in content and captioning in task format. The results seem reproducible given the descriptions in the paper.",
            "summary_of_the_review": "This paper proposes an interesting and potentially significant task, visual transformation telling, for vision-language/video-language learning. However, given the current status of the paper, there are several unclear or unjustified aspects of the paper that still need to be clarified by the authors or considered in future revisions (see weakness). The authors might also want to connect with other works (e.g. text-guided video generation, visual dynamics learning) to better address the significance of their model. Therefore, I'm recommending a reject for this paper and hope the authors could improve in future revisions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1398/Reviewer_NhJc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1398/Reviewer_NhJc"
        ]
    },
    {
        "id": "eim9MXH1Uw",
        "original": null,
        "number": 3,
        "cdate": 1666658700319,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658700319,
        "tmdate": 1666658700319,
        "tddate": null,
        "forum": "NqaGPQXblk",
        "replyto": "NqaGPQXblk",
        "invitation": "ICLR.cc/2023/Conference/Paper1398/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new task as well as a dataset that was collected from instructional video datasets. Apart from this, they also have a model proposed for this task. They analyze the proposed data and achieve good performance with their model.",
            "strength_and_weaknesses": "The difference in state representations or difference between visual features has been widely used in video understanding tasks. The difference in sensitive encoding in this paper is not new since lots of video captioning also has a close idea.\n\nThe masking mechanism in this paper is not to predict the specific ground truth, such as words or attributes; instead, they mask the input features and predict the representation. However, this paper lacks details, and the authors should describe more about the masking and loss function design.\n\n\nAlso, since the feature keeps changing, masking in the feature space makes the MTM harder. Instead of masking 15%, did the authors try other masking ratios?\n\nIn their setting, they miss the SPICE score, which is also widely used in the image captioning domain. Also, considering the gap between machine evaluation and human judgment, it is necessary to have humans evaluate the predictions captions. Since the visual description in this paper is much longer than image captioning, simply replying to language rules may not well reflect the real performance of predictions.\n\nCLIP model was pretrained on image-text data, while the data in this paper is video data; considering the gap between images and videos, what are the effects of the pretrained model for this setting? Did the authors consider some video-text pretrained model for this task?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper is clear, and their main contribution is the data and the proposed method. The model contribution of this paper is limited, and some data analysis is not solid also. Some implementation details are not described clearly in this paper. ",
            "summary_of_the_review": "Generating descriptions for video has been a long time, and this paper proposes a new task called Visual Transformation Telling. This task is new and interesting, and the baseline models reported in this paper cover most of the recent frameworks. However, considering the dataset is their major contribution, I still think this paper still has some room to improve.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1398/Reviewer_ipHd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1398/Reviewer_ipHd"
        ]
    }
]