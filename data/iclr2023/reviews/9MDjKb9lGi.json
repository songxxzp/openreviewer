[
    {
        "id": "bE8rCLP-kU",
        "original": null,
        "number": 1,
        "cdate": 1666224155199,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666224155199,
        "tmdate": 1666224155199,
        "tddate": null,
        "forum": "9MDjKb9lGi",
        "replyto": "9MDjKb9lGi",
        "invitation": "ICLR.cc/2023/Conference/Paper3760/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, authors show that the batch size affects the inference results of deep neural network models. \n\nIn the empirical study, authors studied bidirectional encoder representations from transformers (BERT) and generative pre-trained transformer (GPT) natural language processing (NLP) models, and the super-resolution generative adversarial network (SRGAN) image generation model in FP32 and TF32. \n\nThe study shows batch size will influence TF32 setting while not observed under the FP32 setting.",
            "strength_and_weaknesses": "Overall the topic on the impact of batch size on the performance is interesting. However, this paper lack of sufficient deep understanding on how it happens and how to prevent it. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\n\nQuality and Novelty\n\nOverall I am afraid this paper is lack of strong innovation and understanding. It only presents some experiment results, while did not offer sufficient understanding on why/how it happened.\n\nFor example, here are some questions:\nIn figure 1 (a) I am curious to see in CoLA TF32 has much lower loss compared to TF32 on all batch sizes. Is it expected?\nIn figure 2, why the patterns for 1 vs 2, 2 vs 8, and 4 vs 8 are so similar, while the others are so different? \nContinue my last question, in the 1 vs 2, 2 vs 8, and 4 vs 8  there are clear systematic on error vs max length, is there any understanding why it happened?\nIn table 1 it shows the number of non-identical pixels. I am curious how such much those pixels different? It would be helpful to also add example figures for comparison. \n\nReproducibility:\nMost of the experiment settings are clear, though I was not able to find the code so I am not confident how easy to reproduce the experiment.\nFor example, authors mentioned \"A batch size of 32 was used for training\", so which GPU specifications is used for model training? I assume in evaluation the model is the same for both FP32 and TF32?\n",
            "summary_of_the_review": "Overall I am afraid the depth in this paper is not sufficient for a long paper in ICLR. It may better fit toas a short workshop paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3760/Reviewer_YAmr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3760/Reviewer_YAmr"
        ]
    },
    {
        "id": "ZXhLUgajhgs",
        "original": null,
        "number": 2,
        "cdate": 1666645578669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645578669,
        "tmdate": 1666645578669,
        "tddate": null,
        "forum": "9MDjKb9lGi",
        "replyto": "9MDjKb9lGi",
        "invitation": "ICLR.cc/2023/Conference/Paper3760/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper sheds light on discrepancies in results when using tf32: depending on the batch size used for inference (and training?), the same instance will give different results (as measured as logits, pixel values or loss depending on the setup).",
            "strength_and_weaknesses": "Strengths:\n- It seems like an interesting problem and I don\u2019t think people are aware enough of this behavior.\n\nWeaknesses:\n- The paper is mostly about shedding light on the discrepancy during inference (could not tell whether the training case was studied). For this study to be interesting, I would have expected some deeper analyses of why such discrepancy is happening, if inference batch sizes need to match training batch sizes, if not to what discrepancy is acceptable, formulate recommendations, etc.\n- The paper is very hard to follow, and I had a hard time putting things together. For instance, I am still unclear on most of the experiment steups.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was extremely hard to follow, between grammatical errors, sentences with no verb and acronyms not properly introduced. A few samples that I did not understand:\n- \u201cBecause of the cuBLAS\u2019 heuristics, a vast, deep neural network model with GPUs may produce different test results owing to the batch sizes in both the training and inference stages.\u201d\n- \u201cThe GPT generated sentences depending on batch size, and we show the logit\u2019s mean square error by increasing the token length.\u201d\n- \u201cThe SRGAN model produced different images from batch to batch\u201d\n- \u201cIn addition, there is a problem caused by the difference between the training and inference stages,\u201d\n- \u201cBF16 Kalamkar et al. (2019), which increases the dynamic range of single-precision FP16 and reduces significant figures, and TF32, which enables the dynamic range of 32 bits using the 19-bit representation supported after NVIDIA\u2019s Ampere architecture, etc\u201d\n- Figure 2: \u201cnumber of generated max token length\u201d\n- Figure 1: what is the sequence length used?\n- What does \u201c1 and 2 batch\u201d mean? For instance Figure 2.\n- \u2026\n",
            "summary_of_the_review": "This submission is extremely incomplete and does not meet the standard for a publication venue such as ICLR.\nIt seems like the paper is describing the very first step of a project: verifying and exposing the problem. Beyond that, the contributions are non-existent.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3760/Reviewer_ReKx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3760/Reviewer_ReKx"
        ]
    },
    {
        "id": "Pu-S87goqrM",
        "original": null,
        "number": 3,
        "cdate": 1666711461942,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666711461942,
        "tmdate": 1666711544781,
        "tddate": null,
        "forum": "9MDjKb9lGi",
        "replyto": "9MDjKb9lGi",
        "invitation": "ICLR.cc/2023/Conference/Paper3760/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper experimentally shows that errors occur as the batch sizes for training and inference vary in\na deep learning model by caused as the GEMM operation on the GPU.",
            "strength_and_weaknesses": "An interesting finding! It experimentally finds that the different batch sizes during training and inference will affect the model performance due to the matrix operation on GPU. Therefor the authors suggest to carefully choose the batch size.\n\nHowever,\n1. The paper is not well-written in presentation. The format of the reference is not correct. Table 1 lacks the annotation of the lines and columns. In Figure 2, it is also not clear what the '1 and 2 batch' means. The expression of \"batch/batches\" is also confusing. Do the authors mean batch size? 4 batch and 4 batch size refer to different meanings.\n2. The work is not ready in its current form to be published at the ICLR conference with only very limited experimental results without more deep analysis.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is not ready to be accepted in its current form.",
            "summary_of_the_review": "The work has some interesting findings through experiments. But it is not ready to be published at its current form without any deeper analysis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3760/Reviewer_7wfT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3760/Reviewer_7wfT"
        ]
    }
]