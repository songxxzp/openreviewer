[
    {
        "id": "MJY-Qh7Tb2",
        "original": null,
        "number": 1,
        "cdate": 1665989575807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665989575807,
        "tmdate": 1670454559647,
        "tddate": null,
        "forum": "MHgYMtHpKsC",
        "replyto": "MHgYMtHpKsC",
        "invitation": "ICLR.cc/2023/Conference/Paper5846/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new method for personalized federated learning. The key idea is to represent neural network weights as a linear combination of multiple bases, where the bases are shared across clients and only coefficients are personalized. Doing so makes it easier to personalize the model for a new client who did not participate in the training, by just training the coefficient parts. A coordinate decent approach is introduce to train the both of personalized coefficients and shared bases in an alternate fashion. Experimental results on a new benchmark dataset shows the effectiveness of the proposed method over a variety of existing methods.",
            "strength_and_weaknesses": "\n## Strong points\n- Overall, the paper is well written, structured, and easy to read.\n- The proposed work is well motivated. I agree that learning to personalize a learned model for a new client who was absent in the training is an important and interesting topic of personalized FL.\n- The proposed method is designed reasonably and technically solid. While learning to linearly combine multiple kernels is a known idea (Yang et al., 2019; Chen et al., 2020; Zhang et al., 2021c), adopting it to personalized FL is new as far as I have checked.\n- A new benchmark dataset (more precisely, data splitting protocol) is introduced for personalized FL.\n\n## Weak points\n- While I believe that the proposed method seems to have some technical merits, the current experimental results only demonstrate that \"the proposed method worked better than existing methods on the proposed datasets\", which is a bit less convincing to me. Overall, it is not very clear what motivates to develop a new benchmark dataset, though I appreciate the introduction of new dataset itself. In the paper, it was argued in the introduction that the existing PFL evaluations are (1) misleading due to the mismatches between training/test distributions and (2) less comprehensive because the non-iidness arises only in either labels or input domains, not both. While I agree with (2), I would think that train/test mismatches in (1) could happen in practice as well studied in domain adaptation and generalization. The paper could have been stronger if the proposed method is compared with baselines also on other datasets (i.e., data splitting scheme) used in prior works. \n- It remains not obvious if the proposed method really outperforms baselines from the current results. In Table 2, the performance gains from the most strong baseline (Per-FedAvg + FT) are less than 3 percent points in most cases, on average. How about the variance of test accuracy across clients and multiple runs with different random seeds? Such additional information is necessary to fully demonstrate that the performance improvements by the proposed method are statistically significant. ",
            "clarity,_quality,_novelty_and_reproducibility": "- **Clarity**: The paper as well as its contribution is presented clearly. \n- **Quality**: The quality could be improved a bit more. Specifically, I have some major concerns regarding the experimental evaluation (commented above).\n- **Novelty**: The paper has some technical novelty. While learning to linearly combine multiple kernels is a known technique, adopting it for personalized FL seems new.\n- **Reproducibility**: The current submission has limited reproducibility. The only pseudo code is provided for the proposed method. It is not trivial to fully reproduce the results as doing so requires full replication of data splitting and training procedures with certain random seeds. ",
            "summary_of_the_review": "Overall the paper is well written and presenting some technical contributions for personalized FL. Nevertheless, I am currently neutral about this submission due to its limited experimental evaluation. If the proposed method is confirmed to (1) statistically outperform baseline methods (with standard deviations or confidence intervals regarding multiple clients and multiple random seeds) and (2) work effectively also for other datasets/data splitting scheme used in prior work (e.g., those used in pFedHN), I would be willing to upgrade my score. Code submission for a complete reproduction of this work is also highly welcome.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5846/Reviewer_5bfz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5846/Reviewer_5bfz"
        ]
    },
    {
        "id": "KjuyDsPGHwW",
        "original": null,
        "number": 2,
        "cdate": 1666574293135,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574293135,
        "tmdate": 1666574293135,
        "tddate": null,
        "forum": "MHgYMtHpKsC",
        "replyto": "MHgYMtHpKsC",
        "invitation": "ICLR.cc/2023/Conference/Paper5846/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "First, this paper propose a personalized federated learning (PFL) method, FEDBASIS, based on coordinate descent trying to learn a set of bases for the overall system and personalized coefficients in federated learning setting. Second, a benchmark for PFL, PFLBED, is proposed discussing the procedures to build non-i.i.d. of joint distributions and to maintain reliable evaluation simultaneously.",
            "strength_and_weaknesses": "Strengths:\n[+] An architecture and algorithm, FedBasis, is proposed to decouple \u2018the required total number of personalized parameters from the number of clients\u2019.\n[+] Novel benchmark for PFL: the motivation is worth considering.\n[+] Remarkable experiments\u2019 results.\n\nWeaknesses:\n[-] It seems like that the straightforward ideas of FedBasis are not novel and fail to decouple the personalized parameters from the number of clients in practice due to the requirement of the set of basis and calculation of the gradient based on them during training (of the set of basis even during inference, ). Can you provide more explanation about decoupling rather than duplicating parameters with more copies?\n[-] The procedures of benchmark are not comparing to existing benchmark (e.g. LEAF) experimentally on discrepancy as you claim in Section 5. Can we employ them on any classic ML dataset?\n[-] What about test on LEAF benchmark? And the results are relevant to benchmarks or not?\n[-] FedBasis is claimed to be not sensitive to learning rate and training epoch, but Per-FedAvg is. How about to well-tune two versions of Per-FedAvg, FO and HF.\n[-] The poor experiments about robustness of personalization lead to low persuasiveness.Why not consider comparing methods to reduce structural risk and to use regularization to deal with it, such as pFedMe, since the clients may not have enough data for validation in practice?\n[-] The accuracy, loss, etc. of these training processes are not shown but only tables of acc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This article is very readable and has some novel ideas while being very easy to reproduce.",
            "summary_of_the_review": "The ideas of FedBasis are not novel but the motivations of benchmarks are of interest. Some experiments are not solid enough to support their claims.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5846/Reviewer_2JAA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5846/Reviewer_2JAA"
        ]
    },
    {
        "id": "4wYqnlugnM",
        "original": null,
        "number": 3,
        "cdate": 1666743799121,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666743799121,
        "tmdate": 1666743799121,
        "tddate": null,
        "forum": "MHgYMtHpKsC",
        "replyto": "MHgYMtHpKsC",
        "invitation": "ICLR.cc/2023/Conference/Paper5846/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work considers a new personalized federated learning scenario where this federated training needs to learn some intrinsic model property and generalize this knowledge into new client. This considered scenario is interesting. To overcome it, this work attempts to construct the shareable model bases which each client utilizes to learn combination coefficient. In addition, this work evaluates the effectiveness of the proposed method on datasets with domain-shift.",
            "strength_and_weaknesses": "Strength: \n\n1. The considered new PFL scenario is interesting and practical.\n\nWeaknesses: \n\n1. Some details of the proposed method are not clear.\n\n2. The experimental settings are not convincing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing part is fine, and the proposed method is a litter new in PFL area.",
            "summary_of_the_review": "This work considers a new personalized federated learning scenario where this federated training needs to learn some intrinsic model property and generalize this knowledge into new client. This considered scenario is interesting. To overcome it, this work attempts to construct the shareable model bases which each client utilizes to learn combination coefficient. In addition, this work evaluates the effectiveness of the proposed method on datasets with domain-shift.\n\nHowever, I also have several concerns on this work:\n\n1. How to learn and select the representative model bases is not clear? \n\n2. When the new clients come from new domain or other new distributions, I wonder if the proposed method also can solve it. In addition, the idea of generalizing model into new clients (domains) is very similar with domain generalization (DG). Thus, it would be better to discuss the association and difference with some DG works.\n\n3. This work only conducts experimental comparisons with some baselines on new data setup, which will be not convincing. Thus, it would be better to add some experiments as the traditional PFL works (e.g. FedRep) on Cifar-10/100 or Fe-MNIST.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5846/Reviewer_ES1X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5846/Reviewer_ES1X"
        ]
    }
]