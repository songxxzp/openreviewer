[
    {
        "id": "5_m3qU6Dx8a",
        "original": null,
        "number": 1,
        "cdate": 1666642576083,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642576083,
        "tmdate": 1670168911537,
        "tddate": null,
        "forum": "Tvms8xrZHyR",
        "replyto": "Tvms8xrZHyR",
        "invitation": "ICLR.cc/2023/Conference/Paper3124/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies a power series of the neural tangent kernel (NTK) for an infinitely wide fully-connected neural network when inputs are on the unit sphere. Using the power series, the authors analyze the spectrum of the NTK. In particular, they focus on the effective rank of the NTK matrix, defined as the ratio of the trace to the leading eigenvalue, and compare it to that of the data Gram matrix. For shallow ReLU network, the effective rank of the NTK is a constant multiple of that of the data Gram matrix. Finally, they provide the lower part of the eigenvalues using the spherical harmonic analysis.",
            "strength_and_weaknesses": "Strength:\n- This paper provides a power series of fully-connected NTK (Theorem 3.1) when inputs are normalized. This can be obtained by Hermite expansion of the activation and polynomial composition. Hence, once the coefficients of the series of NTK is computed (which is data-independent), then the NTK is computed by applying some polynomial to each entry of the data Gram matrix. \n\n- The authors make use of the power series of the NTK to analyze its effective rank. They particularly compute the ratio of effective ranks between the NTK and the data Gram matrix, and show that this can be small for common activations (e.g., ReLU).\n\nWeakness:\n- The main focus of the spectrum analysis part is the effective rank. The authors mention that it can quantify some counts of eigenvalues of the NTK. But, it needs more comprehensive motivation why the effective rank analysis is meaningful. And what is expected when the effective rank of the NTK is close to that of the data Gram matrix?\n\n  One question is that Figure 2 looks that the spectrum of the NTK with ``tanh\u2019\u2019 activation matches the data Gram matrix more than that of the ReLU. However, in practice, it is well-known that the ReLU activation performs better than the tanh in terms of training and test (for both CIFAR-10 and MNIST). It is uncertain that such spectrum is meaningful to analyze the NTK. \n\n- The paper should add/edit more related works. The relationship to the Hermite expansion was originally studied by [1], which actually characterizes the NTK for shallow networks. There is also rich literature studying the Hermite polynomials for the NTK, e.g., [2,3,4].\n\n- The paper entirely assumes that inputs are normalized, which can be fairly unrealistic because practical datasets are not. Under this assumption, I think the contributions are quite incremental, which can be derived from the simple composition of power series (coefficients are obtained from the Hermite expansion). For homogeneous activation (e.g., ReLU), the results can be easily extended to the entire R^d (also studied in [3]). But, generally, it seems that the results can be more complicated and different.\n\n- Moreover, the results of this paper are limited only to the ``fully-connected\u2019\u2019 neural network. Practical usage of the NTK is also related to the convolutional neural networks or more results on modern architectures would improve the results. \n\n[1] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In NeurIPS, 2016.\n\n[2] James B Simon, Sajant Anand, and Michael R DeWeese. Reverse Engineering the Neural Tangent Kernel. In ICML, 2022.\n\n[3] Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao and Amin Karbasi.Fast Neural Kernel Embeddings for General Activations. In NeurIPS, 2022.\n\n[4] Lokhande, Vishnu Suresh, Songwong Tasneeyapant, Abhay Venkatesh, Sathya N. Ravi, and Vikas Singh. Generating accurate pseudo-labels in semi-supervised learning and avoiding overconfident predictions via Hermite polynomial activations. In CVPR, 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written but some statements of this paper need to be written more formally. For example, the authors argue that the effective rank of the shallow ReLU is roughly 2.5. But, it is unclear how the network depth is small enough. Similar issue is in Observation 4.4.\n\nNovelty is incremental as the main analysis tool (Hermite expansion <-> NTK) was already studied in many previous works.\n",
            "summary_of_the_review": "The results of this paper are incremental and the motivation of the effective rank ratio of the NTK is unclear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3124/Reviewer_VHSv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3124/Reviewer_VHSv"
        ]
    },
    {
        "id": "HY29zqaxBW",
        "original": null,
        "number": 2,
        "cdate": 1666653217370,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653217370,
        "tmdate": 1669123161737,
        "tddate": null,
        "forum": "Tvms8xrZHyR",
        "replyto": "Tvms8xrZHyR",
        "invitation": "ICLR.cc/2023/Conference/Paper3124/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the infinite-width NTK in the setting of general activation functions and datasets. It is shown that, under the standard Gaussian initialization, the NTK can be expressed as a power series in the data Gram matrix elements. The paper then discusses the coefficients of this series and its convergence for a few common activations. After that the authors discuss spectral properties of kernels that can be represented by such series. In particular, the paper give bounds on the efficient rank of the NTK and the convergence rates of its eigenvalues in several scenarios.",
            "strength_and_weaknesses": "Strengths\n-------------\nI think that the paper develops a new and interesting general analytic approach to the study of NTK. The idea of a power expansion of the NTK appearing in main theorem 3.1 seems new and important to me. Importantly, this approach is applicable to a very large class of activations and generic data sets. As the authors show, the coefficients of this power expansion can be explicitly computed to some extent, and can be related to the spectral properties of the NTK, in particular the eigenvalue convergence rate. The obtained spectral results agree with some earlier results and provide a different perspective on them. All this opens up potential new ways for NTK analysis. \n\nWeaknesses\n-----------------\n1. The experimental part of the paper seems very weak to me. There are only a few experiments with real data in section 4.1, and I don't quite see what these experiments are supposed to demonstrate. The authors observe that there is a long tail of small eigenvalues, but such long tails are common, and it is not clear how this observation confirms the developed theoretical picture. On the other hand, there are some obvious interesting features in Figure 2 that the authors don't comment on, despite there apparent relevance to the theoretical results. In particular, the curves for the activation Tanh almost match the curves for Data - is it because Tanh has a very fast decaying power expansion? Would this also be so if Tanh was replaced by the trivial linear activation? Why is there a constant difference between the Tanh and Data curves in the case of Depth-5 MNIST? Is the slope of the ReLU curve smaller than the slope of Data/Tanh because the coefficient decay of ReLU dominates the data eigenvalue decay? I think that the experimental part and its discussion could have been much more prominent. \n\n2. I didn't understand some statements. In particular, it's not clear to me under which conditions Observation 4.2 holds and why. The notation \\Omega(1) and O(1) here is confusing (what is the respective parameter?); I would suggest to write this in a more explicit form. I also don't understand the statement of Theorem 4.8. Is r(n) here an arbitrary function?  Is K_n some n x n submatrix of K and \\lambda_n(K_n) its lowest eigenvalue?       ",
            "clarity,_quality,_novelty_and_reproducibility": "As I have already written, I generally like the theoretical results of the paper, but I didn't check their proofs, which seem rather involved. Unfortunately the main body of the paper does not include any sketches of these proofs.  \n\nThe results are mostly theoretical, but the authors provide the code for their experiments. ",
            "summary_of_the_review": "An interesting work developing a new and potentially important analytic method. However, the experimental part is weak, and there are some issues with the exposition.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3124/Reviewer_rRGi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3124/Reviewer_rRGi"
        ]
    },
    {
        "id": "0wCAYkscYg6",
        "original": null,
        "number": 3,
        "cdate": 1666700301799,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700301799,
        "tmdate": 1666700301799,
        "tddate": null,
        "forum": "Tvms8xrZHyR",
        "replyto": "Tvms8xrZHyR",
        "invitation": "ICLR.cc/2023/Conference/Paper3124/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents theoretical results on the eigenvalue spectrum of the neural tangent kernel (NTK) based on Hermite polynomial expansions. The results enable expressing the NTK matrices for neural networks of arbitrary depth via power series expansions based on the data points matrix. Bounds on the eigenvalues of the NTK for popular activation functions are also provided.",
            "strength_and_weaknesses": "### Strengths\n* The analysis is mathematically rigorous.\n* A discussion on their practical interpretation follows each of the main results.\n* The approach taken in the analysis is relatively simpler than pre-existing attempts on deriving general results for the NTK spectrum.\n* The power series formulation (Thr. 3.1) and in special the eigenvalue results (Lem. 3.2, Cor. 4.7, Thr. 4.8) have the potential to impact many applications of NTK-based convergence analysis of deep learning frameworks.\n* Empirical results practically validating theoretical insights are also presented (Fig. 1 & 2)\n\n### Weaknesses\n* In Assumption 2, \"The hyperparameters of the network satisfy $\\gamma_w^2 + \\gamma_b^2 = 1$ ... and $\\sigma_b^2 = 1 - \\sigma_w^2 \\mathbb{E}_{Z\\sim\\mathcal{N}(0,1)}[\\phi(Z)]$\" seems somewhat restrictive to me, and this part of the assumption hasn't been discussed in the paragraph following it.\n* Some notation definitions are missing in the main text, e.g.: $r(n)$, $h_k$",
            "clarity,_quality,_novelty_and_reproducibility": "The text is mostly well written, and the results seem novel compared to previous literature, though I haven't checked the proofs in the appendix. Code is provided to reproduce the empirical results",
            "summary_of_the_review": "The paper presents an interesting theoretical analysis deriving bounds for the NTK spectrum decay and relating it to the data eigenspectrum. The results practical interpretation is provided and empirical results to demonstrate theoretical insights in practice are also presented. The approach is novel compared to previous work, and the results are more broadly applicable.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3124/Reviewer_jD66"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3124/Reviewer_jD66"
        ]
    }
]