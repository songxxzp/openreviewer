[
    {
        "id": "2VDi8nskeJA",
        "original": null,
        "number": 1,
        "cdate": 1666434443577,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666434443577,
        "tmdate": 1666434443577,
        "tddate": null,
        "forum": "HZf7UbpWHuA",
        "replyto": "HZf7UbpWHuA",
        "invitation": "ICLR.cc/2023/Conference/Paper5702/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to stabalize GAN training by training Discriminator with adapitvely noised images. The method shows improved performance over GAN baselines across low-resolution and high-resolution image generation benchmarks as well as low and high data regime.",
            "strength_and_weaknesses": "Strength\n1. The paper is clearly written with theoretical analysis to support proposed method. \n2. Experiments are well designed and demonstrated clear improvments over baseline methods, ranging from low-reoslution to high-resolution and from low-diversity to high-diversity\n\nWeakness\nThe method improvment is somewhat marginal: introducing noise to discriminator training has been explored before, introducing different levels of noises as defined in diffusion process seems like a minor improvement. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: High.\n\nQuality: High. Method and experiments are well written with high quality. \n\nNovelty: Fair.\n\nReproducibility: High.",
            "summary_of_the_review": "The paper is well structured with theoretical analysis and convincing experiments. Introducing adaptive noise as formulated in diffusion process to GAN Discriminator training is somewhat a natural extension of previous works but could be a go-to solution to stabilize GAN  training in the future.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5702/Reviewer_xwap"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5702/Reviewer_xwap"
        ]
    },
    {
        "id": "7SRsTbNUL7",
        "original": null,
        "number": 2,
        "cdate": 1666460745018,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666460745018,
        "tmdate": 1666460745018,
        "tddate": null,
        "forum": "HZf7UbpWHuA",
        "replyto": "HZf7UbpWHuA",
        "invitation": "ICLR.cc/2023/Conference/Paper5702/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an improved GAN training method that involves a diffusion chain that generates Gaussian mixture distributed instance noise. The diffusion level is dependent on timestep, and the discrimination task is made increasingly difficult for the discriminator over the course of training. The approach improves generation quality in terms of FID measurements and recall capacity, while allegedly retaining stability.",
            "strength_and_weaknesses": "Strengths:\n- The proposed approach makes intuitive sense and it is explained with sufficient clarity.\n- Generation performance is superior to relevant baselines.\n- Computational costs are on a par with relevant baselines (StyleGAN-ADA).\n- Some theoretical support is provided to ensure the injection of extra noise does not hamper the basic GAN learning properties (though it is difficult to assess the ultimate significance of the provided theorems)\n\nWeaknesses:\n- Claims of stable training were made, but I did not easily find where this was justified. Perhaps the authors could point me to where it is shown that the training is especially stable, in comparison to baselines?\n- It was not clear which hyper-parameters need to be addressed while switching the datasets. Again, a simple summary clarification from the authors might help here.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is in general easy to follow.\n- As far as I am aware, the approach is original; however, given the ubiquity of diffusion-based approaches in recent years, I might be missing some recent similar efforts in this area.\n- Source code is provided.",
            "summary_of_the_review": "The paper contributes a non-trivial new approach to GAN training. It could prove a useful addition to the GAN models and provides one possible bridge between diffusion and GAN methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5702/Reviewer_dVvW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5702/Reviewer_dVvW"
        ]
    },
    {
        "id": "Vim7KGI9WNi",
        "original": null,
        "number": 3,
        "cdate": 1666658918505,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658918505,
        "tmdate": 1666658918505,
        "tddate": null,
        "forum": "HZf7UbpWHuA",
        "replyto": "HZf7UbpWHuA",
        "invitation": "ICLR.cc/2023/Conference/Paper5702/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces the idea of diffusion-GAN by incorporating diffusion process with GANs. In the framework, an adaptive diffusion process for sampling the noise and a diffusion timestep-dependent discriminator are added to a traditional GAN generator. The main contributions are:\n1. Propose a diffusion-GAN framework which combines the technique of diffussion models and GAN;\n2. Theoretically and empirically show the effectiveness of the method and the ability of stabling GAN training;\n3. According to the results reported, the quality of the generative images are better than the state-of-the-art GANs.",
            "strength_and_weaknesses": "The main strength:\n1. Proposed a new architecture which combines two state-of-the-art generative models and theorectially showed the ability of acheiving a better GAN training and also sampling;\n2. The experiments are well-designed and the results are significant and interesting;\n3. The paper is well-written and easy to follow\n\nConcerns/questions:\n1. While the FID and Recall are reported, is it possible to also include other metrics(IS, LPIPS, ..);\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is novel by combining two model training process and is of good clarity. It should also be easy to reproduce.",
            "summary_of_the_review": "This paper studies a combination of two state-of-the-art generative models and theoretically shows its ability to stable the GAN training. According to the results, the images are of better quality than current GANs. I recommend accepting it.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5702/Reviewer_YnnY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5702/Reviewer_YnnY"
        ]
    },
    {
        "id": "IGhWT-Waf9",
        "original": null,
        "number": 4,
        "cdate": 1666700237472,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700237472,
        "tmdate": 1666700237472,
        "tddate": null,
        "forum": "HZf7UbpWHuA",
        "replyto": "HZf7UbpWHuA",
        "invitation": "ICLR.cc/2023/Conference/Paper5702/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a diffusion-based approach to stabilise the learning process of GANs in a instance noise fashion.",
            "strength_and_weaknesses": "Strength:\n- The abstract and introduction give a nice overview and motivation for the problem.\n- The method is supported by a theoretical analysis.\n- The results are good and evaluation is reasonable.\n- The objective is still differentiable wrt the generator using the classical rewritting of the perturbed samples.\n\nWeaknesses:\n- 25-gaussians example (Fig. 5): the purpose of the figure is to exhibit the fact that the propose method is not prone to mode collapse but I am curious to know if the proposed method has shown some mesurable improvement wrt SotA methods on this \"toy example\"? \n- The authors did an ablation study on the adaptive diffusion process. But, reading Section 3.3, it feels like the adaptative diffusion strategy is key to the method whereas the ablation study does not clearly support it.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the writting is good, the proposed method looks sound and in line with previously proposed methods.",
            "summary_of_the_review": "Overall I think this is a good paper tackling a well motivated task.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5702/Reviewer_jaon"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5702/Reviewer_jaon"
        ]
    }
]