[
    {
        "id": "H7iGChTR8kl",
        "original": null,
        "number": 1,
        "cdate": 1665796808195,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665796808195,
        "tmdate": 1670528185862,
        "tddate": null,
        "forum": "_9k5kTgyHT",
        "replyto": "_9k5kTgyHT",
        "invitation": "ICLR.cc/2023/Conference/Paper1555/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "1. This paper proved that the training of deep threshold networks with weight decay can be formulated as a convex optimization problem. The size of the convex optimization problem depends on the total number of hyperplane arrangements, which can be exponentially large in the number of samples in the worst case. \n2. For two-layer threshold networks, this paper proved that the associated convex optimization problem can be solved in polynomial time when (1) the data matrix has constant rank or (2) the hyperplane arrangements of the data matrix is complete. \n3. For deep threshold networks, this paper proved that the associated convex optimization problem can be solved in polynomial time when (1) the data matrix rank and the network widths are all constant or (2) one hidden layer has a width of at least the number of samples.\n4. In the experiments, this paper showed that training the threshold networks using the proposed convex approaches archived better performance than the non-convex counterparts trained by back-propagation. ",
            "strength_and_weaknesses": "Strengths:\nThis paper formulated the training of weight decay regularized threshold networks as convex optimization problems. It then proposed convex approaches to optimize such neural networks, which seems appealing especially when the back-propagation fails because of the zero gradient issues. It also provides a relatively complete picture of the computational complexity of such convex approaches for different-depth networks under different data assumptions. \n\nWeaknesses:\n1. This paper only proved the polynomial-time complexity of the convex optimization problem in some special cases. In many other cases, solving this convex optimization problem can still take exponential time, which then makes the algorithm impractical. For example, when none of the layers has a width of at least $n$, the running time is exponential in the product of layer widths. This can be extremely slow since the width of each layer can easily be larger than $100$ in modern neural networks.\n2. The experiments are also restricted to two or three-layer neural networks, which further reinforces my suspicion that the proposed convex approaches may not be practical for modern deep neural networks. \n\nSome minor questions:\n1. In the two-layer case when the arrangements are complete, why is the reconstruction of parameters only taking O(n)? Shouldn't it also depend on $m$ and $d$ because $W^{(1)}$ has dimension $d\\times m$?\n2. What's the reconstruction time when the arrangements are incomplete? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: \nThis paper is well-written. \n\nQuality:\nThe theory looks correct. Empirical experiments were also provided to verify the proposed convex approaches.\n\nNovelty:\nAs far as I know, the convex approaches to solving the training of threshold networks are novel. \n\nReproducibility:\nBoth theoretical and experimental results seem reproducible. \n",
            "summary_of_the_review": "This paper proposed convex approaches to optimize threshold neural networks, which is particularly appealing due to the zero gradient issues in back-propagation for such networks. However, my major concern is that such convex approach can be exponentially slow in general. The experiments are also restricted to very simple networks, which further reinforces my suspicion that the proposed convex approaches may not be practical for modern deep neural networks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1555/Reviewer_n5sD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1555/Reviewer_n5sD"
        ]
    },
    {
        "id": "ewiwAFk2wB",
        "original": null,
        "number": 2,
        "cdate": 1666556116248,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556116248,
        "tmdate": 1668733508701,
        "tddate": null,
        "forum": "_9k5kTgyHT",
        "replyto": "_9k5kTgyHT",
        "invitation": "ICLR.cc/2023/Conference/Paper1555/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Paper develops a convex program for **memorizing** data with a two-layer neural network that extends to higher layers.  **Memorization** in neural nets is a theoretical topic that studies required conditions on a number of neurons to perfectly fit a finite number of samples. What makes this result apart from existing results in the literature is the use of the threshold activation function. The authors motivate the choice of threshold activation by (i) the compression of outputs and (ii) the biological origins of threshold activations. Despite compression and biological simulations, a network with threshold activation is difficult to optimize. There are rare algorithms to optimize networks with such activations. This paper proposes a convex program for memorization with the following guarantees: \n- #neurons>#samples: the algorithm finds the weights in poly-time \n- #neurons<#samples: the algorithm suffers from the complexity $O(\\text{samples}^{\\text{input-rank}})$",
            "strength_and_weaknesses": "**Strengths**\n- The main strength of the paper is motivating learning threshold neural nets very well. Learning such networks is an important topic. In my opinion, the neural nets community will welcome novel algorithms and optimization techniques for neural nets with threshold activations.\n- Extending memorization to deep neural nets is an interesting contribution of this paper. \n- Experiments on generative and real data are a plus for this theoretical work. \n\n**Weakness** \n- *Memorization.* The paper stresses the proposed algorithm can lead to a practical algorithm. But I am not sure about this. The memorization setting (#samples<#neurons) is not standard in machine learning. \n- *Presentation.* The intro and abstract may lead to a misunderstanding that threshold activation leads to convex optimization. While the memorization setting (#neurons>#samples) enables us to cast training to a convex program. I recommend explicitly declaring that the overparameterization #neurons>#samples are the key assumption enabling us to learn with a convex program. \n- *Literature review* The literature on memorization is missing in this paper. The paper neglects closely related references, including [1-5] and many more references on the same topic. For example, it is very important to drive the connection between general position assumption on inputs [2] and the completeness of the arrangements in this paper.  \n- *Experiments.* I could not find the number of neurons, batch size, stepsize, number of epochs, and more details on experimental settings in Table 2. \n\n**References**\n\n\n1.  Rosset, Saharon, et al. \"L1 regularization in infinite dimensional feature spaces.\" International Conference on Computational Learning Theory. Springer, Berlin, Heidelberg, 2007.\n2. Bubeck, Sebastien, et al. \"Network size and weights size for memorization with two-layers neural networks.\" arXiv preprint arXiv:2006.02855 (2020).\n3. Zhang, Chiyuan, et al. \"Understanding deep learning (still) requires rethinking generalization.\" Communications of the ACM 64.3 (2021): 107-115.\n4. Pilanci, Mert, and Tolga Ergen. \"Neural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks.\" International Conference on Machine Learning. PMLR, 2020.\n5. de Dios, Jaume, and Joan Bruna. \"On sparsity in overparametrised shallow relu networks.\" arXiv preprint arXiv:2006.10225 (2020).",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n- The main problem with threshold activation is that it is not continuous. The argument on gradient zero and non-zero at zero is misleading. \n-  Please make the contributions more precise to avoid over-claiming. For example, the first contribution should include condition #neurons>#samples in the statement.  \n- When I was reading Th 2.2., I wondered how to find $d_1, \\dots, d_p$ in polytime. I recommend to mention that these vectors can not be found in polytime. Indeed finding one of them is equal to optimizing threshold neural nets. \n- Comment on when the completeness of arrangements are met before presenting Thm 2.3. \n\n\n**Novelty** \nSee weaknesses *Literature review*\n\n**Reproducibility** \nSee weaknesses *experiments*",
            "summary_of_the_review": "The paper presents interesting theoretical results. Yet, I believe that experimental results may lead to over-claiming and need further clarifications, and also it misses a large body of related literature. \n\n**Post Rebuttal**\nI thank the authors for their detailed response. I updated my score after reading the response. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1555/Reviewer_VvcU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1555/Reviewer_VvcU"
        ]
    },
    {
        "id": "CBYIRNi1Ht",
        "original": null,
        "number": 3,
        "cdate": 1666583829850,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583829850,
        "tmdate": 1666583829850,
        "tddate": null,
        "forum": "_9k5kTgyHT",
        "replyto": "_9k5kTgyHT",
        "invitation": "ICLR.cc/2023/Conference/Paper1555/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper formulates the problem of training neural networks with threshold activation functions to a convex optimization problem. As a result, global minima can be obtained with standard convex optimizers. ",
            "strength_and_weaknesses": "Strengths:\n1. The paper is well-written and easy to follow. The notations are clear.\n2. Equivalenting neural network training to convex optimization under mild conditions is theoretically interesting.\n3. The theoretical results seem rigorous, though I didn't check the proofs thoroughly.\n4. Experiment were conducted on various datasets and the results in terms of testing accuracy and training time are encouraging.\n5. The code is attached for reproducibility.\n\nWeaknesses:\n1. It seems to me that the weight decay terms in the objective function (3) are needed to derive the equivalent formulation (4). This is somewhat artificial as deep learning practitioners nowadays barely employ weight decay in training. Can the authors shed some lights on the equivalent derivation without weight decay?\n2. The overall approach does not seem scale well with the depth of a neural network, unless some strong conditions are imposed. As in Theorem 3.2, the convex formulation essentially flattens the deep nested structure of the network. Therefore, it can result in exponentially many parameters to optimize, as indicated in Table 1. Please note that the point of doing neural networks is employing the deep nested structure, so as not to have exponentially many parameters, in approximating complicated functions.",
            "clarity,_quality,_novelty_and_reproducibility": "All good.",
            "summary_of_the_review": "Overall, I find this paper is solid and contains interesting theoretical results. But I am concerned with its extensibility and therefore the impact, as written in Weaknesses.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1555/Reviewer_tBqd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1555/Reviewer_tBqd"
        ]
    }
]