[
    {
        "id": "NtElzHYonlV",
        "original": null,
        "number": 1,
        "cdate": 1665858111299,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665858111299,
        "tmdate": 1665907522916,
        "tddate": null,
        "forum": "aCCRmE3Pglv",
        "replyto": "aCCRmE3Pglv",
        "invitation": "ICLR.cc/2023/Conference/Paper3459/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper aims at using \"Energy-based Predictive Representation (EPR)\" to create a \"unified approach to practical reinforcement learning algorithm design for both the MDP and POMDP settings, which enables learning, exploration, and planning to be handled in a coherent way\".",
            "strength_and_weaknesses": "Strengths:\n- The claims are very ambitious\n\nWeaknesses:\n- The paper does not provide a clear formalization and description of the algorithm",
            "clarity,_quality,_novelty_and_reproducibility": "Equation 6 to 18 are not explained clearly. Among others, here are some elements that are not clear:\n- Right after Equation 6, it is written that \"p(o) is a fixed distribution\". If it's fixed, can you give it more precisely? In addition, right after equation 18, it is written that \"$p(o)$ is a mixture of replay buffer and collected trajectories\". (minor: what does it mean i=1 in $\\\\{o_i\\\\}^{m}_{i=1} \\sim p(o)$?)\n- What are $f$ and $g$ in Equation  6 and 7? In Algorithm 1, they have an additional subscript $\\theta$. If these mean that $\\theta$ are parameters of the function approximators $f$ and $g$, I assume that it shouldn't be the same parameters?\n- What is $\\psi_\\omega$?\n\n$\\gamma-observability$ is defined in Equation 1 but is not used anywhere in the paper?\n\nWhat is a \"SAC planner\"? SAC is a model-free algorithm as far as I'm aware of.\n\nMinor comments:\n- The notation $b(x_t)$ was never used before\n- In the following \"Initially, given a state $s_1 \\sim \\mu(s)$ as a starting point, (...)\", should it be $s_0 \\sim \\mu(s)$ since you define $h \u2208 [0, H ]$? \n- Assumption 1: what is $b$, what is $b'$? It looks like the state space (for the belief) needs to the same dimension than the observation space?\n- line 3 in Algorithm 1: Shouldn't there be rewards also collected in the dataset?\n- Figure 1: How many seeds?\n\nTypos:\n- 1st sentence: \"Reinforcement learning (RL) based on Markov Decision Processes (MDPs) has has proved to be\"\n- \"W propose Energy-based Predictive Representation (EPR)\" leverages linear structure in the dynamics\n- \"onecan write\"",
            "summary_of_the_review": "The paper is not clear.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3459/Reviewer_m9BZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3459/Reviewer_m9BZ"
        ]
    },
    {
        "id": "xbYtYJoo9Z",
        "original": null,
        "number": 2,
        "cdate": 1666848203837,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666848203837,
        "tmdate": 1666848203837,
        "tddate": null,
        "forum": "aCCRmE3Pglv",
        "replyto": "aCCRmE3Pglv",
        "invitation": "ICLR.cc/2023/Conference/Paper3459/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper offers a method for reinforcement learning under partial observability by offering a new form of representing the environment model. Specifically, this representation takes advantage of the structure of the environment and linear approximation of value/Q functions. The authors showed the superiority of their method on some MuJoCo tasks.",
            "strength_and_weaknesses": "Strength:\nThe ideas behind taking advantage of the environmental structure were very interesting. Moreover, deep networks are still not performing well in domains with uncertainty, making this line of work very worthy of more exploration. Finally, I think the authors did a very good job of explaining the connection between EPR and other existing representations (although these connections could be moved to the appendix to make room for explaining the domains that EPR works/does not work) \n\nWeakness: \nI think the paper could be significantly improved if the authors give more intuition about the type of tasks in which their method works and tasks where it does not, especially classic POMDP tasks such as RockSample or classic localization problems. Particularly, it looks like the belief should be concentrated around a state and its surrounding (as opposed to multi-modal), so the method works well. \n\nI also found the choice of masking velocity to make the tasks POMDP in experiments a little bit strange. What was the rationale behind that? Generally, while I do believe that taking advantage of real problem structures is totally okay and in fact useful, uncertainty about agents' own sensing is not very much aligned with reality. There is actually a very nice benchmark for POMDPs based on MuJoCo tasks in \"Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs\" by Ni et al, ICML 2022 (repo: https://github.com/twni2016/pomdp-baselines). They have defined different types of Partial Observabilty and I think using even one type of these test-bench marks is extremely valuable for evaluation. Also, it looks like some baseline methods are provided by them. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear, but I think more intuition about the type of environment/tasks where EPR does or does not work is needed. Given the that architectures used to estimate parameters are not complicated, I do not see a major reproducibility issue. In terms of novelty, I think making a method that works well for Partially Observable environments is extremely valuable. However, I think the experiments should be more focused on this part as the MDP part is not new. ",
            "summary_of_the_review": "Overall, I believe this is a very interesting and promising idea, but I wish to see more standard experiments and/or more intuition about the type of tasks/environments where EPR is useful.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3459/Reviewer_TDvH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3459/Reviewer_TDvH"
        ]
    },
    {
        "id": "22P-wUuSlFy",
        "original": null,
        "number": 3,
        "cdate": 1667330727710,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667330727710,
        "tmdate": 1671060892570,
        "tddate": null,
        "forum": "aCCRmE3Pglv",
        "replyto": "aCCRmE3Pglv",
        "invitation": "ICLR.cc/2023/Conference/Paper3459/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes Energy-based Predictive Representation (EPR) for tractable learning, planning and exploration in POMDPs and fully observable MDPs. Concretely, the authors propose a nonlinear energy-based learnable model which does not explicitly parameterize the beliefs. Their model provides a linear sufficient representation for the state-action value function, which can be particularly beneficial to large state spaces. Subsequently, the authors introduce computationally tractable algorithms for learning, exploration/exploitation and planning, based on the principles of optimism and pessimism in the face of uncertainty for online and offline RL. The paper also discusses the connections between EPR and Predictive State Representation (PSR), and points out that the latter is not generally associated with efficient exploration and planning algorithms. The authors conduct an extensive empirical evaluation with both fully observable MDPs and POMDPs, comparing their approach to several model-based, model-free and representation-learning frameworks. The results confirm the superior empirical performance of the proposed representation learning scheme.",
            "strength_and_weaknesses": "Strengths\n- The proposed framework builds upon several prior works, e.g., prior work on linear/low-rank structures and spectral factorization for MDPs, random Fourier features, elliptical confidence bounds, feature space augmentation. The authors explain the various steps for building their framework in detail, and provide plenty of references in the process. This makes the paper not hard to follow.\n- Related work from several domains is covered quite thoroughly. Connections to prior works on representation learning (e.g., PSR and SPEDE) are also established.\n- The proposed framework seems able to solve a real problem for POMDPs, especially as the size of the state space becomes large. Furthermore, the 2 assumptions of \\gamma-observability and linearity are meaningful and well motivated by prior work.\n- The empirical results show strong performance. I particularly like that the EPR framework can be beneficial not just for POMDPs, but for fully observable MDPs as well.\n\nWeaknesses\n- There are quite a few typos. I suggest the authors proofread the document carefully. Examples: Th predictive -> The predictive, has has proved -> has proved, with slightly abuse -> with slight abuse, W propose -> We propose, an straightforward extension -> a straightforward extension, etc.\n- The notation was confusing at times. For instance, the authors introduced a symbol x_t^L for fixed-window histories (of size L) but then used x_t in equations (6) and (7) (and even elsewhere). I think consistency in the notation everywhere in the paper would help.\n- I think there is an (easy to fix) error in Assumption 1. \\gamma-observability needs a \\gamma multiplicative factor in the right term of the inequality. This is currently missing.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is not hard to follow and provides numerous citations to the (many) prior works that it builds upon.\n- The paper seems technically sound. It details with precision the prior results is makes use of, and new derivations are included in the text. One issue is the typos/language, which can nevertheless be fixed with careful proofreading (and some other minor errors in weaknesses listed above).\n- The work is quite novel. The idea of spectral factorization with linear features is not new, but this paper explores this approach in the new context of POMDPs. Furthermore, the overall framework is not a straightforward application of existing ideas but there are novel elements involved, such as the derivation of a linear factorization for the transition function for the beliefs.\n- Empirical results are strong and the comparison with baselines quite extensive.\n- The authors provide the experimental settings/hyperparameters that are necessary for replicating their results.",
            "summary_of_the_review": "Overall, I believe that the current work is a very interesting addition in the literature of representation learning for POMDPs with large state spaces, with strong and promising empirical results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3459/Reviewer_XFFG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3459/Reviewer_XFFG"
        ]
    },
    {
        "id": "3KWPadK4iN",
        "original": null,
        "number": 4,
        "cdate": 1667446478752,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667446478752,
        "tmdate": 1667447740246,
        "tddate": null,
        "forum": "aCCRmE3Pglv",
        "replyto": "aCCRmE3Pglv",
        "invitation": "ICLR.cc/2023/Conference/Paper3459/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new algorithm dealing with reinforcement learning (RL) in partially observable environments by linearly representing action-value functions. To learn linear action-value functions, the energy-based model is applied to represent a valid transition function. This energy-based method bypasses the explicit computation of beliefs. Then the learned energy-based transition on beliefs can be linearly factorized by applying the existing random Fourier feature trick. The action-value function and additional reward are represented by using the random feature, and numerical results show promising results on exploration, exploitation, and planning in RL with MDPs/POMDPs.",
            "strength_and_weaknesses": "The authors dealt with the formulation of RL in POMDPs with various interesting concepts. However, several parts of the paper should be clarified, and additional experiments should be performed. It was hard to check the reproducibility. Please refer to the detailed comments below.\n\n\nComments\n\n1. The core idea of linear value-function representation is not well-motivated. In this paper, the following statement is written: \"To handle the complexity induced by large state spaces, linear/low-rank structures have been introduced in MDPs.\" However, several questions arise.\n- Is linear representation guaranteed to be effective in POMDPs? Since linear representation lacks representation ability compared with nonlinear counterparts, it can be inefficient in POMDPs, and even in MDPs with large spaces.\n- Does the proposed linear representation enjoy better Q-function convergence?\n- This statement contradicts the core idea of linear representation. \u201cIn practice, we perform fitted Q iteration with a nonlinear component extending the linear parameterization.\u201d (page 6)\n\n2. Regarding energy-based parameterization\n- \u201cWe avoid any explicit parameterization and computation of beliefs\u201d (page 4). To support the superiority of the energy-based model over explicit parameterization, we require explicit parameterization methods including e.g., DVRL (Igl et al., 2018) or SLAC (Lee et al., 2020).\n- The background of energy-based parameterization should be explained in detail (e.g., in Preliminaries) since it is one of the core components in the proposed method.\n\n3. Regarding the random Fourier feature trick\n- It would be better to explicitly write the full derivation of Eqs. 9 and 10 for the readers since this is also another core component of the proposed method.\n- What is the sample number $n$ in the random Fourier feature trick? Is the performance robust to  $n$?\n\n4. Regarding exploration\n- Is extracting the random feature $\\phi$ the best method for generating additional bonus reward? Any other intrinsic reward method should be compared to support the claim.\n\n5. Experiments\n- Instead of the choice of masking velocity to make the tasks POMDP in experiments, is the proposed method still effective when masking position$^{a}$ (and let velocity visible), or input with noise$^{b}$? \n\na. Han et al., Variational Recurrent Models for Solving Partially Observable Control Tasks \n\nb. Meng et al., Memory-based Deep Reinforcement Learning for POMDP \n\n- As stated above, other explicit belief parameterization methods and other intrinsic reward for exploration are required.\n- In Tables 3 and 4, please describe the details of MLP and Trans. Does the transformer use causal encoding? Is positional encoding used? What is the number of layers and multi-heads of Trans?\n- Why the proposed method is effective in MDPs? Is that because of linear representation?\n- What is the number of negative samples $m$? Is the performance robust to  $m$ (and $\\lambda, \\alpha$)?\n\n6. Notation\n- Is $x_t$ implemented using RNN?\n- $\\gamma$ is missing in Assumption 1.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality can be improved by properly dealing with the comments above.\n\nMy concern regarding novelty is that without detailed motivation and explanation, the proposed method might be seen as a combination of an existing energy-based model and random feature techniques.\n\nReproducibility can be improved by explicitly writing the source of the baseline code used and submitting the implementation code as supplementary material.",
            "summary_of_the_review": "I have several concerns about the core idea as well as clarity, novelty, and reproducibility, and I hope the authors can address this in their rebuttal. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3459/Reviewer_4vmU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3459/Reviewer_4vmU"
        ]
    }
]