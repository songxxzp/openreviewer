[
    {
        "id": "hBSf_7SqtV",
        "original": null,
        "number": 1,
        "cdate": 1666645024478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645024478,
        "tmdate": 1666645024478,
        "tddate": null,
        "forum": "2SV2dlfBuE3",
        "replyto": "2SV2dlfBuE3",
        "invitation": "ICLR.cc/2023/Conference/Paper1947/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the problem of tracking a drifting optimization target under the assumption that the loss function is a smooth function of both the optimization parameter and time. The key insight is to compute the derivative of the minimizer with respect to time by differentiating the loss function with respect to time. Gradient descent is then augmented with an estimate of this derivative in order to more closely track the target.\n\nHigh level calculations show that with sufficiently good estimates, this method will track the target with a better dependence on the time interval between updates ($h\\rightarrow h^2$ where $h$ is the time interval).\n\nIn any given example, one then needs to build good estimates of the derivative of the target with respect to time. The authors explore several specific examples in which this can be obtained.\n",
            "strength_and_weaknesses": "The proposed approach makes a lot of sense and is presented clearly and intuitively. I agree with the authors that most approaches to tracking tend to ignore any possible temporal structure in the losses, and exploiting this seems a promising direction.\n\nThe requirement for strong convexity seems a bit limiting, however. This seems mostly needed for tracking the parameter vector distance, but one might expect that in many cases we are not interested in $$\\|\\theta -\\theta^\\star\\|$$ but $$R(\\theta)-R(\\theta^\\star)$$. Do the described techniques have any relevance for this metric?\n\nMoreover, modern analysis of gradient descent methods typically focuses on non-asymptotic convergence bounds. Is there any way to obtain such a bound here? \n\nThe paper could have been augmented with a discussion of lower bounds - is there any way in which the described techniques are optimal?\n\nFinally, the authors may wish to compare with the literature on dynamic regret in online convex optimization, which may provide another alternative to gradient descent which has seen a lot of recent development. Since these techniques also ignore temporal structure in the losses, I would expect this method to still compare favorably.\n",
            "clarity,_quality,_novelty_and_reproducibility": "No concerns",
            "summary_of_the_review": "The paper initiates a study of drifting optimization utilizing temporal structure in the loss function. Results are presented that can improve over naive gradient descent, along with experimental validation on certain problems.\n\nThe theoretical development seems promising, although perhaps a little more limited than ideal, establishing some cases in which the techniques can improve over SGD in certain cases, but without many general convergence bounds.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1947/Reviewer_SvER"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1947/Reviewer_SvER"
        ]
    },
    {
        "id": "vyjeFnihYS",
        "original": null,
        "number": 2,
        "cdate": 1666692161694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692161694,
        "tmdate": 1666776227145,
        "tddate": null,
        "forum": "2SV2dlfBuE3",
        "replyto": "2SV2dlfBuE3",
        "invitation": "ICLR.cc/2023/Conference/Paper1947/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the problem of tracking the sequence of best parameters of quadratic functions, where the best parameters changes in a stochastic way. The paper proposes an algorithm called PC and proves its convergence guarantees in terms of ATE. Then, some application problems are introduced, and the PC algorithm is applied. Experimental results show the advantages of PC against the SGD.",
            "strength_and_weaknesses": "Strength\n- simple update rule with a non-trivial theoretical guarantee\n- promising experimental results\n\n\nWeakness\n- some technical issues are not clear (see below)\n\nAs summarized above, the claimed technical contributions are non-trivial and solid. However, I have several concerns on technical details, which could affect the evaluations of the theoretical results. \n\n* The sample complexity or success probability are not discussed. \nThe problem setting is stochastic and the goal is to minimize ATE, i.e., the distance between the proposed and true parameters. To minimize this, the statistician has only access to finite samples from the distributions, which causes estimation errors. So, I am strongly wondering how the authors obtain the claimed results while avoiding these sampling/estimation issues. The paper needs some explanation on this matter to justify the results. Or some implicit assumptions exist.\n\nSo far, I do not have an intuition that the claimed results are correct. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written in general, and the results seem novel. But I raised the concerns above and some explanation on the matter is desirable.",
            "summary_of_the_review": "The theoretical and empirical results seem non-trivial, but I have some concerns about details of theoretical results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1947/Reviewer_iKWJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1947/Reviewer_iKWJ"
        ]
    },
    {
        "id": "ikXlkiHurqd",
        "original": null,
        "number": 3,
        "cdate": 1666792495440,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666792495440,
        "tmdate": 1666856034442,
        "tddate": null,
        "forum": "2SV2dlfBuE3",
        "replyto": "2SV2dlfBuE3",
        "invitation": "ICLR.cc/2023/Conference/Paper1947/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method to incorporate some knowledge about gradual distribution shifts during online gradient descent. The key idea is to add a predictor-corrector term to the gradient (i.e., adding an approximation of the time derivative), which tries to take into account the fact that the expected error changes over time with the current parameters. \n",
            "strength_and_weaknesses": "The paper is somewhat interesting, and appears to be quite thorough, but relatively unconvincing to me, since it is mostly theoretical, and the only experiments are in toy synthetic tasks/settings. For me this is particularly surprising because so much of the text talks about very practical issues, like distribution shift, and object tracking. In fact the paper is not doing any normal kind of tracking, and instead considers a simple synthetic problem where \"sensors\" return estimates of the target's \"distance\", with some zero-mean variance. Maybe this paper will be convincing and helpful to some, but in my case I don't know what to do with it. \u2028Since the paper is out of my domain I certainly won't fight to reject it, but I hope my suggestions on language will be somewhat helpful. ",
            "clarity,_quality,_novelty_and_reproducibility": "\n\"The rest of the paper is organized as follows\" -- and then only sections 2 and 4 are described. What about sections 3 and 5?\n\nlet \\theta is -> let \\theta be\n\n\"i.e. they are in general position\" -> ? What does it mean for things to be in \"general position\"?\n\n\"it is immediate\" -> ? (I don't know what is meant here.)\n\ninterchangeably to denote the same thing -> interchangeably\n\n\" on target function\" -> on the target function\n\n\"it\u2019s estimation\" -> its estimation\n\n\"As statisticians, we query/intervene the model at discrete time steps\" -> ? (this sounds like you are saying all statisticians do this)\u2028\n\n\"As mentioned in the Introduction, we here compare performance of a time-adjusted gradient descent method to a time-unadjusted one\"  -- The introduction did not mention this. The introduction mentioned comparing a predictor-corrector based algorithm versus gradient descent.\n\n\"demonstrated their efficacy in three applications\" -- I am not sure about this. These are toy experiments in simulated data, not \"applications\" by my definition of the word.  \n",
            "summary_of_the_review": "This paper may be interesting to some, but given the language on handling distribution shift and offering improvements to object tracking, I expected something more practical, rather than assumptions/lemmas/theorems and toy experiments in simulated data. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1947/Reviewer_YXuw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1947/Reviewer_YXuw"
        ]
    },
    {
        "id": "VXdo8vjhWDx",
        "original": null,
        "number": 4,
        "cdate": 1666836179065,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666836179065,
        "tmdate": 1666836179065,
        "tddate": null,
        "forum": "2SV2dlfBuE3",
        "replyto": "2SV2dlfBuE3",
        "invitation": "ICLR.cc/2023/Conference/Paper1947/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a novel predictor-corrector algorithm for time-varying stochastic optimization. It exploits the continuity of the domain shift. This work demonstrates the superiority of the proposed algorithm over stochastic gradient descent both theoretically and empirically. Further, this works shows that a simple sample average estimation of the predictor-corrector term may lead to non-convergence. In applications shown, a moving average based method is instead applied to estimate the predictor-corrector term. The utility of the proposed method is shown in three concrete real applications. ",
            "strength_and_weaknesses": "Strengths:\n1. This work focuses on an important and interesting question in ML community.\n2. The proposed solution (adding an additional predictor-correct term to the gradient updating rule) is intuitive, general, and easy to implement.\n3. Some theoretical insights are given to demonstrate the usefulness of the proposed algorithm, together with some concrete applications.\n\nWeakness:\nIt seems to me this work does not provide a general way to estimate the predictor-correct term. Instead, only some specific forms for the applications in Section 4 are discussed. For the proposed algorithm to be truly useful, I wonder how easy it is for a practitioner to come-up with an estimation for the term, or what would be the general recommendation for them. Or maybe, another question whether the proposed moving average algorithm can be shown to be useful in for general applications.",
            "clarity,_quality,_novelty_and_reproducibility": "This is in general a high quality paper, with clear presentations, and some novel ideas. There are some important questions which this work does not address, though.",
            "summary_of_the_review": "I think this paper targets a very important question and provides direction for an intuitive solution. Yet, there are some important parts missing from its current form which somehow prevents it from being more useful and practical.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1947/Reviewer_UaJ8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1947/Reviewer_UaJ8"
        ]
    }
]