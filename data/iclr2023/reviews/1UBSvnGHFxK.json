[
    {
        "id": "0n6LT8cjLA9",
        "original": null,
        "number": 1,
        "cdate": 1666630735417,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630735417,
        "tmdate": 1669042738904,
        "tddate": null,
        "forum": "1UBSvnGHFxK",
        "replyto": "1UBSvnGHFxK",
        "invitation": "ICLR.cc/2023/Conference/Paper3839/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents GMM-based robot policy optimization formulated as a Wasserstein gradient flow, resulting in constraining policy updates for a stable optimization process. The proposed method is compared against two baselines PPO-based GMM update and SAC-GMM. The evaluations are performed in three tasks with a toy robot arm to reach a target, avoid collision during trajectory tracking, and reach multiple targets. The results demonstrate that their proposed approach (WGF) outperforms PPO and SAC in all three tasks. ",
            "strength_and_weaknesses": "*Strength*\n+ A novel Gradient flow approach for GMM optimization\n+ Algothrim seems sound with theoretical backing.\n+ The results demonstrate better performance than PPO and SAC\n\n*Weaknesses*\n- Experiment section is relatively weak. The robot tasks are too simple. Although WGF outperforms PPO and SAC on those tasks, a more cluttered scenario with a realistic robot arm (URDFs of UR5, Panda, etc.) would exhibit the applicability of the proposed approach to practical environments. Perhaps a relevant baseline for simulation environment setup could be [1].\n\n[1] Continuous-time Gaussian process motion planning via probabilistic inference. The International Journal of Robotics Research, 37(11), 1319-1340.     ",
            "clarity,_quality,_novelty_and_reproducibility": "Updated: Overall paper is well-written. The approach is novel for optimizing GMM-based robot policies. However, the experiments need significant improvement to validate the proposed approach under complex collision avoidance constraints.  \n\n",
            "summary_of_the_review": "This paper presents a stable approach to optimize GMM policies for robot control which also outperforms prior methods such as PPO and SAC. However, the experiments could have more complex environments to highlight the proposed approach's scalability better. Even if it does not scale, those evaluations will highlight a limitation to address in future work. \n\nUpdated: The paper presents an interesting idea but lacks experiments that could validate the proposed approach under standard collision avoidance constraints for motion planning problems.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3839/Reviewer_1kyr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3839/Reviewer_1kyr"
        ]
    },
    {
        "id": "-1i_yoM06E",
        "original": null,
        "number": 2,
        "cdate": 1666941662218,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666941662218,
        "tmdate": 1666941662218,
        "tddate": null,
        "forum": "1UBSvnGHFxK",
        "replyto": "1UBSvnGHFxK",
        "invitation": "ICLR.cc/2023/Conference/Paper3839/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a Riemannian optimization method to optimize GMM policies. It uses GMM to represent the policy structure and demonstrates an EM-like approach to optimize such a policy with hidden variables with a maximum entropy RL objective. In each iteration, the Gaussian parameters are optimized with the Riemannian gradients, while the GMM parameters are optimized later with an additional Wasserstein distance constraint. Optimizing GMM with RL objectives enables the authors to transfer a GMM policy from demonstrations to new tasks. The authors considered several reach or collision avoidance tasks and showed better performance.",
            "strength_and_weaknesses": "Strength:\n\n- This paper attacks a very important direction, optimizing a policy with hidden structures. Though the paper only studied Gaussian policy here, I can imagine that a similar idea can be applied to other policy classes, like a neural network.\n- The EM-like optimization approach for optimizing GMM with REINFORCE is novel to me. I think this might be the key to its success, as directly optimizing the likelihood of GMM models are not easy.\n- Please correct me if I am wrong. The proposed Riemannian ensures the optimization does not break the semidefinite constraints of Gaussian models. Optimizing GMM parameters in the BW manifolds constrain the policy update, enabling better transfer performance.\n\nWeakness:\n\n- The GMM policy can hardly generalize to high-dimensional tasks, which will constrain its application. It will be interesting if it is possible to optimize broader policies.\n- I feel the paper lacks the necessary ablation study. The baseline comparison is reasonable. However, it is unclear why the proposed method works by comparing it with SAC. I do not fully understand why the Reinmannian gradient is necessary. Is it possible to optimize the policy by parameterizing the Sigma as a form of V^TV to guarantee its positive semidefinite? What would happen? Besides, what would happen if the Wasserstein part is removed from eq (13)? An ablation study is required to illustrate the contribution of each component.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the method is novel.",
            "summary_of_the_review": "The paper proposes an interesting idea of how to optimize a policy with prior structures. However, I feel the paper lacks enough ablation study and can not be accepted given its current form. I am happy to increase the score if additional results are provided. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3839/Reviewer_zFF5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3839/Reviewer_zFF5"
        ]
    },
    {
        "id": "dTxe18cEqpn",
        "original": null,
        "number": 3,
        "cdate": 1667621453249,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667621453249,
        "tmdate": 1667621453249,
        "tddate": null,
        "forum": "1UBSvnGHFxK",
        "replyto": "1UBSvnGHFxK",
        "invitation": "ICLR.cc/2023/Conference/Paper3839/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors consider the problem of policy optimization; they propose using Bures-Wasserstein gradient flows for optimizing policies which are constrained to the class of Gaussian mixture models. Existing methods in this setting don't exploit the structure enforced by the Gaussian mixture models, and the authors demonstrate the advantage of this perspective through empirical analyses on 2$d$ benchmarks. ",
            "strength_and_weaknesses": "**Strengths:**\n* The paper is well-written and easy to follow. I appreciate the informative, yet concise, introduction on Wassserstein gradient flows for the non-expert reader. \n\n* Even though the idea of formulating policy optimization using Wasserstein gradient flows is already known, the use of the Bures-Wasserstein geometry in the specific setting of Gaussian mixture models is novel and interesting. \n\n* Despite the drawbacks in the experiments (see comments below) the authors do a nice job interpreting the results and providing useful insights. \n\n* The authors claim that their perspective leads to significant computational advantage. (This would have been more compelling if it were reported as a benchmark of time taken for each method). \n\n\n**Weaknesses:**\n* My biggest concern is on the theoretical front. The objective of the paper is to study Wasserstein gradient flows which don't provide any dynamics for the weight update. A weight update, as in Eqs. (13 & 14), will warrant machinery from Wasserstein Fisher-Rao gradient flows instead. See, for example,  [Chizat et al. (2018)](https://link.springer.com/article/10.1007/s10208-016-9331-y) [Liero et al. (2018)](https://link.springer.com/article/10.1007/s00222-017-0759-8) [Chizat (2022)](https://link.springer.com/article/10.1007/s10107-021-01636-z) Therefore, it is unsurprising that the authors note, in their conclusions section, that the \"weight update often hampered with the update of the Gaussian parameters\". \n\n* Notwithstanding, for the proposed method using the Bures-Wasserstein geometry, I would really have liked to see some guarantee that the proposed methodology leads to policy improvement, c.f., Wang et al. (2020, Theorem 2). \n\n* On the experimental front, I found the benchmarks to be lacking. For example, it would have been instructive if the authors compared their method to other parametric policy updates such as [Moskovitz (2020)](https://arxiv.org/abs/2010.05380) ; especially since the notion of Wasserstein natural gradients (Chen & Li, 2020) is mentioned in \u00a73.1 and \u00a7A.4. \n\n* Furthermore, it would have been even more enlightening if the authors contrasted their performance to that of Wang et al. (2020), since this would have demonstrated the advantage of the Bures-Wasserstein perspective in contrast to their perspective which relies on the JKO scheme.\n\n\n\n**Minor Clarifications:**\n* I'm unclear on how the number of Gaussian components, $K$, are chosen for the mixture model. For example, Delon & Desolneux (2020, \u00a76.3) note that performance is sensitive to the choice of $K$. Alternatively, Figueroa & Billard (2018) tackle this issue by choosing a Dirichlet process prior on $K$. How sensitive are the results to this choice?\n\n* The authors implement the JKO scheme for the weight update using the Sinkhorn algorithm, which includes an entropic regularization. How does this interplay with the entropic regularization in the free energy functional J? The proximal map is effectively subtracting  a factor $\\beta$  of the entropy term due to $J_{r, \\beta}(\\pi)$ and adding a $1/2\\tau$ factor due to Sinkhorn?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. It would have been beneficial if there was a clearer introduction to the nomenclature associated with policy optimization and RL for the non-expert reader. \n\nBures-Wasserstein gradient flows and embedding Gaussian mixtures on the Bures-Wasserstein manifold have been studied extensively. However, making these connections in the context of RL and policy optimization are novel and certainly useful.",
            "summary_of_the_review": "The authors propose an interesting method for policy optimization through the lens of Bures-Wasserstein gradient flows. While the theoretical underpinnings are unclear, the experiments show useful improvement over _some_ benchmarks. It would have been even more compelling if the experiments were more exhaustive. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3839/Reviewer_8XBw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3839/Reviewer_8XBw"
        ]
    }
]