[
    {
        "id": "13TgZvHAY6",
        "original": null,
        "number": 1,
        "cdate": 1666744554221,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666744554221,
        "tmdate": 1666744554221,
        "tddate": null,
        "forum": "-Aw0rrrPUF",
        "replyto": "-Aw0rrrPUF",
        "invitation": "ICLR.cc/2023/Conference/Paper3642/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces and open-sources one of the largest 100B bilingual pre-trained language model GLM-130B. It covers a detailed pre-training process (training strategies, design choices) and the resulting model outperforms several large language models on English and Chinese zero-shot understanding benchmarks. The author also investigates the efficient inference of the GLM and show the efficacy of using Int4 quantization. \n",
            "strength_and_weaknesses": "GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL\n*Paper Summary\nThis paper introduces and open-sources one of the largest 100B bilingual pre-trained language model GLM-130B. It covers a detailed pre-training process (training strategies, design choices) and the resulting model outperforms several large language models on English and Chinese zero-shot understanding benchmarks. The author also investigates the efficient inference of the GLM and show the efficacy of using Int4 quantization. \n\nSummary Of Strengths\n- The paper is well-written and clearly presented;\n- It detailed the efforts made during the pre-training process for the GLM-130B, making it very valuable for practitioners in the large language model pre-training area; (though several design choices might not be feasible to ablate at that scale.). \n- The resulting model was tested across benchmarks, and showed surprising robustness towards quantization, making it more accessible for academia. \n\n\nSummary Of Weaknesses\n- Why the bi-lingual GLM performs a lot better in LAMBADA as shown in Figure 2? Is this also true for MMLU, Pile, and NLG tasks, any intuition about this?\n- Why not compare to OPT-175B models for quantization? \n- Intuition why the few-shot performance of GLM-130B in Table 4. falls behind other methods? Also, will the quantized GLM-130B yield similar few-shot performance?\n- Which GPT-3 175B is the GLM-130M compared to? Is it a fairer comparison to the InstructGPT variants?\n- How will the author attribute the performance gain (Bi-directional modeling [1], Multi-Task Instruction Pre-Training [2], or the Bilingual corpus and model architecture)?\n- It is not surprising that the Bi-directional model will outperform the Uni-directional model in understanding tasks. How are the generation tasks performance of GLM-130B compared to uni-directional counterparts?\n\n\n[1] Tay, Yi, et al. \"Transcending Scaling Laws with 0.1% Extra Compute.\" arXiv preprint arXiv:2210.11399 (2022).\n\n[2] Chung, Hyung Won, et al. \"Scaling Instruction-Finetuned Language Models.\" arXiv preprint arXiv:2210.11416 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written and clearly presented;\n- The model is open-sourced; \n- The design choice and detailed engineering efforts \n- More analysis behind the surging performance will be very valuable to add, for example:\n    - Any emergent abilities of the model (in terms of scale and in terms of the amount of data) as shown in [3]. \n    - Why the Bilingual model outperforms the Uni-lingual model by a large margin. \n    - Few-shot performance of quantized GLM-130B; \n",
            "summary_of_the_review": "It is not surprising that a large-scale bi-directional language model will outperform the causal counterpart on a range of zero-shot language understanding tasks. The detailed engineering efforts and design choices make this paper valuable, though my main concerns are what factors make the performant GLM-130B unclear, as well as NLG / few-shot performance. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3642/Reviewer_asnf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3642/Reviewer_asnf"
        ]
    },
    {
        "id": "AQ4Qjmvqys0",
        "original": null,
        "number": 2,
        "cdate": 1666853791810,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666853791810,
        "tmdate": 1666853791810,
        "tddate": null,
        "forum": "-Aw0rrrPUF",
        "replyto": "-Aw0rrrPUF",
        "invitation": "ICLR.cc/2023/Conference/Paper3642/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces GLM-130B, a bilingual English and Chinese LLM. The model is trained for 400B tokens (half on English  and half on Chinese). The model follows a bidirectional attention architecture with the self-supervised blank infilling objective. They also include a small percentage of MIP tokens. Int4 quantization enables inference on a diverse set of platforms. Various model instabilities and mitigation strategies are discussed.\n\nThe model, training log and inference through free APIs are provided and an effort is made to make the entire work reproducible.\n\nDetailed comments:\nTable 4: You state that the reason for PaLM's 1-shot ability being so much better than GLM is the diverse dataset. Could you elaborate a bit more?\nSection 6, subsection name \"Transferring\" --> consider using a better subsection name.",
            "strength_and_weaknesses": "Strengths:\n- The model is more reproducible than other similar sized counterparts.\n- The model's performance is competitive on the benchmarks that are evaluated on and outperforms large models like GPT-3 and PaLM.\n- Quantization for inference and mitigation strategies like deepnorm and embedding shrinkage for training instability are useful for the community.\n\nWeaknesses:\n-  I would have liked to see more English tasks evaluated on. The current set of results on the LaMBADA dataset a lot. I would like to see results on more GPT-3 tasks or SuperGLUE tasks for English.\n- More results at different scales and projection of trends or emerging capabilities as scale increases would have been interesting. The current set of results are missing this.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nPaper is clearly written and easy to follow.\n\nQuality:\nThe work is high quality and various aspects of building the model are studied well.\n\nNovelty:\nOne could argue that the paper lacks some novelty as they do not comprehensively produce new techniques that have been ablated at a smaller scale that have shown to produce improvements. But I understand that this is hard to do at such scale since they only get one shot at training the model. Smaller scale experiments with some of the new techniques could have helped here.\n\nReproducibility:\nOverall they take good care to make the model, training log, evaluation sets are reproducible.",
            "summary_of_the_review": "Overall, the paper is clearly written and details the building of the GLM-130B bilingual LLM. The model is competitive with other similar sized models and care has been taken to make the model set up reproducible. The evaluation suite for English could have been more comprehensive to substantiate the claims a bit more.  I would have also liked to see more experiments at a smaller scale and studies of emerging capabilities of the model at scale. Since this is a bilingual model, studying how much transfer or interference happens between these two languages would have been interesting.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3642/Reviewer_XkYq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3642/Reviewer_XkYq"
        ]
    },
    {
        "id": "i0zFEGvp_7",
        "original": null,
        "number": 3,
        "cdate": 1667164351581,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667164351581,
        "tmdate": 1667165064222,
        "tddate": null,
        "forum": "-Aw0rrrPUF",
        "replyto": "-Aw0rrrPUF",
        "invitation": "ICLR.cc/2023/Conference/Paper3642/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "It's always hard to summarize papers that report on such a big project, but fundamentally this is a tech report of training an English+Chinese LLM with techniques that improve on the state of the art for pretraining methodology and training stability.\n\nGLM-130 is concurrent with OPT and BLOOM, two other GPT-3-class LLMs that were released openly this year. It has d_model=12288 (same as GPT-3, OPT, and BLOOM) and 130B parameters and was trained on 200B tokens each of English and Chinese, including English NLU tasks formatted for multitask instruction pretraining to increase zero-shot performance on other, unseen English and Chinese tasks. It used a GLM objective that mixes span-by-span span-filling and prefix-LM, enabling use of bidirectional attention at inference time to further improve zero- and few-shot performance.\n\nIt consistently outperforms GPT-3, OPT, and BLOOM on English despite using fewer parameters and less compute (also less English data than GPT-3/OPT), and it sometimes also outperforms the much larger PaLM. It also consistently outperforms ERNIE TITAN 3 on Chinese, despite fewer parameters, less compute, and less Chinese data. As it can run in both bidirectional and unidirectional mode at inference time, ablating this shows that bidirectional attention contributes greatly to its outperformance. It also exhibits less bias/toxicity than other LLMs.\n\nIn terms of optimization methodology, it uses relatively standard techniques including AdamW with LR warmup and cosine decay, and also uses early batch size warmup to a relatively large 8.65M token total batch size. For training stability, it uses post-LN with depth-based initialization (\"DeepNorm\"), downscaled gradient in embeddings (\"Embedding Gradient Shrink\"), gradient clipping, and dropout. Either the DeepNorm or something else about the model makes the weight distribution especially narrow, enabling near-lossless int4 weight quantization for small-footprint inference.\n\nIn terms of training efficiency, it achieves about 32.5% MFU at steady state [135e12/312e12 * 3/4 for remat] and 25.1% MFU end to end [(130e9*400e9*6)/(96*8*312e12*60*60*24*60)] using 8 pipeline stages with a PipeDream-Flush (aka 1F1B) schedule, 4-way tensor parallelism, and 24-way (2-way intranode and 12-way internode) data parallelism. The authors also provide an efficient FasterTransformer-based inference setup that supports consumer GPUs.",
            "strength_and_weaknesses": "Strengths:\n- Very careful attention to training stability\n- Comprehensive details\n- Accessibility to smaller scale researchers\n- Open source\n- General openness even when admitting something could have been done better (e.g. \"We later found the instructions to implement two-dimensional RoPE from its author\u2019s blog https://kexue.fm/archives/8397, but our training has proceeded for weeks.\")\n\nWeaknesses:\n- The work chooses optimization hparams like initial LR and LR schedule relatively unsystematically.\n- They use dropout without explaining why, even though there's no repeated data in the dataset\n- The precision discussion mentions avoiding bf16 because of fp32 gradient accumulation, but fp16 or even bf16 accumulation is typically fine in practice.\n- There isn't really an explanation of why GLM-130 has narrower weight distribution than BLOOM (my guess is it's related to DeepNorm)?\n- The paper doesn't explicitly state that the Pile test set was held out of the training set, but I'm assuming it was.\n- Figure 7 leaves out PaLM 1- and 5-shot BBL, which are much better than 0-shot, and Chinchilla (which was only evaluated at 5-shot). Those are reasonable choices, but I'd prefer to err on the side of including more data points. Table 4 also leaves out PaLM and Chinchilla 5-shot.\n- no explanation of how 135 TFLOPS was computed (in particular, it probably includes remat but this isn't stated). Would recommend adopting MFU/HFU from PaLM paper.",
            "clarity,_quality,_novelty_and_reproducibility": "GLM-130B has very clear architecture and training details, relative to some other LLM papers. While it also has relatively few truly novel architectural or training components, this is reasonable as large-scale work isn't the time to introduce those. It also has unprecedented reproducibility for an LLM training paper, due to releasing just about every important component in open source.\n\nWhat appear to be the two key advances relative to other contemporary LLMs (bidirectional attention and instruction pretraining) are concurrent with other very recent work (e.g. UL2 and Flan from Google) so it could be helpful to contextualize with those.",
            "summary_of_the_review": "I think this really represents the gold standard in reproducibility for LLM publications, as well as the most capable open-source English or Chinese language model in existence.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3642/Reviewer_tHby"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3642/Reviewer_tHby"
        ]
    },
    {
        "id": "NBMKlFv8O55",
        "original": null,
        "number": 4,
        "cdate": 1667546534410,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667546534410,
        "tmdate": 1669132040479,
        "tddate": null,
        "forum": "-Aw0rrrPUF",
        "replyto": "-Aw0rrrPUF",
        "invitation": "ICLR.cc/2023/Conference/Paper3642/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper trains a new 130B parameter model with the GLM architecture/objective, with contributions in automatically stabilizing spikes with existing techniques, focussing on keeping inference costs and requirements low and therefore the model accessible to a large number of people. It is an achievement to beat GPT3 numbers and in a few cases PaLM numbers.",
            "strength_and_weaknesses": "Strengths:\n - Impressive set of techniques put together for stability and model quality (DeepNorm initialization, GLM instead of CausalLM objective, Embedding Layer Gradient Shrink)\n - Impressive performance on measured benchmarks esp compared to GPT3 & in a few cases PaLM\n\nWeaknesses\n - Would have loved to see some ablations, especially over the first few ~100B tokens as to which technique contributes how much to performance, either within GLM-130B or implemented across models, i.e. for example with T5 style models the GLM objective would be essentially T5\u2019s denoising task + prefixLM. Otherwise it is not clear what technique is responsible for how much of the gains.\n - More evaluations: The evaluation section is severely limited\n   - It could have evaluated fine-tuning over a few tasks (Ex: PaLM has both few-shot numbers and fine-tuning numbers for a few benchmarks like SuperGlue and TyDiQA) \u2014 this is understandably partly a resource issue.\n   - More extensive few-shot evaluations on say SuperGlue, Winograd style tasks, CBQA style tasks, CommonSense reasoning, Chain of Thought prompting etc \u2014 Since inference is both fast and cheap this shouldn\u2019t be too much of an issue and would increase my confidence of this method\u2019s applicability and more importantly in the released checkpoint.\n   - Not evaluating on tasks with fixed labels (like NLI) seems like severely and unnecessarily restrictive, i.e. I don\u2019t think Xian et al 2018\u2019s advice applies here, one can/should do NLI tasks with encoding the label in the prompt, while not giving an example (for zero-shot) or giving a few examples of each class (in a few-shot setting) and evaluating. Since MultiTask Instruction Pre-training (MIP) is happening here, I think it is fair to only exclude tasks that are substantially similar in the actual example content, if the task type seen is similar (say classification with the same labels) this should be called out and reported separately.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Points of Clarification (did not impact the score):\n\np1 / Introduction - \u201cHowever, both GPT-3 (and other 100B-scale ones)\u2014the model itself\u2014and how it can be trained, have been thus far not available to the public.\u201d\n\nIt might be better that this be reframed for clarity, I think all the models describe how they are trained (i.e. methodology in their paper), two of these have weights released (OPT 175B on request and Bloom 176B on github here - https://huggingface.co/bigscience/bloom/tree/main), and atleast Bloom\u2019s script is at https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme\n\np2 / Introduction \u2014 \u201cassociated with significantly less bias\u201d any causal explanation?\n\np2 / Introduction \u2014 \u201cunique property of GLM architecture\u201d \u201cINT4 quantization introduces very limited performance losses\u201d : It is not clear that ability to do INT4 comes directly from GLM architecture, it could also be the other changes like training objective (MASK + gMASK) or initialization choices like DeepNorm etc or a combination of both. Fig 5 does explain why INT4 works for GLM-130B and not BLOOM, but the claim that this is due to the GLM architecture seems strong, accordingly \u201cunique property\u201d seems overly strong and unsubstantiated.\n\np2 / Introduction \u2014 \u201cassociated with significantly less bias and generation toxicity\u201d: Would the authors make any causal explanation as to why this ought to be the case, i.e. the major contribution seems to be pre-training mechanics, I cannot see how \u201cless bias\u201d would follow, this might be empirically true, but lacking a causal explanation as to why this would be so (for example: data cleanup or filtering) this can only be taken as an incidental fact, rather than a recipe to have less bias.\np4 / Section 2.2 \u2014 \u201cSelf-Supervised Blank Infilling\u201d: This section is slightly confusingly framed, as written it led me to believe that both [MASK] and [gMASK] are in the **same sequence** \u2014 it wasn\u2019t until section 2 ending on p5 where it became clear that these are two separate tasks (due to the lengths being different in MASK and gMASK). I would advice to split the section \u201cSelf-Supervised Blank Infilling\u201d into two, one for MASK, another for gMASK, or at least call out them separately within the Self-Supervised section.\n\n\nMinor comments and typos (that did not impact the score):\n\n - p1 / Abstract - \u201cdisconvergence\u201d, maybe write \u201cdivergence\u201d since that\u2019s the commonly used term? Or do you want to convey that disconvergence = opposite of convergence, so could be either divergence (loss steadily increasing) or NaN or just unstable loss?\n - p1 / Introduction - \u201ccapabilities suddenly arouse\u201d, s/arouse/arise or arose.\n   - Same comment as above on p9 / Sec 6 \u201carouse as models\u201d \u2192 \u201carise as models\u201d\n   - p9 / Sec 7 Point 4 \u201carises\u201d \u2014 however this point is poorly written\n - p1 / Introduction - \u201cpioneers the studies of\u201d, s/studies/study\n - p1 / Introduction - \u201cwe come to realize\u201d, reframe as \u201cwe came to realize\u201d or \u201cwe have come to realize\u201d\n - p1 / Abstract & p2 / Introduction - \u201cmost ever affordable\u201d, \u201cmost affordable\u201d would suffice here.\n - p5 / Sec 3 - \u201cWe spend months\u201d change to \u201cWe spent months\u201d\n - p5 / Sec 3 - \u201cin sacrifice of a significant penalty on model performance\u201d, this is quite unclear, please reframe\n - p6 / Sec 3 - \u201cfirst used in the multi-modal transformer CogView\u201d, maybe just cite the cogview paper right there, sometimes paper title does NOT contain the name of the model, so the reference will be hard to find.\n - p8 / Sec 5.3 - \u201cHere is out intuitive attempt\u201d \u2192 \u201cHere is our intuitive attempt\u201d ; \u201cwe come up with\u201d \u2192 \u201cwe came up with\u201d\n - p8 / Sec 5.3 \u201cif we ever got a chance to continu pre-training GLM-130B\u201d - I would suggest summarizing under \u201cfuture work\u201d\n - p9 / Sec 6 - \u201cdespite many emerged 100B\u201d cut out emerged\n - p9 / Sec 6 - \u201cit has been concentrated on\u201d reframe perhaps\n - p9 / Sec 7 - \u201carchitecture alternative, in addition to GPTs\u201d \u2192  \u201carchitecture alternative to GPTs\u201d you are pointing out that GLM is an alternate way of self-supervision. I would rather frame it as an alternate \u201cobjective\u201d instead.\n - p9 / Sec 7 Point 6 - \u201cgradient to it\u2019s 0.1\u201d - ill framed sentence\n - p9 / Sec 7 Point 7 - \u201cunique\u201d is a strong word for here\n - p10 / Ethics - \u201cAnd to estimate and better collaborate\u201d no need for estimate\n - p10 / Reproducibility - PaLM citation is wrong, Paperno 2016 is mentioned instead of the PaLM citation.\n - p10 / Reproducibility - \u201cWe have paid great exertion\u201d, better way to frame would be \u201cWe paid great effort\u201d\n",
            "summary_of_the_review": "Weak Accept owing to a severely limited evaluation section and methodology, the work itself pulls a lot of techniques together to get a stably pre-trained O(100B) model with GLM objective and stability techniques which is very valuable to get broad recognition.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3642/Reviewer_tevQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3642/Reviewer_tevQ"
        ]
    }
]