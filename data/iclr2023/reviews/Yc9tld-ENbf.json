[
    {
        "id": "J2RyJQ_BTQj",
        "original": null,
        "number": 1,
        "cdate": 1665993523473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665993523473,
        "tmdate": 1665993523473,
        "tddate": null,
        "forum": "Yc9tld-ENbf",
        "replyto": "Yc9tld-ENbf",
        "invitation": "ICLR.cc/2023/Conference/Paper3790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Paper Summary: Deep neural networks (DNNs) are vulnerable to backdoor attacks, and this paper proposes a mitigation framework to detect malicious instances and classify the triggered instance to its original class in the inference time.\n",
            "strength_and_weaknesses": "Strengths:\n\n- Trendy topic\n\nWeaknesses:\n\n- Lack of novelty\n- Hard-to-follow\n- More explanations needed\n\nComments for the authors:\n\nDefense algorithm against DNNs backdoor attack has been a popular topic. In this paper, the authors propose a post-training backdoor mitigation framework to detect triggered inputs and output their original classes. The main intuition of the proposed framework is using a property of backdoor attack: the triggered instances could cause an alteration in the distribution of internal layer activations. Therefore, the authors leverage reverse-engineered triggers to correct  this distribution. The advantages of the proposed framework are twofold: Non-interference in the training process of the model and low computational complexity.\n\nHowever, I do have the following concerns.\n\n- First, the main contribution of this paper needs to be stated more clearly. The authors claim that they discover the triggered inputs can result in distribution alteration.  However, a similar conclusion has been proposed by Tran et al. (2018). Meanwhile, the authors state that the detector in the framework is different from most existing works, so what is the difference between this paper and Wang et al. (2019) and Xiang et al. (2020).\n\n- The writing of this paper needs to be improved. For example, this paper uses lots of notations which makes it hard to follow. The presentation about the choice of the parameters and the construction of the transformation function should be more clear. The authors could simplify them by using simple notations or list them in a table.\n\n- In the defender\u2019s assumptions, the authors assume that the defender has no access to the training set of the classifier, but does possess an independent clean dataset. What\u2019s the meaning of \"independent\" here? What are the distribution requirements for this clean dataset? In the experiments, the authors leverage 10% samples from the test set, which has the same distribution as the training set. However, we can acquire a classifier from the wild and we cannot know its training dataset. So, how can we choose this clean dataset in such a case?\n\n- More experiments are needed to demonstrate the generality of the proposed framework. For instance, the theoretical analysis focuses on the DNNs with Batch Normalization (BN) layers. However, the DNNs could leverage Group Normalization (GN) instead of BN. In the experiments, the architectures of DNNs are ResNet and MobileNet (both using BN). I would appreciate it if the authors could consider more different architectures.\n\n- Recently, Jia et al. [1] proposed a novel entangled backdoor attack. This attack leverages the soft nearest neighbor loss to entangle representations extracted from clean data and triggered data, which allows the model to use the same subset of parameters to recognize clean data and malicious data. As a result, the divergence of activation distribution between clean and backdoored instances is small, so I am curious whether the proposed method can detect such entangled attacks.\n\n[1] Jia H, Choquette-Choo C A, Chandrasekaran V, et al. Entangled watermarks as a defense against model extraction[C]//30th USENIX Security Symposium (USENIX Security 21). 2021: 1937-1954.\n",
            "clarity,_quality,_novelty_and_reproducibility": "good",
            "summary_of_the_review": "see strength and weakness",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3790/Reviewer_y2Ez"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3790/Reviewer_y2Ez"
        ]
    },
    {
        "id": "YvokFXlLtE",
        "original": null,
        "number": 2,
        "cdate": 1666492230000,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666492230000,
        "tmdate": 1666492230000,
        "tddate": null,
        "forum": "Yc9tld-ENbf",
        "replyto": "Yc9tld-ENbf",
        "invitation": "ICLR.cc/2023/Conference/Paper3790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper observes that the activation distributions are different for clean inputs and backdoor samples. It hence proposes to change activation values of inputs such that the predictions of backdoor samples can be corrected. Particularly, this paper utilizes existing trigger inversion methods to find a set of suspicious targets and generate corresponding backdoor triggers. For inputs with predicted labels in the set, it adjusts the activation values by using a transformation function. The transformation function changes the mean and standard deviation of activation values, which is obtained by comparing the activation differences between a set of clean samples with and without the inverted trigger. The experiment is conducted on five datasets and six types of backdoor attacks. Compared to several backdoor removal methods, this proposed approach can achieve good defense performance.",
            "strength_and_weaknesses": "Strength\n\n+ Important topic of backdoor removal\n+ Interesting perspective of adjusting activation values\n\nWeaknesses\n\n- Many parts of the method are not clearly explained\n- Strong assumption of backdoor attacks and the theorem may not hold\n- Lack of background for trigger inversion\n- No comparison with state-of-the-art backdoor removal techniques\n- No evaluation on recent and complex backdoor attacks",
            "clarity,_quality,_novelty_and_reproducibility": "* This paper tends to use mathematical equations to explain ideas and methods, which is good. However, it fails to provide intuitive introductions to the points/ideas that it aims to elaborate. Section 3 is somewhat easy to follow as most parts have intuitive explanations before diving into detailed math equations. It however becomes hard to understand in section 4.2, especially starting from page 6. There are a lot of details without giving an overview of what this paper aims to address. Intuitively, it seems trying to explain what the transformation function is and how it is estimated. But it is really hard to grasp the main procedure without a clear outline. It is suggested to rewrite the whole section 4.2 to make it more structured and intuitive. Details shall be explained after the outline is clearly established.\n\n* Theorem 3.1 is based on the assumption that \\sigma_b \\leq \\sigma. This paper uses a patch attack and additive attacks as examples to validate this assumption. However, it lacks empirical results to validate this assumption. Particularly, there are many complex backdoor attacks such as DFST [1], reflection backdoor [2], composite backdoor [3]. It is very likely they violate the assumption. This paper shall empirically study them. In addition, this paper only considers universal attack, where samples from all classes are misclassified to a target class. It does not consider label-specific attack, which only causes misclassification for one particular source class. Such an attack may not reduce the variance. This paper shall study this type of attacks or state clearly in their threat model.\n\n* The first part of the proposed method just uses existing trigger inversion techniques. However, it does not clearly explain how trigger inversion works, which makes the paper not self-contained. Since challenges 1&2 are directly solved by existing techniques, it does not seem reasonable to claim this part in the proposed method. It would be more clear to separate the first two challenges and the last one using subsections.\n\n* Some results in Table 1 do not seem reasonable. For example, BadNet and WaNet can easily achieve near 100% attack success rate. However, they are much lower in the table. This paper uses generated pair-wise triggers to eliminate injected backdoors, which is similar to existing method [4]. It shall also be compared with in the evaluation.\n\n* As mentioned earlier, there are many complex backdoor attacks [1-3], which may violate the assumption that the proposed defense is based on. It is suggested to evaluate on those attacks to see the performance of the proposed defense.\n\n\n### References\n\n[1] Cheng, Siyuan, et al. \"Deep feature space trojan attack of neural networks by controlled detoxification.\" AAAI 2021.\n\n[2] Liu, Yunfei, et al. \"Reflection backdoor: A natural backdoor attack on deep neural networks.\" ECCV 2020.\n\n[3] Lin, Junyu, et al. \"Composite backdoor attack for deep neural network by mixing existing benign features.\" CCS 2020.\n\n[4] Tao, Guanhong, et al. \"Model orthogonalization: Class distance hardening in neural networks for better security.\" IEEE S&P 2022.",
            "summary_of_the_review": "The idea of transforming activation values to remove backdoors is interesting and the results show potential. However, the paper lacks structure and is hard to understand. In addition, more empirical studies are needed to further validate the defense assumption and the performance on more complex backdoor attacks.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3790/Reviewer_zrrB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3790/Reviewer_zrrB"
        ]
    },
    {
        "id": "tv8yFwuTQY5",
        "original": null,
        "number": 3,
        "cdate": 1666920693908,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666920693908,
        "tmdate": 1666921172694,
        "tddate": null,
        "forum": "Yc9tld-ENbf",
        "replyto": "Yc9tld-ENbf",
        "invitation": "ICLR.cc/2023/Conference/Paper3790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a trojan attack mitigation method. The paper first observe that the the neural representation of clean and triggered samples are different in distribution. The proposed mitigation method first detects whether a model is attacked or not based on existing reverse engineering methods. For models that are considered attacked, the mitigation method minimizes the divergence between activation distributions of a clean input and a triggered input. \n\n",
            "strength_and_weaknesses": "The writing/presentation of the paper is not particularly well structured. A pseudocode can be helpful. \n\nWhile the experiment does seem to support the claim, I have reservations in both novelty and motivation. \n\nThe observation that neural activations of clean and triggered samples at the penultimate layer have different distribution is not particularly surprising, given it is well known that the softmax output of clean and triggered samples are different. The idea of mitigating by pulling the distribution together also seems incremental. \n\nA more serious issue is the fact that the method is highly dependent on the success of the existing detection method, and dependent on the reverse engineered triggers. This makes it hard to believe that the method is robust and generalizes well. First, as attacks continue evolving, it is hard to believe the used classic detection method is still as effective. Second, with so many engineering components involved, the method will be extremely hard to tune. \n\nMotivation-wise it is also unclear: if a model is already detected to be attacked, why is it worthy to mitigate it anyway?  A convincingly good mitigation method should be easy and simple to use. It should work despite whether the model is attacked.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is generally not very clean and a bit hard to read. \n\nNovelty is not strong as explained above.",
            "summary_of_the_review": "Overall, the idea is not quite novel. The solution involves multiple steps and heavily depends on existing attack detection methods. The mitigation-after-detection scheme is not very well justified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3790/Reviewer_2vpF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3790/Reviewer_2vpF"
        ]
    }
]