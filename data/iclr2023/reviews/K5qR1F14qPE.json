[
    {
        "id": "3ctUVkUzc5",
        "original": null,
        "number": 1,
        "cdate": 1666495436102,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666495436102,
        "tmdate": 1666495436102,
        "tddate": null,
        "forum": "K5qR1F14qPE",
        "replyto": "K5qR1F14qPE",
        "invitation": "ICLR.cc/2023/Conference/Paper107/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a method for unsupervised object discovery in videos. In contrast to the previous methods based on the optical flows between adjacent frames, they directly adopted RGB frame sequences. Specifically, the proposed model first encodes RGB frames into visual features, then pass the paired features into frame comparator module that computes relative motion. Finally, dual-layered flow decoder is introduced inspired by slot attention works that outputs two slots, i.e. foreground and background. Experiments are conducted on three benchmarks, surpassing the previous methods.",
            "strength_and_weaknesses": "** Strengths\n1. Directly relying on RGB frames to overcome the posed limitations from the usage of optical flows\n2. Combining several components from other literature to achieve state-of-the-art performances, e.g. iterative routing for layered representation from MG (Yang et al., 2021a) and test-time adaptation from OCLR (Xie et al., 2022)\n3. The submitted code and implementation details in the supplementary material facilitate reproducibility \n\n** Weaknesses \n1. Mild novelties; The overall framework seems quite similar to the previous idea introduced on MG (Yang et al., 2021a), particularly for dual-layered flow decoder, except for using RGB frames as input and encoding them further through frame comparator. The introduced three loss functions also lack of originalities as they are commonly used in literature for video-based prediction; flow reconstruction as a proxy task for unsupervised learning, temporal smoothness, and contrast of binary masks.\n2. Usage of the optical flow; While the authors claim the drawbacks of optical flow in terms of defected supervisory signal, their training object still rely on the pre-computed optical flow, RAFT, that also might suffer from static scenery and occlusions. \n3. The need of supervision; I understand that the proposed method do not require supervision for video object segmentation, but the generated optical flow is yielded by the supervised method, RAFT on MPI sintel and KITTI. I would say the required cost to annotate optical flow ground truth for each pixel is much more than the one of binary segmentation. The authors need to demonstrate their method also fairly works well while using unsupervised optical flow methods; such as ARflow.",
            "clarity,_quality,_novelty_and_reproducibility": " Please see the strength and weaknesses section above.",
            "summary_of_the_review": "As the weaknesses outweigh strengths, I lean towards rejection at this point. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper107/Reviewer_HWKn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper107/Reviewer_HWKn"
        ]
    },
    {
        "id": "1qLZfmNu_c",
        "original": null,
        "number": 2,
        "cdate": 1666967124993,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666967124993,
        "tmdate": 1666967124993,
        "tddate": null,
        "forum": "K5qR1F14qPE",
        "replyto": "K5qR1F14qPE",
        "invitation": "ICLR.cc/2023/Conference/Paper107/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThe paper tackles the task of unsupervised moving objects detection from videos. The approach takes as input the video sequence, without flow information.  The network consists of a sequence of known components, ie encoder, transformers, and slot attention decoder, and output a three layer map, ie, the segmentation mask and the bidirectional optical flow. It is only partially un/supervised, given the fact that the main loss function compares the output optical flow to a ground truth one, the lastest one resulting from a fully supervised approach. One of the motivations of the work is to address the problem of 'weak motion', ie when the object might be temporally static in a few frames of the input sequence, or when there are occlusions.  Experimental results are reported on three datasets and compared with related work. ",
            "strength_and_weaknesses": "\nThe overall goal of the paper touches to one of the key aspect of modern computer vision, ie unsupervised object detection. Using video as input is indeed a key element, as it enables to separate foreground from background. \nOne of the interesting observations of the paper (thought not new), is that pairs of frames are not sufficient to accurately detect the motion and hence the object: each frame should be aware of the whole sequence so that the frame-wise segmentation accounts for the whole sequence global motion. This is enabled by the transformer \\Phi_temp, which fuses altogether the frames embeddings.\n\nThe work attempts to move towards fully unsupervised training, but does in fact use some level of supervision via the OF loss. Similar scenario had been proposed by Choudhury and al (2022). An other choise could have been, instead of supervising the Flow reconstruction loss,  to minimize the image reconstruction loss, after wrapping one of the frames pair to the other. Other proxies to optimize and to constrain the estimated flow could also have been used. \n\nI am intrigued by the behavior of the segmentation masks. I am not sure what prevent the segmentation masks to be uniform in all frames (all ones and all zeros, for s=1,2 or reciprocally). Uniform masks in all frames would lead to an L_entro and L_cons to be zero, which is a situation which could be favored by the network, unless other constraints are given.\n\nIn the experimental section, I am not sure what are the results reported for OLCR.  Xie and al 2022 report results without and with test-time adaptation. Only results without adaptation are reported. However, results with test-time adaptation, which the authors also use, outperform the authors results for SegTrack and FBMS, and are very close for DAVIS2016. ",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity, quality\nThe paper is clear, the reading is fluid, the approach and its advantage are well discussed. The whole framework is well presented, including graphs and illustrations. The ablation stutThe appendices gives a level of details sufficient enough for the work to be reproduced. The authors will open  the source code. \nVisual illustration could be improved by adding the images of the difference between the segmentation output and the object mask ground truth. Since the visual improvements are very local and not ovbious, it would facilitate the interpretation of the visual outputs. \nI would personally suggest to put the section Related work after the introduction rather than at the end of the paper. In particular, the work is built upon previous similar and related approach, and it seems to me preferable to mention them at first.  \n\nNovelty\nThe work motivation, ie unsupervised object detection from video without using OF as input, is a continuation of Choudhury and al (2022). From a technical point of view, the network architecture (and loss functions) mainly reuse components which have already proved to be efficient in similar scenarios. \n",
            "summary_of_the_review": "The main strength of the paper is certainly the clarity and quality of the work.  It is in the continuation of Choudhury and al (2022) (both for the scenario and the quantitative results). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No  ethic concerns",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper107/Reviewer_fGvA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper107/Reviewer_fGvA"
        ]
    },
    {
        "id": "aATjREZhlD",
        "original": null,
        "number": 3,
        "cdate": 1667186696324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667186696324,
        "tmdate": 1667257518907,
        "tddate": null,
        "forum": "K5qR1F14qPE",
        "replyto": "K5qR1F14qPE",
        "invitation": "ICLR.cc/2023/Conference/Paper107/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a self-supervised model for video object discovery. It only takes consecutive RGB frames as input instead of optical flows as in previous works. The model is trained to reconstruct the optical flow between any paired frames that generated through an off-the-shelf optical flow estimator. A temporal consistency loss on motions at different paces is proposed to encourage the model to segment the objects even if they stop moving or are partially occluded at some time point. Experimental results demonstrate its superior performance on several datasets. ",
            "strength_and_weaknesses": "Strength: \n- This paper is well-written and easy to follow. \n\n- The motivation for using RGB as input instead of flow is clear and reasonable, i.e., having cues when static or partially occluded and keeping rich texture information. \n\n- The proposed model architecture and temporal consistency loss have been aligned with these motivations. I like the idea of directly using the RGB frames as input to segment the object as the optical flow is implicitly included in a sequence of the RGB frames. With the design of temporal fusion and frame comparator module, the model can learn optical flow implicitly instead of precomputing it explicitly. The design of temporal consistency loss help to deal with the static or partially occluded situation.\n\nWeaknesses:\n- I read your supplementary material about using DINO for test-time adaptation results, but it is unclear to me how do you manage to use the feature from DINO to do mask prop. If you use the idea of mask propagation. Will the mask error be accumulated along the frame? Why it can boost performance a lot?\n\n- Although the proposed method provides several ablation studies, I still suggest the authors conduct the following ablation studies to enhance the quality of the paper: \n\n    * Does the number of learnable queries in the slot attention module impact the performance? Since the slot attention module is initially designed for segmenting different objects in a toy dataset. If there are more queries, are we able to tackle the limitation of segmenting multiple objects separately? \n\n    * Have you tried other optical flow methods such as PWC-Net or ARFlow. It seems that the RAFT optical flow results are very noisy on SegTrackv2 and FBMS-59 datasets using the script provided by motion grouping. Sometimes it is even enabled to identify the object for instance with frame gap =1,2 when the motion changes are too small. Do you think it will hinder your learning?\n\n- A minor question regarding the sliding window during inference. Is that an overlapping sliding window or a non-overlapping sliding window? \n\n- As you mentioned in the limitation, the proposed method is not able to segment multiple objects. Can you put a figure to visualize how it fails (i.e. only the main object is segmented or multiple objects are partially segmented)?\n\n- Some related work is missing: \n\n    * Yanchao Yang et al. Dystab: Unsupervised object segmentation via dynamic-static bootstrapping. In CVPR, 2021.\n\n    * Vickie Ye, et al. Deformable sprites for unsupervised video decomposition. CVPR, 2022.\n\n    * Yangtao Wang, et al. TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut. arXiv preprint arXiv:2209.00383, 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is well-written and easy to follow.\n\nQuality: Overall, this paper has good quality.\n\nNovelty: This work adds two more modules(i.e., Temporal fusion and frame comparator) on top of Motion Grouping model to get slightly better results, which is not novel enough for me. \n\nReproducibility: I believe the work is reproductive. Code was provided but  not carefully checked.",
            "summary_of_the_review": "This work on self-supervised learning video object discovery using RGB frames only is well-motivated and designed. However, the proposed model seems to add two more modules on top of the Motion Grouping model and the novelty is limited. Although several empirical results have demonstrated the performance on several datasets, more experiments are still needed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper107/Reviewer_NSCm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper107/Reviewer_NSCm"
        ]
    },
    {
        "id": "s6_NRdZJfs2",
        "original": null,
        "number": 4,
        "cdate": 1667202018655,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667202018655,
        "tmdate": 1667202018655,
        "tddate": null,
        "forum": "K5qR1F14qPE",
        "replyto": "K5qR1F14qPE",
        "invitation": "ICLR.cc/2023/Conference/Paper107/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a novel method for self-supervised object discovery from video. The authors cast the object discovery problem as optical flow construction problem, where optical flow provided by an off-the-shelf method(RAFT) is used as training signal. The authors demonstrate the effectiveness of the proposed method on several popular datasets includes DAVIS2016, SegTrackv2 and FBMS-59 by comparing to state-of-the-art methods. Compared to other self-supervised object discovery methods exploiting motion cue, the proposed method directly operates on RGB input domain without optical flow computation in the inference stage, thus has advantage on inference speed. ",
            "strength_and_weaknesses": "Strength:\n1. The proposed method has superior performance on public datasets compared to state-of-the-art methods with similar constraint(Liu et al., 2021). \n2. The proposed method has close or superior performance on public datasets compared to state-of-the-art methods with less constraints, e.g. using optical flow in the inference stage, use synthetic training data. This also proves the effectiveness of the proposed method.\n3. The proposed method effectively exploits temporal fusion so motion cue from multiple frames could be used to discover the object. This makes the method being robust in the cases where object is partially occluded, or is static in partial frames. \n4. The proposed method is faster in the inference stage compared to other methods, thanks to the setting of operating on raw RGB video clips.\n5. The authors did ablation study to show the impact of each component of the proposed model.\n\nWeaknesses/Questions:\n    1. The proposed method is memory intensive. Is this the reason that only up to 7 consecutive frames are used in the experimental setting section?\n    2. If the foreground object only appears in part of the video(we don\u2019t have supervisory signal so we don\u2019t have information on this), would the method still work? What would the alpha value for foreground be in those frames where foreground doesn\u2019t exist?\n    3. Since optical flow is the training signal, how would the OF quality/failure impact the result? It might be good to evaluate on a synthetic dataset and compare the training with both ground truth OF and estimated/flawed OF.",
            "clarity,_quality,_novelty_and_reproducibility": "I would prefer the \"related work\" section to be put before the modeling section, rather than just before conclusion. But the paper is clearly written and well presented overall. The proposed method is novel in my opinion. ",
            "summary_of_the_review": "In this paper the proposed object discovery method is practical and novel. The authors did a good job on modeling the problem, perform experiment on standard datasets and doing ablation study. I would recommend acceptance of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper107/Reviewer_7qqQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper107/Reviewer_7qqQ"
        ]
    }
]