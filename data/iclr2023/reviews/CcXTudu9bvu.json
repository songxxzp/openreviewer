[
    {
        "id": "mzB15s0PB_",
        "original": null,
        "number": 1,
        "cdate": 1666282770228,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666282770228,
        "tmdate": 1666282770228,
        "tddate": null,
        "forum": "CcXTudu9bvu",
        "replyto": "CcXTudu9bvu",
        "invitation": "ICLR.cc/2023/Conference/Paper6402/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors explore the use of importance sampling (IS) its extensions for selecting clients in FL, where IS is based off gradient norms. They first provide a convergence analysis for standard IS sampling (sampling proportional to size of gradients). They then propose an alternative IS sampling which captures both gradient magnitude, as well as gradient diversity (the difference between the client's gradient and the global objective function gradient). They provide a convergence analysis for this sampling scheme and compare the scheme with the original FedIS.",
            "strength_and_weaknesses": "Strengths\n- Strong theoretical analysis. \n- Theory is interpretable and illuminating about the nature of the problem.\n\nWeaknesses\n- Experimental results are somewhat weak, with only a marginal improvement over FedIS in experiments on real datasets. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written, which is impressive for a theoretically focused paper. \nExperiments are well described and would not be too challenging to reproduce.\n",
            "summary_of_the_review": "This paper extends the analysis of IS for FL, and proposes a principled extension which (at least partially) solves a well known diversity issue that is common to IS in general (though it's certainly even more pronounced in FL). The clear theoretical analysis is a valuable contribution to the field of client selection for FL. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6402/Reviewer_xCAd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6402/Reviewer_xCAd"
        ]
    },
    {
        "id": "kKngt5h7Kqu",
        "original": null,
        "number": 2,
        "cdate": 1667293203497,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667293203497,
        "tmdate": 1667293203497,
        "tddate": null,
        "forum": "CcXTudu9bvu",
        "replyto": "CcXTudu9bvu",
        "invitation": "ICLR.cc/2023/Conference/Paper6402/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considered the problem of client sampling in federated learning (FL) to improve the convergence speed of FL training. The authors proposed a new client sampling scheme called DELTA, which is unbiased and able to sample more diverse clients that carry valuable information for global model updates. The authors conducted a theoretical convergence rate performance analysis and verified the theoretical convergence performance of their proposed algorithm through simulation experiments.",
            "strength_and_weaknesses": "Strengths:\n1. The client sampling problem in FL is a timely and important problem in FL.\n2. The proposed DELTA client sampling scheme captures diverse clients similar to the federated importance sampling (FedIS) scheme while offering unbiased client sampling performance.\n3. The new and tighter convergence results for FedIS contribute to new understandings of FedIS.\n\nWeaknesses:\n1. The bounded stochastic gradient assumption in Assumption 4 is a bit restrictive and no longer needed in many state-of-the-art FL algorithms' convergence analyses. It may be interesting to see whether this assumption can be relaxed for the DELTA client sampling scheme.\n\n2. The sampling probability $p_i^t$ in Eq. (11) requires full gradient evaluations, which are difficult to implement in practice (also admitted by the authors). To address this challenge, the authors proposed the use of stochastic gradients to approximate the computations. However, the authors didn't theoretically analyze the impacts of the approximation errors on the convergence rate performance, which is somewhat disappointing.\n\n3. The experiment comparisons in Section 5 may be unfair. In the comparisons between DELTA, FedAvg, and FedIS, the authors only compared the convergence speeds in terms of iterations. However, the proposed DELTA method requires rather complicated calculations of $p_i^t$ in Eq. (11) compared to simple uniform sampling in FedAvg and relatively straightforward calculations of $p_i^t$ in FedIS. That is, the per-iteration complexities of these methods are quite different. Thus, it may be better to also compare the wall-clock convergence time between these methods.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written in general. However, the paper could have been organized in a better way. For example, the DELTA algorithm should be placed in much earlier sections. In the current form of this paper, the DELTA algorithm appears rather late, which left quite a few notations undefined (e.g., $\\eta$ and $\\eta_L$) and created some difficulty/confusion in following the paper. The proposed DELTA algorithm is novel. The reproducibility of this paper is good.",
            "summary_of_the_review": "This paper studied the problem of client sampling in FL. The authors proposed a new client sampling algorithm called DELTA, which could achieve both unbiasedness and capture diverse client information. The authors provided rigorous theoretical performance analysis and also provided new insights for FedIS, which is also a new contribution. However, the authors didn't provide any theoretical performance analysis for DELTA with stochastic gradient approximation. Also, some experimental results on convergence speed comparisons may be unfair.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6402/Reviewer_N8Mo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6402/Reviewer_N8Mo"
        ]
    },
    {
        "id": "mhHJ9TxhRB",
        "original": null,
        "number": 3,
        "cdate": 1667482706728,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667482706728,
        "tmdate": 1670854360211,
        "tddate": null,
        "forum": "CcXTudu9bvu",
        "replyto": "CcXTudu9bvu",
        "invitation": "ICLR.cc/2023/Conference/Paper6402/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes novel client sampling strategies to accelerate the convergence of \nfederated averaging methods with partial client participation. The idea is to determine \nthe sampling strategies to minimize variance from the worst-case convergence bounds of the methods.",
            "strength_and_weaknesses": "Designing optimal client sampling strategies is a problem of broad interest for designing scalable federated learning methods. \nThe authors attempt to **design optimal client sampling strategies in theory and practice** to accelerate the convergence of commonly used federated averaging methods. \n\nHowever, I would like to state the following **major concerns**: \n\n1. **Limitations of using the optimal sampling for DELTA.** \n\nFrom Corollary 3.4, the optimal sampling depends on $\\Vert \\nabla F_i(x) - \\nabla f(x) \\Vert$. This strategy cannot be implemented in practice, mainly because the gradient of the whole objective $\\nabla f(x)$ cannot be accessed usually for problems over huge amounts of data.\n\nTherefore, the authors should state how this strategy is modified to be able to implement it in their experiments.\n\n2. **Redundant assumptions for deriving the convergence results for FedIS and DELTA.**\n\nAssumptions 2 and 3 are commonly used for deriving convergence guarantees for federated methods. This is because Assumption 2 states how different the local stochastic gradient and the local full gradient is, and Assumption 3 implies how different the local full gradient and the whole full gradient is. \n\nHowever, Assumption 4 seems to be redundant to Assumption 2, since both of them impose the property of local stochastic gradients. Therefore, the stated convergence results for FedIS and DELTA seem too restricted. \n\n3. **Lack of motivation on how to design the optimal client sampling strategies for FedIS and DELTA.** \n\nSince FedIS or Algorithm 3 from (Chen et al., 2020) looks similar to DELTA, the proof techniques between these methods should be similar. Hence, I am not sure how the variance from convergence guarantees for FedIS and DELTA is different. Can the authors elaborate on this and perhaps add the motivation before stating theoretical results?  \n\n4. **Numerical evaluations against other unclear existing sampling strategies.** \n\nIn the experiments, the authors compared their sampling strategies against others, e.g. the power of choice, norm, and heterogeneity. However, these strategies are not clearly stated (i.e. what is $p_i^t$)? Are they existing sampling strategies, e.g., from (Chen et al., 2020)? \n\nSince different $p_i^t$ lead to different additional computational costs, it would be also more interesting to compare performance of the algorithms using different sampling with respect to the wall-clock time. \n\nFurthermore, I have the following **small concerns**: \n\n1. The step-size condition in Theorem 3.1 for $\\eta_L$ and $\\eta$ can be simplified to improve readability of this theorem. \n\nThe authors can write $\\eta_L < \\min\\( 1/(8LK), C \\)$ where the constant $C$ is obtained from the condition that\n$1/2 - (10L^2/m)\\sum_{i=1}^m K^2\\eta_L^2(A^2+1)>0$. Then, the author can state the condition for $\\eta$ which is $\\eta \\leq 1/(\\eta_L L)$.\n\nHowever, the constant $A$ is not clearly stated. Does it depend on the index of the client $i = 1,2,\\ldots,m$?\nCan the authors check this? \n\n2. The legend in Figure 5 includes FedSRC-G and FedSRC-D. I believe that they refer to DELTA and FEDIS. Can the authors check and edit the legend? \n\n3. Fedprox from (Li et al., 2018) does not use variance-reduction techniques to design federated optimization methods.\nTherefore, the authors should revise this part of the text which is in their contribution section.\n\n**Typo(s) I can spot:** \n- the data heterogeneity to **fast** the convergence speed $\\rightarrow$  the data heterogeneity to **accelerate** the convergence speed.\n\nNote that due to limited time, I cannot check convergence proofs carefully. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and novelty of the work should be improved and better highlighted. Details related to these concerns are mentioned in the section of strength and weaknesses. ",
            "summary_of_the_review": "I think this paper considers the problem of broad interest. However, the contributions seem to be unclear, due to unclear motivation on why and how the authors design optimal sampling strategies which are better than existing strategies. In addition, some of the proposed strategies cannot be implemented in practice, thus raising the issue on how they modify the strategies to be implementable in the experiments. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6402/Reviewer_LBTW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6402/Reviewer_LBTW"
        ]
    },
    {
        "id": "Z5HcbBbrJ9Y",
        "original": null,
        "number": 4,
        "cdate": 1667523587138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667523587138,
        "tmdate": 1667523587138,
        "tddate": null,
        "forum": "CcXTudu9bvu",
        "replyto": "CcXTudu9bvu",
        "invitation": "ICLR.cc/2023/Conference/Paper6402/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method to improve previous (cluster-based) important client sampling methods in federated learning. The new method is motivated by the insight that it would be beneficial to select clients from diverse groups. Convergence analysis are also provided and the authors claim they improve over existing ones. At last, experiments on FEMNIST and CIFAR-10 are provided to validate the performance.",
            "strength_and_weaknesses": "Strength\n- The idea of sampling from diverse gradient groups is a novel idea and seems to be promising.\n- The authors make the proposed sampling algorithm to be unbiased.\n\nWeakness\n- The writing of this paper should be improved. There are many vague statements. For example, in the introduction, the authors explain why previous works are not good. But all the explanations are just intuitions or conjectures. They cannot be used to support the observations in Figure 2. We do not know whether these conjectures are true or not (e.g. whether cluster-IS really select clients with small gradients and whether this is the core reason causing slow convergence).\n- Also, many mathematical expressions are wrong. For example, in equation 2 and 6, the function $f$ should also have some subscripts because its form changes when we sample different clients. Also, in equation (5), the authors define $E|\\nabla f(x)|^2 = E|\\nabla \\tilde{f}(x)|^2 + \\chi^2$. However, in equation (15), they wrote $E|\\nabla \\tilde{f}(x)|^2 = E|\\nabla f(x)|^2 + \\chi^2$. It is obvious these two equations are conflict with each other.\n- The proof may not be correct. I didn't find any proof details for Theorem D.2, which I suspect is wrong due to the above mistakes.\n- Remark 3.2 (3) is not accurate. \"Chen (2020) ... with additional gradient similarity bound\". This sounds like this paper did not do this and remove this assumption. But in fact, the authors define it in Assumption 3 and also use it.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper should be improved. The authors should polish the writing with rigorous statements and check the correctness of the theoretical results.",
            "summary_of_the_review": "I suspect that this paper has mistakes in the proof. The theoretical results may not be valid.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6402/Reviewer_Yzew"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6402/Reviewer_Yzew"
        ]
    }
]