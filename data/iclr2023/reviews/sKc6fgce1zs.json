[
    {
        "id": "9OZt9QXHesm",
        "original": null,
        "number": 1,
        "cdate": 1666553603576,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666553603576,
        "tmdate": 1669784282103,
        "tddate": null,
        "forum": "sKc6fgce1zs",
        "replyto": "sKc6fgce1zs",
        "invitation": "ICLR.cc/2023/Conference/Paper4590/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method (ELE) to incorporate information from expert (observation-only) demonstrations into an RL agent by first learning an estimate of \"progress\" and using this estimate as an intrinsic reward. Progress in this case is defined to be the temporal distance between two states.\n\nResults are first shown on a toy grid-world environment, where demonstrations are epsilon-greedy oracle policies (choosing the optimal action 2.5% of the time). Further experiments on 7 different tasks in the NetHack environment shows that ELE is competitive against a wide variety of baselines, and is particularly effective in sparse reward settings. \n\nFinally, qualitative visualizations of the progress function, an episode from a trained policy, and saliency maps are presented, as well as ablations on the expert demonstrations dataset size and the \"history length\" hyperparameter.",
            "strength_and_weaknesses": "Strengths:\n\nWell written paper with the main idea presented very clearly and with good motivation. The method is novel (to the best of my knowledge) and clever, the experiments and baselines are comprehensive, and the toy grid-world experiment is succinct and didactic. The conclusion includes candid discussion of the limitations of this approach, which I appreciated.\n\nWeaknesses:\n\nI can see empirically that ELE outperforms FORM on the sparse reward tasks (figure 3), but I think some analysis of why this is the case would be helpful. Both methods get access to the same demonstration data and both are just intrinsic rewards on top of the same RL algorithm (from what I understand). So what makes ELE better than FORM? \n\nThe ablations in figure 6 shows empirically why we would want to use only demonstrations from the top 10 users, but I don't understand fundamentally why more demonstrations would hurt. For example, what about the top 10 players makes it so different from the next 10 players?\n\nThe experiments section could be improved by demonstrating the method on more than just the NetHack environment.\n\nFor which task is the right side of figure 3 plotting? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of high quality, is clear, novel, and reproducible. ",
            "summary_of_the_review": "The paper is well written, the idea is novel, and experiments are fairly comprehensive. Only reservations I have are in the fact that there is only one environment and the lack of analysis comparing ELE to FORM (see weaknesses section).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4590/Reviewer_8ZSu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4590/Reviewer_8ZSu"
        ]
    },
    {
        "id": "PRUoYi9cbvB",
        "original": null,
        "number": 2,
        "cdate": 1666670376067,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670376067,
        "tmdate": 1666670376067,
        "tddate": null,
        "forum": "sKc6fgce1zs",
        "replyto": "sKc6fgce1zs",
        "invitation": "ICLR.cc/2023/Conference/Paper4590/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an approach for improving agent exploration in reinforcement learning tasks using a learned distance model from an unlabelled dataset of human plays. The training paradigm has two steps. First, the authors train a distance predictor for each pair of states. The distance is used to help agents to explore as an intrinsic reward signal.",
            "strength_and_weaknesses": "The paper presented a simple but promising approach to learning intrinsic rewards for exploration. The model can be learned from unlabelled offline data, which contain human expert demonstrations on possibly irrelevant tasks. The model outperforms several baselines on tasks in the NetHack environment.\n\nOverall I think the paper presentation is clear, and the comparisons are sufficient. There are a few points for potential improvements.\n\n1. Is there any way that we can test the accuracy of the pretrained distance prediction model? How does that really translate into performance?\n2. I am not familiar with the NetHack environment. a. Is the map fixed? b. Do the tested tasks require handling partial observability? I am asking because If the map is not fixed how can you model learn the distance function between different \"levels?\"\n3. How similar are the demonstrations to the testing tasks? Are expert demonstrations exactly for the task (e.g., oracle)?\n4. My understanding of the answer to Q3 is no. But then, I don't understand how the baseline BCO works. \n5. Also, while I understand that there will be a lot of \"noise\" in such a large-scale dataset, Figure 6 makes me very worried: it seems that the authors have just used data from 10 players. Do you have any guesses about why model performance drops when you have more data? Is it because the larger dataset requires a larger model? Or is it because of the \"noises?\" If so, what is exactly the \"noises?\" Or is it an issue with the proposed method? (e.g., the proposed objective is too hard to be learned from Internet data).\n6. The authors should consider comparing with works on \"subgoal generation.\" For example,\nCzechowski, Konrad, et al. \"Subgoal search for complex reasoning tasks.\" Advances in Neural Information Processing Systems 34 (2021): 624-638.\n7. Is the state-only setting a real constraint of the environment/dataset? That is, is there any way to obtain large-scale data with actions?\n8. How will the model compare with other exploration strategies, such as Go-Explore (Ecoffet et al., 2019)?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the method is clear. Some baseline implementation can be further clarified.",
            "summary_of_the_review": "Overall the paper presented an interesting idea and promising results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4590/Reviewer_iUvn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4590/Reviewer_iUvn"
        ]
    },
    {
        "id": "KqXVP7lOVJw",
        "original": null,
        "number": 3,
        "cdate": 1666992658450,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666992658450,
        "tmdate": 1669137047924,
        "tddate": null,
        "forum": "sKc6fgce1zs",
        "replyto": "sKc6fgce1zs",
        "invitation": "ICLR.cc/2023/Conference/Paper4590/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the question of how to leverage expert demonstrations when explicit actions are not present in RL environments which have long timescales and sparse rewards, but for which there exists a notion of monotone increasing progress over time. The central idea of this paper is to use the (incomplete) expert demonstration data to estimate a function which predicts the temporal distance between two states. Then, this function is used to define a weighted auxillary reward which measures the progress between the current state and the state $k$ steps in the past; $k$ is a hyper-parameter which is tuned. RL algorithms without this additional signal are then compared to RL algorithms which use this modified reward function, this meta-algorithm is referred to as ELE, and experiments are conducted both in a toy gridworld setting and in the first 1M steps of the game NetHack, a challenging environment with long timescales and sparse rewards that has been previously studied for which incomplete expert demonstrations are available. Compared to other learning algorithms which work over the same information, ELE outperforms (in some cases, quite significantly) the other methods on a variety of sparse signal long-horizon tasks. A curated dataset of expert demonstrations on Nethack (subsampled from an existing dataset) is also planned to be released.",
            "strength_and_weaknesses": "* Strengths: \n   * The paper considers a highly relevant and interesting setting involving expert information, long timescales, and sparse reward signals.\n   * The paper proposes the interesting (and apparently novel) idea of estimating a temporal progress function (while controlling for removing trivial signals from which progress can be deduced), and finds that with some specific choices of how the progress function estimation is implemented and used as an auxillary reward, that it is quite beneficial. This seems like quite a nice simple principle to uncover if it was previously unknown. \n   * The paper provides extensive experiments and comparisons on their domain of choice (which seems to be a reasonable stand-in for any setting with incomplete expert demonstrations), and they examine performance on several tasks with uncertainty estimates for the performance of their algorithm beating existing algorithms.\n   * An open-source curated dataset will be released for the NetHack environment, which may prove useful for further work on long timescale with sparse reward and incomplete demonstration information. \n   * Some useful empirical choices in the estimation of the progress function were uncovered as a result of this work: shorter timescale prediction of progress performs better than not, and log scaling during the training of the progress function was also useful.\n   * A fairly thorough related work section is provided.\n\n* Weaknesses:\n   * It would be very useful to additionally examine a setting where the expert demonstrations are not incomplete; in such settings (still with long time horizons and sparse rewards), how does ELE fair against standard imitation learning algorithms like Behavior Cloning and DAgger (and more recent variants)? That would help disambiguate whether ELE is useful only in the context of missing demonstration information, or whether the idea is more broadly useful even when you have full expert demonstrations.\n   * It would be nice to provide a clearer justification and understanding (empirical or theoretical) of why estimating the progress function is helpful; likewise with the insight that shorter time horizons are more useful to train on. \n   * Only one underlying RL algorithm was tested (Muesli) unless I missed something -- it would be quite valuable to understand whether the underlying RL algorithm being used affects the utility of the augmented reward signal.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: Clarity was good overall, though there were a few points which could benefit from additional clarity: \n   * It would be better to define the setting of interest as long-horizon, sparse reward with limited expert demonstrations (\"action-free imitation learning\"), instead of just mentioning that the algorithm has the benefit of not requiring full expert demonstration info. The reason I propose this modification is because the paper appears to focus on and test exclusively this setting, rather than potentially broader applications of the idea of measuring progress.\n \n* Quality: The paper was of good quality.\n\n* Novelty: The main ideas in the paper appear to be novel. The related work section seemed good, but I am not extremely familiar with the literation on action-free imitation learning.\n\n* Reproducibility: The reproducibility is great - experiment details are provided and they will release the dataset.",
            "summary_of_the_review": "Overall, the paper seems interesting and clear. I would lean towards accepting the paper.\n\n====== POST-REBUTTAL =======\n\nHaving read the other reviews and responses, I remain convinced that the paper should be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4590/Reviewer_hMhc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4590/Reviewer_hMhc"
        ]
    }
]