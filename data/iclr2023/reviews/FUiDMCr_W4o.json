[
    {
        "id": "lAY7rH0RxQ",
        "original": null,
        "number": 1,
        "cdate": 1666722838826,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666722838826,
        "tmdate": 1666722838826,
        "tddate": null,
        "forum": "FUiDMCr_W4o",
        "replyto": "FUiDMCr_W4o",
        "invitation": "ICLR.cc/2023/Conference/Paper5163/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a statistical framework for personalized learned and estimation in a federated learning setting. The authors first explicitly state the modeling assumptions of how local model parameters are drawn from a global distribution of parameters. They then develop private and non-private estimation and learning algorithms. ",
            "strength_and_weaknesses": "Strengths: \n1) The modeling assumptions of how local model parameters are drawn from a global distribution of parameters seems quite reasonable, albeit a little restrictive. \n2) The proofs seemed correct. Although, I am not a theorist, so I wouldn't state this with a lot of certainty. \n3) The authors seemed to have given a lot of thought to the practical problems in federated learning setting and have tried to tackle a lot of them systematically. \n\nWeakness: \n1) The paper tries to do too much in my opinion, which makes it quite difficult to follow at times. \n2) The experiments seem sparse and handpicked, particularly for the personalized learning setting. Several questions that came to my mind are: what is the reason for picking m to be 50 and 66? Why didn't the authors try a range of values and reported all the results? \n3) Similarly, for the estimation task, I'd like to see more experiments. This can be done easily with different choices of synthetic parameters. \n4) Add more motivation for each estimation and learning setting. When are these problems relevant? Why should we care about them? This is not very clear from the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper quite difficult to read. ",
            "summary_of_the_review": "While I think the technical contributions of the paper are quite novel, it is very hard to read. This is in large part because the authors have tried to fit too many things into a single paper. For instance, have the authors considered writing this as 2 separate papers on personalized learning and estimation? Alternatively, it might be better suited as a long-form journal paper (say at JMLR or TMLR). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5163/Reviewer_MwhW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5163/Reviewer_MwhW"
        ]
    },
    {
        "id": "8gV7C5pmcn4",
        "original": null,
        "number": 2,
        "cdate": 1666809891808,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666809891808,
        "tmdate": 1666809891808,
        "tddate": null,
        "forum": "FUiDMCr_W4o",
        "replyto": "FUiDMCr_W4o",
        "invitation": "ICLR.cc/2023/Conference/Paper5163/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors developed a new Bayesian framework for personalized federated learning and derived theoretical bounds and novel algorithms for private personalized estimation. AdaPeD was proposed to use information divergence constraints along with adaptive weighting of local models and population models. AdaMix was proposed to adaptively weigh multiple global models and combines them with local data for personalization. Certain privacy analyses of new private personalized learning algorithms were done.",
            "strength_and_weaknesses": "I think this paper is strong. It studies an important yet remarkably understudied problem that can build the bridge between Bayesian learning and federated learning. It gives two types of algorithms: AdaPeD and AdaMix. The algorithm and the theoretical results are sound. The algorithm is fairly natural in Bayesian perspective, which is to update the Gaussian posterior based on the prior and likelihood. The AdaPeD uses a knowledge distillation regularization. It shows we can federatedly train the model via MAP well when preserving user-level privacy.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and easy to follow when including the appendix. I feel the paper is more suitable to a journal since a lot of important explanations are set aside in the appendix.\nIt is better to compare the upper bounds with the best bounds for the method without the privacy guarantee. By such a comparison, we can see the tightness of the present bounds roughly.",
            "summary_of_the_review": "The paper is clearly written, but the notations are heavy. The contribution is very technical from the perspective of Bayesian Federated Learning. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5163/Reviewer_3hXR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5163/Reviewer_3hXR"
        ]
    },
    {
        "id": "37JKlzGqRFf",
        "original": null,
        "number": 3,
        "cdate": 1666968183320,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666968183320,
        "tmdate": 1666968183320,
        "tddate": null,
        "forum": "FUiDMCr_W4o",
        "replyto": "FUiDMCr_W4o",
        "invitation": "ICLR.cc/2023/Conference/Paper5163/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, authors develop a statistical framework for personalized federated learning, in which they study different choices of population distribution. They provide theoretical bounds and novel algorithms for private personalized estimation, design, and conduct privacy analysis of new private personalized learning algorithms. ",
            "strength_and_weaknesses": "Strengths:\n* The paper is theoretically grounded. Authors provide theoretical results for different population models.\n* In contrast to many competitors, the paper considers privacy aspects of the problem and provides new algorithms with provable privacy guarantees\n\nWeaknesses:\n* \"Based on (Xi, Agg(q1, . . . , qm)), client i outputs an estimate \\hat{\\theta_i} of \\theta. \" Should be the estimate of \\theta_i ? Otherwise not clear what is \\theta as it is not defined before.\n* It seems that there is a missing opportunity to compare with another class of personalized models. Namely [1], when there is a split of shared and personalized parameters. Moreover, this model could be considered as a particular case of your statistical framework, when (for Gaussian population model) \\sigma_\\theta = [0,\\infty] where 0 corresponds to the shared (common) part of parameters, and \\sigma_x = 0. It is worth mentioning the paper in related work and comparing the results.\n* The choice of datasets for experiments seems a bit low-key. I suggest considering CIFAR100.\n\nMinor issues:\n* page 6 first appearance of FedAvg \u2014 no link\n*\"... given 6 election data we did 1-fold cross-validation. \" - what does 1-fold cross-validation mean? Did not you mean 6-fold cross-validation?\n\n\n[1] Collins L. et al. Exploiting shared representations for personalized federated learning //International Conference on Machine Learning. \u2013 PMLR, 2021. \u2013 \u0421. 2089-2099.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The ideas raised in the paper are original, and the paper is well-written and easy to read. ",
            "summary_of_the_review": "I am generally positive about the paper ideas, but there are some issues that are worth correcting (see above).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5163/Reviewer_zS9y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5163/Reviewer_zS9y"
        ]
    },
    {
        "id": "ExO9qPfcGO",
        "original": null,
        "number": 4,
        "cdate": 1667483279773,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667483279773,
        "tmdate": 1667483279773,
        "tddate": null,
        "forum": "FUiDMCr_W4o",
        "replyto": "FUiDMCr_W4o",
        "invitation": "ICLR.cc/2023/Conference/Paper5163/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes algorithms that search for suitable personalized models in a client-server type federated learning setup. The algorithms are inspired by the classical theory of parametric Bayesian risk minimization. In the personalized parameter estimation regime, the authors assume two population distributions: Gaussian and Bernoulli. Under Gaussian distribution, they report that in case the parent population becomes degenerate (i.e., variation tending to nullity), the global average turns out to be the \u2018best\u2019 estimator. Moreover, the posterior personalized mean estimator in this setup also turns out to be optimal in general. If the parent population follows a Bernoulli law, having sufficient observations from local sub-populations suggests against collaborating. The following \u2018personalization\u2019 algorithms utilize different prior distributions and regularization schemes to ensure client privacy.",
            "strength_and_weaknesses": "Strength:\n\nThe language of the article is lucid, and the presentation is also of good quality. The discussion leading up to the theoretical analyses and the algorithms is precise. I find the statistical analysis rigorous and very well represented. Prior works and relevant references are well-placed throughout the paper. \n\nWeakness/Issues:\n\nThe authors have altered the standard structure of the article, as guided by ICLR instructions. The abstract should not be full page-wide. This is a violation of the code and gives them an undue advantage over others. \n\nThe current article looks incomplete, lacking a \u2018Conclusion\u2019 section. Also, sufficient discussion regarding limitations and future work is missing.\n\nI suggest the authors present accompanying codes maintaining anonymity. \n\nIt would be very helpful if the problem statement is presented more precisely in the introduction. The authors provide a lot of references to prior work. However, amidst such a crowd, the motivation somehow fades. \n\nAs I have acknowledged, the discussion is quite rigorous. However, the is significant room for improvement when it comes to organization. \n\nThe empirical results seem insufficient, and I suggest the authors put more datasets to test if feasible.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents an overall well investigated research work. To enhance clarity the authors need to answer the following questions:\n\nQuestions:\n\nWhat does it mean statistically to \u201cestimate i through the help of the server.\u201d?\n\nIn the section \u2018Personalized Estimation\u2019, should it be \u201cclient i outputs an estimate i of i\u201d instead?\n\nDoes the prior distribution necessarily need to have a density in general? How \u2018realistic\u2019 are the special case assumptions of Gaussian and Bernoulli as global population distributions?\n\nIn the statement of Theorem 2., should it be \u201c[-r,r]d\u201d instead? \n\nDoes the notation \u0190 mean expectation [In Theorem 2, 3, etc.]? If so, kindly maintain any one symbol throughout. \nThe authors also use the same notation for a different purpose in Section B.2.1.\n\nShouldn\u2019t we have the sum also over the quantity on the right-hand side of equation (10) [Section 3.1]?\n\nThe quantity it may penalize the heterogeneity, but does not denote the variance. The authors should call it something else instead [Section 3.2, last paragraph].\n\nIs the strict imposition of the value \u20181\u2019 necessary in the definition of \u2018neighborhood\u2019 [Section A.3], since there is a clear possibility to generalize the result even if two datasets D and D' differ at multiple terms?\n\nIn Theorem 2, the upper bound on MSE  (5) loses its worth in a higher-dimensional setup. Can the authors talk about any remedy to the same?\n",
            "summary_of_the_review": "The paper may be considered for acceptance provided the authors address the above listed concerns. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5163/Reviewer_t7xg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5163/Reviewer_t7xg"
        ]
    }
]