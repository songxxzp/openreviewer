[
    {
        "id": "pJqxjNme9xx",
        "original": null,
        "number": 1,
        "cdate": 1666490944115,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666490944115,
        "tmdate": 1666490944115,
        "tddate": null,
        "forum": "CrfhZAsJDsZ",
        "replyto": "CrfhZAsJDsZ",
        "invitation": "ICLR.cc/2023/Conference/Paper969/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper provides rigorous analysis of operator learning of PDEs with discontinuities. The main contributions are two-fold. First, author prove that frameworks using linear reconstruction (DeepONet or PCA-Net) fail to efficiently capture the discontinuities by proving the lower bound of the approximation error decay merely linearly in terms of size of DeepONet. Second, the authors prove that two nonlinear reconstruction architectures, Shift-DeepONet and FNO, outperform the linear reconstruction architectures in the sense that much smaller model size is required. And the theoretical results are firmly supported by extensive numerical experiments.",
            "strength_and_weaknesses": "Strengths:\n1.This paper is the first theoretical paper investigating the operators learning of PDEs with discontinuities.\n2.The authors provide quantitative theoretical results and rigorous proof.\n3.The paper is well-organized and well-written.\n4.Extensive and supportive numerical experiments are provided.\n\nWeakness:\nNumerical examples do not illustrate the quantitative result, for example, the linear decay of model size of DeepONet, exponential decay rate of FNO and Shift-DeepONet.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality\nGood: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.\n\nClarity\nGood: The paper is well organized but the presentation has minor details that could be improved.\n\nNovelty\nGood: The paper makes non-trivial advances over the current state-of-the-art.\n\nReproducibility\nGood: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.\n",
            "summary_of_the_review": "This paper is the first paper rigorously investigate the error estimate of operator learning of PDEs with discontinuities. The authors carefully illustrate why linear reconstruction approaches fail to efficiently handle discontinuities, and how nonlinear reconstruction fix this theoretically and numerically. Additional numerical experiments that support the quantitative result (linear rate in DeepONet and exponential rate in FNO and Shift-DeepONet) are expected.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper969/Reviewer_vjUn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper969/Reviewer_vjUn"
        ]
    },
    {
        "id": "ZhlK4sNs_f",
        "original": null,
        "number": 2,
        "cdate": 1666518810101,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666518810101,
        "tmdate": 1666518810101,
        "tddate": null,
        "forum": "CrfhZAsJDsZ",
        "replyto": "CrfhZAsJDsZ",
        "invitation": "ICLR.cc/2023/Conference/Paper969/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries to investigate the operator learning of PDEs with discontinuous solutions. They prove that the linear method cannot approximate efficiently the solution operator. Furthermore, they propose a nonlinear reconstruction mechanism to overcome these fundamental difficulties.   And they propose new algorithms such as Fourier Neural Operators and a novel extension of DeepONet termed shift-DeepONet. Finally, they provide a rigorous theory for the empirical results. ",
            "strength_and_weaknesses": "Strength: The topic of learning the solution of PDE, especially the discontinuous solution, is very interesting.\n\nWeaknesses: (1) The results in all theorems, from Theorem 3.1 to Theorem 3.3,  look to learn the solution to the PDE at some time T. They \n                            do not tell the readers what kind of numerical method to get G_adv = u(\\cdot, T).\n\n                       (2) The result is about the solution at some T. Does the approximation depend on the time T? \n\n                       (3)  How does the dynamic approximation estimation evolve with time?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The problem looks very new, but there is not a very presentation. It looks like the problem formulation has not been presented clearly.  The dynamic process also has not been shown in the paper.  Whether it makes sense for the theoretical bound provided in this paper is unknown.",
            "summary_of_the_review": "Using the method of deep learning to solve PDE is a very interesting topic.   The basic numerical method for the nonlinear PDE is\n also unknown. Moreover, the motivation, the dynamic process and any kind of approximation still need to be presented clearly.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper969/Reviewer_koS7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper969/Reviewer_koS7"
        ]
    },
    {
        "id": "WMZiuz9pP6",
        "original": null,
        "number": 3,
        "cdate": 1666619378818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619378818,
        "tmdate": 1668802821271,
        "tddate": null,
        "forum": "CrfhZAsJDsZ",
        "replyto": "CrfhZAsJDsZ",
        "invitation": "ICLR.cc/2023/Conference/Paper969/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the problem of learning the solution operator of PDEs with discontinuous solutions (such as hyperbolic conservation laws) from initial data/solution pairs with initial data draw from a given measure.\nThe authors consider the DeepONet approach for operator representation that uses a sort-of finite sum over basis functions leveraging a \"branch-net\" for learning the coefficients and a \"trunk-net\" for learning the \"basis-functions\".\nBased on an existing lower bound on the expected error of DeepONet given the eigenvalues of the covariance of the push-forward measure obtained from the solution operator the authors motivate a variation of DeepONet called shift-DeepONet that through a shift-net enables learning shift relations between initial data and solution that are typical for hyperbolic conservation laws where shock discontinuities in the solution can shift as a result of perturbing the initial data.\nThe authors also consider Fourier neural operators (FNOs) as an alternative for better learning solution operators for discontinuous solutions because these also do not suffer from a similar dependence on the covariance spectrum of the push-forward.\nIn the theory section of the paper the authors prove that for a simple linear transport with initial data measure representing box-type initial data DeepONet size has to scale quadratically as error decays while their shift-DeepONet size only has to scale linearly and an FNO only has to scale logarithmically.\nFurthermore, for a simple nonlinear scalar conservation law (Burgers' equations) with sinus initial data and solution after shock-formation the authors prove that DeepONet has to scale linearly, while shift DeepONet and FNO only have to scale logarithmically.\nIn the experiments section the authors again demostrate that both shift-DeepOnet and FNO have better relative approximation error than DeepONet and some other alternatives for a linear advection, a Burgers, an 1D Euler (shock tube) and a 2D Euler (Rieman) problem.",
            "strength_and_weaknesses": "Strength:\n- Clear structure and motivation, original contribution of the shift idea to the DeepONet operator learning approach\n- High quality mathematical analysis of the given example problems\n- A comprehensive set of example PDEs / initial value instances for the experiments section\n\nWeakness:\n- Intuition of push forward covariance eigenvalue distribution does not become clear to someone who is not already familiar with what that looks like. Some explanation of what eigenvalues would look like\nfor a simple example of a solution operator for a smooth solution vs discontinuous solution would be helpful.\n- In the experiments sections it's not entirely clear why only 128 test examples were used to compute relative errors (that would still allow for relatively high error variance). A training set of 1024 example also seems quite small. Also adding standard defiation bars around the errors would be helpful.\n\nOther remarks:\n- Typo: \"In particular, this bound continuous to hold\" -> should be \"continues\" instead?\n- Typo: \"described in detailed\" -> \"described in detail\"\n- Typo: \"Riemman\" -> Riemann\n- I'm not sure if e.g. Theorem 3.2 should be a theorem or just a proposition as you're considering an example initial data measure and perhaps the generality is not as great as to warrant calling it a theorem. There are perhaps possibilities for achieving more generality by treating general piecewise constant solutions of scalar conservation laws (although I'm not sure having more generality is necessary for making the point that the paper makes).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written and follows a clear structure\n\nQuality: The paper presents a high quality mathematical analysis and uses a good experimental evaluation setup\n\nNovelty: The authors present a novel and original method and analysis for an interesting and important problem in operator learning\n\nReproducibility: The results of the paper should be generally reproducibility with the information provided in the paper and the extensive supplemental material",
            "summary_of_the_review": "I recommend to accept the paper and would ask the authors to try to make it more accessible by providing a better intuition on the issue of the eigenspectrum decay for the covariance of the push-forward with discontinuous solutions. I like the combination of proposing the new shift-parameterization of the model which seems well motivated, providing a theoretical analysis and a comprehensive set of experiments. I'm not sure whether the ICLR audience is the right audience for this sort of problem but the fourier neural operator is an ICLR paper so it could be okay. I also feel like although the theoretical analysis is high quality and I like the insight it provides it is not all that general since it still only pertains a set of examples and does treat any more general class of operators.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper969/Reviewer_ZZzU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper969/Reviewer_ZZzU"
        ]
    },
    {
        "id": "BEnua9oQ15",
        "original": null,
        "number": 4,
        "cdate": 1666649750823,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649750823,
        "tmdate": 1666649750823,
        "tddate": null,
        "forum": "CrfhZAsJDsZ",
        "replyto": "CrfhZAsJDsZ",
        "invitation": "ICLR.cc/2023/Conference/Paper969/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The main premise of the paper is that for hyperbolic conservation laws PDEs, operator learning frameworks like DeepOnets are inefficient as compared to counterparts like Neural Fourier Operators. This is because DeepOnets (and PCA-Nets) are linear reconstruction methodologies (i.e, the trunk and branch net outputs are linearly combined) which effects their representational capabilities, whereas FNO based methodology have a nonlinear reconstructions step. \n\nThe authors therefore extend the DeepOnet framework to a shift-deepOnet Framework with an extra shift and scale terms that are learned. \n\nThe proofs show that for one dimensional Linear Advection equation and inviscid Burgers equation, the size of the linear reconstruction methods scale at least quadratically in the reconstruction error whereas for the nonlinear reconstruction method size scales linearly only.",
            "strength_and_weaknesses": "The paper shows a useful results which delineates DeepONets are not as efficient as FNOs for approximating PDEs with discontinuous solution. \nThey introduce shift-DeepONet, a variant of deepOnet which performs as well as FNOs on pdes with discontinuous solutions. \nThe derive a lower bound (based similar proof techniques used in prior works) for the approximation error of DeepONet for advection equation and burgers equation (in one dimension) and show that FNO and shift DeepONet can upper bound the solution more efficiently. While I didn't go into the details of all the proofs, the overall sketch and techniques seem correct. \n\nThe authors back their theory with empirical results where they show that shift-DeepONet and FNO perform better than normal DeepOnet and resnets on PDEs like linear advection, burgers equation etc. \n\nOne small question i have is how would the upper bound change with increasing the dimension? A trivial way would be to treat each dimension independently, but I wonder if the authors have any other (more efficient way) that they have in mind?",
            "clarity,_quality,_novelty_and_reproducibility": "There are a few places where some extra explanation could help the reader,\n\n- Page 3, the authors mention the lower bound in equation 2.4 does not apply for shift DeepONet, why is that?\n- The proof sketches provided after the theorems and lemmas are a bit hard to follow.\n- For the lowerbound, i think the sketch pretty much follows the setup from Lanthanler et al.. While this fact is clearly mentioned in the appendix, it would be great that the authors could also point that out in the main paper.",
            "summary_of_the_review": "This paper theoretically and empirically shows that conditions under which DeepOnets are less efficient than methodologies like FNOs and also provides an extension of DeepONets that can make up for the difference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper969/Reviewer_sk7n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper969/Reviewer_sk7n"
        ]
    }
]