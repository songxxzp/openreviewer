[
    {
        "id": "5dFMKTtngPv",
        "original": null,
        "number": 1,
        "cdate": 1665820478791,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665820478791,
        "tmdate": 1669255458681,
        "tddate": null,
        "forum": "aBIpZvMdS56",
        "replyto": "aBIpZvMdS56",
        "invitation": "ICLR.cc/2023/Conference/Paper2739/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This is a theory-inspired empirical paper. The authors found that condition number (PL/Lipschitz) is an important parameter related to convergence and generalization, so they try to add condition number as the regularization term during training, and also use it to do pruning. It turns out that by doing that, the algorithm runs faster in training and generalizes better, compared with many baseline algorithms. \n\n",
            "strength_and_weaknesses": "This is a theory-inspired paper, so presumably the emprical ideas were originated from the theory community, instead of random hacking. It is hard for me to evaluate the experimental results as an expert, but it seems to me that all the results are pretty good, because they easily beat all the baselines. \n\nHowever, it is worth pointing out that \n1. The theory contribution of this paper is minimal. It seems to me that the main theory result is Theorem 6, which is a direct application of the definition of Pl condition and stability. \n2. Regularizing the condition number is difficult. So what they do is convert it to the minimal eigenvalue of NTK. But even minimal eigenvalue of NTK is also difficult to compute, so they further convert it to the norm of Hessian. But the norm of Hessian is still expensive to compute, so they further convert it to the trace of Hessian. Finally, the trace of Hessian can be efficiently approximated using random vector sampling. After so many steps of transformations, it is hard to justify that the empirical success is indeed for the theory of condition number. \n\nIt seems to me that this paper can be greatly improved, if the authors can show that adding regularization of the trace of Hessian can help optimization & generalization, because it seems to be what the model is actually doing. The theory of NTK relies on pretty strong assumptions on the learning rate and network width, so it is difficult for me to believe that the condition number is the actual reason for the model to work. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: reasonable. The paper is easy to follow. But I have the feeling that the authors intentionally hide the transformation from condition number to trace estimation, which may delivery misleading signals to the readers. \n\nQuality: I think this paper is an empirical paper, so it is difficult for me to evaluate the quality of the experimental results. It looks solid to me. \n\nNovelty: I think it is interesting to use the trace of Hessian as the regularizer, to improve training and generalization. \n\nReproducibility: the experiments look reproducible to me. ",
            "summary_of_the_review": "Since I think this paper is an empirical paper, and the connection between the trace estimation and condition number is weak, I currently give a weak reject. \n\nHowever, I am willing to change it to weak accept, if the authors, or other reviewers can convince me, that the regularization + pruning techniques along is a very good contribution to the community. \n\n-----After rebuttal ----\nAfter reading the rebuttal, I think: \n1. I still agree with Reviewer a5MJ that this is not a good theory paper. \n2. I think the authors demonstrated that their methods are useful emprically to the community. \n3. Other two reviewers seem to like the paper. \n4. The authors demonstrated that there exists some connections between trace estimation and condition number. \n\nSo I raise the score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2739/Reviewer_ng9v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2739/Reviewer_ng9v"
        ]
    },
    {
        "id": "VfIwe5olYZ",
        "original": null,
        "number": 2,
        "cdate": 1665871651441,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665871651441,
        "tmdate": 1668899023504,
        "tddate": null,
        "forum": "aBIpZvMdS56",
        "replyto": "aBIpZvMdS56",
        "invitation": "ICLR.cc/2023/Conference/Paper2739/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes adding a PL condition related regularization term onto the optimization loss function of over-parameterized deep neural networks. The intuition was that, by regularizing the inverse of the PL condition, the condition number was decreased, which leads to a faster convergence rate. This regularization was implemented by training a simple gating network that predicts a mask on the weights, so that after applying the mask the pruned network has a smaller condition number. Experiments on BERT, Switch-Transformer and VGG-16 were conducted, and shows that the proposed method results in better performance.",
            "strength_and_weaknesses": "Strength:\n\nThe idea of regularizing the PL condition (hence, the condition number) is novel and interesting. It is widely believed that the condition number directly controls the convergence speed, directly regularizing it should be helpful in optimization speed.\n\nIt is a fascinating idea to learn a mask to select the \u201cgood\u201d weights (or weight groups) such that the resulting pruned network has a low condition number. This method makes it feasible to regularize the PL condition. I have never seen such ideas in the literature, and I think it is a good addition to the machine learning community.\n\nThe experimental results on several commonly used networks show that the proposed method is effective in improving the network performance. \n\nWeaknesses:\n\nSeveral *important* aspects of the proposed method and its implementation are missing, which restrict the quality of this paper. Addressing them should improve the paper. I list them below.\n\n> The connection between the regularization term (condition number) $L_f/\\lambda_{min}$ and the trace of Hessian matrix $H_f$ is not clearly established. The authors refers to Prop 11 for the connection, however, there is no explicit relation between the two quantities presented in Prop 11. The only relation is $\\lambda_{min}/L_f \\ge \\lambda_0/L_f-2\\sqrt{n}R\\epsilon$. However, on the right hand side: the first term $\\lambda_0/L_f$ which is the major contribution of the two is not obviously related to the Hessian, noting that $\\lambda_0$ is the smallest eigenvalue of the NTK at initialization; the second term $-2\\sqrt{n}R\\epsilon$, which is negative, should have a much smaller magnitude than the first term, especially when the width is large. \n\n> The regularization term in Eq. (6) and (7) are evaluated over a large set $\\mathcal{W}$, typically large enough to cover the optimization trajectory, and in principle the trace of Hessian should be also taken minimization over $\\mathcal{W}$; however, the trace of Hessian is evaluated on a single point $w$, see Eq. (41). This is a mismatch, and there should be some theory to justify this. \n>>Note that, in most practical cases (including the experiments of this paper), the network is not super wide, and the Hessian matrices may be significantly different from each other within $\\mathcal{W}$. In these cases, a single point evaluation (in Eq.(41)) should not be enough.\n\n> Many important implementation details are missing. It is necessary to present them in the paper.\n\n>> How is the NTK computed for the neural networks: BERT, VGG-16 etc? As mentioned in Section 3.2.2, quantities of the NTK (e.g., $\\lambda_{min}$ and entropy Eq. (8)) are needed as input for the gating network. As far as I know, the NTK for these networks is very hard to compute. It is important to present the computation in the paper.\n\n>> How is the gradient $\\nabla_v L_{prune}$ computed? Especially, what is the expression of this gradient? It seems that the derivative goes through many complicated functions, for example, TopK, trace of Hessian. \n\n>> The most important of these is that: how are the parameter subsets $w_{[i]}$ partitioned for the neural networks, BERT, VGG-16, etc? Difference partition methods may largely affect the performance. \n\nI am also a bit suspicious about Figure 1(a). Condition numbers usually take very large values, typically greater than 1k. Moreover, in theory it should never be less than 1. However, the condition numbers in Figure 1(a) are quite small, even less than 1. I am confused about it, and hope the authors can give some explanation. \n\nThe paper only compares the convergence based on the number of epochs. As the proposed algorithm has an extra training module (training the gating network), I would like to see a comparison based on the wall-clock time also.\n\nDefinition 1 is actually the definition of interpolation, not over-parameterization. Although the two concepts often co-exist in many cases, they are not the same thing. A definition of over-parameterization has to be related to the number of model parameters.\n\nI\u2019d also like to know:\n> at which epochs are the VGG-16 pruned? (I might miss this details, but I did not find it).\n\n> performance of the proposed method on ResNets (e.g., ResNet-32, ResNet-50) with CIFAR-10. \n",
            "clarity,_quality,_novelty_and_reproducibility": "See the comments in the section of \"strength and weaknesses\".",
            "summary_of_the_review": "In summary, the paper proposes very novel and interesting ideas in regularizing PL (or condition number) to improve convergence speed, and in implementing the regularization. The weak point is that it lacks a lot of important implementation details and theoretical justifications of some claims. I tempororily put it as below threshold. But I would consider it as a good paper, as long as my concerns are addressed and more details are included.\n\n\n-------------After author feedback----------------------------------------------------\n\nThe authors provided very detailed feedback, which clarified many of the implementation details that was missing in the original submission. Most of my initial concerns are addressed, except this one: \n\n> \" The regularization term in Eq. (6) and (7) are evaluated over a large set $\\mathcal{W}$ ... however, the trace of Hessian is evaluated on a single point $w$, see Eq. (41). This is a mismatch, and there should be some theory to justify this.\"\n\nIn addition, the revision, especially Figure 3, provides a directly experimental evidence that the proposed method is successful in decreasing the condition number by prunning, which is align with the motivation of this work.\n\nGiven the above facts, I would like to raise my score accordingly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2739/Reviewer_nR2e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2739/Reviewer_nR2e"
        ]
    },
    {
        "id": "Z0ZJTIgMCsY",
        "original": null,
        "number": 3,
        "cdate": 1666452639286,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666452639286,
        "tmdate": 1669897300081,
        "tddate": null,
        "forum": "aBIpZvMdS56",
        "replyto": "aBIpZvMdS56",
        "invitation": "ICLR.cc/2023/Conference/Paper2739/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method for pruning overparameterized neural networks. The method is based on the intuition that the smaller condition number leads to better optimization and generalization. That is, the proposed method is designed so that the pruned neural network has a small condition number. The experiments verify the effectiveness of the method on training large-scale networks such as BERT, Switch-Transformer, and VGG-16.",
            "strength_and_weaknesses": "**Strengths**:\n\nThe idea of pruning the network to reduce the condition number is original and interesting. Moreover, the empirical performance seems good.\n\n**Weaknesses**:\n\n- My main concern is the correctness of the theory. Specifically, there is a conflict in assumptions. Specifically, PL-condition implies the quadratic growth condition as shown in [Karimi, et al, (2016)]. Hence, the objective function should be unbounded which contradicts the boundedness and Lipschitz continuity of the objective (assumed in Theorem 6). Some techniques to guarantee the boundedness of the parameters are probably needed.\n- The notation is inconsistent and confusing. Both notations $L(w)$ and $L_S(w)$ are used for empirical risk, although the authors say \"*we denote the loss function by L(x,y;w), or simply L(w), and also use L_s(w) to denote the empirical risk*\". For instance, the notation $L(w)$ in Definitions 4 and 5 and Theorem 6 should be the empirical risk. There is also the description \"*We denote $L$ and $L_S$ as the expected risk and empirical risk, respectively*\" in Section B.2.1. ",
            "clarity,_quality,_novelty_and_reproducibility": "There is much room for improvement in the quality and clarity of the paper. In particular, inconsistent notations as commented above should be fixed. \n\nThe proposed method itself is novel and the empirical performance is good.\n\nOther comments on the clarity and quality:\n\n- In the proof of sample complexity, the quadratic growth condition (27) between $w_1$ and $w_1^*$ is used, which is not true in general. Here, for condition (27) to hold, $w_1^*$ should be the projection of $w_1$ on the solution space.\n- The Lipschitz continuity is used for the gradient $\\nabla L_S(w)$ as well as $L_S(w)$ in the proof of exponential convergence, which should be clarified.\n- Assumption in Theorem 10 seems strong. In particular, my concern is the convergence to an optimal solution regardless of the dataset $S$. It would be better to provide an example for this assumption. Moreover, the statement of Theorem 10 is ambiguous. What is $\\epsilon_f$?",
            "summary_of_the_review": "Although the empirical performance of the proposed method is good, there are several concerns about the correctness, quality, and clarity. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2739/Reviewer_a5MJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2739/Reviewer_a5MJ"
        ]
    },
    {
        "id": "Kyvo-fHMdF",
        "original": null,
        "number": 4,
        "cdate": 1667000873444,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667000873444,
        "tmdate": 1667000873444,
        "tddate": null,
        "forum": "aBIpZvMdS56",
        "replyto": "aBIpZvMdS56",
        "invitation": "ICLR.cc/2023/Conference/Paper2739/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a novel method for regularizing over-parametrized models during optimization. The method is motivated by a theoretical result based on the condition number of the model. It improves the generalization performance empirically.  ",
            "strength_and_weaknesses": "Strength: \n- The motivation of the algorithm is well supported by a rigorous theoretical result. \n- The empirical results demonstrate the algorithm works quite well. \n\nExperimental weakness:\n- The paper shows the results of training until convergence for VGG-16 but not for BERT or Switch-Transformer.\n- The paper uses VGG-16 instead of more popular models in computer vision for the third experiment. Why?\n- Is it possible to refer some other papers (better if official) where the results of training BERT on WikiText-2 and other experiments are report? \n- Can we conclude the algorithm is more efficient than traditional optimization algorithms with the loop for pruning? It involves additional complexity. \n\nQuestion: \n- What will happen if we decrease the number of parameters in the model (such as BERT) gradually by tuning hidden dimension, for example? Does it achieve better performance as well. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well rewritten, and proposes novel algorithms and achieves better empirical results. The reproducibility issue is addressed in the above \"Strength And Weaknesses\" section.",
            "summary_of_the_review": "The paper proposes a novel algorithm based on a solid theoretical result. The algorithm achieves better generalization than baselines. However, there are several questions to be addressed in the experiment section. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2739/Reviewer_QD8B"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2739/Reviewer_QD8B"
        ]
    }
]