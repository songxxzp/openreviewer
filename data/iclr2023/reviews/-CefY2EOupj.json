[
    {
        "id": "NruMwPBHgI8",
        "original": null,
        "number": 1,
        "cdate": 1666481831303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666481831303,
        "tmdate": 1666481831303,
        "tddate": null,
        "forum": "-CefY2EOupj",
        "replyto": "-CefY2EOupj",
        "invitation": "ICLR.cc/2023/Conference/Paper1087/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "With the deep-learning revolution, large-scale training has been at the forefront of innovation. As the data sizes grow, the training time and cost explode. There have been techniques such as compression and bit quantization to alleviate these problems. These techniques work well in linear information but not in non-linear information. The paper proposes a new method called 0/1 Adam, leveraging insights within updates and stationary via steps. \n",
            "strength_and_weaknesses": "They provide convergence guarantees with theoretical analysis and experimental results comparing 1-bit Adam on language modeling training and image-net training showing a much faster reduction in training loss in both cases. The authors also open-sourced their algorithm. \n\nOne missing piece in this setup is the generalization, I think. I believe the experimental setup might be decent, and the problems are large enough to demonstrate efficacy. But I am not sure about the broader applicability to various other applications of AI, such as siamese networks (wide-and-deep models) or GANs, where the scale might come from a different angle than language models.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well. The experimental setup and parameter settings seem to be decent and explained thoroughly. I liked the discussion about policy, convergence speed, and quality analysis. Novelty is not impressive but decent and impactful enough. With the given implementation and parameter details, it seems that there is no reproducibility issue. ",
            "summary_of_the_review": "The paper focuses on an important problem to speed up model training. Though there are some minor limitations on generalization, I think it is good enough to be represented in the venue. I wanted to vote 7/10 (but only 6 or 8 are available). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1087/Reviewer_SjhX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1087/Reviewer_SjhX"
        ]
    },
    {
        "id": "5UZoYuB0nAu",
        "original": null,
        "number": 2,
        "cdate": 1666503541809,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666503541809,
        "tmdate": 1666676956202,
        "tddate": null,
        "forum": "-CefY2EOupj",
        "replyto": "-CefY2EOupj",
        "invitation": "ICLR.cc/2023/Conference/Paper1087/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces 0/1 Adam, a variant of the 1-bit Adam algorithm.  While the 1-bit Adam algorithm leveraged a fixed burn-in period wherein vanilla Adam was run before the variance term stabilized and was frozen, 0/1 Adam proposes an adaptive variance-freezing policy.  Additionally, 0/1 Adam leverages local SGD wherein synchronicity rounds among workers are skipped.  Convergence results similar to 1-bit Adam are provided for 0/1 Adam, and experimental results showing the efficacy of the algorithm for Imagenet and BERT are provided.",
            "strength_and_weaknesses": "Strengths\n-The paper extends 1-bit Adam in a natural and intuitive way.  Both adaptive variance-freezing and local SGD are promising directions to improve the original 1-bit Adam algorithm\n-The results are compelling\n\nWeaknesses\n-Presentation of results could be significantly improved (see clarity section for further information).\n\n-The writing and description of the novelty of the presented work requires extensive reevaluation.  The description of 0/1 Adam as a novel algorithm is not correct, it builds upon 1-bit Adam (also borrowing a large amount the analysis from the corresponding paper) and this point should be made clear throughout the paper (however, adaptive variance freezing is an important and novel contribution to the 1-bit Adam algorithm). Furthermore, 0/1 Adam is not the first work to consider local SGD for distributed algorithm.\n\n-Even after reading the paper, it is unclear how adaptive variance-freezing or local SGD (i.e., skipping rounds/local steps) may be set in practice.  Indeed, these two policies must be known \\emph{before} running Algorithm~\\ref{1}.\n\n-Claims are made without substantiation or are misleading.  E.g.:\n\n         1) \"the change of variance over steps in Adam is generally smooth\" <- Please substantiate this claim.  In the 1-bit Adam paper, they \n              assumed this was the case and stated so explicitly.\n\n         2) \"While this paradigm avoids compressing non-linear information with a one-time frozen variance, the experimental results from \n             (Tang et al., 2021) indicate the fullprecision stage still incurs non-trivial overhead.\" <- Overhead discussed in the paper was with \n              regards to vanilla Adam.  Please remove this claim or provide additional evidence to substantiate it\n\n         3) \"Furthermore, 1-bit Adam is restricted in the scope of gradient compression, and cannot be trivially adapted when other \n              techniques are used, such as local steps.\" <- In the 1-bit Adam paper, several variants were \n              implemented and contrasted against, including compressing the gradient and compressing the momentum.  Why can it not be \n              trivially adapted?  Furthermore, how to leverage local SGD with Adam is not an open problem, previous work in [1] has tackled \n              this. 1-bit Adam could trivially be combined with CADA.  Please reassess, and support, your claim.\n\n         4) \"Besides, the empirical success of (Tang et al., 2021) was not substantiated on generative models (GPT-style models), for instance, \n              175B GPT-3 Brown et al. (2020), 530B MT-NLG Smith et al. (2022), etc.\" <- This is disingenuous; 1-bit Adam was evaluated on \n              GANs (generative model) as well large-scale models (BERT).  530B MT-NLG Smith et al. was also \n              published after the 1-bit Adam paper, and neither GPT-3 no MT-NLG are evaluated using 0/1 Adam.  I would recommend revising \n              this claim, as well as elaborating why BERT is not a significant enough benchmark that GPT model evaluations are required.\n\nReferences:\n[1] Chen, Tianyi, et al. \"Cada: Communication-adaptive distributed adam.\"\nInternational Conference on Artificial Intelligence and\nStatistics. PMLR, 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nPresentation of results has significant room for improvement and requires more explanation.  For instance, it is unclear exactly what is being demonstrated in Figure 5: is \"w/o Skipping Round\" equivalent to w/o Local Steps?  What are 1.2x/1.3x denoting?  Similarly, in Figure 2 (a-c), what is being denoted by 1.6x, 2x, and 1.4x?  One suggestion is to describe each figure and the corresponding results after they are presented.  As it stands, Figures 2-5 all appear before a single figures' results are described in the text.\n\nQuality:\nWriting quality can be significantly improved.  Furthermore, comparing against Adam in all results would greatly improve the paper.  E.g., \"Since Tang et al. (2021) already demonstrated that 1-bit Adam has similar statistical results to Adam, we omit the comparison of end-to-end model accuracy to Adam for brevity.\" <- This is unreasonable; 1-bit Adam has been shown to provide near Adam accuracy, and so the comparison between the two may be ommitted.  However, the newly described algorithm (which is not equivalent to 1-bit Adam) has not been shown to achieve similar performance to Adam, and thus this must be demonstrated.  For the purposes of the paper, including Adam as a baseline in Figures 4 and 5 would not overcomplicate these results, but would significantly strengthen the paper.\n\nNovelty:\n0/1 Adam is an extension of the 1-bit Adam algorithm.  Additionally, much of the motivation and theoretical analysis strongly follow the 1-bit Adam paper.  The paper needs to do a better job of establishing that 0/1 Adam builds on this previous work, what exact theoretical results were presented in the 1-bit Adam algorithm and how the presented theoretical results stand as novel contributions, and exactly what are the novel contributions being made.  E.g., \"We propose 0/1 Adam, a novel optimization method that addresses the non-linearity challenges in Adam when applying aggressive 1-bit quantization and local steps\" <- disingenuous to call 0/1 Adam novel, but rather \"0/1 Adam builds upon the 1-bit Adam algorithm.\"  Furthermore, the use of local SGD/steps for Adam has been previously addressed by the Cada algorithm of Chen et al.\n\nReproducibility:\nThe 1-bit Adam has been open-sourced in a deep learning library by the authors.  However, the presented experiments are non-trivial, and providing experimental code to reproduce the featured results would greatly help other researchers to further build on their work.",
            "summary_of_the_review": "The paper makes some natural extensions to the 1-bit Adam algorithm, while providing convergence guarantees for the resulting 0/1 Adam algorithm.  Furthermore, the results seem compelling.  However, the presentation of the paper's novelty and contributions requires serious reevaluation.  Also, the general presentation and writing has significant room for improvement, particularly discussion of the presented results.  Important key details regarding figures are lacking or insufficient to understand exactly what is being stated.  Finally, while the extensions made from 1-bit Adam to produce 0/1 Adam make intuitive sense, it is unclear how exactly these are (or can be made) adaptive in practice, particularly given their predefined values in Algorithm 1.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1087/Reviewer_LNoo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1087/Reviewer_LNoo"
        ]
    },
    {
        "id": "uIV2hkflZAz",
        "original": null,
        "number": 3,
        "cdate": 1666548858949,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548858949,
        "tmdate": 1666548858949,
        "tddate": null,
        "forum": "-CefY2EOupj",
        "replyto": "-CefY2EOupj",
        "invitation": "ICLR.cc/2023/Conference/Paper1087/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed 0/1 Adam to reduce communication during training. 0/1 Adam addresses the existing issues by adaptively freezing variance and linearly approximating momentum and parameter update locally. The authors provide theoretical convergence guarantee and experimental results on large-scale training tasks, showing that the proposed optimizer can achieve similar accuracy compared to fp16 training at a higher throughput. ",
            "strength_and_weaknesses": "Strength:\n1. Firstly, the paper is generally well-written and easy to follow. \n2. The evaluation is comprehensive. The authors provided experimental results on language model training (transfer learning and zero-shot evolution) and ImageNet classification, where the proposed optimizer can match the performance of fp16 Adam. \n3. The proposed method achieves higher training throughputs across different benchmarks and hardware setups, which seems to be pretty general.\n4. The motivation of the method is \n\nWeakness:\n1. The adjustment of hyper-parameter T_v and T_u seems a bit non-trivial (especially the latter one). Can the author confirms that the same hyper-parameter can scale across model scales without careful tuning? For example, does the same hyper-parameter scale to training on the CIFAR dataset with the same learning schedule?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the paper are satisfying.  The method contains enough technical novelty to outperform existing work (1-bit Adam). The reproducibility should be good since the code is released and integrated into open-source repos. ",
            "summary_of_the_review": "Generally, I think this is a good paper. It contributes to the distributed training area by reducing communication to increase throughput. The method outperforms existing work by further reducing the high-precision training steps. My only concern is the difficulty of hyper-parameter adjustment. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1087/Reviewer_en2k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1087/Reviewer_en2k"
        ]
    }
]