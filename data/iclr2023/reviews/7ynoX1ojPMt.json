[
    {
        "id": "CKtjEGUp1c",
        "original": null,
        "number": 1,
        "cdate": 1666483878701,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666483878701,
        "tmdate": 1669763849358,
        "tddate": null,
        "forum": "7ynoX1ojPMt",
        "replyto": "7ynoX1ojPMt",
        "invitation": "ICLR.cc/2023/Conference/Paper708/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents OTOv2, an improved version of Only-Train-Once (OTOv1) framework. It introduces two major improvements over OTOv1, including automated zero-invariant groups (ZIGs) partition and a new optimizer called Dual Half-Space Projected Gradient\n(DHSPG). Experiments are conducted on several small image classification datasets and ImageNet. ",
            "strength_and_weaknesses": "Strength:\n1. Compared with the multi-stage model compression procedures, the proposed framework is simple and easy to use. \n2. OTOv2 shows better performances than previous methods on several small image classification datasets and ImageNet.  \n\nWeakness:\n1. Lack of ablation study experiments. It is unclear to me how many improvements the proposed DHSPG bring. Adding ablation study experiments on ImageNet with different model architectures and target sparsities will help resolve this concern. \n2. It is unclear whether the proposed method is effective on recent state-of-the-art models. For example, [1] shows that ResNet50 can achieve 80+ ImageNet top1 accuracy with an improved training setting. Adding additional results on these recent state-of-the-art models will help resolve this concern.  \n\n[1] ResNet strikes back: An improved training procedure in timm",
            "clarity,_quality,_novelty_and_reproducibility": "I think the writing of this manuscript can be improved. Regarding the Automatic ZIG partition, it seems like this part is for dependency tracking. What is the core technical contribution in this part? Regarding DHSPG, it is not clear to me why it works. Adding more discussions and illustrations may be helpful. ",
            "summary_of_the_review": "comments above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper708/Reviewer_eYjt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper708/Reviewer_eYjt"
        ]
    },
    {
        "id": "6K5vaLooaPk",
        "original": null,
        "number": 2,
        "cdate": 1666660640707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660640707,
        "tmdate": 1669747815138,
        "tddate": null,
        "forum": "7ynoX1ojPMt",
        "replyto": "7ynoX1ojPMt",
        "invitation": "ICLR.cc/2023/Conference/Paper708/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a set of methods for inducing group sparsity in DNN architecture. Their contributions build on an existing system Only-Train-Once (OTO) by automating the identification of groups that must be pruned together (referred to as zero-invariant groups) and improving on the optimization method, half-space projected gradient descent (HSPG),  by splitting the parameter groups into penalized and non-penalized sets and tuning per-group regularization coefficients.",
            "strength_and_weaknesses": "Strengths\n- The authors empirical evaluation considers many different tasks and many previously published works.\n- The automated detection of constraints that must be respected in order to maintain a valid DNN architecture after group pruning is impactful for lower the cost applying neuron-pruning methods to DNNs.\n\nWeaknesses\n- I found it hard to assess the results of experiments in section 4. For each experiment the authors include many prior results in tables. Each technique offers a different tradeoff in terms of accuracy lost to achieve a given level of compression. I believe it would be much easier to understand the results of these two-dimensional comparisons if they were shown in a scatter plot. If OTOv2 is more effective, I would expect it to represent the Pareto frontier of the solutions across a range of compression levels. I am not sure that the reported data is sufficient to demonstrate this. For example, the results in Table 4 show OTOv2 achieving a maximum top-1 accuracy of 75.4%, which is as much as 1% off the highest accuracy reported in the table. I think the experiments could be made to be more effective if the authors focused on thorough analysis of one or two benchmarks rather than incomplete analysis of many.",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty and technical depth of the work are both high in my opinion. I did however struggle to understand the paper at times. In particular, I had to re-read the first few sections of the paper repeatedly to grasp what a zero invariant group is. I also found it challenging to parse how the proposed DHSP technique improves over HSPG. I think the paper could be greatly improved if the authors emphasized and clarified these elements of the paper early on.\n\nIn places I found the writing to be sensational, which I think took away from the authors' work. For example, the use of \"revolutionary\" in the abstract, the sentence \"Consequently, in both academy and industry, compressing full DNNs into slimmer ones with negligible performance regression becomes ubiquitous.\", and the phrase \"\u201cguaranteeing ultimate sparsity\u201d.",
            "summary_of_the_review": "Overall, I would like to see the authors improve the clarity of the text and the comparisons between their technique and prior work. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper708/Reviewer_iQEU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper708/Reviewer_iQEU"
        ]
    },
    {
        "id": "2LFRkg1nM4",
        "original": null,
        "number": 3,
        "cdate": 1666665655450,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665655450,
        "tmdate": 1666665655450,
        "tddate": null,
        "forum": "7ynoX1ojPMt",
        "replyto": "7ynoX1ojPMt",
        "invitation": "ICLR.cc/2023/Conference/Paper708/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed OTOv2, which is a follow-up work of OTOv1. OTOv2 makes structured pruning more automatic, generic and user-friendly by addressing several problems in OTOv1. Specifically, they proposed an algorithm to automatically find Zero-Invariant Group for arbitrary DNNs and an optimizater to address the sparse optimization problem. Their empirical results is on par with other sota pruning methods, but requires less human efforts.",
            "strength_and_weaknesses": "Strength:\n- This paper built a model compression system to minimize the human effort put into weight pruning.\n- The technical details are well-explained.\n- The proposed DHSPG algorithm can satisfy different sparsity requirement without finetuning.\n- The empirical results are better or on par with sota pruning works.\n\nWeakness:\n- The experiments are still limited to popular standard networks including VGG/ResNet/DenseNet, which have been already very well-studied and known to have large redundancy. To support the claim that the proposed system can work on arbitrary DNN, it would be interesting to see how it does on modern architectures like RegNet/Swin/ConvNeXt.\n- It would be great to show some training speed benchmarks.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is well-motivated and makes solid technical contributions.",
            "summary_of_the_review": "In conclusion, this paper designs a model pruning system OTOv2. OTOv2 can do ZIG partition automatically for arbitrary neural architectures, optimize the sparse model using a novel optimization algorithm DHSPG and produce the pruned model without finetuning. In my opinion, this work makes clear contribution to the model compression community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper708/Reviewer_F5Jh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper708/Reviewer_F5Jh"
        ]
    }
]