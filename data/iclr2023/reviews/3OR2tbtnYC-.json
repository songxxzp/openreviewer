[
    {
        "id": "j_NFWKxf5b",
        "original": null,
        "number": 1,
        "cdate": 1666685303575,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685303575,
        "tmdate": 1669252759938,
        "tddate": null,
        "forum": "3OR2tbtnYC-",
        "replyto": "3OR2tbtnYC-",
        "invitation": "ICLR.cc/2023/Conference/Paper2043/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is concerned with near-optimal policy identification in the generative model setting. The contributions of this paper includes:\n\n(1) The development of the AE-LSVI algorithm for $\\epsilon$-optimal identification;\n(2) A concrete proposal of the algorithm based on the RKHS model;\n(3) Establishment of the theoretical findings to justify the proposed algorithm;\n(4) Empirical verification of the proposed algorithm. ",
            "strength_and_weaknesses": "The contributions and strengths of paper are listed above. Below, I list some points to discuss: \n\n1. Page 3. It was mentioned that the goal in this setting is to identify an $\\epsilon$-optimal policy while minimizing the number of necessary episodes $T$. This differs from the standard setting on cumulative regret minimization. The paper would benefit from adding more motivations to justify such an objective. Is this objective commonly used in the generative model setting?\n\n2. Under the generative model setting, the agent has access to a simulator that can be used for adaptive data generation. The optimal policy that maximizes the cumulative reward in simulators is not necessarily maximizing that in real applications, due to potential difference from simulator and real-world environments. Would it be better to consider the more realistic setting with sim-to-real gap?\n\n3. Instead of obtaining $s_h^t$ according to Equation (12), we can alternatively define some uncertainty measure for the value and directly selects $s_h^t$ that minimizes the uncertainty. Would this procedure work?\n\n4. While kernel methods enjoy nice theoretical properties, their empirical implementation requires tuning of the kernel bandwidth. What kernel functions did you choose in the numerical experiments? Are the results sensitive to the choice of the bandwidth? How would you suggest practioners to select this hyper-parameter? \n\n5. In continuous action setting, the author(s) proposed to first discretize the action space and then apply the proposed method. Can your methodology be extended to the continuous action setting? Alternatively, can we adaptively discretize the action space to improve the performance (see e.g., http://proceedings.mlr.press/v80/lee18b/lee18b.pdf; https://arxiv.org/pdf/2007.00717.pdf; https://openreview.net/pdf?id=rvKD3iqtBdk)? \n\n6. The current methodology and theory are developed under the episodic MDP setting with time-varying dynamics. Can these results be extended to time-homogeneous MDP models?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and is of good quality. The proposed algorithms and the associated theoretical analysis are novel, to our knowledge. The implementation details are provided in the Appendix. Code is also provided in the supplementary materials. ",
            "summary_of_the_review": "The paper is well-written and studies an interesting research question. The proposed algorithm and the theoretical analysis are novel. I have some comments regarding the practical usage of the proposed method and the model setup. I hope the author(s) can address my concerns. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2043/Reviewer_Zsrd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2043/Reviewer_Zsrd"
        ]
    },
    {
        "id": "Dbxtl5Pc_g",
        "original": null,
        "number": 2,
        "cdate": 1666983090344,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666983090344,
        "tmdate": 1666983090344,
        "tddate": null,
        "forum": "3OR2tbtnYC-",
        "replyto": "3OR2tbtnYC-",
        "invitation": "ICLR.cc/2023/Conference/Paper2043/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a policy search algorithm that combines kernelized LSTD with the \"optimism in the face of uncertainty\" approach, which is a well-known method in bandit algorithms. The paper analyzes the theoretical properties of the resulting algorithm and reports results on a number of simulated control environments.",
            "strength_and_weaknesses": "Strengths:\n   - Despite being a combination of multi-decade-old ideas, the eventual algorithm is simple, solid, and novel.\n   - The theoretical results are thorough and sufficiently informative, while being a corollary of existing work.\n   - The experiments are comprehensive enough and the reported results support the central claim of the paper.\n\nWeaknesses:\n   - The baselines chosen for comparison are rather arbitrary and not state-of-the-art.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a very clear language. This is a very high quality work with prominent novelty. The reported results are reproducible.",
            "summary_of_the_review": "Overall very solid work, which neatly combines the theory and practice of online policy search.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2043/Reviewer_maWK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2043/Reviewer_maWK"
        ]
    },
    {
        "id": "xfqhxHY01Wg",
        "original": null,
        "number": 3,
        "cdate": 1667560630165,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667560630165,
        "tmdate": 1668415584190,
        "tddate": null,
        "forum": "3OR2tbtnYC-",
        "replyto": "3OR2tbtnYC-",
        "invitation": "ICLR.cc/2023/Conference/Paper2043/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a novel algorithm, based on LSVI to perform best-policy identification uniformly across the state space. The method is based on a RHKS assumption on the reward and value functions and performs targeted 'exploration' based on the performance gaps defined as the difference between upper and lower bounds of the action value function. The authors study theoretically the performance of the algorithm in function of $\\Gamma_k$, a property of the specific kernels used and give specific bounds for some families of frequently used kernels. Moreover, the authors perform an empirical evaluation on several domains, comparing AE-LSVI with algorithms from SOTA, explicitly showing the conditions under which this method outperforms the baselines.",
            "strength_and_weaknesses": "Strengths:\n - The authors study the interesting problem of best policy identification in episodic reinforcement learning\n - The authors propose a fairly simple algorithm, easy to implement and with performance guarantees\n - The sample complexity bound does not scale with the state and action space sizes\n - The authors perform a thorough empirical evaluation explicitly identifying the limitations of the algorithms and the settings in which it performs better than the baselines\n\nWeaknesses:\n- The central point of the algorithm is choosing states to sample from he generative model based on the perfomance gap, i.e. the difference between the upper and lower bounds on the V-function of the state. The authors should  cite other works that performs selection on similar metrics based on this uncertainty. For example, there is a large body of work in the best-arm identification literature that uses the difference between upper and lower bounds to select which arm to choose, [1], [2] etc.\n- The paper does use a fairly common assumption from the literature, but nonetheless it should be more self contained. I would have expected to see a discussion on how restrictive is the RHKS assumption as well as a comparison of the guarantees provided non only with methods taking the same assumption but also with other method that make less or more limiting assumptions. For example, a comparison with methods that assume linearity of the reward and value functions would improve the clarity of the paper. \n- A bit more discussion on why the bound presented loses the dependency on the state and action space sizes might also improve the paper.  Especially when comparing with the algorithm presented in Cher et. al. 2019 at the end of Section 4 which also makes the RHKS assumption, has a dependency on $\\Gamma_k$ but also on the state and action spaces.\n- The algorithms adds an additional parameter $\\beta_t$, fairly important since it is used to define the upper and lower bound on Q-values, central for the \"exploration\" method used in AE-LSVI. Unfortunately, no discussion on the selection of this hyperparameter is provided. The authors present all the results with the value 0.5 (why 0.5). Instead, an empirical evaluation of the effect off the value of $\\beta_t$ is warranted.\n- While I am mostly positive on the experimental section, I am surprised with the choice of DDQN as a baseline. DDQN was chosen as a SOTA method for online RL, but there are more advanced methods right now for online RL, especially it would have been beneficial to see other online RL methods that use uncertainty estimates just like AE-LSVI like [3] [4] or [5]. Any of them would make a better baseline then DDQN but especially [3] since it makes decisions also based on upper and lower bounds on Q.\n\n[1] Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, S\u00e9bastien Bubeck, Multi-Bandit Best Arm Identification, NeurIPS 2022\n\n[2] Marta Soare, Alessandro Lazaric, R\u00e9mi Munos, Best-Arm Identification in Linear Bandits, NeurIPS 2014\n\n[3] Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, Michael Arbel, Michael I. Jordan, Tactical Optimism and Pessimism for Deep Reinforcement Learning\n\n[4] Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, Deep Exploration via Bootstrapped DQN\n\n[5] Alberto Maria Metelli, Amarildo Likmeta, Marcello Restelli, Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. I would suggest to rewrite the title to contain \"Best policy identification\" instead of just \"policy identification\" as the former might be confused with \"policy space identification\" another branch of RL concerned with identifying the minimal space of policies containing the optimal (near-optimal) ones.\n\nThe related works is solid, it's almost exhaustive and clearly positions the paper in the literature. The only comments around it would be to include some previous work on best-arm identification (not only classical bandits) as it is fairly close to the setting considered in this paper (see also previous section of the review). Moreover I would suggest to the authors to consider moving this section after the preliminaries or even after the section 4, as the related works also compare AE-LSVI with other algorithms from the literature, and without reading section 2, 3 and 4 it might be hard for the reader to appreciate the differences.\n\nThe paper is fairly novel, it uses standard assumptions from the literature (RHKS) and similar ideas from the other research areas to define and analyze a novel algorithm.\n\n",
            "summary_of_the_review": "Overall I am positive about the paper. The issues I describe all all fairly minor and solvable. I would increase my score if my concerns are adequately addressed. (See previous sections for details)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2043/Reviewer_v5Ka"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2043/Reviewer_v5Ka"
        ]
    }
]