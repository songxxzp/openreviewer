[
    {
        "id": "hEWs3UAt9C",
        "original": null,
        "number": 1,
        "cdate": 1666529715348,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529715348,
        "tmdate": 1666529715348,
        "tddate": null,
        "forum": "QUaDoIdgo0",
        "replyto": "QUaDoIdgo0",
        "invitation": "ICLR.cc/2023/Conference/Paper1639/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a method for self-supervised representation learning of LIDAR point clouds, to initialise training of standard supervised methods for LIDAR 3D object detection and point cloud segmentation. The paper has two main contributions:\n\nThe self-supervised representation is learnt via contrastive learning on point clouds from LIDAR mounted on vehicles and LIDAR mounted on infrastructure, synchronised with respect to each other. This provides more diversity than state-of-the-art methods, that use ad-hoc augmentations or LIDAR point clouds collected by the same vehicle but at different timestamps (which is problematic due to moving objects)\nA shape-based loss as part of the unsupervised learning step that is meant to bring in task-specific information into this step\nThe paper is evaluated on standard 3D object detection and LIDAR semantic segmentation benchmarks, showing marginal (but consistent) improvements over alternative self-supervised initialisation methods.",
            "strength_and_weaknesses": "Strengths:\n\nThe core ideas in the paper are interesting. I liked the concept of introducing a more task-relevant loss based on shape in the self-supervised stage\nThe evaluation procedure is mostly thorough and sound, using several public benchmarks, comparing against relevant related work, and with appropriate ablative analysis. Results are consistent across datasets, which is convincing (see however my concern under weaknesses on the magnitude of the deltas)\nThe paper is written with a level of clarity that is significantly above average (I commend the authors for this, it makes reviewing so much easier!)\nWeaknesses:\n\nIn my opinion, the main weakness is the magnitude of the improvements. I am looking for example at Table 2, where the difference between the various methods in <0.5 mAP. It would be valuable to have some sort of estimate of the variance, for example, how much does performance change upon different runs of training (e.g. with different random initialisation) and (less important) across the values of hyperparameters (e.g. the temperature, the shape descriptor radii) . However, it has to be said that, while small, the improvement can be observed consistently across various datasets, tasks, and networks used for supervised learning\nThe method to generate data for self-supervised training seems hard to scale, as it seems quite expensive and operationally hard to get to work (collaborative infra and vehicles), which might be the reason why the cooperation dataset is fairly small. Related to this, could the authors comment on the sensitivity of the method to the accuracy of synchronisation and relative point estimation between the vehicle and the infra point cloud? How is the relative pose computed?\nThe paper needs a more in depth overview of related work on two fronts: use of task-relevant losses as part of the self-supervised learning, and use of shape descriptors like the one in Sec. 3.3 in the context of learning from point clouds.\nMinor details for the authors for improvements in the next version of the paper:\n\nI would recommend to bring Table 7 from the Appendix into the main paper\nIt would be useful to add some rationale or ablative analysis on the choice of some parameters (e.g. the radii of the shape descriptors (Sec. 3.3)\nIt is hard to understand/see the negative and positive pairs in Fig. 3\nThe math notation is a bit cumbersome. For example, it took me a while to understand the v/f notation\nI have some questions on invariance to rotations: is the shape descriptor rotationally invariant? If yes, how does it work? Or it does not matter because the infra and vehicle point clouds are aligned first? If the point clouds are aligned, how do methods learn about rotational invariance: are there such kind of augmentations in the supervised learning stage?\nSome statements are fairly strong, such as: \"This stems from the sparsity of LiDAR point clouds, which sometimes make it difficult to find good positive pairs to perform contrastive learning\". Unless there is some strong evidence backing this claim, I would recommend it softening, e.g. \"We believe this stems from the sparsity...\"",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality: As mentioned above, the clarity and the quality of the paper are high\n\nNovelty: I believe the amount of novelty is sufficient for publication, if we consider the combination of using a new source of data for unsupervised representation learning for LIDAR point cloud and the use of a task-relevant loss based on shape\n\nReproducibility: Sufficient implementation details are provided. However, the paper uses quite a number of hyperparameters (e.g. temperature, shape descriptor radii, etc.), and it is unclear how sensitive the method is to the values of these hyperparameters",
            "summary_of_the_review": "Overall, I recommend accepting the paper, and I'd like to invite the authors to address the points I listed in the weaknesses section.\n\nThere is enough novelty in the contributions (using a new source of point cloud data for self-supervised representation learning for point clouds and use of task-relevant losses based on shape during self-supervised learning). There is also enough signal showing the usefulness of the method. While the gains compared to previous methods are overall small, they are very consistent across datasets, supervised tasks, and supervised methods. Some additional analysis on the sensitivity to multiple runs (to get an estimate of the variance) would make the case stronger. The paper is clearly explained, with enough details for reproducibiltiy.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1639/Reviewer_bVKL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1639/Reviewer_bVKL"
        ]
    },
    {
        "id": "uKpFkm8jtV",
        "original": null,
        "number": 2,
        "cdate": 1666615411537,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666615411537,
        "tmdate": 1666615411537,
        "tddate": null,
        "forum": "QUaDoIdgo0",
        "replyto": "QUaDoIdgo0",
        "invitation": "ICLR.cc/2023/Conference/Paper1639/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose CO3,\u00a0Cooperative\u00a0Contrastive Learning, and\u00a0Contextual Shape Prediction for unsupervised 3D representation learning in outdoor scenes. By using DAIR-V2X, a vehicle-infrastructure-cooperation dataset, the data mitigates challenges due to the moving objects that would make it hard to find correct correspondence for contrastive learning. The experiments demonstrate that the representation learned by CO3 can be transferred to various architectures and different downstream datasets and tasks to achieve performance gain.",
            "strength_and_weaknesses": "Strength\n\n1. As we know, it is non-trivial to annotate point cloud data in scale. Unsupervised 3D representation learning emerges. The paper proposes an interesting approach that leverages point cloud data collected in a V2X setting. The setting mitigates challenges due to the moving objects that would make it hard to find correct correspondence for contrastive learning.\n2. Empirically, the representation learned by the proposed\u00a0**Co**operative\u00a0**Co**ntrastive Learning and\u00a0**Co**ntextual Shape Prediction (CO3) can be transferred to different outdoor scene datasets collected by different LiDAR sensors. Specifically, they show favorable performance compared with STRL (Huang et al., ICCV 2021) on the KITTI and nuScene datasets on LiDR 3D detection and semantic segmentation, respectively.\n\nWeaknesses\n\n1. LiDAR is indeed a valuable sensor for perception. However, it is too strong to claim that LiDAR is the most reliable sensor in outdoor environments. LiDAR has many issues, e.g., sparsity, visibility under occlusion, and false alarm due to smoke and fog. Please consider rephrasing the first paragraph.\n2. The key question the authors aim to answer is: how to learn 3D representation from unlabelled point cloud data for outdoor scenes? The authors argue that existing approaches are not suitable for outdoor scenes. However, they ignore a recent advance in Freespace forecasting (Hu et al., CVPR 2021). The work could be an alternative to be compared with the proposed one. Using point clouds from both the vehicle and infrastructure sides is okay because we want data captured simultaneously. Thus, views share adequate semantics. However, the motivation is not convincing by arguing the limitations of existing self-supervised learning for point cloud data without discussing Hu et al., CVPR 2021.\n- Hu et al., Safe Local Motion Planning with Self-Supervised Freespace Forecasting, CVPR 2021\n3. The proposed approach's novelty is limited: This paper's two ideas are cooperative contrastive objective and contextual shape completion. The cooperative contrastive objective is the contrastive loss proposed by He et al., 2020. The only change is the input features. It seems that the contribution (cooperative contrastive objective) is over-claimed. The enabler of the proposed cooperative contrastive objective is the DAIR-V2X dataset. As mentioned by the authors, many existing approaches for shape context modeling exist. Why Eq. (5) is a better choice? Without a clear justification, we cannot conclude the importance of Eq. (5).",
            "clarity,_quality,_novelty_and_reproducibility": "The work proposes to leverage a vehicle-infrastructure-cooperation dataset that mitigates challenges due to the moving objects that would make it hard to find correct correspondence for contrastive learning. However, two major concerns are found. First, the authors did not discuss recent work by Hu et al., CVPR 2021, that aims to learn 3D point cloud representation directly from point cloud data with moving objects by formulating a freespace forecasting task. Second, the novelty of the proposed cooperative contrastive objective and contextual shape completion is questionable. ",
            "summary_of_the_review": "The reviewer thinks it is interesting to leverage DAIR-V2X for unsupervised 3D representation learning. However, significant concerns are found and discussed in the Weaknesses section. The reviewer would like to know the feedback on the questions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1639/Reviewer_1HxB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1639/Reviewer_1HxB"
        ]
    },
    {
        "id": "V7RTUQu8zPp",
        "original": null,
        "number": 3,
        "cdate": 1666664394865,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664394865,
        "tmdate": 1666664394865,
        "tddate": null,
        "forum": "QUaDoIdgo0",
        "replyto": "QUaDoIdgo0",
        "invitation": "ICLR.cc/2023/Conference/Paper1639/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes Cooperative Contrastive Learning and Contextual Shape Prediction (CO3). It features 1) using vehicle-infrastructure-cooperation dataset to obtain proper views for unsupervised contrastive learning; 2) a shape pretext for enforcing the learning to be task-relevant; and 3) the learned representation can be generalized to other autonomous driving datasets. \n\nThe proposed CO3 has been evaluated over the ONCE, KITTI and nuScenes datasets. Results show that it is effective in boosting detection performance.  \n",
            "strength_and_weaknesses": "\nStrength\n1. The paper is well motivated and clearly explained. \n2. The idea of using vehicle-infrastructure-cooperation dataset for unsupervised contrastive learning is interesting.\n3. Experiments are extensive. Results on semantic segmentation are good.\n\nWeakness\n1. The overall improvement by the proposed CO3 is marginal for object detection. For example, the PV-RCNN results on the KITTI and ONCE datasets. If we have a larger supervised training set, will unsupervised contrastive learning still be helpful? \n2. In ablation study Table 6, the results on KITTI need more explanation. What is the reason that using either of proposed contextual shape prediction and cooperative contrastive alone gives worse or marginally better performance than the random baseline but the combined method CO3 is better than the baseline?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the paper are both good. \nThe paper also has novelty by using vehicle-infrastructure dataset for contrastive learning.\nSince the experiments are based on published algorithms and datasets, this work should be reproducible. ",
            "summary_of_the_review": "I like the idea of using vehicle-infrastructure dataset for contrastive learning. But I have some concerns that the improvement on object detection is not significant. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1639/Reviewer_BVa4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1639/Reviewer_BVa4"
        ]
    },
    {
        "id": "1VhJjKt6ie",
        "original": null,
        "number": 4,
        "cdate": 1666787622729,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666787622729,
        "tmdate": 1670929276369,
        "tddate": null,
        "forum": "QUaDoIdgo0",
        "replyto": "QUaDoIdgo0",
        "invitation": "ICLR.cc/2023/Conference/Paper1639/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on self-supervised point cloud representation learning. The authors combine the contrastive loss with KL divergence between the predicted feature and manually crafted 3D shape context. \n\nTo construct better views for contrastive loss,  the authors utilize a recently proposed dataset where point clouds are captured from both vehicle sensors and infrastructure sensors. \n\nTo demonstrate the effectiveness of the proposed methods, the authors finetuned the pre-trained model on  3D object detection and segmentation tasks and have achieved better performance than other self-supervised learning methods or even supervised pre-trained models.",
            "strength_and_weaknesses": "Strength\n+ The motivation of construct views from infrastructure sensors and vehicle sensors is well introduced.\n+ The performance gain is significant on various downstream benchmarks.\n+ The evaluation is extensive.\n\nWeakness:\n+ The paper converts the view construction of outdoor scene to a case similar to indoor by using the new dataset. So the proposed solution should also works for indoor, but which is not demonstrated in the paper.\n+ The novelty and the insight of the method itself is limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The novelty of the method itself is limited.\nHowever, the reproducibility is high since most of the components are off-the-shelf.",
            "summary_of_the_review": "Although the performance gain is significant, I prefer to reject the paper because it resolves the (positive) view construction of 3D point clouds by using an existing dataset, and the novelty and insights of the method itself are insufficient.\n\n\n[post-rebuttal] I appreciate the efforts the authors did on proving the effectiveness of the proposed method on indoor data. Although I maintain my original opinion that the paper is more about construct data on a very specific dataset and is less inspiring for further researches in general case, I updated the rating due to the additional effort the authors have made.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1639/Reviewer_XpxU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1639/Reviewer_XpxU"
        ]
    }
]