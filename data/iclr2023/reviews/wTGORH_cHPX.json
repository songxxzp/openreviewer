[
    {
        "id": "Jq_LNsq_bpb",
        "original": null,
        "number": 1,
        "cdate": 1666503972589,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666503972589,
        "tmdate": 1666503972589,
        "tddate": null,
        "forum": "wTGORH_cHPX",
        "replyto": "wTGORH_cHPX",
        "invitation": "ICLR.cc/2023/Conference/Paper4117/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose the Deep Graph Neural Diffusion method, which ensures the over-smoothing issue and guarantees stability of the model in theoretical. The experimental results also show the improvement of the proposed method compared with the baseline methods. ",
            "strength_and_weaknesses": "Strength:\n1.\tConvincible theoretical analysis on the diffusion process, the over-smoothing, and the bound of the feature.\n2.\tFully designed experiments show the performance of the proposed method systematically.\n3.\tWell organized paper and logical writing. \n\nWeaknesses:\n1.\tLack of the explanation about the importance and the necessity to design deep GNN models . In this paper, the author tries to address the issue of over-smoothing and build deeper GNN models. However, there is no explanation about why should we build a deep GNN model. For CNN, it could be built for thousands of layers with significant improvement of the performance. While for GNN, the performance decreases with the increase of the depth (shown in Figure 1). Since the deeper GNN model does not show the significant improvement and consumes more computational resource, the reviewer wonders the explanation of the importance and the necessity to design deep models.\n2.\tThe experimental results are not significantly improved compared with GRAND. For example, GRAND++-l on Cora with T=128 in Table 1, on Computers with T=16,32 in Table 2. Since the author claims that GRAND suffers from the over-smoothing issue while DeepGRAND significantly mitigates such issue, how to explain the differences between the theoretical and practical results, why GRAND performs better when T is larger? Besides, in Table 3, DeepGRAND could not achieve the best performance with 1/2 labeled on Citeseer, Pubmed, Computers and CoauthorCS dataset, which could not support the argument that DeepGRAND is more resilient under limited labeled training data.\n3.\tInsufficient ablation study on \\alpha. \\alpha is only set to 1e-4, 1e-1, 5e-1 in section 5.4 with a large gap between 1e-4 and 1e-1. The author is recommended to provide more values of \\alpha, at least 1e-2 and 1e-3.\n4.\tMinor issues. The x label of Figure 2, Depth (T) rather than Time (T).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, general quality and novelty is good.",
            "summary_of_the_review": "This paper provides convincible theoretical analysis on the diffusion process, the over-smoothing, and the bound of the feature. However, the reviewer still has some concerns, listed below.\n1.\tLack of the explanation about the importance and the necessity to design deep GNN models . In this paper, the author tries to address the issue of over-smoothing and build deeper GNN models. However, there is no explanation about why should we build a deep GNN model. For CNN, it could be built for thousands of layers with significant improvement of the performance. While for GNN, the performance decreases with the increase of the depth (shown in Figure 1). Since the deeper GNN model does not show the significant improvement and consumes more computational resource, the reviewer wonders the explanation of the importance and the necessity to design deep models.\n2.\tThe experimental results are not significantly improved compared with GRAND. For example, GRAND++-l on Cora with T=128 in Table 1, on Computers with T=16,32 in Table 2. Since the author claims that GRAND suffers from the over-smoothing issue while DeepGRAND significantly mitigates such issue, how to explain the differences between the theoretical and practical results, why GRAND performs better when T is larger? Besides, in Table 3, DeepGRAND could not achieve the best performance with 1/2 labeled on Citeseer, Pubmed, Computers and CoauthorCS dataset, which could not support the argument that DeepGRAND is more resilient under limited labeled training data.\n3.\tInsufficient ablation study on \\alpha. \\alpha is only set to 1e-4, 1e-1, 5e-1 in section 5.4 with a large gap between 1e-4 and 1e-1. The author is recommended to provide more values of \\alpha, at least 1e-2 and 1e-3, etc.\n4.\tMinor issue. The x label of Figure 2, Depth (T) rather than Time (T).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4117/Reviewer_3KbW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4117/Reviewer_3KbW"
        ]
    },
    {
        "id": "KCn2wwZktg",
        "original": null,
        "number": 2,
        "cdate": 1666715615816,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666715615816,
        "tmdate": 1666715615816,
        "tddate": null,
        "forum": "wTGORH_cHPX",
        "replyto": "wTGORH_cHPX",
        "invitation": "ICLR.cc/2023/Conference/Paper4117/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "It is somehow incremental work based on GRAND. An objective of diffusion is quite simple, whether continuous or discrete. This paper states that it can alleviate over-smoothing, but there is not enough evidence to show the alleviating.",
            "strength_and_weaknesses": "### Strength \n- This paper gives an excellent theoretical analysis\n- The language is relatively standard\n- The experimental results are relatively high\n\n### Weaknesses\n- The method is not original enough.\n- They didn't fully explain the difference with Grand",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: good\nNovelty : borderline\nReproducibility:good\n",
            "summary_of_the_review": "It is somehow incremental work based on GRAND. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4117/Reviewer_spFT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4117/Reviewer_spFT"
        ]
    },
    {
        "id": "84RFXpEKj-",
        "original": null,
        "number": 3,
        "cdate": 1667486783209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667486783209,
        "tmdate": 1667487036229,
        "tddate": null,
        "forum": "wTGORH_cHPX",
        "replyto": "wTGORH_cHPX",
        "invitation": "ICLR.cc/2023/Conference/Paper4117/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, a continuous depth map neural network based on the diffusion process on the graph, DeepGRAND neural network, is proposed. It makes use of the scale term dependent on data and the disturbance to the diffusion rate of the graph, so that the real part of all the eigenvalues of the diffusion rate Marcus becomes negative, thereby alleviating the problem of over smoothing and ensuring the stability of the model. This paper empirically proves that DeepGRAND is superior to many existing graph neural networks in various graph depth learning benchmark tasks.",
            "strength_and_weaknesses": "In this paper, a continuous depth map neural network based on the diffusion process on the graph\u3002",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is of fair novelty and quality.",
            "summary_of_the_review": "My research field is not related to this manuscript and I am not very familiar with the research content.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4117/Reviewer_UGaV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4117/Reviewer_UGaV"
        ]
    }
]