[
    {
        "id": "lGuylVM2i3m",
        "original": null,
        "number": 1,
        "cdate": 1665849466242,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665849466242,
        "tmdate": 1665849466242,
        "tddate": null,
        "forum": "ThXqBsRI-cY",
        "replyto": "ThXqBsRI-cY",
        "invitation": "ICLR.cc/2023/Conference/Paper4927/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper certifies the robustness of the neural network under geometric transformations. While L2 norm bounded certification has been used in training to further improve the certification bound, no one has used this in geometric transformations. Following the established GPU accelerated certification method, this paper can speed up the geometric robustness.",
            "strength_and_weaknesses": "Strength:\n1. This paper is the first one that uses GPU accelerated training for certified robustness against geometric transformations. \n\n2. The paper conducts an extensive study for certifying different models and datasets.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, of good quality. The novelty is fair. Code is not provided, thus it is a little questionable that the paper can be reproduced.",
            "summary_of_the_review": "I do not work in the certification of neural networks, the results read solid in speed up and improving the geometric transformation certification. Thus I recommend for acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4927/Reviewer_XMPM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4927/Reviewer_XMPM"
        ]
    },
    {
        "id": "fIgV9CJcDVM",
        "original": null,
        "number": 2,
        "cdate": 1666524295832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666524295832,
        "tmdate": 1669018763906,
        "tddate": null,
        "forum": "ThXqBsRI-cY",
        "replyto": "ThXqBsRI-cY",
        "invitation": "ICLR.cc/2023/Conference/Paper4927/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a framework to improve and certify robustness against certain geometric perturbations. The main contribution lies in the parallelization of geometric transformations and their inverses. These improvements enable efficient training as proposed in the paper, as well as efficient LiRPA verification. The conducted experiments demonstrate speedup up to multiple orders of magnitude. ",
            "strength_and_weaknesses": "It is an interesting idea to improve the scalability of the formal verification algorithms by accelerating / parallelizing perturbations. The appendices have also provided some additional insights. \nWeaknesses: \n\n1. The algorithm 1 as a core contribution would perhaps need more motivation and explanation. For instance, one could further modularize the big chunks of the algorithm into smaller functions with better defined purposes and input/output. Furthermore, it is not clear to me how this algorithm is better parallelizable than the standard methods. The paper could have been much stronger if this point had been highlighted. But since I'm no expert in algorithm efficiency and have probably missed certain aspects, I'm open to other reviewers' opinion. \n\n2. To be more self-contained, perhaps one could include a small recap of the LiRPA. \n\n3. I\u2019m wondering why PGD+DeepG is expected to produce SoTA performances, since PGD is gradient based and DeepG geometric. The authors might want to include references that also make use of / explain this specific design. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well structured and easy to follow.\nThe novelty, to the best of my knowledge, lies in the parallelization of geometric perturbations that rely on interpolation operations. The proposed training loss is however straightforward and the certification is performed using auto LiRPA. \nThe authors promise that the codes will be made available upon publication. \n",
            "summary_of_the_review": " In general this is a sound paper with an interesting idea but limited novelty. This could be improved, however, if the main contribution, the parallelization, could be elaborated. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4927/Reviewer_56ik"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4927/Reviewer_56ik"
        ]
    },
    {
        "id": "hrCz2X7uA4",
        "original": null,
        "number": 3,
        "cdate": 1666610401502,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666610401502,
        "tmdate": 1666610401502,
        "tddate": null,
        "forum": "ThXqBsRI-cY",
        "replyto": "ThXqBsRI-cY",
        "invitation": "ICLR.cc/2023/Conference/Paper4927/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper deals with training neural network to be robust to geometric transformations of the inputs. Here, robustness is taken to mean *certified* robustness, which can be formally proven, even in the worst case scenario.\n\nThe contribution are two-fold:\n- Improvements in terms of speed of evaluating bounds on the output of the network given that the input is subject to geometric transformations.\n- Using the speed-up in verification, it becomes feasible to include the robust loss into the training objective, in order to train the network to be certifiably robust, which will make it easier to verify.\n\nThe speed up of the verification is achieved by:\n- Relying on input splitting to only consider *small* perturbations, such that for each pixel, only a small number of original image pixels can contribute to the interpolation. This has two impacts: the bounds produced are tighter (because the bound is taken over only a small number of pixel), and the transformation can be represented using a very sparse representation, which can make it efficient.\n- Batching. During training, the sampled transformation range is applied to *all* of the elements in the batch (it's not a different transformation per sample), which means that the computation of the interpolation coefficients is amortized over the batch. I assume that the same is done at verification time, where a batch of test samples are verified at the same time, and the same \"slice\" of the input domain is verified for all the images at the same time.\n\n",
            "strength_and_weaknesses": "# Strength\nThe problem studied is quite an important one as geometric perturbations are a more realistic specifications than $\\mathcal{L}_p$ bounds.\nThe paper is clearly written, with proper background being given for the problem studied (geometric perturbations, robust objectives), which makes it easy to follow along. The annotated Algorithm 1 is easy to follow, allowing for easy re-implementation by other people, and the running example of Appendix B is also useful for understanding.\nThe empirical results are quite convincing, and evaluated on a wide range of datasets and specification.\n\n\n# Weakness - Things to improve.\n- There is a lack of precision as to how bounds are propagated through the network. The authors describe very clearly how to propagate bounds through the geometric transformation, but then only describe that they use auto_lirpa to propagate through the network, without describing which algorithm they are using. The text after Definition 5 seems to indicate that this might be IBP, but it would be good to be clear on whether that's the case (is it the same both at training and verification time?)\n- This is only a suggestion, but after reviewing the paper, I think what one of the things that the authors should highlight more is how different their method is from the most comparable works (Balunovic et al.) and (Mohapatra et al.). Both of these work go through the trouble of deriving bounds that are **explicitly dependent on $\\theta$**, so that they can propagate linear bounds that are dependent on $\\theta$. This paper completely circumvents that problem by using worse bounds which do *not* depend on $\\theta$. It seems to me like this is an integral part of what makes things so efficient: given that you're going to chop up the input space in such tiny sub-intervals anyway, there is no reason to have super representative bounds.\n- More generally, I think that the ablation study in 5.3 is the weakest part of the paper. The speedup number presented are impressive, but it seems to me that the comparison is a bit unfair, if the baseline method is not batched and operates on CPU rather than GPU. By all means, the author should provide these results but they also should include a runtime for their method under similar conditions (batch size of 1, running on CPU). I think that these would highlight clearly what the contribution is (parallelizing, vectorizable). As it is, this ablation study is not ablating anything, it's just comparing to a baseline. I'll clarify that even if the severely handicapped version of their algorithm is not outperforming Semantify-NN, that is still an acceptable result and that highlighting what choices need to be made to enable certified training is very interesting.\n- I think the authors are missing a more detailed discussion of the verifiable robustness to rotation of (Singh et al.,  2019). It is cited as verifying the robustness against norm-based adversarial perturbations, but that paper actually also contains discussion of robustness to rotation, using exactly the same type of bounding through adversarial transformation.",
            "clarity,_quality,_novelty_and_reproducibility": "## Quality\nThe paper achieves great empirical results, on an important problem, by proposing a simple solution. \n\n## Clarity\nThe paper is very clear in its explanation of their algorithm, and presents itself correctly in the context of the existing literature.\n\n## Originality\nThe idea is relatively simple, combining the concept of certified training with the specification of robustness to geometric transformation, but the solution proposed to enable this combination is definitely novel.\n\n## Minor note:\n- In Algorithm 1, I assume that line 17 should be a call to the `MakeInterpGrid` procedure defined above?\n",
            "summary_of_the_review": "The paper provides a good solution to enable fast propagation of bounds through geometric transformations at training time, enabling certified training against geometric transformation. This simple idea, which is well explained and thoroughly evaluated leads to good empirical results. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4927/Reviewer_2P26"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4927/Reviewer_2P26"
        ]
    },
    {
        "id": "tsna-qKWdJ",
        "original": null,
        "number": 4,
        "cdate": 1666658868532,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658868532,
        "tmdate": 1666658868532,
        "tddate": null,
        "forum": "ThXqBsRI-cY",
        "replyto": "ThXqBsRI-cY",
        "invitation": "ICLR.cc/2023/Conference/Paper4927/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a novel GPU-optimized verifier (FGV) for geometric transformations [1]. This verifier enables training using a sampling-based [2] robust classification loss leading to robust networks that are efficiently verifiable with the FGV. For small datasets (MNIST / CIFAR10), the proposed algorithm is orders of magnitude faster with comparable or better certified accuracy when compared to the state of the art. It also demonstrates non-trivial certification results on larger datasets (Tiny-ImageNet/Udacity self-driving car) for the first time.\n\n[1] Rotations, translations, scaling, shearing, changes in contrast and changes in brightness\n[2] Rather than computing the robust loss across the full input domain, the method computes the worst-case loss for a small subset of the input domain.",
            "strength_and_weaknesses": "Well written paper with all the details required to reproduce the result. Improves verification runtime by several orders of magnitude while matching or improving on robust accuracy, via a novel GPU-accelerated verifier.\n\nA minor concern I have is around the hyperparameter $\\nu$. The schedule of values for these hyperparameter are provided in the Appendix, but it's not clear to me how the work arrives at these values in the first place. Referencing Table 5, $\\nu$ does not have a clear relationship to the size of the original domain. If hyperparameter tuning was used to determine the appropriate value of $\\nu$, this should be reflected in the main body - since that would increase the amount of time actually required for training and verification.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nWell written overall, with good flow and clear definitions.\n\nSome questions:\n\nQ: How do we define the output for geometric transformations (any rotation, any translation, shrinking, any shearing) that move some parts of the image out of the frame and leave some areas of the frame with no corresponding source pixel in the image? This does not have a bearing on your method per se, but doesn't seem to be defined either in the main body or the Appendix.\n\nQ: Are interpolated transformations bijective? Otherwise, how do you handle this? My toy example are the following matrices:\n\n```\n1 0\n0 1\n\n0 1\n1 0\n```\n\nWhen 'rotated' by 45 degrees, they seem to yield the same output:\n\n```\n0.5 0.5\n0.5 0.5\n```\n\nWhen rotating this output matrix by another 45 degrees, which of the original matrices should you obtain?\n\nAgain - this is not necessarily an issue with your method (but it does complicate checking that the math is correct)\n\n### Quality\n\nImproves on the state of the art by several orders of magnitude in runtime (for a problem that is of interest to the community) and matching or improving on certified accuracy.\n\n### Novelty\n\nClearly novel verifier, and novel loss (albeit one that is a variation on existing robust losses).\n\n### Reproducibility\n\nThe evaluation section (and additional details in the Appendix) provides all the information necessary to reproduce the results. (For example, the work specifies the exact hardware used and the architecture of the networks verified).\n\n---\n\nSeparately from the comments above, a minor nit: I don't think that the (low probability) susceptibility of probabilistic guarantees to false negatives for verification is so serious to be 'not permissible'. What's a real-world use case under which such false negatives would cause a problem? Some thoughts:\n\n- In the case of offline evaluation on a dataset, the low rate of false negatives should not be a problem.\n- In the case of determining online for a particular scene (say, validating stop sign detection in the driving context), having a deterministic method that verifies at a low rate that the stop sign detected is robust to adversarial examples seems just as bad as a probabilistic method that verifies at a high rate ...\n\nPerhaps I'm missing something here. I'd be interested to hear your thoughts\n",
            "summary_of_the_review": "This paper is a clear accept. It introduces a novel GPU-accelerated verifier FGV for geometric transformations (the idea of using the GPU for verification is not novel, but applying it to geometric transformation is), allowing for a certifiable training approach that results in networks that are robust and verifiable via FGV. My only concern is that there does not seem to be a principled way to select the domain-splitting hyperparameter $\\nu$; this may be something that has to be selected on a dataset-by-dataset basis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4927/Reviewer_Z6j5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4927/Reviewer_Z6j5"
        ]
    }
]