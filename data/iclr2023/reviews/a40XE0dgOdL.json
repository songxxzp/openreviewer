[
    {
        "id": "OPgqXRCYbdS",
        "original": null,
        "number": 1,
        "cdate": 1666262289125,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666262289125,
        "tmdate": 1666262289125,
        "tddate": null,
        "forum": "a40XE0dgOdL",
        "replyto": "a40XE0dgOdL",
        "invitation": "ICLR.cc/2023/Conference/Paper4599/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This is primarily a theoretical paper addressing the problem of solving differential equations with neural networks. Existing approaches do not provide any guarantees about their error relative to the true solution. Specifically, optimising the loss function doesn\u2019t bound the error of the solution.\n\nSection 2 of the paper sets up a formalism and notation used throughout the rest of the paper. \n\nSection 3 describes some basic results about bounding the error of a NN solution. Specifically, some asymptotic results are given about the loss going down with the solution error of the neural net, and a bound is given for the solution error dependent on Spec of the non-linear portion of the DE.\n\nSection 4 describes the main contribution of the paper. It describes a way to give more useful bounds on the solution error, without knowing the solution already. Essentially, they describe a simple procedure to define a new DE whose unique solution is the error of the NN solver on the original problem. We can extend this to create a chain of networks, each estimating the error of the solution defined so far (i.e. the sum of the original solution, and subsequent error estimates).\n\nSection 5 describes some potential extensions of the work.\n\nSection 6 shows numerical experiments.",
            "strength_and_weaknesses": "Overall, the paper is very well-written. Despite having a lot of dense theorems, overall the explanation is clear, and it is very well-connected to practical implications.\n\nI have a few questions:\n- Can you bound how much the solution error goes down at each stage of estimating the error of the previous network? How do we know we won\u2019t get some degenerate behaviour, e.g. where the networks in the chain just alternate between overcorrecting each other.\n- Are there ablations to other simpler methods for training NN solvers? For example, your approach requires K models to perform K - 1 steps of error correction. If we just take one model that is (roughly) K times as big and train it once, how does the solution error compare?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well-written, and addresses one of the most fundamental questions in the field. The writing is very lucid.\n\nSufficient details are provided to reproduce the experiments.\n",
            "summary_of_the_review": "Overall, the paper is very well-written. Despite having a lot of dense theorems, overall the explanation is clear, and it is very well-connected to practical implications.\n\nI have a few concerns regarding some additional ablations (see Strengths and Weaknesses).\n\nI have not checked the proofs of any of the theorems, and I do not have an extensive theory background, hence the low confidence in my score.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4599/Reviewer_25yz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4599/Reviewer_25yz"
        ]
    },
    {
        "id": "EXaOrltH7m",
        "original": null,
        "number": 2,
        "cdate": 1666656958146,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656958146,
        "tmdate": 1666656958146,
        "tddate": null,
        "forum": "a40XE0dgOdL",
        "replyto": "a40XE0dgOdL",
        "invitation": "ICLR.cc/2023/Conference/Paper4599/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The authors propose techniques for building more efficient neural network differential equation solvers based on a novel analysis of the optimization objective used by existing solvers. The authors study and derive new bounds, equations, and convergence rate for existing solvers and use these to propose a new algorithm. They provide numerical experiments that show improvements over existing baselines.",
            "strength_and_weaknesses": "STRENGTHS:\n* The algorithmic ideas proposed in the work are novel, technically sound, and non-trivial. The depth of the technical results and their derivations is significant.\n* The experiments show promising results against existing baselines.\n\nWEAKNESSES:\n* This work could benefit from a better contextualization of the results relative to a real-world problem.\n* The experiments section could benefit from additional analysis on downstream applications of the proposed solvers.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. However, this work could benefit from a better contextualization of the results relative to a real-world problem.. The results seem to be novel, high quality, and reproducible.",
            "summary_of_the_review": "I can imagine this paper being a useful addition to this line of literature, but this is outside my scope of expertise, and I would mostly defer to the other reviewers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4599/Reviewer_6cUn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4599/Reviewer_6cUn"
        ]
    },
    {
        "id": "S1Ut13wLEN",
        "original": null,
        "number": 3,
        "cdate": 1667263312503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667263312503,
        "tmdate": 1667263401266,
        "tddate": null,
        "forum": "a40XE0dgOdL",
        "replyto": "a40XE0dgOdL",
        "invitation": "ICLR.cc/2023/Conference/Paper4599/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "NNDE solvers are popular these days, e.g., PINN, etc. This paper presents a method to estimate the error of those NNDE solvers and an iterative method to improve their accuracy. The authors introduce two fundamental theorems, from which their algorithms are designed. They also present some experimental results to show the correctness of their findings and algorithms.",
            "strength_and_weaknesses": "- S1) As I know, this paper is the first to measure the error level of NNDE solvers in an unsupervised manner.\n- S2) Their findings are novel, and their error estimation method is easy to follow and seems effective.\n\n- W1) Experimental results are weak. They need to test with PINN models.\n- W2) Some PINNs use additional loss functions other than F. In such a case, I am unsure if the proposed method can still be applied.\n- W3) Do you consider boundary conditions?\n- W4) The correction method is iterative, which requires large resources.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is acceptable overall. However, there are some parts to be enhanced, e.g., a weird line between Eq 2 and 3. Their findings and algorithms are novel.",
            "summary_of_the_review": "I think this paper has non-trivial value in the PINN community. I like their overall idea to estimate the error level and iteratively correct in an unsupervised manner. However, the biggest problem is their experiments. To be accepted, I think they need to test with many PINN variants. Ever since the first introduction of PINN, there have been many other variants. The authors should show the efficacy of their method to them. If not, their nice findings cannot be appreciated. Please answer the weak points above to help my understanding. During the rebuttal period, I recommend that the authors work hard to test with real PINN models. I will reconsider my score after the rebuttal since this paper has non-trivial value in my opinions with one critical problem in their experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4599/Reviewer_3Hkr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4599/Reviewer_3Hkr"
        ]
    },
    {
        "id": "gqjvQFh4T7",
        "original": null,
        "number": 4,
        "cdate": 1667795848564,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667795848564,
        "tmdate": 1670093382115,
        "tddate": null,
        "forum": "a40XE0dgOdL",
        "replyto": "a40XE0dgOdL",
        "invitation": "ICLR.cc/2023/Conference/Paper4599/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This article proposes a novel method for training neural networks to solve partial differential equations. It begins by pointing out the necessity to infer from the vanishing PDE residual the vanishing of the error of the solution of the PDE. \n\nIt begins by providing theorems that, under certain conditions, allow bounding the error as a function of the residual. \nIt then proceeds to attempt to obtain better estimates for the error by plugging it into the PDE, and thus obtaining a PDE (closely related to the original PDE) that has to be satisfied by the mismatch between the true solution and the PDE. \n\nBy introducing a *second* neural network that is trained to solve for this mismatch and adding its results to the first neural network, the authors obtain remarkable improvements in accuracy.",
            "strength_and_weaknesses": "### Strengths\n\n- To the best of my knowledge, the methodology proposed by the authors is novel. \n- The empirical performance of the proposed method is very promising.\n\n### Weaknesses\n\n- It seems unclear that the theory presented in section 3 is applicable to any PDE\n- The claim of an improved error estimation does not seem to hold up. ",
            "clarity,_quality,_novelty_and_reproducibility": "I found in the beginning of the paper somewhat confusing. In particular, the discussions in section 2 and 3 seem largely immaterial to the proposed method. I found them to be rather confusing than providing helpful motivation. \n\nThis, as well as spurious claims and vague language such as \"However, this is still substantially better than the situation that existed before: even an ambiguous estimate of $\\Phi_{\\epsilon_1}$ has clear practical benefits\".\n\nThe proposed method appears to be novel and surprisingly effective. ",
            "summary_of_the_review": "I am very much on the fence about this work. On the one hand, the proposed method is showing surprisingly clear gains, despite being very simple. On the other hand, the present work contains too many spurious claims or handwavy claims and too little insight into why the method works. I therefore believe that it is not ready for publication in the present state.\n\nMore concretely, my main points of criticism are summarized below:\n\n1. Not a single example is given for the application of Theorem 1. In particular, I was not able to discern what the norm is denoted by $\\|\\cdot\\|$ throughout the paper. When using the norms from elsewhere in the paper (p-norms for the residuals, L^2 error for the solution error in the final results), not even a linear differential operator will satisfy the assumptions of the theorem. \n\n2. The paper leads with the promise of improving the error estimation in NN-based PDE solvers. However, as the authors themselves remark, this problem is merely hidden in the \"error estimate of the error estimate\". I was not able to observe any argument as to why estimating the error of the error should be any easier than that of the original error. To this end, the authors seem to only produce the unsubstantiated claim that \"... this is still substantially better than the situation that existed before: even an ambiguous estimate of $\\Phi_{\\epsilon_1}$ has clear practical benefits\". \n\n3. It is quite surprising that the proposed method achieves the improvements reported by the authors. It would therefore be very valuable to provide additional insights. For instance, it would be helpful to provide the derivation of the PDEs for the error corrections in the case of one particular PDE. Also, additional ablation studies would be welcomed, for instance understanding how much of the improvement simply arises from the architectural change of representing the solution as the sum of the outputs of two neural networks.\n\nOverall, I believe that the work shows promising results but is not yet ready for publication in ICLR. \n\n==============================================================\nThe discussion with the authors has further confirmed my concerns about the presentation of the paper. I therefore changed my recommendation to 3 (reject) ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4599/Reviewer_cgGX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4599/Reviewer_cgGX"
        ]
    }
]