[
    {
        "id": "_ZGwgXW620",
        "original": null,
        "number": 1,
        "cdate": 1665759814536,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665759814536,
        "tmdate": 1665759814536,
        "tddate": null,
        "forum": "53yQBJNQVJu",
        "replyto": "53yQBJNQVJu",
        "invitation": "ICLR.cc/2023/Conference/Paper2675/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to evaluate the robustness of recognition systems to adversarially sampled training sets, and thus get an estimate of the worst case performance. The main contribution is an approach to identify the worst possible training set; this approach is based on maximizing the MMD between the training set and the test set, and introduces approximations based on the neural tangent kernel for tractably working with the MMD. Results are shown on four standard datasets and a few different model architectures.",
            "strength_and_weaknesses": "Strengths:\n1. The problem of finding worst case performance is an important one. There is not much analysis of how training sets impact downstream accuracy, so this is a step in the right direction.\n2. The idea of using NTK to get around the intractability of MMD is clever, and leads to a practical algorithm.\n\nWeaknesses:\n1. This paper positions itself as few-shot evaluation. Yet, experiments are with 50-500 examples per class, which is actually a moderately large training set. This is not few-shot evaluation in any shape or form.\n2. In addition, while the paper talks of evaluating few-shot models, *not even a single few-shot learning system is evaluated*. Instead, the paper chooses to evaluate standard classification approaches that are explicitly designed to work with large training sets. As such, the fact that these networks lose performance in the worst case evaluation is unsurprising and yields no useful insight. Few-shot learning techniques are intended to work well with small training sets, and may in fact be more resilient in this worst case evaluation. Comparing different few-shot learning techniques would yield some insight re:possible benefits of few-shot learning.\n3. At the very least, with standard classification approaches, there are knobs such as data augmentation and weight decay that one can change. There are also ways of training the network with adversarial data augmentation. I would expect at least some analysis along these lines to get some insight into what affects worst case performance. No such analysis is present.\n4. The fact that the worst case performance is going to be low is obvious just from the average case numbers: with the exception of MNIST, all models are clearly overfitting dramatically on the training set (see large gap between average case test and train performance in Table 1). It is therefore not surprising at all that the test-train gap will be even larger when the training set is adversarially chosen. This further underscores the need to evaluate regularization/augmentation.\n5. The one novel contribution of this paper is an approach to produce adversarially sampled training sets. But this is not evaluated well. The only baseline presented is average case performance. Here is a simple baseline that I would like to see: sample N different training sets, and report the *minimum* accuracy over all N. \n6. In practical scenarios, one usually has some control over what images to get labeled. I was hoping for some insight on this, and some general advice to downstream applications on how to intelligently sample a good training set. This is once again missing.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is relatively clear.\nReproducibility: The paper is reproducible to the extent I see.\nQuality: I am not sure what is meant by quality. If this is an overall rating of the work, then I think this work does not meet the bar for ICLR.\nNovelty: The idea of maximizing MMD is novel, but none of the experimental findings are novel.",
            "summary_of_the_review": "This paper is a step towards an important problem: understanding the robustness of neural networks to different training sets. However, in its current form it offers no useful insights and has minimal experiments that are not relevant to the stated title or thesis. Therefore it does not meet the bar.\n\nThe issues in this paper are too significant to be addressed in the rebuttal. I recommend that the authors address the weaknesses above and resubmit to a future conference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2675/Reviewer_Hiwt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2675/Reviewer_Hiwt"
        ]
    },
    {
        "id": "BgbT1HhJNz",
        "original": null,
        "number": 2,
        "cdate": 1666504585332,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666504585332,
        "tmdate": 1666506386091,
        "tddate": null,
        "forum": "53yQBJNQVJu",
        "replyto": "53yQBJNQVJu",
        "invitation": "ICLR.cc/2023/Conference/Paper2675/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper seeks to investigate the worst-case performance of neural network models in the low-data regime with respect to data sample index choice. Studying this problem is motivated by wanting to investigate neural network\u2019s tendency to rely on spurious correlations for classification. The authors propose a method called NMMD attack that specifically selects the dataset subset that results in the worst performance. Experiments on several popular vision datasets (MNIST, CIFAR, ImageNet) and a variety of neural network architectures show that model performance does indeed suffer from picking the worst-case subset of the dataset.",
            "strength_and_weaknesses": "Strengths:\n- S1. This paper\u2019s topic is certainly interesting. Given only a few examples to learn from, the risk of overfitting to non-robust features is understandable. It\u2019s well-known that few-shot accuracy on meta-test sets can vary wildly depending on which images/classes happen to be sampled, which is why the FSL community usually reports results averaged over thousands of meta-test sets. I\u2019m not aware of any prior works that have specifically studied this problem.\n- S2. Experiments do show a significant drop-off in performance with the worst case datasets produced by the proposed NMMD attack. This drop-off is reflected across several models and datasets, so this appears to be a general phenomenon rather than a quirk of a particular architecture or dataset. This is perhaps unsurprising, but Table 1 does clearly quantify how bad this drop can be.\n- S3. The writing is fairly understandable. That said, the draft could use another round of proofreading, as there are a number of grammatical and idiomatic errors. See Miscellaneous below for a non-exhaustive list.\n\nWeaknesses:\n- W1. Terminology: I\u2019m not confident that the framing of the methodology is quite right. I can see the connections and how the authors may have been inspired, but there are some subtle differences here that require care, as using terminology from other common problems can lead people to make assumptions about the setting and the proposed solution.\n  - Adversarial attacks: Adversarial attacks typically refer to a scenario where an adversary trying to sabotage the model with inputs with imperceptible changes. There is no adversary here, nor are the changes \u201cimperceptible,\u201d as there are no modifications to individual images here. Rather, we\u2019re specifically selecting the worst images for training.\n  - Distribution shift: While the distribution here technically is indeed different and therefore a \u201cshift,\u201d it\u2019s due to a specific sampling strategy, not because of factors that people usually refer to as distribution shifts (e.g. domain changes, temporal evolution). \n  - Robustness: I would also take caution when talking about neural networks having poor \u201crobustness\u201d, which often refers to individual samples during inference, rather than training datasets.\n  - Few-shot: The experimental evaluation is more akin to standard supervised training and evaluation in the low data regime, as opposed to the meta-dataset set-up common in FSL: FSL typically has both far fewer examples per class (e.g. 5, as opposed to 500 in this paper), and evaluation of transfer from base classes to novel classes (train and eval here are done on the same classes). These differences do not necessarily invalidate the results, but it may be a bit confusing to people expecting more typical FSL benchmarks like MiniImageNet/TieredImageNet when calling the method few-shot. NMMD seems like it should apply to such setting, so the authors may consider more standard FSL benchmark as well.\n\n- W2. Methodology: If my understanding is correct, the method is just identifying the examples in each class that result in the largest magnitude gradients when the model is randomly initialized. This doesn\u2019t strike me as necessarily resulting in the worst case few-shot sets. In particular, tossing the off-diagonal terms means inter-sample terms are discarded. This seems incongruous with the authors\u2019 claim in the Introduction that lack of robustness can occur due to spurious correlations between samples. Furthermore, when I look at the resulting few-shot sets in Appendix G, these don\u2019t strike me as being adversarially challenging few-shot sets, nor are spurious correlations visible. While it was the original goal, I don\u2019t know if \u201cworst\u201d case is the right term here.\n\n- W3. Notation: While I appreciate the desire to be precise, I do find some of mathematical notation somewhat unnecessarily verbose. For example, Section 4 uses several paragraphs largely to say that we assume there\u2019s a model that generalizes from the support set to the query set (an underlying assumption in ML), that the support and query sets have the same label distribution (a standard property of classification benchmarks), and that we\u2019re trying to find data samples that result in the highest error (already stated in the Introduction and Related Work). \n\n- W4. Practicality: While the results are somewhat interesting, what\u2019s the significance? When collecting a dataset, it\u2019s highly unlikely that one would be unlucky enough to assemble such a worst case dataset, and one can mitigate the effects by collecting more data. As an adversarial attack perpetuated by a malicious actor, NMMD is not practical either, because the adversary would have to replace the entire dataset (in contrast to, say, data poisoning attacks), and the choice of including the hardest image samples would likely be highly noticeable to the model owner. \n\n\nQuestions:\n\n- Q1. Given that popular datasets are known to have errors or ambiguity [1,2], I wonder if the worst case results in support sets with mislabeled support sets [3]? How do we ensure that worst case is still representative or useful?\n- Q2. What were the pre-trained models pre-trained on? ImageNet? If so, there is a lot of overlap with the CIFAR-10 classes, so this isn\u2019t truly a few-shot setting. These models would similarly be much less significantly impacted by a handful of training samples, which would explain the much smaller drop in performance from NMMD.\n- Q3. How was the test accuracy evaluated? Was it the accuracy at the end of training, or did you use early stopping? Or was the best test accuracy during training reported? Asking because the train accuracies show strong evidence of overfitting (mostly 99, 100%) and 200 epochs as reported in Table 4 is a lot. When the test accuracy is reported may have a strong impact on the results.\n- Q4. In few-shot learning evaluations, we often randomly sample not only the K shots per class, but also the N-way classification task. Have you considered how specific combinations of classes may result in extra poor performance?\n- Q5. Have you considered looking at the reverse scenario? Which samples give the best performance? How are they different from the worst case samples? Can the best case samples be used a compact, more portable version of the dataset?\n\n\n[1] Yun et al. Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels. CVPR 2021.\n\n[2] Northcutt et al. Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks. NeurIPS D&B 2021.\n\n[3] Liang et al. Few-shot Learning with Noisy Labels. CVPR 2022.\n\nMiscellaneous:\n- \u201cfew-shot\u201d vs \u201cfew shot\u201d as a hyphenated adjective\n- P_few isn\u2019t really used and therefor doesn\u2019t seem like a necessary notation to introduce.\n- Section 4: \u201cAssumption. 4.1\u201d, \u201cAssumption. 4.2\u201d <= extra period\n- Section 5: \u201cHere, We first proves\u201d <= \u201cHere, we first prove\u201d\n- Section 5: \u201cLast\u201d <= \u201cLastly\u201d\n- Definition 5.1: notation collision. y is already used as the labels in Sec. 3 + 4.\n- Theorem 5.1: \u201coccurred\u201d <= \u201cincurred\u201d?\n- Equation 4: Should 1/m be 1/k and x_i_m be just x_m?\n- Section 6: \u201ca feed-forward neural networks\u201d <= \u201ca feed-forward neural network\u201d. Also, all the models listed are feed-forward neural networks. The description of 1) relative to the other models needs to be clearer. \n- Section 6: The distinction between models like ResNet and DenseNet with \u201cpretrained models\u201d ViT and EfficientNet is confusing. All of the listed models can be pre-trained.\n- Section 6: notation collision for m, which is used as the number of trials and as an index in Eq 4.\n- Section 7.1: Missing space after \u201c3)\u201d\n- Table 3: \u201cAverage-case\u201d is confusing. I initially thought this column was the average across the previous columns. Better call it \u201cRandom\u201d to better match the text.\n- Reproducibility Statement: What\u2019s 6? \u201cA\u201d is \u201cAppendix A\u201d?\n",
            "clarity,_quality,_novelty_and_reproducibility": "As I noted above, the method is presented in a way that\u2019s perhaps more formal and verbose than is actually necessary, especially since the diagonal assumption this paper makes dramatically simplifies the algorithm being proposed. Regardless, the paper is still fairly clear and understandable. I\u2019m not aware of other previous works studying this topic, so in that sense the paper is original, though I don\u2019t find the key findings to be all that surprising.\n\nA Reproducibility Statement is included, and reproducing code is provided.\n",
            "summary_of_the_review": "Studying the worst case performance of models as a function of which subset of the data is selected is an interesting problem, and not one I\u2019m aware of much prior work on. W2 and W4 are my main concerns. With the approximations that the authors made in order to achieve tractibiliy, the authors aren\u2019t truly finding the \u201cworst\u201d case datasets, as is claimed. Even if they are though, it\u2019s not clear to me what the practical use of the proposed method is: as an adversarial attack, it would fail to be particularly sneaky, and as a warning to ML practitioners, it would seem that the likelihood of picking such a bad dataset subset are relatively unlikely. If the goal is just to show that neural network performance drops if we try pick the training set giving the worst-case accuracy, then the findings are perhaps somewhat unsurprising.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2675/Reviewer_di2U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2675/Reviewer_di2U"
        ]
    },
    {
        "id": "FuVBDFl8s0",
        "original": null,
        "number": 3,
        "cdate": 1666676807562,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676807562,
        "tmdate": 1666676807562,
        "tddate": null,
        "forum": "53yQBJNQVJu",
        "replyto": "53yQBJNQVJu",
        "invitation": "ICLR.cc/2023/Conference/Paper2675/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper aims to evaluate the few-shot capabilities of a neural network on the worst-case subset of a dataset. The main motivation is to showcase the fragile ability of neural networks to memorize spurious statistical cues in the dataset, leading to poor generalization. To find the worst-case subset, the authors propose an algorithm that picks the subset with the highest gradient norms at initialization. With extensive experiments, they showcase the failure of well-known models in fewshot setting on datasets like CIFAR-10, CIFAR-100, and Imagenet-1k.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper conducts multiple experiments to show that for a randomly initialized or a  pretrained model we can form a small subset of examples with the largest gradients at initialization (not necessarily of the same model), on which the model's performance drops drastically, compared to a random subset of examples. This clearly shows the fragility of different models in the few-shot setting. Furthermore, with a small ablation study, the authors show that the \"adversarial\" subset discovered is as diverse as a random subset of examples, but doesn't contain much spurious correlation that the model can latch on. This further shows the fragility of the real-world models to latch on to spurious correlations to perform well in few-shot settings.\n\n\nI have the following questions/concerns:\n\na) In eq. (2), the difference between $\\epsilon_Q( f_{I_k} ) $ and $\\epsilon_Q (f)$ is given by the MMD( P_{I_k}, P ). However, in theorem 5.2, we approximate $ MMD_f ( P, P_{I_k} ) $ with the NTK w.r.t. the function $f$. How do the authors relate the difference between $\\epsilon_Q( f_{I_k} ) $ and $\\epsilon_Q (f)$ with NTK w.r.t. $f$? \n\nb) Does the NTK assumption (the kernel doesn't change much during training) hold true for the models in the few-shot training in Table 1? \n\nc) What is the empirical gap observed in eq. (3) using the NTK of the models in Table 1? Does the NTK estimate give a non-trivial empirical bound for $\\epsilon_Q( f_{I_k} ) - \\epsilon_Q(f)$ in equation (3)?\n\nd) How do VGG-attack and ResNeXt-attack perform in table 2? Will we observe a similar drop in performance?\n\nFurthermore, how do subsets using gradients of ViT-B/16  and EfficientNetV2 perform in Table 2? Should one expect the drop in performance to be higher than the ones with the FFN attack?\n\ne) If the NTK assumption holds true, then we can also use the NTK of a few-shot finetuned model to gather the \"adversarial\" subset.\n\nFor the models used in Table 1, how do the models perform against the NMMD attack from their finetuned versions? That is if we finetune a model on a small random subset and use the model to create the adversarial subset, will we observe a similar drop in performance?  What about an NMMD attack with respect to the gradients of a model that has been finetuned on the entire dataset?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The theoretical statements have some issues, which I have pointed out above. However, the experiments are novel and clearly show the fragility of models in few-shot settings.",
            "summary_of_the_review": "Overall, my scores are borderline. The paper proposes a novel attack to understand the relation between few-shot evaluation and spurious correlations between training and test sets. However, there are issues with the theoretical motivation underlying the proposed algorithm. Hence, I would like the authors to clarify the questions raised above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2675/Reviewer_X5Fg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2675/Reviewer_X5Fg"
        ]
    }
]