[
    {
        "id": "U6jPAbbK-s",
        "original": null,
        "number": 1,
        "cdate": 1666352616090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666352616090,
        "tmdate": 1666352616090,
        "tddate": null,
        "forum": "k8_yVW3Wqln",
        "replyto": "k8_yVW3Wqln",
        "invitation": "ICLR.cc/2023/Conference/Paper5320/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method to prevent existing large language models to produce toxic discourse. Contrary to existing methods that modify the training procedure for LLMs or eliminate tokens at inference based on rules, the proposed method uses reinforcement learning to compute, at each step of the generation, a probability that the final discourse will be toxic and modifies the sampling probability accordingly. \n\nThe authors propose a separate small rectification model that may be applied to any LLM as long as they have the same vocabulary, based on the $\\beta$-dead-end theory in RL. They compare the new method with retraining-based and decoding-based baselines and report competitive results. The method can be combined with the TestFilter decoding-based method to out-perform existing techniques.",
            "strength_and_weaknesses": "Strength: The method is based on a theory that looks sound, and is confirmed by the results. The proposed model can work for any LLM, alleviating the need for retraining. The paper is well written and could be understood by someone (like me) not very familiar with LLMs and RL. The results look impressive.\n\nWeakness: The TestFilter method seems to be simpler and to work quite well already. The best results presented in this paper are in combination with TestFilter. While showing that the two methods can be complementary, it might be interesting to cary out more comparisons between the proposed method and TestFilter, in particular for the human evaluation and the qualitative results.",
            "clarity,_quality,_novelty_and_reproducibility": "I am not expert enough in these topics to provide a relevant evaluation of the quality, soundness and novelty of the method. \nNonetheless, I found the paper very well written, easy to follow and to understand and convincing. \nMany details are provided in the appendices, which should help reproduce the results.\n\nMinor formatting comments:\n\n\"methods like Raffel et al. (2020);\" --> like those proposed by\n\"computationally expensive and slow Gehman et al. (2020); Yang & Klein (2021).\" --> citation formatting",
            "summary_of_the_review": "This paper is clear and well written. It addresses a relevant and important problem. The method looks soundly grounded in theory and the results are good. As detailed in the other sections, more analysis and comparisons could be useful to evaluate the method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5320/Reviewer_ZwPr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5320/Reviewer_ZwPr"
        ]
    },
    {
        "id": "ySYUhzzKZk",
        "original": null,
        "number": 2,
        "cdate": 1666381637180,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666381637180,
        "tmdate": 1669746420241,
        "tddate": null,
        "forum": "k8_yVW3Wqln",
        "replyto": "k8_yVW3Wqln",
        "invitation": "ICLR.cc/2023/Conference/Paper5320/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors frame the task of LM \"detoxification\" as an RL problem.\nThey propose \"rectification,\" which requires only access only to token\nprobabilities from an LM API, rather than internal states.  The method\ndown-weights tokens that are likely to cause eventual toxic discourse,\ntreating toxic generations probabilistically as \"dead ends\".\nAccording to both human and automatic evaluation, the author's new\nmethod provides new pareto-optimal trade-offs between fluency and\ntoxicity compared to retraining/decoding baselines.",
            "strength_and_weaknesses": "The task of making LMs output less harmful language is interesting and\nimportant. The authors present connections to the RL literature,\nincluding some new theorems about about rectification/dead ends/etc.\nThe authors should be commended for their willingness to empirically\nengage with larger language models than I've seen in prior RL+NLP\nwork, e.g., GPT-2 XL (1.5B) and GPT-3 (175B). Their choice of\nbaselines are reasonable (save for my biggest concerns, detailed\nbelow). I tend to agree with the authors that developing methods\nassuming only specific levels of API access will be important\nin the regime of very large language models, and so efforts in that\ndomain are appreciated.\n\n\nMy biggest concern are missing empirical comparisons to work that\nexplores a very similar setup --- i.e., using RL from human feedback\nto \"detoxify\" an LM using\nperspective/realtoxicityprompts. Specifically, the authors mention\nthat the two main approaches to detoxification in this setting are\nretraining-based/decoding-based. But, there are works that have\nreadily applied RL methods to this setup before, e.g., Lu et al. 2022\nimprove upon the improved decoding time method for detoxification of\nLiu et al. 2021 in a comparable setup.\n\n- Liu, Alisa, et al. \"DExperts: Decoding-time controlled text\n  generation with experts and anti-experts.\" arXiv preprint\n  arXiv:2105.03023 (2021).\n\n- Lu, Ximing, et al. \"Quark: Controllable Text Generation with\n  Reinforced Unlearning.\" arXiv preprint arXiv:2205.13636 (2022).\n\nIt's difficult to directly compare the results from Lu et al. 2022\nwith those that are presented here. (my best effort:  the\nauthor's gpt2 baseline is 61% toxic and 18 perplexity, and the most\naggressive detox is 10% toxic and 22 perplexity. Lu et al.'s gpt2\nbaseline is 52% toxic and 11 perplexity (on wikitext). Their method\ngives 4% toxic and 12 perplexity (on the in-domain training data),\nwhich seems better (?).) Both methods conduct human evaluations,\nbut are not directly comparable.\n\nOverall, it's difficult to know which method performs best at this\ntask because the comparison is missing --- and because the Lu et\nal. method has been out for a while, I think it's fair to expect some\ncomparison with it. That being said, it appears that the Lu et\nal. method may be solving a slightly different problem: instead of\nupdating the parameters of the policy, the proposed rectification is\n(at inference time) a decoding-based method. Thus, some APIs, e.g.,\nGPT-3, may be more compatible. Though, notably --- GPT-3 may be\nfine-tuned as well, but not via an API that requires access to\nmodel-internal representations.  But, if that's the case, I would have\nappreciated a comparison to Liu et al. 2021 instead, which also\nappears to be a decoding-time method for this \"detoxification\" task.\n\nA few additional thoughts:\n\n- For a decoding method, I would have liked to have seen a reward\n  filtering baseline, i.e., sample N times from the model, and select\n  the single sample with lowest model-estimated perplexity.\n\n- The human evaluation results are not that definitive --- ignoring\n  ties, the author's model wins in 63% of cases versus the baseline in\n  terms of detoxifying.\n\n- I would have appreciated a more thorough discussion of the potential\n  for reward hacking in detoxification. This is briefly alluded to in\n  Appendix E, but there are missing references, e.g., about the\n  shortcomings of perspective, for example:\n\nHosseini, Hossein, et al. \"Deceiving google's perspective api built\nfor detecting toxic comments.\" arXiv preprint arXiv:1702.08138 (2017).",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "Overall, the paper offers a strong empirical contribution to RL tuning\nof LMs from human feedback in the setting where policy parameters are\nnot allowed to be updated. The idea of training a policy to do\ndecoding time modification of logits is quite interesting, and it's\npromising that the method can be applied to models which provide a\nmore limited API like GPT-3. The authors make it clear in the abstract\nthat they believe this setting is useful, but --- it appears there's\nsome missing discussion (and possibly empirical comparison) to methods\nwhich use RL from human feedback (with full policy access) or\ndecoding-time methods beyond DAPT.\n\n\n=== after the response period:\n\nThe authors added more experiments that I had requested which indeed better situate Rect in comparison to the prior work I had mentioned. Thank you! I still am somewhat concerned about the human evaluation results being not as definitive --- even if they are statistically significant, the magnitude is relatively small. Nonetheless, I have boosted my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5320/Reviewer_6y1n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5320/Reviewer_6y1n"
        ]
    },
    {
        "id": "b2yJ9_YU4u3",
        "original": null,
        "number": 3,
        "cdate": 1666869214937,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666869214937,
        "tmdate": 1666869214937,
        "tddate": null,
        "forum": "k8_yVW3Wqln",
        "replyto": "k8_yVW3Wqln",
        "invitation": "ICLR.cc/2023/Conference/Paper5320/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a detoxification method that aims to reduce the risk of large language models (LLMs) generating toxic contexts such as rude, disrespectful, and unreasonable language without retraining the LLMs. The idea behind this method is to reduce the probability of selecting the next token if that token will cause a toxic text to be generated eventually with some level of certainty. The authors take a langue generation procedure as a standard Markov decision process and formulate the detoxification tasks as an auxiliary reinforcement learning problem. They demonstrate that the proposed method can significantly mitigate toxicity by both human and automatic evaluation.",
            "strength_and_weaknesses": "Strength:\n\n(1) It is novel to contemplate a language generation procedure as a standard Markov decision process and formulate the detoxification problem as a reinforcement learning problem. \n\n(2) One learned model can be used to detoxify many language models if they share the same vocabulary.\n\n(3) The experimental results show that the proposed approach can generate better results compared to the baselines in detoxification by both human and automatic evaluation.\n\nWeaknesses: \n\n(1) Although the detoxification model would be smaller than the pre-trained language models, it is unclear that whether the proposed method has advantage in computational efficiency over existing detoxification methods both at the training and inference phases.\n\n(2) I doubt the rectification method would decrease the diversity of generated texts. It would be better to add some experiments to evaluate the generated contexts in their diversity.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and written in general. It is novel to formulate the detoxification problem as a reinforcement learning problem, and the authors clearly present the theory and the proof of the related theorems. ",
            "summary_of_the_review": "This word presents a detoxification method by extending dead-end theory from the recent reinforcement learning methods to alleviate toxicity for large language models. A dead-end state was defined to be a state from which a toxic text would be generated with a high probability. The main idea is to avoid toxic texts to be generate by adjusting the probability of token selection proportional to the risk of ultimate toxicity when the generation process is finished. This study was is motivated and timely. The experimental results show that the proposed method can significantly mitigate toxicity by both human and automatic evaluation. However, the proposed method might decrease the diversity of generated texts. Some experiments are required to evaluate the generated contexts in their diversity.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5320/Reviewer_p8Zr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5320/Reviewer_p8Zr"
        ]
    }
]