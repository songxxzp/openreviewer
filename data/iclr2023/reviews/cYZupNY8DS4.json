[
    {
        "id": "RZptspLdVKu",
        "original": null,
        "number": 1,
        "cdate": 1666581248145,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581248145,
        "tmdate": 1666581248145,
        "tddate": null,
        "forum": "cYZupNY8DS4",
        "replyto": "cYZupNY8DS4",
        "invitation": "ICLR.cc/2023/Conference/Paper1741/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper considers robust MDPs and proposes an online optimization policy where the agent interacts with the nominal environment.",
            "strength_and_weaknesses": "1. Most importantly, the contribution of this paper is confusing. While this paper considers an \u201conline\u201d setting without assuming knowledge of the environment, the proposed algorithm requires the size of the ambiguity set as an input. In this spirit, it is not clear to me how the proposed algorithm differs from existing algorithms, such as [i] and [ii].\n\n2. Moreover, the paper assumes that the agent interacts with an unknown nominal environment, but the regret is defined to be the difference between policy pi_k and the optimal robust value function, instead of the optimal value function under the nominal environment.\n\n3. The writing of the technical content is confusing. For example, it seems that the authors attempt the compute the support function sigma(V) for the s-rectangular case; however, for s-rectangular robust MDPs, the corresponding Bellman equation is not the same as the (s,a)-rectangular case, and it does not use the support function.\n\nMore comments/questions:\n\nSection 1, 4th paragraph, for (s,a)-rectangular robust MDPs, they have optimal deterministic policies; thus, the argument is not clear.\n\nSection 3, should {P}_{h=1}^H be {P_h}_{h=1}^H?\n\nSection 3, what is P_h = { P_h^o }? Or should it be P_h = P_h^o?\n\nSection 3, before definition 3.2. I don\u2019t think s-rectangularity provides a \u201cstronger robustness guarantee\u201d\n\nSection 3, assuming P_h^o(.|s,a) > 0 appears to be a strong assumption. For example, this assumption does not hold for the classical machine replacement example. Could it be relaxed by perturbing P_h^o? What is the error bound?\n\nSection 3, Learning protocols and regret. What is V_1? Should it be V?\n\nClaim 4.1. This is confusing as this paper assumes that the agent interacts with the nominal environment.\n\nEquation (3). What are the upper and lower bounds when using the bisection method?\n\nEquation (3). Efficiently computing the support function with the L1 ambiguity set is not new. See [i].\n\nEquation (4). To the best of my knowledge, for the s-rectangular case, the support function is not used in the Bellman equation.\n\nEquation (4). Why it could be solved in O(A)?\n\n[i] Marek Petrik, and Dharmashankar Subramanian, RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning, 2014.\n[ii] Yue Wang, and Shaofeng Zou, Policy Gradient Method For Robust Reinforcement Learning, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see my major and minor comments above.",
            "summary_of_the_review": "1. Most importantly, the contribution of this paper is confusing. While this paper considers an \u201conline\u201d setting without assuming knowledge of the environment, the proposed algorithm requires the size of the ambiguity set as an input. In this spirit, it is not clear to me how the proposed algorithm differs from existing algorithms, such as [i] and [ii].\n\n2. The definition of \"regret\" is less common.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1741/Reviewer_ZEUJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1741/Reviewer_ZEUJ"
        ]
    },
    {
        "id": "MJLVR3XGKae",
        "original": null,
        "number": 2,
        "cdate": 1666668266947,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668266947,
        "tmdate": 1666668266947,
        "tddate": null,
        "forum": "cYZupNY8DS4",
        "replyto": "cYZupNY8DS4",
        "invitation": "ICLR.cc/2023/Conference/Paper1741/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is the first provable policy optimization algorithm for robust MDP in online RL with a $\\ell_1$ uncertainty set, along with a finite-sample complexity bound. The main contribution is that compared to the previous sample complexity of robust MDP, under an online RL setting, it gives the first provable optimization algorithm with a non-asymptotic regret bound and sub-linear sample complexity. The sample complexity also recovers a non-robust policy optimization result of a prior work when the uncertainty set $\\rho \\rightarrow 0$. Experiments are conducted to show the results.",
            "strength_and_weaknesses": "Strength:\n1. This is a very interesting topic whether some algorithms can achieve $O(\\sqrt{K}$ regret in an online robust RL case with an uncertainty set. The results are of a somewhat significant novelty since this work gives the first provable policy optimization algorithm with a non-asymptotic regret bound and sub-linear sample complexity.  \n2. I didn't check the proof in the appendix, but the writing and presentation are almost clear and easy to follow.\n\nWeaknesses:\n1. This work claim that the algorithm is designed by adaptation to the robust case from the non-robust algorithm in [2]. However, [2] seems not the best non-robust policy-based algorithm for online RL in the literature with regret bound $O(\\sqrt{S^2AH^4K})$. Since the optimal policy-based algorithm is [1] with bound $O( \\sqrt{SAH^3K}+ \\sqrt{AH^4K} )$ is better than [2], it seems to apply a robust version of the algorithm in [1] will lead to a better policy and better results. It is not clear why this is not considered.\n2. The sample complexity of prior works in Table 1 seems wrong if I didn't miss something. In $(s,a)$-rectangular case, if the claimed sample complexity means the required number of episodes, the results of [D] is $O(\\frac{H^4S^2A(2+\\rho)^2}{\\rho^2 \\epsilon^2})$, and also the $s$-rectangular case.\n2. The difficulty of addressing additional challenges due to the combination of robustness requirements and the online setting is not introduced in detail. Even if there is a section describing Challenges in Optimistic Robust Policy Evaluation, it will be better to have more discussions. Or it will seem like a combination of the techniques in non-robust online RL and robust RL.\n3. In the experiments, the parameters of the perturbed transition dynamic are not given in detail. So it is a little bit hard to rerun the experiments or check the results.\n\nSome extra questions or minors:\n1. Why the sample complexity does not depends on the uncertainty level with uncertainty level $\\ell_1$? That means the sample complexity does not depend on it? If so, how to compare the results with [Yang et al. 2021] with the $\\rho$ in the denominator of the sample complexity.\n2. The first equation on Page 5 may have typos if I didn't miss something. should $s_0$ be $s_0^k$?\n\n[1] Wu, Tianhao, et al. \"Nearly optimal policy optimization with stable at any time guarantee.\" International Conference on Machine Learning. PMLR, 2022.\n[2] Shani, Lior, et al. \"Optimistic policy optimization with bandit feedback.\" International Conference on Machine Learning. PMLR, 2020.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality is very good.\nThe reproducibility of the experiments may need some more details such as the parameters of the perturbed transition dynamics.\n\nThe novelty of the results is great. The novelty of the technical details is a little bit hard to see. It may be better to give more discussions about the main technical tools used to deal with the challengs.\n",
            "summary_of_the_review": "I recommend borderline acceptance since this is the first provable policy-based robust algorithm with finite-sample complexity. However, as the technical tools are not well-introduced, it is hard to see whether this is a combination of the tools in non-robust online RL and robust RL or far beyond. The results are somewhat significant and the technical tools may be novel but it is hard to see owing to no discussions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1741/Reviewer_QpYY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1741/Reviewer_QpYY"
        ]
    },
    {
        "id": "N3xhXLwBcnS",
        "original": null,
        "number": 3,
        "cdate": 1666713149746,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713149746,
        "tmdate": 1666718797737,
        "tddate": null,
        "forum": "cYZupNY8DS4",
        "replyto": "cYZupNY8DS4",
        "invitation": "ICLR.cc/2023/Conference/Paper1741/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In Strength And Weaknesses",
            "strength_and_weaknesses": "This paper extends robust MDP with a generative model/ offline dataset to the online setting by optimizing policies. This is the first online algorithm in robust MDPs. Though the extension is mainly based on prior works for non-robust MDPs, which means this paper's novelty is limited, I think this work is fundamental. I didn't go through all the details and appendix in this paper due to my heavy review load. Though I want to give the score 4, it is not in the choice. The authors should know I can't be more positive at the current stage by giving score 5. I will leave the decision to AC.\n\nBelow I list some questions and comments that I hope the authors could answer.\n\n1. As we know, in $L_1$ uncertainty set, the size $\\rho$ has an upper bound $2$. Thus, the result in Theorem 1 has nothing to do with $\\rho$. Then, what's the role of robustness? By [1], we know that when $\\rho$ is not small, the sample complexity could be reduced. But I can't see such similar results from this paper.\n2. About Eqn (4), why it is different from the expression in [1] (Lemma B.6), which means Eqn (4) is independent with policy $\\pi$?\n3. I would encourage the authors to pay more attention to their proof sketch. For example, they could illustrate how they do error decomposition and how to control each error term. The current description in Sec 5 is more like a redundancy.\n4. A missing related work. I believe the authors should discuss with it.[2]\n\n\n[1] Yang W, Zhang L, Zhang Z. Towards theoretical understandings of robust markov decision processes: Sample complexity and asymptotics[J]. arXiv preprint arXiv:2105.03863, 2021.\n\n[2] Liu Z, Bai Q, Blanchet J, et al. Distributionally Robust $ Q $-Learning[C]//International Conference on Machine Learning. PMLR, 2022: 13623-13643.",
            "clarity,_quality,_novelty_and_reproducibility": "In Strength And Weaknesses",
            "summary_of_the_review": "In Strength And Weaknesses",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1741/Reviewer_JsyR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1741/Reviewer_JsyR"
        ]
    },
    {
        "id": "8k6RfwKb_6",
        "original": null,
        "number": 4,
        "cdate": 1667494501729,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667494501729,
        "tmdate": 1667494501729,
        "tddate": null,
        "forum": "cYZupNY8DS4",
        "replyto": "cYZupNY8DS4",
        "invitation": "ICLR.cc/2023/Conference/Paper1741/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the online robust MDP and proposes a policy optimization algorithm called Robust Optimistic Policy Optimization (ROPO) for achieving sublinear regrets. This paper starts by showing a sub-optimality result of non-robust optimal policies, which motivates the study of ROPO. The authors then provide two kinds of regret bounds of ROPO under different assumptions of the underlying uncertainty set of transition kernels. The result appears to be the first regret bound for policy-based methods in the online robust RL setting. Finally, this paper presents experimental results in simple RL environments to corroborate the performance.",
            "strength_and_weaknesses": "**Strength**\n\n- A solid paper supported by theoretical regret analysis. \n- This is the first work that provides an algorithm with theoretical regret bounds under robust MDP in the online setting.\n- Proposition 4.1 that shows the sub-optimality of the policy learned from the nominal transition model is interesting and provides a good motivation for ROPO.\n- The paper is well-written and easy to follow.\n\n**Weaknesses**\n- There are some issues regarding the main technical analysis: \n    - The statement \u201cprevious sample complexity results cannot directly imply a sublinear regret\u201d does not make much sense to me as it seems feasible to convert the sample complexity results in Table 1 to regret bounds. Why cannot the regret bound of [A] in Table 1 be converted from the sample complexity result?\n    - In the proof of Lemma 4, \u201cL(\\tilde{\\eta}, \\lambda)(s, a) is inversely proportional to \\lambda\u201d is not true since \\tilde{\\eta} also depends on \\lambda. As a result, the optimal \\lambda derived in P.16 appears incorrect, and this could affect the subsequent analysis.\nShould Eq.3 be a \u201cconstrained\u201d optimization problem? Specifically, in the proof of Lemma 4, it is required that $\\tilde{\\eta} - min_s \\hat{V}(s) \\leq \\lambda$. And the issue also arises in Eq.4. It is not immediately clear whether this would affect the analysis.\n    - In the second paragraph of P.3, the description about Badrinath & Kalathil (2021) is incorrect. It appears that the theoretical result of this work only provides asymptotic guarantees instead of a convergence rate.\n- The experiment results only show the performance of ROPO under the (s, a)-rectangular sets, lacking the result under the s-rectangular uncertainty sets.\n\nAdditional comments:\n\n1. Does the policy improvement step in Algorithm 1 require $-$?\n2. In the proof of Theorem 1, \u201c By Lemma 2 and Lemma 4,\u201d => \u201c By Lemma 2 and Lemma 3,\u201d\n3. The simplex set of the uncertainty set shall be $\\Delta_{S*A*H}$?\n4. What does it mean to have \"a fixed step of time-dependent uncertainty kernels\" mentioned at the beginning of Section 3?\n5. The definition of $\\Delta_S$ in Section 3 is not provided. \n6. The statement \u201cThis characterization is then more general, and its solution gives a stronger robustness guarantee.\u201d on top of Definition 3.2 does not make much sense.\n7. Is it still NP-hard to solve the robust MDP under the two assumptions of the transition kernel?\n8. There seems to be a typo that the estimator of the transition is conditioned on $s\u2019$.\n9. $\\hat{V}$ is not defined.\n10. The value function used in Eq.4 should be the estimator of value function \\hat{V}?\n11. The statement above the \u201cPolicy Improvement Step\" is weird. \\rho and $\\hat{P}$ seem to be in different terms.\n12. For the step of policy improvement, should \u201cargmin\u201d be \u201cargmax\u201d?\n13. How to define the gradient of \\hat{V}^{\\pi_k}? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow in most places. For reproducibility, the details of the environment are provided. The solid theoretical result is novel and is expected to have good impacts to the RL community.",
            "summary_of_the_review": "This paper takes the first step towards establishing the theoretical regret bounds for the online robust MDP. The contribution is significant as the technique introduced in this paper is valuable for sparking further results on online robust RL. My current rating is 6 mainly due to the concerns mentioned above. The score would be reconsidered if these issues can be addressed during the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1741/Reviewer_yGXw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1741/Reviewer_yGXw"
        ]
    }
]