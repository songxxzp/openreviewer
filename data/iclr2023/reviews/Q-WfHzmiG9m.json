[
    {
        "id": "tdfF7IKe-s",
        "original": null,
        "number": 1,
        "cdate": 1666327858070,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666327858070,
        "tmdate": 1666327948762,
        "tddate": null,
        "forum": "Q-WfHzmiG9m",
        "replyto": "Q-WfHzmiG9m",
        "invitation": "ICLR.cc/2023/Conference/Paper2467/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method for training \"fair\" models via a classwise variant of Group DRO. The authors show that this is theoretically equivalent to optimizing for Difference of Conditional Accuracy, but they introduce some relaxations to make the problem tractable in practice. Experiments are conducted on two tabular, two image, and two language datasets with comparisons of the proposed method to baselines.\n",
            "strength_and_weaknesses": "\n## Major Comments\n\n* The authors suggest that their approach exactly optimizes for the DCA. However, in practice, they admit to being unable to do so, and instead optimize for a convex relaxation of it by using cross-entropy (Remark, Section 4.3). While this may be a minor point, the entire framing of the paper is based on *exactly solving* the DCA problem, not a relaxation of it. I think the abstract, introduction, and framing throughout should clarify this.\n\n* The experimental results are not particularly good. They seem somewhat promising on 1/2 tabular datasets (Adult) and 1/2 vision datasets (CC), but on the others there is weak, if any, evidence that the proposed method is better. We have no estimates of statistical uncertainty, so it is hard to tell much from the point estimates -- but even more, there is no clear evaluation metric for the prpposed model. Do we care about \\delta_DCA? If so,the proposed method doesn't clearly outperform baselines. Do we care about tradeoffs/envelopes? If so, the evidence is mixed at best, with baselines still outperforming the proposed method on at least half the datasets as mentioned. The ablation study is similarly mixed.\n\n\tI am particularly concerned about empirical performance because there is already an abundance of theoretically-motivated methods for DRO, as the authors acknowledge in their review, which don't offer much benefit in practice (probably due to being overly pessimistic). If the proposed method doesn't clearly improve, on real tasks according to metrics we as a community agree are important, I don't see a strong justification for publishing yet another robustness method. I'd like to be wrong about this, as I think the authors' direct connection to fairness and accuracy (also an under-investigated metric in the context of robust learning) are important, but the strong theoretical underpinnings should be accompanied by clear empirical results in order to make a case for acceptance.\n\n* It feels somewhat that the authors bury the lede that DCA is equivalent to EO, in the binary case. It would be useful to make this point much earlier, as some of the introduction/setup feels as if it is emphasizing a \"new\" approach to fair/robustness, but it is really a generalization of a very well-known measure.\n\n## Minor Comments\n\n* It is not intuitively clear to my why we ought to, in principle, balance over classes in ECA (eqn 1). Why is this the right thing to do? It seems to me this could be quite harmful in highly imbalanced datasets, and that this might be part of what is driving the subpar empirical results on many of the experiments.\n\n* The authors should probably cite and discuss the following; it seems relevant to e.g. Sec 4.2: Khani, Fereshte, Aditi Raghunathan, and Percy Liang. \"Maximum weighted loss discrepancy.\" arXiv preprint arXiv:1906.03518 (2019).\n\n* It is often unclear what the authors mean by \"directly\" optimizing for group fairness (e.g. \"both...do not use DRO as a direct tool for achieving group fairness\"). Please clarify.\n\n* \"Debiasing\" is mentioned as an application at least twice; I'm not sure this is an application. I believe \"image classification\" would be a better description of the application in the work cited as a debiasing paper.\n\n* How is the \"arg max\" in Equation (11) solved? It is not clear.\n\n* All of the tables are way too small. Please revise.\n\n* Why are the x-axes in Figures 1,2 both reversed?\n\n* The y=0 curve for Fig. 3 are hidden away in Appendix D.4, but those results look qualitatively different -- oscillating wildly between extremes. Please comment on these results.\n\n## Typos etc.\n\nThere are *many* grammatical problems with the paper. I list a few here, but it needs a thorough revision by a native English speaker.\n\nAbstract: \"between the reweighting-\" -> \"between reweighting\"\n\nP1: \"in learning various decision-making\" -> \"in various decision-making\"\n\nP1: \"such application is\" -> \"such applications is\"\n\nP1: \"re-weighting sample weights\" -> \"re-weighting samples\"\n\nP1: \"To address such issue, the fairness-aware\" -> \"To address such issues, fairness-aware\"\n\nP2: \"in algorithmic fairness literature\" -> \"in the...\"\n\nP7 \"finds the better\" -> finds better",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the clarity is reasonable, despite many minor typos and grammatical issues. Experiments are described in sufficient detail for reproducibility, but a code release would help a great deal.",
            "summary_of_the_review": "Overall, this is an interesting and promising work, but it feels of borderline quality. It is nice to see DRO taken in the direction of more practical problems grounded directly in the fairness literature. I think that the main contribution of this work is on the theoretical side; on the empirical side, the results are quite mixed, and it is hard to disentangle the effects of some of the many implementation and experimental decisions on the empirical results. There is also not a clear framework for evaluation, which makes it even harder to say whether this method clearly \"works\" -- or whether it can be added to the pile of theoretically interesting, but practically unhelpful, robust optimization methods. I have other more minor concerns that are detailed above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2467/Reviewer_cPS3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2467/Reviewer_cPS3"
        ]
    },
    {
        "id": "T7xtaMr_mAt",
        "original": null,
        "number": 2,
        "cdate": 1666458450513,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666458450513,
        "tmdate": 1666458450513,
        "tddate": null,
        "forum": "Q-WfHzmiG9m",
        "replyto": "Q-WfHzmiG9m",
        "invitation": "ICLR.cc/2023/Conference/Paper2467/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper seeks to enforce the group fairness metric \"Difference of  Conditional Accuracy\", a generalization of the equalized odds criterion, and points out an interesting connection between optimizing this metric and (class-wise) distributionally robust optimization (DRO). Specifically, they show that the criterion can be approximately written in terms of variances of group losses, and uses a result from Xie et al. (2010) to show an equivalence to DRO. Experiments on a number of benchmark datasets show case that the use class-wise DRO can indeed achieve better trade-offs between accuracy and DCA. ",
            "strength_and_weaknesses": "Pros:\n- The connection between enforcing DCA (which is typically imposed as constraints in the optimization problem) and DRO is pretty interesting. \n- The experimental results are elaborate enough to show case that DRO can indeed be used to achieve competitive DCA performance\n\nCons:\n- The paper seems to emphasize that the prescribed use of DRO allows one to tackle the non-differentiability of DCA. I think as the authors themselves point out, even after the DRO reformulation, the objective is still intractable, and one does have to use surrogates for at least one step of the min-max optimization. In fact, similar \"selective of use of surrogates\" is common even in the constrained optimization literature. For example, the approach of Cotter et al (2019) formulates a constrained optimization problem to enforce constraints such as equalized odds, and does so by using surrogates for only updates on the model parameters and uses the original 0-1 loss for the group weights. So in terms of being able to address the non-differentiability of DCA, I don't this paper offering a solution that is very different what already exists in the literature.\n\n- A important baseline that is missing is the use of constrained optimization in the above manner to enforce DCA. I see that authors include Zafar et al. (2017b), but this method uses covariance-based surrogates, which I doubt are as tight as the more common cross-entropy style losses. I think either Cotter et al. (2019), which uses surrogates in the same way as this paper, but does so through a Lagrangian formulation instead of DRO, is one compelling baseline to include. Agarwal et al (2018) is another compelling baseline, which comes with theoretical guarantees --  this method uses a Lagrangian formulation based on the 0-1 loss to generate example weights, and solves the resulting cost-weighted optimization problem using a standard cost-sensitive learning (where one could employ a surrogate of choice). From what I can tell, the techniques in both Cotter et al. and Agarwal et al. are amenable to metrics like DCA.\n\nRef:\nCotter et al., \"Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals\", JMLR 2019.\n\nMinor questions:\n- In the experiments, is there a reason why the \"Scratch\" method, which if I understand correctly seeks to optimize a classification loss without fairness constraints, doesn't always do the best on accuracy (e.g. Adult, UTK faces, Civil comments)?\n- In discussing prior re-weighting methods, you mention \"they lack sound theoretical justifications for enforcing group fairness\". I don't think this is particularly true of methods like Agarwal et al.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the connection between DCA and DRO is interesting. Even though both the objectives seek to enforce equitable performance across groups, establishing a precise connection is a nice contribution. The specific algorithm used in the experiments is not new.\n\nWhere the paper lacks is in discussion of how similar approaches to handling non-differentiability already exist in the group fairness literature (e.g. Cotter et al., Agarwal et al.), and a comparison with principled constrained optimization methods that more directly seek to enforce DCA.\n\n",
            "summary_of_the_review": "The paper makes an interesting connection between DCA and DRO, but requires a more elaborate discussion on some related methods and comparing against them in the experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2467/Reviewer_U6MV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2467/Reviewer_U6MV"
        ]
    },
    {
        "id": "WsAltiPROqa",
        "original": null,
        "number": 3,
        "cdate": 1666655601609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655601609,
        "tmdate": 1666655601609,
        "tddate": null,
        "forum": "Q-WfHzmiG9m",
        "replyto": "Q-WfHzmiG9m",
        "invitation": "ICLR.cc/2023/Conference/Paper2467/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors investigate the classification problem with the penalization of the difference in conditional accuracy. They provide a tight characterization between the empirical DCA and variance of 0-1 losses. By combining it with the result of Xie et al., they derive the group-wise distributionally robust optimization form, which is more tractable than the original form. They also provide an iterative algorithm for solving the group DRO form of their optimization by utilizing the closed-form solution of the group DRO part. The empirical comparison demonstrates that the proposed algorithm is competitive or outperforms the existing fair classification method for equal opportunity. Also, the ablation study shows the robustness of the proposed mechanisms. ",
            "strength_and_weaknesses": "Strength:\n- Clearly written and easy to follow\n- A tight connection between DCA and variance of losses is interesting and non-trivial.\n- The proposed algorithm is novel and reasonable.\n\nWeakness:\n- A lack of comparison with the reduction-based methods.\n- The empirical evaluations have some concerns.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors reveal a tight connection between the standard deviation of the losses and the DCA. This result, particularly the lower bound part, is non-trivial. Combining with Xie et al.'s result is an interesting idea, and the resulting algorithm is reasonable and theoretically grounded. \n\nI found a similarity between the present algorithm and reduction-based fair classification approaches, including\n- Agarwal et al. A Reductions Approach to Fair Classification. In ICML 2018.\n- Cotter et al. Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals. JMLR, 2019.\nThese algorithms also carry out alternating updates consisting of the update of the model parameter and the update of the group-wise scalar parameters. The authors should clarify the essential difference between the proposed and these algorithms.\n\nI have some concerns about the empirical evaluations.\n1. The authors state that the accuracy is measured using balanced classification accuracy. Using such a measure as assessing accuracy is no problem. However, the existing methods, including Scratch, might employ the standard (not balanced) loss as their objective function. It is unfair that only the proposed method minimizes the balanced loss directly, whereas the other methods might not. \n2. In Figures 1 and 2 (and related figures in the appendix), there are cases where FairDRO achieves higher accuracy than Scratch. It is very weird because Scratch optimizes the accuracy without a constraint of fairness. The authors should clarify why such phenomena happen.\n3. The authors report the minimum unfairness criterion while achieving at least 95% of the original accuracy. However, comparing this measure is not a good way because the accuracy is not aligned. It is better to compare the Pareto frontier between the comparison methods. \n4. The ablation study does not adequately demonstrate the necessity of each component. In Table 3, RVP achieves the best DCA in UTKFace, and FairDRO(w/o classwise) reaches the best DCA in Adult. Hence, it fails to adequately demonstrate the optimal performance of FairDRO, as claimed in the main body.\n\n\nMinor comments:\n- The convergence of Algorithm 1 is crucial property. It is better to clarify what assumptions are needed for convergence and discuss the satisfiability of these assumptions in standard situations.\n- The statement about $C$ in Corollary 1 is somewhat misleading. The paper says $C$ is some positive constant, which looks like there is a constant $C$ independent of the dataset. However, $C$ might be changed by the dataset, indeed. An accurate statement would be that for any dataset $D$, there is a corresponding constant $C$ such that theta achieves Eq. 7. It is better to revise the statement so that it does not mislead.",
            "summary_of_the_review": "This paper is well-written and easy to follow. The tight characterization between DCA and the standard deviation of the losses is interesting and significant. The developed algorithm is novel and reasonable. Hence, I recommend acceptance at this point. However, the paper lacks a comparison with the crucial related work. I also have a concern about the empirical evaluations. I'd like the authors to address these concerns.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2467/Reviewer_QUdb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2467/Reviewer_QUdb"
        ]
    },
    {
        "id": "cqzVF76xMU",
        "original": null,
        "number": 4,
        "cdate": 1666686881369,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686881369,
        "tmdate": 1670447772878,
        "tddate": null,
        "forum": "Q-WfHzmiG9m",
        "replyto": "Q-WfHzmiG9m",
        "invitation": "ICLR.cc/2023/Conference/Paper2467/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an in-processing based method to design a fair classifier for the metric difference of conditional accuracy (DCA). First, the authors show that empirical DCA can be approximated by the root of empirical group variance. This suggests using the root of empirical group variance as a regularizer in the learning objective. The authors then use a characterization due to [Xie et. al., 2020] to convert such a regularized ERM objective to a group distributional robust optimization.\n\nThe authors provide an iterative method to solve the fair DRO problem where the min player $\\theta$ uses gradient descent and the max player $q$ uses smoothed best response. This algorithm is evaluated on several datasets (tabular, vision, and language) and compared with several reweighing and regularization based methods. The results show that compared to the existing fair classifiers, the proposed method achieves better trade-off between accuracy and DCA.",
            "strength_and_weaknesses": "Strengths:\n- The connection between DCA and empirical variance is interesting and I wonder if one can show similar connections for other fairness measures.\n- The experimental results clearly show that the proposed method pareto dominates several other in-processing based methods.\n\nWeaknesses:\n- The proposed approach seems limited to difference of conditional accuracy (DCA) and it is not clear how this approach can be generalized to other fairness metrics. In fact, the main algorithm is motivated by proposition 1 which shows that empirical DCA can be approximated by the square root of empirical variance.\n- The authors claim that the iterative method for solving fair-DRO is efficient. However, I didn't see any convergence guarantees for the proposed method.\n- The authors also emphasized the use of exact regularization in the learning objective. However, this approach cannot guarantee any desired level of fairness in the training/test set. In particular, if one wants to output a classifier with fairness violation at most $\\alpha$, the current approach seems to require a lot of hyperparameter tuning.\n- There is no result on generalization i.e. how does the fairness guarantee and accuracy generalizes to unseen datasets. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well-written, and the connection with prior work is highlighted. \n\nQuality and Novelty: I doubt if the proposed approach is applicable beyond the fairness measure DCA. In terms of novelty, the authors make the connection between DCA and root of empirical variance. However, the solution of the regularized ERM uses the group DRO formulation shown in [Xie et. al. 2020]. This is the only step where distributionally robust optimization comes into the picture, and I wouldn't call such an application of DRO well motivated.",
            "summary_of_the_review": "Overall, the authors present an interesting approach to design a fair classifier and the experimental results demonstrate that the proposed method pareto dominates existing fair classifiers on several datasets. However, I wonder if the proposed approach works beyond the fairness metric DCA. Moreover, the main connection with DRO relies on past literature [Xie et. al. 2020] and the proposed method lacks convergence and generalization guarantees.\n\nUpdate after rebuttal: Many thanks to the authors for the detailed response. It seems to me that the connection between fair classification and DRO is indeed non-trivial, as pointed out by other reviewers. Additionally, the new experimental results look convincing to me. Furthermore, the fairness metric DCA covers the more interesting fairness metrics for binary classification. But I hope the authors add some discussion on how generalizable this approach is to other fairness measures, particularly for multi-class classification. Overall I now feel positive about the paper and will increase my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2467/Reviewer_uGcU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2467/Reviewer_uGcU"
        ]
    }
]