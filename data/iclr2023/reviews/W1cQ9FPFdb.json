[
    {
        "id": "3uHIR655yc",
        "original": null,
        "number": 1,
        "cdate": 1666338007553,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666338007553,
        "tmdate": 1666730765602,
        "tddate": null,
        "forum": "W1cQ9FPFdb",
        "replyto": "W1cQ9FPFdb",
        "invitation": "ICLR.cc/2023/Conference/Paper5176/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a tail averaging method for stochastic optimization that maintains two averages of different lengths in order to adaptively choose tail averging length.\nThe average produced by the longer of the two averages is provably close to optimal in length every once in a while.\nThe advantage of the proposed method over existing tail averaging methods is that it does not have tail averaging length as a hyperparameter.\nThe only hyperparameter is the number of iterations that pass between evaluating and comparing the two averages and the proposed method is robust to larger changes of this parameter.\nThe paper provides a theoretical analysis of the algorithm under a set of assumptions and argues how applicable the assumptions are in practice.\nThe paper also provides a demonstration that the method works well in practice also compared to other iterate averaging approaches for a toy example using an LM on Penn Treebank.",
            "strength_and_weaknesses": "Strength:\n- Paper does a great job explaining the algorithm itself and also the motivation for it\n- Assumptions are clearly stated and even though they seem to be too strict for a nonconvex setting they should still be motivated in practice close to optimum and otherwise treated by the heuristics\n- Great figures, they take some time to think about but then make sense and illustrate useful properties once you understand them\n- Figure 3 nicely demonstrates that the approach competes with hyperparameter tuned alternatives but in a purely adaptive fashion\n- Figure 5 nicely demonstrates the TTA has the potential to improve over not using any tail averaging\n- Figure 6 nicely demonstrates the effect of a required reset heuristic from nonconvexity and again how much too-short-averaging-length hurts the validation loss\n\nWeakness:\n- Authors fairly point out that the analysis would become more cumbersome when trying to make a statement about the worst case difference between f^O and f^L but explain why this should not be a problem in practice. Under simplified assumptions where f is convex extending the analysis could potentially not be as cumbersome? Perhaps the loss is also monotonous in the averaging length before the averaging length becomes suboptimal? Having this analysis would present a nice addition for the paper making the theoretical analysis even more high quality.\n- Figure quality (e.g. of Figure 3, Figure 5) could be improved (the numbers on the axes look blury)\n- Would be nice to see the experimental evaluation also on modern architectures and in general on a broader set of evaluations\n- The structure of the experiments section could be clearer by separating the different research questions into separate sections making the presentation cleaner\n\nQuestions:\n- How much wall-clock time cost is added in practice by maintaining the additional sets of weights?\n- In this paper about anytime tail averaging: https://arxiv.org/pdf/1902.05083.pdf the averaging length k seems to depend on t (k=ct) which makes maintaining the average more expensive and still requires tuning c. But such approach does not seem to be used for comparison in the author's paper / they seem to compare to a constant averaging length \"only\". Does the k=ct approach behave qualitatively differen\ntly / more similar to the author's proposed adaptive method before the last iterate for which it was tuned?\n- The authors mention that tail averaging performs similarly to learning rate decay. Are there comparable adaptive methods for learning rate decay and how does the presented method compare in terms of com\nputational cost (memory and wall-clock time)?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written clearly and the analysis is elegant and understandable\n\nQuality: The paper presents a high quality theoretical analysis, the experiments section could be improved by being a bit more extensive\n\nNovelty: I'm not familiar with the tail averaging literature so I'm not sure about this point but to me the presented method seems like novel and original work \n\nReproducability: The focus of the paper is on the algorithm and theory which is clearly presented and reproducable. The presented experiment is a typical setup which could be easily reproduced with simila\nr models and the same dataset which is publicly available.",
            "summary_of_the_review": "I recommend to accept the paper due to the clear and elegant theoretical contribution that adds to the tail averaging literature, an important area of stochastic optimization, and the method is also directly applicable and practically feasible for many different machine learning applications.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5176/Reviewer_bawy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5176/Reviewer_bawy"
        ]
    },
    {
        "id": "0Y5M1FgxGpj",
        "original": null,
        "number": 2,
        "cdate": 1666730321110,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666730321110,
        "tmdate": 1669314405471,
        "tddate": null,
        "forum": "W1cQ9FPFdb",
        "replyto": "W1cQ9FPFdb",
        "invitation": "ICLR.cc/2023/Conference/Paper5176/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This submission proposes a new iterative averaging scheme for stochastic\ngradient methods. \nThe scheme, which works by maintaining two iterate averages\nat any given time, manages to be parameter free by setting the average length\nparameters dynamically based on the validation loss. \nAs such, it is parameter-free unlike suffix/tail averaging, where the tail \nlength parameter must be specified a-priori.\nThe authors prove that their \"two-tailed\" averaging scheme is optimal at\nspecific iterates infinitely often (\"once in a while\") under assumptions\non the behavior of the validation loss with respect to the length of the \naverage.\nThe submission concludes with several experiments on the Penn Treebank dataset.",
            "strength_and_weaknesses": "The main strength of the submission is the relative simplicity and elegance of\nthe averaging method.\nBy maintaining two averages --- one over a short time scale and one over a long time scale ---\nthe averaging scheme can capture both local improvement in the objective\nand long-term behavior of the optimizer. \nI think it is possible that the length of the short/long intervals could be\nset to give near-optimal anytime convergence rates for the training objective,\ne.g. something like Theorem 5 of Rakhlin et al. [1].\n\nThe major weaknesses of the submission are the following: (i) the theoretical\njustification for two-tailed averaging is unrealistic and \n(ii) the experimental evaluation is to small in scope to justify the \nmethod without strong theory. \n\n**Theoretical Justification**: The theory in this submission does not justify\nthe proposed method.\nMy major complaint here is that the authors assume exactly the properties of \nthe validation loss that they need for their averaging scheme to work.\nIn particular, Assumptions 1 --- monotonic behavior of the validation loss \nwith respect to the length of the average --- is deeply unrealistic for several\nreasons:\n\n- It is unlikely to be satisfied for stochastic optimization of non-convex models,\n    where monotone improvement of any train/validation/test metric is unlikely.\n- It makes a very strong assumption on the connection between the training objective\n    and the validation objective. That is, progress on the training objective\n    should roughly equate to progress on the validation metric. \n\nBecause of the weakness of the theoretical analysis, I can only evaluate\ntwo-tailed averaging as a heuristic method for setting the suffix parameter.\n\n**Empirical Evaluation**: The empirical evaluation is small scale and does\nnot investigate the assumptions on which the theory is based.\nOnly two experiments are presented, both on Penn treebank.\nNo comparison is made against other anytime averaging methods \n(see \"Missing References\"), making it difficult to establish a baseline for\nthe performance of two-tailed averaging.\n\nWhile two-tailed averaging performs well on Penn Treebank, one experiment does \nnot justify it's wide-spread use.\nIn particular, I disagree with the authors statement that two-tailed averaging \"is approximately equivalent \nto a series of TA which were tuned for each evaluation step separately\" and thus requires no other experiments;\nthis only holds under their assumptions.\nWhile the assumptions were tested via downstream metrics like behavior\nof the average lengths, this is only (approximately) addresses Assumption 3. \nChecking the assumptions properly requires carefully plotting the behavior of\nthe validation loss and optimal average length on multiple heterogeneous datasets.\n\nFinally, some experiments appear to be omitted from the manuscript.  \nSpecifically, the authors say: \"To explore the effect of the evaluation period E, we tuned models with four times larger and four times smaller E than in our previously discussed experiments.\"\nI do not see these results presented anywhere, so we can only take the authors\non their word that $E$ doesn't strongly affect performance. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The paper is generally well written. Some of theoretical derivations were\ndifficult to follow because of poor notation choices (see \"Minor Comments\").\n\n**Reproducibility**: The experiments are not reproducible with the information\nprovided. Batch-size and other critical details are missing for the experiments in Figures 3/4.\n\n**Missing References**:\n- Tail averaging as described by Jain et al. (2018) is functionally identical to\n    suffix averaging, which was proposed by Rakhlin et al. [1] in 2012. \n    Rakhlin et al. proved that suffix averaging is optimal for strongly convex\n    stochastic optimization. I'm not certain why Jain et al. don't discuss \n    the work by Rahklin et al., given that they even note tail averaging is \n    the same method on page 4 of their work.\n    Regardless, this highly related branch of work should be properly discussed\n    here.\n\n- Several other works have developed anytime variants of suffix averaging which\n    are also optimal. For example, polynomial-decay averaging [2] is easy to compute\n    and also obtains the optimal convergence rate. Lacoste-Julien et al. [3]\n    provide yet another optimal anytime averaging scheme with a simpler proof\n    of convergence than that for polynomial-decay averaging.\n\n**Novelty**: The two-tailed averaging method is new to the literature as far as I am aware.\n\n### Minor Comments\n\n- Equation 2: Note that Polyak-Rupert averaging is easily implemented as an EMA using \n    $\\beta_t = t/(t+1)$. The optimal scheme of Lacoste-Julien et al. [3] is also easily\n    implemented in this fashion.\n\n**Related Works**: \n- \"In summary, existing averaging methods which behave well in practice all have one or more hyper- parameters to govern the weighting of early iterates.\" --- this is not necessarily true. See \"Missing References\".\n- \"these methods are not flexible enough to estimate the optimal average at multiple optimization steps in general\" --- this is not true once the missing references are considered. \n\n**Problem Statement**:\n- The notation $O(t)$ for optimal averaging length is somewhat confusing due to the clash with standard \"Big-O\" notation.\n\n**Analysis**:\n- Given assumption 3, doesn't assumption 4 simply imply the conclusion holds for all $E$? I don't see why the precondition is even introduced here given assumption 3.\n- Figure 2: Is this generated from a real optimization problem, or is it just a synthetic example? \n- What is the notation $E | S(n)$ in Prop. 2 (i)? It doesn't appear to be introduced anywhere. \n\n### References\n\n[1] Rakhlin, Alexander, Ohad Shamir, and Karthik Sridharan. \"Making gradient descent optimal for strongly convex stochastic optimization.\" arXiv preprint arXiv:1109.5647 (2011).\n\n[2] Shamir, Ohad, and Tong Zhang. \"Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes.\" International conference on machine learning. PMLR, 2013. \n\n[3] Lacoste-Julien, Simon, Mark Schmidt, and Francis Bach. \"A simpler approach to obtaining an O (1/t) convergence rate for the projected stochastic subgradient method.\" arXiv preprint arXiv:1212.2002 (2012).",
            "summary_of_the_review": "=== Update ===\n\nI appreciate the discussion with the authors, who have clarified several issues in the manuscript. I think the paper is improved, but I remain skeptical of the main assumptions required for the theoretical analysis. In particular, I think that Assumption 1 is not justified and I am reluctant to accept this submission without (i) an in-depth experimental justification for the assumption, or (ii) a revised theoretical analysis. Although I would normally increase my score to four reflect our discussion, I cannot due to the review form. As such, I will maintain my score of 3.\n\n=====\n\nThis submission is an interesting attempt to develop anytime methods for iterate \naveraging. I like two-tailed averaging scheme; perhaps it can be shown to admit\na near optimal convergence rate for the training loss.\nInstead of pursing this direction, the authors rely on difficult-to-trust\nassumptions on the validation for their theoretical analysis, which shows that the averaging\nscheme is once-in-a-while optimal.\nAs the experiments are limited to a single dataset and do not verify the assumptions, \nI feel the proposed method is not well-justified by theory or experiment. \nI recommend the submission be rejected.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5176/Reviewer_NMLB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5176/Reviewer_NMLB"
        ]
    },
    {
        "id": "OlkBlD-qgGF",
        "original": null,
        "number": 3,
        "cdate": 1666832235031,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666832235031,
        "tmdate": 1666832235031,
        "tddate": null,
        "forum": "W1cQ9FPFdb",
        "replyto": "W1cQ9FPFdb",
        "invitation": "ICLR.cc/2023/Conference/Paper5176/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Tail-averaging is a very useful trick, in both theory and practice, to improve the performance of iterative learning/optimization method. Yet, it introduces an important hyperparameter, the starting point for the tail-averaging. This work proposes a two-tailed averaging method as an adaptive and hyperparameter-free alternative to the tail-averaging method. Under suitable conditions, it is proved that two-tailed averaging has many good properties, inclduing being (1) anytime, (2) adaptive, and (3) optimal once in a while. Besides to the method and theory, some experiments are conducted to verify the advantage of two-tailed averaging. ",
            "strength_and_weaknesses": "# Strength \n1. Overall I think the paper is written well. I find the problem well motivated, that treating with the hyperparameter in tail-averaging might be painful, so we need a fix to that. \n2.  The proposed two-tailed averaging is novel to my knowledge. \n3. Both theory and experiments are presented for better demonstrating the advantages of  two-tailed averaging. \n\n# Weakness\n1. It is claimed that two-tailed averaging is \"adaptive\" and \"hyperparameter-free\". However I find this a bit misleading, because the procedure needs to be able to evaluate the \"loss\" periodically. Note that the \"loss\" refers to the targeted objective to be minimized if I understand correctly. For example, in the context of stochastic optimization [Jain et al, 2018], the \"loss\" refers to the population loss. But being able to evaluate the true loss is a very strong requirement, and makes the point of \"hyperparameter-free\" less interesting. For example, one can easily come up with a variant of tail-averaging that is \"hyperparameter-free\" provided the ability to evaluate the loss. Let us consider the setting of [Jain et al, 2018] again, one can simply save all the SGD iterates, and evaluate the loss over all possibilities of tail-averaging (or any general weighted averaging of the iterates), and output the optimal one --- this is \"hyperparameter-free\" and \"optimal\". I would like to see some discussions on the requirement of accessing the true loss. I just think this is too strong in many settings. \n\n\n2. I think the current presentation is somewhat \"abstract\". Perhaps the authors can discuss a potential application of the proposed two-tailed averaging in some theory/application problem? As the authors mentioned [Jain et al, 2018] many times, could you formally state a bound for two-tailed averaging in the setting of learning least squares with SGD, and compare it to the one obtained by tail-averaging? Being \"anytime\", \"adaptive\" and \"optimal once in a while\" sounds very cool but I would like to see some more specific benefits of using two-tailed averaging. \n\n\n3. I also think the assumptions could have been checked with a specific exemplar problem. The current assumptions are just very abstract and I do not immediately see how applicable/realistic they are. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is fine. \n\nQuality needs some discussion. Please see my questions above. \n\nNovelty is good. \n\nReproducibility is good. ",
            "summary_of_the_review": "Please see above. Due to the mentioned issues I would like to keep my score low. I would like to hear the authors rebuttal to see whether or not I have misunderstood anything. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5176/Reviewer_bYZd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5176/Reviewer_bYZd"
        ]
    }
]