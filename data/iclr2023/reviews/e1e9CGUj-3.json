[
    {
        "id": "Ebv2nGTNv9V",
        "original": null,
        "number": 1,
        "cdate": 1665990455669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665990455669,
        "tmdate": 1668835559483,
        "tddate": null,
        "forum": "e1e9CGUj-3",
        "replyto": "e1e9CGUj-3",
        "invitation": "ICLR.cc/2023/Conference/Paper150/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed a new method, named TT-NF, for learning neural fields in terms of tensor train networks. Technically, a storage-efficient algorithm for sampling from TT-NF was introduced: the storage complexity is reduced from O(BDR^2) to O(BDR), where R denotes the maximum bond dimension (TT-ranks). Empirically, the new method was applied to the task of neural radiance fields. The numerical results showed that the proposed methods achieve competitive performance with SOTAs.",
            "strength_and_weaknesses": "++Strength\n\nThe part of \u201csampling from TT-NF\u201d introduced a practically valuable trick to reduce the storage complexity when calculating the (partial) contraction of tensor networks.\n\n\u2014 Weakness\n\n1. Although a lot of literature was overviewed in the \u201cRelated work\u201d part, for each highlighted aspect, the overview is not sufficient, especially for the parts \u201ctensor decomposition\u201d and \u201cneural network compression with tensors.\u201d\n2. The novelty is relatively weak. In Section 4 \u2014 \u201cMethod\u201d \u2014 the paper introduced an initialization of TT-NF in Sec 4.1 and the sampling trick from TT-NF in Sec 4.1. For the former, the variance-preserving core initilization has been discussed in (Pan Y et al., 2022); for the latter, it is practically useful (mentioned in Strength), but the trick seems trivial in my own opinion.\n\nPan, Yu, et al. \"A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks.\"\u00a0International Conference on Machine Learning. PMLR, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality:**\n\n1. Following Eq. (2), there should be proofs to say why $\\hat{\\sigma}$ guarantees the variance of the full tensor.\n2. In the experiment of tensor denoising, the proposed method outperforms TT-SVD and others, but there is no discussion on where the advantage comes from. One reason, I guess, is that the TT-SVD assumes the semi-orthogonality of the TT-cores, but the proposed method does not. It makes the TT-SVD is representationally weaker than the proposed one due to the additional constraints. Therefore, I doubt if the experiment of tensor denoising in the paper can support the central claim properly.\n3. For the second experiment of neural radiance fields, Tab 2 shows that the proposed method is only comparable with the existing methods. I am thus confused about the main advantages of the new methods then. If there is another advantage, such as computational efficiency or interpretability, it should be clearly mentioned in this part.\n\n=========================\n*Update*:\n1. \" One reason, I guess, is that the TT-SVD assumes the semi-orthogonality of the TT-cores, but the proposed method does not. It makes the TT-SVD is representationally weaker than the proposed one due to the additional constraints.\"\n\n-- I am convinced that the additional semi-orthogonality will not affect the representational power of the TT model. In other words, it is fair to say the TT-SVD, TT-OI, Contraction and the proposed v2 and v3 in Fig. 4 have similar representational power, and the main differences among these methods are optimization schemes. Based on this point, it is not clearly explained why v2/3 outperforms other methods a lot in Fig. 4. Although the authors mentioned that Contraction might get stuck in saddle points, it cannot convince me due to the lack of more detailed investigation. \n\n**Novelty:**\n\nPlease check the \u201cStrength and Weakness\u201d section.\n\n**Reproducibility**\n\nThe experimental settings were introduced clearly.",
            "summary_of_the_review": "The trick on \u201csampling from TT-NF\u201d is practically useful for other tensor researchers to implement their codes on computers more efficiently. The drawbacks of this paper are also obvious: (relatively) weak novelty and unconvincing experimental results. My recommendation is thus on the boundary.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper150/Reviewer_wzYM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper150/Reviewer_wzYM"
        ]
    },
    {
        "id": "bG0RfnPmzx7",
        "original": null,
        "number": 2,
        "cdate": 1666733325409,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666733325409,
        "tmdate": 1666739959071,
        "tddate": null,
        "forum": "e1e9CGUj-3",
        "replyto": "e1e9CGUj-3",
        "invitation": "ICLR.cc/2023/Conference/Paper150/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes TT-NF, a low-rank representation for learning neural fields, along with a sampling method for improve the training efficiency. It applies TT-NF to a synthetic tensor denoising task and a neural radiance field rendering task, which demonstrate the performance of the approach.",
            "strength_and_weaknesses": " Strength:\n\n- The idea of incorporating tensor train decomposition to neural fields is exciting, albeit it is a natural way when thinking of different tensor decompositions beyond the CP/VM ones proposed in Tensorf [Chen et al. 2022]\n\n- The sampling algorithm does alleviate the computational issue with the TT formulation\n\nWeakness:\n\n- One major issue of using such TT form is that it involves many matrix multiplications instead of the element-wise multiplications used in CP/VM decomposition as in [Chen et al. 2022]. Therefore, the computational complexity of a single sampling point is O(Dr^3) compared with O(Dr) in CP, where r is the internal rank. Based on my understanding, the sampling method only aims for reducing the space complexity during the inference, and relaxes the matrix multiplication to vector-matrix multiplication. However, the vector-matrix multiplication is still **inefficient** compared with the point-wise multiplication in CP/VM as in [Chen et al. 2022]. Moreover, for back-propagation during training, I believe this would take more GPU memory compared with CP/VM.\n\n- The experimental results on Synthetic NeRF dataset is not convincing. In particular, it apparently has inferior performance than TensoRF in both SSIM and LPIPS. Even for PSNR, I don't see a clear advantage since in the original paper of TensoRF the reported quantitative performance is much higher than that of reported here. Although it is said in the paper that QTT-NF intentionally avoid voxel pruning, total variation loss, etc, I don't think this is a reasonable argument since we can also adopt the same practice here to see if QTT-NF can achieve the similar results under the same setting. Moreover, I know that VM decomposition does a great job in rendering, which is exactly the novelty of TensoRF, and thus the paper misses quantitative comparisons with this representation, making the comparisons not convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clear, and should be reproducible.",
            "summary_of_the_review": "Although the paper has its novelty in applying TT to NF, my major concerns are the efficiency of TT (matrix multiplication vs point-wise multiplication) and its final performance, as discussed above. I think the current results are not convincing to me that TT-NF is superior than TensoRF.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper150/Reviewer_woGR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper150/Reviewer_woGR"
        ]
    },
    {
        "id": "2xPyMpj9DV",
        "original": null,
        "number": 3,
        "cdate": 1666967037915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666967037915,
        "tmdate": 1671100080604,
        "tddate": null,
        "forum": "e1e9CGUj-3",
        "replyto": "e1e9CGUj-3",
        "invitation": "ICLR.cc/2023/Conference/Paper150/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new way to sample tensor trains via efficient linear layers in deep learning frameworks. By further leveraging automatic differentiation, they propose to fit tensor trains to data by stochastic gradient descent and backpropagation. This gives a method that is robust to noise and that can be sampled at arbitrary batches of indices. The authors apply their sampling and fitting ideas to tensor denoising where it outperforms existing tensor decomposition methods. They further experiment with novel view synthesis and show improvements over a \"bare-metal\" version of a recent tensor based framework.\n",
            "strength_and_weaknesses": "The manuscript studies an important and very hot topic through the lens of efficient low-rank tensor decompositions. The authors' implementation indeed exhibits better robustness to noise than earlier decomposition algorithms, while offering more flexibility. The idea to use automatic differentiation in this task seems like the right thing to do, and the overall idea to use structured tensors to represent natural images and scenes is intriguing.\n\nOn the critical side, I am uncertain about the level of contribution. The idea to use structured tensors for radiance fields and novel view synthesis has been proposed before. The same is true for the tensor train ideas. Solving non-convex factorization problems by gradient-based optimization is also by now standard. I feel that many relevant comparisons are missing (please see below) and in the one with TensoRF they \"cripple\" the baseline to only compare the tensor-related algorithms. I can see why one may do that but it makes it unclear in what situation one should opportunely use QTT-NF. Some related questions below.\n\n### Various questions and comments\n\n- I am a bit confused by the first paragraph in the introduction: \"Since, in extreme cases, the dimensionality of such fields can exceed the memory size of a typical computer by several orders of magnitude, we look at the problem of learning such fields from the angle of stochastic methods.\"\nI suppose you mean dimensionality in some raw representation? But this depends on the chosen resolution. Since the underlying objects are continuous the resolution can be chosen as high as one desires but without introducing new information, so the notion of dimensionality is relative (as you show, these fields can be compressed). Can you back this up by concrete examples from practice?\n\n- In the second paragraph you write \"Both methods operate under the assumption of noise-free data and are not guaranteed to output sufficiently good approximations in the presence of noise.\"---this statement implies that data is regular in some predetermined sense and low-rank tensors respect this regularity. \"Noise\" would then be any excursion from this model. But there exists a great variety of models for images and other data---can you give an intuitive explanation for why TT-NF or any other tensor decomposition method should be the right choice?\n\n- On p4, first paragraph, you write that \"... $\\cdot$ denotes merged (flattened) dimensions\" but there are no $\\cdot$s in the hierarchical expression a few lines up.\n\n- What exactly does it mean that some algorithms \"don't support noise in observations\"? What happens when there _is_ noise? In particular, since this work is purely algorithmic, how would non-sub-gaussianity of noise manifest with TT-OI? (From Figure 4 it seems that TT-OI does a bit better than TT-SVD which according to your classification should not \"support\" noise.)\n\n- is there a principled way to choose R_max in practice? is it possible to dynamically increase rank (e.g., if the loss stops decreasing at a value that is too high?)\n\n- what is the influence of initialization (2) on the final result? \n\n- This is to some degree semantics, but a lot of (most?) recent work on neural fields is about representations that can be queried at a continuum of input coordinates. There are many good reasons this is convenient, one important one is that it is easy to interface them with the various PDE solvers which evaluate spatial derivatives (often adaptively). Unless I am missing something, this is not possible with TT-NF. So TT-NF are primarily useful for compression, or some sort of regularization via the implicit multiscale inductive bias (from structured low-rankness).\n\n- I find that the prose lacks clarity. A lot of it is too wordy (e.g. \"... in the presence of access to full tensor elements...\") and I had great difficulties following certain parts of the technical exposition. A case in point is the description of \"v2\" sampling scheme. (Up to this point I understood what is going on.) What does it mean to produce a batch of intermediates v? In what sense do linear layers require the samples to be packed densely? What is the alternative? Does overline over 2, D signify a range set? The statement \"... we split the inputs v according to which M_i slices...\". (After some effort I think I understand this part, but it should be written in a way that is friendlier to readers who only occassionaly dabble in tensor decompositions.)\n\n- In Section 5.2, I understand the motivation to turn off the various techniques TensoRF uses to improve results, but then I find it a bit unfair to criticise it for grid alignment artifacts, at least without proper hedging. The method was designed as a package with a reason. For a fully transparent comparison I would like to see results for TensoRF with all the bells and whistles turned on, as well as results where neither TensoRF nor QTT-NF use the \"tiny MLP\".\n\n- Further, it would be good to see comparisons with NERF or some state of the art neural-network based neural field on the rendering task. For full transparency, I would be interested in times (in addition to flops) required to fit some good neural model and QTT-NF on this task. I would also like to see comparisons like in Figure 3 for TT-{SVD, OI, cross}, perhaps also with the timings.\n\n- Related to the above, I find it strange to use the term neural field for a tensor-based method which does not use a neural network.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Per my comments above, I think that clarity could be improved, but the majority of the manuscript is reasonably clear. I have some reservations about novelty which I also expressed above. The results seem reproducible with the code provided in the supplement (I have not attempted to run it myself.)",
            "summary_of_the_review": "I think this is a nice paper (modulo a couple of editorial remarks) and the premise of using structured tensors to represent images and scenes is exciting. But currently I also feel that the  novelty is not quite at the ICLR level. This is compounded by some lack of transparency in the numerics.\n\n## post-rebuttal (copy)\n\nThank you for the responses to my critical remarks. \n\nAbout the first one: I don't think that suggesting that any of the reviewers advocate for a \"monoculture\" is a great move. \n\nThe comments of Ivan and Rafael adopt a better strategy which is to clarify the contributions of your paper. A good paper needs good results and good presentation. After reading your responses and Ivan's and Rafael's comments, I do think that I misjudged the novelty of your contribution. (Although I still think it is specialist and not at the level of a clear accept.)\n\nRegarding the presentation, it is certainly possible that all reviewers made comments about neural fields, initializations, experiments, ..., because they are advocates of monocultures. But it could also be because the way you structured the story puts too much emphasis on these themes. Your TL;DR is not \"We remove important bottlenecks in tensor decomposition algorithms by doing this and that\" but \"We repurpose the tensor train decomposition to learning compressed neural fields via backpropagation through samples.\" The abstract and the introduction share this emphasis.\n\nSince I do feel that I mischaracterized the novelty (thanks, again, to the remarks of Ivan and Rafael) I will increase my score to a 6. But I will also reiterate my proposal that before employing lofty high-road arguments like \"if you have it your way machine learning will be a monoculture!\", the authors may consider the possibility that a different way to tell the story might have led to different discussions.\n\nWith regard to this last point, I don't think that a revised manuscript has yet been uploaded. I hope that the authors will consider restructuring the story to better emphasize the contributions if the paper is accepted.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper150/Reviewer_2ByK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper150/Reviewer_2ByK"
        ]
    }
]