[
    {
        "id": "pM4re5dcRnO",
        "original": null,
        "number": 1,
        "cdate": 1666390584104,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666390584104,
        "tmdate": 1666390584104,
        "tddate": null,
        "forum": "3LUxNRrhK1",
        "replyto": "3LUxNRrhK1",
        "invitation": "ICLR.cc/2023/Conference/Paper4770/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper applies predictive coding (PC)\u2014an alternative to backpropagation\u2014to graph neural networks, and attempts to show an improvement in robustness to adversarial attacks. The authors describe the predictive-coding scheme for graph convolutional networks (GCNs), and evaluate robustness against several forms of adversarial attacks compared to other GNNs.",
            "strength_and_weaknesses": "This paper applies PC onto GCNs. It is an interesting idea and is a natural next step (after previous work), and there are analyses which quantify robustness. The authors have done comparisons across several tasks and datasets.\n\nBelow are some weaknesses which I feel detract from the paper\u2019s main points:\n### There is little intuition or exploration on why PC might improve robustness\nAlthough existing works on PC and on classifier energy are cited, it is not well explained why applying PC to models like GCNs might improve robustness against adversarial attacks. The authors offer some minor amount of justification based on the concept of \u201clow-pass filtering\u201d in the abstract, but this is not really expanded upon significantly. It is unclear why one might expect PC to be a reasonable technique for improving robustness (relative to backpropagation).\n### The analyses on predictive performance and ECE/MCE are only against severely weakened GCNs\nThe analyses on predictive performance (Tables 1 and 2) and the reliability diagrams only compare GPCNs (GCNs trained with PC) against vanilla GCNs which are not trained with things like normalization. It is rather well-known that normalization is very important for GCNs, and so these comparisons are rather unfair\u2014the GCNs are severely weakened and so these results are not particularly compelling. It would be more reasonable to compare with a standard GCN trained with dropout/normalization.\n### Comparisons in robustness are essentially only against a single method, where other more recent methods exist\nAlthough many methods for improving robustness in GNNs exist (the authors cite several in Section 5), the comparisons on robustness really only compare GPCNs with GCNs, RGCNs, or GATs, and not consistently. In these comparisons, RGCN is the only model I would expect would be a fair baseline for robustness. GCNs and GATs are less meant for robustness, although Table 4 shows that GATs oftentimes outperform GPCNs in terms of robustness anyways.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper demonstrates limited technical novelty, as it largely reapplies existing work on predictive coding on graphs, onto GCNs. The authors do attempt to show that this may improve robustness against adversarial attacks.\n\nThe paper suffers from several issues in clarity and in writing. Below is an incomplete list:\n- BP is never defined as \u201cbackpropagation\u201d\n- In Section 2, the update function is not defined\n- In Eq. 4, $x$ should be $h$, and in Eq. 5, $\\theta$ should be $w$\n- \u201cWe follow the formulation of PG graphs\u201d should be \u201cPC\u201d, I believe\n- I think the citation next to \u201cGCN\u201d in Table 3 is a mistake\n- The model architecture is not entirely clear from the supplement (more in-depth explanation would be nice), although the table of hyperparameters is much appreciated\n- There is a broken figure link in Appendix D\n- There are many grammatical errors in the writing, which should be fixed",
            "summary_of_the_review": "The paper offers an interesting application of predictive coding (PC) for GNNs, and attempts to show that GCNs trained with predictive coding rather than traditional backpropagation are more robust against adversarial attacks. Unfortunately, the comparisons in robustness are only done against a single method (i.e. RGCN), even though several other methods exist. Since the models trained with PC are not confidently outperforming existing methods in terms of predictive performance or robustness, this detracts a lot from the main point of the paper. If the paper were to be rewritten more as an exploration of PC on GCNs rather than a novel way to improve robustness, it would be more compelling as a piece of scientific work. Unfortunately, the paper also suffers from numerous issues in terms of clarity and writing, which make it difficult to support its passage.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4770/Reviewer_evdf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4770/Reviewer_evdf"
        ]
    },
    {
        "id": "w-mAY-6QSN",
        "original": null,
        "number": 2,
        "cdate": 1666571092524,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666571092524,
        "tmdate": 1666571092524,
        "tddate": null,
        "forum": "3LUxNRrhK1",
        "replyto": "3LUxNRrhK1",
        "invitation": "ICLR.cc/2023/Conference/Paper4770/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper address the problem of GNNs being vulnerable to imperceptible adversarial attacks and also has some issues in generalizing out-of-distribution data. The proposed solution, graph predictive coding network (GPCN), uses a novel message-passing scheme developed based on the theory of predictive coding. The authors argue that their approach enhances the robustness of the learned representations. Their experiments show that the representations learned are more robust to adversarial attacks as well as deliver improved results over the GCN counterpart. ",
            "strength_and_weaknesses": "Pros:\n1. This work attempts to expand Predictive coding techniques for multilayer graph neural networks. There has been limited success for PC graphs and this work identifies and extends the predictive coding approach. \n2. The choice of experiments and related methods are well chosen.\n\nComments:\n1. Line 3 of Preliminaries section, should it be $X\\in \\mathbb{R}^{|V|\\times d}$ ?\n2. In eq.1, what is (t) ? Please define it properly.\n3. \u201cHere, the <aggregation> function is a weighted combination of neighbour characteristics with predetermined fixed weights, and aggregate function is a linear transformation.\u201d You meant the <update> function?\n4. In eq. 4, what is $x_{u, t}$ ? Shouldn\u2019t it be $h_{u, t}$ ? It seems to me that the notation got mixed up with the referred PC graph paper. \n5. (page 3.) typo `PG graphs\u2019? Should be PC graphs. \n\n[PC graph paper]:  \u201cLearning on Arbitrary Graph Topologies via Predictive Coding\u201d\n\nThere are lots of typos and errors in the write-up. I have pointed out a few of them. The paper is slightly difficult to follow. I think the contribution of this paper is very incremental in terms of architecture novelty. It is a direct extension of the PC graph paper. The results on adversarial attacks presented are however good and seems promising. I would request the authors to revisit their draft to polish their presentation as IMO the current draft is not ready for the conference. ",
            "clarity,_quality,_novelty_and_reproducibility": "<mentioned in comments above.>",
            "summary_of_the_review": "There are lots of typos and errors in the write-up. I have pointed out a few of them. The paper is slightly difficult to follow. I think the contribution of this paper is very incremental in terms of architecture novelty. The results on adversarial attacks presented are however good and seems promising. I would request the authors to revisit their draft to polish their presentation as IMO the current draft is not ready for the conference. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4770/Reviewer_3kAN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4770/Reviewer_3kAN"
        ]
    },
    {
        "id": "HXCBNhMgqB",
        "original": null,
        "number": 3,
        "cdate": 1666669054532,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669054532,
        "tmdate": 1666669054532,
        "tddate": null,
        "forum": "3LUxNRrhK1",
        "replyto": "3LUxNRrhK1",
        "invitation": "ICLR.cc/2023/Conference/Paper4770/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a predictive coding approach to graph representation learning. The authors show that this approach yields performance that is often worse than GCN and other variants like GAT, but that robustness to structural attacks are better. ",
            "strength_and_weaknesses": "Overall, I have seen the use of predictive coding as an alternative to backpropagation since PC is more biologically plausible and since it\u2019s easier to parallelize at scale.  The motivation for using PC here doesn\u2019t really come through as they don\u2019t seem to value the biological plausibility or provide benchmarking for efficiency in training. \n\nIt does appear to be more robust than the chosen comparators (especially GCNs) on some of the tasks. However, the other formulations (robust GCN and graph attention network) seem to outperform it, which makes me wonder what the real benefit to this model is.\n\nThe authors state that GAT and RGCN, which consistently outperform the present method, do so because they are trained with batch normalization and dropout and both use attention mechanisms. Why not try those mechanisms with GPCN? Is there some reason those cannot be used, and if so, isn\u2019t that a limitation of the present model?",
            "clarity,_quality,_novelty_and_reproducibility": "I think the use of predictive coding on graphs is novel, it has been tried more often with image processing using convolutional layers. The results overall seem reproducible. However, the attempt at predictive coding does not seem very successful here. \n\nThe predictive coding model proposed here could use a schematic.\n\nTable 4: Is GPCN-GCN supposed to be the present study\u2019s architecture? Why are references to the model inconsistent?\nWhat are the +/-\nArgument that GPCN outperforms GCN across the board but the confidence intervals (or standard deviation, or whatever is being reported, which should be made clear) are overlapping in over 50% of comparisons.\n\nFigure 3: what do the points represent? What do the different colors of the points represent? Also they are illegible  plotted over the boxplot as presented.\n\nThere are typos and awkward phrasings throughout. \n",
            "summary_of_the_review": "This seems to be an interesting idea but not yet mature enough for publication given its performance on chosen tasks. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4770/Reviewer_b3aa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4770/Reviewer_b3aa"
        ]
    },
    {
        "id": "UjkyxC8YJ-p",
        "original": null,
        "number": 4,
        "cdate": 1666748789947,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666748789947,
        "tmdate": 1666748789947,
        "tddate": null,
        "forum": "3LUxNRrhK1",
        "replyto": "3LUxNRrhK1",
        "invitation": "ICLR.cc/2023/Conference/Paper4770/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper brings fresh ideas to adversarial graph learning. The proposed methods provide new schemes to train GNN models by predictive coding instead of backpropagation. In particular, Graph Predictive Coding Network modifies predictive coding from general machine learning to message-passing neural networks (GNNs). The design of the experimental section provides a new perspective (using confidence to compare robustness) to show GCPN's better effectiveness on attacked graph samples than other baselines.",
            "strength_and_weaknesses": "Strength:\n\n1. This paper has a clear motivation and advancing idea. In the introduction section, the authors point out that the robustness of graph neural networks remains a lot of issues to address. Inspired by prior works in other domains, the work presents a new training mechanism on graph neural networks. It finds that the energy-based model and the predictive coding are worth replacing the commonly used backpropagation, then introduces both ideas into message-passing neural networks. \n\n2. The proposed methods establish a non-back propagation approach to train GNNs, which is a technically novel style and different from prior GNNs training works. In detail, Graph Predictive Coding Networks include an energy-based model and predictive coding, which not only minimize the same energy function with two different message-passing mechanisms but also filter the message being passed down from direct neighbors of a particular node.\n\n3. Experimental section is plentiful. Unlike previous work that evaluates the robustness of graph neural networks, this paper measures the test results of the proposed model from a more fundamental perspective (e.g., using confidence to compare robustness). Table 1 also shows that the proposed GCPN achieves the best robustness against perturbations compared with GCN and RGCN.\n\nWeakness:\n\n1. The technical contributions in methods are heavily similar to prior works, making the novelty limited. An idea of predictive coding has been already presented in [1, 2]. Although using the energy-based model and the predictive coding to replace backpropagation is novel to graph learning tasks, the detailed techniques seem like simply adoption on message-passing networks from general machine learning. In addition, the technical description cannot provide new insights for graph neural networks.\n\n[1] An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity, Neural computation.\n\n[2] Learning on arbitrary graph topologies via predictive coding, arXiv.\n\n2. In the experimental comparisons, there are insufficient baselines for comparison. Table 1 compares the proposed GCPN only with the GCN and RGCN. There are, however, numerous other defensive approaches to graph neural networks that may be equally or more effective than RGCN. Since readers may be curious about the efficacy of GCPN in the context of the entire domain, additional baselines need to be added.\n\n3. The number of attack methods used for evaluation is insufficient. Specifically, in Table 4 Poisoning Attack and Section 4.3 Evasion Attack, the authors create adversarial graph samples using only Mettack and Nettack, respectively. Mettack and Nettack are not the only methods for attacking sample generation; many others may be equally or more effective. If the authors provide additional attack methods during evaluation, the defense results of GCPN will demonstrate its superiority more convincingly.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation for this study is clear, i.e., to use techniques (such as energy-based design and predictive coding) that have been demonstrated to be effective in enhancing the robustness of deep learning in other more general domains. Graph Predictive Coding is a straightforward method that is easy to follow. However, its novelty is limited because the proposed techniques have been developed in prior work that has not been mined for graph-specific insights.",
            "summary_of_the_review": "The paper is easy-to-follow and the methodology is plainly stated. The proposed techniques are an early attempt at graph tasks. To evaluate GPCN's robustness, the experiments are pretty comprehensive from various dimensions: poisoning and evasion, global and targeted, direct and indirect. However, because both the energy-based model and predictive coding that makeup GPCN are existing technologies, the originality, and novelty of the paper are limited. Overall, I tend to reject this submission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4770/Reviewer_opfV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4770/Reviewer_opfV"
        ]
    }
]