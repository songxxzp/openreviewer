[
    {
        "id": "cLdZHhQ45Wk",
        "original": null,
        "number": 1,
        "cdate": 1666585142740,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585142740,
        "tmdate": 1666668700298,
        "tddate": null,
        "forum": "nfMQJ4pz2Y",
        "replyto": "nfMQJ4pz2Y",
        "invitation": "ICLR.cc/2023/Conference/Paper1810/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a new application of neural operators. They target a well-studied problem of optimal control where there is a dynamical system and the goal is to find a control function that minimizes a certain cost specified by the control system. \n\nTo do this, they first transform the OCP (optimal control problem) into a BVP by utilizing a known technique in optimal control known as the Pontryagins Maximum Principle. Then they apply a DeepONet-based operator on the resulting BVP. The authors then use previous work on DeepONet approximation bounds to provide a new theorem on the approximation error of their approach. They then provide simulations\nof multiple systems as their set of experiments (mainly focused on a pendulum).",
            "strength_and_weaknesses": "Notes on Each section:\n\nRelated Work and Baseline:\n\nWeakness:  When talking about neural operators, only the DeepONet is mentioned but another related line of work Fourier Neural Operator (https://arxiv.org/pdf/2010.08895.pdf) is not mentioned, nor compared in the experiment section. In addition, the authors mentioned about another neural operator work Graph neural operator https://arxiv.org/pdf/2003.03485.pdf, still, no comparison of the GNO to DeepONet is conducted when learning the optimal control operators. The authors mentioned GNO may has high computational complexity this slower computational time, but a comparison between DeepONet, FNO, and GNO in learning the optimal control operator is needed to justify the choice of network structure.\n\nMethodology: \n\nStrength: The methodology section is well explained and easy to follow. \n\nWeakness: \n\n* One of the major weaknesses of this paper is that the approximation error bound in Section 3 does not match the experiment setting in Section 4. The author mentioned that \u201cif the encoder is implemented by functional approximations e.g. point-wise evaluation, then the error should be considered\u201d. In fact, their experiments do use a discretization grid/point-wise evaluation, so the approximation bound in Section 3 should match the actual setting and include the error from the encoder.\n\n* V needs to be defined in Eqn. 8 \n\nExperiments: \n\nWeakness: \n\n* My biggest concern is regarding the comparison to necessary baselines in order to benchmark the proposed approach. The authors only compared the DeepONet against MLP. They need to compare against other neural operators including both FNO and Graph-based Neural Operator, in terms of both accuracy and computation speed. \n\n* Related to the first weakness point, another useful baseline to compare against is PINN. Though the authors mentioned in related work that PINNs need to solve instance by instance, while DeepONet learns the entire operator. However, how about learning accuracy? Similarly, as my first comment, such a comparison between the proposed method and PINN in terms of both accuracy and computational efficiency is necessary to justify the choice of network structure. Especially on data that is unseen during the training stage or completely Out of Distribution test data, PINN might have better accuracy than neural operators since it solves the equation rather than suffering from generalization error. I would love to see such a comparison on test data that are sufficiently different from the training case and see the comparison between the proposed approach and PINN.\n\n* How can we tell which method is better from the state trajectories plot (e.g., Fig. 4 and Fig. 7)? Why is one trajectory better than another?\n\n* I don\u2019t understand how the data for the stochastic pendulum was created using RL. \u201cFor stochastic OCP (i.e. Stochastic Pendulum), we choose a reinforcement learning algorithm named Proximal Policy Optimization(PPO) (Schulman et al., 2017) as the ground truth OCP solver. \u201d Page 21. The authors need to provide sufficient descriptions and details if they want to include this part of the results.\n\n* Another thing was I think the authors ran out of space. There is no extensive discussion about the real-world examples provided.\n\nSmall questions/weaknesses:\n* Why vary the time horizon? What did this achieve in demonstrating?\n* Why are the noise distributions for the pendulum different? One is positive and the other is negative (epsilon_in and epsilon_out)\n* Why is the batch size 10k for a data set of 1k (Pendulum example) - which seems like a typo\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above ",
            "summary_of_the_review": "This paper proposes a new application of neural operators in learning the optimal deterministic control operator derived from the Pontryagins Maximum Principle. The problem of the study is interesting. The main limitation/weakness of the proposed paper is as follows: \n\n1) the theory analysis in Section 3 does not match the actual method. The authors should include the encoder approximation error to the approximation error bound; \n\n2) Lack of necessary comparison to baselines to justify the choice of the model architecture. The authors need to compare against FNO, GNO, and PINN in terms of computational efficiency as well as approximation accuracy (in both in-distribution data and OOD samples) to benchmark the proposed approach and argue why such a design is desired; \n\n3) Except for the pendulum example, other experiments are not discussed in necessary detail - the authors may want to re-assess the distribution of space to include at least one more real-world like control example and comparison in the main text beside the pendulum example. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1810/Reviewer_p9mD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1810/Reviewer_p9mD"
        ]
    },
    {
        "id": "FJCyrqIpBNg",
        "original": null,
        "number": 2,
        "cdate": 1667597357280,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667597357280,
        "tmdate": 1670353875821,
        "tddate": null,
        "forum": "nfMQJ4pz2Y",
        "replyto": "nfMQJ4pz2Y",
        "invitation": "ICLR.cc/2023/Conference/Paper1810/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a neural network architecture to estimate a mapping from cost functionals to optimal inputs in order to solve optimal control problems. The authors extend the work involving DeepONet (Lu et al., 2021 and Lanthaler et al., 2022) to optimal control problems and also extend the analysis to theoretically bound the error of their method, OptCtrlOP. The performance of OptCtrlOP was shown through simulation on six different systems ranging in states from one to nine dimensions and input dimensionality ranging from one to four. The provided results show an improvement in running time and accuracy against two baselines.",
            "strength_and_weaknesses": "The paper does a good job of providing background information and providing a theoretical analysis of their approach. I also appreciate how the paper is organized and includes both theoretical and empirical results. However, I think the largest weakness is the lack of evidence to support the strong claims of the paper. \n\n- Claim: \"[OptCtrlOP] solves OCPs in a one-shot manner with no dependence on the explicit expression of dynamics or iterative optimization processes.\"\n\n  - While I understand how this claim can be reached based on the wording, I believe it is a bit misleading. As presented, the method requires quite a bit of data involving the optimal control of the system. While the execution of the operator is not depending on the expression of the dynamics or an iterative nature, developing the mapping and training certainly do.\n\n- Claim: \"The design is in principle endowed with substantial speedup in running time...\"\n\n  - Results from the simulations support this claim, but I do not believe the baselines support such a claim. The speed shown for inference of OptCtrlOP is impressive and I believe would still be the better performer. However, the performance shown for the direct method appears to be slow compared to existing methods in that category. I understand the use of IPOPT as a generic non-linear solver; however, most implementations of a DM involve more specific implementations. I recommend a comparison of the systems to SCP-based implementations closer aligned with current state-of-the-art implementations. Additionally, do you expect this performance disparity to scale as the problem complexity increases? In Figure 2, there it appears there is more than a 2x increase in run time when going from problems with an input size of one to four (pendulum/robot arm/cart pole to quadrotor). This leads me to wonder how the inference time scales as the network size needs to grow.\n\n- Claim: \"... the model reusability is guaranteed by high-quality in- and out-of-distribution generalization.\"\n  - The discussion about model reusability was confusing for me to follow and see the connection to this claim. The experiments did provide a look at what happens with the goal location was outside of the training distribution. However, what about other model parameters (e.g. length of the pendulum, weight, other physical parameters)? Would you expect the same performance with modifications to other parameters? Providing a discussion on the connection of what is demonstrated or experiments expanding on this would add support for this claim.\n\n- Claim: \"Extensive experiments on 6 physical systems verify the effectiveness and efficiency of our approach.\"\n  - While I do appreciate the experiments conducted on different systems. I feel like the use of \"extensive\" and \"physical\" are misleading with simulation-only approaches involving only two modest baselines. I would recommend reevaluating this claim or supporting the experiments with more baselines and hardware.\n\nMy understanding of operators in the control space is that they are global operators mapping cost functionals to another functional space. The authors do a great job of discussing that concept in section 3.1. However, I believe there is a disconnect when generating data and training the operator with a single initial condition. This process appears to produce an approximation to the operator that is only valid in a local region (starting from the initial condition or near). Does this process work from arbitrary initial conditions or is a separate trained network needed? My understanding is that it does not work from arbitrary initial conditions. If it does, I recommend adding a discussion supporting that and/or experiments that also show this attribute. If it does not work from arbitrary initial conditions, I am struggling to see the applicability to a control approach. For example, when using direct methods in an MPC fashion, we do solve for an open loop control sequence from an initial condition. However, we replan at each step. So while a lot of MPC-based methods assume deterministic dynamics when solving an open-loop problem, a lot of uncertainty is handled by the replanning at the new state at each time step.\n\nDoes the frequency of training data influence the performance at different control frequencies? For example, if we are wanting to control something at a rate much slower than its dynamics, would this approach still be valid to estimate the operator? Would the rate of data gathered for training influence this performance?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity/Quality**: The paper was well organized and written. The overall production quality of the paper is good and easy to follow. I believe most of the paper is also clear. I mentioned some sections in the Strengths and Weaknesses section that would benefit from more explanation. In particular, if there are any assumptions or claims about the approach (e.g. related to the initial conditions previously discussed) I recommend stating/citing those.\n\n**Novelty**: The work expands upon previous work in the field of using networks to approximate operators. The primary contribution appears to be a related network architecture applied to optimal control problems. With my current understanding of the initial condition limitation, I do not believe this is a significant contribution. The method, as I understand it, uses a neural network architecture in an efficient way to estimate optimal controls from a single initial state. The network required a set of optimal controls to train on and the optimal controls were generated from another method that could perform real-time replanning at different states (e.g. SCP DM).\n\n**Reproducibility**: The paper provides many details to be able to recreate the work. Additionally, the authors that the code will be made available. I did not try to reproduce the results, but I currently do not have any concerns about reproducibility.\n",
            "summary_of_the_review": "The concept of using a neural network as an operator mapping from cost functionals to control functionals is an important area of research. The paper is well written and the inclusion of both theoretical analysis and empirical results is a big strength. My primary concern with the paper as stands is the lack of support throughout for the very strong claims. Additionally, based on my current understanding of the approach, I do not see the novelty and how this approach would apply to control problems (if limited to a single initial condition). If this approach is the initial step towards work to provide a mechanism for a feedback/replanning process, then I recommend stating that more explicitly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1810/Reviewer_Yqn8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1810/Reviewer_Yqn8"
        ]
    },
    {
        "id": "7WIkNOgt0Y",
        "original": null,
        "number": 3,
        "cdate": 1667613035011,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667613035011,
        "tmdate": 1667613035011,
        "tddate": null,
        "forum": "nfMQJ4pz2Y",
        "replyto": "nfMQJ4pz2Y",
        "invitation": "ICLR.cc/2023/Conference/Paper1810/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a new, data-driven method to solve optimal control problems that is significantly faster than existing methods at execution time and produces similar suboptimality of solution when compared to other approaches. The method is explained through the lens of learning an operator that maps an optimal control problem to a control function. Specifically, the authors learn a neural network that maps the goal state and the time index to a control input for the dynamical system. The network architecture is inspired by the DeepONet paper.",
            "strength_and_weaknesses": "The main strength of this paper is that the method is incredibly fast \u2013 the method is many orders of magnitude faster than more traditional methods. Speed is one of the main bottlenecks of optimal control approaches, so methods that obtain similar performance with much less computational cost are valuable. \n\nThe main weakness of this paper is that the writing presents the content hastily; with little exposition, explanation, or context, making it difficult to follow the logical argumentation. Despite being someone who does research in the area of control theory, I had to read several other papers just to understand this paper. The audience will never have exactly the same background and expertise as the authors and the paper should be written with that expectation. This is especially relevant because this is a controls paper submitted to a machine learning conference \u2013 the vast majority of registrants to ICLR have little to no background in control theory but it should be accessible to them, as they are the intended audience. \n\nSpecific points to address:\n1. The second paragraph of the introduction jumps immediately into \"Lebesgue measurable function space\" with no definitions. I would ask \u2013 is this reference to the foundations of optimal control theory necessary? Or can it be omitted and instead a plain-language description of the optimal control problem added, after which the paper transitions directly to the formulation of the optimal control problem (Eq. 1)? This is the type of hasty writing without exposition, explanation, or introduction that appears throughout the paper and makes it difficult to read and understand.\n\n2. The concept of model reusability is not defined but should be defined in the paragraph where it is mentioned on page 2. \n\n3. The concept of two-phase optimal control solvers is not defined but should be defined in the paragraph where it is mentioned on page 2.\n\n4. In the discussion of PINNs in section 2.1, the paper mentions that \"the magnitude of the two loss terms is inherently imbalanced\" but this has little meaning for the reader without the appropriate context. Which two loss terms? Why should the reader care about this imbalance? As a note for here and throughout the paper: the paper should summarize other works with enough context so that the reader can appreciate the points the paper is making without having to re-read every reference. \n\n5.  It says in section 2.1 that for Direct methods of solving optimal control problems, \"The\nreformulation essentially constructs surrogate models, where the state and control function (infinite dimension) is replaced by polynomial or piece-wise constant functions.\" I do not understand what reformulation the paper is referring to. To my knowledge, using a direct method to solve an optimal control problem does not necessitate using a surrogate model. What am I missing here?\n\n6. At the end of section 3.1 the paper claims that the approach is highly reusable, but as I mentioned above, there is no definition of reusability that the paper has given so it is difficult to evaluate this claim. On my first read through this paper, I understood this to mean that the cost function could change even if the dynamics had to remain the same. However, later in the methodology the paper defines the encoder as simply returning the target state and effectively learn with respect to a fixed cost function. It seems like a major simplification. Does this claim of reusability just refer to the fact that the goal point can change? (because it's an input to the network)? I think this point needs major clarification. If possible, it would be nice to see an experiment where the encode of the cost function is more sophisticated. \n\n7. The main algorithm of the paper (Alg. 1) appears in the appendix. It would be nice to see it appear in the text, although I know space is limited. In contrast, the definition of a neural network in section 3.2 list item 2 (Approximator) is not necessary for the readers of ICLR papers who are all familiar with machine learning. It should be moved to the appendix. \n\n8. I had difficulty following the theoretical proof in 3.3. Much more intuition, definition, and explanation could be added. The terms in eq. 6 are not well defined. What is a push forward measure? Even using Lip() to denote Lipschitz constant (I presume) should be defined. \n\n9. For the pendulum example appearing in the text \u2013 I would encourage you to switch this out for something with more interesting dynamics that you have already run experiments on. It seems that every controls paper I read uses an inverted pendulum and it does not demonstrate the capability of the method. One would only use an involved learning based control algorithm like the one in this paper for a more complex system. Also, I believe there may be an error in the way in-distribution and out-of-distribution noise was defined for the pendulum \u2013 they appear to be switched, I think? It would make more sense for the in distribution noise to be centered around the goal point. \n\n10.  As a high-level point of feedback \u2013 I believe that a more concrete description of the method is in order. E.g. perhaps a diagram of network architecture similar to the DeepONet paper as well as a straightforward description like this line from the DeepONet paper: \"Let G be an operator taking an input function u, with G(u) being the corresponding output function. For any point y in the domain of G(u), the output G(u)(y) is a real number. Hence, the network takes inputs composed of two parts: u and y, and outputs G(u)(y) (Fig. 1a).\"\n\n11. Because the scale of the plots is logarithmic, the reader cannot easily verify a claim like the method is \"50% faster\" by e.g. looking for a bar that is 50% larger than another. \n\n12. The paper discusses the performance gap between ID and OOD performance but does not give numbers and also plots these on separate graphs, so again, it is hard for the reader to validate these claims. \n\n13. The paper gives a *hypothesis* as to why your architecture is better at handling OOD shift, but it is stated more that you *know.* I would clarify that this is an explanatory hypothesis.\n\n14. In the abstract, six experiments are mentioned, but the plots in Fig. 2 omit the heat problem. Why?\n\n15. It is difficult to extract information from fig. 4. Perhaps you could use 2D plot views instead? Thinner lines? Euclidean distance from the optimal path?\n\n16. The paper does not mentioned limitations of the work in the conclusion, but should. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The clarity of the paper is low. I would rate this as a 2/10. Many specific reasons mentioned in the Strengths and Weaknesses list.\n\nQuality: I'm not sure what this is intended to refer to. Overall, the technical work seems sound (as far as I can understand) but it is not polished. \n\nNovelty: It is novel so far as I know.\n\nReproducibility: As I mentioned earlier, network architecture diagrams would be useful and help with reproducibility. As would improving the overall clarity of the paper. ",
            "summary_of_the_review": "To summarize, I think that this is exciting work but the current presentation does not do it justice. Were this an option I would give the recommendation of: accept conditioned on major revision. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1810/Reviewer_vSM1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1810/Reviewer_vSM1"
        ]
    }
]