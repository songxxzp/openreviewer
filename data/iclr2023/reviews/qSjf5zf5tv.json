[
    {
        "id": "z7o4fwOWesZ",
        "original": null,
        "number": 1,
        "cdate": 1666641376919,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641376919,
        "tmdate": 1668760674468,
        "tddate": null,
        "forum": "qSjf5zf5tv",
        "replyto": "qSjf5zf5tv",
        "invitation": "ICLR.cc/2023/Conference/Paper3295/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies how to approximate Lipschitz functions in high dimensions for signals with low-dimensional structures. By assuming the existence of a linear Johnson-Linderstrauss embedding on the signals, the appropriation bounds of Lipschitz functions based on neural networks are given. It allows for better explanation of the empirical success of deep learning models in inverse problems. ",
            "strength_and_weaknesses": "I think the main limitation of the theory is about that the model (y=F(x)) considered in this paper has no noise term, which is not very common in the literature of inverse problems. \n- If the results could be generalized to this case, the contribution would be stronger. It not, a discussion regarding this point is welcome in the conclusion.\n- Do you have an estimation of the d in practice, so as to justify the theory is relevant to practice?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the results are well presented. ",
            "summary_of_the_review": "Revision: I tend to keep my score it is still not so clear how the results could be extended to noisy cases (without modifying Assumption 1). Also the application to inverse problems remain theoretical, therefore I think the impact of the paper is limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "na",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3295/Reviewer_RMbx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3295/Reviewer_RMbx"
        ]
    },
    {
        "id": "ZfpBiq2flO",
        "original": null,
        "number": 2,
        "cdate": 1666650023654,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650023654,
        "tmdate": 1666650023654,
        "tddate": null,
        "forum": "qSjf5zf5tv",
        "replyto": "qSjf5zf5tv",
        "invitation": "ICLR.cc/2023/Conference/Paper3295/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper the authors answer why the size of a Lipschitz neural network in practice is much smaller than the theoretical bounds (which have an exponential in D dependence). The key idea that the authors provide is that number of parameters will vary exponential in the intrinsic dimension if there exists a matrix $A$ which projects a sample from higher dimension D to d, such that it also preserves the distance between the points. \n\nTherefore, if there exists a neural network with a given width, depth and parameters (epsilon-dependent) that can epsilon-approximate a Lipschitz d, then it can also approximate a function in D with only few extra parameters and layers.",
            "strength_and_weaknesses": "[Strengths]\n\nThe results provided in the paper are technically correct, and easy to follow and the proofs are well presented.\n\nThe authors also provide conditions under which it is reasonable for a JL to exist. \n\nThe authors provide the estimates of size for solving some classical problems like sparse recovery and matrix completion. \n\n[Weakness]\n\nThe key contribution of the paper is dependent upon the existence of a JL-map $A$. However it is unclear if such a map can exist in natural domains for which neural networks are used. \n\nThe proof techniques used to show epsilon-approximation in the intrinsic dimension, and the resulting size of the neural networks (in terms of depth, width and parameters) is very similar to previous works by Yarotsky (2018,2022) and indeed borrows a lot of details from the their proof. The key idea of the existence of a JL-Map $A$ does not seem like sufficiently novel.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear to follow, however seems to be lacking novelty in the key parts of the result.",
            "summary_of_the_review": "The techniques used to prove the main theorem seem to build heavily upon existing results, due to which I am currently leaning towards a weak-reject. I am open to having a discussion with the authors in this regard.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3295/Reviewer_ZXNm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3295/Reviewer_ZXNm"
        ]
    },
    {
        "id": "W0bi9tly_B6",
        "original": null,
        "number": 3,
        "cdate": 1666771969889,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666771969889,
        "tmdate": 1666771969889,
        "tddate": null,
        "forum": "qSjf5zf5tv",
        "replyto": "qSjf5zf5tv",
        "invitation": "ICLR.cc/2023/Conference/Paper3295/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors use the Johnson--Lindenstraus lemma to argue that a Lipschitz function over a suitable low-dimensional subset of a high-dimensional ambient space can be approximated by neural networks whose complexity mainly depends on the intrinsic dimensionality of the low-dimensional set, and only weakly on the ambient dimension. The authors extend the JL lemma to domains whose unit secant sets have controlled covering numbers or Gaussian width (effectively: low-dimensional domains). They give results for fully-connected and convolutional networks and apply them to two compressed-sensing-style problems.\n",
            "strength_and_weaknesses": "The theme of approximation rates that adapt to intrisic dimensionality is a familiar one in machine learning and signal processing. The current manuscript studies a natural construction where the data is first linearly projected into a low-dimensional Euclidean space. The injectivity and stability of this projection are guaranteed by the JL lemma and its extension to continuous sets provided in the manuscript.\n\nIn my opinion the main strength of the manuscript is that it focuses on a specific but still sufficiently general setting, and that it uses familiar tools to get a very explicit result that is natural and easy to understand. As far as I can tell, the mathematical arguments are sound, and the presentation is clear.\n\nOn the critical side, the novelty is in my opinion limited in view of existing work. The presented results seem to be a special case of \n\n``Universal Approximation Theorems for Differentiable Geometric Deep Learning'', JMLR, Anastasis Kratsios, L\u00e9onie Papon; 23(196):1\u221273, 2022.\n\nTo see this, set\n\n- $I = 1$ \n- $X = \\text{the subset S of}~\\mathbb{R}^D$\n- $Y = R^p$\n- $\\varphi(x) := (1/2M) * (Sx - (M,\\ldots,M))$; i.e. the JL-embeding S->R^d and the rescaling of the cube [-M,M]^d to the unit cube [0,1]^d\n\tnote that this map is injective and continuous (since it is bi-Lipschitz) whence it satisfies Assumption 7\n- $\\rho$ be the identity on R^p\n- Take the exponential maps to be about the origin; i.e.\n- Exp_{R^d,0}(u) = u +0 =u\n- Exp_{R^p,0}(u) = u +0 =u\n\nIn fact, the above paper derives sharper approximation rates that are dimension-free when $f \\circ \\varphi^{-1}$ admits a Whitney-type extension of $\\varphi(S)$ (Corollary 45). Related arguments are used Theorem 3.3 in ``Non-Euclidean Universal Approximation'' by Kratsios and Bilokopytov; in particular that should allow to substitute any universal regressor.\n\n\nThe present work is also closely related to\n\n- Deep Fried Convnets: https://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Deep_Fried_Convnets_ICCV_2015_paper.pdf\n- NN approximation w/ random projections: https://core.ac.uk/download/pdf/153400703.pdf\n- The classic work on nonlinear learning with local coordinate coding: https://papers.nips.cc/paper/2009/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf\n- Direct inference on compressive measurements: https://ieeexplore.ieee.org/document/7532691\n- Sketching and neural networks: https://arxiv.org/pdf/1604.05753.pdf\n\n### A few additional remarks\n\n- You mention that a mild drawback of the work of Chen et al. is that the bounds depend on manifold curvature which is unknown or hard to estimate. But in your JL-based results the covering number of the secant set will surely depend on similar parameters of the data domain. How would you reliably estimate covering numbers or Gaussian widths from data? \n\n- Your architecture first reduces dimension, a bit like an autoencoder, but standard architectures for inverse problems in imaging don't do that (one example is the U-Net). They first create many channels to expand dimensionality and then reduce it back to the image dimension. This demonstrably improves the expressivity. These architectures are very successful at addressing inverse problems, even at high resolutions. How does this fit your narrative? A related comment is that there are many situations where overparameterization is known to help (random kitchen sinks, most neural networks). In some cases we have theory. Again, it would be nice to hear how this combines with your narrative. Somehow I think that (admittedly vague) notions like inductive bias will deteriorate after compression.\n\n- One thing that will certainly be problematic is that in many problems with convolutional structure a \"good\" architecture can opportunely leverage local information in the full-dimensional input (e.g., skip connections in the U-Net or the ResNet). A low-dimensional projection is likely to destroy this structure.\n\n- I am slightly confused by the convolutional results (Theorem 3). Since there is no equivariance assumption on f, the output space of the network, R^p, should not be interpreted as an image? (assuming that we work with images). The result of Zhou, for example, seems to study a different model than the one used in inverse problems (where again in the case of images the channels and the output of a convolutional network like the U-net are image-like + equivariance plays an important role). Zhou as well as Oono & Suzuki seem to approximate a generic f \\in C(Omega) and not leverage any image-related inductive bias of the domain or the range of f.\n\n### Questions, suggestions, comments\n\n\n- The covering numbers in Proposition 1 scale favorably when the set in question has some low dimensional structure (for example, it's an embedded manifold). It might be good to state this explicitly and write down some common scalings. Also, I assume there is a relation which such things as the box dimension and doubling constants.\n\n\n- In compressive imaging the input to the reconstruction map is low dimensional to begin with and presumably cannot be compressed any further. Could you comment on the most natural classes of problems where your results apply?\n\n- A philosophical comment: Your initial assumption is that the inverse operator is Lipschitz. I understand that you're looking at a problem which is already discretized, but many inverse operators in interesting inverse problems are not Lipschitz between function spaces on which they are usually defined. This is the case even for the Radon transform which smooths by half a derivative (in 2S) and thus has a H\u00f6lder inverse, though this can be \"made Lipschitz\" by changing norms. But in problems like electric impedance tomography, the inverse problem is only log stable for any reasonable norm (this is very bad). By the results of Bourgain and later others (e.g. Stefanov and Uhlman), once we go to finite dimension things do become Lipschitz but the Lipschitz constants will still reflect the fundamental instability of the continuous problem.\n\n\n### A few misprints I caught\n\n\nIntroduction\n\n- Nah et al. 2017 is a deblurring paper but it is cited in the context of low-does CT reconstruction; please double check all references\n\n- Spurious ) after m < n in the second paragraph\n\n- In paragraph 3, The inverse of the linear measurement map -> delete \"inverse of the\"\n\n- Second sentence of paragraph 4: delete either alternatively or instead\n\nSection 2: Related work\n\n- The sentence starting with \"Therefore, to study expressive power... \" is broken \n- Last paragraph on p2: Guassian -> Gaussian\n\n\nSection 3: Main results\n\n- The sentence \"Since our theory is to be applied...\" is broken\n- The title of 3.1 has two ands\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to understand. (There are quite a few misprints and the prose could be streamlined, but that should be easy to fix.) As for originality, I have some doubts expressed in the previous box. The theme of rates that adapt to the intrinsic dimension of data is old and well-understood so I occasionally had the impression that the high-level idea is a bit oversold. My main objection is that I know of prior work that addresses the same problem and derives considerably more general results.",
            "summary_of_the_review": "This is a nice paper with a good, important idea, and a clear message. I think the presentation should include a careful comparison to earlier related results in the literature (some of which address the exact same problem and even make stronger statements). I very much appreciate the motivation via inverse problems but that also raises certain questions about inductive bias (arguably _the_ reason why the current architectures work so well across the board), and mangling this inductive bias by low-dimensional projections. But the bulk of the basis for my rating is the overlap of the result with the previous (more general) work of Kratsios and Papon, cited above. I think that the specific setting studied by the authors _is_ among the most interesting instances of this theory and warrants a specialized treatment, but I feel that this requires a substantial revision to properly reflect existing results.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3295/Reviewer_Q6xo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3295/Reviewer_Q6xo"
        ]
    },
    {
        "id": "3Qbc_FKNp3",
        "original": null,
        "number": 4,
        "cdate": 1666794585225,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794585225,
        "tmdate": 1669294146924,
        "tddate": null,
        "forum": "qSjf5zf5tv",
        "replyto": "qSjf5zf5tv",
        "invitation": "ICLR.cc/2023/Conference/Paper3295/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors in this work study the approximation of high-dimensional Lipschitz functions and characterize the size of the NN required for an arbitrarily good approximation. They are able to say if the data $x \\in R^{D}$ is $\\rho$-JL embedable in a low-dim manifold $R^d$, then there exists an apporpriate deep network (with say ReLU activation) that can approximate any $L/(1-\\rho)$-Lipschitz function $g: [-M,M]^d \\rightarrow \\mathbb{R}^p$, then there exists a deep network with only a small constant increase in depth, and the size of the network that increases exponentially in $d$ instead of $D$ (the input dimension). They show this by an existence of a JL-embedding for general sets in $\\mathbb{R}^D$, in terms of it's Gaussian width (or covering numbers). They use these results to obtain the required size of NN to get an $\\varepsilon$-approximation of inverse maps that are Lipschitz continuous.\n",
            "strength_and_weaknesses": "Strengths:\nUnderstanding training w.r.t the low-dimensional embeddings are important and this work has a good characterization of the size of the network required for Lipschitz continuous functions in high dimensions. \n\nPrior works such as (Chen et al. 2019) characterize the number of neurons that depend on the curvature of the low-dimensional manifold in question which can be difficult to estimate. This work sort of alleviates, with a less abstract notion, which are Gaussian widths of the set or the covering number, which seem to be more amenable to estimation from data.\n\nThe size of the network grows exponentially in $d$ rather than $D$, the input dimension and the results offer for tradeoffs in practice.\n\nWeakness/Clarifications:\nAre there any lower bounds known for the size of the networks? Could the authors comment on the tightness of the size of the networks.\n\nIt would have been great to see experiments on some high dimensional datasets as there are usually gaps in approximation characterizations and its training and generalization qualities. \n\nIt would be nice to get some bounds on the samples required to for a reasonable approximation, when you estimate the GW or the covering number of the set. Or at least a some discussion pertaining to this.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and well motivated and the techniques are novel. \n\nMinor comments:\nThe citation (Iwen et al. accepted (See Arxiv)) could be just changed to it's arxiv link.\nMinor typo: Guassian->Gaussian in the last line of page 2.",
            "summary_of_the_review": "Overall it is an interesting work on the size of NNs required for approximation of Lipschitz continuous functions in high-dimensions by embedding them in a low-dimensional manifold with complexity parameters that are practical to estimate, as compared to prior work. It would have been great to see some experiments to see this in action.\n\nUPDATE: After the response and the other reviews, it feels like many clarifications were not convincing and thus I decrease my score to 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3295/Reviewer_MC9E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3295/Reviewer_MC9E"
        ]
    }
]