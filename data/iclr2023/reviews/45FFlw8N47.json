[
    {
        "id": "_KAEyVmbyXm",
        "original": null,
        "number": 1,
        "cdate": 1666612390758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612390758,
        "tmdate": 1666612436579,
        "tddate": null,
        "forum": "45FFlw8N47",
        "replyto": "45FFlw8N47",
        "invitation": "ICLR.cc/2023/Conference/Paper3132/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method called MoMA, which leverages multiple external text corpus for retrieval-augmented language models on zero-shot dense retrieval tasks. MoMA has an augmentation component for retrieving several augmentation document and a main model for measuring the relevance between the query with these augmentation document and the target document. Both models take sentence-T5 as their model architecture and trained iteratively using ANCE. Experiments on 18 datasets in BEIR demonstrate the effectiveness of MoMA.",
            "strength_and_weaknesses": "Strength: well-motivated idea\nWeakness: Experiments can be further improved\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. Leveraging multiple external text corpus to improve retrieval-augmented language models on zero-shot retrieval tasks is well motivated and novel. Many details w.r.t. the model architecture and experiment implementation are discussed in the paper, which is helpful for understanding the whole paper.\n\n",
            "summary_of_the_review": "Experimental settings need further explanation. I'm curious why not just following the settings in those papers listed in the related work sections. it can be found that most dense retrieval models (including MoMA-DR w/o Target) performs worse than BM25 (a very simple baseline) and even MoMA-DR  exceeds BM25 with a slight margin. What's the cause for this phenomenon? Is the source training corpus (MS MARCO passage) and/or being unrelated to the BEIR datasets?\n\nBesides, it can be found in Table 2 that the target dataset is the core for MoMA-DR compared to external corpus (i.e., the 5th column versus the 8th column, 0.431 versus 0.436), which weakens the claim that multiple external corpus used in MoMA is helpful for zeroDR. Is it possible to make some case study to show that the external corpus does contribute some knowledge that is uncovered in the target dataset?\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3132/Reviewer_KU9C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3132/Reviewer_KU9C"
        ]
    },
    {
        "id": "yMGNHm2aDJ",
        "original": null,
        "number": 2,
        "cdate": 1666670335177,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670335177,
        "tmdate": 1666679201693,
        "tddate": null,
        "forum": "45FFlw8N47",
        "replyto": "45FFlw8N47",
        "invitation": "ICLR.cc/2023/Conference/Paper3132/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed a mixture-of-memory augmentation framework for a zero-shot dense retrieval (MoMA-DR). The key idea is to enrich the query representation with external resources such as wiki, knowledge graphs, etc. The framework consists of two components: a retrieval model and an augmentation model. The retrieval model follows the usual dense retriever settings, and the augmentation model is used to select some relevant/useful documents from external resources for query representation enrichment. To verify the effectiveness of this idea, they conducted experiments on the BEIR benchmark datasets and showed that it could provide some performance boost.",
            "strength_and_weaknesses": "### Strength:\n- Making the query augmentation process trainable and automatic is interesting and novel in the current dense retrieval field.  The iterative joint optimization mechanism of the retrieval model and the augmentation model is natural. \n- Detailed study on the impact of different memories and the joint learning mechanism.\n-The paper is easy to follow and well-orgnaized.\n\n### Weaknesses:\n- This idea is similar to query augmentation methods [a], but it seems that this related line of research is not well explained and reviewed in the paper. \n- Preparing the external resource is not a trivial thing (the impact varies on different datasets). Important questions about this, such as what resources to include and where to find them, are not answered. Given the difficulty of finding suitable external resources and the high cost of maintaining them, the applicability/effectiveness of this framework is limited.\n- The reported performance is not that promising. Compared with a recent work contriever [b], this model performs much worse. Also, if we include hybrid models such as BM25+CE, the results of MoMA-DR are even less attractive. \n\n\n- [a] A Survey of Automatic Query Expansion in Information Retrieval, CSUR, 2012\n- [b] Unsupervised Dense Information Retrieval with Contrastive Learning, JMLR, 2021",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nI do have a few questions that may need clarification from the authors:\n- What is the computational cost of the proposed framework in terms of training and inference, and how is it compared with other approaches? In my understanding, if the augmentation corpus is very large (e.g., much larger than the corpus of interest), the process of retrieving augmented documents can be very costly. And this process can not be taken offline as it is query specific.\n- Could you give more details on the \"BM25 warmed-up T5 retriever\"?\n- What is the impact of K and N? Why not tune them? Does tuning them bring additional benefits or worsen the model?\n- What are the training datasets of these dense retriever baselines?\n\n### Reproducibility\nThe source code is not available.\n\n\n\n",
            "summary_of_the_review": "This paper is overall well-written, and the idea is also well-motivated. The proposed learning paradigm is novel, although the idea of query expansion is not original. The performance improvement is not that encouraging. Some related work and stronger baselines are not included for discussion/comparison. The expensive augmentation process is done online, which can heavily limit its use in practical/industrial environments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3132/Reviewer_VaeQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3132/Reviewer_VaeQ"
        ]
    },
    {
        "id": "mumc80H8Fbu",
        "original": null,
        "number": 3,
        "cdate": 1666756377903,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756377903,
        "tmdate": 1666756424791,
        "tddate": null,
        "forum": "45FFlw8N47",
        "replyto": "45FFlw8N47",
        "invitation": "ICLR.cc/2023/Conference/Paper3132/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a new way of dense retrieval using a retrieval augmented model. The modeling differs from previous models in the sense that it allows integrating new corpora that are never used during the training. This allows the proposed method to perform zero-shot retrieval nicely on the new domain that is just integrated at the inference time. Authors evaluated the proposed method in the BEIR benchmark.",
            "strength_and_weaknesses": "**Strength**\n* Authors provide various ablations that were helpful to understand the proposal.\n* Authors provide benchmarks on a large set of corpus.\n\n**Weakness**\n* Generalizable retrieval augmented language model were proposed by [1] and it does also covers zero-shot scenario (e.g. Table 6) and the idea is a bit similar. I encourage authors to discuss and compare thoroughly.\n* Since the embedding function, $f_a$, needs to be compatible with the new corpus, it is undeniable this method implicitly has some assumptions to the new corpora. For example, if the $f_a$ function performs only on the text inputs, a new corpora with product hierarchy codes will not work well. \n* Since the proposed work assumes there is a given retrieval stack, comparing to the model with a similar number of parameters in DNN is not fair. For example T5-ANCE is similar in their size of the network to MOMA-DR, but MOMA-DR is actually much larger in their space/inference cost. The size of the database should be also considered for the fair comparison.\n* Since this paper heavily relies on retrieval augmented language models, authors should also empirically compare to some popular retrieval augmented models (RETRO/REALM/FiD). All the baselines that authors considered are not retrieval augmented models.\n* I am not sure why ColBERT cannot be part of the comparison. It is actually much smaller than MoMA-DR and even without any database (but it largely outperforms on many tasks?).\n\n[1] Izacard, Gautier, et al. \"Few-shot learning with retrieval augmented language models.\" arXiv preprint arXiv:2208.03299 (2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n* Mostly easy to follow.\n* Few suggestions: move Figure 1 and high-level description of the method at the beginning of the paper.\n\n**Novelty**\n* Please see the weakness #1 for the novelty discussion. Since [1] proposed a fairly similar generalizable retrieval augmented language model (it\u2019s also based on joint learning), the novelty of this paper is a bit limited.\n",
            "summary_of_the_review": "At the current state of the paper, it does not have sufficient discussion towards highly relevant recent work; however, I would like to hear what authors\u2019 thoughts are on the discussion points.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3132/Reviewer_DtfB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3132/Reviewer_DtfB"
        ]
    },
    {
        "id": "lxoRz3JVyK3",
        "original": null,
        "number": 4,
        "cdate": 1667244299733,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667244299733,
        "tmdate": 1667244299733,
        "tddate": null,
        "forum": "45FFlw8N47",
        "replyto": "45FFlw8N47",
        "invitation": "ICLR.cc/2023/Conference/Paper3132/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a new retrieval algorithms by enriching the representation of the query with augmentation documents from multiple information corpora. ",
            "strength_and_weaknesses": "Strengths:\nThe proposed algorithm improves retrieval models\u2019 generalization abilities in term of zero-shot accuracy on the various retrieval tasks\n\n\nWeaknesses/questions: \n\n1. One claim made by this paper is the better efficiency. I'm curious to understand the system performance overhead (flops, training step time, serving latency etc) introduced by this mixture of memory method. For example, eq-5 required multiple runs of the same encoder with different data, I assume it would significantly increase the compute flops required for a q-d matching. \n\n2. How do the MoMA approaches compare to other baseline models with matching compute? Table 1 shows the parameter count, however, it doesn't translate to efficiency or compute cost directly. What's the compute cost for MoMa-DR comparing to that of GTR_large?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The description of the proposed algorithm and the related work are well written. ",
            "summary_of_the_review": "The proposed method showed better generalization abilities over the retrieval tasks. However, it's unclear what's the added compute cost. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3132/Reviewer_U4cd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3132/Reviewer_U4cd"
        ]
    }
]