[
    {
        "id": "afePIJ5UAr",
        "original": null,
        "number": 1,
        "cdate": 1666042220740,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666042220740,
        "tmdate": 1666042220740,
        "tddate": null,
        "forum": "0JD3EN75NJE",
        "replyto": "0JD3EN75NJE",
        "invitation": "ICLR.cc/2023/Conference/Paper3833/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the robust overfitting (RO) problem for adversarial training (AT) which usually happens after the learning rate (LR) decay. In particular, it hypothesizes that RO is caused by a loss of balance (trainer becomes stronger than attacker) after LR decay. It verifies the hypothesis by looking into target-class information in test robust features, bilateral class correlation, and confusion matrix symmetry for models with and without LR decay. Based on this understanding, the authors propose three techniques (model flatness regularization, smaller LR decay and stronger attacker during training) to rebalance the trainer and the attacker. The proposed method BoAT is evaluated against several baselines on two model architectures and three datasets.",
            "strength_and_weaknesses": "## Pros\n\n-neat observations and formulation of the RO problem\n\n-the experiments mostly verify the proposed hypotheses\n\n## Cons\n\n-BoAT has much lower natural accuracy compared with KD+SWA\n\nThe results in Table1 seem to suggest BoAT simply trades off natural accuracy and adversarial accuracy compared with existing methods (e.g., KD+SWA) so it might not be very useful in the sense of improving the natural-adversarial accuracy trade-off. I would like to see how the trade-off of natural accuracy and adversarial accuracy look like for different hyper-parameters of BoAT and baselines, and which one lies on the Pareto front.\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nThe paper is overall clear and well-written. However, the plotting needs to be improved. For example, Figure 2(c) has way too tiny x/y labels. The color in 3(d) is too close for 12/255 and 16/255 to enable one to differentiate one from the other. There are also some typos, for example, in the third to last line of Sec3.3, \u201cunder \\epsilon = 8/255\u201d seems to be \u201cunder \\epsilon = 16/255\u201d instead? \u201cpurposed BoAT\u201d (end of page 7) should be \u201cproposed BoAT\u201d. Please carefully check the paper for similar errors.\n \n\n## Quality\n\nThe claims regarding the RO after LR decay are mostly supported by the experiments.\n\n## Novelty\n\nThe insight of the cause for RO after LR decay is novel. The mitigation strategy leveraging self-supervision is also moderately novel. \n\n## Reproducibility\n\nAlthough code is not provided, the method seems to be reproducible. \n",
            "summary_of_the_review": "Overall, the paper provides neat observations on the RO problem in AT and verifies the hypotheses. Its proposed three techniques mitigating RO seems to be functional for alleviating RO but might not be useful as general techniques to improve robustness.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_vLyN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_vLyN"
        ]
    },
    {
        "id": "NyEgbZqpTk_",
        "original": null,
        "number": 2,
        "cdate": 1666232494292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666232494292,
        "tmdate": 1666232494292,
        "tddate": null,
        "forum": "0JD3EN75NJE",
        "replyto": "0JD3EN75NJE",
        "invitation": "ICLR.cc/2023/Conference/Paper3833/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Summary\n\nBased on former findings, this paper first proposes a new definition of robust features with taking dynamics into consideration. Focused on the phenomenon that robust overfitting usually starts after learning rate decay, the authors provide an explanation of robust overfitting based on several experiments. New approaches to mitigate robust overfitting are proposed and validated. The main difference from previous studies is their emphasis on the necessity of considering the min-max nature of AT in robust overfitting. ",
            "strength_and_weaknesses": "\nPros\n\n1. The background and motivation of the paper are well-written, and the validation experiments of the proposed hypothesis are convincing.\n2. The paper is well-written, clear, and easy to follow.\n\n\n\nCons.\n\n1. The proposed dynamic feature robustness framework needs to be more formally and clearly defined, especially the difference from the former work. For instance, it is encouraged to provide a formal mathematical definition of dynamic, robust, and non-robust features.\n2. The mitigation approaches \u201dadding regularization loss\u201d, \u201cusing stronger attacker\u201d and \u201cdecreasing learning rate\u201d are well-established tricks and therefore this paper does not bring technical novelty.\n3. The proposed methods are only evaluated on 2 architectures in the same model family, which is highly insufficient. More network backbones like MobileNet and VGG should be included.\n4. Although the proposed methods lead to improved robustness, it suffers from natural accuracy degradations. It is hard to say whether the BoAT is better compared to other approaches. Also, this needs to be mentioned in the front of this paper, otherwise only mentioning the robustness benefits is misleading.\n5. The paper mentions that non-robust features will be much more robust after the learning rate decay. I am very curious about whether the non-robust features will also approach robust features visually.\n6. \"RO usually happens after the learning rate decay in AT\". Actually, this claim is insufficiently supported. Since most previous papers decay the learning rate at the 100th epoch. In order to well support this claim, the authors need to conduct experiments with LR decay at different epochs and show the consistency of RO observations.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good.\n\nQuality and Novelty are fine.\n\nReproducibility is unknown",
            "summary_of_the_review": "Good paper with novelty and experiment limitations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_kLGk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_kLGk"
        ]
    },
    {
        "id": "3sIdz6UVMl",
        "original": null,
        "number": 3,
        "cdate": 1666305412743,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666305412743,
        "tmdate": 1666306723486,
        "tddate": null,
        "forum": "0JD3EN75NJE",
        "replyto": "0JD3EN75NJE",
        "invitation": "ICLR.cc/2023/Conference/Paper3833/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper begins with an argument that the robustness of a model should also be conditioned on the training process. They then focus on  the one specific aspect of the training process -- learning rate (LR) decays. They found that after LR decays, the confusion matrix becomes more symmetric. The author claimed that this observation helps to understand robust overfitting. Then, they proposed the BoAT loss, which seems to perform better than some prior work in their experiment.",
            "strength_and_weaknesses": "Weakness:\n- The first paragraph in page 5 is confusing.\n  - In Figure 2, there are no measurement on feature, only on classes. But the conclusion is on target-class features. What is the definition of non-robust feature?\n  - The relationship between the confusion matrix being symmetric and robust overfitting is unclear.\n  - What does A->B and B->A mean in Figure 2?\n  - What does accuracy with misclassified labels in Figure 2a mean?\n  - Is the confusion matrix computed with adversarial attack?\n- Some other parts of the paper that are confusing:\n  - Should the output of f be a feature or a label? In the text, it's called feature while in equation 2 and 3, they are compared with label y.\n  - The authors claimed \"False Non-robust Mapping Opens Shortcuts for Test-time Adversarial Attack\". What is a shortcut? Could you measure this \"shortcut\"? Are there any hypothesis on why this shortcut is related to LR decay?\n- What is the learning rate schedule used in Figure 2? Does different scheduling or decay method matters?\n- The method used in [1] should also be compared. Their method is effectively make the model locally smooth, which in some way also improves landscape flatness.\n- For Equation (4), is the adversarial example computed on the cross-entropy loss or on the BoAT loss? The inner minimization should be included in the equation to make things more clear.\n- How is the lambda in Equation (4) chosen?\n- This paper discussed a lot on robust overfitting, while in section 3 and 4, there is not discussion on the generalization gap comparing with other methods.\n- The hyper parameter choices in Table 4 seems arbitrary.\n- I think it is arguable whether adversarial training is SOTA algorithm for adversarial robust training. I would suggest checking out some of the methods in https://robustbench.github.io/\n- In the paragraph before Section 4.1: \"We train each model for 200 epochs and record the best-epoch (selected based on PGD attack)and final-epoch robust test accuracy.\". One should not just select the best epoch on test data. This should be done on a validation set.\n\n[1] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" International conference on machine learning. PMLR, 2019.\n",
            "clarity,_quality,_novelty_and_reproducibility": "No code is provided and the paper is not really clear to me. I don't think I will be able reproduce this work.",
            "summary_of_the_review": "This paper is not well written. The flow of the paper is also not logical (e.g. the relationship between the Symmetrized Confusion Matrix and robust overfitting is not explained). There are major flaws in the experiment setup (e.g. selecting the best epoch based on the performance on test data), and the experimental result cannot support all their claims.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_p7yt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_p7yt"
        ]
    },
    {
        "id": "VPn3-SnFZNQ",
        "original": null,
        "number": 4,
        "cdate": 1666705278721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666705278721,
        "tmdate": 1669005826244,
        "tddate": null,
        "forum": "0JD3EN75NJE",
        "replyto": "0JD3EN75NJE",
        "invitation": "ICLR.cc/2023/Conference/Paper3833/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work aims to provide a new perspective to explain the robust overfitting and improve the adversarial robustness built upon the proposed viewpoint. To do so, the authors revisit and generalize the static framework of feature robustness. Built upon the proposed framework, the authors claim that the balance between the attacker and the trainer is the key to understanding robust overfitting. Consequently, the authors propose three methods inspired by the framework and demonstrate the effectiveness of these (two, as one is dropped in the main page) methods.",
            "strength_and_weaknesses": "Strength:\n- The considered symmetrized confusion matrix is interesting and convincing, which is novel to me. \n- The designed objective function is inspiring. Specifically, the authors introduce a KL term, minimizing the KL divergence between the learnable model f_{\\theta} and the detached model f_{\\phi}, which is actually equal to assigning a soft label to each adversarial example. Thus, it is promising and exciting to explore a novel approach to designing different assignments of soft labels. And, maybe, it has potential benefit to the research of certifiable defense.\n- The paper is well-written, easy to follow, and has solid experimental analysis.\n- The proposed method is simple yet effective, verified by the experiments.\n\nWeakness:\n- A stronger attack for stronger defense is not employed due to its harmful impacts on natural accuracy, but I cannot find any explanations about the observation from the proposed perspective. It is necessary to add the corresponding explanation using the proposed view because methods inspired by the proposed framework are dropped from the main page, making the framework less convincing.\n- The definition of Eq. (3) seems confusing. The trainer T is directly applied to update the model, i.e., changing the learned representations, thus, introducing the trainer T, given the representation, seems meaningless, but the trainer T is important for the rest of the work. Hence, the authors should clarify the point. Similarly, the utilized F_f^T is also confusing.\n\nSuggestions:\n1 The claimed theory seems to be a hypothesis, as merely experimental analysis is given in the current version.\n\n2 According to the claimed perspective -- the main goal is to prevent the model trainer T from fitting non-robust features too quickly and too adequately --, a straightforward approach can be that the trainer decreases the learning rate progressively from the previous epoch decaying the learning rate by 0.1. Thus, the corresponding experiments are required. However, it is lacking in the current version. \n\n3 Different from the section on the method, the section on explanation seems weak, I suggest employing more references to convince the claim, the related perspective is the spurious feature perspective, such as CausalADV and group DRO.\n\n4 The \u2018feature\u2019 considered in Eq. (2) corresponds to the data space, while the \u2018feature\u2019 considered in this work Eq. (3) is more about the representation space, thus, I suggest the authors make these two features distinguishable as they are conceptually different.\n\n[CausalADV] Adversarial robustness through the lens of causality. Zhang et al. 2021\n\n[group DRO] Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. Sagawa et al., 2020\n",
            "clarity,_quality,_novelty_and_reproducibility": "Good clarity, quality, novelty, and reproducibility.",
            "summary_of_the_review": "The proposed framework is novel to me, the proposed methods are simple yet effective, and the conducted experiments are convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_yb7Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_yb7Z"
        ]
    },
    {
        "id": "7_PWcFwqgk",
        "original": null,
        "number": 5,
        "cdate": 1666840888681,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666840888681,
        "tmdate": 1670814685276,
        "tddate": null,
        "forum": "0JD3EN75NJE",
        "replyto": "0JD3EN75NJE",
        "invitation": "ICLR.cc/2023/Conference/Paper3833/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper cites the feature robustness framework of Ilyas et al. 2019 as its starting point. It claims that framework fails to explain overfitting in adversarial training (Note: that paper wasn't claiming to explain overfitting). The paper then proposes a narrower definition of feature robustness (where an attacker and a trainer are given). They claim that it explains overfitting.  \n\nThe paper, I think, mischaracterizes Ilyas et al. 2019 definition of feature robustness when describing Limitations. \n\nThe writing is not particularly clear on why the change in definition explains overfitting, which was the original motivation for the paper. The paper acknowledges past work that robust overfitting can be largely \"surpassed\" (suppressed?) by enforced a smoother landscape with additional regularizations. In the end, it seems the proposed schemed BoAT also ends up doing something similar (Section 3.1: \"In order to suppress robust overfitting of the model trainer T after LR decays, we need to put further regularization on its local fitting ability, e.g., by enforcing better landscape flatness.\").\n\nSo, I am still trying to figure out the key contribution of the paper and new teachings. It could be that the theoretical arguments need improvement to explain overfitting.  \n\nThe BoAT training method is a potential contribution since the claim is that it eliminates overfitting even over 500 epochs. But, the authors should compare that with other approaches such as simply picking an earlier saved model (prior to overfitting) and prior techniques that introduce additional regularizations to smooth the landscape (many of which they cite in the 2nd para of the Intro). \n\nThe other issue is efficiency of BoAT training. How long does the training take compared to existing techniques, e.g.,  early stop? I am trying to figure if BoAT is proposed as a practical technique that practitioners should use or is simply a tool to help explain overfitting. \n\nThe Intro states: \n\n\"Specifically, we hypothesize that due to the strong local fitting ability endowed by smaller LR, the model learns false mapping of non-robust features contained in adversarial examples, which opens shortcuts for the test-time attacker and induces large test robust error. \"\n\nBut, why doesn't standard training also learn false mappings on non-robust features and also cause overfitting? \n\nIn the end, the paper started with an interesting promise of explaining the difference in overfitting between standard training and adversarial training. But, that difference in behavior still remains unclear after reading the paper.\n",
            "strength_and_weaknesses": "\nStrengths: \n\nI am still trying to figure that out. I wish the paper was clearer in writing. It seems the authors are trying to draw a connection between feature robustness (using their definition, not Ilyas et al.), learning rate decay,  and overfitting. And why the authors think that standard training doesn't run into overfitting but adversarial training does at a fundamental level.\n\nIn the end, it is not clear if BoAT is a better algorithm over prior methods for adversarial training.\n\nWeaknesses: \n\nEnglish writing is poor, making the paper difficult to follow. The abstract could be better written to begin with. \n\nLimitations on page 3: I didn't understand your argument. I didn't find in Ilyas a discussion or claim regarding overfitting.  Furthermore,  during model training, isn't the classifier changing? And also isn't f changing, if if is part of the model? If it is some abstract function f independent of the trained model, then why you do believe that robustness of f is changing during model training?\n\nFurthermore, I don't see an attacker A in definition (2). \n\nI don't agree with the characterization of  Ilyas et al. 2019 that they only consider attacker A and fail to view adversarial training as a dynamic min-max game. For instance, in Robust Training para on page 4 of Ilyas et al., they explicitly discuss adversarial training a min-max optimization problem.  \n\nFurthermore, the definition I Alias et al. considers the adversarial space, but does not explicitly mention an attacker strategy. I am still trying to understand your criticism of Ilyas et al. 2019.\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper can be drastically improved. \n\nFigure out what the key contribution is and really support that well. I am not sure the conceptual framework of robust features from Ilyas et al. contributed to the conceptual understanding since you ended up changing their definitions anyway. \n\nThe writing is tentative. For instance: \"Therefore, due to the change of relative strength between A and T , these non-robust features could become robust after LR decay.\" \n\nThe stated findings are not particularly enlightening. For example: \n\n\"The discrepancy in feature robustness explains why the rise of training robustness does not consistently lead to higher test robustness, leading to a large robust generalization gap. \n\nIt is not clear that one thing explains the other.  What about robust features? Is their overfitting ruled out?\nAnd the above statement begs the question as to why non-robust features cause overfitting.\n\nPerformance Experiments: \n\nHow do your findings compare with Yu et al.  in ICML 2022 (Understanding Robust Overfitting of Adversarial Training and Beyond)? They proposed a method called minimum-loss constrained adversarial training (MLCAT).  And what about efficiency of training?",
            "summary_of_the_review": "The paper needs to be better written with a  more careful discussion of related work to better position the paper. The paper is also missing some related work that also attempts to explain overfitting and propose techniques to prevent overfitting.  \n\nAdditional comments for the authors after the rebuttal: \n\nI thank the reviewers for detailed explanations. I think the algorithm is a good contribution. But, I still feel that the overall pitch in the Intro and Section 2 should be changed in a future version of the paper. I am not sure Ilyas et al. is the correct baseline to criticize, since it was not directed at providing an explanation on robust overfitting. Other papers that attempt to explain that in other ways would be a better starting point.  In case you use Ilyas et al., perhaps a better pitch would be to say that even if a feature is fundamentally robust,  it can be perceived as non-robust by a given model because of inability to find the correct decision boundary for that feature -- even if one exists.  That way, perhaps Ilyas et al. are not necessarily wrong -- static and dynamic views become more in harmony. I don't think the paper proves that their view was necessarily wrong, but the paper suggests so, which I think is an issue with the current thrust. \n\nAnother area that the paper could  do a better job in  would be to better explain why normal training does not overfit, but adversarial does. Are there situations where normal training would also overfit, based on the understanding provided by the work? Even normal data should have some non-robust features, no -- or is that fundamentally missing in normal datasets?  More in-depth discussion would be useful. \n\nOverall, I am not changing my evaluation, but  I encourage authors to rethink the overall pitch for presenting the Boat algorithm and add more depth for an improved paper in the future.\n\n\n\n\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_zGeX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3833/Reviewer_zGeX"
        ]
    }
]