[
    {
        "id": "hiJuUp8BB16",
        "original": null,
        "number": 1,
        "cdate": 1666177901521,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666177901521,
        "tmdate": 1666178018709,
        "tddate": null,
        "forum": "JDuEddUsSb",
        "replyto": "JDuEddUsSb",
        "invitation": "ICLR.cc/2023/Conference/Paper4497/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a transformer-based model to recover first order ordinary differential equations (ODE) in symbolic form, from points sampled from their trajectories at N regular time steps. In the paper, the trajectories are supposed to be noiseless, save perhaps for approximation error when solving the ODE, or rounding error of the values sampled on the trajectory. The ODE considered are of the form y' = f(y).\n\nThe model is a sequence to sequence 6 layer transformer (using HuggingFace implementation of BigBird as the base layer), with 512 dimensions and 16 heads (47M parameters overall). Training data are generated by sampling random functions f, using the \"tree algorithm\" introduced by Lample & Charton (2019), symplifying f with Sympy, solving the corresponding equation y' = f(y) with a numerical solver (LSODA), and sampling the corresponding trajectories for a number of initial values. The model loss is the cross-entropy of the sequence corresponding to f (after preorder enumeration of the corresponding tree). Beam search is used for generation.\n\nThe authors evaluate their model on three test sets: \n* solutions from the training set, but different initial values\n* solution skeletons from the training set, with different initial values, and constants\n* skeletons not found in the training set\nThey show that, with a large enough beam size, the model can reach significant accuracy (around 40%), both when numerically approximating f and recovering its skeleton. \n\nThey also provide (smaller) benchmark samples, and compare their model with three popular implementations : Sindy, GPLearn and AIFeynman. Their model outperforms Sindy and GPLearn, but AIFeynman, while over 200 times slower, is much more accurate. The authors suggest that this is due to differences between the training distribution and the benchmark samples (an out-of-distribution generalization problem).",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper adresses a hard and important problem of science: given numerical measurements over time, discover the underlying dynamics. \n\nThe paper is clearly written. The architecture and data generation techniques are clearly described and sound. \n\nThe experiments are well-thought, and support the claims. The comparisons with baselines are adequate, and I am very happy to see a negative result here (i.e. AIFeynman beating the model), together with a likely explanation of why this is so.\n\nOverall, a very solid paper, on an important subject. \n\n\n**Weaknesses** \n\nThere are no major weaknesses or flaws in the paper. I have a number of questions and suggestions for the authors.\n\n* **Limitations of the class of ODE**: at present, the only ODE considered have the form y'=f(y). A more general class of autonomous ODE would have the form f(y',y)=0, which could be generated with the same procedure, but will feature more difficult cases (i.e. singularities). The authors should mention this limitation, and possibly discuss it in appendix A. \n* **Simplifying training functions**: all the functions in the training and test set are simplified using Sympy. D'Ascoli et al. (https://proceedings.mlr.press/v162/d-ascoli22a.html, appendix C) observe that this is not necessary. On the other hand, it slows the generation procedure, and might make the model less robust since it reduce the diversity of the expressions used for training. Have the authors tried to train the model without simplification?\n* **Model ablation**: at present, the model uses a symmetric architecture, with 6 layers in the encoder and decoder. Some authors (e.g. https://arxiv.org/abs/2112.01898) have reported good results with asymmetric architectures. In this case, it seems that the decoder task is much harder, so maybe a shallow encoder and a deeper decoder would make sense. Kamienny uses a 16 layer decoder, and a 4 layer encoder. Have the authors experimented with such architectures? Also the models also use a fairly low dimension (512) and a large number of heads (16). Are there ablation studies justifying these choices? \n* **Scaling**: the models are relatively small (46 M parameters, 6 layers and 512 dimensions). On simpler problems, d'Ascoli uses 8 layers, and Kamienny 83M parameters. Experimenting with slightly larger models might improve performance, especially in the \"out of domain\" case of new skeletons (testset-skeleton).\n* **test-skeleton**: this test set is slightly smaller than the others. However, it provides the most practical assessment of model accuracy, since it measures model performance when the solution skeleton is not in the training set, a very likely case in practical situations. Could the authors provide a larger set when they make their dataset available? \n* **beam search**: experimental results suggest that very large beam sizes are needed for the model to perform correctly. D'Ascoli (https://proceedings.mlr.press/v162/d-ascoli22a.html, section 2.3) proposes a technique for ranking the hypotheses in the beam, and other techniques such as MCTS have been discussed as possible improvements over top-k perplexity beam search. Have the authors considered such approaches?\n* **benchmarks and out-of-domain generalization**: as the authors note, the better performance of AI-Feynman on benchmark sets might be explained by out-of-distribution generalization. These test sets feature much simpler functions that the ones seen at training. Have the authors considered retraining their model on a dataset of simpler functions (reducing the number of operators, and maybe changing the probabilities, in the algorithm they use), to provide for a better comparison (and support their interpretation of the benchmark results)?  ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity** - The paper is extremely clear, and very well written. The supplementary material is helpful.\n\n**Quality** - The architecture and data generation techniques are solid. The experiments and comparisons with baselines are sound.\n\n**Novelty** - This is, to my knowledge, the first case of symbolic regression recovering ODE. It is also one of the first papers proposing an end-to-end transformer-based solution without fine-tuning of the constants. That architecture and the data generation techniques are based on previous work, but the two-hot encoding of constants is an interesting innovation.\n\n**Reproducibility** - Details are provided, both on the architecture and the data generation process, that allow to reproduce the results. The authors agree to make the model and dataset public. The model and datasets will certainly be useful for other research teams. ",
            "summary_of_the_review": "This is a very strong paper, on an important subject. It is clearly written, the experimental setting is very good, and the results are compelling. \n\nI recommend acceptance, and would be happy to increase my rating if the questions asked in the \"weaknesses\" sections are adressed during the rebuttal phase.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4497/Reviewer_LsD3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4497/Reviewer_LsD3"
        ]
    },
    {
        "id": "sBCV9kbls6",
        "original": null,
        "number": 2,
        "cdate": 1666366711454,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666366711454,
        "tmdate": 1666366952817,
        "tddate": null,
        "forum": "JDuEddUsSb",
        "replyto": "JDuEddUsSb",
        "invitation": "ICLR.cc/2023/Conference/Paper4497/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper a transformer is trained to infer from a set of scalar time series observations the dynamical law behind these observations, in symbolic form. A large data set of equations and trajectories generated by these is generated, from which various test set scenarios are created. Performance is evaluated both in terms of the recovery of the \u201ccorrect\u201d symbolic expression, as well as numerically in terms of match between local time derivatives. The model is compared to various other approaches, and in general performs favorably, although it does not always outperform other models for reasons discussed.",
            "strength_and_weaknesses": "Strengths:\n- yields a human-interpretable explicit symbolic form for the governing equation generating a given time series\n\nWeaknesses:\n- puts too much emphasis on an exact symbolic form, rather than on dynamically important properties\n- works only for very simple situations (scalar, noise-free, autonomous ...), unlike other methods developed in the dynamical systems field\n",
            "clarity,_quality,_novelty_and_reproducibility": "I\u2019m a bit undecided about this paper. On the one hand, it might be an interesting step toward deducing a symbolic representation of time series data in terms of governing equations. On the other hand, for its purported application domain, the (natural) sciences, I would say it\u2019s hardly useful, and way more powerful approaches exist.\n\nIn more detail:\n\n1) The approach is only developed for scalar, autonomous, noise-free, fully observed ODEs (although the authors allude to possible extensions in Appx. A, but my feeling is there is actually a long way to go). In most modern scientific applications, on the contrary, we deal with high-dimensional, noisy, partially observed, potentially non-stationary and non-autonomous systems. Autonomous scalar (first-order) ODEs cannot even produce oscillations, a bare minimum for many scientific applications. Especially for chaotic systems the type of symbolic inference proposed by the authors I think will become extremely challenging (if not impossible; for instance, accurate estimation of numerical constants will be become super-important). There is a huge body of work meanwhile in scientific machine learning (hardly reviewed here) which deals with inferring the governing equations from time series data for these much more complex and real-world situations. Besides the older Brunton paper, here are a few more pointers:\n[1] https://www.nature.com/articles/s41598-022-13644-w\n[2] https://arxiv.org/abs/2106.06898\n[3] https://arxiv.org/abs/2207.02542\n[4] https://arxiv.org/abs/2006.13431\n[5] https://arxiv.org/pdf/2202.07022\n[6] https://arxiv.org/abs/1712.09707\n[7] https://openreview.net/pdf?id=aUX5Plaq7Oy\n[8] https://arxiv.org/pdf/2201.05136.pdf\n[9] https://arxiv.org/abs/2110.05266\n[10] https://arxiv.org/abs/2207.00521\n\n2) For any real-world situation, I\u2019m not even sure the authors work with sensible optimization targets: In their artificial database, they have exact ground truth symbolic expressions. But in science of course one doesn\u2019t have that, and there are usually several equivalent ways to formally describe \u201claws of nature\u201d (i.e., nature does not assign one specific symbolic expression to any set of observations). It is, in my opinion, therefore not very relevant to recover the exact symbolic representation of some ground truth data for which such a symbolic expression exists. Rather, it is way more important to correctly recover the dynamical behavior supporting the system, i.e. the vector field and various of its topological and geometrical properties (see [3], [9], [10], and common textbooks like Strogatz). Yet, the authors, on the contrary, mostly emphasize the symbolic agreement. In addition, I didn\u2019t find the % recovery of symbolic form even under quite ideal conditions (iv-163 with large k) overly impressive for any of the models.\n\n3) Of course, one main point of the present paper is that many systems that have been used for extracting dynamical equations from data are not easily interpretable (e.g. NODE), and this is a valid point. I am therefore not contesting that, in principle, it makes sense to think along the lines followed by the authors (I just think the objectives need to be different for this to be scientifically useful). There are, however, also other approaches which are much more powerful (in the sense that they can deal with high-dim, noisy, partial etc systems) yet yield an interpretable form. Here, by \u2018interpretable\u2019 I mean how easily the recovered system can be analyzed and related to the data (which I think is more important from a scientific angle), not necessarily whether a specific symbolic form matches that of ground truth data (and given that we don\u2019t have this for natural systems anyway). SINDy and its successors ([6], [8]) fall into this class: The statement that SINDy is not very expressive is wrong in my mind, since it can function as a universal approximator if equipped with the right basis (e.g. Stone-Weierstrass). Yet it yields an interpretable, sparse form. Other interpretable, in the sense above, systems for recovering dynamical laws have been based on piecewise-linear approximations [3].\n",
            "summary_of_the_review": "The authors may follow a useful, complementary direction, different from that most commonly engaged in scientific ML. However, I believe it is partly based on the wrong objectives if application in science is aimed for. It also falls far behind the methods based on universal approximation in expressivity, power, and scalability. Finally, Fig. 3 is not overly impressive (for any of the models tested), and the drop-off in success with model complexity hints at fundamental limits for this class of models regarding scalability to more realistic and higher-dim scenarios.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4497/Reviewer_UR3m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4497/Reviewer_UR3m"
        ]
    },
    {
        "id": "2D5OvMnjrXB",
        "original": null,
        "number": 3,
        "cdate": 1666501150753,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666501150753,
        "tmdate": 1666501150753,
        "tddate": null,
        "forum": "JDuEddUsSb",
        "replyto": "JDuEddUsSb",
        "invitation": "ICLR.cc/2023/Conference/Paper4497/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a Neural-Symbolic Ordinary Differential Equation (NSODE) that generates models straight out of the data. It is reported in the paper that once trained, the pipeline can produce models much faster than other techniques.",
            "strength_and_weaknesses": "Strengths:\n1. The generation of symbolic models that use expressions commonly used by people to do analytical work is of tremendous importance.\n2. The proposed pipeline, once trained, is much faster than other techniques.\nWeaknesses:\n1. The authors state the Neural-Symbolic Ordinary Differential Equation (NSODE) is a symbolic model generator, as opposed to Neural Ordinary Differential Equations (NODE) (Chen et al., 2018) that generate opaque models. However, both develop symbolic models. The only difference is that the NSODE causes ones that use functions that we people understand and are helpful for analytical work. But, from a definition point of view, both approaches produce equally symbolic models.\n2. Even though the authors show that their \"model performs better or on par with existing methods,\" it still cannot resolve most of the presented problems, signaling that this work is inconclusive.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clear and somewhat novel.",
            "summary_of_the_review": "The authors present a Neural-Symbolic Ordinary Differential Equation (NSODE) that generates analytical models that are simpler to handle. The presented experiments show that it is as good as current techniques.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4497/Reviewer_iM1D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4497/Reviewer_iM1D"
        ]
    },
    {
        "id": "XuxJtYhglM",
        "original": null,
        "number": 4,
        "cdate": 1666627573781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627573781,
        "tmdate": 1669025672156,
        "tddate": null,
        "forum": "JDuEddUsSb",
        "replyto": "JDuEddUsSb",
        "invitation": "ICLR.cc/2023/Conference/Paper4497/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose Neural Symbolic Ordinary Differential Equation (NSODE) a model mapping trajectories of numerical observations of the evolution of the behavior of a single variable through time to a symbolic expression that can generate those observations. They also present an approach to generating random symbolic expressions representing ODEs and simulations thereof starting from different initial conditions to train the model motivated by related work on symbolic regression for non-differential algebraic expressions.",
            "strength_and_weaknesses": "Strengths:\n- Self contained. Presents a model and an approach to generate sufficient training data.\n- Comparative analysis.\n- Comparison on multiple datasets\n\nWeaknesses:\n- Models only single dimensional ODEs.\n- Doesn't include approaches to inferring ODEs in the related work.\n- Comparison to non-ODE baselines.\n- Biased evaluation datasets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and reproducible. While it is somewhat novel it is not well positioned in the body of existing literature. Due to improper evaluation its quality can't be reliably judged.",
            "summary_of_the_review": "The presented approach while well written and even promising, it is severely limited and not correctly evaluated. Therefore, I lean towards rejection at this time.\n\n- Models only single dimensional ODEs.\nWhile the authors claim to discover dynamical laws, attempting to generalize to the general domain of science, the scope of model dynamical systems is severely limited. Physical systems tend to be more complex and models of such systems used in a broad range of scientific disciplines are represented as systems of differential equations.\n- Doesn't include approaches to inferring ODEs in the related work.\nIn the background and related work section the authors mostly discuss approaches to symbolic regression for non-differential algebraic expressions which address a different problem than the problem that the authors tackle with their approach. In the literature there exists a significant body of work addressing this problem. For example, Schmidt and Lipson 2009, or equation discovery for differential equations or process based modeling of dynamical systems. Furthermore, the only method mentioned in the related work claimed to address the problem of learning ODEs from data is NODE (Chan et al. 2018) which is in fact an approach to integrate (solve, simulate) differential equations.\n- Comparison to non-ODE baselines.\nRelated to the previous point, the authors compare to symbolic regression approaches that have not been designed for learning differential equations and yet interestingly outperform the proposed approach both with regards to several performance metrics and execution time.\n-Biased evaluation datasets.\nThe datasets that are generated using the approach can contain examples seen by the model and can be biased towards the presented approach. The main variability of examples in the dataset comes from the randomly selected initial conditions. Given the limitation to a single dimension, most of the equations are highly probable not to be sensitive to initial conditions and either converge to a single stable point including zero and infinity. Therefore multiple initial conditions can have very similar trajectories.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4497/Reviewer_qT9N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4497/Reviewer_qT9N"
        ]
    }
]