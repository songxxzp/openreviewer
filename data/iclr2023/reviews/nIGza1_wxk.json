[
    {
        "id": "gG7VnspL4As",
        "original": null,
        "number": 1,
        "cdate": 1666625049493,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625049493,
        "tmdate": 1666625049493,
        "tddate": null,
        "forum": "nIGza1_wxk",
        "replyto": "nIGza1_wxk",
        "invitation": "ICLR.cc/2023/Conference/Paper3030/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors explore what guarantees we can identify on the performance of classifiers in response to classification subjects which act strategically - that is, attempt to update their inputs so as to achieve a better output. They call this loss the \"induced risk\": the risk the model incurs on the input distribution which its deployment induces. Their results are mostly of two forms: upper bounds on the induced risk above the optimal induced risk, and lower bounds on the minimum of the source risk and the induced risk. The machinery used for their proofs is mostly domain-adaptation-inspired, where the source distribution is the original data, and the target distribution is the strategically modified data.",
            "strength_and_weaknesses": "Strengths:\n- These are useful results and I think more general than what has previously been shown in strategic classification\n- paper is mostly pretty clear and exposition is good\n\nFeedback: \n-Sec 4 and 5: it isn't clear to me why Covariate/Target shift would be useful assumptions for the strategic classification setting. One would imagine that the induced distribution will have a much lower P(Y = 1 | X) for X which are right over the decision boundary, for instance; or that P(X | Y = 1) would cluster more closely near the decision boundary in the induced distribution. I'm happy to be convinced otherwise, but it seems to me that by the definition of strategic classification, these results aren't so useful.\n-Sec 4.2: it would be good to explain why these might be natural assumptions: I think I understand it has something to do with people choosing to move their inputs towards the desirable outcome Y=1, but not totally sure\nSec 4.3, Setup 3/4: I think there needs to be a little more definition around how the adapted feature is generated. For instance, if \\tau = 0.5, x = 0.4, and B = 0.2, then the probability of a successful update is 0.5, but it is drawn from the distribution U(0.5, 0,3), which doesn't obviously make sense to me.\n-Prop 4.7: I don't quite understand why as Err_D_S(h*_T) goes up in this bound, the gap increases - it would be nice to get some intuition on this relationship\nReplicator Dynamics: It would be good to have a better description of this in the body of the paper, I'm not so clear on exactly how this works\nExperiments: I don't totally understand the data generation process, even after looking at the supplement. It would be good to have a clearer explanation of the exact strategic modification model here (e.g. what is \\epsilon or \\alpha, as defined in the supplement)\n\nNotes:\nEq (3) - would be good to define the optimal model here: it's not clear at this point in the paper if h*_T is the model with the optimal induced risk, or optimal source risk (you explain later, but should explain here)\nSec 4.2 - should define X_+(h) and X_-(h)\nSec 5: a little bit confusing to me to overload w here as the induced positive rate, when it was the weighting coefficient earlier\nThm 5.2: what is p here?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: decent clarity around the exposition and proofs, some confusion around replicator dynamics and experiments\nQuality: I think the work is solid, however I'm not sure that the assumptions relied on by the proofs are all appropriate\nNovelty: I think this is novel because it uses lighter assumptions (mostly) than previous strategic classification work\nReproducibility: I have a little confusion around the experiments at the moment, with a bit more information this could be reproducible",
            "summary_of_the_review": "Overall, this paper provides some useful results in the strategic classification space. I have some doubts about the applicability of the covariate shift/label shift assumptions to this setting, and therefore not sure how useful those results are. Additionally, I have some confusion around other assumptions made around the data generative process with replicator dynamics and the experiments. Due to some good results and exposition alongside these concerns, I'm recommending a Weak Reject for now.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3030/Reviewer_tvWE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3030/Reviewer_tvWE"
        ]
    },
    {
        "id": "n9EJlKj1iiE",
        "original": null,
        "number": 2,
        "cdate": 1667357127125,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667357127125,
        "tmdate": 1667357127125,
        "tddate": null,
        "forum": "nIGza1_wxk",
        "replyto": "nIGza1_wxk",
        "invitation": "ICLR.cc/2023/Conference/Paper3030/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies model transferability when human decision subjects respond to the deployed model. It provides a series of lower / upper bound between empirical risk minimizer classifier and the optimal classifier. In particular, teh paper studies two common cases: covariate shift and target shift.",
            "strength_and_weaknesses": "pros\uff1a\nthe paper is clearly written, and the introduction of the problem is well motivated. The organization of the paper also makes it easy to follow, with clear notations / explanations to the theorems etc. \n\ncons:\nAs the authors write in Section 3.3, the bounds in the paper is mainly of theoretic interest. I would like to see some disucssions on the tightness of these bounds, even a single example would be helpful. The mathematical tools used are also not deep.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is pretty clear. My main concern is still the use of these bounds. In the scenarios described in Section3.3, I would be interested in knowing\n1. if the provided bound tight in any sense for the empirical risk minimizing model h_S?\n2. as the authors write in Section 6  \"indicating the suboptimality of training on D_S\", is there any other learning algorithm that provides better bounds? Intuitively, training with a penalty term would likely give better results.\n\nOf course it may involve more assumptions, but I think a tightness result, even a asymmpototic one would be very useful.",
            "summary_of_the_review": "The paper gives a series of bounds for the model transferability, the proofs I checked (non-exhaustive) are all good. It would be nice to see more discussions on the tightness of these bounds, as in the general case, the positiveness of the lower bound is not even clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3030/Reviewer_Rs7K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3030/Reviewer_Rs7K"
        ]
    },
    {
        "id": "nlcuvld8yE",
        "original": null,
        "number": 3,
        "cdate": 1667415209723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667415209723,
        "tmdate": 1667415209723,
        "tddate": null,
        "forum": "nIGza1_wxk",
        "replyto": "nIGza1_wxk",
        "invitation": "ICLR.cc/2023/Conference/Paper3030/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This papers studies the performance of models on the distribution induced by the model itself. The paper first provides examples of when this situation arises. Then, they develop formal definitions of metrics that measures the performance of the model on the induced distributions. These new quantities are inspired by the theory of domain adaptation (DA), like the induced risks that the model should minimize, which is similar to the target risk. Then authors show lower and upper bounds of these quantities. Afterwards, authors use different examples to illustrate their bounds. Finally, their bounds are computed on a real-world dataset.",
            "strength_and_weaknesses": "This paper studies a new problem and defines new quantities to understand and study it. It makes the paper mostly theoretical and related to the statistic field. Authors define many quantities inspired from DA. However as in this problem we do not have access to target samples unlike in DA, this problem seems more related to domain generalization (DG). I wonder if some theoretical or training procedure developed in DG could be applied to this problem and maybe this should be discussed in the paper. \n\nRegarding the bounds, they are explained and applied to several examples. They are also evaluated on the FICO credit score dataset.",
            "clarity,_quality,_novelty_and_reproducibility": " The related work is, to the best of my knowledge, complete. Regarding discussion, I think a short discussion about the challenge to minimize induced risk should be added in the main paper and it should refer to appendix D for full discussion. The paper is well written. A short paragraph about the difference in setting between domain adaptation and the considered problem could also be useful. In particular, the lack of access to target samples. ",
            "summary_of_the_review": "I agree that the problem is appealing. The defined quantites and theorems are also interesting. The main downside is that I would have expected a training procedure to minimize the induced risk. While authors discuss the challenges of this question in appendix, it should also be discussed in the main paper.\n\nThe lack of training procedure to minimize the induced risk and the lack of empirical contributions make that I recommend 'marginally below the acceptance threshold'. I would be happy to increase my score if authors provide additionnal details/toy experiments on how to minimize the induced risk.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3030/Reviewer_vuF4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3030/Reviewer_vuF4"
        ]
    },
    {
        "id": "o6W2VHseOS0",
        "original": null,
        "number": 4,
        "cdate": 1667573107991,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667573107991,
        "tmdate": 1667573107991,
        "tddate": null,
        "forum": "nIGza1_wxk",
        "replyto": "nIGza1_wxk",
        "invitation": "ICLR.cc/2023/Conference/Paper3030/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThis paper formulates a very interesting and novel problem (IDA, induced domain adapatation) in transfer learning. Consider the supervised classification setting where one usually trains a classifier $h : X \\mapsto Y$ from samples $\\{(X, Y)\\}$ drawn from some distribution $\\sim \\mathcal D$. Oftentimes, when the data are generated from human input, the human could possibly modify $(X, Y)$ to adapt to $h$, resulting in a distribution shift over $\\mathcal D$. Therefore, when training $h$, it is important to take this distribution shift into consideration. However, this can become complicated and interactive if the human further adapt to $h$.\n\nThis paper conducts a rather detailed study on this problem. It proves upper and lower bounds for the transfer risks for several important fundamental questions in this setting, as outlined in page 2. Besides, the paper realizes their bounds by both showing how to compute them in practice and computing them on real datasets.\n",
            "strength_and_weaknesses": "\n### Strength\n\n1. The setting studied in this paper is important, interesting, and novel.\n2. This paper studies most fundamental questions in this setting and presents satisfactory results that covers several fundamental lower and upper bounds in this setting.\n3. As a mainly theoretical paper, this paper is aware of the practical impact of their theoretical results. To this end, this paper shows how to compute their upper bounds in real-world problems and conducts experiments to demonstrate their results. Furthermore, the experimental results corroborates their theoretical results and suggests that the theoretical results could give meaningful upper and lower bounds for the transfer risks.\n\n### Weakness\n\n1. The proof techniques look simple and the techniques themselves might not inspire broader community.\n2. The authors could possibly conduct experiments on more datasets to strengthen their experimental results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe writing is good. No apparaent typos are noticed. The authors supplies sufficient materials (codes) for reproducing the experimental results in this paper.\n",
            "summary_of_the_review": "This paper has good theoretical and empirical results, yet the techniques might not be exciting and the experiments might not be convincing enough due to paucity of datasets evaluated.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3030/Reviewer_Cedq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3030/Reviewer_Cedq"
        ]
    }
]