[
    {
        "id": "f28JIVReUQ",
        "original": null,
        "number": 1,
        "cdate": 1666639990909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639990909,
        "tmdate": 1666639990909,
        "tddate": null,
        "forum": "9Nj_gNdvqYf",
        "replyto": "9Nj_gNdvqYf",
        "invitation": "ICLR.cc/2023/Conference/Paper4336/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an approach for subset selection for training models in a batch setting.  The proposed approach, called IWeS, uses importance sampling to select examples for inclusion in the subset, where the sampling probability is based on the model\u2019s entropy.  Experimental results show that IWeS provides moderate to substantial improvements in predictive performance compared to several competing baseline methods.  Furthermore, a theoretical analysis for a closely related approach, IWeS-V, including generalization and sampling rate bounds.  ",
            "strength_and_weaknesses": "Strengths:\n* IWeS is applicable to arbitrary model families.  It can also be used in an active learning where only unlabeled examples are available.\n* IWeS matches or outperforms a number of competing baseline approaches for several datasets on multi-class and multi-label prediction tasks.\n* Useful theoretical guarantees, in terms of generalization and sampling rate bounds, are provided for a closely related algorithm, called IWeS-V.\n\nWeaknesses:\n* The entropy-disagreement-based variant of IWeS (IWeS-dis) requires training two models, which can be expensive in terms of computation and memory.\n* Theoretical guarantees are provided for the IWeS-V algorithm, which is closely related to the IWeS-dis algorithm.  However, theoretical guarantees are not provided for a variant of the IWeS-V algorithm that is closely related to the entropy-based IWeS algorithm (IWeS-ent), which is computationally less expensive since it requires training only one model.\n* No detailed analysis of the computational costs associated with the proposed methods is presented in the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and clear.  The contributions appear to be novel, with good empirical and theoretical contributions.  Although the authors do not provide source code, enough detail appears to be present in the paper to allow the proposed methods to be reproduced.\n",
            "summary_of_the_review": "This is a strong paper, with good empirical and theoretical results.  The experimental results are convincing, showing that the proposed method matches or outperforms a number of baselines.  While there are some issues, as mentioned in the list of weaknesses above, they do not significantly detract from the value of the contributions.  Overall, this is a good paper worthy of acceptance.  \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_7Jex"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_7Jex"
        ]
    },
    {
        "id": "l5nHyPLA2i",
        "original": null,
        "number": 2,
        "cdate": 1666726788117,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666726788117,
        "tmdate": 1666726896324,
        "tddate": null,
        "forum": "9Nj_gNdvqYf",
        "replyto": "9Nj_gNdvqYf",
        "invitation": "ICLR.cc/2023/Conference/Paper4336/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on subset selection. Based on general loss functions and hypothesis classes, the author(s) proposed an importance-weighted subset selection algorithm called IWeS. Through Empirical experiments, the author(s) tried to show the advantage of the proposed algorithm.\n\n---\n",
            "strength_and_weaknesses": "### Strength:\n\n1. Selecting an information subset from a large dataset can effectively reduce the computation time and storage costs. In this paper, the author(s) further considered the weights of selected samples.\n\n2. Empirical evaluations are performed for validating the proposed algorithm.\n\n---\n\n### Weaknesses:\n\nThe theoretical analysis of the proposed algorithm, such as the generalization bound and sampling rate bound, may require further empirical experiments.\n\nFor more details, please see the section of \"Clarity, Quality, Novelty And Reproducibility\".\n\n---\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the topic in this paper is interesting and this paper is generally well-written. I have the following comments/questions. I look forward to the response/clarification from the author(s). Thanks.\n\n1. In the introduction, the author(s) mentioned another related method, IWAL. Maybe an empirical comparison with the algorithm IWAL is needed? Or is IWAL difficult to implement in practice?\n\n2. One motivation of subset selection is to find the most informative subset from a large number of training samples to approximate training with the entire training set. So, in experiments, it is necessary to compare the performance on the whole training set, which can be regarded as a baseline to further evaluate the effectiveness of the proposed algorithm.\n\n3. For a better understanding of the theoretical analyses, such as the generalization bound and sampling rate bound, some empirical experiments are needed for further verification.\n\n4. If the input data matrix is transposed, can the work in this paper, i.e., data subset selection, be applied to feature selection?\n\nIn addition, the format of the references is quite inconsistent. Please check carefully and correct it.\n\n---\n\n\n",
            "summary_of_the_review": "The work in this paper is interesting and it considers weighting the selected samples; However, there are some unclear/inadequate aspects in the description (including experiments) of this paper. Maybe it needs the author(s) to clarify them. Thanks.\n\nIn addition, in this paper, the author(s) performed empirical experiments. I am not sure the description would be enough to reproduce since no code seems to be provided (other than some pseudo-code descriptions). \n\n---",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_q4Qj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_q4Qj"
        ]
    },
    {
        "id": "U4kGPjCZg1r",
        "original": null,
        "number": 3,
        "cdate": 1667306512122,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667306512122,
        "tmdate": 1669094563589,
        "tddate": null,
        "forum": "9Nj_gNdvqYf",
        "replyto": "9Nj_gNdvqYf",
        "invitation": "ICLR.cc/2023/Conference/Paper4336/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed a method for subset selection, which is useful for applications such as active learning. Specifically, in the proposed method, when constructing the subset, data is selected by importance sampling, where the sampling probability relates to the model's entropy. In the subset, the selected data is weighted inversely proportional to its sampling probability for computing a weighted loss function. This method is empirically evaluated on six multi-class classification image datasets and a large-scale multi-label Open Images dataset, and shows good performance when compared with baselines. The theoretical motivation of the proposed method is also presented in the paper. \n\nThe paper's contributions include a theoretically motivated method for subset selection that is suitable for general losses and hypothesis classes and extensive experimental results for evaluating the proposed method and baselines. ",
            "strength_and_weaknesses": "Strengths:\n1. In this paper, the considered subset selection problem is promising and useful in many cases, such as efficient deep neural network training, limited budget for data storage, and active learning. The proposed method is applicable for deep models, and for both labeled and unlabeled data during the subset selection.\n2. The proposed method is theoretically motivated and evaluated by extensive experiments on multi-class datasets and a large-scale multi-label dataset. \n3. The paper is well-written and presented. This paper's main messages are clear and easy to follow for readers.  \n \nWeaknesses:\n1. The core idea of the paper is largely based on prior work (i.e., Beygelzimer et al. (2009)), which may reduce the novelty of this paper. Compared with prior work, this paper contributes to using the model's entropy to define the sampling probability to make the method suitable for deep models.\n2. I am wondering if the seed set's size or the quality of pertained models affects the performance of the proposed method. If the seed set is very small, then the model may not be well pretrained (in Line 6, Algorithm 1). In this case, perhaps the data cannot be effectively weighted, especially during the beginning of the training.\n\nOther comments:\nIn the proposed method, it seems that once one data is selected, its weight would be fixed. However, as the model is trained to become better, the weight of one data may also change if recomputed. Then, for the proposed method, it may happen that the weights for one particular data may differ by the order of the data selection. Would it improves the proposed method if recomputing weights of all existing selected data at some specific training epoch?",
            "clarity,_quality,_novelty_and_reproducibility": "For clarity and quality, the paper is good and clear in presenting the core idea, related work, and baseline methods. \n\nFor novelty, the paper's main idea is based on one prior work, but improving the sampling probability to make the method more suitable for deep models. The relationship between prior work and this work is well-explained and discussed in the paper. \n\nFor reproducibility, it seems that the authors did not provide the code for reproducing the work, but they presented lots of details for reproducing the proposed method in the paper.",
            "summary_of_the_review": "The paper presents a subset selection method suitable for deep models with theoretical motivation and extensive experimental results. Although the paper's main idea is based on one prior work, this paper contributes to using model's entropy to define the sampling probability for the benefit of using deep models. This paper is also well-written and completed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_EWd7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_EWd7"
        ]
    },
    {
        "id": "4NdK8PeDrnC",
        "original": null,
        "number": 4,
        "cdate": 1667376316609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667376316609,
        "tmdate": 1667376896197,
        "tddate": null,
        "forum": "9Nj_gNdvqYf",
        "replyto": "9Nj_gNdvqYf",
        "invitation": "ICLR.cc/2023/Conference/Paper4336/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an active learning method by utilizing importance reweighting technique. Authors derive two strategies, including entropy-based disagreement and entropy to sample examples. Then, authors reweight losses by the sampling probability of each example to achieve an unbiased estimator. Experiments are conducted on multiple benchmark datasets.",
            "strength_and_weaknesses": "Strength\n\nThe paper proposes the entropy-based disagreement sampling strategy, which is slightly different from the previous sampling strategies.\n\nThe paper conducts comparison experiments on a large-scale dataset OpenImage, which is relatively realistic.\n\nWeakness\n\nThe main idea of the proposed two strategies is similar to the previous strategies, such as uncertainty sampling and largest margin sampling [1]. It seems that the paper does not propose some new techniques. I think its technical contribution is limited.\n\n[1] Active learning literature survey\n\nThe theoretical analyses proposed in the paper are also trivial. The main results follow the previous works [1][2]. Furthermore, the main theoretical results are based on the unbiased estimator, which has been first proposed in [2].\n\n\n[1] Active Learning for Convolutional Neural Networks: A Core-Set Approach\n[2] Importance Weighted Active Learning \n\nThe experiments are weak in some extent due to the lack of comparison with sota methods. In fact, besides two baselines uncertainty sampling and random sampling, the proposed method is only compared with two methods, BADGE (2019) and Coreset (2017). These methods have been proposed three years ago. The paper is suggested to compare with more recent methods [1]\n\n[1] Influence selection for active learning, 2021\n[2] Task-aware variational adversarial active learning, 2021\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of this paper is clear.\n\nHowever, the contribution of this paper is insufficient for the following reasons: 1) The paper does not propose new techniques, e.g., novel sampling strategies. The main idea of the proposed two strategies is similar to the previous works. 2) The main theoretical results are applications of the prior works.",
            "summary_of_the_review": "The technical contribution is insufficient and the novelty is limited.\nAlthough the paper reports the extensive experimental results, it suffers from the lack of comparisons with some sota methods.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_j2Bp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_j2Bp"
        ]
    },
    {
        "id": "LbIV9t13omb",
        "original": null,
        "number": 5,
        "cdate": 1669464446989,
        "mdate": 1669464446989,
        "ddate": null,
        "tcdate": 1669464446989,
        "tmdate": 1669464446989,
        "tddate": null,
        "forum": "9Nj_gNdvqYf",
        "replyto": "9Nj_gNdvqYf",
        "invitation": "ICLR.cc/2023/Conference/Paper4336/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes generic active learning methods for any learning method.  The main method is motivated by the other method with theoretical guarantees. More precisely, the main proposed method is a practical heuristic one. The proposed method is compared with other previous active learning methods and naive ones and shows better performances on the benchmark data sets. ",
            "strength_and_weaknesses": "Strength:\nThe proposed method shows remarkable improvements over previous work in practice. Although the method does not have any theoretical guarantees, but it is motivated by the other theoretically guaranteed algorithm. \n\nWeaknesses:\nThe weakness of the paper is, as mentioned above, the lack of theoretical guarantees. However, other previous work does not have guarantees either. So, this is not a disadvantage over previous work. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well in general. However, I have a minor concern about the presentation of the contribution. The method is presented as a subset selection method of the training sample. But, then, the standard stochastic gradient methods are in the same category and I wonder if those needs to be compared as well. As with other previous work, I feel more convinced if the proposed method is presented as an active learning method for querying labels. \n\nI think the methods are novel but not significantly innovative (motivated by several previous results). The experimental section seems to contain enough information to reproduce the results.",
            "summary_of_the_review": "The paper proposes a practical heuristic method for active learning by querying labels. The paper shows non-trivial practical improvements over previous work. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_DSZY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4336/Reviewer_DSZY"
        ]
    }
]