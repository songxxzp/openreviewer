[
    {
        "id": "D8xQFSbdPF5",
        "original": null,
        "number": 1,
        "cdate": 1666669351019,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669351019,
        "tmdate": 1666669351019,
        "tddate": null,
        "forum": "SZynfVLGd5",
        "replyto": "SZynfVLGd5",
        "invitation": "ICLR.cc/2023/Conference/Paper3012/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Adversarial transferability is often studied in the image-model-to-image-model setting. This paper studies how to transfer adversarial examples from image models to video (or multi-view) models. They note that simply transferring adversarial examples from image models to video models are suboptimal, and propose a prompt tuning method, inspired by recent development in prompt fine-tuning. They demonstrate that the proposed temporal prompt adaptation to learn dynamic cues on videos or multi-view renderings of 3D objects is effective to transfer adversary from pre-trained image models to video models and multi-view models.",
            "strength_and_weaknesses": "## Strength\n- As far as I know, this is the first work that tries to transfer adversarial examples from image models to video models using prompt tuning.\n- It seems their approach empirically works well.\n- \"Mimicking dynamic behavior on image datasets: Image samples are static and have no temporal dimension. So we adopt a simple strategy to mimic changing behavior for such static data. We consider images at different spatial scales to obtain a scale-space (Fig. 2) and model temporal prompts for image dataset\"\n\t- This is equivalent to approximating camera motions via zooming. It's interesting that such a simple approach works.\n\n## Weaknesses\n\n- The way to present their results is sometimes confusing. For example, I think it would be easier to see if the authors only focus on Deit-B, Dino, and CLIP in the Table 1, and expand the discussion regarding the difference between training strategies. Separately, it would be nice to have more discussion regarding the model size difference. For example, I'm surprised that Deit-T, Deit-S and Deit-B are performing almost the same for FGSM in Table1. Can you comment on that?\n\n- Section 4.1.1 (Ensemble Adversarial Transfer) needs more clarification. What attack was used to produce the numbers in Figure 4? It seems like for some datasets (e.g. SSv2, Shared, Depth, ) there is not benefit for ensemble adversarial transfer when I compare the bar plot of Figure 4 and the numbers of Table 2. If you claim that adversarial transfer is boosted by ensemble, I think the work should compare Figure 4 with the the number of Table 2. As of now, it's hard to see this.\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity/Quality\n- There are several run-on sentences and grammatical mistakes in the paper, which could be improved after revision. Some examples include: \"The patch tokens combined with the class token1 cls \u2208 R 1\u00d7D (are?) processed by multiple multi-head self-attention (MHSA) blocks before passing the cls through the task specific head MLP.\"\n\n- Table 1 and Table 2 are not referenced explicitly in text.\n\n### Novelty\n- Applying prompt fine-tuning to adversarial transfer is novel.\n\nQuestions:\n- Why is projected gradient descent (multi-step) weaker than FGSM for Table 1? (Does FGSM transfer better than PGD in general?) Also do they use different epsilons?\n",
            "summary_of_the_review": "While prompt fine-tuning has been studied before and this work is a straightforward application of prompt fine-tuning, applying prompt fine-tuning to adversarial transfer is novel, and it seems that their approach empirically works well.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3012/Reviewer_QXGV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3012/Reviewer_QXGV"
        ]
    },
    {
        "id": "is-YwLQU9bY",
        "original": null,
        "number": 2,
        "cdate": 1666672681548,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672681548,
        "tmdate": 1670889593210,
        "tddate": null,
        "forum": "SZynfVLGd5",
        "replyto": "SZynfVLGd5",
        "invitation": "ICLR.cc/2023/Conference/Paper3012/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "his work studies the role of temporal dynamics in adversarial transfer between image and video models. The authors introduce temporal prompts to adapt pretrained image models to video inputs, giving them the capacity to capture dynamic cues. Motivated by visual prompt tuning, patches from video clips are processed by a self-attention layer and spatially pooled into one prompt token per frame. Spatial tokens from a single video frame and the temporal prompts are then concatenated and fed to the pretrained image classifier. Experiments show that when temporal prompts are used in surrogate model, transferred adversarial examples have a higher success rate on various target video models.",
            "strength_and_weaknesses": "## Pros\n- Incorporating temporal prompts appears to be a promising strategy as it significantly improves the transferability of attacks compared to using image models alone, when evaluated on various datasets and models.  \n- The proposed method also seems to have a small computational overhead as it involves learning a single attention layer while preserving the weights of the pretrained image model.  \n- A variety of visualizations are provided for understanding the generated adversarial patterns.  \n\n## Cons\n- I am not sure I fully understand process of \"mimicking dynamic cues on static images\" (figure 2). Can the authors explain how the dynamic cues are created by varying the spatial scale? What would be the input to the prompting module $\\cal T$ in this case? I am also confused about its role in the proposed system, as the prompting module is trained on video data (bottom of page 6). I assume mimicking is only used when evaluating on image datasets, but would appreciate if the authors can provide further clarifications.  \n- While the prompting module enjoys the benefit of efficiency, I am not sure if a single attention layer is sufficient to capture complex temporal dynamics. Table 5 confirms that spatial cues are retained in the temporal prompt learning (which is expected as the backbone encoder is frozen), but the accuracy on temporal-heavy dataset SSv2 is still quite low, even compared to small spatiotemporal networks. I think additional experiments are needed to show that models trained with prompting indeed learn meaningful temporal cues (e.g. testing with original vs. shuffled frame ordering), instead of relying merely on spatial features. I also think more work is needed to understand if the improvement in adversarial transfer can entirely be attributed to temporal cues, since the difference introduced by temporal prompting is larger on UCF and HMDB, which have less temporal cues than SSv2.  \n- Alternative ways have been studied to adapt pretrained image models to video inputs (e.g. I3D, TimeSformer). While they might not be as efficient as temporal prompting, it would still be an important comparison between proposed method and using those video models as surrogate in terms of adversarial transferability.  \n- There has been prior work on adapting adversarial samples between image and video modalities (e.g. https://arxiv.org/abs/2112.05379). There should be discussion/comparison to these related works.  \n- Results on transfer from ensemble models can be better presented. In figure 4 it is difficult to compare across rows of bar plots, or to single-model results from the main tables.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall clarity of the writing is fine. There are a few confusions as detailed above which I expect the authors to clarify in the rebuttal period. The formulation of the solution is novel to my knowledge, and I have no particular concerns on its reproducibility.",
            "summary_of_the_review": "The paper presents an interesting approach towards adversarial transfer from pretrained image models to video tasks. While the proposed method has its own strengths and shows decent performances compared to baselines, I have a few concerns with the validity of hypothesis made in the paper on temporal modeling, as well as the absence of certain comparisons. Therefore, I set my initial rating as borderline reject and look forward to responses and clarifications from the authors over the discussion period.\n\nUpdate 12/12: Increased rating after rebuttal and discussions with authors. See details in thread.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3012/Reviewer_TDqC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3012/Reviewer_TDqC"
        ]
    },
    {
        "id": "LkLRx8BwXD",
        "original": null,
        "number": 3,
        "cdate": 1666702284891,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666702284891,
        "tmdate": 1666702514086,
        "tddate": null,
        "forum": "SZynfVLGd5",
        "replyto": "SZynfVLGd5",
        "invitation": "ICLR.cc/2023/Conference/Paper3012/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a transfer-based adversarial attack for black-box video models. The surrogate model is built via prompt tuning from a pre-trained image model with additional temporal prompt tokens. It is validated that a surrogate model with the learned temporal prompts (or \"dynamic cues\") achieves better adversarial transferability for various attacks than a surrogate model without them. This approach can also extend to target models on image or 3D data.",
            "strength_and_weaknesses": "[Strength]\n1. The problem studied in this paper, i.e., using image models to attack black-box video models, is interesting and important in practice, as there are a large and increasing number of public pre-trained image models.\n2. The proposed method is simple and seems effective. The idea of mimicking dynamic cues on static images with different spatial scales is interesting, which can improve the adversarial transferability on image data.\n\n[Weakness]\n1. The problem settings are not clearly stated. For example, it can only be inferred from the method and experiments that the attacker has the information on which dataset the target model is trained on and also the access to this dataset.\n2. Some of the claims are not well supported.\n- Abstract: \"Our attack results indicate that the attacker does not need specialized architectures ... Image models are all we need\nto optimize for an effective surrogate ...\" Without baseline results on surrogate models with those \"specialized architectures\", it cannot be determined whether the proposed method is really \"effective\" compared with the baselines.\n- Section 4.1 (Table 5): \"Deit models retain their top-1 (%) accuracy on ImageNet ..., while also exhibiting decent performance on video datasets and ...\" To support this claim, it is expected that Table 5 should contain: (1) the ImageNet results for raw Deit models; (2) a baseline result for video datasets.\n- Section 4.1.1 (Figure 4): \"Ensemble of different training frameworks performs favorably to boost attack transferability.\" Visually comparing the two rows in Figure 4 to figure out which is better can be difficult. A better demonstration is preferred.\n3. In the proposed method shown in Figure 1, a random frame is sampled from the input video. It is not stated whether the input receives gradient from this branch during attacks. Besides, does this single frame sampling strategy limits the attack to the video data where any single frame contains a large portion of information of the video?\n4. It is discovered that \"CLIP adversaries are relatively less transferable as compared to fully-supervised ViT or self-supervised DINO model\". This may be weird since CLIP is pre-trained on a wider range of data. Is there any reasonable explanation?\n5. The paper still learns the downstream task when attacking the video task, which only replaces fine-tuning with visual prompt tuning. This adversarial attack transfer is not a direct attack on the image model. Considering Transformation and the image model as a whole, it can be considered as a video model learned by the prompt tuning method.  And although visual prompt tuning needs to learn fewer parameters, the training time is not shorter than fine-tuning.\n6. The prompt tuning approach usually does not perform as well as the fine-tuning approach. Would an attack using a fine-tuning model on a downstream video task perform better?\n7. The paper only compares the use of Temporal Prompts in the experimental section. Currently, some methods have been proposed to improve the cross-task or cross-model transferability of adversarial attacks, and these methods should also be involved in the comparison.\n8. The approach of the paper only tries to attack the transformer model and has limited transferability to CNN models. Can this approach also be used to attack pre-trained CNN models, such as MOCO for Resnet?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The problem settings and some parts of the proposed method are not clearly stated to some extent. The problem is new and there are some novel ideas. The reproducibility may depend on the release of code.",
            "summary_of_the_review": "The problem studied in this paper is meaningful and the proposed method is intuitive. The major issue is the unsupported claims with some important baselines missing. Besides, the writing and organization can be improved to make it easier to understand the method and settings.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3012/Reviewer_iNv7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3012/Reviewer_iNv7"
        ]
    },
    {
        "id": "dperSQyFD42",
        "original": null,
        "number": 4,
        "cdate": 1666703187475,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703187475,
        "tmdate": 1666703187475,
        "tddate": null,
        "forum": "SZynfVLGd5",
        "replyto": "SZynfVLGd5",
        "invitation": "ICLR.cc/2023/Conference/Paper3012/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors explore the problem of robustifying a video transformer using a pre-trained image transformer as a surrogate model. Authors argue that using adversarial patterns optimal for large image models (such as Deit, DINO and CLIP) improves adversarial robustness of other image models, but is not suitable out-of-the-box for robostifying video models because image models lack temporal cues (e.g. motion). Authors train a network that generates a sequence of temporal tokens (one per frame) for the entire video, concatenate them with spatial tokens (one per spatial location) for a single random frame, and pass through a frozen pre-trained image transformer with fine-tuned heads. The resulting pipeline is trained using a combination of supervised and unsupervised losses and is then attacked to obtain adversarial examples. Authors train their method across multiple scales to improve transferability and mimic scale changes in videos on image datasets. Authors show that in the resulting pipeline, larger image models yield better image-to-video adversarial transferability, self-supervised DINO transfer best, that their approach is more robust then 3D convolution, that the ensembling helps their approach as well. Authors claim that the resulting procedure is significantly more memory efficient compared to a naive baseline of processing all spatial tokens of all frames jointly.\n",
            "strength_and_weaknesses": "Strengths:\nThe problem is well-motivated - it is clear why one might want to adjust the surrogate model when going from images to videos\nResults are promising - the resulting model bests all baselines by a significant margin\nEvaluation is thorough and shows similar trends across a wide range of datasets, surrogate models, adversarial attacks, resolutions, and ensembling techniques.\nWeaknesses:\nI am not familiar with prior work on adversarial transferability, so my confusion might be stemming from that. Nevertheless, I found the paper quite difficult to follow. The syntax in the introduction is sometimes difficult to parse, but is generally understandable (\u201cin a real-world setting, a scene is not static but mostly involves various dynamics, e.g., object motion\u201d,  \u201capproach offers the benefit that the attacks do not need to rely on specialized networks designed for videos towards better adversarial transferability\u201d, \u201cdynamic cues are shown here to aid in better transfer not only to video models, but also to the original domain (image) models due to the expressivity of dynamic information\u201d). The method section is more difficult to follow, because \n\n1) The method is introduced only in a figure and scattered throughout two pages of plain text - no model expression summarizing how final outputs are obtained via inputs, no clear definition of used losses;\n2) After re-reading the method section and following the diagram several times, I am still confused about some of the wording. \n\nWhy does the caption in Figure 1 say that \u201cthe spatial tokens are ignored\u201d? \n\nDo authors use the term \u201ctoken\u201d and the output of the transformer at that token\u2019s position interchangeably? (e.g. I_cls in the first paragraph of Sec 3.1) \n\nIn sec 3.2 \u201cspatial class token Icls serves in an unsupervised objective\u201d - means that it is used only in the unsupervised objective? Is self-supervised L_ss - that unsupervised objective? \n\nI also found it difficult to follow the line of thought in the experiment results section 4.1 - it is only a few dozen lines long and it basically lists all provided Tables, and follows straight to conclusions, not connecting observations to conclusions in the main text.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed in the previous section, my main concern regarding this paper is the clarity of writing and method definition, and consequently, reproducibility. The evaluation seems thorough. I lack deep knowledge of prior work to judge its novelty.\n",
            "summary_of_the_review": "I think that the paper does a good job of motivating the problem it tackles, and that results reported in this paper would benefit the community at large, but the lack of clarity severely undermines the impact this publication can make in its current form.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3012/Reviewer_yaNd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3012/Reviewer_yaNd"
        ]
    }
]