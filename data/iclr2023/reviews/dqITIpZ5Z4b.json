[
    {
        "id": "A_ydt3OtuR",
        "original": null,
        "number": 1,
        "cdate": 1666381313352,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666381313352,
        "tmdate": 1668872652811,
        "tddate": null,
        "forum": "dqITIpZ5Z4b",
        "replyto": "dqITIpZ5Z4b",
        "invitation": "ICLR.cc/2023/Conference/Paper2264/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a general framework which unifies problem complexity measures with function approximation in the model-based and model-free lens for reinforcement learning.  In particular, the authors propose Admissible Bellman Characterization (ABC) class that subsumes many prior models for problem complexity in the prior literature.  The authors also provide a new algorithm, OPERA, achieving regret bounds that match or improve over the prior literature.\n\nMore concretely, the authors consider an MDP $(S,A,r,P,H)$ where $S$ is state space, $A$ the action space, $r$ the reward function and $P$ the global transition distribution, and $H$ the horizon.  Given access to a function class (representing either the transition kernel in model-based or the $Q$ function for model-free) typical algorithms come up with complexity measures of the function class and design algorithms with sublinear regret with respect to that complexity measure.  The main point of this paper is to present a unified complexity measure framework that includes all model-free and model-based RL classes, while simultaneously allowing a simple algorithm with sample efficiency guarantees.  To this lens, the authors make two several frameworks:\n1. The authors propose a new framework called ABC that covers structural assumptions previously studied in model-free and model-based RL.  To some extent, the framework can be thought of as extending the Eluder Dimension to include (a) arbitrary coupling functions for measuring the error, (b) a discriminator function which characterizes the interaction between different hypotheses\n2. Under the ABC framework the authors design an algorithm OPERA based on maximizing value function in a confidence region around model estimation - showing that the framework has sublinear regret guarantees.\n\n## Questions\n- What is the role of $\\pi_{est}$ on page 8?\n\n## Minor Comments\n- FLAMBE is used to refer to both the model and the algorithm\n- Value terms are swapped in regret definition on page 3\n- Definition of $\\pi_f(s)$ on bottom of page 3 has typo in it",
            "strength_and_weaknesses": "## Strengths\n\n1. The authors provide a novel problem complexity measure which subsumes several others which are studied in the literature.\n2. The authors provide a simple algorithmic framework based on optimistic principle over uncertainty sets which achieves strong regret performance.\n\n## Weaknesses\n\n1. The authors include no discussion on the computational complexity of the algorithm.\n2. There are no theoretical matching lower bounds (or discussion along this point)\n3. The authors provide no empirical results of their algorithm's performance\n4. The theoretical contributions and algorithm design seems like a straightforward extension of prior mechanisms with general function approximation techniques.  The novel algorithmic contributions could be highlighted more in the writing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nThe paper is extremely well written and easy to follow, although a bit on the technical side. I listed some small comments on writing earlier up in the review. The two most useful clarifications which I think could help:\n- Clarify the roll of $\\pi_{est}$ on page 8\n- Simplify section 3.2 (delaying some examples which readers could be familiar with in the literature) to allow for more discussion around ABC in Section 3.2\n\n## Quality + Novelty\n\nThe paper provides novel theoretical contributions for developing an encompassing framework to understand the complexity of model-free and model-based reinforcement learning with function approximation.  The theoretical results then highlight the fact that the definition encompasses many well-studied notions of complexity in the literature.  They complement the new measure with a simple algorithm based on confidence sets which achieve strong regret guarantees (although no matching lower bounds are included).  ",
            "summary_of_the_review": "The paper provides strong theoretical contributions for developing a framework which unifies model-based and model-free RL via their Admissible Bellman Characterization formulation - which subsumes most models in the literature for tractable RL with function approximation.  The paper is extremely well written, although very technical, but highlights the main differences and contributions between the paper and the related work. However, the paper offers no empirical results (or discussion on computational tractability of their proposed algorithm), although standard in the RL with function approximation literature.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2264/Reviewer_2LkA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2264/Reviewer_2LkA"
        ]
    },
    {
        "id": "Z56GDBASRq2",
        "original": null,
        "number": 2,
        "cdate": 1666583998138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583998138,
        "tmdate": 1666583998138,
        "tddate": null,
        "forum": "dqITIpZ5Z4b",
        "replyto": "dqITIpZ5Z4b",
        "invitation": "ICLR.cc/2023/Conference/Paper2264/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a unified perspective and analysis for almost all of the existing sample efficient RL algorithms.\nThis notion is based on Admissible Bellman Characterisation, and provides an a bound in terms of the functional eluder dimension.\nTogether with this analysis, the authors present an algorithm OPERA, which really is a general form of optimism in the face of uncertainty.\nWhile this algorithm is not necessarily practical (it may not be computable) it provides some general flavour of the types of results that might be possible.",
            "strength_and_weaknesses": "There are several things to like about the paper:\n- The writing and paper structure are good, and give a good overview of the many lines of research in this area.\n- The general analysis and flavour of the results are clear and concise. Take, for example, Theorem 12 as an example. This clearly highlights the main dependencies in the regret bound.\n- The ABC class supersedes the many different notions of complexity, as outlined well in Figure 1.\n\nThere may be a few places where the paper falls down in terms of top-level impact:\n- First, it feels like this work is really mostly consolidating/summarising threads of research that have been going on for a while in the eluder-dimension++ pieces of RL with generalization. This is valuable, but feels more \"cleaning up\" than forging new course.\n- The OPERA algorithm is really more of a high-level principle than a practical algorithm. Thinking about how we can make scalable/practical algorithms is probably not just an \"unnecessary complication\" but I agree it is OK to leave this to future work in this conference paper.\n- There is a *lot* of material to go through in reviewing this paper... so many theorems and definitions. This is great for clarity, since it allows a motivated reader to go through piece by piece, but there is a slight concern with *so many* pieces that it takes away from the \"main thing\".",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I think the paper scores very highly in clarity.\nThe quality, and reproducibility also appear to be solid... although I have not checked all steps in detail.\n\nIn terms of *novelty* I think the main contribution is the calm and coherent way of combining lots of existing perspectives, and rationalizing this treatment in one place.\nThis is definitely valuable, but I would not rate *novelty* as one of the main strengths.",
            "summary_of_the_review": "This paper collates many existing results in the OFU-RL literature with function approximation.\nBy revisiting the functional eluder dimension under the ABC condition, they show that OPERA algorithm provides a more unified perspective on statistically efficient reinforcement learning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2264/Reviewer_TL1B"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2264/Reviewer_TL1B"
        ]
    },
    {
        "id": "Cmutf92NDLO",
        "original": null,
        "number": 3,
        "cdate": 1666640908513,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640908513,
        "tmdate": 1666640908513,
        "tddate": null,
        "forum": "dqITIpZ5Z4b",
        "replyto": "dqITIpZ5Z4b",
        "invitation": "ICLR.cc/2023/Conference/Paper2264/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a unified framework which encompasses both model-based and model-free MDP algorithms. The framework, Admissible Bellman Characterization, covers a wide range of MDPs. The key idea is to use decomposable estimation function which has decomposable sturctural properties for optimization-based exploration (which is also their algorithm). To measure the complexity of model class, they use the functional eluder dimension. ",
            "strength_and_weaknesses": "Strength:\n1. The authors proposed a unified framework which covers a large section of MDPs, both model-free and model-based\n2. The framework maintains the best-known sample complexity results for special instances.\n3. They also propose a novel algorithm, OPtimization-based ExploRation with Approximation (OPERA), which maximizes the value function constrained in a small confidence region around the model minimizing the estimation function.\n\nWeakness:\nAlthough this framework improves the sample bounds, it is as generic as the existing framework, meaning the algorithms which were not covered in previous framework (such as those with state-action aggregation) are also not covered here.",
            "clarity,_quality,_novelty_and_reproducibility": "Good",
            "summary_of_the_review": "It is good paper, written quite well. It improves on the existing frameworks which could not preserve the sample complexity bounds. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2264/Reviewer_JjVG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2264/Reviewer_JjVG"
        ]
    }
]