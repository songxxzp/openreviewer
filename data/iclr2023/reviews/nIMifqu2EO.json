[
    {
        "id": "nw6XyuvuYk",
        "original": null,
        "number": 1,
        "cdate": 1666580015466,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580015466,
        "tmdate": 1668824993855,
        "tddate": null,
        "forum": "nIMifqu2EO",
        "replyto": "nIMifqu2EO",
        "invitation": "ICLR.cc/2023/Conference/Paper5302/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Three types of biologically-plausible learning algorithms --- predictive coding, contrastive hebbian learning, and equilibrium propagation --- are reformulated as energy based models. This framework is used to provide a unifying description for when these algorithms closely approximate gradient descent (backprop), and the authors use this new perspective to suggest a new variant of predictive coding called PC-Nudge. Experiments with a multi-layer neural network on MNIST are used to support the proposed ideas. ",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-written and clear.\n- The new energy-based model perspective is a way to compare and contrast three different non-backprop  learning algorithms, and understand how they deviate from backprop (gradient descent on the supervised loss). \n\nWeaknesses:\n- It is not obvious to me that viewing these algorithms as gradient descent on an energy function is useful.\n- One justification given for proposing this new framework is that it can be used to propose new algorithms.  PC-Nudge is proposed. However, this algorithm seems to be a minor variation on existing algorithms, and the experimental results on MNIST are not particularly interesting. \n- Performing gradient descent on an energy function is not necessarily biologically plausible, and this paper does not have much to say about the biological plausibility of these various algorithms. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, high-quality, and original. ",
            "summary_of_the_review": "This is a well-written paper that might be of interest to some in the community, but despite my familiarity with the algorithms discussed, I was unable to see how it is particularly useful to view them from the energy-based model perspective.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5302/Reviewer_oU1M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5302/Reviewer_oU1M"
        ]
    },
    {
        "id": "bro10lNT8K",
        "original": null,
        "number": 2,
        "cdate": 1666670447223,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670447223,
        "tmdate": 1666670447223,
        "tddate": null,
        "forum": "nIMifqu2EO",
        "replyto": "nIMifqu2EO",
        "invitation": "ICLR.cc/2023/Conference/Paper5302/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper starts from a PCN and expands around the limit where the output is not tethered to the target. Perturbing in the parameter $\\lambda$, which may be interpreted as the precision parameter deciding how tightly would the target be tied to the output, builds a useful bridge to many other approaches to training neural networks.",
            "strength_and_weaknesses": "Strengths:\nAnalysis of the small $\\lambda$ limit\nThe PC-nudge model\nDiscussion of contrastive Hebbian learning, starting from PC.\n\nMinor issues:\nIn equations (3) and (4) the indexing conventions of $W_l$ and $x_l$ seems to be inconsistent with the definition of $\\epsilon_l$ in the previous page.\nUnder equation(5), the sentence mentioning \u201c..neurons which are co-active at the clamped phase are strengthened\u2026\u201d is confusing. What does 'neurons being strengthened' mean? What is really strengthened? Synaptic strengths? That seems to have the opposite effect. A clearer explanation here would help.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this is a well-written paper, needing some small modifications.",
            "summary_of_the_review": " I found this paper useful for relating predictive coding networks with other competing approaches. It is a welcome addition to the literature.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5302/Reviewer_wqJn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5302/Reviewer_wqJn"
        ]
    },
    {
        "id": "-b_pYQwBZpn",
        "original": null,
        "number": 3,
        "cdate": 1666696731429,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696731429,
        "tmdate": 1668522567476,
        "tddate": null,
        "forum": "nIMifqu2EO",
        "replyto": "nIMifqu2EO",
        "invitation": "ICLR.cc/2023/Conference/Paper5302/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper unifies several approximation to backpropagation through the energy-based models view. ",
            "strength_and_weaknesses": "### Strengths\n\nThe paper unifies several backprop approximation under the same energy-based framework.\n\nThis framework leads to different approximations.\n\n### Weaknesses\n\nI don\u2019t understand what the take-home message is. Predictive coding and equilibrium propagation were already formulated in a similar fashion, but what can we do with it now? I'd guess deriving more plausible algorithms would be interesting, but it's not really discussed.\n\nContrastive Hebbian learning narrowly fits the proposed framework. Eq. 18 seems to be just the gradient of the loss (since $I$ is zero in that case), which is not what CHL is doing. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nOverall, the paper is well-written. Some minor comments:\n\nBoth figures (especially Fig. 1) should be exported in either a vector format or much higher resolution.\n\nFig.3B: PC-nudge is completely hidden behind backprop dynamics.\n\nSec. 2.3, first line: capitalize Hebbian.\n\nBefore Eq. 15: If we assume that the gradients of each component of the energy with respect to the weights -is- continuous and differentiable -> are.\n\nJust before Sec. 3.4: capitalize Hopfield.\n\nEq. 23: the norms should not be squared in cosine similarity.\n\n### Quality \n\nThe technical contributions are somewhat limited as the paper mainly re-writes the three existing algorithms in a different form. The experimental contributions are very limited as the proposed algorithm, PC-nudge, is tested on a single network and only on MNIST. It's not clear if the reasonable alignment with backprop will hold for large networks/harder tasks. Also, what's the takeaway from this algorithm?\n\n### Novelty\n\nThe connection between different algorithms is, to my knowledge, novel. All relevant literature is cited.\n\n### Reproducibility\n\nThe code is not provided but it's implied that it will be. The authors can attach a zip file with the code to the submission.\n\n\n**UPDATE**: the code is now provided.",
            "summary_of_the_review": "Potentially interesting connection between algorithms, but I'm not sure what we can do with it. I'm willing to discuss it further, however.\n\n**UPDATE**: updated the score from 5 to 6 post-rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5302/Reviewer_mXjX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5302/Reviewer_mXjX"
        ]
    },
    {
        "id": "d3bhU4rAKI",
        "original": null,
        "number": 4,
        "cdate": 1666724928356,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724928356,
        "tmdate": 1666724928356,
        "tddate": null,
        "forum": "nIMifqu2EO",
        "replyto": "nIMifqu2EO",
        "invitation": "ICLR.cc/2023/Conference/Paper5302/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A number of different algorithms have been proposed as more biologically plausible alternatives to backpropagation.  While different works have shown how several of these proposals approximate backprop by converging to it in some limit, the authors here outline an overall framework of energy-based models that encompasses several predictive coding proposals, equilibrium propagation, and contrastive Hebbian learning, showing how they all can be formulated as approximations to backprop.  By breaking the global energy into components corresponding to the internal energy of the network and the network's cost function, the authors show that different algorithms can approximate backprop by finding equilibrium points near the minimum of the internal energy (by, e.g., employing weak feedback).  They finally show that other algortihms can be derived from their approximation, with \"PC-nudge\" described as an example. ",
            "strength_and_weaknesses": "Strengths\n\n1. The authors gather previously disparate theoretical results together under the same umbrella of energy-based models.\n\n2. Several versions of predictive coding, along with equilibrium propagation and contrastive Hebbian learning are shown how, under the same framework, they can be derived as approximations to backprop.\n\n3. A novel algorithm, \"PC-nudge,\" is shown to be derivable from the proposed framework\n\nWeaknesses\n\nNo major weaknesses of the manuscript were found\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the present work stems from the amalgamation of several lines of work under one framework.  The manuscript is fairly clear and staightforward",
            "summary_of_the_review": "Several results are gathered and combined by showing how a number of proposed biologically plausible alternatives to backprop can be shown to approximate backprop when formulated as energy-based models. The work then leads to an example of a novel algorithm that can be derived using this common framework",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5302/Reviewer_Cz4e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5302/Reviewer_Cz4e"
        ]
    }
]