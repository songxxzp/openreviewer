[
    {
        "id": "nqDV0GoAZSN",
        "original": null,
        "number": 1,
        "cdate": 1666468804953,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666468804953,
        "tmdate": 1666468804953,
        "tddate": null,
        "forum": "53T6FlFulCV",
        "replyto": "53T6FlFulCV",
        "invitation": "ICLR.cc/2023/Conference/Paper258/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose DyDecNet, a new network architecture for counting sounds in highly polyphonic environments. The method contains a series of T-F band-pass filters which double in each layer to provide a learnable alternative to other frequency representation methods. It then contains a backbone and regresses the final count of sound sources. They also propose 3 new metrics for measuring polyphonic sounds.",
            "strength_and_weaknesses": "The paper overall is good and interesting. The problem is one that has not been explored as thoroughly, and they are tackling highly polyphonic sounds in a way that hasn't been done before. The idea of learnable TF filters that grow exponentially with depth is interesting, and the ablation studies show that it is better than using other features like Mel or STFT features. The results seem pretty good, able to achieve solid counting accuracy with high numbers of concurrent sounds.\n\nThe clarity and organization of the paper could be improved significantly. For example, the section with Table 1-4 are very confusing. It should be possible to look at the tables and interpret them without searching for the description in the text. Table 1 is a dataset description, 2, and 3 are results, and 4 is an ablation study. But they are all presented as one unit, making it confusing. The caption for table 3 needs to say what dataset it was on, and table 4 should say \"ablation study\" and probably be included further down when the text is talking about the result. All the tables in general could do with a slightly more descriptive caption describe what result is being presented. I had to search through the text and back at the table to understand them.\n\nI would also like to have some supplementary results to listen and compare  qualitatively . Hearing some of these datasets would be great (both for reviewers and listeners). It would also help us understand how difficult this problem is for human listeners. The ideal demo would be a video of highly polyphonic sounds with the prediction from your network being overlaid as the video plays.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - Described Above. Presentation and flow could use work\nQuality - Good experiments, ablation studies, results.\nReproducibility - Well described\nNovelty - I don't know if the proposed learnable T-F filters has been done before. To my knowledge I haven't seen anything exactly the same, but I wouldn't be surprised if something similar exists in the literature.",
            "summary_of_the_review": "Interesting approach, good results and experiments. Paper needs to be more clearly presented and qualitative results/examples would strengthen the work a lot by giving the necessary context for the problem. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper258/Reviewer_6EcX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper258/Reviewer_6EcX"
        ]
    },
    {
        "id": "XSWiZr2OH9",
        "original": null,
        "number": 2,
        "cdate": 1666544727168,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666544727168,
        "tmdate": 1668759525477,
        "tddate": null,
        "forum": "53T6FlFulCV",
        "replyto": "53T6FlFulCV",
        "invitation": "ICLR.cc/2023/Conference/Paper258/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work introduce a soundcount method. Their method is about how to conunt the number of distinct sound event. ",
            "strength_and_weaknesses": "I am not a person in acoustic data. Everything in this area is interesting to me. My comments for this work are listed as follows:\n1. Can the author better claify the correlation bettwen this work and blind signal speration? It seems that this work has some close correlation with that problem. \n2. I am a person in image separation. I know that such kinds of problems are very difficult.Even with different priors or assumption, we still cannot infer some highly mixed image contents. I think it should also be a problem for the acoustic issue. How do they address this problem in your work?\n3. Do you need to consider the domain adaption issues? Or do you need to rely on some specific priors to achieve this goal about sound count?\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think this work is interesting to me. Its method is also nover in my opinion",
            "summary_of_the_review": "I spend about 1 hour to review this paper. I just feel that the authors needs to provide more details about how do they solve the domain generalization problems. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "I think this work will lead to some privacy issue, since such technolgy is improperly used. It may lead to some negative social impacts. ",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper258/Reviewer_5ud3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper258/Reviewer_5ud3"
        ]
    },
    {
        "id": "DQSBHE7X7tf",
        "original": null,
        "number": 3,
        "cdate": 1666574422923,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574422923,
        "tmdate": 1666574422923,
        "tddate": null,
        "forum": "53T6FlFulCV",
        "replyto": "53T6FlFulCV",
        "invitation": "ICLR.cc/2023/Conference/Paper258/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper discusses a new network architecture for counting the number of sound events in an audio signal, primarily focusing on bird calls but also evaluating with other sound classes. The main contribution of the work are two-fold:\n1. A novel frontend for raw audio feature learning called dyadic frequency decomposition. This involves progressively increasing number of band-pass filters focusing on extracted information from high frequency and low frequency content of the parent filter's outputs. Additionally, a learned energy normalization layer is applied at each layer for improved loudness invariance. This approach converts raw waveform to a frequency representation using a multi-stage approach as opposed to other approaches which use a single-stage filterbank. \n2. The authors propose new polyphony-based difficulty metrics for tasks involving polyphonic audio. They rate the datasets they use for evaluation using these metrics. They find that the results are consistent with the difficulty of the datasets.\nComparison against baselines from sound-event detection literature show that the so-called DyDecNet outperforms these baselines on most metrics. These comparisons are carried out using various modalities of audio data such as bird sounds, everyday sounds like phone calls, car engine noise, as well as music.\nVarious ablation studies show the benefits of the different techniques used in the network.",
            "strength_and_weaknesses": "Strengths:\n\n- The front-end network is novel and involving a multi-stage filterbank design for audio representation learning. The authors make a strong case for this frontend compared to traditional approaches for fixed or learnable filters for audio.\n- The results obtained are strong compared to the baseline approaches.\n- The ablation studies account for most if not all moving parts of the proposed model DyDecNet. \n- The authors evaluate on a combination of real and synthetic datasets. The synthetic dataset is constructed using non-naive approaches involving room acoustics simulation and stochasticity.\n\nWeaknesses:\n\n- The Dydecnet architecture is presented as a generally more useful frontend for processing audio in neural networks, however it is only evaluated using sound counting as the task. It is not clear how this architecture is specifically useful for the counting problem. This paper has the potential to be a really strong paper had the authors not focused only on sound counting but also evaluated for other audio-related tasks as well, such as SED, source separation, etc.\n- I am very familiar with the OpenMIC dataset for musical instrument classification. The authors mention that it contains the number of instrument events present in each clip. This is not true. The OpenMIC dataset is a partially labeled dataset where each audio clip is labeled with whether a subset from 20 instruments is present or not. As an example, there may be a clip which is labeled with 'vocals', 'guitar' and 'bass' present, and 'flute' absent. This does not mean that the clip has only 3 counts of sound events. It could have any number of notes being played by each instrument. Additionally, this also does not mean that the remaining 16 instruments are absent. It could very much be the case that there are 'drums' and 'violin' playing in the clip. They are just not labeled during the crowdsourcing campaign. This makes me question the validity of any experiments involving this dataset since the regression target is neither the number of sound events bring played, nor the number of unique instruments present. Perhaps a different music dataset should have been using such as MusicNet (https://zenodo.org/record/5120004).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written clearly and I was able to understand most of the details well. My only qualm is that a lot of details are relegated to the appendices and I find myself jumping back and forth often.\nQuality: The quality of work done is satisfactory apart from my one major complaint about the authors misunderstanding a particular dataset/\nNovelty: The architecture and task presented are both novel and the original. \nReproducibility: A lot of the details are present in the appendix which would enable the public to build the dydecnet model architecture. The supplementary materials contain the code as well. Not clear if the authors will release code to reproduce their results.",
            "summary_of_the_review": "My overall impression is that the authors identified an issue in existing learnable audio frontends and proposed a new network architecture to address those issues. However, they demonstrate its effectiveness using the sound-counting task which is not a very popular area of research and could be considered a novel task. The issue lies in the fact that it is not clear how the new architecture is specialized for counting. The paper would be stronger if the evaluations included additional tasks like audio classification, source separation, etc. \nAdditionally, while I commend the authors including a music dataset since music is often highly polyphonic, based on my knowledge it would seem that there is a mismatch between the dataset chosen and the task the network is trained for as I have described in the weakness section. Given these points I am choosing a rating of 3.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper258/Reviewer_3j8s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper258/Reviewer_3j8s"
        ]
    },
    {
        "id": "lCkLMuItA22",
        "original": null,
        "number": 4,
        "cdate": 1666624216934,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624216934,
        "tmdate": 1666626424478,
        "tddate": null,
        "forum": "53T6FlFulCV",
        "replyto": "53T6FlFulCV",
        "invitation": "ICLR.cc/2023/Conference/Paper258/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new task, sound counting and a novel neural network architecture to tackle it based on a dual decomposition of the raw waveform. The proposed Dyadic model is first presented, along with new metrics for the task. The model is then evaluated on four partially synthezised datasets and compared with baseline methods. The proposed model is shown to outperforms the baselines on every dataset. Ablation studies are also presented, validating the different aspect of the architecture.",
            "strength_and_weaknesses": "Strengths:\n - The paper introduces a new task, proposes a novel model to tackle it and provides thorough comparison with baseline models.\n - The paper introduces new metrics for the task.\n - The proposed approach is significant as it outperforms the baselines\n\nWeakness:\n - The paper is not very clear:\n      - It sometime makes some very bold and unsupported statements (see below)\n      - Some references are missing and the proposed approach needs to be situate better with the existing works.\n      - The experiments are a bit confusing.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written and somewhat easy to follow, but it has some issues:\n - My main concern is that the paper is written in way that assumes that the dyadic approach is better than any other CNN-based approach by making bold statements without any reference to support them. For example: \"Such shallow and one-stage processing is not powerful enough to learn robust representation where large loudness variance and heavy spectrum overlap exist in sound count task under high-polyphonic condition\" (Sec 3). Moreover, there is no literature review on dyadic networks applied to other tasks that could support those claims. So I think it the paper should be rewritten slightly and should state that it is a hypothesis that the proposed approach is better, which will be then confirmed by the results.\n- In line with my previous comment, a short literature review on dyadic networks should be provided.\n- The experiments section is also quite hard to follow. I think it would be better to have one table which summarises all the results from the four corpora instead of splitting between the main paper and the appendix. The new metrics are also quite confusing: while they are sound for the task, they seem to never be used during the evaluation (only in Table 1). Please clarify.\n- Some better motivation should also be added for the proposed architecture: why splitting the signal in two part (high and low frequencies) is better than a CNN, which could learn that by itself? I like the architecture, a bit more motivation would benefit the paper.\n- In the Introduction, the footnote 1 seems contradictory to the statement it's attached to: \"sound counting problem under highly polyphonic, cluttered and concurrent situation\" and \"To make sound countable, we focus on short, consistent and acoustically separable sound in this work.\". So is the audio cluttered and concurrent, or consistent and separable? Please remove the footnote and clarify.\n- In Section 3.3 the paper seems to imply that it is the first one to use 1D convolution: \"Unlike existing methods [7, 1] that treat the frontend learned TF representation as a 2D image that is convolved by 2D Conv. operations, we treat it as a temporal representation so that we just need 1D Conv. to process it\". This is misleading, as 1D convolution are widely used in audio and speech. Please clarify.\n- The 5th paragraph of the Introduction starting with \"Alongside the network, we propose three polyphony ...\" need to be rewritten as it's not finished.\n- I never saw the term \"backbone\" used here, wouldn't \"backend\" be better?\n- The visualization comparison with MFCC is not helpful: It says that \" traditional TF features (in our case, MFCC) encode cluttered and mixed TF representation that is much less visually separable\" but MFCCs were never designed to be visually separable, and I don't understand why it matter. Please clarify or remove the visaulisation entirely.\n - In the Experiment section, the authors should be more objective with their statements: \"Our framework shows excellence\". This is discussable, as excellence is not well defined. Writing some like \"is better than the baseline\" is more objective.  \n- Some earlier references are missing for the raw speech literature review, like [1] and [2].\n\nA part from those concerns, The proposed approach is novel as far as I can tell and the findings are significant as a new task is introduced, providing a good benchmark. The relevance of this task to the ICLR community is maybe limited.\n\n[1] Palaz, D., Collobert, R., & Magimai-Doss, M. \"Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal Using Convolutional Neural Networks\", proc. of Interspeech, 2013\n[2] T\u00fcske, Z., Golik, P., Schl\u00fcter, R., & Ney, H. \"Acoustic modeling with deep neural networks using raw time signal for LVCSR\", proc. of Interspeech, 2014 ",
            "summary_of_the_review": "The paper introduces a new audio task and proposed a novel approach to solve it, which outperform the baselines. The architecture is novel as far as I can tell and the findings are significant, so I am keen to accept the paper if all of my concerns about clarity are addressed. But in the current form I recommend rejection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper258/Reviewer_xW8d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper258/Reviewer_xW8d"
        ]
    },
    {
        "id": "R9-CoIBKh9y",
        "original": null,
        "number": 5,
        "cdate": 1666861830173,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666861830173,
        "tmdate": 1666861830173,
        "tddate": null,
        "forum": "53T6FlFulCV",
        "replyto": "53T6FlFulCV",
        "invitation": "ICLR.cc/2023/Conference/Paper258/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes:\n1) A novel model architecture and training method for the task of sound source counting.\n2) Several novel measures to assess the difficulty of the sound source counting task.\n\nThe model architecture comprises 1) a feature extractor that operates hierarchically as a binary tree of cascaded filters, 2) a mechanism to pool across filters, and 3) 3 different methods to compute the output count from the pooled features.  Counting can be done both per-frame and per-session.\n\n3 measures are proposed to assess the difficulty of the sound source counting task, all related to the amount of polyphony present in the audio.\n\nMultiple datasets are used to assess the performance of the proposal, but public release of the created synthetic datasets is not reported in the paper for experimental reproduction.\n\nQuestions:\n- What aspect of the model's performance is the chosen depth expected to impact?  Does the depth affect the effective maximum number of sources that the model can count?  Does the depth affect the frequency resolution that the model can rely on?  Does the depth affect the high frequency limit that the model can use?  How does the exponential scaling of the number of filters with depth affect the computational cost of training and inference?\n- What are the sizes of the train and test splits of each dataset?  Such information is important to allow a reader to judge the applicability of the measured results.\n- No reference is given to justify the motivating assumption of \"human\u2019s capability in discriminating different sound events reduces\nseriously when the number of temporal-overlapping sound event number increases\".\n- In the statement \"The child filter carrying the higher half frequency response\nencode the parent\u2019s processed intermediate waveform\u2019s detail while the other one carrying the lower\nhalf frequency response instead encodes the approximation\", why does the higher half encode detail and the lower half encode approximation?  What do you mean by \"approximation\" and \"detail\"?\n\nIthenticate similarity score is 1%, which is good.\n",
            "strength_and_weaknesses": "Strengths:\n- The design of the proposed model and training, and the spectrum of datasets experimented upon, seem quite well thought through, to overcome many potential questions that may have otherwise arisen.  Specific justifying examples are:\n-- The use of the hierarchical filtering binary tree seems to be well justified, based on an understanding of the expected acoustic properties of the sounds involved.\n-- The separation of the model into feature extractor and backbone provides a natural taxonomisation of the model parts to different functions, while still being able to be jointly trained.\n-- The model is able to count the sound sources both per-frame and per-session.\n-- The use of energy gain normalisation shows an example of the authors carefully considering potential issues that may go wrong, for this specific task.\n- The experiments are repeated on multiple diverse datasets, to demonstrate generalisable trends of the proposal.\n\nWeaknesses:\n- There is a lack of literature review to the highly related fields of speaker diarisation and sound source separation.  Sound source separation is only mentioned briefly in the conclusion, and no reference is given to related papers.  Speaker diarisation consists of two subtasks; 1) counting the number of speakers in an audio session, and 2) clustering the time segments of audio that belong to the same speaker together.  Sound source separation consists of two subtasks; 1) Counting the number of sources in an audio session, and 2) computing the audio of each separated source.  In both speaker diarisation and sound source separation, the two subtasks can either be done jointly within a single model or separately in a cascaded manner.  If done separately, then the first subtask is exactly the same task that this current paper is trying to address.  If done jointly, then the speaker count can be computed from the joint output, which indirectly also does the task that this current paper is trying to address.  Both speaker diarisation and sound source separation have several publications that consider neural network methods to either do these subtasks separately or jointly.  Thus, the existence of these works that have not been described in this current paper may somewhat reduce the novelty that this current paper claims.\n\n- There are many spelling and grammatical inaccuracies in the paper.  Please proofread the paper.\n\n- The created synthetic datasets do not seem to be made publicly available for experimental reproduction.  However, the paper also uses existing public dataset, which should allow the results to be experimentally reproducible.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper describes many many concepts, and as a result, the writing is fairly dense in content, and somewhat difficult to keep track of.  However, the need for this density is understandable, given the potential queries that may arise during the review process.\n\nThere are many spelling and grammatical inaccuracies in the paper.\n\nIt may not be accurate to claim sound source counting as an underexplored problem, because of the existence of conferences with major tracks dedicated to acoustic event detection, sound source localisation, speaker diarisation, and speech separation, all of which involve sound source counting.  Thus, the existence of prior works related to the same problem in these tasks may somewhat limit the claimed novelty of this current paper.\n\nThe method proposed appears to be reproducible.  Several public datasets are used, which also allows for reproducibility.  However, the created synthetic datasets do not appear to be made publicly available, which therefore imposes difficulty of reproducing those specific experiments.\n",
            "summary_of_the_review": "The claimed novelty of the task and model are somewhat diminished by the existence of prior works in diarisation and sound source separation.  However, the proposed task difficulty measures still appear fairly novel.  Taken together, the novelty in this paper should suffice.\n\nSufficient details are given in the paper for adequate experimental reproduction.\n\nThe proposed approaches seem well justified.\n\nThere is a need to further proofread the paper, to minimise spelling and grammatical inaccuracies.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper258/Reviewer_6cJE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper258/Reviewer_6cJE"
        ]
    }
]