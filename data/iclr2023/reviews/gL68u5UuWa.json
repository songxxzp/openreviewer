[
    {
        "id": "jaLBeCmA-l2",
        "original": null,
        "number": 1,
        "cdate": 1666692771177,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692771177,
        "tmdate": 1670286011348,
        "tddate": null,
        "forum": "gL68u5UuWa",
        "replyto": "gL68u5UuWa",
        "invitation": "ICLR.cc/2023/Conference/Paper5327/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes new methods for Simulation Based Inference. The goal is to perform Bayesian inference on model parameters given data for situations where the likelihood function is not known in closed form and can only be accessed through a simulation mechanism. The central challenge of SBI learning is \"doubly intractibility\" because the normalizer of the posterior distribution depends on the model parameter. This work proposes two energy-based methods for SBI. The first bypasses the doubly intractibility by defining a joint EBM over $X$ and $\\theta$ so that the normalizer does not depend on $\\theta$. This allows amortized posterior sampling for any observed data. To tailor the posterior inference to a specific observed datapoint, the work further proposes to use an EBM to learn the conditional distribution and performs doubly intractible inference. Results show competitive performance with score based SBI models for toy datasets and a crab biology dataset.",
            "strength_and_weaknesses": "STRENGTHS\n\n1. The AUNLE formulation provides a nice way of dealing with a major limitation of double intractibility by tilting the normalizer. There are clear methodological and computational benefits to this approach, but it still remains limited in its ability to fine-tune the learned posterior to a specific observed datapoint.\n2. Experimental results show that AUNLE and SUNLE can outperform score-based SBI for multimodal datasets, and that SUNLE can outperform score-based SBI for complex biological data.\n\nWEAKNESSES\n\n1. The presentation of equations and algorithms is not always clear. In Algorithm 2, should the dataset be $(X_i, \\theta_i)$ instead of just $(X_i)$? In Equation 5 and Algorithm 4 is the data variable in the intractible term meant to have the index on the $X_i$, or should it be $X$ like in Algorithm 2, since it is a generated sample and not a data sample?\n2. The novelty of the SUNLE method is somewhat limited because it simply accepts the doubly intractible limitation and proceeds with standard EBM learning. Standard methods for flow and score models have already been used for a nearly identical purpose. \n3. It would be interesting to see other real-world applications similar to the crab synapses, although benchmarking and evaluation are likely difficult.",
            "clarity,_quality,_novelty_and_reproducibility": "The AUNLE method deals with a major theoretical limitation of existing SBI methods and is the most technically original part of the work. The SUNLE method is a straightforward application of EBM learning to a conditional density estimation problem that was previously tackle by flows and score models. More thorough experimental results would improve the paper, although I am not familiar with the area of SBI and I cannot suggest specific examples.",
            "summary_of_the_review": "The most interesting part of the paper is the tilting of the normalizer in the AUNLE method, although unfortunately this method likely will not be able to achieve the performance of models tailored towards inference for specific data. The lack of novelty of the main SUNLE method and sparse experimental results lead me to recommend not accepting this paper at this time.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_kgMn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_kgMn"
        ]
    },
    {
        "id": "nikLAA3Xn4i",
        "original": null,
        "number": 2,
        "cdate": 1666731733597,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731733597,
        "tmdate": 1666731733597,
        "tddate": null,
        "forum": "gL68u5UuWa",
        "replyto": "gL68u5UuWa",
        "invitation": "ICLR.cc/2023/Conference/Paper5327/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for learning synthetic likelihoods in the likelihood-free setting of simulation-based-inference (SBI).  The method fits a particular tilted version of the joint distribution of the parameters and observations.  Under certain assumptions, the solution of this optimization problem is proportional to the conditional density of the data given the parameters.  As such, the authors then use this unnormalized conditional density to estimate posteriors using standard approaches (e.g., MCMC).  The authors also present a sequential version of this approach.  The authors apply these methods to 4 toy datasets and compare to other SBI posterior inference methods, where they obtain better performance on one of the toy problems.  Finally, they apply their method to a neuroscience dataset, where they can obtain a good approximate posterior using fewer simulations than a previous method.",
            "strength_and_weaknesses": "Strengths\n* The proposed approach is straightforward, conceptually neat, and very interesting.\n* The paper is well-written and the method and results and presented exceptionally clearly.\n* I appreciate that in the toy models the strengths _and_ weaknesses of the present approach relative to existing methods (e.g., normalizing flows) are presented.  I think that the gains of the present approach on complex posteriors are sufficiently interesting (and well-explained and well-motivated) to more than outweigh the comparable (or slightly worse) performance to existing methods on simpler toy problems.\n\nWeaknesses:\n* The key assumption of AUNLE is $(Z, \\theta)$-uniformization.  This allows one to interpret the learned energy model as a likelihood up to an unknown normalizing constant that is independent of $\\theta$.  If this uniformization does not hold, then the normalizing constant will depend on $\\theta$, resulting a double-intractable problem.  Given that uniformization only holds if 1) the energy model is sufficiently flexible such that the true joint distribution lies within the space of learnable models; and 2) the optimization problem can be exactly, globally solved, it seems like in practice uniformization will always be violated.  Such assumptions are frequently made in SBI (e.g., it is common to plug approximate posteriors into various formulas involving the true posterior) so I don't think that this is deeply problematic (and indeed such assumptions are often justified by good empirical performance), but I would be interested to see some empirical results about how much uniformization is violated in practice and how much that affects downstream inference.  One possibility would be to run double-intractable methods on the learned energy model (without assuming that the normalization is independent of $\\theta$) for an extremely long time to obtain a \"ground truth\" and compare to the output of AUNLE.  Another possibility would be to estimate different normalizing constants for the implied likelihoods for different values of $\\theta$, and see the extent to which they differ.  In any case, I think it should be made more explicit in the main text that this uniformization is unlikely to _exactly_ hold in practice, and the rest of the AUNLE methodology hinges on it holding  _approximately_.\n* For all results it would be good to show some degree of replicability across initializations and random seeds.  How different are the different methods relative to differences in neural network initialization and randomness in the simulations?  Are the exact same simulations used to train the different methods, or are different random sets used?  E.g., the difference between using MALA and SMC within AUNLE results in comparable differences on SLCP as compared to the difference between AUNLE and NLE.\n\nTypos:\n* \"making naive Bayesian inference impossible\" --> \"making standard Bayesian inference impossible\"\n* \"each of which endowed with\" --> \"each of which is endowed with\"\n* Species names are never capitalized: \"_Cancer Borealis_\" --> \"_Cancer borealis_\"\n* \"given an observed an neuronal recording\" --> \"given an observed neuronal recording\"\n* \"We show that using this new methods can\" --> \"We show that using these new methods can\"\n* \"does not suffer from the bias of incurred by the\" --> \"does not suffer from the bias incurred by the\"\n* \"the number of steps (or intermediate distributions) is SMC is beneficial\" --> \" the number of steps (or intermediate distributions) in SMC is beneficial\"\n* \"When applying SMC to within AUNLE\u2019s training\" --> \"When applying SMC within AUNLE\u2019s training\"\n* \"using more SMC samplers steps usually increase the quality\" --> \"using more SMC samplers steps usually increasea the quality\"\n* \"comparing multirond sequential methods\" --> \"comparing multiround sequential methods\"\n* \"posterior estimate on the _C. Borealis_ simulator\" --> \"posterior estimate on the _C. borealis_ simulator\"",
            "clarity,_quality,_novelty_and_reproducibility": "The approach proposed in this paper is, to my knowledge, novel.  It is interesting, and clearly presented.  The authors have code for reproducing results available at an anonymous github repo.  I did not run their code, but it looks thoroughly documented.",
            "summary_of_the_review": "Overall, I think this is a well-written interesting paper providing a promising approach for simulation-based-inference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_m5HY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_m5HY"
        ]
    },
    {
        "id": "I6JweumCOx",
        "original": null,
        "number": 3,
        "cdate": 1666895876028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666895876028,
        "tmdate": 1666895876028,
        "tddate": null,
        "forum": "gL68u5UuWa",
        "replyto": "gL68u5UuWa",
        "invitation": "ICLR.cc/2023/Conference/Paper5327/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces (adapts) ML training of EBMs for SBI problems.\nThe main problem in SBI is that one cares about sampling from posterior $q(\\theta|x)$, and in the case of using EBMs, sampling from posterior results in doubly intractable inference since the partition function depends on $\\theta$ as well. \nThe paper suggest parameterizing the joint model $q_\\psi(\\theta, x)$ as $\\pi(\\theta)\\exp(-E\\psi(\\theta, x))/Z_\\psi$ and show that the optimal posterior also have similar form:  $q(\\theta | x) = p(\\theta)\\exp(-E\\psi(\\theta, x))/Z_\\psi$. This is important since they now can directly sample from the posterior without worrying about the partition function. The main limitation of this approach is that they need to have an analytical form p(\\theta) as prior, which is not always available, especially for the sequential setup, in which the prior is defined using posteriors on the previous rounds. Therefore, the sequential version of their approach (SUNLE) just trains an EBM using contrastive divergence and uses doubly-intractable MCMC for inference.\n\nThe authors compare AUNLE and SUNLE with NLE and SNLE, which use normalizing flow, on four toy datasets and one real-world example.\n",
            "strength_and_weaknesses": "AUNLE is well-motivated, the approach is interesting and novel, and its correctness has been backed up by Proposition 1.\nHowever, SUNLE is simply training EBMs with CD, I am not seeing any novelty there.\n\nI don't understand why the authors didn't use SMNLE as a baseline. They ruled out SMNLE for being double-intractable, but they are already using double-intractable MCMC for their real-world setup. The comparison in Figure 1 is not enough to rule out SMNLE from the baselines. Moreover, SMNLE shows a better likelihood ratio than NLE in Figure 1.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written easy to follow and partially novel (AUNLE)",
            "summary_of_the_review": "The paper is well-motivated and discusses an important application. However, the main novelty of the paper is the introduction to AUNLE, however, the amortized version is not very useful in practice since using sequential modeling reduces the number of required samples. \nI am not seeing any novelty in the sequential version of their work except only training the EBM using CD in their setup.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_jDWH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_jDWH"
        ]
    },
    {
        "id": "Gozo8CIT01e",
        "original": null,
        "number": 4,
        "cdate": 1666912624513,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666912624513,
        "tmdate": 1666913790668,
        "tddate": null,
        "forum": "gL68u5UuWa",
        "replyto": "gL68u5UuWa",
        "invitation": "ICLR.cc/2023/Conference/Paper5327/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper try to solve the Simulation-Based Inference (SBI) task using the energy-based model (EBM). In SBI task, one assume a known simulator that can generate samples x from interesting distribution parameterized by $\\theta$. The task is then estimate $p(\\theta | x)$. The authors using EBM to parameterize $q_\\phi(x|\\theta) $ or $q_\\phi(x, \\theta)$ and then inference can be done using bayes rule. In the paper, the authors propose two algorithms. AUNLE use a parameterization with tilting trick to make the normalizing constant of EBM independent of $\\theta$ and thus reduce the computational cost in the previous SMNLE method. And AUNLE replaces the normalizing flow model used in previous method with EBM. Experimental results seem to support that the proposed models outperform its baselines.",
            "strength_and_weaknesses": "I mainly work on EBM instead of SBI. So my opinion my focus more on EBM part.\n\nFor the shining point of this paper, I think I like idea of this paper of using EBM to model the joint or conditional distribution of the paper. On one hand, EBM can be more expressive than normalizing flow given similar number of parameters and on the other hand, it makes sense to me that by cleverly design the EBM term, one can avoid the approximate doubly intractable estimation needed in previous method.\n\nHowever, I also have some questions.\n1. As pointed out by [2] and [4], EBM trained with MCMC as a sampler and MLE as objective may have the \"short-run\" effect. That is, instead of learning a correct energy function, the model converges to a point that the Langevin sampler becomes a noise injection generator. Since in the task of SBI, one actually needs a correct energy term to infer the likelihood. I'm wondering whether this problem exists in the current training process, especially for those complex tasks.  \n\n2. The authors mentioned that their model can be more effective comparing with previous methods. Thus, I think it is necessary to add a complexity analysis part for both AUNLE and SMNLE, e.g. list different models' (both the new models and baselines) accuracies together with their training/testing/inference time and make a comparison.\n\n3. I'm not very sure whether the SBI experiments results listed here are strong enough to make conlusion that the new proposed method outperforms the previous ones. Would like to listen to other reviewer's opinions.\n\n4. Some important EBM literatures are missing. Recommend the authors to add the following [1] [2] [4] papers.\n\n[1]  A Theory of Generative ConvNet. ICML 2016\n\n[2] Learning non-convergent nonpersistent short-run mcmc toward energy-based model.  NeurIPS, 2019\n\n[3] Cooperative Training of Descriptor and Generator Networks. PAMI, 2020\n\n[4] A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model. ICLR 2022",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is in general well written and easy to follow but since in the paper there are many algorithms with abbreviation. I do recommend the authors to try to make these more clear.",
            "summary_of_the_review": "In general, I like the idea of this paper of using EBM to solve the Simulation Based Inference (SBI) problem. And I think the theory part is OK for me.  But since I'm not an expert in SBI, I would like to listen other reviewers' opinion on how solid the experiments are.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_mHnK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_mHnK"
        ]
    },
    {
        "id": "WqGBcOiDzxO",
        "original": null,
        "number": 5,
        "cdate": 1667595122867,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667595122867,
        "tmdate": 1667597158334,
        "tddate": null,
        "forum": "gL68u5UuWa",
        "replyto": "gL68u5UuWa",
        "invitation": "ICLR.cc/2023/Conference/Paper5327/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose two new synthetic likelihood methods for simulation based inference using energy based models, UNLE and AUNLE.\n\nThey introduces a tilting trick and an amortized sequential model to perform posterior inference with intractable likelihoods and improve both modeling quality and inference budget.",
            "strength_and_weaknesses": "Stregths:\n- the authors propose an interesting \"tilted\" inference procedure which performs well and is a neat trick\n- the utilization of conditional EBMs in SBI makes sense and is well executed\n- the authors propose a useful sequential version of their system\n\nWeaknesses:\n- like in many papers on this topic, the experiments are quite toy and not as ambitious as they could be, but they are clean and well-executed.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written, has clarity on the details of the techniques and makes sense.\nIt also nicely ties into prior work on SBI and positions the paper fairly.\nThe main contribution is also fairly interesting as it reduces the computational budget for SBI, which is a core goal in the field.",
            "summary_of_the_review": "The authors propose a rigorous and interetsting spin on SBI using EBMs.\n\nThe proposed extensions make sense and should be useful in the field, and the experiments appear well justified -if comparatively simple.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_VwrC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5327/Reviewer_VwrC"
        ]
    }
]