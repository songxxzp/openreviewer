[
    {
        "id": "gIhVwxcocC",
        "original": null,
        "number": 1,
        "cdate": 1666635738679,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635738679,
        "tmdate": 1669650196623,
        "tddate": null,
        "forum": "V_V53-WG9m6",
        "replyto": "V_V53-WG9m6",
        "invitation": "ICLR.cc/2023/Conference/Paper1539/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a cross-model framework to learn molecular embeddings jointly from the chemical structure and images of drug-treated cells. Using four contrastive losses to pre-train a model on a high-throughput imaging dataset, the paper demonstrated improved performance on three downstream tasks, including retrieving chemical structures or images from the other channel, predicting clinical outcomes, and predicting molecular properties. \n",
            "strength_and_weaknesses": "Strength:\n- The work tackles the important problem of learning molecular representations by bringing high-throughput imaging data.\n- Performance improvement of the pre-training strategy is demonstrated on three different downstream tasks compared with SOTA.\n- An interesting case study is reported to find molecules that have similar effects as single gene overexpression.\n\nWeakness\n- While I am no expert on cross-modal learning, it seems the framework just embraces several state-of-the-art ideas in contrastive learning without novel technical contributions.\n- No ablation studies for combinations of contrastive losses\n- Given the application nature of this work, related work should discuss the state of the arts in learning embedding from cell images. Particularly, the published workSanchez-Fernandez, Ana, et al. \"Contrastive learning of image-and structure-based representations in drug discovery.\" ICLR2022 Machine Learning for Drug Discovery. 2022.\nseems conceptually similar to the proposed work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Regarding clarity:\nThe paper is well written in general and can be improved by filling in some missing details:\n- Parameters to generate masked context graph modeling in eq 4 and 5. It would also be helpful to clarify the cross-entropy used in eq 5.\n- Eq 7: There should be k=1 instead of k in the summation?\n- Details about the VAE architecture and training in Generative Graph-image matching are not provided.\n- It would be helpful to discuss what features other than molecular embeddings are used in Clinical Outcome Prediction and how it fits into the pre-trained GNN.\n - It would be helpful, if possible, to provide details of cDNA and molecules used in the case study of zero-shot graph retrieval. Would be good to see Hit@10 of other SOTA as well. \n\nRegarding quality and originality:\nThe work is a nice application of representation learning in biology and drug discovery, albeit It seems the paper did not provide a new technical contribution. The idea of contrastive learning from chemical structures and cell images seems not new.  Performance improvement is properly demonstrated by evaluation experiments and case studies. \n",
            "summary_of_the_review": "I would not recommend this paper for publication in ICLR due to my concern about the limited novelty and tech contribution. It is a nice piece of work that I enjoyed reading through. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1539/Reviewer_PcWA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1539/Reviewer_PcWA"
        ]
    },
    {
        "id": "bEevtVC6Wo",
        "original": null,
        "number": 2,
        "cdate": 1666677557803,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677557803,
        "tmdate": 1666678921287,
        "tddate": null,
        "forum": "V_V53-WG9m6",
        "replyto": "V_V53-WG9m6",
        "invitation": "ICLR.cc/2023/Conference/Paper1539/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose an approach to co-learn representations from molecular graphs and fluorescence microscopy images.",
            "strength_and_weaknesses": "Strengths:\n- A relevant problem is approached. The retrieval-task between cellular images and molecular graphs is appealing and significant. \n- The paper is clearly written and structured; Figures are informative; Notation follows ML standards\n- The authors took care of splitting the data appropriately since multiple  images correspond to one molecular graph\n\nWeaknesses:\n- The approach is not novel, but has been suggested before. \n- The presented results are not relevant due to missing embedding into related work, missing informative baselines and incorrect scope. \n- The presented work has several severe technical errors. There is also a fundamental problem in the approach (see below).",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe paper is clearly written and structured.\n\n### Novelty \nThe almost exact same approach has been presented in ref [1]. Even the identical dataset (cell painting) and contrastive learning objectives (InfoNCE, CLIP-like) have been used. The authors should propose a new method, approach and concept and state their novel contributions, e.g., at the end of the Introduction section.\n\n### Quality\ni) There is a fundamental problem with the approach and this is the focus on pre-training the graph neural network, i.e. molecule encoder, using cellular images. Many changes in molecular structure, do not lead to changes in cell morphology, such that the graph network is not required to code these changes in the learned representation. However, for downstream tasks exactly these structural changes might be relevant. Comparing to CLIP: CLIP focuses on pre-training the image-encoder (equivalent here: cellular image encoder) and not on the text-encoder (equivalent here: graph encoder) because for the text-encoder other pre-training tasks, such as masking/de-noising, are more appropriate. The authors should justify their focus on the graph neural network encoder. \n\nii) It is unclear why multiple types of losses are necessary to train this architecture. For CLIP-like methods, the loss in Eq(3) is usually sufficient. Since the loss terms (Eq (9)) are not balanced against each other (except for Eq(8)), it is very likely that one of the loss terms dominates over the others, such that the training could completely neglect the image-modality. The ablation study (Section 4.3) does not investigate this at all. However, the bottom of Table 1 provides some insights: the strongest increase of performance arises from the masking loss, indicating that the cellular images are hardly used. Therefore, a pre-training strategy purely using the MGM loss (Eq 4,5) might be superior to cross-modal pretraining. Due to the missing error bars, it is unclear whether there is a difference between MIGA and \u201cGIC+MGM\u201d at all. The authors should justify their design choices and perform an ablation study to demonstrate that all four loss terms are necessary and need not be balanced against each other.\n\niii) Missing embedding into related work. The author's are unaware of closely related work ([1]) and also unaware that their suggested graph-image contrastive loss (Eq (3)) is identical to the CLIP algorithm. The authors focus on pre-training the graph neural network to encode molecules, but are unaware of related work on molecule encoders that perform much better on the MoleculeNet tasks (Table 3). \n\niv) Informative baselines are missing. I quickly compared against a simplistic baseline, a fixed encoder (ECFP [7]) and linear probing and reached 77.05 on Tox21 and 69.43\u00b10.0 on ToxCast. This baseline outperforms MIGA indicating that the graph neural network has not even learned to encode as well as an encoder that just checks for all substructures [7]. Furthermore, the best methods in the Tox21 data challenge reached an AUC of 0.845, with mostly descriptor-based approaches at that time, which means that the pre-training task for the GNNs is lacking. Comparing how much computational effort (energy and carbondioxide costs) it takes to pre-train the MIGA molecule encoder that in the end does not even perform better than an encoder that costs almost nothing (ECFP), makes it really difficult to justify the use of MIGA. Or in other words: the suggested approach to properly pre-train graph neural networks would mean that first a multi-million dollar experiment to collect the microscopy images has to be done. The authors should include informative and state-of-the-art baselines into their method comparisons. In table 2, it is unclear on which representation LR, RF and XGboost are running. The authors should use standard molecular fingerprints and descriptors as baselines in all studies. \n\nv) Presented performance metrics in many of the tables (e.g. Table 3 and 4) are results of single runs and therefore any performance differences can arise just by chance. For the ones with standard deviations it is unclear what they represent: re-runs on the same split or cross-validation? The authors should perform re-runs, or cross-validation, to obtain error bars and confidence intervals on all performance metrics.\n\n### Reproducibility \nThe dataset is publicly available, but needs strong pre-processing to be used for ML. The authors provide code as supplementary material, which makes the work likely to reproduce. One difficulty could be the dependency on the preprocessing of the Cellpainting dataset\n\nReferences:\n[1] Sanchez-Fernandez, A., Rumetshofer, E., Hochreiter, S., & Klambauer, G. (2022, March). Contrastive learning of image-and structure-based representations in drug discovery. In ICLR2022 Machine Learning for Drug Discovery.  \n[2] Yang, K., Swanson, K., Jin, W., Coley, C., Eiden, P., Gao, H., ... & Barzilay, R. (2019). Analyzing learned molecular representations for property prediction. Journal of chemical information and modeling, 59(8), 3370-3388.  \n[3] Ma, J., Sheridan, R. P., Liaw, A., Dahl, G. E., & Svetnik, V. (2015). Deep neural nets as a method for quantitative structure\u2013activity relationships. Journal of chemical information and modeling, 55(2), 263-274.  \n[4] Jiang, D., Wu, Z., Hsieh, C. Y., Chen, G., Liao, B., Wang, Z., ... & Hou, T. (2021). Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models. Journal of cheminformatics, 13(1), 1-23.  \n[5] Wang, H., Kaddour, J., Liu, S., Tang, J., Kusner, M., Lasenby, J., & Liu, Q. (2022). Evaluating Self-Supervised Learning for Molecular Graph Embeddings. arXiv preprint arXiv:2206.08005.\n[6] You, Y., Chen, T., Shen, Y., & Wang, Z. (2021, July). Graph contrastive learning automated. In International Conference on Machine Learning (pp. 12121-12132). PMLR.  \n[7] Rogers, D., & Hahn, M. (2010). Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5), 742-754.  \n",
            "summary_of_the_review": "The main idea of the work is not novel, there is a fundamental problem with the approach, i.e. that microscopy images cannot capture all changes in molecular structure, and the experiments are insufficient due to lacking baselines. The choice of the loss function with many terms is particularly problematic for learning, but this is not ablated. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1539/Reviewer_4Jez"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1539/Reviewer_4Jez"
        ]
    },
    {
        "id": "h-CQRXJIxBz",
        "original": null,
        "number": 3,
        "cdate": 1666971853311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666971853311,
        "tmdate": 1666971853311,
        "tddate": null,
        "forum": "V_V53-WG9m6",
        "replyto": "V_V53-WG9m6",
        "invitation": "ICLR.cc/2023/Conference/Paper1539/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work utilizes the self-supervised learning paradigm to improve the data representations in Graph Neural Networks. Different from previous literature, the proposed method employs cross-modal correspondences between the Graphs of molecular structures and the associated (paired) cell microscopy images. The intuition behind combining these specific modalities is clear, drug molecules have the ability to cause perturbations (subtle morphological changes) to cells, e.g. their shapes, numbers, structures, and so on. To that end, the paper proposes a pretraining method called MIGA that processes these paired modalities, and aligns them in the feature space. MIGA is trained with three combined loss terms, which are inspired from existing literature on contrastive learning and generative modeling. Then, the learned data representations, stored in the form of neural network weights in the Graph and Image encoders, are evaluated on several downstream tasks. The chosen downstream tasks seem relevant to the domain, and are sensible overall. The reported results on these downstream tasks confirm the improvements obtained by pretraining by their method, MIGA, as opposed to the baselines from literature.",
            "strength_and_weaknesses": "1- The proposed method, MIGA, utilizes the cross-modal information that exist naturally in microscopy images to improve graph data representations. While the underlying algorithms and losses to perform this cross-modal alignment are not novel, the idea of employing these heterogeneous modalities is inspired and novel. \n2- The paper is easy to understand, and the used language is clear. \n3- The experimental results are comprehensive, and confirm the merits of the proposed method. Particularly remarkable are the improvements obtained on clinical outcome prediction tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of high quality and is clearly written.",
            "summary_of_the_review": "Overall, I recommend accepting the paper. The method is novel, the writing is clear, and the experimental results are sound.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1539/Reviewer_pEPS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1539/Reviewer_pEPS"
        ]
    },
    {
        "id": "ENfzC36LDfK",
        "original": null,
        "number": 4,
        "cdate": 1667249963296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667249963296,
        "tmdate": 1667249963296,
        "tddate": null,
        "forum": "V_V53-WG9m6",
        "replyto": "V_V53-WG9m6",
        "invitation": "ICLR.cc/2023/Conference/Paper1539/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present MIGA, a method for combining graphs of molecules and corresponding high-content cell images into a related embedding space. This learned embedding space can then be used for several tasks of clinical relevance, namely cell image retrieval, predicting clinical outcomes of drugs, and molecular property prediction. The authors evaluate MIGA against several reasonable baselines in all areas.",
            "strength_and_weaknesses": "Strengths:\n- The method is well-reasoned and is intriguing in this context, in what is a very challenging problem but of great relevance for the biomedical community.\n- MIGA is applicable to several important real-world applications, namely clinical trial outcome and molecular property prediction.\n- The authors compare to an impressive number of baselines, showing a consistent advantage of their method.\n- Related, the authors have thoroughly considered proper metrics and ablation studies to evaluate MIGA.\n\nWeaknesses:\n- I am concerned that there may be other works in this area that are not considered. There are entire companies, such as Recursion, that work in this area of deep learning-based high-content image screens for drug design. I do not have a sense of how significant of an advance is their method in comparison to such work.\n- The performance increase is modest, which is expected given the difficulty of the problem. I believe it is worthwhile to publish still, given the consistency of the improvement for such an important problem, but it is still a weakness that it does not yield a more significant improvement.",
            "clarity,_quality,_novelty_and_reproducibility": "Clear:\n- Overall, the paper is well-written and easy to follow. The logic of the introduction, methods, and results flow well.\n- The figures give helpful explanation of the context of the problem and the method and its applications.\n\nUnclear:\n- The cell images are in general very small throughout the paper. It is hard to perceive the details. For example, in the examples for image retrieval in Fig. 3, the individual cells are very small. The images of molecules are also small in these figures, though this is not as significant of an issue as the cell images.\n-  A related issue is the contrast of the images being very low. Even after zooming in, I admit I cannot perceive much difference between the images of BRCA1 overexpression and TP53 overexpression. \n\n- The method seems reproducible.\n- The method is significantly novel, to my knowledge.",
            "summary_of_the_review": "Overall, the paper is well-done and the approach is an intriguing way to combine cell images and molecular structure. The experiments are very well-done and thought out. My hesitations about giving a more confident recommendation to accept are that the improvements, while consistent, are modest, and I am concerned that there is much other work out there that I am not aware of, given how many companies and researchers are working in this area.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1539/Reviewer_mr25"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1539/Reviewer_mr25"
        ]
    }
]