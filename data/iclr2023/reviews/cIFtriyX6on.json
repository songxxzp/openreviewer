[
    {
        "id": "JzGC2iEo1mP",
        "original": null,
        "number": 1,
        "cdate": 1666610649144,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666610649144,
        "tmdate": 1666679340541,
        "tddate": null,
        "forum": "cIFtriyX6on",
        "replyto": "cIFtriyX6on",
        "invitation": "ICLR.cc/2023/Conference/Paper2198/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a method to accelerate both training and inference in Graph Neural Networks by quantizing both the\nweights and all the intermediate results (features, errors, gradients) to 8 bits. The proposed training and inference\npipeline can be executed on integer-only hardware.\n\nTo deal with the high dynamic range arising in GNNs due to various node degrees and feature types, this work proposes\nto use a dynamic quantization technique to map 32-bit intermediate results from multiply-and-accumulate steps back to\n8-bits shifting right with a dynamically-determined amount and clipping to 8 bits. The shift count is determined such\nthat the number of outliers resulting in overflow or underflow is minimized.\n\nSome experiments are carried out on 8 GNN benchmarks, matching the accuracy of floating point counterparts. No\ncomparisons against state-of-the-art GNN quantization methods are provided. ",
            "strength_and_weaknesses": "* Strengths:\n\n  * Due to the scalability issues and online-training demands for, integer-only training and inference is likely to have\n  high practical impact.\n\n* Weaknesses:\n\n  * Technical contribution is incremental (see notes below on novelty).\n  * Clarity could be improved a lot (see remarks below on clarity).\n  * No comparison is provided against any competing method. The only baseline is the floating point pipeline.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity:\n\n  * I found the paper hard to follow, as explanations seem convoluted and structure needs improvements. Some examples:\n  \n    * On page 2, \" a shaping model can be obtained directly\". What do the authors understand by a \"shaping\" model?\n    * On page 2, what do the authors understand by \"quantification methodology\"?\n    * The authors refer in several places to \"dequantization\" from FP to INT (e.g. on page 3, \n    \"the output [...] must be dequantized from the FP32 to INT16 format\"). Shouldn't it be quantization that is needed\n    here?\n    * On page 4, \"$x_{fn}$ represents the signed <i>bitn</i> to which $x$ is quantized\". I am confused as to what a\n    \"bitn\" is.\n    * I guess $x_{in}$ and $x_{bn}$ are both the integer representation of $x$, just that the first is viewed as an\n    integer in base 10, and the other is viewed in binary. It is confusing to use different mathematical notations\n    for the exact same mathematical object (the same integer).\n    * In Figure 3, they authors plot a histogram of a tensor multiplication output, but they do not state how was this\n    histogram obtained. Were the tensors sampled from a certain distribution? Or is this obtained from a training/test\n    sample?\n    * The function $f$ appearing in equation (2) is never introducted (the only place it appears is on the vertical axis\n    of figure 3).\n    * The authors mention that \"We utilize Kullback-Leibler divergence (Joyce, 2011) to prove that when g(k) in \n    Equation 2 gets the maximum, we may derive a 7-bit(k, k + 6) expression for this data set with reduced Kullback-\n    Leibler divergence and more Shannon Entropy (Bromiley et al., 2004)\", but no proof is provided, nor is it clear how\n    KL divergence/Shannon entropy is related to maximizing g(k).\n    * In Algorithm 1:\n      * line 4, who is $j$? This variable is never iterated over nor assigned anywhere.\n      * line 5, AdderTree has a height argument, but what output is it invoked over?\n      * LoadAtoeachThreadGroup and AdderTree are never defined.\n      * The vector $C$ is never initialized. \n\n* Novelty:\n\n  * From a technical point of view, the framework is very similar to the NITI method, see\n  <i>M. Wang, S. Rasoulinezhad, P. Leong and H. So. NITI: Training Integer Neural Networks Using\n  Integer-Only Arithmetic,  IEEE Transactions on Parallel and Distributed Systems, 2020, DOI:10.1109/TPDS.2022.3149787\n  </i>. While not applied to GNNs, their training pipeline works on full 8-bit integer arithmetic, and their dynamic\n  shift and shift-and-round criterion is almost identical, with the only difference that they select the shift\n  count to minimize overflow alone, while here the authors also take into account underflow. From a technical\n  standpoint, this work is just an incremental step over their method.\n  * There is no empirical novelty, as this work simply compares the integer-only training pipeline against the FP32\n  baseline on standard GNN datasets.\n\n* Quality:\n\n  * Relevant baselines are not compared against (see baselines mentioned in section 2 and the NITI reference above)\n  and no analysis of results is provided.\n  * No theoretical justification for the criterion is provided (minimizing KL divergence is mentioned, but no proof is \n  given).\n  * Overall, the scientific quality of this work, in its present form, is below acceptance standards for ICLR.\n\n* Reproducibility:\n\n  * The authors provide some information on their experimental procedure in the caption of Table 3.\n  * Some code snippets are provided in the manuscript, but no intention to release publicly available code is\n  mentioned. They do mention \"implementing in existing framework\" as a future intention, but it is not clear what\n  framework they are referring to or whether this means the code will be public.",
            "summary_of_the_review": "This seems to be unfinished work with incremental technical novelty. The manuscript needs a major rewrite to\nmeet the presentation quality standards for publication (see notes on clarity above). At lease one relevant baseline is\nmissing (see e.g. NITI above), and no comparison against any competing work is provided. Overall,\nin its current form, this work does not meet acceptance standards for ICLR.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2198/Reviewer_Po7t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2198/Reviewer_Po7t"
        ]
    },
    {
        "id": "wgFQf86aSjz",
        "original": null,
        "number": 2,
        "cdate": 1667526278867,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667526278867,
        "tmdate": 1667526278867,
        "tddate": null,
        "forum": "cIFtriyX6on",
        "replyto": "cIFtriyX6on",
        "invitation": "ICLR.cc/2023/Conference/Paper2198/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes four factors that restrict the applicability of traditional quantization methods in GNNs. Based on that, a specialized and efficient quantization framework is proposed to improve forward, backward, optimizer, and loss functions with the dynamic quantization (DyQ), static quantization (StQ), and weight quantization (Qw ) operators. Experiments show the reduced memory and time complexity with comparable accuracy.\n",
            "strength_and_weaknesses": "**Strength** \n- The quantization of GNNs is an important problem, and the paper provides a comprehensive comparison of different quantization methods.\n- Different quantization operators are proposed to handle real and pseudo overflows, and are plugged into the GNN workflow.\n- The empirical results show improvement in efficiency.\n\n**Weaknesses**\n- Though I believe this paper has some nontrivial technical contributions, the writing of this paper needs to be greatly improved to make it accessible for the community. I have tried hard and spent a significant amount of time, but can still get a rough idea without understanding the details of why pseudo flow is a key bottleneck and how it is handled by DyQ. For example, Section 3.1 is hard to follow for a general reader like me: what exactly the pseudo overflow problem is (e.g. when and why $c$ cannot be represented by $2^n-1$), what is the correlation and difference between real and pseudo overflow. Figure 2 is almost impossible to understand for general readers too.\n- It is not clearly justified why dequantization is not needed in this method.\n- The claim that this method can accelerate inference is not supported by experiments.\n- This method is not compared with existing works, for example, the memory reduction compared with EXACT.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper should be improved for general readers.\n\nThough the quality of writing is not satisfactory, I feel this paper contributes some technical novelty.\n\nThe code snippets are provided for reproducibility.\n",
            "summary_of_the_review": "This paper identifies some nontrivial challenges in quantization of GNNs. However the writing of this paper is hard to follow for a larger audience. I believe it is a work with good potential if it can be better presented. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2198/Reviewer_5yWh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2198/Reviewer_5yWh"
        ]
    },
    {
        "id": "F19p0rDH7I",
        "original": null,
        "number": 3,
        "cdate": 1668046367873,
        "mdate": null,
        "ddate": null,
        "tcdate": 1668046367873,
        "tmdate": 1668046367873,
        "tddate": null,
        "forum": "cIFtriyX6on",
        "replyto": "cIFtriyX6on",
        "invitation": "ICLR.cc/2023/Conference/Paper2198/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The work proposes quantization methods for low bit training of GNNs. ",
            "strength_and_weaknesses": "Pro: The paper claims to improve training speed by using dynamic quantization. \n\nCon: The solution is complex, which limits its applicability. And technical novelty is rather limited. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is reasonably clear, but some claims which was to justify the proposed method were not demonstrated. \n",
            "summary_of_the_review": "The paper is not ready for publication. Additional works are needed to justify the claims and to simplify the algorithms. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2198/Reviewer_o9Df"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2198/Reviewer_o9Df"
        ]
    }
]