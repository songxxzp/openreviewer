[
    {
        "id": "8v-7uw1R9n",
        "original": null,
        "number": 1,
        "cdate": 1665694789632,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665694789632,
        "tmdate": 1668627498808,
        "tddate": null,
        "forum": "XFSCKELP3bp",
        "replyto": "XFSCKELP3bp",
        "invitation": "ICLR.cc/2023/Conference/Paper2180/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "1. This paper interpreted self-supervised contrastive learning (SSCL) as a type of stochastic neighbor embedding (SNE) methods that preserve the pairwise similarities specified by the data augmentations. \n2. Based on the connection with SNE, this paper provided theoretical insights for domain-agnostic data augmentation, implicit bias, and OOD generalization of SSCL. In particular, for the implicit bias of SSCL, this paper proved the equivalence between minimizing the expected Lipschitz constant of the SSCL feature map and SNE with uniformity constraint. \n3. Motivated by the SNE perspective, this paper proposed modifications to the SSCL methods, including re-weighting positive pairs and t-SNE style matching, and achieved improvements in experiments. ",
            "strength_and_weaknesses": "Strengths:\nI think viewing SSCL as a type of SNE is an exciting and potentially fruitful perspective. Many theoretical analyses and practical techniques can be potentially applied to SSCL to improve its performance. As already demonstrated in this paper, the re-weighting of positive pairs and t-SNE style matching inspired by SNE literature can improve the performance of SSCL.\n\nWeaknesses:\nThe equivalence between SSCL and SNE is not hard to see after a re-writing of the objective function; this connection is interesting but not very surprising. The bulk of this paper focuses on the theoretical analysis of SSCL from the SNE perspective. However, all the theoretical analysis is restricted to toy examples and some arguments are handwavy and lack depth.\n1. The analysis is restricted to a toy example: the gaussian mixture model. The positive samples are generated by sampling from the same component. This is impossible to conduct in SSCL since we cannot access the label information in SSCL and therefore cannot decide which component a sample belongs to. \n2. In the analysis domain-agnostic data augmentation, why is $P(x_1 \\text{ and } x_2 \\text{ form a positive pair })$ equal to the density $\\phi(x_1-x_2)$? For example, suppose the original dataset only contains one sample, all the augmented data are positive pairs, in which case $P(x_1 \\text{ and } x_2 \\text{ form a positive pair })$ always equals $1$ but the density may not be $1$. \n3. Regarding the implicit bias, this paper hypothesized that SSCL minimizes the expected Lipschitz constant $C(f)$. Then it proves that minimizing $C(f)$ in SSCL is equivalent to solving an SNE problem with uniformity constraint. But the SNE problem is not standard ($P$ needs to be transformed in a strange way and $Q$ is also unorthodox). This paper did not prove or give sufficient empirical evidence to show SimCLR is minimizing $C(f)$.\n4. The arguments for the effects of normalization on OOD generalization are very handwavy and the authors did not provide any rigorous theorem for this part. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nIn general, this paper is written well. Some of the theoretical claims need to be more precise. For example, in Proposition 3.2, what's the data set and how are the positive samples defined? \n\nQuality:\nI think the SNE perspective of SSCL is interesting and the empirical results look promising. However, I think the theoretical analysis in this paper is too restricted and lacks depth.\n\nNovelty:\nAs far as I know, this is the first paper that connects SSCL and SNE.\n\nReproducibility:\nI think the theoretical results and experiments in this paper are reproducible. ",
            "summary_of_the_review": "I think building the connection between SSCL and SNE is very important and many techniques in SNE can be applied to SSCL to improve its performance (as already demonstrated in this paper). \n\nMy major concern with this paper is the theoretical part. I found the theoretical analysis in this paper restricted to exceedingly simple toy examples and some arguments are handwavy and lack depth. The connection between the implicit bias of SSCL (minimizing $C(f)$) and SNE with uniformity constraint is pretty interesting. Unfortunately, this paper fails to either prove or empirically demonstrate that SimCLR is minimizing $C(f)$. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2180/Reviewer_qnUF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2180/Reviewer_qnUF"
        ]
    },
    {
        "id": "FGwSiujzc2k",
        "original": null,
        "number": 2,
        "cdate": 1666688324085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688324085,
        "tmdate": 1666688324085,
        "tddate": null,
        "forum": "XFSCKELP3bp",
        "replyto": "XFSCKELP3bp",
        "invitation": "ICLR.cc/2023/Conference/Paper2180/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A novel perspective of self-supervised contrastive learning (SSCL) is investigated. To be specific, this paper interprets SSCL as stochastic neighbor embedding (SNE), and studies implicit bias and out-of-distribution generalization under this framework. The case study about OOD is interesting to me, and also the extension of SSCL based on SNE is impressive. ",
            "strength_and_weaknesses": "Strength:\n\n(1) A novel perspective to interpret SSCL as SNE.\n\n(2) The case study based on the Gaussian mixture model for implicit bias and OOD is impressive.\n\n(3) The two variants of SSCL based on SNE are reasonable, which is interesting for exploring OOD generation of SSCL.\n\nWeaknesses:\n\n(1) It is claimed that \"Can SNE be revived in the modern era by incorporating SSCL?\". It is not discussed in the main manuscripts. \n\n(2) Eq.3.1 is not a good choice. Actually, the conditional probability P_{j|i} is not normalized. It would be a better choice to add the coefficient term 1/2n  as the normalization term for summarization over j.    \n\n(3) How are the negative samples constructed in the case study in Sect.3, such as Gaussian mixture setting? \n\n(4) This whole paper claimed to connect SSCL to SNE, but conducted experiments on MoCo-v2, which is not typically an SSCL method as it does not require negative samples. The connection between SNE and MoCo should be discussed as well. \n\nSome minor typos:\n\n(1) \"two random sampled data as negative pairs\", there are more than two negative pairs. \n\n(2) \"is the is a temperature parameter.\"\n\n(3) In Eq.3.1, it should be $\\tilde{x}_i$ and $\\tilde{x}_j$\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. It is novel to interpret SSCL as SNE, and especially the authors proposed two effective variants of SSCL based on this novel perspective.  ",
            "summary_of_the_review": "Overall, this paper is interesting and impressive. If the weaknesses paints can be well addressed, it would be better. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2180/Reviewer_RZus"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2180/Reviewer_RZus"
        ]
    },
    {
        "id": "4aBNpeQj_Mm",
        "original": null,
        "number": 3,
        "cdate": 1666916212830,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666916212830,
        "tmdate": 1666916212830,
        "tddate": null,
        "forum": "XFSCKELP3bp",
        "replyto": "XFSCKELP3bp",
        "invitation": "ICLR.cc/2023/Conference/Paper2180/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper shows a connection between self-supervised contrastive learning (SSCL) and stochastic neighborhood embedding (SNE), a data visualization method based on preserving distances. More formally, the authors show that SSCL is a form of SNE with pairwise distance/similarity defined by the data augmentation. Leveraging this connection and working with a simple mixture of gaussians setting, the authors provide theoretical insights into\n- performance of domain-agnostic augmentations such as mixup\n- uniformity and alignment of the learned features\n- *expected Lipschitz constant* as an implicit bias of SSCL\n- out of distribution robustness with normalized versus unnormalized features\n\nThese theoretical results are accompanied with new practical ideas that give significant improvements over standard SSCL. These ideas are:\n- using a weighted loss with the weighting dependent on the augmentation pair\n- $t$-SSCL in the spirit of $t$-SNE by replacing the gaussian distribution to a heavy tail $t$-distribution\n\nLastly, the paper show the benefits of these modifications in large scale experiments in terms of domain transfer and o.o.d. generalization.",
            "strength_and_weaknesses": "**Strengths**:\n- The connection to SNE, although intuitive, is new and provides useful insights, which the paper explores through different angles.\n- Performance gains from the suggested algorithmic improvements are pretty impressive!\n\n**Weaknesses**:\n- Theoretical results in the paper are not clearly presented, and seem more like intuitions than formal statements. For example, in section 3.1.1, the distance proposition is limited to only the simplistic gaussian noise injection setup, and the difference with mixup is not formalized. Same holds for section 3.1.2. Section 3.1.3 claims that \u201cSSCL exhibit neighbor-preserving property and we identify it as an implicit bias\u201d, but the corollary assumes that if $C(f)$ is minimized then show distances are preserved. It is not clear to me why $C(f)$ would be minimized by SSCL.\n- Comparison to prior work on relating SSCL to spectral clustering and inductive bias of function classes is not discussed (see HaoChen et al. 2021, HaoChen et al. 2022, Saunshi et al. 2022).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably clear, and the ideas seem novel. The statements are reproducible from the proofs, however, there are few hand-wavy statements without proper proofs.\n\nSuggestions for improvement:\n- More formal technical writing, with exact statements. Clarifying when the statement is about a toy setting vs in general\n- Empirical evidence to show that $C(f)$  is minimized\n- Adding discussion on recent relevant work (mentioned above)\n- In Fig B.4, are the labels incorrect? It seems that SimCLR is doing better than $t$-SimCLR. Fix this?",
            "summary_of_the_review": "Despite the issues with the technical content in the paper, the practical advantages of the proposed viewpoints are pretty strong. Therefore, I lean towards acceptance. I would encourage the authors to clean up the theoretical sections to improve the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2180/Reviewer_DMAs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2180/Reviewer_DMAs"
        ]
    },
    {
        "id": "iDa55W_uFW",
        "original": null,
        "number": 4,
        "cdate": 1666976049412,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666976049412,
        "tmdate": 1666976049412,
        "tddate": null,
        "forum": "XFSCKELP3bp",
        "replyto": "XFSCKELP3bp",
        "invitation": "ICLR.cc/2023/Conference/Paper2180/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies contrastive self-supervised learning from the view of SNE. The authors show that InfoNCE loss can be seen as a special case of SNE, by setting the data similarity matrix P according to the data augmentation and embedding matrix Q as the softmax of pairwise similarity. Under the SNE framework, new insights for domain-agnostic data augmentation, implicit bias and OOD generalization are presented. Moreover, from SNE perspective, this paper proposes to improve SimCLR by 1) introducing weighted matrix P to better utilize the augmentation detail and 2) t-SNE style InfoNCE loss, i.e, l2 instead of cosine to improve OOD generalization and exponential to polynomial to improve feature dimension efficiency. Various experiments show that improvement can be achieved in both in-distribution and out-of-distribution generalization.\n",
            "strength_and_weaknesses": "Strength: \nThe novelty is strong. The connection between SimCLR and SNE is rarely discussed and the SNE perspective of SSCL is novel to me. The connection is presented in an inspiring way where the authors provided several new insights and demonstrated several SNE inspired modifications to SimCLR. While mainstream self-supervised learning focuses on methodology and benchmark performance, it\u2019s nice to see that this work started by investigating the Gaussian mixture setting. Though oversimplified, the low-dimensional simulated setting does provide plenty of insights and can be explicitly analyzed. The acquired insights are further validated by the following real-data experiments.  \n\nThe implicit bias towards SNE with uniformity constraint (sec 3.1.3) is most interesting to me, which points to a very important, but perhaps overlooked question in understanding contrastive learning. Contrastive learning works extremely well in practice, which is surprising since the learning objective is so simple and has (infinitely many) trivial solutions. I agree with the authors that the discriminative features are mainly learned implicitly. The order preserving phenomenon does shed light on understanding the CL for real data. \nFollowing the authors\u2019 argument, the role of positive pair alignment is merely to simplify the input space, by merging equivalent classes. Then the feature learning process is essentially SNE with uniformity constraint.  This is a very interesting point to me and may call for more investigations. \n\nThe dimensional efficiency (sec 4.2) and OOD generalization are also interesting and are very thought-provoking. In Figure2(b), it\u2019s surprising to see that the dimensionality gains for CIFAR-10 classification are almost non-existent after only 16. It is interesting to see how the dimension affects the performance in the OOD setting. \n\nWeaknesses: \nThe title may have been overclaimed. The paper only concerns the InfoNCE loss (SimCLR, MoCo) and there are various other self-supervised learning methods, e.g., SimSiam, BYOL, etc. How does the connection to SNE carry over to them? Can we observe the same phenomenon? \n\nSome questions/comments:\n- I am curious to see how does feature dimensions affect the performance in the OOD setting. My guess is that the performance gain due to higher dimensions should be much more significant than IID case, e.g., Figure 2(b). \n\n- The order/neighbor preserving phenomenon is not very easy to see in the 2-d Gaussian case. I would suggest the authors make the clusters more extreme so that which should be closer to which is easier to see. \n\n- Typo: The first sentence of sec 3.1.3, results on SSCL **provide**\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The writing is easy to follow. It\u2019s a little dense but I guess it\u2019s ok.\n- Quality: The quality is good. The provided insights are significant. The analysis is well supported by numerical experiments. \n- Novelty: The novelty is high. The connection between SNE and InfoNCE type SSCL is new to me and the analysis that followed is original and natural.",
            "summary_of_the_review": "This paper is novel and original. The contribution is solid. The SNE perspective is helpful for both theoretical understanding of SSCL as well as guidance for practical improvement. I believe this work is helpful for both the SNE and SSCL communities and may facilitate further exchange between the two.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2180/Reviewer_emn1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2180/Reviewer_emn1"
        ]
    }
]