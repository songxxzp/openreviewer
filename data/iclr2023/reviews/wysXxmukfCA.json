[
    {
        "id": "wHWs8dASc",
        "original": null,
        "number": 1,
        "cdate": 1666674333061,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674333061,
        "tmdate": 1666674333061,
        "tddate": null,
        "forum": "wysXxmukfCA",
        "replyto": "wysXxmukfCA",
        "invitation": "ICLR.cc/2023/Conference/Paper4373/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an adversarial training-based framework to ensure robustness in the watermark of deep neural networks. Specifically, a normalized gradient method is applied to find the worst case within the vicinity of the original model such that we can minimize the loss in terms of noisy parameters. A clean sample-based BatchNorm is also proposed to improve consistency during the training process. ",
            "strength_and_weaknesses": "Pros:\n1.\tThis paper validated the vulnerability in the watermark of deep neural networks and shows that it is necessary to develop a robust watermark framework.\n2.\tThis paper solves the inconsistency issue during the training by introducing a clean-sample-based BatchNorm.\n3.\tExtensive experiments show that the proposed method achieves superior robustness of watermark verification.\n\nCons:\n1.\tI have concerns about the approximation of perturbation for parameters. Equation 3 aims to find the worst case of the delta that can maximize the loss over parameters.t I do not see the clues that equation 5 is an approximation form of equation 3. It looks like equation 5 randomly selects a point (depending on the epsilon) within the vicinity of the original model.\n2.\tOnly minimizing the worst case is not theoretically enough. The author should select multiple neighbor points in parameter space for each training iteration.\n3.\tThe ablation study is not enough. Figure 6 only shows results against FT attacks.\n4.\tThe paper controls the perturbation size with epsilon parameters. It is hard to ensure that the l norm of perturbation is within a fixed bound cause theta in equation 5 constantly changes during training.\n5.\tIt would be good to show the results on more complex datasets like ImageNet or CIFAR10 with a simpler architecture. Because we all know that a slight change of parameters can significantly impact output, the proposed method work on CIFAR10 might be because the res-net is too redundant for CIFAR10 such that small changes in parameters have no impact on it.\n6.\tIt is unclear for how to distinguish the proposed method from traditional adversarial training method. The SOTA adversarial training methods can still be applied in the context of the watermark of deep neural networks.",
            "clarity,_quality,_novelty_and_reproducibility": "It is understandable to a large extent, but parts of the paper need more work.",
            "summary_of_the_review": "In general, the studied problem is interesting and important. In addition, the methodology is principled with three major merits as discussed above. However, the work still has some unaddressed concerns to well justify its technical and empirical contributions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4373/Reviewer_y4Xt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4373/Reviewer_y4Xt"
        ]
    },
    {
        "id": "PsNtLvJCwWm",
        "original": null,
        "number": 2,
        "cdate": 1666729246272,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666729246272,
        "tmdate": 1666729246272,
        "tddate": null,
        "forum": "wysXxmukfCA",
        "replyto": "wysXxmukfCA",
        "invitation": "ICLR.cc/2023/Conference/Paper4373/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a minmax approach to improve the watermarked model's capacity to counter opponents' deceiving methods. This approach originates from the observation that many watermark-removed models exist around the vicinity of the watermarked one. ",
            "strength_and_weaknesses": "This language reads well. And the proposed techniques seem effective from experimental results.\n\nHowever, there are several downsides:\n1) Motivation is neither clear nor reasonable. One assumption made by the authors is that the adversary has methods to obtain an unauthorized copy of the watermarked model. In practice, I don't think it is possible and if that happens, one should upgrade their security system instead of finding a method that can fight against different attacks. Besides, wouldn't it be more practical to release a second watermarked model once that happened? \n2) Comparison is only conducted on some baselines instead of SOTA approaches. It is mentioned in the related work (robust black-box model watermark) that several recent methods also tried to propose better methods to counter attacks. I wonder why the authors did not compare with them. Comparison with SOTA methods is desperately needed.\n3) The cBN technique is a trick, lacking enough technical novelty, though it is not listed as the main contribution. Then it makes no sense to spare a huge section explaining it, especially when more details are needed to elaborate the main approach better.",
            "clarity,_quality,_novelty_and_reproducibility": "The method section can be better if provided with better figures and examples. The novelty of the main approach that formulates a minimax problem seems good but the other techniques used in this approach such as cBN pose questions that whether this approach is robust enough. Some details are missing to reimplement the approach but generally seems enough to have a picture of the computation graph.",
            "summary_of_the_review": "According to the several drawbacks I mentioned in previous parts, I do not quite believe this paper is adequate enough for publication at ICLR. The paper can be better if the motivation goes more clear and the comparison is conducted with SOTA methods. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "This paper works towards a better watermarked model. Adversaries can fight against this approach as well and do harm to users.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4373/Reviewer_9HCi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4373/Reviewer_9HCi"
        ]
    },
    {
        "id": "nUj_4vS5E",
        "original": null,
        "number": 3,
        "cdate": 1666799819061,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799819061,
        "tmdate": 1666799819061,
        "tddate": null,
        "forum": "wysXxmukfCA",
        "replyto": "wysXxmukfCA",
        "invitation": "ICLR.cc/2023/Conference/Paper4373/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for watermarking deep neural networks for classification against adversarial attacks. The authors proposed a min-max formulation of the watermark defense problem and solved the problem through first-order approximations to the inner maximization problems. Experiment results show the method is robust towards various attacks while keeping a reasonable accuracy on regular samples. ",
            "strength_and_weaknesses": "Strength: \nWatermarking neural networks is an important research problem. The \"black-box\" setting under which the authors studies the problem is realistic for practical applications. The proposed min-max formulation is clear and intuitive, and the experiment results support the claims that the method is effective in defending against adversarial attacks. \n\nWeakness:\nSome minor issues with writing and experiments. \n\n1. The BatchNorm on clean samples technique should be highlighted more in Algorithm 1 as it is critical to the method. \n2. Section 4.1 claims experiments were done on CIFAR-100, but I don't see any results on CIFAR-100. In particular, I am curious to see whether this method scales reasonably with respect to the number of class labels. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The proposed method is novel. ",
            "summary_of_the_review": "Overall, I recommend the acceptance of this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4373/Reviewer_6TWD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4373/Reviewer_6TWD"
        ]
    }
]