[
    {
        "id": "sR5raVGvYVY",
        "original": null,
        "number": 1,
        "cdate": 1666292612320,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666292612320,
        "tmdate": 1666292664425,
        "tddate": null,
        "forum": "OE4uriQtuDJ",
        "replyto": "OE4uriQtuDJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5706/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores multi-view representation learning and introduces the Multi-View Masked Autoencoder (MV-MAE) framework. A video autoencoder is updated to function with multi-view input video streams. The learned representations are shown to be useful for the downstream task of visual control by training reinforcement learning agents. Evaluation is conducted on visual manipulation tasks from RLBench. Results show improvement over baselines. ",
            "strength_and_weaknesses": "Strengths\n\n- Interesting problem area.\n\n- Well written and easy to follow.\n\n- Evaluated versus baselines, including an ablation study.\n\nWeaknesses\n\n- Lack of technical novelty\n\nThe proposed MV-MAE is an agglomeration of recent techniques. The underlying method is MAE [1], but using input in the style of [2] (convolutional features, instead of raw pixels). A learnable parameter reflecting the camera viewpoint / frame timestep is added to each token in the style of [3]. The rest follows a standard video masked autoencoder [4]. Though a view masking strategy is proposed, it is ultimately not compared against any alternative strategies. The visual control portion of the paper uses representations from MV-MAE as input to an off-the-shelf RL algorithm from [2] with evaluation from benchmark suite RLBench.\n\n[1] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[2] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. \"Masked world models for visual control.\" Conference on Robot Learning, 2022a.\n\n[3] Geng, Xinyang, et al. \"Multimodal Masked Autoencoders Learn Transferable Representations.\" International Conference on Machine Learning: Workshop on Pre-Training, 2022.\n\n[4] Feichtenhofer, Christoph, et al. \"Masked Autoencoders As Spatiotemporal Learners.\" arXiv preprint arXiv:2205.09113 (2022).\n\n- Lack of detail\n\nMost of the pieces of the proposed approach are from pre-existing works, therefore no detail is given in the manuscript outside of the reference. For example, what's the format of the learnable parameter representing camera ID / frame ID and how are they integrated? Additionally, what are the optimization details and how does the iterative training process work?\n\n- Limited evaluation\n\nThough the evaluation is fairly extensive, the experiments don't give insight into specific design choices that are crucial for MV-MAE to \"work\". For example, what is the impact of the view masking strategy? ",
            "clarity,_quality,_novelty_and_reproducibility": "- Easy to follow. Well written.\n\n- Likely reproducible.\n\n- The work appears to be original.",
            "summary_of_the_review": "The topic area, multi-view representation learning, is interesting.  Empirically, the proposed method (MV-MAE) shows improvement over baselines on the downstream task of visual control. However in terms of technical novelty, the representation learning component is a slightly tweaked masked autoencoder. The visual control portion exists just for evaluating the learned representations, and uses an off-the-shelf RL algorithm from a recent paper. The manuscript is of high quality but very little space is allocated to describing the proposed method (essentially just Section 3.1). For me, the paper reduces to applying masked autoencoders to a pre-existing visual control task. The results are compelling, but novelty is lacking, which makes this a tough paper to review. \n\nUltimately, I feel this paper falls on the \"marginally below the acceptance threshold\" side of things. For me, I would want the paper to allocate more space to method description: highlighting important details that make it work (e.g., is it the specific choice of view masking strategy?) and adding experiments that reflect why this is the right way to structure things in the multi-view case. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5706/Reviewer_HaAG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5706/Reviewer_HaAG"
        ]
    },
    {
        "id": "f0qHn8eLgLy",
        "original": null,
        "number": 2,
        "cdate": 1666552643071,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666552643071,
        "tmdate": 1666552643071,
        "tddate": null,
        "forum": "OE4uriQtuDJ",
        "replyto": "OE4uriQtuDJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5706/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Motivated by the fact that data diversity plays a key role in representation learning, this paper proposes \"Multi-View Masked Autoencoder\" to leverage the cross-view and cross-frame information to learn the visual representation for control tasks. Experiments validate that the proposed multi-view representation learning framework outperforms both single-view and other multi-view representation learning baselines. ",
            "strength_and_weaknesses": "__Strengths:__\n- The paper writing is in general clear and easy to follow. \n- The performance gain is significant compared with single-view and other multi-view baselines. Ablation experiments are also on point and validate various design choices\n-  Detailed appendix about reproducibility.\n\n\n__Major Weaknesses:__\n- \"Method\" part is argurably the weakest part of the paper, as this work pretty much follows the _Masked World Models for Visual Control_ paper and extend it into a cross-view setting without major changes. It applies MAE into the multi-view scenario and validates its performance in this setting.\n- The message this work delivers is well-known already. It is obvious that learning from cross-view cameras can boost the single-view results.   \n- Some minor issues regarding the paper writing, typo and citations. (See below)\n\n__Minor Issues:__\n- First paragraph of introduction. starting off the paper with multi-modal learning is inappropriate. The paper is using camera only and it's not comparing itself with nor improving upon multi-modal approaches. Quoting multi-view camera papers makes more sense in backing up your argument in diversity, as opposed to quoting multi-modal papers.\n- Do not include Google Brain in the citation of \"Time-contrastive networks: Self-supervised learning from video\" \n- Typo. Figure 1 caption, the autoencoder reconstruct\"s\" all frames at the same time",
            "clarity,_quality,_novelty_and_reproducibility": "__Clarity__: The paper is clearly written and easy to follow.\n\n__Quality__: The quality of writing and visualization are both good.\n\n__Novelty__: The technical novelty is marginal. The paper uses existing architecture and applies it into a new setting. \n\n__Reproducibility__: The paper seems to provide enough details to reproduce the results. But I did not try it myself.\n",
            "summary_of_the_review": "The paper is well written and it has its significance in experimental results as the improvement over baselines is noticable and the ablation study is well executed. But the technical novelty is limited as it directly applies existing Visual Control MAE model into the cross-view scenario. The message about the importance of using multi-view information is delivered by many other vision work already so it's not new as well. Given the great results and the marginal novelty, the paper straddles right on the acceptance threshold, leaning a bit towards rejection due to the lack of new messages. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5706/Reviewer_ntdx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5706/Reviewer_ntdx"
        ]
    },
    {
        "id": "p10xA_VajM",
        "original": null,
        "number": 3,
        "cdate": 1666656131770,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656131770,
        "tmdate": 1666656131770,
        "tddate": null,
        "forum": "OE4uriQtuDJ",
        "replyto": "OE4uriQtuDJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5706/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents Multi-View Masked Autoencoder (MV-MAE) for multi-view representation learning and show its benefits in visual manipulation tasks. Experiments are conducted on RLBench demonstrating that MV-MAE (with MWM) significantly outperforms baselines (MWM, MWM+MSN, TCN) consistently across various setups. Ablation experiments provide enough details to understand the effects of different design choices. Written presentation is clear and easy to read & understand. ",
            "strength_and_weaknesses": "# Strength\n- The idea of multi-view representation learning with MAE and applied to visual control tasks is novel and interesting.\n- Experimental results are strong and superior to baselines. The set of selected baselines fully cover most cases of comparisons(to understand the differences). Ablation experiments also fully cover most of design choices.\n- Written presentation is great (high clarity and easy to follow).\n\n# Weakness\n- Experiments are done on only one benchmarks.\n- On the downstream application of visual control tasks, only one model, aka MWM, is used with MV-MAE.",
            "clarity,_quality,_novelty_and_reproducibility": "* The proposed approach is novel.\n* The experiments are solid and the author(s) provide code (assume to be reproducible).\n* The paper is written in high clarity and easy to read.",
            "summary_of_the_review": "As mentioned above, the proposed approach is novel and experimental results solid, I would recommend an acceptance for this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "* The author(s) mention in their statement about the possibility of misuse in robot and recommend safety use of the approach. This is applied to all research related to robotics not just this paper.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5706/Reviewer_Zds9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5706/Reviewer_Zds9"
        ]
    },
    {
        "id": "-p1MyOGalf",
        "original": null,
        "number": 4,
        "cdate": 1666667476702,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667476702,
        "tmdate": 1666667476702,
        "tddate": null,
        "forum": "OE4uriQtuDJ",
        "replyto": "OE4uriQtuDJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5706/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper designs a multi-view masked autoencoder for learning features for visual control. The motivation is that better visual representation leads to better control, which can be obtained from multi-view pre-training using the proposed framework. Results on RLBench have validated the effectiveness of the proposed framework and each introduced components. ",
            "strength_and_weaknesses": "**Strength**\n\n- This paper first verified the effectiveness of using multi-view data for pre-training. Moreover, the multi-view pre-training also helps the single-view control tasks. \n- The proposed framework MV-MAE is conceptually simple yet works great. It out-performs the single-view baseline and alternative self-supervised methods baseline. Each sub-modules also contribute to the final performance. \n- Experiments are solid and the results are promising.\n- Writing is easy to follow. Related work is properly discussed.\n\n**Weakness**\n\n- To me, the problem setting isn't challenging enough to test transfer learning. The pre-training and the testing scene are all RLBench and almost the same (same background, robot, etc). Compared to the normal setting in CV tasks, the training-testing gap isn't significant enough. It's encouraging that the visual pre-training still help in this scenario, but it could be more convincing if such representations can be tested under different setting. \n- Even though the masked auto-encoder (MAE) models outperfoms the Masked Siamese Network (MSN) and Time Contrastive Network (TCN), I don't get the motivation behind it. What's the issue that multi-view MAE addressed? What's the scenario that MSN/TCN fail to handle but MAR is able to? What's the difference between the learned features? Can we somehow visualize it and see the difference clearly? \n- The technical contribution is somewhat limited as 1) video MAE is proposed in Feichtenhofer et al., 2022, and 2) the benefit of using multi-view data for control has been studied before (related work last paragraph). This work is a bit incremental considering prior work MWM (Seo et al. 2022a). \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality**\n\nThe proposed framework MV-MAE is technically sound. It's an multi-view extension of prior work MWM (Seo et al. 2022a). Even though simple, MV-MAE works well on RLBench tasks. Each sub-module is properly ablated. Overall, the paper is clean and easy to understand.\n\n**Novelty**\n\nAs I stated in the weakness section, the proposed MV-MAE is a bit incremental considering prior work in visual pre-training (MAE, video-MAE) and in pre-training for control (MWM). Also, there are existing work studying the benefit of multi-view data.\n\n**Reproducibility**\n\nThe source code has been submitted. Even though I didn't run it and try to reproduce the results in the paper, the code reads reasonable. ",
            "summary_of_the_review": "This paper presents a framework for using multi-view visual pre-training for robot control. Even though sound and promising, the technical contribution and the experimental evaluations are both limited. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't have any ethics concerns so far. However, I didn't check research integrity issues (e.g., plagiarism, dual submission) since I don't know what's a good way to do this. ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5706/Reviewer_mrqQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5706/Reviewer_mrqQ"
        ]
    }
]