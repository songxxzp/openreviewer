[
    {
        "id": "1IWuJsFdXf",
        "original": null,
        "number": 1,
        "cdate": 1666644832834,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644832834,
        "tmdate": 1666644832834,
        "tddate": null,
        "forum": "QwqxO8URJzn",
        "replyto": "QwqxO8URJzn",
        "invitation": "ICLR.cc/2023/Conference/Paper4568/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The author has discussed the transformers models' training stability on various tasks. The author has investigated the sharpness of attention concerning attention entropy on each attention head during training. The author observes that the attention entropy first decreases then increases, and then enters into the long stable stage. To address the issue, the author has proposed a simple technique \\sigma_reparametrization, which reparametrize all the linear layers with spectral normalization and learnable scalar multiplication. Finally, the author has evaluated the proposed techniques on several tasks, such as supervised image classification, self-supervised learning, automatic speech recognition, and language modeling tasks, and claimed better stability and robustness concerning the choice of hyperparameters.",
            "strength_and_weaknesses": "Strength:\nThe main reason to accept this paper is empirical results and showing performance o various tasks. The author has provided more quantitative results on various tasks.\n\n\nWeaknesses:\n\nThe current literature survey does not discuss the advantages and disadvantages of the paper. The author should validate why I should include this paper in this current work. \n\nHow did you get this equation-2? Is there any intuition of this equation?\n\n\nThe generalization of the method. The equation-2 is valid for the linear layer. What happened to the convolution layer? Is this valuable reparametrization for the convolution network?\n\nThe paper does not explain the relation between attention entropy and \\sigma_reparameterisation.\n\nHow does this learnable parameter contribute to the stability of the transformer?\n\nInstead of evaluating the proposed method on multiple tasks, the author could have done an ablation analysis on different kinds of networks and various types of transformer architecture. The author should provide ablation analysis results to clarify the proposed method's work principle.\n\nWhy do we need such reparametrization, learning another parameter? Did you analyze other kinds of techniques to tackle the attention entropy collapse problem? This requires a substantial literature survey and motivation for the proposed method. Can you justify this? How is the attention entropy collapse solved by the proposed reparametrization technique?\n\n\nThe author should compare the stability analysis results of this transformer with the existing state-of-the-art stable transformer. Also need to add a small paragraph in the related work section about stability/robustness in the transformers.\n\nCan you elaborate on the number of parameters and FLOPS required for this? What about the complexity of the proposed method?\n\nThe name spectral reparametrization in the title makes it confusing here. It seems it works on frequency domain reparametrization. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the weaknesses section.",
            "summary_of_the_review": "\nJustification :\nThe overall writing quality is ok, and the proposed method is simple and beneficial. However, the experiment comparison lacks justification, and the technical contribution is plain.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4568/Reviewer_qd4k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4568/Reviewer_qd4k"
        ]
    },
    {
        "id": "5dLt90KIiw",
        "original": null,
        "number": 2,
        "cdate": 1666648869515,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648869515,
        "tmdate": 1666648869515,
        "tddate": null,
        "forum": "QwqxO8URJzn",
        "replyto": "QwqxO8URJzn",
        "invitation": "ICLR.cc/2023/Conference/Paper4568/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Setting: Transformer model, test on speech recognition and other tasks.\n\nMeasure attention entropy.\nFind correlation between the minima of attention entropy and the model\u2019s training stability.\n(How is training stability measured?)\nReparametrize all linear layers with Spectral Normalization and an additional learned scalar.\n\nExperiments with \u03c3Reparam on image classification, image self supervised learning, automatic speech recognition and language modeling.\n",
            "strength_and_weaknesses": "Strength:\n- Somewhat novel reparameterization of the weights.\n- Some analysis on attention entropy and its correlation to the training stability.\n\nWeaknesses:\n- Very similar to weight norm, so the novelty is limited.\n- No actual comparisons to weight norm?\n- Source code not released?\n- Results are not good?\n- Librispeech train-clean-100 is not representative as a speech recognition task.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Weight norm is mentioned, which sounds very similar from the motivation to decouple the magnitude of the weights. It is claimed that its effectiveness in Transformers is limited. Was that tested? Where are the results? It would be interesting to see such a comparison.\n\nI'm not so familiar\u00a0with image classification. ImageNet-1k current SOTA seems to be 88.3% for Top1 accuracy (https://paperswithcode.com/sota/image-classification-on-imagenet?tag_filter=171). The provided 82.2% are quite far away from that?\n\nThe table 1 should contain some recent SOTA results.\n\nWhy only the train-clean-100 subset of Librispeech? This subset is way too easy, way too small, and not really used often in the literature, so not good for comparison. It should be tested on the real standard Librispeech, and potential other relevant speech corpora.\n\n\"We use SpecAugment (Park et al., 2019) only activated right at the beginning of training.\" - What does this mean? Later in training it is deactivated?\n\n\"Training We use Adagrad (Duchi et al., 2011) if not specified otherwise,\" - why Adagrad? This is quite unusual. How does it compare to SGD or Adam?\n\nIn Table 3, there should be other results from the literature for comparison. Esp, it is said that the model is used from\u00a0(Likhomanenko et al., 2021a). Where is that in the table? The numbers in\u00a0(Likhomanenko et al., 2021a) all look better?\n\nI don't exactly understand the speech recognition results. So it seems that in general, the proposed method yields worse results than the baseline?",
            "summary_of_the_review": "Unfortunately there are too many weaknesses which should be addressed. If those are addressed, and it can indeed yield improvements over good SOTA models, on actual relevant benchmarks, then this would be very interesting. But this requires much more work.\n\nIf the argument is only about more stable training and not about better performance, this anyway needs to be better justified and tested.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4568/Reviewer_fFS8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4568/Reviewer_fFS8"
        ]
    },
    {
        "id": "4q4Q7_4E-g",
        "original": null,
        "number": 3,
        "cdate": 1666662606811,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662606811,
        "tmdate": 1669755172779,
        "tddate": null,
        "forum": "QwqxO8URJzn",
        "replyto": "QwqxO8URJzn",
        "invitation": "ICLR.cc/2023/Conference/Paper4568/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes the $\\sigma$-reparameterised transformer. The authors note a phenomenon in during training of transformers where the 'attention entropy' (essentially, how sharply peaked the attention scores are) decreases and then increases again. In the cases where the transformer fails to learn, this attention entropy does not recover. In order to combat this, the $\\sigma$-reparameterisation is introduced, essentially a version of spectral normalization where the matrices are rescaled by a learned parameter (instead of having unit norm). This provides impressive gains in stability of training and empirical performance on a variety of tasks. ",
            "strength_and_weaknesses": "Strengths\n+ The empirical results are strong, showing increased robustness to changing hyperparameters and similar or better final performance. This is a good result for democratisation of ML, since it reduces the amount of time and money that must be spent on fiddly hyperparameter tuning. Figure 4 is particularly compelling. \n+ The method is extremely simple to implement, yet has (to my knowledge) not been proposed before.\n+ The experimental results are fairly comprehensive. \n\nWeaknesses\n+ While I believe the final experimental results stand on their own, on a scientific level I am not convinced by the argument relating the failure to train to the collapse of the attention-entropy. In the case in figure 1, the two phenomena are definitely correlated, but this doesn't appear to continue in figure 2, where the baseline attention entropy 'collapses' but still gives good downstream performance. Similarly, none of the successful baseline models in figure 3 seem to have this 'dip' phenomenon where the entropy decreases then increases as the model begins to learn properly. Similarly, the connection between the singular values and attention entropy doesn't seem to hold practically: look at figure 3 where we have the green solid and dotted line have similar singular values but completely different attention entropy. It seems plausible that the collapse of the attention entropy and the failure of downstream performance could both be symptoms of some other problem, which the $\\sigma$-reparameterisation avoids. I realise the authors are careful to not say that entropy collapse causes training instability, but I believe it could be possible to get some direct evidence for this claim, which would strengthen the scientific contribution of the paper. \n  \n  + Nitpicks:\n    + Could you try a baseline of dividing the initalization by a factor of the spectral norm of a random matrix?\n    + In the theory you use $\\|W_KW_Q^\\top\\|_2$, but in the implementation it seems that you apply the reparameterisation on all of the weight matrices. Did you try only reparameterising the $W_KW_Q^\\top$ matrix directly?\n    + How does the overall memory usage compare when using LARS and fp32 on attention compared to adam and full mixed precision?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThe paper is quite clear\n\nQuality\n\nThe paper is high quality in terms of the experiments. The argument motivating of $\\sigma$-reparameterisation from the standpoint of fixing the problem of entropy collapse could be improved. However I wouldn't classify this as the main contribution. \n\nNovelty\n\nAs far as I am aware, the method is novel. However, I am not an expert on transformer regularization, and this approach could well be discussed in an appendix of previous work or similar. \n\nReproducibility\n\nThe code does not seem to be available, but the method is so simple it could be implemented and reproduced quite quickly (modulo the expense of training a large transformer model nowadays)",
            "summary_of_the_review": "The paper is a good empirical contribution to the state of the art of training transformer-based models, giving a new form of parameterisation of the weight matrices that dramatically improves the robustness to hyperparameter choices. This will lead to improved reliability of training transformers. The scientific argument analysing the attention collapse is a bit weak, and could be improved. But since that is not the focus of the paper, it shouldn't be required for an accept. \n\nUpdate after rebuttal:\nAfter reading the authors' rebuttal, I believe they have addressed my queries and some of the insightful remarks from the other reviewers.\n\nUpdate after end of rebuttal period: \nGiven the lack of response by the reviewers to the rebuttal to the experimental queries from the other reviewers, I'm unsure to what extent the issues raised by the other reviewers have been satisfied. Since I'm not completely familiar with the state of the art in stable transformer architectures, I've reduced my confidence. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4568/Reviewer_janJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4568/Reviewer_janJ"
        ]
    },
    {
        "id": "HU2j0mQEUz",
        "original": null,
        "number": 4,
        "cdate": 1667062302274,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667062302274,
        "tmdate": 1667783549402,
        "tddate": null,
        "forum": "QwqxO8URJzn",
        "replyto": "QwqxO8URJzn",
        "invitation": "ICLR.cc/2023/Conference/Paper4568/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the instability of Transformers by studying the progression of attention entropy: first a decrease followed by a quick increase then a long stable phase. A strong correlation exists between the minima of attention entropy and training stability. In the study, decreasing learning rate and increasing the warmup epoch leads to lower training loss. In addition, the following observations were made: Increasing the learning rate can lead to attention entropy collapse.To lower attention entropy,  reparametrizement of all linear layers with spectral normalization is considered.  The proposed algorithm is evaluated on 4 tasks: image classification, image self-supervised learning, ASR, and language modeling.",
            "strength_and_weaknesses": "(-) The readability of the paper needs much to be desired. It seems that instability is measured by the entropy of the attention head. Decreasing the learning rate or increasing warmup epoch leads to more stability. Without providing any explanation, the paper mentions that the training loss decreases with increase in learning rate. The paper should state whether decrease in training loss comes from stability. What is the consequence of entropy collapse? Explain how entropy collapse relates with the stability and accuracy? Is  stability and accuracy related? It is not clear how the lower bound of the attention entropy is related to stability.\n\n(-) The paper is proposing a re-parameterization of the weights of the linear layer using a spectral norm. It is not clear how this is related to regularization and performance increase? Many normalization methods have been proposed in the past, and it is not clear how the proposed is different previous normalization methods- a table should compare the performance of previous methods.",
            "clarity,_quality,_novelty_and_reproducibility": "(-) The paper is not clear. It introduces a spectral normalization, but the explanation of how it differs from past spectral normalization is not convincingly given.\n\n(-) Many normalization methods have been proposed, and it is not clear how this differs from the past such as Eq(2). The paper discusses briefly the differences, but the constant gamma in the normalization does not seem to be a major difference.  \n\n(+) The paper should be reproducible but it is not certain.",
            "summary_of_the_review": "See strength and weakness.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4568/Reviewer_3X2m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4568/Reviewer_3X2m"
        ]
    }
]