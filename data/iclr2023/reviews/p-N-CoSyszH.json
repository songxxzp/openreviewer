[
    {
        "id": "YY15Ey4vB_",
        "original": null,
        "number": 1,
        "cdate": 1666335729886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666335729886,
        "tmdate": 1666335729886,
        "tddate": null,
        "forum": "p-N-CoSyszH",
        "replyto": "p-N-CoSyszH",
        "invitation": "ICLR.cc/2023/Conference/Paper1485/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel graph transformer architecture that jointly aggregates structural neighbors and semantically related neighbors to encode each node. The proposed method is evaluated on various levels of graph learning tasks and confirmed to be effective.",
            "strength_and_weaknesses": "strengths:\n1. This paper studies a critical problem, that is to say, how to preserve the advantage of transformer to enjoy a large receptive field and, at the same time, efficiently aggregate discriminative information. In this regard, the contribution is likely to make a significant impact to the community.\n2. The proposed neural architecture is novel to me, which, instead of considering sparsity trick or hierarchical structure, directly defines neighborhood in the latent semantic space.\n3. The experiments are solid in terms of their comprehensiveness. Besides, the ablation studies confirm the effectiveness of introduced components. The case study of semantic neighbors also implies that the semantic encoder works with meaningful neighbors.\n\nweaknesses:\n1. It seems that there is a lack of theoretical analysis about the proposed architecture. I am really curious about what semantic similarities would be learned eventually. Meanwhile, what is the negative distribution should be adopted deserves a more detailed discussion, especially considering the number of examples increases along with the number of hops exponentially.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Generally, this paper is well-written. One suggestion is to notate the structural and semantic embeddings (i.e., $h$) with layer index (e.g., by superscript) and explain how the transformer layers are stacked.\n\nQuality: The proposed novel architecture is motivated in a natural way. The design of the semantic component sounds good from a technical perspective, but a related theoretical analysis would be more in a demand. The proposed architecture is evaluated on various levels of tasks, and its advantages over SOTA methods seem to be consistent. It would be better to provide standard deviations and a statement of whether the advantage is statistically significant.\n\nNovelty: The proposed neural architecture is novel to me, although it is somewhat straightforward.\n\nReproducibility: All the adopted datasets, considered baselines, and the implementations are publicly available.",
            "summary_of_the_review": "I think the proposed transformer architecture is somewhat novel to me. It is empirically evaluated in an extensive way, showing its effectiveness. Currently, I can only evaluate the behavior of semantic component based on the case shown in experiment, where the semantic neighbors seem to be truly semantically relevant. I don't have confidence to suggest an acceptance now, and more theoretical analysis would resolve my concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1485/Reviewer_FttT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1485/Reviewer_FttT"
        ]
    },
    {
        "id": "yiRtuFG7UY",
        "original": null,
        "number": 2,
        "cdate": 1666386326517,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666386326517,
        "tmdate": 1666386326517,
        "tddate": null,
        "forum": "p-N-CoSyszH",
        "replyto": "p-N-CoSyszH",
        "invitation": "ICLR.cc/2023/Conference/Paper1485/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new Graph Transformer architecture, named dual-encoding Transformer (DET), which has a structural encoder to aggregate information from near neighbors and a semantic encoder to focus on useful semantically close neighbors. The empirical results demonstrate that the proposed DET achieves superior performance compared to the respective state-of-the-art attention-based methods.",
            "strength_and_weaknesses": "strength:\n- The idea of disentangle and combine structural neighbors and semantic neighbors looks interesting to me.\n- The authors conducted thorough experiments on various datasets.\n\nweaknesses:\n- Some writing is a bit confusing. How to understand and interpret Figure 2? How do the authors define the semantic set $\\mathcal{N}^{\\text{se}}$? \"In our implementation, it is sampled from the top candidate\" what are the pool to select top candidates then? Is it all the other nodes?\n- What's the runtime analysis of the proposed method? How to understand Table 7? Why sometimes there's a nontrivial overhead and sometimes not?\n- The empirical performance of the proposed method is no longer the state-of-the-art performance. Please check [1]'s results on ZINC, for example. The authors should include and compare with this paper.\n\n[1] Dwivedi, Vijay Prakash, et al. \"Graph Neural Networks with Learnable Structural and Positional Representations.\" International Conference on Learning Representations. 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is not satisfactory, certain modules are not very clear to me.\nThe authors provide source code that looks well documented so I guess there's no concern on reproducibility.\nI feel there's quite limited novelty in the proposed approach. The dual module idea is similar to model ensemble.",
            "summary_of_the_review": "The submission has thorough empirical results, but the empirical results do not look very competitive and the proposed method has only limited novelty",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1485/Reviewer_e1hj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1485/Reviewer_e1hj"
        ]
    },
    {
        "id": "3J8HVWx4_i",
        "original": null,
        "number": 3,
        "cdate": 1666532612221,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666532612221,
        "tmdate": 1666532734606,
        "tddate": null,
        "forum": "p-N-CoSyszH",
        "replyto": "p-N-CoSyszH",
        "invitation": "ICLR.cc/2023/Conference/Paper1485/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes DET, a transformer architecture for handling large graphs.\n\nDET relies on two attention mechanisms:\n- A structural attention mechanisms, i.e. for a given node, attention applied between a virtual global node and the neighboring nodes only. Moreover, a learnable position encoding is added (including centrality, distance to other neighbors and edge type).\n- A semantic attention mechanism, i.e. attention applied to semantic neighbors. More precisely, and for a given node, the other nodes are ranked depending on a learned embedding similarity score which is then used as an attention weight. This score can be learned via a self-supervised objective.\nAlthough computing the similarity score for each node has quadratic complexity in the number of nodes., the ranking step is done every few epochs only, providing an efficient attention mechanism in practice.\n\nThe authors then proceed to demonstrate the effectiveness of graph and node level tasks, with strong results for PCQM4M, ZINC, ogbn arxiv and knowledge graph completion. Finally, further studies of DET are provided: an ablation is done to understand the role of structural attention, semantic attention, and the semantic attention loss, a study on the usefulness of semantic encoding w.r.t. graph homophily, and an example of learned relations by the different mechanisms.\n",
            "strength_and_weaknesses": "Strength:\n- This work offers a potential solution, DET, to an important problem, scaling transformers to large graphs.\n- The method is simple and motivated: a local attention scheme is augmented by an attention scheme that is able to fetch remote useful nodes. An SSL loss allows to learn the score used for ranking and attention weighting. Ranking only every few epochs seems to provide an efficient attention scheme in practice.\n- DET demonstrates strong results on various and important benchmark such as PMQC4M and ZINC.\n- This work studies in detail the proposed attention scheme and in particular the semantic attention: we see that the nodes fetched by the semantic encoder make sense, and that the semantic encoder is as expected useful when considering low homophily graphs.\n\nWeaknesses:\n- One novelty of this work, and what makes it scalable, is applying a custom attention to top ranked nodes, with an updated ranking every few epochs. In light of this, a natural baseline would be to apply linear attention instead of the proposed $f_s$: it would be more convincing to demonstrate the usefulness of $f_s$ compared to linear attention.\n- The other novelty is the use of a virtual node for a local attention mechanism. I am not sure to understand the motivation of this mechanism compared to a purely local attention scheme (attention between the node of interest and its neighbors): clarification and/or ablation could be useful to better understand the interest of the proposed scheme.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is generally well-written with a few caveats:\n- The motivation for the SSL loss is not clear: why enforcing high similarity between neighboring nodes? At first sight it seems redundant with the structural attention scheme which only applies attention to neighbors.\n- If I understand correctly, for a given graph, DET has to sort $n$ times $n$ scores and store $n^2$ scores in memory?\n\nQuality:\n- The methods is clearly described, correctly motivated and simple.\n- Experimental protocol seems thorough: varied tasks (graph and node level) as well as baselines are used. Moreover, sensible ablations are conducted although one seems to be missing in my opinion (see weakness).\n\nNovelty:\n- DET relies on two attention mechanisms: a local one, aimed at reflecting the structure, and a sparse, global one, aimed at gathering semantically similar remote nodes. Both mechanisms seem at least partially novel to me. Encoding the structure by considering substructures such as neighboring nodes has already been proposed [1][2] without involving a virtual node, and could potentially be discussed to better delineate the contribution of this work.\n\n[1] GraphiT: Encoding Graph Structure in Transformers (Mialon et al. (2021))\n\n[2] Structure-Aware Transformer for Graph Representation Learning (Chen et al. (2022))\n\nReproducibility: the code is provided along with experimental details in the appendix.\n",
            "summary_of_the_review": "This work offers a sensible solution to an important problem, with strong results. Although some elements could be clarified in the writing (novelty, more motivation for the virtual node in local attention and the SSL loss) and some ablation may be in my opinion missing, the pros outweight the cons and I tend to recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1485/Reviewer_aSXN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1485/Reviewer_aSXN"
        ]
    },
    {
        "id": "ig09pSYReg",
        "original": null,
        "number": 4,
        "cdate": 1666698759678,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698759678,
        "tmdate": 1666780571221,
        "tddate": null,
        "forum": "p-N-CoSyszH",
        "replyto": "p-N-CoSyszH",
        "invitation": "ICLR.cc/2023/Conference/Paper1485/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes Dual-Encoding Transformer (DET), which aggregates information from both local neighbors and semantically close neighbors to address the scalability issue of existing Transformer architectures for graphs. Also, the authors design self-supervised semantic neighbor fetching loss, which is jointly optimized with the main task loss.",
            "strength_and_weaknesses": "**Strengths**\n\n(1) This paper addresses the limitation of Transformer-based models for graphs, which is one of important topics to generalize Transformer to the graph domain.\n\n(2) The proposed DET shows the best performance on KG completion task.\n\n**Weakness**\n\n(1) I think that the novelty of this paper is limited. \n\n- The combination of structurally and semantically close neighbors has been already discussed in various works (Geom-GCN [1], Non-local GNNs [2]).\n- Also, semantic neighbor fetching loss seems similar to self-supervised loss of SuperGAT [3].\n\n(2) The accuracy of GCN and GraphSAGE on ogbn-arxiv is very low. From the leaderboard of ogbn-arxiv, the accuracy of GCN and GraphSAGE are 71.74 and 71.49, respectively. What makes deteriorating performance?\n\n(3) From Table 7, DET requires more average training time compared to Graphormer on PCQM4M-LSCv1 and ZINC datasets although DET uses less neighborhoods compared to Graphormer.\n\n(4) In Section 5.2, the paper discussed the correlation between semantic encoding and graph homophily. So, it would be better if the paper compare the performance on heterophilic graph datasets with baselines such as H2GCN [4].\n\n---\n\n[1] Pei, Hongbin, et al. \"Geom-gcn: Geometric graph convolutional networks.\"\u00a0ICLR 2020.\n\n[2] Liu, Meng, Zhengyang Wang, and Shuiwang Ji. \"Non-local graph neural networks.\"\u00a0TPAMI 2021.\n\n[3] Kim, Dongkwan, and Alice Oh. \"How to find your friendly neighborhood: Graph attention design with self-supervision.\"\u00a0ICLR 2021.\n\n[4] Zhu, Jiong, et al. \"Beyond homophily in graph neural networks: Current limitations and effective designs.\"\u00a0NeurIPS 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "**************Clarity**************\n\nThe paper is clearly written to understand overall method.\n\n**************Quality**************\n\nSome experiments for baselines are not properly conducted. Also, it would be better to conduct more experiments to validate the effectiveness of proposed models.\n\n**********Novelty**********\n\nThe proposed method has a limited novelty. \n\n******************************Reproducibility******************************\n\nThe authors share their source code in the supplement. The paper has good reproducibility.",
            "summary_of_the_review": "Overall, I am leaning towards rejection. My major concern is the novelty and experimental results. If you address my concerns, I will raise my score. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1485/Reviewer_4oiv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1485/Reviewer_4oiv"
        ]
    }
]