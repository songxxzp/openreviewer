[
    {
        "id": "2RRtmcjaNpQ",
        "original": null,
        "number": 1,
        "cdate": 1666260300349,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666260300349,
        "tmdate": 1666260300349,
        "tddate": null,
        "forum": "AFhaaOZTkKA",
        "replyto": "AFhaaOZTkKA",
        "invitation": "ICLR.cc/2023/Conference/Paper2947/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper explores the idea of addressing catastrophic forgetting through rehearsal using a memory buffer. It studies strategies to select the most important samples for storage in a memory buffer. The core idea is to design a consistency score that ranks samples for storage according to how easy they are to learn and at the same time are representative of previous tasks. Experiments using three common continual learning benchmarks\u00a0are performed to demonstrate that the proposed method is effective.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper is clear and easy to follow.\n\n2. Experiments on three major continual learning frameworks are provided.\n\nWeaknesses:\n\n1. The idea is incremental.\n\n2. The approach is heuristic and without much theoretical justification.\n\n3. Comparison against prior methods is extremely limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow. However, the novelty of the paper is incremental. It merely proposes another heuristic approach for selecting samples for experience replay. However, the strategy is not supported by theoretical justification. Experimental results do not demonstrate that the method leads to competitive performance against prior works. Hence, the novelty and contribution of this work are limited and do not offer an improved understanding of the sample selection strategies for continual learning.",
            "summary_of_the_review": "The idea of experience replay using selected samples is not new and many existing works have been proposed. The proposed idea is a heuristic idea without no theoretical justification. Experiments also do not demonstrate that the proposed method leads to state-of-the-art performance. Hence, the paper does not provide theoretical novelties or sufficient empirical results to demonstrate its effectiveness. In conclusion, it is not ready for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2947/Reviewer_Yna5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2947/Reviewer_Yna5"
        ]
    },
    {
        "id": "X-_69BkmK4",
        "original": null,
        "number": 2,
        "cdate": 1666580645596,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580645596,
        "tmdate": 1666591856893,
        "tddate": null,
        "forum": "AFhaaOZTkKA",
        "replyto": "AFhaaOZTkKA",
        "invitation": "ICLR.cc/2023/Conference/Paper2947/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates continual learning under limited memory setup  and proposes a storage policye CAWS (Consistency AWare Sampling), that leverages a learning consistency score (C-Score) to enable effective and efficient learning. CAWS adds diversity to the memory while allowing the model to learn a far more detailed decision boundary. Practical proxies that require no extra training can still achieve similar results according to some experimental results.",
            "strength_and_weaknesses": "Pros: \nThe paper focused on a very interesting scenario in continual learning. The problem setting of continual learning with efficient learning is well motivated. It is reasonable to apply C-score in evaluating the learnablity of a sample and this method could be extended to other areas including Curriculum Learning.\n\nCons:\n1. The structure of the paper is not clear. The authors proposed to use C-Score to measure how learnable a specific sample can be concerning a set of models, and C-Score of some datasets have already be calculated. They mentioned they needed a proxy to evaluate the score for the other datasets, but the definition of the proxy appear after several sections. The writing order will put extra reading burden for the readers. Also the original paper appears in Section 4.1, while the C-Score definition appears in Section 2.\n\n2. For the datasets we already have the C-Score computed by Jiang 2021, does this C-Score rely on the model architecture? If we switch to a new architecture, does that mean this C-Score does not work any more? \n\n3. There are a few proxies proposed by Jiang 2021, what is the difference between the proxy in this paper and the proxies in Jiang 2021?\n\n4. How to determine the threshold of high C-score and low C-score? How to determine the X in CAWS? \n\n5. The results in Figure 4 a and Figure 4 b are leading to two different directions when we increase the threshold. Could you please explain why the 1000 memory capacities would increase with threshold = 0.9 in the CIFAR100 and all the other curves seens going downwards?",
            "clarity,_quality,_novelty_and_reproducibility": "Although the problem setting is new, my main concern is the limited novelty of the proposed CAWS method.The proposed CAWS method, as illustrated in Section 3, simply sampling randomly from the top C-Score samples.\n\n- The paper is not easy to follow because they split the method and the results into multiple pieces, and some references and definitions are not set close to their first appearance.\n- How the dataset is splited into 5 tasks is not explained explicitly. \n- Why the accuracy would decrease with more training epochs in CIFAR-10-100 in Figure 3\uff1f",
            "summary_of_the_review": "Overall, even though the problem is important, I find the structure of the paper needs further modifications. The method CAWS they proposed seems like just sampling from the top C-Score samples under the continual learning framework, I think the authors could put more emphasize on the novelty of this method and how much of the memory size is saved or the performance boost in using CAWS sampling. \nOn top of this, there are a few technical aspects (inconsistent results, hyperparameter thresholds etc) I do not completely agree with, therefore, would request authors to answer above questions for clarity.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics corcerns.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2947/Reviewer_gaRy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2947/Reviewer_gaRy"
        ]
    },
    {
        "id": "yqHmD577aM",
        "original": null,
        "number": 3,
        "cdate": 1666685238469,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685238469,
        "tmdate": 1666685238469,
        "tddate": null,
        "forum": "AFhaaOZTkKA",
        "replyto": "AFhaaOZTkKA",
        "invitation": "ICLR.cc/2023/Conference/Paper2947/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents an idea to select important examples to be buffered in continual learning. The proposed Consistency Aware Sampling (CAWS) considers examples that are easy to learn and representative of previous tasks. Empirical investigation has been conducted to evaluate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "The main strength of the paper is the topic itself, selective buffering in continual learning.\n\nHowever, the weaknesses of the paper overweigh the strength by a large margin:\n- The organization of the paper is scattered and hard to follow. Interleaving methods and experiments are not clear. Moreover, the writing style makes it hard to distinguish the motivation and original parts of the paper.\n- Saving the \"easy to learn\" examples in the buffer is counter-intuitive. An easy-to-learn example should already been learned by the model in early iterations, why bother saving it in the replay buffer instead of giving the model more exposure to hard examples? \n- Lack of latest competing methods. As far as I am concerned, there has been recent work that studies the same topic as this paper, including but not limited to GSS [1], LARS[2] and DDR [3]. The authors should definitely compare the proposed method against these recent methods.\n\n\n[1] Aljundi, Rahaf, et al. \"Gradient based sample selection for online continual learning.\"NeurIPS 2019.\n\n[2] Buzzega, Pietro, et al. \"Rethinking experience replay: a bag of tricks for continual learning.\" ICPR 2021.\n\n[3] Wang, Zifeng, et al. \"SparCL: Sparse Continual Learning on the Edge.\" NeurIPS 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall clarity and quality of the paper do not meet the ICLR standards.\nThe novelty of the paper is quite limited.\nThe reproducibility is not ensured since the codebase will be public after acceptance.",
            "summary_of_the_review": "Although the topic of selective buffering is interesting, the proposed method is not well justified neither intuitively or empirically. Therefore, I recommend rejection of the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2947/Reviewer_1H7t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2947/Reviewer_1H7t"
        ]
    },
    {
        "id": "IofZ7ljXGv",
        "original": null,
        "number": 4,
        "cdate": 1667412410744,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667412410744,
        "tmdate": 1667412410744,
        "tddate": null,
        "forum": "AFhaaOZTkKA",
        "replyto": "AFhaaOZTkKA",
        "invitation": "ICLR.cc/2023/Conference/Paper2947/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to use C-scores as a score to select representative samples to insert into a memory buffer in continual learning. More concretely the authors:\n- describe current limitations of memory selection methods\n- propose to use C-score, an expensive but accurate score capable to identify learnable samples, to select a memory that is highly representative of the class and can be efficiently learned\n- identify two limitation in a naive use of C-scores and try to alleviate them:\n    - C-scores are expensive to compute, and the authors propose more efficient proxies to approximate them\n    - selecting a memory greedily w.r.t. C-scores can result in lack of diversity, so the author propose to select uniformly at random (promotes diversity) but only out of samples that achieve a C-score above a certain threshold (promotes relevancy) \n\nThe proposed method is evaluated using several ablations and baselines, but only within the context of other memory-based CL methods.",
            "strength_and_weaknesses": "The strong points of the method are its conceptual simplicity (C-scores are an intuitive metric of relevance) and good performance compared to other memory-based baselines. The motivation section is especially insightful and helps to highlight how the memory selection problem is still very open and hard in general. The algorithm exposition is clear and its most relevant hyperparameter are ablated. The experimental section covers multiple datasets.\n\nHowever the work also has some clear weaknesses\n- One of the main contributions is the derivation of new proxies for C-scores, but the approximation accuracy (e.g. proxy C-score/actual C-score) of these proxies w.r.t. the real score or Jiang et al.'s approximation. The only metric reported is downstream accuracy of the classification task, but without checking the quality of the C-score approximation it is possible that the proxies are injecting additional biases that could actually be helpful for these specific tasks.\n- The memory budgets considered in the experimental section are quite small (i.e. ~100-1000), and much smaller than what off-the-shelf hardware is capable of handling currently (i.e. 10k and up). It would be interesting to see how the various proxies and the baseline behave with a larger memory, and if the gain are preserved.\n- For the real algorithm (i.e. the one using C-score proxies) only accuracy is reported, while usually the more relevant metrics in CL are forgetting or some sort of transfer.\n- None of the experiments are reporting error bars or uncertainty, and it is not clear if the experiments with proxies have also been run with 3 seeds.\n\nMinor comments:\n- The baselines considered are only memory-based. While this is acceptable (CL is a broad field) it is a bit of a limitation of the comparison.\n- The paper seems to only consider the setting where sample are explicitely labeled with their tasks during training. This is acceptable but it should be stated that it is a special case of the more general CL problem without explicit tasks or task boundaries.\n- Accuracy and forgetting are never defined as metrics in the paper.\n- In A.2 CAWS should include points with C(x) >= delta",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite clear and the results are interesting. The novelty is limited since C-score were already applied in CL, and the proxies are relatively straightforward. The authors included code for their experiments so reproducibility should not be an issue.",
            "summary_of_the_review": "Overall the paper presents an interesting approach and is overall acceptable, but the empirical evaluation undermine the message that C-scores are an interesting metric in CL. In particular\n- the quality of the proposed proxies are never truly investigated\n- it is not clear which computational regime the authors are targeting for their approach, and they should justify their choiches for memory size, number of epochs, and size of datasets in each epoch in terms of memory and compute, providing back-of-the-envelope estimations supporting their choice (e.g. if this approach should target compute on a phone, a consumer desktop, an industry server, or a whole cluster).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2947/Reviewer_zPPV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2947/Reviewer_zPPV"
        ]
    }
]