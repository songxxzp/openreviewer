[
    {
        "id": "PuUVFIQ6ZK",
        "original": null,
        "number": 1,
        "cdate": 1666588870344,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588870344,
        "tmdate": 1666619404154,
        "tddate": null,
        "forum": "NhR0jUSuelq",
        "replyto": "NhR0jUSuelq",
        "invitation": "ICLR.cc/2023/Conference/Paper2471/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper combines sharpness-aware minimization (SAM)  with decentralized SGD. It established the convergence rate and demonstrated that it could improve the generalization performance. \n\nHowever, the novelty is incremental, some assumptions are too strong, it missed some important literature. ",
            "strength_and_weaknesses": "Pros:\n1. The proposed algorithm could improve the generalization performance of decentralized federated SGD. \n2. This paper provided the theoretical convergence rate. \n\nCons:\n1. The novelty is incremental. It is not surprising that this combination could improve the generalization performance. In addition, it is not difficult to combine existing theoretical analysis to establish the convergence rate of this algorithm. \n2. This paper assumes that the gradient is bounded. It is too strong. FedAvg and FedSAM do not need this assumption.  With this assumption, the theoretical analysis becomes much easier. \n3. The first decentralized FedAvg is https://arxiv.org/abs/1910.09126 . However, the authors totally ignore this important literature. This is NOT acceptable!\n4. Why does the MGS version even outperform FedSAM for CIFAR10? This should be discussed clearly. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good.\n\nQuality: neutral\n\nNovelty: incremental\n\nReproducibility: No source code. It is unclear if it is reproducible. ",
            "summary_of_the_review": "The novelty is incremental.  Some assumptions are too strong. It missed some important literature. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "This paper ignored some important literature. In decentralized federated learning, it is unbelievable that the authors do not know this seminal work: https://arxiv.org/abs/1910.09126 \n",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2471/Reviewer_Yu1u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2471/Reviewer_Yu1u"
        ]
    },
    {
        "id": "OpJLFsvGn1n",
        "original": null,
        "number": 2,
        "cdate": 1666591084899,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591084899,
        "tmdate": 1666591084899,
        "tddate": null,
        "forum": "NhR0jUSuelq",
        "replyto": "NhR0jUSuelq",
        "invitation": "ICLR.cc/2023/Conference/Paper2471/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Sharpness-aware minimization (SAM) has recently been shown to improve various aspects of deep learning. In this paper, the authors show empirically that SAM also helps to improve the performance in decentralized federated learning.\n\nA main contribution of this paper is a theoretical analysis of their federated learning algorithms. However, this theory appears disconnected from the rest of the paper and it is unclear why it matters in practice (see weaknesses). ",
            "strength_and_weaknesses": "The main weakness of the paper is the theory presented in Section 4. A lot of work is spent to derive a complicated and intimidating bound, and I am not convinced that this bound is important or matters in practice. For example, it is unclear if it matches any situations observed in practice. \n\nTo improve the paper, some experiments could be added which demonstrate that this theory is correct or useful, as it is near impossible to confirm the math due to its complexity and presentation.  Just notice that for the considered deep learning applications, Assumption 1 and Assumption 3 may not hold.  Perhaps the authors could provide some \"sanity checks\" and show what happens on a simple federated linear regression problem. Does the theory work in that setting? Do the bounds / analysis simplify? \n\nIf the theory Section 4 could explain some aspects of why SAM improves decentralized federated learning or give some guidance on the choice of parameters I would be convinced. But as of now, I don't see the point of including it. \n\nMinor typos (no influence on my rating):\n- decebtralized -> decentralized\n- Eq.2, SAM uses norm ||.||, not the squared norm. ||.||^2 is not a norm, so this sentence is technically wrong. Anyway, why write it with squared norm if simplified later?\n- Commincation -> Communication\n- typology -> topology\n- desira ble -> desirable\n- How are the weights W chosen? Perhaps a sentence can be added in the paragraph after Eq. (1) on how W is picked in practice.\n- Section 3.2 is not very clearly written. In particular, does the neighbourhood N include the node itself, i.e., does Eq. (4) include also the weights of node i? In Eq. 5 why does it suddenly switch to \"q\" from \"k\"? The difference between Q and K needs to be explained, I suspect that the averaging happens every few inner iterations here? However, after reading Algorithm 1 in the appendix, these things became clear.\n- Figure 5 (c) is very hard to read, especially for color-blind people. Perhaps the best result can be highlighted with a different line style or bold? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/Quality:  see strengths/weaknesses. \n\nNovelty: The paper seems to be the first one to apply SAM to decentralized federated learning. However, the application is straightforward and applying an existing algorithm to an existing problem domain is of rather limited novelty. \n\nReproducibility: the results seem possible to reproduce if the code is provided. ",
            "summary_of_the_review": "The paper provides an interesting and novel application of SAM in decentralized federated learning. However, the theory presented in the paper seems a bit disconnected from the practice. Moreover, the application of an existing method to an existing problem lacks novelty. Therefore, I cannot recommend acceptance at this stage. To improve the paper, I encourage the authors to either remove the theory (as it is not illuminating) and provide additional larger experiments with the gained space. An alternative possibility is to provide some experiments which showcase the theory (e.g. on linear regression problems). \n ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2471/Reviewer_YEku"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2471/Reviewer_YEku"
        ]
    },
    {
        "id": "VvDgPYQheTt",
        "original": null,
        "number": 3,
        "cdate": 1666667030380,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667030380,
        "tmdate": 1666667073599,
        "tddate": null,
        "forum": "NhR0jUSuelq",
        "replyto": "NhR0jUSuelq",
        "invitation": "ICLR.cc/2023/Conference/Paper2471/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to apply SAM in the decentralized learning scenario to alleviate the distribution shift, termed DFedSAM. Convergence results are provided for smooth non-convex objectives under a bounded gradient assumption. Numerical experiments are conducted on several datasets.",
            "strength_and_weaknesses": "Strengths:\n- The ablation study provides a detailed discussion on multiple cases in the experimental setups.\n- The convergence results are provided.\n- The paper is generally well written and easy to follow.\n\nWeaknesses:\n- My main concern is that this paper can be viewed as the extension of Generalized Federated Learning via Sharpness Aware Minimization.  The authors changed the federated learning into a decentralized learning and added multiple gossip from another paper. \n- How does the algorithm scale on the client size? Could you please provide a figure on small and large number of clients to see whether the relationship between training performance and client size is linear or not.\n\nTypo: commincation found in figures --> communication round ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is a very good balance between motivations, background, a clear explanation of the method with all the hard maths in appendix. \nThe idea is easy to understand and should lead to improvement in decentralized learning.\nQuestion:\n1. Figure 2: Are these two figures based on read dataset?\n",
            "summary_of_the_review": "While the paper is well-written and the proposed approach is clearly explained. Experiment results are promising and convincing. My main concern is that the idea is a natural entension of a SAM built in federated learning. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2471/Reviewer_LjuW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2471/Reviewer_LjuW"
        ]
    },
    {
        "id": "-D1O9piyanM",
        "original": null,
        "number": 4,
        "cdate": 1666894151380,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666894151380,
        "tmdate": 1666894151380,
        "tddate": null,
        "forum": "NhR0jUSuelq",
        "replyto": "NhR0jUSuelq",
        "invitation": "ICLR.cc/2023/Conference/Paper2471/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new algorithm DFedSAM within the decentralized federated learning framework, where communications between servers are only performed within local neighborhood. To tackle the sharper landscape generated by the decentralization, the algorithm adapts a sharpness aware minimization strategy (SAM) by adding perturbation on the iterate before gradient evaluation. A multi-gossip step variant is also provided to improve model consistency. Extensive experiments are included to show the effectiveness of the method. ",
            "strength_and_weaknesses": "Strength \n- Theoretical soundness \n- Extensive experiments \n\nWeakness\n- limited novelty ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly easy to read through, the novelty is limited as the algorithm is an application of existing method SAM under the decentralized federated learning setting. ",
            "summary_of_the_review": "The paper provides solid theoretical analysis on the proposed algorithm. The part that is a bit counter intuitive to me is on the local maximization step in (2) helping the sharpness awareness. In particular, I would like to see a  comparison between the current approach and the one replacing the normalized gradient perturbation with a random direction. \n\nIn the experiment, the x-axis are showing in the communication round. However the current algorithm requires two gradient evaluation per local iteration instead of one in the standard FedAvg type method, is that taking into account in the plots? \n  \nMoreover, the selection on the parameter Q seems very adhoc in the current version, will it be possible to provide some theoretical guidance on how to select it, for example something similar to [1]\n\n[1] Scaman et al 2017, Optimal algorithms for smooth and strongly convex distributed optimization in networks",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2471/Reviewer_YESX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2471/Reviewer_YESX"
        ]
    }
]