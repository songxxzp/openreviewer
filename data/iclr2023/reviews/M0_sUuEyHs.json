[
    {
        "id": "igOa_1sPgJ",
        "original": null,
        "number": 1,
        "cdate": 1666317214943,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666317214943,
        "tmdate": 1666317214943,
        "tddate": null,
        "forum": "M0_sUuEyHs",
        "replyto": "M0_sUuEyHs",
        "invitation": "ICLR.cc/2023/Conference/Paper3927/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to use dynamic prior knowledge (DPK) that uses the teacher\u2019s feature as inputs for the feature distillation by mixing the student features and using an encoder-decoder framework that learns the teacher\u2019s feature as the target. Using DPK has an advantage in that the performance of the student network has a positive correlation with that of the teacher.\nThe performance gain in image classification datasets (CIFAR-100, ImageNet) and object detection (COCO) is remarkable, achieving SOTA performance.\n",
            "strength_and_weaknesses": "Strength\n- To the author\u2019s best of knowledge, this work is the first work to take the features of teachers as \u2018input\u2019, not just \u2018target\u2019 in knowledge distillation. If this is true, this paper can take credit for novelty. Also, this work showed the \u2018large models are not always better teachers\u2019 issue and tried to solve the issue with the proposed idea. This work did a good job of analyzing the introduced problem with an experiment. The proposed method shows the SOTA performance in various benchmarks regarding the classification and the object detection task. \n\nWeakness\n- It seems that the transformation module is too heavy, for example, it uses the ViT encoder and decoder module as the feature transformation module which requires heavy computation and process. Also, its dynamic mechanism process is a little hard to follow. \nIn table 8, the Encoder-Decoder shows only marginal performance gain compared to MLP-Decoder, considering the heavy computation required by the ViT encoder.\n\n- Since the encoder-decoder is ViT, it would have been more informative to address the training computational cost, even though it does not affect the test phase.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and shows justifying ablation study results that support the proposed idea of this work. Also, it shows SOTA performance compared to existing methods. Its overall clarity, quality, and novelty are good enough, and it provides detailed information to reproduce the reported results. \n",
            "summary_of_the_review": "I recommend this paper to be accepted since it tackles a very important issue in knowledge distillation and solved the issue with a novel idea. It has done extensive experiments to support the proposed idea and the results.  However, its performance gain shown in the ablation study is somewhat marginal.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3927/Reviewer_aN37"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3927/Reviewer_aN37"
        ]
    },
    {
        "id": "qb6tCyfKdcd",
        "original": null,
        "number": 2,
        "cdate": 1666529607658,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529607658,
        "tmdate": 1666530484905,
        "tddate": null,
        "forum": "M0_sUuEyHs",
        "replyto": "M0_sUuEyHs",
        "invitation": "ICLR.cc/2023/Conference/Paper3927/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes the prior knowledge mechanism for feature distillation, which can fully excavate the distillation potential of big models. Furthermore, the authors propose the dynamic prior knowledge (DPK) to solve the \u2018larger models are not always better teachers\u2019 issue, which makes the performance of the student model positively correlated with that of the teacher model.\nFinally, experiments are conducted to verify the effectiveness of the proposed algorithm.\n",
            "strength_and_weaknesses": "Main Review:\n\nStrength: \n\n1)This paper presents a dynamic prior knowledge mechanism for feature distillation to provide a solution to the \u2018larger models are not always better teachers\u2019 issue. The proposed method is simple and technically sound.  \n\n2)The experimental results of the proposed method outperform that of the other comparable distillation knowledge models.\n\n3)This paper is well-written and easy to follow.\n\nWeaknesses:\n\n1)The technical novelty of the proposed method is somewhat limited, and the authors did not provide theoretical analysis to support the proposed algorithm. This article lacks a strong reason to explain why the proposed prior knowledge mechanism can perform well. What is the relationship between the proposed prior knowledge mechanism and the feature gap?\n\n2)There is no detailed description of the progress of knowledge distillation research in the introduction. Are there any other articles that attempt to address the \u2018larger models are not always better teachers\u2019 issue? I think this article lacks the most relevant baseline, e.g. [1].\n\n3)Is the prior knowledge mechanism that incorporates the teacher's feature into the student model fair in comparison to other baselines?\n\n4)This paper lacks complexity analysis, which is important.\n\n[1] Beyer L, Zhai X, Royer A, et al. Knowledge distillation: A good teacher is patient and consistent[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 10925-10934.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper presents a dynamic prior knowledge mechanism for feature distillation to provide a solution to the \u2018larger models are not always better teachers\u2019 issue. The proposed method is simple and technically sound. However, the motivation behind \"why prior knowledge can relieve the feature gap\" is unclear.",
            "summary_of_the_review": "It lacks evidence and reasons to support its assumption on prior knowledge, and also this assumption may be unfair to the general teaching. Besides, the authors avoided some related work from machine teaching. I don't know why.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3927/Reviewer_cqrC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3927/Reviewer_cqrC"
        ]
    },
    {
        "id": "lC3gjj4lW1a",
        "original": null,
        "number": 3,
        "cdate": 1666578504973,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578504973,
        "tmdate": 1670479092235,
        "tddate": null,
        "forum": "M0_sUuEyHs",
        "replyto": "M0_sUuEyHs",
        "invitation": "ICLR.cc/2023/Conference/Paper3927/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new method on feature distillation. They utilize a transformer-like module for feature alignment between each stage of the teacher and student networks. Considering the gaps between teacher and student network, they propose the mask ratios to dynamically guide the feature distillation training. The experiments obtain good performance on image classification (CIFAR-10 and ImageNet) and object detection (COCO). ",
            "strength_and_weaknesses": "Strength:\n\t\n1. The authors utilize a transformer-like module to integrate the student and teacher features for feature alignment.\n2. Considering the capacity gaps between teacher and student networks, the authors propose a dynamic mechanism for training.\n3. The authors do lots of experiments and the proposed method obtain good performance on image classification and objection detection tasks.\n4. The paper is well-organized and easy to follow. \n\t\nWeakness:\n\t\n1. The technical novelty is marginally novel for the community. Although lots of ablation studies and comparisons demonstrate the good performance of the proposed method, this paper is an incremental work. The authors introduce widely used transformer module and mask ratio for feature distillations.  \n2. The authors should give more analysis of the Receptive Field (RF) for feature distillation. For object detection, how to facilitate the different RF between teacher and student features is important for feature distillation. \n3. In Appendix 4, I notice that CRD outperforms the proposed DPK on four settings. Is the performance also influenced by the RF?\n4. I am also curious about the values of dynamic mask ratios for each stage in training. According to the difference between receptive filed, the mask ratio for the shallower stage should be larger than the deeper stage. \n5. The similar work [1] should be discussed and compared in the paper. In [1], they use nonlocal module, which is similar to transformer, for feature distillation of object detection. Although the teacher backbone is a little different, they obtain better performance (e.g. Faster RCNN R50(S), 41.5[1] vs 40.6). \n\n\n[1] Zhang, Linfeng, and Kaisheng Ma. \"Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors.\" In International Conference on Learning Representations. 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "See strength and weakness.",
            "summary_of_the_review": "See strength and weakness. \n\nThis is an incremental work for community. The authors introduce some existing techniques into feature distillation. They do lots of exmperiments to demonstrate the superiorities of the proposed DPK method, but lack detailed analyses of the essential problems for feature distillation. \n\nI am glad to improve the final rating if the authors address my concerns or point out that I misunderstand  some parts of this work. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3927/Reviewer_ERUi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3927/Reviewer_ERUi"
        ]
    }
]