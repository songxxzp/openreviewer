[
    {
        "id": "6wxitzCa64",
        "original": null,
        "number": 1,
        "cdate": 1666272240508,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666272240508,
        "tmdate": 1666272240508,
        "tddate": null,
        "forum": "HTJE5Krui0g",
        "replyto": "HTJE5Krui0g",
        "invitation": "ICLR.cc/2023/Conference/Paper6551/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper addresses the general family of tasks when the input needs to be processed by a neural model, then processed by a symbolic model, and produce an output. \nFor example, the input may be an *image of* a handwritten arithmetic calculation, e.g. `7*8-5-9`. The neural model needs to recognize the digits and the operators, and is then allowed to use a symbolic model such as a standard simple calculator to produce the result (`42`).\nThese tasks are very difficult when we only have labels for the final answer, i.e. `42`, and no intermediate labels for the digits themselves.\n\nThe paper proposed a \"soft\" way to solve this problem by introducing a probability over the input, and a way to solve and optimize.",
            "strength_and_weaknesses": "## Strengths\n* The problem is interesting and important, as providing good solutions will allow neural models to leverage existing and exact tools (e.g., a calculator) instead of learning to calculate themselves, since a calculation by the neural network is expected to make mistakes, and is known to not generalize well.\n* The proposed technique sounds correct and clever, although I did not understand most of the details.\n* The approach is evaluated on three tasks and shows significant improvements over the baselines.\n\n## Weaknesses\n* Readability - I did not understand most of the technical details, and the paper is not very friendly for readers out of this sub-field.\nI did understand the general idea, but many mathematical steps are mentioned only briefly or cite a reference, which makes the reader feel that they are expected to read many references before being able to understand the paper.\nI do believe that the paper could have been written in a way that will make it more readable to the broad ICLR audience.\nSee details in the next section.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As I wrote in the previous section, the paper is unreadable for readers outside of this subfield, and I believe that this complication is not needed, and could have been written in a more friendly way.\nFor example:\n\n* The abstract was completely unreadable for me. Although I understood every individual word, when reading the abstract, I could not understand anything from the contributions:\n```\n(1) modeling of deterministic symbol solution states as a Boltzmann distribution, which avoids expensive state searching and facilitates the interaction between network training and symbolic reasoning;\n(2) an efficient\nMCMC sampling technique leveraging projection and SMT solvers, which overcomes the connectivity barrier in sampling symbol solution spaces; \n(3) an annealing mechanism that avoids the trap of sub-optimal symbol groundings\n```\n(what are `deterministic symbol solution states`? What is the `connectivity barrier`? what is `an annealing mechanism`? What is `the trap of sub-optimal symbol groundings`?)\n\n* The introduction continues to be completely unreadable, for example:\n```\nFrom the perspective of game theory, the softening forms a series of mixed strategy games during the annealing process, which encourages  stronger interactions and thus obtains a strict improvement of the utility (Conitzer, 2016).\n\nFurthermore, the softening approach allows us to solve the original bi-level optimization by the (min-oracle) stochastic gradient descent (Jin et al., 2020). To avoid the sparsity problem and obtain the gradient estimate efficiently\n```\nand so on. (how is this related to \"mixed strategy games\", and what are \"mixed strategy games\"? What is this \"softening approach\"? what is exactly the \"bi-level optimization\" problem? What is \"min-oracle\"?)\n\n* Later in Section 2:\n```\nAdvantages. Game theory provides a perspective to understand why our softened strategy improves over problems (1) and (2) with a better interaction between model training and symbolic reasoning. Either problem (1) or (2) can be viewed as a Stackelberg game with pure strategies, in which the player takes a single action (i.e., selecting a deterministic mapping h). However, in problem (P), the player takes a randomized action with the distribution Q\u03d5. Such a mixed strategy does provide more information of the game, and thus strictly improves the utility (Letchford et al., 2014).\n```\nWhat is a \"Stackelberg game\", and how does it \"improve the utility\" (and what is exactly the utility here)? Such paragraphs just makes it frustrating for the reader.\n\n* `By applying the Metropolis algorithm` ... `creating the so-called connectivity barrier.` What is the Metropolis algorithm? What is the so-called connectivity barrier? Readers that are not familiar with this algorithm and barrier cannot follow the paper when such terms are mentioned.\n\n## Additional comments\n* I could not estimate whether the baselines are sufficient and whether the approach is novel.\n* Presentation - the authors have modified the `hyperref` package settings in a way that does not highlight the hyperlinks and makes it difficult to read. Why? Please fix that.",
            "summary_of_the_review": "I appreciate the difficulty of the problem that this paper is trying to solve, and the empirical results look promising.\nHowever, I could not understand most of the paper,  which sometimes seems to be written cryptically deliberately.\n\nI thus vote for rejection with low confidence, because even if the paper's solution is sound and novel, the paper will be valuable only to a very limited audience.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6551/Reviewer_ypT3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6551/Reviewer_ypT3"
        ]
    },
    {
        "id": "JpLP9x_ahU",
        "original": null,
        "number": 2,
        "cdate": 1666409130663,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666409130663,
        "tmdate": 1668984081113,
        "tddate": null,
        "forum": "HTJE5Krui0g",
        "replyto": "HTJE5Krui0g",
        "invitation": "ICLR.cc/2023/Conference/Paper6551/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a neurosymbolic learning approach for tackling the problem of symbol grounding when the logic of symbolic constraints is provided.Their method uses an SMT solver to initialize one possible symbol grounding for a problem such that it satisfies a given \u201csolution constraint\u201d, and then uses a Boltzmann machine to enable sampling from a distribution over potential symbol groundings that satisfy a given solution constraint (such as a visusal equation taking on a particular value). The method is evaluated on three tasks: visual math, visual sudoku, and visual shortest path finding. The method is compared to baselines from reinforcement learning and symbolic-parser-based approaches. Across the tasks, they find that their method significantly outperforms the baselines.",
            "strength_and_weaknesses": "Strengths:\n* The paper presents a reasonably straightforward but well motivated idea for allowing the perceptual and symbolic reasoning aspects of a neuro-symbolic method to interact. While I am not an expert in this kind of neurosymbolic reasoning, the method was well motivated by gaps in related work as pointed to.\n\n* The method is at least partially theoretically justified, including the use of stochastic gradient descent as a method for removing some of the sampling bias introduced by the paper\u2019s MCMC procedure.\n\n* Neurosymbolic models have seen increasing attention over the last several years, and given this paper\u2019s strong results, it seems well posed to have an impact in the community.\n\nWeaknesses:\n* Baselines seem especially weak and oddly chosen given the extensively cited related work that is more recent. For example, Li et al, 2020, which seems like one of the most recent and related pieces of work, is only compared on the first of the three tasks, without explanation for why it is not used for the other two. \n* The reasoning for the variants of the chosen tasks was confusing and seemed misleading. For example, the paper states \u201cSince the original dataset (Li et al, 2020) consists of formulas with lengths varying from 1 to 7, which may lead to the label leakage problem, we only take the 6K/1.2K formulas with length 7 as the training/test set.\u201d What does this mean? What would happen if you compared on all formulas? This seems particularly important because the performance of Li et al in this paper is *90 absolute percentage points* worse than the original paper. To ensure there isn\u2019t a bug with the implementation, or the interpretation of the experimental results, I would like to see a comparison to their method on the full dataset as opposed to just the length 7 formulas. I also do not understand how including the whole dataset would lead to a label leakage problem.\n* The paper has several issues with clarity that would need to be improved before it could be published. \n  * For example, the authors use \u201cStage I\u201d and \u201cStage II\u201d to describe different aspects of their method throughout the results, but Stage I and Stage II are never defined in the text, so I was not able to follow what this was supposed to mean. \nIn proposition 2, the paper states that the method is a stict generalization of prior work which is the special case of gamma = 1. I believe this should be gamma = 0 (the deterministic case).\n  * Similarly there is jargon in several places that general machine learning readers may be unfamiliar with, for example \u201cEither problem (1) or (2) can be viewed as a Stackelberg game with pure strategies, in which the player takes a single action (i.e., selecting a deterministic mapping h)\u201d. What is a Stackelberg game? Why is this important?\n  * I did not understand the point about how after the annealing stage the problem becomes a semi-supervised one. Perhaps this could be spelled out a bit more clearly with some examples of what the semi-supervised problem looks like (what is the data, what parts of it are unlabeled, what are the labels).\n* The paper is missing key ablations to support the claims. \n   * In particular, a main claim is that using an annealed gamma for first having stochastic representations of symbols and then sharpening them over time is not directly ablated. It would be nice to see a comparison to the method when gamma is always fixed to be 0 from the start of training, as this is the key difference to prior work (as also noted by the paper\u2019s proposition 2 remarks). \n   * A secondary claim relates to the projection operator, and its importance for enabling well-behaved MCMC proposals. The projection operator is chosen in a bespoke way for each problem. How sensitive is the method to the particular choice of projection operator? What happens if no projection operator is used? Given that this choice injects a significant amount of domain knowledge from a practitioner and does not appear to generalize across problems, this seems like an important experiment to run.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see strengths and weaknesses above.",
            "summary_of_the_review": "Despite the novel and interesting method introduced in this paper, there are major weaknesses in the clarity and quality of the paper, specifically as it relates to missing ablations, sensitivity analyses, and justification for task modifications. If the concerns can be addressed by the authors, and the paper\u2019s clarity improved, I am willing to increase my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6551/Reviewer_78tc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6551/Reviewer_78tc"
        ]
    },
    {
        "id": "THLtfC4YSD0",
        "original": null,
        "number": 3,
        "cdate": 1666511736908,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666511736908,
        "tmdate": 1666511993962,
        "tddate": null,
        "forum": "HTJE5Krui0g",
        "replyto": "HTJE5Krui0g",
        "invitation": "ICLR.cc/2023/Conference/Paper6551/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes softening the symbol grounding problem in joint neural-symbolic learning. The major challenge in the neural-symbolic learning system studied in this work is the lack of supervision for the symbol grounding problem. This work presents efforts to model the symbol grounding problem as a Boltzmann distribution with efficient MCMC sampling to bridge the worlds of neural grounding and symbolic reasoning. Of particular interest is the sampling method the authors use to resolves the connectivity barrier in searching for feasible symbol representation. In experiments involving handwritten formulae, visual sudoku, and shortest path, the method shows improved performance compared to existing baselines.",
            "strength_and_weaknesses": "As the authors mention, the key problem in the neural-symbolic systems is how to efficiently search for the feasible hidden symbol state in the exponentially large and discrete space. The authors propose a very interesting formulation to address the problem, prove the convergence and even connect with existing works. The entropy formulation that softens symbol grounding is really interesting and I quite like the accompanied sampling method proposed for the connectivity barrier. Both empirical results and the theoretical analysis are convincing enough.\n\nI do not have many technical questions but would rather like to ask some open ones.\n\nTo me, the scope of problems that this method can be applied is limited, a minor problem though. The problems studied in this work are only challenging without hidden symbol supervision. However, in practice, for the toy problems studied, it is not hard to obtain, and may take only a few hours to get enough data to train a decent symbol grounding network. Is it possible to show your method applied to problems where data is either too complex to label for humans while your method can obtain good performance, or it simply is too bordering to get labels for intermediate representation?\n\nAnother issue with the method is efficiency. I can see that the sampling process, though improved, could still be very slow (or correct me if not). SMT solvers could also be a major bottleneck for efficiency, as they might only be feasible for moderately small problem scales. Can you show some quantitative results regarding efficiency? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, of high quality and original to the best of my knowledge.",
            "summary_of_the_review": "The paper is well done and the strengths far outweigh the weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6551/Reviewer_qkDm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6551/Reviewer_qkDm"
        ]
    },
    {
        "id": "XAILYgzj8Ik",
        "original": null,
        "number": 4,
        "cdate": 1667534411435,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667534411435,
        "tmdate": 1667534411435,
        "tddate": null,
        "forum": "HTJE5Krui0g",
        "replyto": "HTJE5Krui0g",
        "invitation": "ICLR.cc/2023/Conference/Paper6551/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a neuro-symbolic learning framework with an explicit design for addressing the symbol grounding problem.  The key idea is softening symbol grounding by using a Boltzmann distribution to represent the entire symbol space, rather than a specific symbol grounding. Such a design achieves an efficient interaction between neural perception and symbolic reasoning. Then, a novel MCMC sampling method combined with SMT solving is proposed to facilitate learning the correct symbol grounding. Furthermore, an annealing mechanism with three different realizations is applied to avoid sub-optimal symbol groundings. Experimental evaluations show significant improvement over several state-of-the-art approaches. \n",
            "strength_and_weaknesses": "Strengths:\n- the paper is very well-written; all necessary background  (either classic or recent work) is carefully introduced, and related work is organized in a systematic and insightful manner.\n- the paper addresses a fundamental problem in AI from a novel and promising angle -- view symbol grounding distribution as a mixed strategy in game playing; the combination of the MCMC sampling with SMT solving is both novel and elegant\n- the paper has extensive experimental evaluations and achieves superior performance over many state-of-the-art approaches\n- the paper also presents a formal analysis of the convergence guarantee (with some mild assumptions)\n\nWeaknesses:\n- the projection and inverse operations may be quite sensitive and require some non-trivial domain expertise\n- the visual Sudoku classification task seems to be a simplified version of the original one. The 4-by-4 Sudoku puzzle used in the evaluation is much simpler than a standard 9-by-9 Sudoku. SATNet should be the state-of-the-art for the Sudoku task, which is however not included\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Each individual technique used in this paper is standard, but the particular combination has great novelty and elegance, especially the interaction between MCMC sampling and SMT solving. The symbol grounding problem studied in this paper is fundamental, and the result \nachieved in this paper significantly outperforms several state-of-the-art approaches, which are highly non-trivial. Detailed implementations and evaluation datasets have been shared publicly, so the results should be easily reproducible. \n\n",
            "summary_of_the_review": "This paper makes significant progress toward addressing a fundamental problem in neuro-symbolic AI. The presented approach is novel, effective, and elegant, which is likely to have a great influence on future neuro-symbolic system design. \n ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6551/Reviewer_262g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6551/Reviewer_262g"
        ]
    }
]