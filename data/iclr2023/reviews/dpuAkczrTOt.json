[
    {
        "id": "dMWFI5uRwb",
        "original": null,
        "number": 1,
        "cdate": 1666620967923,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620967923,
        "tmdate": 1666780644381,
        "tddate": null,
        "forum": "dpuAkczrTOt",
        "replyto": "dpuAkczrTOt",
        "invitation": "ICLR.cc/2023/Conference/Paper6368/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The goal of the paper is to introduce a numerical procedure for extracting inductive biases, defined as ease of generalization,  for a differentiable trained recognition model. This procedure involves two layers of optimization: an inner learning model optimized on a training set of input-output pairs, and a meta-learner which is optimized so as to pick the training set that makes it easier for the inner learner to generalize to unseen data. ",
            "strength_and_weaknesses": "+ conceptually simple procedure, easy to apply to a range of models\n+ sequential introduction of multiple meta-learners, intriguing multi-learner scenarios for comparative across models analysis\n+ code provided for all demos\n\n- there is a little bit of circularity in that the set of functional hypotheses being considered itself comes from a functional class with its own inductive biases. \n- would have liked a little more details about the multi meta-learner scenario from the perspective of the inner loop learner",
            "clarity,_quality,_novelty_and_reproducibility": "- generally the writing is clear and easy to follow",
            "summary_of_the_review": "I like this a lot, found the approach clever and elegant. I imagine it would make a fun contributed talk/spotlight",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6368/Reviewer_TZjT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6368/Reviewer_TZjT"
        ]
    },
    {
        "id": "Q0RpN7Ne9c",
        "original": null,
        "number": 2,
        "cdate": 1666631927794,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631927794,
        "tmdate": 1670005988907,
        "tddate": null,
        "forum": "dpuAkczrTOt",
        "replyto": "dpuAkczrTOt",
        "invitation": "ICLR.cc/2023/Conference/Paper6368/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "## Updated Score. Changed from 3 to 5 \n\nThis paper proposes to address the critical question of understanding the inductive bias of machine learning models, i.e., what kind of function/problem models are most suited based on their design choices or other parameters like learning rules, etc. \n\nThe authors choose to design a model that can be used to characterize the inductive biases of various models systematically. The authors show that the meta-learner for given tasks and simple, relatively well-understood models can learn the inductive bias functions that the learning model itself has. They extend their study to more complex \"standard\" neural networks trained with backprop and spiking neural networks. \n\nAlthough the paper offers an interesting approach, one important objection is that, as opposed to the paper from Li et al. 2021, the meta-learners would themselves require a meta-learner to understand their own biases. \n\n",
            "strength_and_weaknesses": "The paper is well-written, and the breadth of models studied appears sufficient to touch a broad audience of ML and Neuroscience enthusiasts. The authors offer an alternative to the GP (Li et al.) and theoretical (Bordelon et al.) approaches. \n\n\nThere are many weaknesses with the current approach, which I believe makes it a bit premature for publication. \n\n1) The choice of the Sinkhorn distance seems somewhat arbitrary. How does the selection of the different objectives affect learning? \n2) The paper offers primarily qualitative comparisons (i.e., visual) rather than quantitative ones. Are there ways to properly understand the quality of the approximation of the implicit by different types of meta-learners? \n3) How does the meta-learner's choice affect the implicit bias (given the meta-learners have their own inductive biases)?\n4) What type of novel, non-trivial, insights can we obtain from the current paper? Various future directions and discussions are nicely laid out. However, as of now without them, this paper does not offer enough to be even a proof of concept. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly explained, but some of the claims of the usefulness of the approach are lacking as mostly known results are recovered, and little to no predictions are made that would have to be validated. The authors rightfully point out that some tools are lacking to properly study their model. ",
            "summary_of_the_review": "The paper addresses an interesting question for both the ML and Neuroscience community. Given an observed model, how can we determine its inductive bias when its theoretical study is hard or impossible? \n\nThe paper, nonetheless, does not address critical questions and make enough predictions to make this paper even a proof of concept. It appears novel because it uses neural networks instead of GPs as meta-learners. Still, the resulting interpretability is lost because the meta-learners inductive bias is hard to understand. \n\nI look forward to hearing from the authors and clarifying points I might have missed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6368/Reviewer_wT2V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6368/Reviewer_wT2V"
        ]
    },
    {
        "id": "gwZGyWq82GZ",
        "original": null,
        "number": 3,
        "cdate": 1666679475140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679475140,
        "tmdate": 1666679475140,
        "tddate": null,
        "forum": "dpuAkczrTOt",
        "replyto": "dpuAkczrTOt",
        "invitation": "ICLR.cc/2023/Conference/Paper6368/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a simple method for visualizing the inductive bias of a supervised learning algorithm. The method involves meta-learning the labels for a dataset, such that the learning algorithm is able to easily generalize when trained on a subset of those labels. For example, if the inputs are 1D, the meta-learned labels will map out a function over the 1D space that corresponds to the \"easiest\" function for the learning algorithm to learn, hence one can interpret that function as representative of the inductive bias of the algorithm. The paper applies their method to a couple of toy examples (linear and kernel regression in 2D) to build intuition, as well as to a feedforward ReLU network and a spiking neural network.",
            "strength_and_weaknesses": "Strengths:\n- Novel idea, well motivated\n- Clear explanations and plenty of simple examples to build intuition\n- The approach behaves reasonably for simple problems\n\nWeaknesses\n- The method is really only applied to simple networks. Why not apply the method to visualize or understand the inductive biases of more complex networks (even stuff like networks trained on MNIST\u2014which is still a relatively easy task). Is it because the meta-learning fails for these bigger networks, or are the resulting learned labels hard to interpret?\n- Most (all?) of the examples in the paper are cases where the learning algorithm has a bias towards smooth functions. It would be nice to see an example where the learning algorithm had some other kind of bias, and to show that the method recovers that specific inductive bias. You could even use simple 1D regression problems (instead of 2D) for simplicity. For example, the original MAML paper has a nice toy problem where they use MAML to meta-learn a network that has an inductive bias towards sinusoidal functions (by meta-training on sinusoids). If one were to take that network, and try to recover it's inductive bias using this method, could you recover what kind of function class was used to meta-train the network in the first place? I think that would be another powerful example of the technique (to demonstrate that the meta-learner can uncover structure besides \"smooth\" data).\n- Instead of the orthogonalization procedure, what what happen if you just initialized using multiple random seeds and re-ran the meta-learning procedure? Would different random seeds solve the problem that the orthogonalization is trying to address?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is of high quality, introduces a novel idea, and is clearly written.",
            "summary_of_the_review": "Overall, I found this approach pretty interesting as a way to understand the function of a network. I think I would need to see more examples of being able to visualize or understand the inductive bias of larger-scale networks, or at the very least more toy examples where the inductive bias was something deeper beyond \"smooth functions are easier to model\". If this concern is addressed I would consider increasing my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6368/Reviewer_VQde"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6368/Reviewer_VQde"
        ]
    },
    {
        "id": "CdpDiq8mPcg",
        "original": null,
        "number": 4,
        "cdate": 1666768937460,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666768937460,
        "tmdate": 1670201307933,
        "tddate": null,
        "forum": "dpuAkczrTOt",
        "replyto": "dpuAkczrTOt",
        "invitation": "ICLR.cc/2023/Conference/Paper6368/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a meta-learning framework to infer a system's or circuit's inductive bias. In their work, the authors claim that their method connects architectural design choices to function space features. This work demonstrates the usability of the method in inferring the inductive bias for relatively simple models, i.e. linear and kernel regression as well as for shallow neural networks and spiking neural networks. The authors also leverage their framework and coupled with an adversarial training strategy, they demonstrate a potential application of learning functions that distinguish the inductive bias of two models with different architectures. Although current experimental evidence is limited to low-dimensional toy problems, the proposed method seems promising in understanding the inductive bias enforced by different architectural choices in high-dimensional end-to-end learning.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is very well-written (barring a few typos) and it is easy to follow and understand the core tenets of the proposed framework.\n2. The authors present incremental evidence to test out different hypothesis pertaining to their proposal. In doing so, they enable the reader to develop the intuitions behind what the expect from the proposed framework, when put in practice.\n3. The idea seems simple and elegant, yet powerful and highly applicable to systems identification problems in both the fields of machine learning and neuroscience.\n\nWeakenesses:\n1. A key weakness of the work is its limited empirical validation or experimental evidence, specifically for non-toy problems. Although the intuitions presented in toy tasks are clear and illustrate the usefulness of the method, it remains to be seen how well the bootstrapped learning framework performs in high dimensions. \n2. A considerable failure mode in high-dimensional bootstrapped learning, something that this work proposes with the use of a learner that uses outputs of the meta-learner and vice versa, is dimension collapse. Specifically, it has been shown (for self-supervised bootstrapped learning) that often the functions learned are low-rank and therefore, the outputs do not span the entire space. Owing to this possible failure mode, it is reasonable to wonder whether this framework would be as applicable in high-dimensions as it is for low dimensionality problems.\n3. I was a bit confused about the utility of the proposed method for biological circuits, specifically how does the proposal fit in when attempting to understand the inductive bias of biological circuits. Firstly, the framework expects the learner to be given some output. How would such a training protocol look like in a particular circuit in a behaving animal? It would be nice to clarify this in the main text as a major motivation revolves around using the framework for biological circuits. Secondly, is it necessary that the system requires gradients to learn? In my understanding, the framework would work if there is some learning happening in the learner. Or could the framework still work without any plasticity in the learner? Have the authors tried testing the limits of their proposal by varying the learning rate for the learner. \n4. The authors present their framework and demonstrate that it works for low-dimensional toy settings but it is not clear why or how it does so. It would be nice to have some intuitive explanation or insights from learning theory or dynamical systems that conveys how this meta-learning setup converges to the inductive bias functions of a learning system. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is generally clear and easy to understand, except in the discussions. The order of points while describing the issues with practical applications seems to have been reversed later in the discussions. Furthermore, it is not very clear why access to the gradients of the learning system is required. \nThe proposal seems to have merit and seems to be a promising direction in general. It is also a novel framework (to my knowledge). But given the lack of theoretical backing and empirical evidence in challenging problems in high-dimensions, I am unsure how this method scales with task (and network) complexity. \nThe authors plan to release their code on github, which should improve the overall reproducibility of the work. ",
            "summary_of_the_review": "Although the general idea seems exciting and promising, I have my doubts over the scalability of this method. Unless authors can provide more evidence from higher dimensional problems (maybe something with MNIST where you try to visualize canonical functions it learns over the pixel space), the utility of the method seems to be limited. Moreover, it is not very clear from the discussions how this method can be used in computational neuroscience. Therefore, my current assessment is marginally below the threshold but I am happy to revisit my rating if the authors address some of my concerns raised above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6368/Reviewer_QUHj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6368/Reviewer_QUHj"
        ]
    }
]