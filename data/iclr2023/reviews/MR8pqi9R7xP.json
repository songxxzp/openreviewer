[
    {
        "id": "2V9AfSDvvP",
        "original": null,
        "number": 1,
        "cdate": 1666496005887,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666496005887,
        "tmdate": 1670481556170,
        "tddate": null,
        "forum": "MR8pqi9R7xP",
        "replyto": "MR8pqi9R7xP",
        "invitation": "ICLR.cc/2023/Conference/Paper1235/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper shows that recent self-supervised ASR model are uniquely vulnerable to black-box adversarial attacks, which is an interesting observation and can inspire future research in this field. ",
            "strength_and_weaknesses": "Strength:\n1. The paper shows that recent self-supervised ASR model are uniquely vulnerable to black-box adversarial attacks.\n2. The paper conducts an ablation study to show that self-Supervised learning is the main cause of that phenomenon.\n3. The paper provides an explanation for this phenomenon.\n\nWeakness:\n\nThe experiment settings in this paper are inconsistent and confusing. I summarize it with the following question. Although I rate a relative low score for this paper, I am willing to change my score if the issues are addressed.\n\n1. In Section 3.4, the results show that SSL-based model is vulnerable to black-box adversarial attacks. However, the attack model is also trained to fool a SSL based models. What if the attack model is trained to fool a non-SSL based model such as the bottom three models in Table 1?\n2. The experiments in Section 3 use an improved version of that mention in Section 2.2. However, in Section 4 only the default version of Section 2.2 is utilized. What is the motivation for this inconsistency?\n3. (A question similar to the first question) In Section 4.1, it seems that the attack model is also trained to fool a wav2vec2-based model. In this way, it is natural that the wav2vec2-based model is more likely to be fooled compared with the non-wav2vec2 model.\n4. In Section 5.1 and 5.2, the conclusion is that a hard targeted attack is hard to achieve compared with a mildly targeted attack, which is an interesting but natural result. However, the intuition of why this result can explain the vulnerable SSL-based ASR model is not discussed in detail.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The phenomenon reported in this paper is interesting.",
            "summary_of_the_review": "The paper observes an interesting result. However, the explanation to this phenomenon is unclear. It seems that it is possible that the SSL based model are vulnerable than non SSL-based model mainly because the attack model is trained to fool a SSL model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1235/Reviewer_6Mf8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1235/Reviewer_6Mf8"
        ]
    },
    {
        "id": "hA-SS2sgLKU",
        "original": null,
        "number": 2,
        "cdate": 1666564279617,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666564279617,
        "tmdate": 1666564279617,
        "tddate": null,
        "forum": "MR8pqi9R7xP",
        "replyto": "MR8pqi9R7xP",
        "invitation": "ICLR.cc/2023/Conference/Paper1235/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies transferable targeted adversarial attack on self-supervised ASR models. Target attack adds a small perturbation to a model such that the model makes the targeted prediction desired by the attacker instead of the correct one corresponding to the original input. Transferability refers to generalizing the attack to private models which have not been used to generate the adversarial perturbation.\n\nPast studies show that targeted attack is hard to generalize for supervised ASR models. However, in contrast to previous findings, the authors demonstrate that such attack can in fact generalize to ASR models pre-trained with self-supervised learning using similar datasets. The authors then present a series of study to understand what leads to successful attack transfer, including self-supervised objective, models size, training data size, and specificity of the attack.\n",
            "strength_and_weaknesses": "- Strengths\n  - The authors presented an interesting study with new observations contrary to previous studies of adversarial attacks on ASR models. This could open up a new direction of empirical and theoretical study to understand robustness in self-supervised setups\n  - Many factors are studied to reason and hypothesize why self-supervised ASR models are more vulnerable to targeted adversarial attack, providing valuable data points for future work.\n  - Attacks of different levels of specificity have also been considered to support the hypothesis.\n  - A more generalizable scheme for optimizing the adversarial attack is proposed in this paper, which uses multiple proxy models and validates/selects checkpoints with a different model.\n\n- Weaknesses\n  - While the authors have compared several different SSL models (wav2vec, hubert, data2vec, WavLM, UniSpeech), they all have very similar architectures (convolution encoder for waveform followed by transformer layers). The argument of \u201cattack on SSL models are transferable when they are pre-trained on the same dataset\u201d would be more convincing if the authors had considered another SSL that have a different model architecture (e.g., LSTM).\n  - The experiments studying the effect of pre-training data could have been expanded. The authors showed in Table 1 that target attack performance is worse on W2V2-Large (CV), which is an interesting observation. The study will more complete if the authors include a) optimizing attack on CV and transfer to LV; b) optimizing attack on a model pre-trained on more datasets (CV+LV+Fisher checkpoint is available on Github) and transfer to one pre-trained on a subset of it; c) the reverse of b.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The topic studied in this paper is novel. The paper is easy to follow and well stated. Experimental details are provided to facilitate reproduction.\n",
            "summary_of_the_review": "While the paper can be improved as mentioned in the sections above, the current presentation already provides substance that will be useful and interesting to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1235/Reviewer_fvn3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1235/Reviewer_fvn3"
        ]
    },
    {
        "id": "KBNbMqTy6J_",
        "original": null,
        "number": 3,
        "cdate": 1666610725393,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666610725393,
        "tmdate": 1666610725393,
        "tddate": null,
        "forum": "MR8pqi9R7xP",
        "replyto": "MR8pqi9R7xP",
        "invitation": "ICLR.cc/2023/Conference/Paper1235/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes targeted and transferable adversarial examples for self-supervised asr models which are in pretraining + fine-tuning architecture. An adversary can make use of the transferability property, that is, an adversarial sample produced for a proxy asr can also fool a different remote asr.\nRich self-supervised asr models, such as wav2vec2, Hubert, data2vec and wavlm are investigated and similar results are shown with detailed experiments and comparisons.\n",
            "strength_and_weaknesses": "Strong:\n\n1 The targeted and transferable adversarial examples are interesting and should benefit the research ad developing of better and more robust asr systems of pretraining+fine-tuning architecture.\n\n2 Rich existing sota models are investigated showing the \u201ctransferability\u201d is a frequent thing.\n\n3 Code is attached making the paper to be with high score of reproducing ability. \n\nWeak:\n\n1 Prefer to see richer experiments on language models using quite different datasets of among different languages to better learn the transferability among pretrained speech representations;\n\n2 If solutions to these well-designed adversarial examples can be provided, this paper will be rich of novel solutions as well.\n\n\n\nDetailed questions and comments\n\n1 So, how to improve current self-supervised models to better dealing with the transferable adversarial examples? This is more valuable for building robust asr systems.\n\n2 Can your asr attacking strategies influencing other people\u2019s normal usage of existing asr systems? Say, you prepared a special group of inputs and all asr systems failed \u2013 then does this influence other people\u2019s usage or if same types of attacking data were used for training these models, then they will fail forever if not fixed. Basing on my experience, most asr systems are still quite fragile \u2013 they fail a lot even with quite clean and clear voice inputs and they will for sure fail if the inputs are further including rich carefully designed noises.\n\n3 Any further comparison of the transferability of from one language\u2019s pretrained model to another language\u2019s pretrained model? Say how large the datasets used are involved in the transferability?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "this paper is clearly written with code attachment - a high reproducibility.\nThe idea of transferability adversarial examples is interesting. \nThis paper can be scored higher if with solutions to the adversarial examples (or investigating existing anti-attach methods)",
            "summary_of_the_review": "Generally, the paper is well written with a good motivation and rich experiments on detailed evidences. One issue is that the investigation of the datasets used for training these sota ssl models and also among different languages. \nIf with one solution to dealing with the adversarial examples, this paper can be ranked higher.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1235/Reviewer_5s8w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1235/Reviewer_5s8w"
        ]
    },
    {
        "id": "aG3EAePs8GZ",
        "original": null,
        "number": 4,
        "cdate": 1667165668959,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667165668959,
        "tmdate": 1667165668959,
        "tddate": null,
        "forum": "MR8pqi9R7xP",
        "replyto": "MR8pqi9R7xP",
        "invitation": "ICLR.cc/2023/Conference/Paper1235/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigate the transferability property of adversarial attacks on ASR models. They evaluate the robustness of modern transformer-based ASR architectures such as Wav2Vec-2.0 etc. With a series of experiments on a set of ASR models by using a set of 85 adversarial samples, they show that many state-of-the-art ASR models are in fact vulnerable to the transferability property. Additionally, they also claim to show that SSL-pretraining is the reason for this vulnerability to transferability. \n\n",
            "strength_and_weaknesses": "\nThe approach for generating adversarial examples is interesting but it is not explained very well in section 3.1 as the details are missing such as: \nWere only 85 samples used during fine-tuning the models?\nWhy 85 and how were these selected? \nIt is not clear how the third model (Data2Vec BASE) was used as stopping criteria? \nResults are presented in section 3.4, but before that I have not seen how the adversarial examples are generated. \n\n\n\nIn section 4, why character error rate (CER) was used instead of WER? CER might not be indicating the significant impact on WER as these could have been fixed by LM duding decoding (if there was one added). I guess CER was not used in the previous section due to the same reason. So it would be good to see these results in terms of WER. \nThis section in particular was difficult to understand. There should be some more details added for clarity. For example the following sentence is not at all clear: \"we observe that the Character Error Rate between two random sentences is about 80-85% on average. Therefore attack success rates higher than 20% indicate a partially successful attack\u201d. How is CER computed between two samples? Is it not between the reference and hypothesis? How 20% is selected as indicator for successful attack?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Study is very interesting but not novel. Many experiemnts are not clearly explained, and reproducibility of these experiments might not be easy without all details. ",
            "summary_of_the_review": "Overall the paper targets an interesting problem. At the beginning the paper sounded very interesting based the claims but as I went through the paper I could not understand some experiments or reasoning behind some experiment criteria. Therefore, I won\u2019t feel confident to vote for acceptance of this paper.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1235/Reviewer_xki8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1235/Reviewer_xki8"
        ]
    }
]