[
    {
        "id": "-r36piVfoHQ",
        "original": null,
        "number": 1,
        "cdate": 1666645544167,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645544167,
        "tmdate": 1666645544167,
        "tddate": null,
        "forum": "pmUH7A8wZz",
        "replyto": "pmUH7A8wZz",
        "invitation": "ICLR.cc/2023/Conference/Paper3448/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A proposal of hyperbolic generative models is provided in the paper based on some existing structures in the literature, a stable training/optimization is observed empirically. The authors adopt the framework to learning molecule structure with the goal to learn the structural information. ",
            "strength_and_weaknesses": "Strength. -- The proposed framework follows clear logic and easy to follow, mostly used the layer definition and function in [Chen et al. 2021] fully hyperbolic neural network. The authors also propose a straightforward version for concatenating/split vectors in the Lorentz model, which empirically shows stable gradients during optimization. The comparison of Lorentz Direct Concatenation and Lorentz Tangent Concatenation looks interesting and helpful. \n\n-- The authors evaluate the proposed HAEGAN on learning molecules structure and compare against several baselines using structure metrics. \n\n-- The improvement on learning structure of molecules seems to be significant when compared with various baselines. \n\nWeakness. -- Novelty of the model. Apart from the proposed concatenating/split, all other layers were proposed in prior works, particularly from [Chen et al. 2021], the whole framework looks to me a combination of existing pieces into a hyperbolic version of common GAN.\n\n-- The authors claim that the stable training of the model is guaranteed. However, the numerical stable property is only evaluated for the concatenating/split operation empirically. The numerical stable hyperbolic centroid distance layer is proposed in early work. There is no theory to guarantee the stable training. Particularly, large gradients into exp/log map is one side of the numerical instability, the other side of the numerical instability comes from float representation of hyperbolic space, a line of work such as [Yu et al. 2019], which happens when the hyperbolic point becomes far from the origin. Please modify the claim in the paper in an appropriate way. \n\nSuggestion and Question:\n-- Instead of putting 5 rows of NaN in table 1, you can just illustrate them in plain text. Though still it looks surprising to me that all hyperbolic baselines produce NaN, can you find a hyperbolic baseline without NaN, i.e., with less epochs or small graph just for a comparison?\n\n* [Chen et al. 2021], Fully hyperbolic neural networks. \n* [Yu et al. 2019], Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models",
            "clarity,_quality,_novelty_and_reproducibility": "The paper follows clear logic, easy to follow. Lack of enough novelty. Some statements/claims need to be changed. ",
            "summary_of_the_review": "The paper proposes a new framework of hyperbolic GAN and adopt it in a new task, learning the structure of molecules. The proposed model outperforms Euclidean baselines, where hyperbolic baselines are not available for comparison. However, the paper is lack of enough novelty, as most used layers are proposed before. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3448/Reviewer_PLXj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3448/Reviewer_PLXj"
        ]
    },
    {
        "id": "GF1DseZ0X8Z",
        "original": null,
        "number": 2,
        "cdate": 1666673001875,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673001875,
        "tmdate": 1666674017252,
        "tddate": null,
        "forum": "pmUH7A8wZz",
        "replyto": "pmUH7A8wZz",
        "invitation": "ICLR.cc/2023/Conference/Paper3448/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a new architecture for generative modeling that makes use of hyperbolic geometry. Specifically, the builds on Chen et al 2022 to propose a generative framework that combines both Autoencoders and GANs. Additionally, the paper also a presents a new method for splitting and concatenating vectors in the Lorentz manifold. The paper adapts the $W$-GAN framework and proves a similar proposition for the characterization of the Wasserstein Distance. \n\nThe paper then tests these pieces out individually - generating images from MNIST, testing their concatenation method and then finally generating molecules using their method. ",
            "strength_and_weaknesses": "**Strengths**\n--\n\n1) I think Proposition 3.1 is a strength. WGANs have improved GAN stability and being able to use a similar formulation (beyond the architecture mentioned here) is interesting and significant. \n2) The method seems to perform well and produce valid and diverse molecules. \n3) The paper is very well written. It is very easy to read and follow. All details are present and it should be easily reproducible. \n4) The ablation study is well conducted. \n\n**Weakness**\n---\n\n1) The motivation for the architecture is not quite clear. The authors, mention that training Hyperbolic Neural Networks is unstable and hence GANs can't model complex distributions. While I tend to believe this for the older methods that use the Tangent space, I however, do not know if this is true for the newer method that uses Lorentz rotations and boosts. Hence evidence of this via experimentation would be appreciated. \n\nSecond, this issues rather being resolved is pushed to a different part and then not discussed. In a sense, the method still learns a map from wrapped gaussians to the data distribution (GAN generator + Autoencoder decoder). Hence the autoencoder must now figure out how to map from a complex distribution to a simple distribution and then back to the complex distribution. Maybe because the training of autoencoders is more stable than GANs this is okay. \n\nFinally, the representation learned by the autoencoder is supposed to ``simpler'' than the original representation. But lower dimensional representations of data need not be simpler than higher dimensional representations. More discussion on this is needed. \n\n2) While the paper in general does a good literature survey, the paper has a section 2.3 which is on Embedding form Euclidean to Hyperbolic Spaces, which misses crucial literature. The paper claims it is unavoidable to use the exponential or logarithmic maps to represent the data hyperbolic space. However, this is not true. There are many embedding techniques such as Nickel and Kiela NeurIPS 2017, Nickel and Kiela ICML 2018, Sala, De Sa, Gu and Re ICML 2018, Sonthalia and Gilbert NeurIPS 2020. In particular, Sala et al 2018 and Sonthalia and Gilbert 2020 do not use the exponential or logarithmic maps. (This is a more minor comment.)",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n--\n\nThe paper is very well written and easy to follow. \n\n**Novelty and Significance**\n---\n\nWhile the architecture is novel, the motivation behind it is not the clearest. The results produced do however seem significant. \n\n**Reproducible**\n---\n\nThe paper seems to be reproducible. \n\n**Questions**\n---\n\n1) The results for the variants HVAE-w, HVAE-r, and HAEGAN-H surprise me. (The other seem reasonable). Specifically, the VAE ones. VAEs are  fairly stable to train (to my knowledge) and these use the fully hyperbolic layers. Hence I would imagine that while this model might not perform well it should train and produce non-NAN results. The HAEGAN-H is less surprising but is still surprising. Maybe this is an issue related to learning rate and the learning rate scheduler?\n",
            "summary_of_the_review": "In summary, I think this is a well written paper with interesting results. However, some parts could be better explained. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3448/Reviewer_V3p6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3448/Reviewer_V3p6"
        ]
    },
    {
        "id": "zEoyo71iQX",
        "original": null,
        "number": 3,
        "cdate": 1666814890292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666814890292,
        "tmdate": 1666814890292,
        "tddate": null,
        "forum": "pmUH7A8wZz",
        "replyto": "pmUH7A8wZz",
        "invitation": "ICLR.cc/2023/Conference/Paper3448/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a new network that combines autoencoder and generative adversarial network to guarantee stable training of hyperbolic neural networks. Experiments show that the proposed method can generate complex data, and there are some interesting results in the experiments. Nevertheless, there are still some issues that need to be resolved.",
            "strength_and_weaknesses": "Pros:\n1. This paper designs a model that can guarantee stable training of hyperbolic neural networks.\n2. In the experiments, different metrics have been utilized to evaluate the proposed method. There are some interesting experimental results in this paper.\n\nCons:\n\n1. It seems that this work simply combines the hyperbolic neural networks with autoencoder and generative adversarial networks. I think the authors have to clarify the novelty and contributions of this work more clearly.\n\n2. It is not clear how the proposed model can guarantee stable training. A further explanation/clarification is necessary.\n\n3. The organization of this paper is somewhat confusing. Lots of the contents in this paper are utilized to introduce the background (e.g., Section 2, Section 4.1, Section 6). I think the authors should put more effort on explaining the proposed model. Maybe some background introduction can be moved into Appendix. The organization and presentation in this paper should be modified.\n\n4. The scalability of the proposed technique should be discussed.\n\n5. The experiments are limited, the authors should carry out experiments on one or two more datasets.\n\n6. This work lacks complexity analysis, the authors should analyze the complexity (e.g., time complexity) of the proposed method and compare it to state-of-the-art.\n\n7. Many notations have been used in this paper, I suggest the authors could try to make a table to explain all the notations in the Appendix.\n",
            "clarity,_quality,_novelty_and_reproducibility": "-The organization and clarity of the paper need improvement.\n-The proposed technique exhibit certain novelty.\n-Code is not provided.\n\n",
            "summary_of_the_review": "Overall, I still have some concerns about the organization and clarify the proposed technique. The experiments are also limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3448/Reviewer_6PpY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3448/Reviewer_6PpY"
        ]
    }
]