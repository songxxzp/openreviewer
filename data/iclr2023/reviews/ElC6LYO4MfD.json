[
    {
        "id": "bRk9f3nvNAN",
        "original": null,
        "number": 1,
        "cdate": 1666528023826,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666528023826,
        "tmdate": 1666528023826,
        "tddate": null,
        "forum": "ElC6LYO4MfD",
        "replyto": "ElC6LYO4MfD",
        "invitation": "ICLR.cc/2023/Conference/Paper3312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose a new federated learning optimization based on variance reduced proximal point algorithm. Furthermore, a theoretical superiority of the proposed algorithm is derived. Preliminary experiments also demonstrate the effectiveness of the proposed algorithm. ",
            "strength_and_weaknesses": "The authors propose a new federated learning algorithm under the second-order similarity condition. Solid theoretical findings show the superiority of the proposed algorithm. However, before this work is accepted, several concerns should be resolved:  \n(i)\tMore discussion should be given on the second-order similarity. There exist several similarity conditions in analyzing the federated learning algorithms. Can the authors provide their relationships of the existing similarity conditions? In addition, do the results derived in this work still hold when assumption 1 is replaced by other similarity conditions? If not, can the authors provide several explanations?\n(ii)\tExisting experiments are too simple to demonstrate the practical effectiveness of the proposed approach. More comparisons with SOTA baselines such as FedSAM, FedDyn, FedCM, FedPD, and FedDR, should be included. \n(iii)\tIn addition, the efficacy of SPPM and Catalyzed SVRP should be verified. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written.  However, the experiments are too toy to demonstrate the efficacy of the proposed approach. ",
            "summary_of_the_review": "see the comments above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_sQ1q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_sQ1q"
        ]
    },
    {
        "id": "Ol0PX9X90j",
        "original": null,
        "number": 2,
        "cdate": 1666549754024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666549754024,
        "tmdate": 1666549754024,
        "tddate": null,
        "forum": "ElC6LYO4MfD",
        "replyto": "ElC6LYO4MfD",
        "invitation": "ICLR.cc/2023/Conference/Paper3312/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies Federated stochastic optimization with client sampling. Under the assumption of second-order similarity condition, the authors propose a variance-reduction based proximal point algorithm which enjoy a better convergence rate wrt the number of clients. They also proposed an accelerated version which enjoys an even tighter rate wrt to the condition number. \n\n",
            "strength_and_weaknesses": "This paper is in general easy to read and the improvement seems significant under the corresponding assumptions. However, I have the following concerns:\n\n1.\tI am slightly confused about the setting: in federated learning, one important thing is protecting the privacy of the local learners, and this is the reason why the local learner only submits their model instead of their data. However, in this paper, if my understanding if correct, the sever need to compute the full gradient. Thus, does this mean all local clints have to submit their data? If the sever has access to all gradients, and the computational complexity of gradient computation is low, we do we still need the local clients? I think I may have some misunderstandings on this paper, and I hope the authors can help me understand this point.\n\n2.\tThe experiments are not sufficient (for a stochastic optimization/federated learning paper). It is only conducted on one very simple data set. More experiments should be conducted to demonstrate the effectiveness of the proposed algorithms. \n\n3.\tThe section on Catalyst is not well-developed: It seems that the Catalyst algorithm is just being plugged-in and it is hard to tell where the novelty is. Moreover, can the authors be clearer about how Algorithm 3 works in a sever-client setting?\n\n4.\tFor assumption 1, I do not see why this inequality holds for the settings the authors mentioned. Can the authors elaborate on 1 or 2 examples?\n\n\nA small question: In experiments, how do we compute x^*?\n\nSome minor issues:\nPage 2: \u201d several methods exist for methods exist for solving Problem\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "see above. ",
            "summary_of_the_review": "This paper is in general easy to read, and the improvement seems significant under the corresponding assumptions. However, I have some questions/concerns, which are listed above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_gy4c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_gy4c"
        ]
    },
    {
        "id": "nkezh_GDz4",
        "original": null,
        "number": 3,
        "cdate": 1666634252156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634252156,
        "tmdate": 1666634575564,
        "tddate": null,
        "forum": "ElC6LYO4MfD",
        "replyto": "ElC6LYO4MfD",
        "invitation": "ICLR.cc/2023/Conference/Paper3312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies federated optimization under strong convexity and second-order similarity assumptions. Using the idea of trading off local computational cost for less communication, the paper proposes a new algorithm SVRP based on proximal point methods and variance reduction (SVRG). With client sampling, SVRP can achieve better communication cost compared to the state-of-the-art when the second-order similarity constant is small enough. Moreover, the catalyst acceleration of SVRP is also studied when assuming the exact proximal oracle. It attains strictly better complexity than all existing works.",
            "strength_and_weaknesses": "**Strength:**\n\nThe paper achieves better communication cost than related works for strongly-convex finite-sum federated optimization under second-order similarity, showing the benefit of client sampling and the trade-offs between local computation and global communication. The novel method that incorporates SVRG into the proximal point framework can also provide new algorithmic insights. Although the use of proximal point algorithms in federated optimization and the ability of catalyst framework to accelerate convex optimization is well-known in the literature, it is still interesting to see the improved rates in this work combining all the techniques.\n\n**Weakness:**\n\n1. It is not immediately clear whether the analyses can be extended to other settings, e.g. the constrained case because it is explicitly used that $\\nabla f(x^*)=0$ in all the proofs. However, the original PPM (and also SGD) works for strongly-convex, convex, and nonconvex settings in both constrained and unconstrained regimes. The proposed algorithms also require extra memory for $g_k$.\n\n2. As mentioned in the paper, the current catalyzed SVRP requires the exact proximal oracle, and it is not clear how to satisfy eq. (41) with the inexact proximal update. The double-looped (actually tripple-loop if considering local steps) catalyzed SVRP can be complicated to implement in practice. Will it be possible to directly accelerate SVRP to design a simpler algorithm?\n\n3. It will be good to include the server and clients in the algorithms for a better understanding since some steps are only for clients and some are for the server. For the communication complexity, it should also take into account that the server needs to first communicate the stepsize $\\eta$ and accuracy $b$ to all the clients even though it does not affect the final complexity.\n\n4. More details about the experiments can be provided in the appendix, e.g. values for the stepsizes. What is the behavior if one tunes the stepsize for each algorithm instead of setting it to the optimal theoretical suggestion?\n\n5. No experiments of catalyzed SVRP are provided. In the case that $\\delta/\\mu \\leq\\sqrt{M}$, SVRP is enough and the catalyst will not bring any benefit (is there a typo in the comment after Theorem 3 stating that Catalyzed SVRP is strictly better than SVRP when $\\delta/\\mu\\leq\\sqrt{M}$), and all experiments fall into this regime. It might be good to verify the behavior of catalyzed SVRP as well when $\\delta$ is large.\n\n6. Minor: The emphasis that SPPM does not require smoothness in the abstract can be misleading and not precise. Smoothness is required to solve the proximal operator efficiently when its closed form is unknown; It is better to include the example of differentially private optimization in the main text because it is also emphasized in the abstract; It is mentioned in the related work that Kovalev et al. (2022) remove logarithmic factors but it is still $\\tilde O$ in the table; The symbol $\\mathcal{M}$ in the catalyst might cause some confusion with $M$ for the number of clients; What is the relationship between second-order similarity and other smoothness conditions used in variance reduction, e.g. average smoothness and individual smoothness, and what are the benefits of introducing it?\n\n7. Typos: \"several methods exist for solving Problem (1)\"; \"SVRP trades off a higher lower computational complexity\"; \"Ryu & Boyd (2014) convergence rates\"; $\\eta=\\mu\\epsilon/(2\\sigma^2)$? after Algorithm 1; Theorem 2: \"and and that each $x_{k+1}$\"; \"We linear regression\"; duplication in eq (17); Proposition 3 and proof of Theorem 3: should be $1/(\\cdots+2)$ in $\\tau$. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and easy to follow. The proofs are provided in great detail and correct to my understanding. The algorithms and analyses can provide novel insights into the community.",
            "summary_of_the_review": "The paper provides strictly better complexity for strongly-convex federated optimization under second-order similarity, with a novel algorithm SVRP and its catalyst acceleration. I am happy to increase the score based on the discussions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_2cGY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_2cGY"
        ]
    },
    {
        "id": "bND-p25Yjl9",
        "original": null,
        "number": 4,
        "cdate": 1666660645088,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660645088,
        "tmdate": 1670819212451,
        "tddate": null,
        "forum": "ElC6LYO4MfD",
        "replyto": "ElC6LYO4MfD",
        "invitation": "ICLR.cc/2023/Conference/Paper3312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied the finite-sum federated optimization under a second-order function similarity condition and strong convexity, and proposed two new SVRP and Catalyzed SVRP algorithms based on approximate stochastic proximal point evaluations, client sampling, and variance reduction. It provided the convergence analysis of the proposed algorithms, and  prove that the proposed Catalyzed-SVRP algorithm has a lower communication complexity  than the existing methods. Some experimental results demonstrate the efficiency of the proposed algorithms. ",
            "strength_and_weaknesses": "Strength:\n\nThis paper proposed two new SVRP and Catalyzed SVRP algorithms based on approximate stochastic proximal point evaluations, client sampling, and variance reduction. It provided the convergence analysis of the proposed algorithms, and \nprove that the proposed Catalyzed-SVRP algorithm has a lower communication complexity  than the existing methods.\n\nWeakness:\n\nThe novelty of this paper is limited. This paper basically combines the existing approximate stochastic proximal point evaluations, client sampling, and variance reduction. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing can significantly be improved. \nThe novelty of this paper is limited. This paper basically combines \nthe existing approximate stochastic proximal point evaluations, client sampling, and variance reduction. ",
            "summary_of_the_review": "This paper studied the finite-sum federated optimization under a second-order function similarity condition \nand strong convexity, and proposed two new SVRP and Catalyzed SVRP algorithms \nbased on approximate stochastic proximal point evaluations, client sampling, and\nvariance reduction. It provided the convergence analysis of the proposed algorithms, and \nprove that the proposed Catalyzed-SVRP algorithm has a lower communication complexity \nthan the existing methods. Some experimental results domenstrate the efficiency of the proposed algorithms. \n\nSome Comments:\n\n1. It would be great if the authors would detail the second-order function similarity condition. \n\n2. It would be great if the authors would detail the reason of reducing the communication complexity in \nthe proposed methods.  \n\n3. There will be a strict upper limit of 9 pages for the main text of the submission, \nwith unlimited additional pages for citations (https://iclr.cc/Conferences/2023/CallForPapers). \nThis paper maybe not fit for this request.\n\n\n\n--------------------------------------------------------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------------------------------------------------------\nThe authors still did not solve my main concern- the limited novelty of this paper. So I keep my score.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_Q8H3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_Q8H3"
        ]
    },
    {
        "id": "yeDjQpmmJ1",
        "original": null,
        "number": 5,
        "cdate": 1666812764373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666812764373,
        "tmdate": 1666812764373,
        "tddate": null,
        "forum": "ElC6LYO4MfD",
        "replyto": "ElC6LYO4MfD",
        "invitation": "ICLR.cc/2023/Conference/Paper3312/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the federated learning optimization problem under the assumption of second-order function similarity. In contrast to the previous work that considers the full client participation model, this work studies the problem under the partial participation/client sampling model. Two communication-efficient algorithms are proposed, namely SVRP and Catalyzed SVRP, which use the stochastic proximal point method as a building block and adapt it via an SVRG-style variance reduction method. \n\nThe authors provide analysis of the stochastic proximal point method without the smoothness assumption. Moreover, the analysis of the algorithms shows the improved communication complexity of the proposed methods as compared to the prior methods. Experiments on real and synthetic data show the improved performance of the proposed methods in terms of communication.",
            "strength_and_weaknesses": "* Strengths:\n\n1.\tOverall, the paper is well-written with a clear motivation. \n\n2.\tUsing stochastic proximal point framework to unify the analysis is interesting.\n\n3.\tThe authors provide an elegant combination of prior techniques used in the federated learning literature including client sampling, variance reduction, and stochastic proximal point method.\n\n* Major concerns:\n\n1.\t(Computational cost) The analysis shows that under the partial participation setting, the proposed methods improve communication complexity. However, this gain is achieved at the expense of additional computation on the local clients. Even though the experiments demonstrate superior performance in terms of communication, no experiments are provided to compare the computation trade-off. It would be interesting to evaluate this trade-off empirically. \n\n2.\t(Small-scale experiments) Moreover, the experiments are provided on relatively simpler learning problems. It would be interesting to evaluate the methods on more complex federated learning problems, e.g. using logistic regression on image data, or training a neural network, as done by the authors of SCAFFOLD (Karimireddy et al., 2022)\n\n3.\t(Limited novelty) Since distributed optimization has already been studied under assumption 1 in the full participation setting, improving the existing algorithms under the partial participation setting seems rather limited in terms of novelty.\n\n* Minor concerns:\n \n1)\tTypo: In the paragraph above section 1.1, the phrase \u201cmethods exist for\u201d is redundant\n\n2)\tMissing word: In the third last paragraph of section 2, the sentence \u201cRye & Boyd (2014) convergence rates\u2026\u201d should instead be something like \u201cRye & Boyd (2014) provide convergence rates\u2026\u201d\n\n3)\tFigure 1: I found the font size to be very small. For better visibility, I would recommend turning the grid on and increasing the font size of the axis labels, and legends.\n\n4)\tMissing word: In the beginning of section 5, the sentence \u201cWe linear regression with\u2026\u201d should instead be something like \u201cWe run linear regression with\u2026\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written with a clear motivation.",
            "summary_of_the_review": "Overall, the idea of the paper is sound. However, the experiments could be made more thorough (please see the comments above). My main concern is the amount of computation which seems to be quite high. Even though communication is the bottleneck in FL, the amount of computation at each local client should also not be too high. An empirical evaluation of this aspect would be highly encouraged to demonstrate the practicality of the proposed methods on FL devices.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_JTga"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3312/Reviewer_JTga"
        ]
    }
]