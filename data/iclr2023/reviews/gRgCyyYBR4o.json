[
    {
        "id": "ieaJGQWWmR",
        "original": null,
        "number": 1,
        "cdate": 1665999183752,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665999183752,
        "tmdate": 1665999183752,
        "tddate": null,
        "forum": "gRgCyyYBR4o",
        "replyto": "gRgCyyYBR4o",
        "invitation": "ICLR.cc/2023/Conference/Paper2647/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigate a pevious work on stein mixtures and show that it corresponds to inference with the Renyi $\\alpha$-divergence for $\\alpha=0$, and using other values of $\\alpha$ may stablize the inference.",
            "strength_and_weaknesses": "Strength:\n\n1. This paper is overall well-written and the literature review is thorough.\n\nWeaknesses:\n\n1. The motivation of the proposed method is vague. No comparison to other SVGD variants.\n2. The orginality is low as the major contribution seems to be some empirical investigation of a hyperparameter.\n3. The paper is a little bit hard to follow. What is the main difference between stein mixture and a simple mixture model?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, mostly clear, and lack of originality.",
            "summary_of_the_review": "My main concern is that the original contribution of this work is not significant enough.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2647/Reviewer_Ra8d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2647/Reviewer_Ra8d"
        ]
    },
    {
        "id": "Ln1ZjaU2TR",
        "original": null,
        "number": 2,
        "cdate": 1666612456683,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612456683,
        "tmdate": 1666612456683,
        "tddate": null,
        "forum": "gRgCyyYBR4o",
        "replyto": "gRgCyyYBR4o",
        "invitation": "ICLR.cc/2023/Conference/Paper2647/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors generalized the Stein Mixtures approach of Nalisnick and Smyth 2017 to general alpha-divergences.  The paper is mostly nicely written and was a good read.  But it did not have a clear motivation nor impressive empirics.",
            "strength_and_weaknesses": "Strengths\n* The explanation and demonstration of the trade-off between the need for a large number of Monte Carlo samples when estimating the gradients without too much bias / variance is nicely laid out and is clear.\n* The introduction of alpha-divergences into Stein mixtures is a nice application of a compelling line of work on generalized variational inference and I was glad to see it explored here.\n* The worked out toy example with the analytic fixed points is very nice and illuminating!\n\nWeaknesses:\n* The authors should better defend a key, undefended premise of their work about which I was unconvinced: that Stein mixtures alleviate an \u201cexponential\u201d dependence of SVGD on the dimensionality on the model.  This premise is not made precise nor is it attributed to previous work that establishes it more precisely.  Why is an exponential number of particles needed for SVGD?  This is certainly not the case for MCMC for example.\n* If there is a situation where the dependence is exponential with dimension, would the authors please explain (ideally with a theoretical result) such a situation?\n* Or if there is no theoretical justification or proof to be made about the better dependence on dimension, empirical validation of this claim is needed.  For example, it would help to have an illustrative example of a case (e.g. in simulation) where the dimension scaling is exponential with the dimension. E.g. perhaps a plot with an increasing dimension on the x axis and the number of particles needed to achieve some level of posterior precision on the y-axis (for all three of the SVGD method, Stein mixtures, and the new proposed method).\n* Or alternatively, if this exponential dependence on dimension is known from prior theory or empirics, even replicating those prior results (with citation) would strengthen the paper.\n* Or if the dependence is not actually exponential, the authors must either be precise about the way in which dimensionality poses a problem that is addressed by their approach or drop this as the premise of the method.\n\n* Given that the paper is about an improvement upon Nalisnick and Smyth 2017 (whose applications and limitations most readers may not be very familiar with), the paper could benefit from more justification as to why this method is important/useful and why it must be improved.  At present, I found it challenging to see what problem the new methodology was trying to solve that Nalisnick and Smyth 2017 had not solved.\n* On problems where the developed methodology is applicable, how does it compare to other Bayesian inference methods?  E.g. HMC, MFVB, Laplace approximations etc., in terms of computational cost and accuracy (experiments are not necessarily needed, but I think commentary at least is).  For the UCI comparisons, previously run baselines could be used.",
            "clarity,_quality,_novelty_and_reproducibility": "* I found the paper mostly well written but had trouble following it in some places.  For example, around equation 2 it is very unclear whether the $phi_j$ are part of the model or if they are hyper-parameters of the variational approximation since they are written to have a density under p.  This should be made much more explicit, as the notation in equation 2 suggests that $phi$ exists in the original model $p$.\n* The work is original.",
            "summary_of_the_review": "The paper is an enjoyable read about extending Stein mixtures to use alpha divergences.  But I recommend rejection because in my assessment it falls short on:\n* Motivating how or why the proposed method addresses limitations of existing methods (particularly dimension dependence) and\n* Empirical demonstration of improvement relative to existing methods.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2647/Reviewer_r9He"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2647/Reviewer_r9He"
        ]
    },
    {
        "id": "oqk_TLZmEs",
        "original": null,
        "number": 3,
        "cdate": 1666676731675,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676731675,
        "tmdate": 1666676731675,
        "tddate": null,
        "forum": "gRgCyyYBR4o",
        "replyto": "gRgCyyYBR4o",
        "invitation": "ICLR.cc/2023/Conference/Paper2647/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to improve the numerical stability of stein mixture in variational inference. To achieve this, the proposed EBLO-within-stein method allows the choice of a class of hierarchical attractives forces indexed by the hyper-parameter $\\alpha$ in the Renyi divergence. Different values of $\\alpha$ acounts for different types of latent variables, which could be measured by the signal-to-noise ratios. The paper includes empirical results to show various effects of $\\alpha$ on real data analysis.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is overall well-written and the method is clearly explained.  The reviewer really like the writing in section 2 that builds up the knowledge step by step\n- The literature review is thorough\n- The connection with the Renyi divergence and the VR bound seems interesting\n\nWeakness: \n\n- Although the connection with existing works is interesting, the reviewer is not fully convinced why ELBO-within-Stein is preferred over the existing ones.\n\n- given a real data analysis, how should one pick the value of $\\alpha$\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some minor comments in terms of clarity:\n\n- the third contribution mentionds global latent and local latent variables, could the authors clarify their definition. The explanation in section 3.1 is not crystal clear to the reviewer.\n\n- on page 2, when the authors claim the motivation of the method, they state \"for such estimates, we cannot expect XXX as stein force will compensate the gradient estimation error by a counter force\", can the authors be more specifc?\n",
            "summary_of_the_review": "Although more empirical results can make the paper much stronger, this is an overall well-executed paper. The authors need clarify on a few raised questions.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2647/Reviewer_k1hY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2647/Reviewer_k1hY"
        ]
    }
]