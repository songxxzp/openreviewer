[
    {
        "id": "jnefw5jjZPd",
        "original": null,
        "number": 1,
        "cdate": 1666611041875,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611041875,
        "tmdate": 1666611041875,
        "tddate": null,
        "forum": "y81ppNf_vg",
        "replyto": "y81ppNf_vg",
        "invitation": "ICLR.cc/2023/Conference/Paper3130/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper \"AutoTransfer: AutoML with Knowledge Transfer - An Application to Graph Neural Networks\" makes two major contributions: First, the authors devise a benchmark for searching graph neural networks, consisting of 12 task and comprising 120,000 evaluated task-model combinations. Second, they propose a method for transfer learning, suggesting a prior based on past experience for searching neural architectures on a new dataset.",
            "strength_and_weaknesses": "[+] The provided benchmark dataset is valuable and hopefully facilitates and enables research on transfer learning graph neural networks.\n\n[+] The proposed transfer learning approach seems to perform reasonably well refining the state of the art.\n\n[+] To the best of my knowledge, the proposed method is novel.\n\n\n\n[-] The authors state that all GNNs can be described in terms of three stages: pre-processing with MLPs, message passing layers, and post-processing with MLPs. However, I would argue that this is quite restrictive and not able to reflect all possible architectures. See for example \n\nLee, Junhyun, Inyeop Lee, and Jaewoo Kang. \"Self-attention graph pooling.\" International conference on machine learning. PMLR, 2019.\n\nand \n\nMa, Yao, et al. \"Graph convolutional networks with eigenpooling.\" Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 2019.\n\nwhere pooling operations are added between the convolutions resulting in iterative changes of the graph structure. Of course, such a benchmark database needs to be restricted in some sense so that the set of candidates is reasonably limited. However, I would suggest to rephrase the statement in Section 4.2 a little bit so that it makes clear that the benchmark limits itself to such GNNs.\n\n[-] It is not clear why the authors chose the mentioned datasets for evaluation and why results on the other datasets are not reported.",
            "clarity,_quality,_novelty_and_reproducibility": "- There is already another transfer learning approach named \"Auto-Transfer\" and frankly speaking the name is not very innovative or self-describing: Murugesan, Keerthiram, et al. \"Auto-Transfer: Learning to Route Transferrable Representations.\" arXiv preprint arXiv:2202.01011 (2022).\n- Why is only Kendall's correlation coefficient considered? There could be different degrees of similarity, something like weak similarity where only a top-k ranking needs to be consistent. However, since one is not interested in the precise ranking of low performers, one could argue that this is less relevant for comparing similar datasets. Maybe a correlation measure such as normalized discounted cumulative gain (NDCG) would be a better choice here? The authors state that they see a justification for Kendall's tau in the results but do not discuss other ranking correlation measures.\n\n### Clarity\nThe clarity is overall quite good. Language is clear and the paper is well structured.\n\n### Quality and Novelty\nThe quality of the paper is very good and the approach is novel - at least to the best of my knowledge.\n\n### Reproducibility\nImportant details regarding the methods evaluated are missing. In particular how competitors have been parameterized, what kind of evolution was used as a baseline and how it was configured etc.",
            "summary_of_the_review": "The paper makes good contributions to the field and provides some new artefacts for the community which can be quite useful to foster future research on NAS for GNNs. The paper has some minor issues which can however be fixed quite easily. Therefore, I would recommend to accept the paper under the condition that the issues are fixed for the camera ready version.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3130/Reviewer_SdtT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3130/Reviewer_SdtT"
        ]
    },
    {
        "id": "VrN3KnCifKb",
        "original": null,
        "number": 2,
        "cdate": 1666682392323,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682392323,
        "tmdate": 1666682392323,
        "tddate": null,
        "forum": "y81ppNf_vg",
        "replyto": "y81ppNf_vg",
        "invitation": "ICLR.cc/2023/Conference/Paper3130/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel AutoML method, AutoTransfer, for graph learning tasks. It introduces a task-model bank that can transfer the model design to similar new tasks. The experiments indicate that the method is efficient and improves the performance of the found models.",
            "strength_and_weaknesses": "### Strengths\n1. The paper introduces an interesting method for transferring knowledge from a previously searched task to a new coming task with a task-model bank design.\n2. It proposes an interesting task embedding with several random-initialized anchor models, and the experiments show that the task similarity is promising.\n3. The methods are tested on multiple benchmarks and are proven to be efficient and performant.\n4. It releases a large task-model dataset that can inspire future research.\n\n### Weaknesses\n1. How are the anchor models selected? I failed to find the architecture design of the anchor models, and it would be good to see how diverse the anchor models should be.\n2. The graph architectures used in the paper are quite limited. For example, on the OGB leaderboard, many different GNN (non-GNN) algorithms are applied and achieve significantly better results. It would be interesting to have some of those models in the search space.\n3. It would be interesting to see some examples of the models selected for a group of tasks, to get some insights of the model design.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The proposed methods are interesting.",
            "summary_of_the_review": "The paper proposes an interesting method for transferring knowledge in AutoML for graph learning tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3130/Reviewer_1hga"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3130/Reviewer_1hga"
        ]
    },
    {
        "id": "1qatitYiAP",
        "original": null,
        "number": 3,
        "cdate": 1667198542919,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667198542919,
        "tmdate": 1667198542919,
        "tddate": null,
        "forum": "y81ppNf_vg",
        "replyto": "y81ppNf_vg",
        "invitation": "ICLR.cc/2023/Conference/Paper3130/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studied NAS algorithm for GNN. The author proposed to transfer the prior architectural design knowledge to the novel task of interest. The author first extracts scale-invariant measures based on Fisher Information Matrix (FIM). The scale-invariant measures are used to form the task feature, which are further projected to task embeddings. The task embeddings are learned via a ranking loss built on top of GraphGym's extact metric space. Experiments justified the importance of the task embeddings in the AutoTransferer algorithm. The AutoTransferer also outperforms baselines like GraphNAS, \n",
            "strength_and_weaknesses": "Strength\n\n1. The idea of utilizing FIM to produce task-feature and further finetune it with ranking loss is novel.\n2. Ablation analysis justifies the effectiveness of the task embedding algorithm proposed by the paper.\n\n\nWeaknesses\n\n1. It seems that \"[AAAI2021] One-shot graph neural architecture search with dynamic search space\" also discussed task transfer. The author may need to compare with one-shot NAS or clarify the difference.\n2. The author pointed out that there are three stages in GNN architectures: 1) pre-processing, 2) message-passing, 3) post-processing. The author only considered message-passing layers and pointed out that it is commonly regarded as the most important part of GNN. However, since different tasks adopt different pre-/post-processing modules, it is not convincing to directly state tat message-passing is the most important.\n3. It will be good if the author can list the performance of SOTA algorithm in each dataset discussed in Table 1. This will help readers understand the gap between NAS models and human-engineered SOTA architectures.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is easy-to-understand.\nQuality: Overall, the paper is technically sound. Weaknesses pointed out in the previous section.\nNovelty: To the best of the reviewer's knowledge, the algorithm of obtaining task embedding is novel \nReproducibility: The reviewer has not checked reproducibility but the author has provided enough details in the appendix.",
            "summary_of_the_review": "Voted for weak accept because the algorithm is novel. However, I still have concerns pointed out in the Weakness section.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3130/Reviewer_uQiP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3130/Reviewer_uQiP"
        ]
    }
]