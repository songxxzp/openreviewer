[
    {
        "id": "deu12VA7RP",
        "original": null,
        "number": 1,
        "cdate": 1666636613994,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636613994,
        "tmdate": 1669042778214,
        "tddate": null,
        "forum": "FkRMv-mlSTy",
        "replyto": "FkRMv-mlSTy",
        "invitation": "ICLR.cc/2023/Conference/Paper6230/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes a novel way to add adaptivity to transformer models. By storing or generating a bank of tokens for each input, the input can be adaptively extended with a variable number of tokens. The method is compared to transformers without adaptivitiy as well as the Universal Transformer. The benchmarks for comparison include computing parity of bit stings as well as image classification. ",
            "strength_and_weaknesses": "This paper proposes a memory bank that can be queried adaptively for a variable number of tokens to append to the input sequence. As a way to add adaptivity to existing neural architectures, this is an alternative to recurrence or weight sharing. There is novelty in this choice, although it isn't adequately contextualized among the existing adaptive methods (listed below). Additionally, the choice of benchmarks is quite odd. While the first task considered, parity, is popular in some early adaptive compute papers, it strikes me as outdated. The latest work -- largely branded as algorithmic reasoning, but with the same focus -- includes methods that can compute prefix sums of binary strings (arguably more work and including the parity task) of many more bits than considered here (listed below). The second task is image classification which is not considered in most other adaptive compute/algorithmic reasoning papers. \n\nIf the goal of this work is to study, explore, and improve adaptivity in neural networks, then the choice of benchmarks and competing models is lacking. If the goal is to augment transformer architectures to improve their performance it aught to be better framed in the text. \n\nStrengths: \n- Creative and novel method for added adaptivity to transformer models\n\nWeaknesses: \n- Missing related work \n    - Adpative methods that use recurrence and solve algorithmic tasks like parity:\n        - [1] Schwarzschild, Avi, et al. \"Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks.\" Advances in Neural Information Processing Systems 34 (2021): 6695-6706.\n        - [2] Veli\u010dkovi\u0107, Petar, et al. \"The CLRS Algorithmic Reasoning Benchmark.\" arXiv preprint arXiv:2205.15659 (2022).\n        - [3] Ibarz, Borja, et al. \"A Generalist Neural Algorithmic Learner.\" arXiv preprint arXiv:2209.11142 (2022).\n    - Adaptive model that use recurrence can compute prefix sums (and therefore parity) of inputs with 512 bits. \n        - [4] Bansal, Arpit et al. End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking. Neural Information Processing Systems (NeurIPS), 2022. \n- Some minor writing issues (not affecting score, only a few examples here): \n    - Middle of Page 4: The following excerpt is not full sentences and has incorrect punctuation. \"For instance, using a smaller patch size for generating the bank can be seen as dynamic\nmulti-scale processing of the input where we can select what fine-grained information from the input\nis useful which is much more efficient than using all the small patches. , which would consume a\nlarge amount of computation.\" \n    - Bottom of Page 4: \"The of-the-shelve ACT ...\" should probably be \"...off-the-shelf...\".\n    - Bottom of Page 4: \"For instance, as shown in algorithm 1...\" where \"algorithm\" should be capitalized.\"\n    - Bottom of Page 5: \"In order to incentives shorter sequences...\" should probably be \"...incentivise...\".\n    - The Appendix has bad references to Figures that are not there.\n\n\n___\n\nIn their reply, the authors addressed all my concerns. I have changed my score accordingly.\n \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, but contains many typos/grammatical errors. The methods are original, however there is no code or promise to release code. The details in the text might make reproducing the findings difficult, although I did not try to reproduce them myself.",
            "summary_of_the_review": "I find this paper well below the threshold for acceptance. The experiments seem out of tune with the related work. Although the exact results support the very narrow claim that this model is better than one existing model, the contribution to the field is limited by the scope of comparison.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6230/Reviewer_vn6P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6230/Reviewer_vn6P"
        ]
    },
    {
        "id": "tVAkk65IJ4",
        "original": null,
        "number": 2,
        "cdate": 1666643813637,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643813637,
        "tmdate": 1666643813637,
        "tddate": null,
        "forum": "FkRMv-mlSTy",
        "replyto": "FkRMv-mlSTy",
        "invitation": "ICLR.cc/2023/Conference/Paper6230/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Paper proposes a new method called AdaTape that enables adaptive computation method. It is applied to vision transformers to achieve dynamic sequence length that will make compute adaptive to the input image. First, a set of learnable or predefined tokens are stored in the Tape bank, then Adaptive tape reader queries tape tokens from it. The method is inspired and built upon ACT proposed for RNNs. The AdaTape is tested on image classification task.",
            "strength_and_weaknesses": "Strength:\n\n- The motivation of the paper is great. Compute should be adaptive with respect to the complexity of input sequence. \n- The method, AdaTape seems to be novel and differences to ACT are clearly mentioned and sufficient. \n- Analysis of the effect of LayerNorm on ACT is insightful.\n- Large scale image classification task results are great to test methodology. \n\nWeaknesses:\n- Comparison to other adaptive networks/methods is missing.\n- Main results are presented with the model trained on JFT-300M that is not a public dataset (to my knowledge) and will make comparison in the future hard. Running main experiments on ImageNet-1K (not few shot) will make the work more valuable for future comparisons. \n- Not clear if the code will be available for reproducibility. \n- More clarification will be helpful for the points below.\n\n\nQuestions:\n- How to make sure that elements in $q$ are sorted by importance? Selecting the first $h$ implies that elements are ranked. Algorithm . \n- Can the compute be controlled by user? For example, during the inference compute is adapted to the current hardware load. Probably changing the stoping criteria will work.\n- Why not to do selection of input tokens instead?  \n- In 2.1 authors mention that ACT uses a linear component $q$ to compute halting score. I am not sure it is linear as it usually has sigmoid at the output to limit the score in 0..1. \n- There works that enable dynamic compute for neural networks. For example, [1], ConvNet-AIG, Dynamic-ViT[3], Avit[2], Adavit[5]. Even more papers are mentioned in [5]. Comparing to these works will help to understand the contribution better. \n\nReferences:\n\n[1] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR, 2017.\n\n[2] Yin, Hongxu, et al. \"A-ViT: Adaptive Tokens for Efficient Vision Transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[3] Rao, Yongming, et al. \"Dynamicvit: Efficient vision transformers with dynamic token sparsification.\" Advances in neural information processing systems 34 (2021): 13937-13949.\n\n[4] Veit, Andreas, and Serge Belongie. \"Convolutional networks with adaptive inference graphs.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n\n[5] Meng, Lingchen, et al. \"AdaViT: Adaptive Vision Transformers for Efficient Image Recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[6] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A survey. TPAMI, 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality - great\nClarity - mostly clear, however, understanding Algorithm 1 is not easy, having a diagram will help\nNovelty - great",
            "summary_of_the_review": "Paper attacks a n important problem of enabling dynamic compute in neural networks. The method is novel and technically sounds. However, have main results on not public dataset limits its contributions. The absence of comparisons to other dynamic compute methods for CNNs/Transformers makes it hard to understand the impact. The code is not available and reproduction of results is difficult. Hopefully, these limitations will be addressed during the rebuttal, I am happy to reconsider my score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6230/Reviewer_pZjZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6230/Reviewer_pZjZ"
        ]
    },
    {
        "id": "is9XuSih9k",
        "original": null,
        "number": 3,
        "cdate": 1666673729099,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673729099,
        "tmdate": 1666673729099,
        "tddate": null,
        "forum": "FkRMv-mlSTy",
        "replyto": "FkRMv-mlSTy",
        "invitation": "ICLR.cc/2023/Conference/Paper6230/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors investigate the problem of adaptive computation time and propose AdaTape, which can adaptively select different function types and computation budgets for different inputs. It incorporates an adaptive tape reader (ATR) to learn or extract variable-length tape tokens for dynamic computation in neural networks. The authors conducted experiments on the parity task and the image classification task. Experimental results show that the proposed AdaTape algorithm outperforms several baselines such as ViT, Universal Transformers, and their variants, while maintaining the same training efficiency.",
            "strength_and_weaknesses": "Strength:\n1. The proposed AdaTape not only adaptively selects different parameters to handle different inputs, but also adjusts the computational budget by using variable-length storage tapes.\n2. The introductory part is well written and engaging.\n\nWeaknesses: \n1. There are quite a few grammatical errors starting from Section 2.2 that make the paper difficult to understand.\n ",
            "clarity,_quality,_novelty_and_reproducibility": "There are quite a few grammatical errors starting from Section 2.2 that make the paper difficult to understand.\n",
            "summary_of_the_review": "The writing (except the introduction section) needs to be significantly improved to enhance readability. It is recommended to add more clarification on the purpose of the tape token and compare with other possible methods that can achieve the same goal. There is no separate related work section. It may be helpful to add that section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6230/Reviewer_V6XW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6230/Reviewer_V6XW"
        ]
    },
    {
        "id": "DV0LQV-dFxS",
        "original": null,
        "number": 4,
        "cdate": 1667214590961,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667214590961,
        "tmdate": 1667214590961,
        "tddate": null,
        "forum": "FkRMv-mlSTy",
        "replyto": "FkRMv-mlSTy",
        "invitation": "ICLR.cc/2023/Conference/Paper6230/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel method to add \"on demand\" computation to transformers by selecting additional tokens from a token bank. In particular the authors use a variation of the adaptive computation time algorithm used in universal transformers to add additional tokens in the input sequence. Experiments with image classification shows that the proposed method can indeed make use of the extra computation to improve the classification results.",
            "strength_and_weaknesses": "Strengths\n-----------\n\n- Adaptive computation is a very interesting and important subject, both in terms of saving computation by only computing as much as it is necessary as well as mimicking the human thought process of considering a problem again and again and collecting more information.\n- Adding extra tokens from a token bank is a very intuitive way to inject variable sized pieces of information to a transformer model.\n\nWeaknesses\n---------------\n\n- AdaTape is significantly slower than the baselines often being 2 times slower. This significantly reduces the usefulness of the method since the baselines with a few extra layers would probably perform as well.\n- The experimental evaluation is not very clearly written. In particular the information about finetuning the datasets except for JFT-300 is missing.\n- The usefulness of the adaptive computation abilities of AdaTape is put in question from the experiments in section 3.4 . In particular we see that in most cases, not using the adaptive length algorithm actually yields better results. It would also be significantly faster on GPUs and TPUs.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper can be improved. In particular, the experimental evaluation should contain more details regarding the training and evaluation on the datasets other than JFT-300M.",
            "summary_of_the_review": "The paper seems a great relatively novel idea, however the experimental evaluation is lacking. The comparisons show that using the proposed method is actually significantly slower albeit better in terms of final accuracy. However, not using the adaptive tape reading algorithm actually performs better and would be possibly faster in modern accelerators.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6230/Reviewer_rSKm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6230/Reviewer_rSKm"
        ]
    }
]