[
    {
        "id": "pPVnSINJ3TH",
        "original": null,
        "number": 1,
        "cdate": 1666462372025,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666462372025,
        "tmdate": 1668709845144,
        "tddate": null,
        "forum": "G-uNfHKrj46",
        "replyto": "G-uNfHKrj46",
        "invitation": "ICLR.cc/2023/Conference/Paper3557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the gap between random-feature-based attention (RFA) and standard attention. The paper characterizes the gap using control variates and proposes a novel efficient attention mechanism based on the analysis. The authors evaluate their method EVA in terms of model quality, efficiency, and runtime.",
            "strength_and_weaknesses": "+ A nice analysis of RFA that provides some insight into what the method is doing.\n+ Interesting method building on the analysis.\n\n- Some weak empirical results suggest that this method may not scale to larger models (this is common for RFA-based efficient attention mechanisms).\n- A couple missing baselines.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well-written.\n\nQuality: Overall good, the evaluation methodology is standard and complete (thank you for evaluating runtime instead of just FLOPs). A few concerns:\n\n1. There are gaps between EVA and standard attention in language modeling on Books3 and Wikitext-103. It appears the gap on Books3 can be closed with a larger model, but the gap on WikiText3 has not been cloased (even when reducing the parameters of the attention model). This does not bode well for scaling to larger models -- can you try to close the gap more or explain why a gap remains?\n\n2. The results on efficiency are very exciting! It would be stronger to also compare against some recent \"memory-efficient attention\" mechanisms, such as those in xformers or FlashAttention. I believe those methods will also have memory that scales linearly (but compute scaling quadratically).\n\nNovelty: The contribution is a little bit incremental, but I think it provides some nice insight and recontextualization of existing methods.\n\nReproducibility: Seems clear from the paper.",
            "summary_of_the_review": "Overall I think this paper makes a positive contribution to the literature. The results also show minor improvements over previous efficient attention mechanisms. A few gaps and baselines remain, but I think this paper is above the acceptance threshold.\n\nUpdate after rebuttal: my concerns have been addressed, so I am increasing my score to an 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3557/Reviewer_xJRA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3557/Reviewer_xJRA"
        ]
    },
    {
        "id": "MeJxV4YxuP",
        "original": null,
        "number": 2,
        "cdate": 1666634207863,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634207863,
        "tmdate": 1666634207863,
        "tddate": null,
        "forum": "G-uNfHKrj46",
        "replyto": "G-uNfHKrj46",
        "invitation": "ICLR.cc/2023/Conference/Paper3557/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Firstly, as in Zheng 2022, the authors interpret the computationally efficient RFA (randomized feature-based attention) as a SNIPS estimate (self-normalized importance sampling estimate). They then use Vlassis' control variate formulation of SNIPS to link exact attention with RFA via a single global control covariate coefficient. This naturally suggests using more control coefficients to obtain more accurate coefficients while maintaining efficiency in the form of exact coefficient estimates for certain keys most correlated with the query and coefficients per cluster for the remaining keys. This new approach is termed EVA and empirical demonstrations of its efficacy are quite convincing. On ImageNet using the DeiT-Tiny-784 architecture, EVA outperforms Zheng's LARA approach and other forms of efficient attention in Table 2, while in Table 1, using a different architecture, EVA outperforms all efficient attention variants, but not the slower exact attention mechanism itself. In 3 NLP applications (MLM, MT, Autoregressive LM), EVA again outperforms other efficient variants in terms of BLEU score or test perplexity, although there were issues with replicating Kasai's perplexity for exact attention.",
            "strength_and_weaknesses": "Strengths\n\n1.\nThe proposed EVA approach is motivated quite well and the paper is quite clear.\n\n2.\nThe theoretical results (Propositions 1, 2 and ) in the paper are impressive and appear to be quite sound.\n(I didn't check the extensive appendix though.)\n\n3.\nThe experimental validation is quite convincing on a variety of problems.\n\nWeaknesses\n\n1.\nCertain choices are not well-motivated. For example, rather than learning the partitioning scheme via a K-means-like algorithm for the keys receiving clustered coefficients (instead of optimized control coefficients), the clusters (partitions) are chosen as contiguous chunks. This is called out in the final section though.\n\n2.\nIt's not clear how hyper-parameters such as E and C in EVA were tuned.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally quite clear apart from the weaknesses pointed out above.\n\nThe quality of the theoretical and experimental results is extremely high.\n\nIn this reviewer's opinion, the novelty is high. Connecting the attention mechanism with known results on control variates and proposing practically efficient decompositions is non-trivial.\nThe authors have provided an interesting derivation of ScatterBrain as a special case of their control variate approach in Appendix G. \n\nThe github link to the code is apparently not shared and is hence an impediment to reproducibility.",
            "summary_of_the_review": "I strongly recommend acceptance on account of the novel theoretical results and the extensive convincing experimental results.\nIt should be straightforward to address the minor concerns regarding clarity and reproducibility noted above.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3557/Reviewer_KXnA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3557/Reviewer_KXnA"
        ]
    },
    {
        "id": "VUfWtJ4G-zy",
        "original": null,
        "number": 3,
        "cdate": 1666678046980,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678046980,
        "tmdate": 1666678124382,
        "tddate": null,
        "forum": "G-uNfHKrj46",
        "replyto": "G-uNfHKrj46",
        "invitation": "ICLR.cc/2023/Conference/Paper3557/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work improves Random-feature-based attention through the lens of control variates. The paper develops a more flexible form of control variates, which forms a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. The EVA achieves competitive results on several tasks. ",
            "strength_and_weaknesses": "Strength:\n\n- The paper develops an efficient attention mechanism EVA via control variates. This novel attention mechanism is efficient and can bridge the gap between RFA and exact softmax attention. The EVA attains a good trade-off between modeling quality and efficiency. \n\n\nWeakness\n\n- The author should compare more state-of-the-art linear attention like Flowformer or FLASH.",
            "clarity,_quality,_novelty_and_reproducibility": "The reviewer agrees that the work tackles an essential problem. However, several parts need further clarification by the authors to conclude the contributions of the work.",
            "summary_of_the_review": "In this paper, the author proposes an efficient attention mechanism called EVA with the help of control variates.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3557/Reviewer_B8ci"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3557/Reviewer_B8ci"
        ]
    },
    {
        "id": "OHNftQbYjT",
        "original": null,
        "number": 4,
        "cdate": 1666678115482,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678115482,
        "tmdate": 1666678115482,
        "tddate": null,
        "forum": "G-uNfHKrj46",
        "replyto": "G-uNfHKrj46",
        "invitation": "ICLR.cc/2023/Conference/Paper3557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose an efficient attention EVA, where the work reals that exact softmax attention can be recovered from RFA by manipulating each control variate. The mathematic analysis is provided to prove the dissecting RFA with control variates. And the authors also implemented their EVA and applied it to image classification and natural language tasks, showing performance boosting over SOTA. ",
            "strength_and_weaknesses": "Strength\n- The idea is easy to follow, and the paper is well-written. \n- The mathematic analysis is given to explain the dissecting RFA with control variates.\n- The EVA attention with simplification achieving higher accuracy than softmax in image classification is impressive. \n\nWeakness\n\n- Overall, I would say the paper is in good shape in terms of detailed mathematic analysis and solid experiments on multiple vision and natural language tasks. My main concern is about the usage in the practical. Though the EVA achieves higher accuracy (very subtle) than softmax in table 1, the softmax is still a better choice in terms of accuracy, #params, and FLOPs. According to numbers reported in all tables, softmax still dominates the performance and has comparable #param and FLOPs to EVA. \n- The not-well-designed approximation in computing control variate could impact the EVA's performance. As no code is provided, it is hard to trace the author's implementation. ",
            "clarity,_quality,_novelty_and_reproducibility": "The EVA implementation includes the heuristics to approximate control variate computing. It might be not easy to reproduce author's results without code provided. ",
            "summary_of_the_review": "Though in terms of accuracy/#param/Flops, the EVA might not beat SoftMax completely, the work with theoretical evidence for approximating the  SoftMax attention can provide insights into the future works of light-weight attention development. The paper is in well-written, and the experiments are solid. Hence, the work reaches the acceptance bar. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3557/Reviewer_rNYj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3557/Reviewer_rNYj"
        ]
    }
]