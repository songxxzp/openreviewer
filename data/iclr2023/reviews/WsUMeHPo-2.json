[
    {
        "id": "UJiRbadp-26",
        "original": null,
        "number": 1,
        "cdate": 1666716757310,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666716757310,
        "tmdate": 1668664534926,
        "tddate": null,
        "forum": "WsUMeHPo-2",
        "replyto": "WsUMeHPo-2",
        "invitation": "ICLR.cc/2023/Conference/Paper4074/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This manuscript focuses on the problem of learning better aggregation attention. The motivation comes from the observation that there is no clear winner of the existing three aggregation methods, including native GCN and GAT. The authors propose a new CAT aggregation method and merge it with GCN and GAT. The proposed method is evaluated on several standard datasets to show its effectiveness.",
            "strength_and_weaknesses": "Strength\n\n1, The motivation that different data may need different aggregation makes sense.\n\n2, The analysis of the evaluation of $\\lambda_1$ and $\\lambda_2$ is interesting.\n\nWeaknesses\n\n1, The comparison is insufficient. Only two methods are compared on most of the datasets: GCN and GAT. The weak comparison makes it hard to support the effectiveness of the proposed method.\n\n2, The novelty is limited. The core idea of the proposed method is to integrate several different aggregation strategies. There are already several research works along this line like the sampling method proposed in [1]. \n\n3, In terms of technical detail, it seems that the proposed method is designed only for native GCN and GAT. It is hard to generalize the proposed method to other kinds of aggregations. It is more like another new aggregation scheme, which limits the contribution.\n\nOther issues:\n\nThe highlights of Table.2 are improper.  E.g. the GCN performs the best on proteins while the L-CATv2 is highlighted.\n\n[1] Meta-Aggregator: Learning to Aggregate for 1-bit Graph Neural Networks, ICCV 2021 ",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the above reviews. ",
            "summary_of_the_review": "In short, the main concerns of this manuscript are two parts: the insufficient evaluation and the limited novetly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4074/Reviewer_CSVs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4074/Reviewer_CSVs"
        ]
    },
    {
        "id": "Mt3S-ChcxhT",
        "original": null,
        "number": 2,
        "cdate": 1666773370356,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666773370356,
        "tmdate": 1666773370356,
        "tddate": null,
        "forum": "WsUMeHPo-2",
        "replyto": "WsUMeHPo-2",
        "invitation": "ICLR.cc/2023/Conference/Paper4074/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes graph Convolutional Attention Networks (CAT) to exploit the advantages of Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). Further, the authors design Learnable graph Convolutional Attention Networks (L-CAT), which softly selects GCNs, GATs, and CAT. The paper presents theoretical analysis of CAT and conduct node classification tasks using ten datasets.",
            "strength_and_weaknesses": "**Strengths**\n\n(1) This paper shows the effectiveness of proposed methods with theoretical analysis. \n\n(2) The proposed CAT shows consistently good performance across datasets.\n\n(3) Experiments on synthetic datasets prove the CAT\u2019s effectiveness compared to GCNs and GATs. \n\n**Weakness** \n\n(1) The formulation GCNs is different from the formulation proposed in [1] on undirected graphs. Since the datasets used in experiments are undirected graph, it is more appropriate to use $\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$.\n\n************************Additional Questions************************\n\n(1) I wonder the performance of GCN, GAT, and CAT when using more than 1 layer on the synthetic dataset.\n\n---\n\n[1] Fountoulakis, Kimon, et al. \"Graph Attention Retrospective.\"\u00a0arXiv 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "**************Clarity**************\n\nThe paper is clearly written to understand overall method.\n\n**************Quality**************\n\nSome experiments for baselines are not properly conducted. \n\n**********Novelty**********\n\nThe proposed method has a fair novelty. \n\n******************************Reproducibility******************************\n\nThe paper has good reproducibility.",
            "summary_of_the_review": "Overall, I think this paper is marginally over the acceptance threshold. The paper demonstrates the effectiveness of their methods through theoretical analysis. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4074/Reviewer_kUTn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4074/Reviewer_kUTn"
        ]
    },
    {
        "id": "oryOk8Tta-c",
        "original": null,
        "number": 3,
        "cdate": 1666863363267,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666863363267,
        "tmdate": 1666863363267,
        "tddate": null,
        "forum": "WsUMeHPo-2",
        "replyto": "WsUMeHPo-2",
        "invitation": "ICLR.cc/2023/Conference/Paper4074/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Combining Graph Convolutional Networks (GCNs) with Graph Attention Trees (GATs), the authors propose a novel architecture for Graph Neural Networks, Learnable Convolutional Attention Layer (L-CAT).\n\n\nIn order to bridge the gap between the graph convolutional layer, the graph attention layer, and the graph convolutional attention layer, this paper introduces a new layer type for graph neural networks.\n\n\nThey add two scalar parameters per layer and learn them along with the other parameters in an end-to-end fashion. Inspiring theoretical conclusions based on prior work are presented, and the authors' suggested strategy is experimentally validated. ",
            "strength_and_weaknesses": "- The overall quality of the writing and organization of the work is quite high. \n\n- The background of the study and the significance of the contribution are laid out in great detail. \n\n- The second and third\u00a0parts (preliminaries and limitations of GCN and GAT)\u00a0offer\u00a0an insightful summary of GNNs. \n\n- Findings from theory are addressed, and they are also confirmed by empirical research.\n\n- In general, improving graph neural networks by interpolating across different layer types using parameters that can be learned in an end-to-end manner seems promising.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As described in the above section, the motivation and the problem definition are clear.\n\nTheir approach is logical and seems to have sufficient applicability.",
            "summary_of_the_review": "I recommend to accept the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4074/Reviewer_UgAb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4074/Reviewer_UgAb"
        ]
    }
]