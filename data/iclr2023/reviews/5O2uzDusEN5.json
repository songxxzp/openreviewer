[
    {
        "id": "Vb9EMOiJ5z",
        "original": null,
        "number": 1,
        "cdate": 1666142211990,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666142211990,
        "tmdate": 1666142211990,
        "tddate": null,
        "forum": "5O2uzDusEN5",
        "replyto": "5O2uzDusEN5",
        "invitation": "ICLR.cc/2023/Conference/Paper1279/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a differentiable data generation pipeline for optical flow which depends on a target optical flow dataset, a common base dataset and prior data generation constraints on grid warping and texture noise. The data generation process is much more efficient than existing work of AutoFlow and the method achieves state-of-the-art performance on public Sintel benchmark.",
            "strength_and_weaknesses": "Strength:\n1. The fully differentiable data generation process is interesting and efficient which includes various components including color perturbation, geometric warping, flow field translation, layer composition and real-world effects. \n2. The experiment on the analysis of each data generation component is clear and convincing.\n\nWeakness:\n1. The data generation process of DFlow needs a target dataset and a common base dataset on which the proxy network models are trained in fully supervised way, this makes the generated dataset somewhat task-dependent. \n2. The declaration of SOTA performance on the Sintel public benchmark is not correct: recently there are lots of published optical flow algorithms achieved better performance than RAFT, please ref to: http://sintel.is.tue.mpg.de/results for more details. Table 5 shows comparison with RAFT but does not show SOTA performance, is it possible to apply DFlow to more recent deep network structures?\n3. It is not clear whether the dataset size of DFlow affects the optical flow estimation result and whether the data generation process can use multiple target datasets for better generalization on pre-train or better fine-tune performance.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The presentation is not very clear and sometimes confusing, i.e. in Sec 3.1 page 4 \"We hypothesize that, since a snippet of the target dataset is typically on small-scale in general scenarios\", and \"For example, KITTI 2015 consists of just 200 number of training samples\", but the DFlow's target dataset is Sintel which is not small.\n2. In Sec. 3.3. \"Implementation details can be found in the appendix\", it would be good to add \"A.5\"\n3. In Table 3. the DFlow (Ours) targeted on Sintel achieved best performance on KITTI 2015, but worse performance on Sintel final, it would be good to add a few sentences for explanation.\n4. The novelty of the proposed method is OK but the reproducibility is not good since the training of data generation pipeline is complex and may need considerable parameter tuning and training adjustment, public open of training source code is neccesary for good reproducibility.",
            "summary_of_the_review": "The idea of differentiable data generation is interesting but the experiment result is not impressive and the reproducibility is not good.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1279/Reviewer_dkLC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1279/Reviewer_dkLC"
        ]
    },
    {
        "id": "HdK4-ELBBtm",
        "original": null,
        "number": 2,
        "cdate": 1666661999806,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661999806,
        "tmdate": 1670463708560,
        "tddate": null,
        "forum": "5O2uzDusEN5",
        "replyto": "5O2uzDusEN5",
        "invitation": "ICLR.cc/2023/Conference/Paper1279/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a differentiable pipeline for learning an optical flow dataset. Given a pre-defined set of dataset generation parameters and network, the method optimizes the variable using an optical flow loss in an end-to-end manner. Comparing to the previous work, the method achieves competitive accuracy but mostly better accuracy. Especially in terms of computational cost (e.g., training time), the method is an order of magnitude more efficient than the previous work. ",
            "strength_and_weaknesses": "__Strength__\n\n- Computational efficiency\n  Comparing with AutoFlow, the method is substantially more efficient (ref. Table 1). It enables the algorithm to run on a small-scale experiment setup. \n\n- Detail ablation study\n  The paper provides clear & detail ablation study for each design choice. It makes paper more transparent.\n\n---\n\n__Weakness__\n\n- I don't see any significant weakness of the paper so far. Rather there are some details that are better to be clarified. (continued in the following section).",
            "clarity,_quality,_novelty_and_reproducibility": "__Clarity__\n\n- (In Section 3) Optimization: I wonder how optimization works. Does the method jointly optimize network parameters and dataset parameters together? I didn't fully understand how they work. For example on Page 4, the paper introduces a base network. How the two networks and the parameters are optimized together? It would be good if any pseudo source code is provided for an explanation (for example, at each step, what is optimized, which training dataset is used, which test dataset is used for evaluation, etc..)\n\n- (Eq. 6) Is there any ablation study for the layer decomposition? (Softmax splatting vs. alpha blending)\n\n- (Just an idea) (Table 6) Is it possible to optimize the binary setups in a differentiable manner? (e.g., learning a continuous variable between 0 and 1 as a probability for the On or Off selection).  It's because some binary setups show different results on different datasets (e.g., Noise regularization, Fog, Motion blur, etc..), it would be also great if those are optimizable as well.\n\n---\n__Others__\n\n- Quality: Good quality. Sufficient figures and numbers for a clear understanding of the paper\n\n\n- Novelty: Somewhat significant, considering its performance in general.\n\n- Reproducibility: Despite clear details included, it would really help to have source code to reproduce the result. It may be difficult without it.",
            "summary_of_the_review": "The paper shows empirically good results, great efficiency, and it provides detailed ablation study. However, it was difficult for me to understand some technical parts (see the clarity section above). I would like to raise my rating after clearing up those concerns through a discussion phase. \n\n----\n\n__(Post discussion)__\n\nI would like to raise my rating to '8: accept, good paper'. The authors' response resolved my main concern about the clarity of the paper. I also read other reviews and responses on them, and they seem fairly well addressed. The paper mainly demonstrates empirically good accuracy over the direct competitor (AutoFlow) with better training efficiency. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1279/Reviewer_tPfm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1279/Reviewer_tPfm"
        ]
    },
    {
        "id": "NIW0kgKe5Py",
        "original": null,
        "number": 3,
        "cdate": 1666669750257,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669750257,
        "tmdate": 1666669823004,
        "tddate": null,
        "forum": "5O2uzDusEN5",
        "replyto": "5O2uzDusEN5",
        "invitation": "ICLR.cc/2023/Conference/Paper1279/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new data generation pipeline for training optical flow networks. The pipeline consists of parameterized differentiably geometric warping, flow field translation, color perturbation, and real-world effects. It also proposes a new objective function that drives the data optimization by leveraging the compressed knowledge of the proxy networks pre-trained on target and base datasets, respectively. Optical flow models trained on the datasets achieve favorable or superior performance against the competing datasets on pre-training and fine-tuning experiments.  ",
            "strength_and_weaknesses": "Strength: the problem this work focuses on is meaningful and useful. The authors do propose a solid and effective solution for optical flow data generation. \n\nWeakness: Limiation and future work should be discussed.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality is good. \nPaper Clarity is good. \nTechnical novelty is marginal but makes sense for the target problem. \nIt can be reproduced. ",
            "summary_of_the_review": "My overall rating for this work is positive. The solid and comprehensive experimental results show the method and the generated datasets would benefits the community.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1279/Reviewer_iPLs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1279/Reviewer_iPLs"
        ]
    },
    {
        "id": "tNsE7UFSv_",
        "original": null,
        "number": 4,
        "cdate": 1666775696157,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666775696157,
        "tmdate": 1666775943403,
        "tddate": null,
        "forum": "5O2uzDusEN5",
        "replyto": "5O2uzDusEN5",
        "invitation": "ICLR.cc/2023/Conference/Paper1279/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposed a differentiable optical flow data generation pipeline and a loss function to drive the pipeline. The proposed modules enable automatic and efficient synthesis of a dataset effectively to a target domain, given a snippet of target data. This distinctiveness is achieved by proposing an efficient data comparison method. Experiments show the competitive performance of DFlow against the prior arts in pre-training.",
            "strength_and_weaknesses": "Strength:\n\n+ A simple and efficient differentiable data generation pipeline for optical flow.\n\n+ A contrastive-style learning scheme and its loss function by approximating expensive dataset-todataset comparison to leverage proxy neural networks.\n\nWeaknesses:\n\n- The claim that \"the RAFT model pre-trained with DFlow achieves state-of-the-art performance on the Sintel public benchmark in fine-tuning\" is questionable. According to the benchmark in Sintel. The performance of \"DF-RAFT\" is even not as good as the orignal RAFT.\n\n- The paper did not discuss the limitations of the proposed approach.\n\n- There are some recent work generating optical flow dataset in a rather cheap way such as :\nRealFlow: EM-Based Realistic Optical Flow Dataset Generation from Videos, ECCV 2022.\nYunhui Han, Kunming Luo, Ao Luo, Jiangyu Liu, Haoqiang Fan, Guiming Luo, Shuaicheng Liu",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper writing is general clear.\n\nQuality: The paper quality is good in general.\n\nNovelty: The paper owns its novelty in generating optical flow dataset. However the benefits of the new dataset has not been fully validated.\n\nReproducibility: The proposed method is relatively complex. It is not very easy to reproduce the results.\n",
            "summary_of_the_review": "The paper proposed a differentiable optical flow data generation pipeline (DFlow) and a loss function to drive the pipeline. This is a simple and efficient differentiable data generation pipeline for optical flow. It also proposed a contrastive-style learning scheme and its loss function by approximating expensive dataset-todataset comparison to leverage proxy neural networks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1279/Reviewer_uY3h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1279/Reviewer_uY3h"
        ]
    }
]