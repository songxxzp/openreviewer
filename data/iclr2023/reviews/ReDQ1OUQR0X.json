[
    {
        "id": "8ZxLRnMNIqP",
        "original": null,
        "number": 1,
        "cdate": 1666577317524,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577317524,
        "tmdate": 1669400177575,
        "tddate": null,
        "forum": "ReDQ1OUQR0X",
        "replyto": "ReDQ1OUQR0X",
        "invitation": "ICLR.cc/2023/Conference/Paper2842/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors investigated the alignment between DNNs' representations of objects in the THINGS dataset and humans' judgments of those same images. They found that the newest and largest-scale models trained on larger-than-imagenet datasets like the JIT-300 or trained with caption embeddings (e.g., CLIP) performed the best. They complement this finding with several surveys to identify the architectural/training choices that drive performance on THINGS triplet predictions, and analyze the concepts captured by DNN representations. \n\n\nUpdated:\n\nThe additional experiments done over the rebuttal period strengthen this paper and make it a good and rigorous contribution for AI and the brain sciences.",
            "strength_and_weaknesses": "Strengths\n- Awesome introduction.\n- I am all for evaluations that assess how well today's models are explaining human perception, so I appreciate that contribution.\n\n\nWeaknesses\n- There's no finding here that moves the field of cog/vis sci or AI. The authors look at odd-one-out accuracy on the things dataset. They test a bunch of models. Some do better than others.\n- The potentially most interesting finding is that \"scaling ImageNet\nmodels does not lead to better alignment of their representations with human similarity judgments.\" However, the authors also found that pretraining models on bigger-than-imagenet datasets (like JIT-300) does improve alignment. Of course those large-data models are also bigger, so the deleterious effect of scale that they found can be easily counteracted by pursuing the scaling laws that have taken hold of the field today \u2014 there's no novel prescription that falls out of this work.\n- The gap that the authors note between the optimal score of 67% and models' scores of 57-58% is numerically small but potentially important. However, unlike similar work like Geirhos et al., 2021 \"Partial success in closing the gap...\" which focused on OOD generalization for humans and models, it's a stretch to find a similar utility for the things dataset triplet scores.\n\n- The analysis on what model specs affect odd-one-out accuracy could be refined. \"Varying objective\" is a combination of varying losses, augmentations, and regularizations. \"Varying architecture\" similarly has a lot of stuff going on at once. \"Model size\" is not just model size, but rather confounded with the many different bells-and-whistles in ResNet vs. VIT vs. VGG. I think this analysis should be redone. Choose one model architecture, like ResNet, and in separate plots, look at how different losses affect performance vs. different regularizations vs. different augmentations vs. varying depth vs. varying width vs. varying self supervision objective. Right now there's too many hidden differences between the models that go into each of the subplots in Fig. 3 to interpret those results.\n- The authors describe odd-one-out accuracy as being higher in the final vs. penultimate layer of the network. It would be helpful to understand why this finding is significant. Do we learn anything about human vision based on which layer the accuracy is the highest?\n\nMinor\n- The results are written to sound like extended figure captions (\"The plot in the bottom left corner of Figure 3 compares...\"). It would help readability if the results and findings were woven into the overall narrative, and figures were referenced parenthetically.\n- I think it would help interpretability if you plotted scores normalized by the optimal performance. In other words, models achieve X% of the optimal score \n- \"Prior work has investigated this question indirectly, by measuring models\u2019 error consistency with humans (Geirhos et al., 2018; Rajalingham et al., 2018; Geirhos et al., 2021) and the ability of their representations to predict neural activity in primate brains (Yamins et al., 2014; Guc\u00b8l \u00a8 u &\u00a8 van Gerven, 2015; Schrimpf et al., 2020), with mixed results.\" Can you add a sentence clarifying what you mean by mixed results.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Very clear writing. \n\nThe work involves running an existing dataset (THINGS) through pre-trained DNNs. It seems to be the first time this has been done but the question/answers here do not feel very novel.\n\nThe work looks to be highly reproducible.",
            "summary_of_the_review": "I think the paper is either a preliminary report pointing towards bigger questions about understanding human vision and how DNNs can be improved as models of human vision or, in its current form, a solid workshop paper. There is nothing here that changes my thinking about human or machine vision and so I do not feel strongly about supporting its acceptance into ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2842/Reviewer_u84n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2842/Reviewer_u84n"
        ]
    },
    {
        "id": "xiTKlQDLM1",
        "original": null,
        "number": 2,
        "cdate": 1666666535850,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666535850,
        "tmdate": 1667789486545,
        "tddate": null,
        "forum": "ReDQ1OUQR0X",
        "replyto": "ReDQ1OUQR0X",
        "invitation": "ICLR.cc/2023/Conference/Paper2842/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores the how human's representation of visual images is aligned with network representations. \nFor this goal, this paper utilized an \"odd-one-out\" test setting. This paper built a sparse Bayesian model to represent human's responses. And utilizes multiple methods such as zero-shot, linear probing and regression. By comparing the choices from the machine features under these distance metrics to human's choices. This paper identified factors in deep models that affect the alignments.  \n",
            "strength_and_weaknesses": "This paper is in good flow so that I can easily follow. \nThe direction of research is inherently interesting and could be of great importance for many researches such as network interpretability. \nThe experiment design is comprehensive. I think this would be a very solid paper if I had agreed with the methodologies provided by the paper (see the reason below). \n",
            "clarity,_quality,_novelty_and_reproducibility": "Despite the clear flow. There are some ambiguities that obstructs the understanding:  the consistency among users were not provided: is it 67.22 (sec 3)?\n\n",
            "summary_of_the_review": "I fully respect the effort in this paper and I think all the experiments seem solid.\nHowever, I strongly disagree with the method utilized by this paper to measure the alignment between the two representations. \nI believe one acceptable way would be measuring both representations based on separate signal sources (human representation from EEG etc), then analyze  the correlation. And due to the complexity, human representation should not be only based on one task.\nWe are still not able to build up a real human representation. The models proposed by the paper are based on representations of human behavior (decisions). This representation, however, is only based on observations, not based on cog signals. And even for the representation of human decisions, there can be many different representations, Why is the odd-on-out method the right way?\nIn addition, as also pointed out by the paper in the discussion section, the conclusion on machine/human feature similarity is heavily dependent on the metrics used. With a slight different metric, there are can be totally different conclusions.  \n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2842/Reviewer_hcki"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2842/Reviewer_hcki"
        ]
    },
    {
        "id": "A4nC0BQJIH",
        "original": null,
        "number": 3,
        "cdate": 1666694810622,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694810622,
        "tmdate": 1666694810622,
        "tddate": null,
        "forum": "ReDQ1OUQR0X",
        "replyto": "ReDQ1OUQR0X",
        "invitation": "ICLR.cc/2023/Conference/Paper2842/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the degree representations of computer vision models are aligned with human judgements of similarity, and find that changes in model size and architecture have little effect on the degree of a model's alignment, while training data does. The authors show this result via a thorough set of experiments comparing many models and settings, measuring human aligning via annotations from an odd-one-out task. ",
            "strength_and_weaknesses": "S1: Paper is very clear and easy to follow. \n\nS2. The paper conducts a very systematic and thorough investigation across a variety of different models architectures and settings, thus strongly supporting their claim that scale does not result in improved human alignment. \n\n\nW1: Why is the odd-on-out method the right way to measure human alignment of representations (versus pair similarities as in Peterson (2018) or other approaches)? Given that the paper's primary goal is to make a thorough claim on what properties (e.g. scale, objective) affect the learned representations alignment with humans, there is currently not much discussion on why the triple task is the best way to measure alignment. \n\nW2 (minor): The reference triplet task for CIFAR-100 appears quite different in spirit from the triplet task with human responses on the THINGS dataset, so I'm not sure how useful it is as a baseline to compare with. The process itself is explicitly \"coarse\" class driven, unlike the human judgements which aren't necessarily driven by class/object. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/Quality: Strong\n\nNovelty: There has been a lot of work on learning \"human-compatible representations\" for specific tasks and downstream settings, so I think the main novelty of this paper is not the consideration of whether representations are aligned with human perceptual representations, but rather whether properties such as scale affects them. I largely consider this limited novelty, as its focused on a very specific measure of human alignment. \n\nReproducibility: The THINGS dataset actually used seems to be different from the examples the authors are able toe show, but barring that everything else seems reproducible. \n",
            "summary_of_the_review": "I think the paper is a great contribution to the ICLR community, both from its thoroughness in experiments and its timeliness as we seek to understand the full benefits of large, pre-trained models and the role of pre-training data. My primary concern is the assumption that the triplet task is fully representative of \"human alignment\" as a general property, which can be addressed with more discussion, and perhaps examples from the annotation task, with which I'm happy to increase my score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2842/Reviewer_fmAV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2842/Reviewer_fmAV"
        ]
    }
]