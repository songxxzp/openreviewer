[
    {
        "id": "OjKH29Wn_L",
        "original": null,
        "number": 1,
        "cdate": 1666243563200,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666243563200,
        "tmdate": 1666243596028,
        "tddate": null,
        "forum": "rfvuuHmqHOQ",
        "replyto": "rfvuuHmqHOQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4555/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a model perturbation strategy PERTURBGCL to perform graph contrastive learning without data augmentation. It consists of two key components: message propagation and transformation. They design weightPrune on transformation weight according to magnitudes, which create a dynamic perturbed model to contrast with the target encoder. Then they present randMP on the steps of message propagation. Finally they use above two strategies together to perform effective contrastive learning on graphs.",
            "strength_and_weaknesses": "Strength:\n1. The paper proposed a novel model perturbation framework, consisits of weightPrune on transformation and randMP on message propagation.\n2. The authors conduct extensive experiments for both node-level classification task and graph-level classification task on benchmark datasets to demonstrate the superiority of PERTURBGCL. \n\nWeaknesses:\n1. The paper only analyzes the flaws of the model perturbed baseline SimGRACE, and the ablation experiment also confirmed that PERTURBGCL is superior to the perturbation of Gaussian noise. However, there is no specific theoretical analysis of the reasons why the design of the two perturbation methods can make contrastive learning obtain better performance.\n2.  Although the paper mentions that the lottery hypothesis: the sparse subnetwork can achieve performance comparable to dense network with fewer model parameters. In my opinion, this advantage comes from iterative magnitude pruning and rewind. The weightPrune is a dynamic pruning with mask updates, and these two elements do not exist. Therefore, please explain what is the source of the advantage of creating perturbed branches with weightPrune.\n3.  What is the difference between k times of message passing and data augmentation at the edge level?\n4.  The author pointed out that weightPrune actually regularizes the weight of the model. Then how about directly increasing the regularization, such as increasing the weight decay?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. The paper is well organized but the presentation has minor details that could be improved.\n\nQuality: Good. The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.\n\nNovelty: Fair. The paper contributes some new ideas or represents incremental advances.\nReproducibility: Fair. Key resources (e.g., proofs, code, data) are unavailable and/or some key details (e.g., proof sketches, experimental setup) are unavailable which make it difficult to reproduce the main results.\n",
            "summary_of_the_review": "The authors propose a model perturbation strategy PERTURBGCL to perform efficient contrastive learning instead of data augmentation. The method develops more effective model perturbation framework to generate contrastive views. The authors conduct extensive experiments to demonstrate the superiority of PERTURBGCL. However, in this paper, there are still some core points that have no theoretical support, no specific proof or analysis. The survey on the latest model perturbation research for contrastive learning is not sufficient.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4555/Reviewer_4KLm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4555/Reviewer_4KLm"
        ]
    },
    {
        "id": "TYiDgZcQZz3",
        "original": null,
        "number": 2,
        "cdate": 1666613076053,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613076053,
        "tmdate": 1666613126152,
        "tddate": null,
        "forum": "rfvuuHmqHOQ",
        "replyto": "rfvuuHmqHOQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4555/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, authors propose a new augmentation based on graph neural networks for graph contrastive learning. They decompose the graph neural networks as two parts: the diffusion part and the transformation part. Based on these, they propose weightPrune for the transformation and randMP for the diffusion part. They call the technique as model pertubation and evaluate it on 15 benchmarks.",
            "strength_and_weaknesses": "Strength:\n1. This paper suggests a possible direction for research. Do not focus on raw graph data like attributes and adjacent matrix for augmentation rather than graph neural networks. And the method is more flexible than simGRACE.\n2. weightPrune is a new way for augmentation in GCL. \nWeakness:\n1.  First of all, this graph augmentation has no theoretical support, at least in my opinion, it should not be difficult to prove the alignment loss bound before the transformation.\n2. Because of the lack of theoretical analysis, there is no way to draw a conclusion as to what this graph augmentation actually does, what is its significance, and what does it tell us about future graph augmentation for models?\n3. Multi-hops diffusion based augmentation is not new, MVGCL has proposed.\n4. In my opinion, weight pruning is also a noise (biased salt noise rather than gaussian). Therefore the paper lacks a comparison with other noise methods, such as Gaussian noise.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall I think the quality of the paper is good overall and it is also very clear. But I don't think either the novel, or the technical contributions are up to an acceptable ICLR paper. There is a lack of novel solutions and no good theoretical support to give some insight, and the results of the experiments are not really surprising.",
            "summary_of_the_review": "Between my comments above, I am personally inclined to reject this paper. Of course I am happy to see other different opinions before making a decision.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4555/Reviewer_SXei"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4555/Reviewer_SXei"
        ]
    },
    {
        "id": "JqWW4eon-SM",
        "original": null,
        "number": 3,
        "cdate": 1667102625592,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667102625592,
        "tmdate": 1667103100500,
        "tddate": null,
        "forum": "rfvuuHmqHOQ",
        "replyto": "rfvuuHmqHOQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4555/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors propose a principled framework, namely PerturbGCL, for augmentation-free graph contrastive learning. Considering that GNN can be divided into the two parts of message propagation and feature transformation operations, the authors develop two tailored perturbation strategies, namely randMP and weightPrune, to effectively disturb GNN. Through extensive experiments on the node classification task and graph classification task, the authors show that PerturbGCL can achieve competitive results against the SOTA graph contrastive learning methods.",
            "strength_and_weaknesses": "Strength\n\nS1: The proposed method is simple and effective.\n\nS2: The authors verify PerturbGCL on two downstream tasks.\n\nS3: The authors provide the code of PerturbGCL.\n\nWeakness\n\nW1: The paper is not well written. There are lots of syntax errors and unclear representation in this paper.\n1. In Section 2.1 (Page 2), where the i-th row of X denote -> where the i-th row of X denotes; in Page 4, Figure 2 provide the overview our framework.-> Figure 2 provides the overview of our framework; in the title of Figure 4, The gap between the blue and orange lines indicate -> The gap between the blue and orange lines indicates; the vertical spacing between Figure 4 and Table 1 is too small; in Table 1, compute -> computers; In Table 5, |G| -> |D| (D denotes the graph set)\n2. In Table 1, the exact digits of numbers are not the same, e.g., on Cora, the result of PerturbGCL is 83.3 \u00b1 0.5; while on PubMed,  the result of PerturbGCL is 82.10 \u00b1 0.37.\n\nW2: The novelty is limited.\nThe model perturbation or augmentation-free method has been explored in graph contrastive learning by prior work. RandMP and weightPrune are more like two tricks rather than a novel framework.\n\nW3: Lack of comparison with important baselines.\n1. For the node classification task, considering that the design of PerturbGCL is based on the framework of GraphCL, why do not compare PerturbGCL with GraphCL?\n2. Some other graph contrastive learning methods without using data augmentation, such as AFGRL [1], are not compared with PerturbGCL.\n\nOthers.\n1. The abstract is too long. I think that reducing the content of the abstract appropriately is helpful to highlight the core of this work.\n2. In Table 3, the authors only compare PerturbGCL with BGRL or GraphCL on time costs. Why not compare with other graph contrastive learning methods, such as SimGRACE? Since SimGRACE does not use data augmentation, I guess that SimGRACE may be more efficient than BGRL and GraphCL.\n3. In Table 4, the split ratios of Amazon-Computers, Amazon-Photo, Coauthor-CS, and Coauthor-Physics are not given. Also, please check whether the split ratio of Cora and PubMed is right (note that 85+5+15=105, not 100).\n4. In Algorithm 1, the pruning rato s, as one of the inputs, is not used in this algorithm; g() is not given a specific definition or statement.\n5. In Algorithm 2, the pruning rato s, as one of the inputs, is not used in this algorithm; the input data should be a graph set D, not a graph G; in Line 12, p(\\cdot)->h(\\cdot).\n6. The results in Figure 6 are important and I suggest moving Figure 6 into the main body of the paper.\n\n[1] Lee N, Lee J, Park C. Augmentation-free self-supervised learning on graphs[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(7): 7372-7380.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the representation quality and originality of this paper are not good. Also, the novelty of this work is limited.",
            "summary_of_the_review": "Please see my comment in 'Strength And Weaknesses'.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4555/Reviewer_bYXS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4555/Reviewer_bYXS"
        ]
    }
]