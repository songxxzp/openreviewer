[
    {
        "id": "FhaK2_ZDgL",
        "original": null,
        "number": 1,
        "cdate": 1666493031445,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666493031445,
        "tmdate": 1666493031445,
        "tddate": null,
        "forum": "WAgXmT8BeRj",
        "replyto": "WAgXmT8BeRj",
        "invitation": "ICLR.cc/2023/Conference/Paper2557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new Bayesian meta learning algorithm which aims to relax the traditional restricted task prior. The idea is to use a stochastic process prior for all tasks rather than plain diagonal Gaussian prior for over-parameterized Bayesian neural networks. The new prior is claimed to be with less possibility of over-fitting and improved uncertainty estimation. Further, instead of estimating the prior, the authors propose to only fit the score function by a transformer encoder and then use fSVGD in meta training and test. Specifically, a GP is fitted for each task in order to generate the values on all possible measurement set and spectral normalization is used to further reduce the tendency of overfitting. ",
            "strength_and_weaknesses": "Strength\n\n1. The paper is well scoped and design flow is clear. Each new idea is given a clear motivation or observation. \n2. The idea of fitting score function by a network for meta learning is novel and interesting. \n3. The experimental results show the consistently good performance comparing benchmark methods. \n\nWeakness\n\n1. The authors use fSVGD rather than functional BNN, but no comparative evaluations are given in the experiments to verify this choose. Since fSVGD is particles-based, it may affect the computational efficiency comparing normal fBNN. It is expected to what the additional advantages comparing with fBNN with GP prior.\n\n2. A transformer encoder is used to model score function considering the permutation equivariance property of the process. However, there is another property of the process that is marginal consistency. Is there any control of the encoder to ensure such property? If not, what is the possible effect when loss such property? \n\n3. In 5.2, the authors propose to fit each task by a GP in order to use eq. (2). If you can use a GP to model each task, do you still need the score network? Can we just fit a prior for all GPs? like a Hierarchical GP? Have you tested on such option? \n\n\n\n\n\n\n\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. The overall idea is novel and interesting. ",
            "summary_of_the_review": "The paper is well written and easy to follow. The overall idea is novel and interesting. There are still some weaknesses regarding the design (please see Weakness for more details). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2557/Reviewer_HXi8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2557/Reviewer_HXi8"
        ]
    },
    {
        "id": "1tiLF7nWoCy",
        "original": null,
        "number": 2,
        "cdate": 1666686886015,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686886015,
        "tmdate": 1671161542651,
        "tddate": null,
        "forum": "WAgXmT8BeRj",
        "replyto": "WAgXmT8BeRj",
        "invitation": "ICLR.cc/2023/Conference/Paper2557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method for Bayesian meta learning by learning priors in the function space. The key insight that distinguishes this work from prior literature is that functional inference techniques only make use of the prior score function and do not require a normalized prior density; therefore, instead of fitting a restricted form of prior distribution during meta training, one can fit a flexible score model of the function outputs and use it to transfer knowledge to new tasks. ",
            "strength_and_weaknesses": "## Strengths\n\n* The paper leverages ideas from several different areas (meta learning/GPs/BNNs/score estimation) and combines them in a creative way. The review paragraphs are very well-written and clearly convey the author's line of thought. \n\n* The idea of using score models instead of normalized priors for Bayesian meta learning is a clever one and overcomes many challenges faced by existing methods. \n\n* The experimental results are strong, if not extensive, and come with careful ablation study that shows the benefit of each component. \n\n## Weaknesses\n\n* One big limitation I could see from the formulation is that different tasks must share the same input space and output space. I am not an expert in meta learning so not sure if this is a commonly-made assumption, but it does feel significantly limiting the application scope. \n\n* Using a user-specified measurement distribution seems unnatural to me. Why not just rely on the empirical distribution of X from the training data? \n\n* Relying on GPs to interpolate at places where there is no training data is also a bit unsatisfying. I could imagine the GP's behavior will have a great influence on the performance. Could we do an ablation study by remove the GP? Also, it might be difficult to answer this question--why not use a GP prior if we are relying on a GP anyway?\n\n* In the ablation study, it would be nice to compare to the nonparametric score estimator using curl-free kernels (i.e., those based on kernel exponential families) which were shown to have drastically better performance in Zhou et al.\n\n* Some experiment details are missing, e.g., what is the measurement distribution being used in experiments? What kind of algorithms are used to fit the GP? Do you use a sparse approximation or the dataset is small enough?",
            "clarity,_quality,_novelty_and_reproducibility": "This could be a solid paper if the weaknesses are addressed: The idea is original. The literature review is thorough and mostly accurate. The execution is smooth and the writing is easy to follow.\n\nSome minor issues:\n* Section 2 > score estimation > \"there is a body of work on nonparameteric score estimation. The KSD paper did not propose a score estimation method; the same applies to the inverse problem book. \n\n* Section 4.3 > Second paragraph > \"From this viewpoint, ..., is p(h) a stochastic process on the function space\", p(h) should be removed.",
            "summary_of_the_review": "Overall I am positive about this paper and would be happy to raise my score if the rebuttal sufficiently addresses the weakness part. The idea is novel and addresses an important problem in Bayesian meta-learning. The empirical study is very careful and demonstrates the effectiveness of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2557/Reviewer_VdFm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2557/Reviewer_VdFm"
        ]
    },
    {
        "id": "aKeUvznmc3t",
        "original": null,
        "number": 3,
        "cdate": 1667593128333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667593128333,
        "tmdate": 1669097962068,
        "tddate": null,
        "forum": "WAgXmT8BeRj",
        "replyto": "WAgXmT8BeRj",
        "invitation": "ICLR.cc/2023/Conference/Paper2557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method for meta learning in function space, denoted MARS.\n\nTheir approach cosists of two stages:\nfirst, the authors train a GP on task-specific data to interpolate between different function values.\nThey then use this to be able to train a score function model given by a transformer which estimates the score of the meta learning model per task.\nSecond, the authors use their learned score matching network to perform functional BNN inference and meta-learn a prior.\n\nIn their experiments the authors show good performance compared to various baselines.\n",
            "strength_and_weaknesses": "Strengths:\n- The approach is very elegant conceptually, since the authors strive to overcome the specific problems of meta-learning and functional BNN estimation via score functions jointly and develop an integrated approach which utilizes amortization of the score estimate using a transformer based on a graceful objective.\n- the performance of the approach seems to be good in the empirical part of the paper\n\n\nWeaknesses:\n- There is one aspect in this paper that feels dissatisfactory: the fact that the authors require a GP to be learned on their datasets in order to be able to learn a NN, since this inherently assumes the same inductive bias as the GP and the score function estimator will be biased towards the types of interpolations of the GP. It is basically somewhat suspicious that the authors rely on the GP as the key to overcome here but in their model they cannot overcome the GP's limitations in case it is misspecified as it directly is upstream of their score estimator.\n- Given that the GP is fit directly on the observations, this also means the authors inherit all the GP limitations in terms of computation and so on, which here of course is only relevant for training and not for testing, but is still somewhat dissatisfactory.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is beautifully written and very clear and well-organized.\nI want to highlight this as an achievement since there are many moving parts in this work that are orchestrated in a complex way and the authors writing guides the reader through them lucidly and provides context at each stage.\n\nFurther, the approach tackles a popular problem, but uses an interesting combination of techniques and I particularly enjoy the amortization over the score estimate via the attention model as a novel and interesting contribution to approximate inference.",
            "summary_of_the_review": "The authors propose an interesting, complicated, and ultimately empriically useful way to perform meta-learning in function space using BNNs.\n\nWhile the approach has some modeling drawbacks by its dependence during training on GPs which I feel makes the approach somewhat less likely to generalize than I would have hoped, I am very excited about the ideas around amortizing the score estimate and the overall positioning and technical execution here.\n\nAs such, I believe this paper has value to the community for its  technical ideas and the strong presentation, even if I have doubts about the core model.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2557/Reviewer_Wct9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2557/Reviewer_Wct9"
        ]
    }
]