[
    {
        "id": "ex3cd6gETHi",
        "original": null,
        "number": 1,
        "cdate": 1666600022018,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600022018,
        "tmdate": 1666600022018,
        "tddate": null,
        "forum": "ObWiIiKihBf",
        "replyto": "ObWiIiKihBf",
        "invitation": "ICLR.cc/2023/Conference/Paper5696/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This submission proposes a method for inverting samples in diffusion generative models. The proposed approached simply applies the forward diffusion process for a given number of steps and then samples from the resulting latent. The approach is evaluated on 3 different problems: privacy-preserving reconstruction, data augmentation and super-resolution. ",
            "strength_and_weaknesses": "Strengths:\n- Successfully inverting a sample in diffusion models is an interesting problem that deserves more attention from the community.\n- The proposed approach is technically correct.\n\nWeakness:\n- The technical contributions are not novel, the same approach was introduced in the original DDPM paper https://arxiv.org/abs/2006.11239  (see appendix for interpolation results).\n- The empirical evaluation is rather limited. It is hard to evaluate the value of the contribution with qualitative results on data augmentation. ",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity:The paper is well written and easy to understand.\n- Novelty: the proposed approach for inversion was already introduced in the original DDPM paper.\n- Reproducibilty: As far as this reviewer can tell all the details for reproduction are given in the paper and results are shown on standard benchmark datasets.",
            "summary_of_the_review": "This submission tackles an interesting problem (eg. Inversion in diffusion models). The paper is clearly written and easy to understand. The main drawback of the proposed approach is the novelty of it, since an equivalent method was already introduced in the original DDPM paper.  Furthermore, the qualitative evaluation is not complete and makes it hard to validate the overall value of the contribution",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5696/Reviewer_z9Wv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5696/Reviewer_z9Wv"
        ]
    },
    {
        "id": "m3FroWkqG6",
        "original": null,
        "number": 2,
        "cdate": 1666731867019,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731867019,
        "tmdate": 1666731867019,
        "tddate": null,
        "forum": "ObWiIiKihBf",
        "replyto": "ObWiIiKihBf",
        "invitation": "ICLR.cc/2023/Conference/Paper5696/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method for generating variations of an existing image with diffusion models. The key idea is to add noise to the given image and then reconstruct it using denoising diffusion models. As you add more noise, the variability increases.",
            "strength_and_weaknesses": "I think what this paper proposes is known and not new. Particularly, it seems that the Boomerang Algorithm is exactly the same as the SDEdit paper: https://arxiv.org/abs/2108.01073. The latter is not cited and the differences are not discussed, but I really don't understand if there is anything new here.\n\nThe super-resolution application seems interesting. However, it would be good to see MSE and FID scores and compare it against other approaches - e.g. compare with super-resolution with GANs.\n\nFor the data augmentation application, I think that the comparison with StyleGAN-XL is unfair. Unconditional generation with StyleGAN-XL is expected to not match the quality of the original data samples. But data augmentation with Boomerang is not unconditional. Dataset samples are only slightly augmented. A more fair comparison would be to do inversion with StyleGAN-XL and add the inverted images to the dataset. There are many methods for controlling the fidelity of the inverted image to the given input which would be the equivalent of the t_boomerang knob. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The work does not seem original to me.",
            "summary_of_the_review": "The paper introduces a method for producing variations of a given image. I believe that this method is already known. Hence, I recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5696/Reviewer_Wv4p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5696/Reviewer_Wv4p"
        ]
    },
    {
        "id": "Yt1eEuMOOa",
        "original": null,
        "number": 3,
        "cdate": 1666746093038,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666746093038,
        "tmdate": 1666746093038,
        "tddate": null,
        "forum": "ObWiIiKihBf",
        "replyto": "ObWiIiKihBf",
        "invitation": "ICLR.cc/2023/Conference/Paper5696/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce a method for local sampling in diffusion models. This is achieved by leveraging the stochasticity of the diffusion models. An input image is propagated through the forward process for t steps then through the reverse process for t steps. In the forward process each step produces an output that is the linear combination of the input and a noise vector \u2013 the combination coefficient for the input is determined by the choice of variance for the noise. The reverse process is governed by a learned transition function. The authors demonstrate the utility of their local sampling procedure for anonymization of data, dataset augmentation, and image super-resolution.",
            "strength_and_weaknesses": "*Strengths:* This paper introduces a method for locally sampling the data manifold in a diffusion model. The proposed method is intuitive and the authors have demonstrated the applicability of the approach to several tasks. Moreover, the method proposed can be used with pretrained models, on a single GPU making it accessible to the larger community. \n\n*Possible typos:*\n- \u201cbetter alternative to preserving\u201d \u2192 better alternative for preserving\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "*Quality:* The work appears to be technically sound, and the claims of the paper are well supported. Moreover, the authors clearly communicate the limitations of the approach.\n\n*Clarity:* The paper is well organized and clearly written.\n\n*Originality:* The work appears to be an original use of an existing tool.\n\n",
            "summary_of_the_review": "I think the authors have identified a simple and powerful approach for local sampling in diffusion models. The paper is clearly written, and appears detailed enough that others could reproduce the findings.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5696/Reviewer_ydwA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5696/Reviewer_ydwA"
        ]
    },
    {
        "id": "PjTcUHto8d",
        "original": null,
        "number": 4,
        "cdate": 1667506273047,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667506273047,
        "tmdate": 1667506273047,
        "tddate": null,
        "forum": "ObWiIiKihBf",
        "replyto": "ObWiIiKihBf",
        "invitation": "ICLR.cc/2023/Conference/Paper5696/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper under consideration proposes a \u201cBoomerang\u201d approach for image variability-related applications. The idea is quite simple. Given a standard diffusion model:\n- run forward diffusion for several times resulting in noised image\n- run backward diffusion for several times resulting in an image which is controllably different from the original one.\n\nThe authors demonstrate the application of the aforementioned method in several applications.\n",
            "strength_and_weaknesses": "Strength: The idea is quite simple and natural and could be applied for arbitrary pretrained diffusion models.\n\nWeaknesses: \nIn general. My main concern about the paper is that the proposed approach is too incremental and kind of obvious. It is really nice that such a simple method helps to solve rather important problems listed in the application sections but from my point of view it is not enough reason for the paper under consideration to be published in top A* conference.\n\nIn particular. \n1) No comparison with competitive super-resolution methods is provided. Therefore it is difficult to estimate the usefulness of the Boomerang approach in this application. \n2) No quantitative comparison of the vanilla Boomerang and cascaded Boomerang super-resolution is provided. It is not clear, if the \u201ccasacadeness\u201d actually improves samples quality\n3) Table 1 doesn\u2019t seem to be convincing and requires clarifications. The reported accuracies of ResNet-18 model trained on standard Cifar-10 (and, probably ImageNet-200) data are actually not sota accuracies for ResNet-18 and corresponding datasets (see, https://github.com/kuangliu/pytorch-cifar for example). Therefore they are just particular accuracies obtained in particular training loops. This observation gives rise to the questions like as follows: Can we just train ResNet-18 for 200 (180 for ImageNet) epochs and achieve the competitive accuracy compared to Boomerang-augmented dataset cases? Can we achieve competitive performance using standard augmentation techniques? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality: The paper is more-or-less well-written.\n\nNovelty: The work under consideration could be characterized as a description of a nice scenario of diffusion models application. \n\nReproducibility: The code not provided in the supplementary materials\n",
            "summary_of_the_review": "Summary Of The Review: The proposed approach is quite straightforward and incremental. The applications are nice and practically interesting but require further investigation. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5696/Reviewer_qWcF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5696/Reviewer_qWcF"
        ]
    }
]