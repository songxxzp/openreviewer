[
    {
        "id": "GScz9GCqOd",
        "original": null,
        "number": 1,
        "cdate": 1666251373004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666251373004,
        "tmdate": 1666251373004,
        "tddate": null,
        "forum": "YhKScHeK4Ed",
        "replyto": "YhKScHeK4Ed",
        "invitation": "ICLR.cc/2023/Conference/Paper5138/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduce a novel learning environment to study how task characteristics affect measured difficulty for the learner. Besides, it detailed the expressive rule syntax in the environment and showed how to control over tasks. Some example results and analysis are presented in the paper and additional benchmark rules are provided in public site.",
            "strength_and_weaknesses": "Strengths:\n\nThe proposed environment is novel and seems to be used to study the impact of task characteristics on learning since it is constructed specifically for configurability.\n\nThe GOHR has expressive rule syntax so that researchers can control over tasks and make precise changes to rules of interest to study how they affect algorithm performance.\n\nWeaknesses:\n\nThis paper claims that researchers can control and design tasks in GOHR. In my understanding, this controllable environment should have the ability to compare different tasks once the task is designed so that researchers can change the task design in advance. However, the examples in this paper utilize the cumulated error curves to evaluate the difficulty between tasks. If the evaluation is realized after the experiment, what is the use of this so-called controllable task since we need to carry out experiments after designing tasks to compare different tasks.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear, novel and can be reproduced.",
            "summary_of_the_review": "Novel environment but I'm not sure about the usefulness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5138/Reviewer_FEUd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5138/Reviewer_FEUd"
        ]
    },
    {
        "id": "F8igDo-nsh",
        "original": null,
        "number": 2,
        "cdate": 1666675190712,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675190712,
        "tmdate": 1669954468967,
        "tddate": null,
        "forum": "YhKScHeK4Ed",
        "replyto": "YhKScHeK4Ed",
        "invitation": "ICLR.cc/2023/Conference/Paper5138/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors describe an expressive platform, GOHR, for generating game\nrules (of a certain number and complexity) and for evaluating how fast\nthe various agents figure out the rules.  Such a platform can lead to\ninsights into difficulty of rule sets, and relative capabilities of\ndifferent learning approaches, through systematic studies. For\ninstance, are there strategies that are uniformly good, or in some\nsense pareto optimal? The authors also present an example MDP-based RL\nagent, and a few different types of rules, and study the relative\nhardness of learning those rules with respect to the agent (how long\nthe agent takes, until it makes no mistakes).\n",
            "strength_and_weaknesses": "Strengths:\n\n-- Paper is clear and well written.\n\n-- A platform and a suite of tools is provided so researchers can do\n   different analyses (the authors show some of what one can do).\n\nWeaknesses (More details after the list):\n\n-- Because the setting is so contrived (by definition or from the\n get-go!) and combinatoric, how well any findings translate to\n real-world performance or difficulty is always questionable (this is\n admittedly an issue with any game and/or synthetic setting). If the\n authors could provide an example of a real-world problem (drug\n discovery, optimization, planning, etc) where engaging in this study\n could provide insights, that would strengthen the paper.\n\n-- [related to above] It would be good to have more uncertainty (see\n below). The authors focus on and motivate determinism, perhaps to\n allow for systematic study and comparisons (page 2, deterministic\n rules leading to 'clear distinctions.. [among].. learning tasks', but\n uncertain rules and environments could be insightful too and lead to\n clear distinctions..).\n\n\n----------------------------------\n\nMore details:\n\n\n-- How about non-deterministic rules and incomplete rule sets? so that\n the agent needs to learn or anticipate that the world is noisy and\n uncertain? This would be more applicable to real life: (often) no\n rule learned is perfect, and no rule set is complete!  As the authors\n explained, there could be some non-determinism in how rules are\n applied (eg in the order the rules become applicable), but that's\n very limited.\n\n-- For the above case, perhaps the agent needs to learn when to stop\n learning, ie it has learned all that it could and can do no better\n (which changes the interface to the platform).\n\n-- More non-stationarity could be another complicating factor: the rule\n set changing over time, but not in a deterministic/periodic fashion.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is well-written and clear.  There are no reproducibility issues.",
            "summary_of_the_review": "\nThe work in trying to give researchers control over level difficulty in a space of tasks, and providing an easy to use\nplatform is a good contribution.  I am concerned the space remains too narrow and may not generalize to the real world.  \nHowever, overall, I am positive on the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5138/Reviewer_Ccmc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5138/Reviewer_Ccmc"
        ]
    },
    {
        "id": "IWcIaWShPuj",
        "original": null,
        "number": 3,
        "cdate": 1666768602215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666768602215,
        "tmdate": 1666768602215,
        "tddate": null,
        "forum": "YhKScHeK4Ed",
        "replyto": "YhKScHeK4Ed",
        "invitation": "ICLR.cc/2023/Conference/Paper5138/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper the authors present a new test domain for ML ideas, called the Game of Hidden Rules, meant to help researchers disentangle how different aspects of the task and setting impact the level of difficulty for learning algorithms. In each episode of the game, the learning agent is presented with a game board containing game pieces, each drawn from a configurable set of shapes and colors. The objective is to remove game pieces from play by dragging and dropping them into buckets located at each corner of the game board. A hidden rule determines which pieces may be placed into which buckets at a given point in the game play. For instance, a rule might assign game pieces to specific buckets based on their shape or color. If the agent attempts a move that is not permitted, the piece remains unmoved. Rules can also be nonstationary, for instance by changing the logic of which moves are allowed during play. ",
            "strength_and_weaknesses": "Strengths\n-----------\nThe authors propose an interesting game idea, certainly, and I can certainly see it could be useful as a domain.\n\nWeaknesses \n------------\nIt's not clear to me whether proposing this game is in itself enough of a contribution to merit publication in a venue such as ICLR. Especially since there is not a sufficiently strong analysis of how different learning algorithms fair in this domain, at least as baseline, which is something I would expect given the nature of this paper. \n\nFurthermore, it's not clear to me what uniquely qualifies this game to study the way task parameters impact learning, more than countless other domains used in contemporary ML, from Starcraft and FLOW to classic games such as chess and Go, which all have their configurable characteristics, which researchers indeed tweak. Furthermore there are plenty of games in which the transition dynamics may change from episode to episode - what makes this domain better suited for study than any of the countless games authors have come up with in decades of ML research? I am not claiming this is not the case, just that I don't think it's sufficiently clear from the paper what the differentiators are.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is pretty clear. It is somewhat novel, in the sense that I have not seen this game before, but I have seen plenty of games where the transition dynamics can change from episode to episode. It seems fairly easily reproducible, particularly since the paper is not focused on any specific empirical results, and the game proposed is very flexible.",
            "summary_of_the_review": "This is an interesting idea but I don't think there's enough of a contribution here at the moment to be useful to the general ICLR community. There is a ton of potential though, so I truly hope the authors bulk up this study with more results and insights before calling upon us to embrace it as a new central test domain for ML research.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5138/Reviewer_9f8z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5138/Reviewer_9f8z"
        ]
    }
]