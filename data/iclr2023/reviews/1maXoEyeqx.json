[
    {
        "id": "CD-YfPP8af",
        "original": null,
        "number": 1,
        "cdate": 1666656886241,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656886241,
        "tmdate": 1669304356826,
        "tddate": null,
        "forum": "1maXoEyeqx",
        "replyto": "1maXoEyeqx",
        "invitation": "ICLR.cc/2023/Conference/Paper876/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approach for estimating the out-of-distribution performance of a model using unlabeled test examples. The approach is relatively simple and involves computing the class correlation matrix based on the model's predictions on the text examples. Across a wide range of problems, models, and even training checkpoints, the proposed method shows strong empirical performance compared to well established baselines.",
            "strength_and_weaknesses": "Strengths\n---\n\n- The paper makes experimental comparisons across a wide range of problems and models.\n- The paper is reasonably well written and easy to follow.\n- To the best of my knowledge, the proposed method is novel and addresses an important problem.\n\nWeaknesses\n---\n\n- The proposed method is not always the best performing, and test sample size is not considered (see below).\n- The paper would benefit from additional motivation and discussion around the method itself, which currently is explained only very briefly.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality\n---\n\nOverall, the paper is of high quality, and extensive experimental results are presented. One experiment that is missing is an analysis of how sample size factors into the method's performance. Currently, it is unclear if a large test set is required for good performance. A stratified analysis of all methods under various test set sizes would help to answer this question.\n\nClarity\n---\n\nThe paper is also relatively well written, though the description of the proposed method itself is woefully short. I find the paragraph starting with \"Our motivation is two-fold...\" difficult to follow, and expanding on these points would be useful. In my opinion, this would be worth moving some experiments to the appendix, as the core contribution of the work is this method, and currently the paper lacks details surrounding its motivation and explanation.",
            "summary_of_the_review": "In summary, the paper is reasonably well written and the experiments are sound, and I have recommended a few additions that will further strengthen the paper. I am recommending weak accept and am happy to discuss further.\n\nEdit after author response\n---\n\nApologies for the late reply. I appreciate the authors' comments, clarifications, and revisions, and I am maintaining my recommendation of acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper876/Reviewer_Ku6q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper876/Reviewer_Ku6q"
        ]
    },
    {
        "id": "drvwfzzjpW",
        "original": null,
        "number": 2,
        "cdate": 1666730712069,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666730712069,
        "tmdate": 1669473020950,
        "tddate": null,
        "forum": "1maXoEyeqx",
        "replyto": "1maXoEyeqx",
        "invitation": "ICLR.cc/2023/Conference/Paper876/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies qualitative measures using prediction probabilities that can predict the out-of-domain generalization. They argue that softmax probabilities are informative of accuracy in new domains and moreover measures that maximally exploit structure in prediction probability are at an advantage. ",
            "strength_and_weaknesses": "Strengths.\n1. Developing predictive measures for accuracy is important and has several practical applications from ML safety to adaptation. \n2. Their method is simple and computationally inexpensive to derive. \n\n\nWeakness/questions.\n1. Cosine-similarity of matrices is not formally defined and there is no standard definition of the same either. \n2. Infomax vs SoftmaxCorr. Both the methods of estimation use almost similar information. They both look at diversity and confidence of predictions, but why is SoftmaxCorr so much better than InfoMax?\n3. Variant B and SoftmaxCorr of Table 2. They are very similar, yet on wilds dataset, variant-b is far worse (even negative). Variant-B, however, considers only the diagonal elements and I expect it should be even closer to the Identity matrix when compared with SoftmaxCorr. \n4. In all the figures, the authors should clarify what different data points (dots) in each plot are. Are these evaluations from different models? If so, how are they obtained and how do they differ?\n5. Lack of understanding. In the discussion section, authors remark about how in-domain validation is not always informative of out-domain and that SoftmaxCorr could be more universal. But I do not see what merits SoftmaxCorr to be universal, on one of the WILDS dataset, we see softmaxCorr does not predict accuracy well. I would like to see the authors comment on when and why SoftmaxCorr is expected to work. \n6. More evaluation. There have been several empirically derived measures of accuracy prediction, but the authors only compared with simple approaches that exploit strictly lower information from softmax probabilities (such as max probability or difference between highest two probabilities). Authors should compare with the most recent related work on this problem. The case of low performance on Camelyon-OOD should be further investigated and explained. \n7. The motivation of Section 5.5 is unclear, especially in the last sentence.  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: mostly clear.\nReproducibility: code not released. \n",
            "summary_of_the_review": "The paper is mostly well-written and does a good job with its thorough systematic experiments. But some of the results and their method needs more explanation as further explained by my questions.\n\n--------------\n**Response to author comments**   \nI thank the authors for providing detailed clarification. Some of my concerns related to correctness of their results (such as questions 3, 4) are now addressed. I have also read other reviews and author responses. Reviewer 8Aud and I share similar concerns regarding comparison with other baselines, which the authors added in response. Accuracy-on-the-line explains accuracy poorly on 3/14 datasets while SoftmaxCorr is more consistent. However, it's unclear why the SoftmaxCorr is more consistent, even intuitively. Also, the definition of SoftmaxCorr is still not clear, what is $C.I$, elementwise product? \n\nIn summary, SoftmaxCorr is thoroughly evaluated and performs consistently well. Yet, I do not find the paper exciting. It is known that prediction probabilities are a good proxy for predicting accuracies and this work suggests that one could perform even better by also considering diversity of predictions. SoftmaxCorr is motivated from symptoms of a poor classifier: uncertain and skewed classification. So, a random classifier could as well be expected to have good performance on any dataset, which is why it is important to characterize and explicitly state the assumptions on the classifier and OOD datasets (and its relation to training data). The author's response to when SoftmaxCorr works is answered with more numbers on why it is important to look at diversity of predictions (along with confidence), and hence does not address my question. Also, the response on why SoftmaxCorr suffers on Camelyon-OOD is unconvincing. They explain that it could be due to underspecification, but why is underspecification only seen on this dataset and why does SoftmaxCorr fail for *underspecifed* setting despite not using ID datasets for accuracy predictions? \n\nThis work does not contain any surprising findings that I would like to eagerly share with a broader audience. For this reason, I recommend rejection and retain my original score.  \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper876/Reviewer_hesX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper876/Reviewer_hesX"
        ]
    },
    {
        "id": "JcssoVlSvfF",
        "original": null,
        "number": 3,
        "cdate": 1666788160940,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666788160940,
        "tmdate": 1666788160940,
        "tddate": null,
        "forum": "1maXoEyeqx",
        "replyto": "1maXoEyeqx",
        "invitation": "ICLR.cc/2023/Conference/Paper876/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method, SoftmaxCorr, which is an easy to compute metric to evaluate the generalization performance of a neural network on an OOD dataset. The metric is computed as the cosine similarity between the identity matrix and a correlation matrix of the matrix of predictions on the OOD dataset. The metric empirically is shown to approximate well the confusion matrix. SoftmaxCorr compares favourably to other metrics which make use only of the network predictive probabilities to compute an OOD generalization metric, when evaluated on OOD datasets sharing the same label space as the ID dataset.",
            "strength_and_weaknesses": "**Strengths**\n\n- The paper is well written and clear.\n- SoftmaxCorr is easy to compute and the underlying idea is simple and intuitive.\n- The comparison to the confusion matrix is compelling, Fig. 2.\n- SoftmaxCorr outperforms other metrics computed solely from the softmax predictive probabilities.\n\n**Weaknesses**\n\n*Comparison to non-softmax probability baselines*\n\nThe motivation for restricting the baselines to metrics computed solely on the softmax predictive probabilities is not clear to me. It would be understandable if the authors compared to other methods which may have additional requirements such as architectural changes, availability of ID labels, etc. and then stated that such methods have disadvantages due to these extra requirements. But given that these requirements may not be unreasonably prohibitive it seems to me that it is necessary to add such baselines.\n\nIn particular:\n\n- It would have been useful to compare against methods which make use of ID performance. W.r.t. \"we emphasize that SoftmaxCorr is overall a good alternative when labeled ID data is inaccessible or accuracy-on-the-line does not hold\", I regard it as atypical that labeled ID data would not be available so it would be reasonable to evaluate against such methods e.g. Miller et al. (2021) and/or Wenzel et al. (2022).\n- SNGP, Liu et al. (2020) and DUQ, Van Amersfoort et al. (2020), are competing methods. While they require architectural changes, it would be important to contextualize the SoftmaxCorr results by comparing against these methods.\n- A further interesting baseline, would be to use the confusion matrix directly (i.e. cosine similarity of the confusion matrix with the identity matrix). Clearly on new unlabelled data points we will not have labels with which to compute the confusion matrix. But given that the SoftmaxCorr metric must be computed on a full dataset rather than individual samples and that it effectively assumes a stationary OOD distribution, then it would not seem unreasonable to label a small number of these samples and then compute the confusion matrix directly. I am open to arguments as to why this may be impractical or undesirable in certain scenarios and naturally not requiring labels is an advantage of SoftmaxCorr in any case, nonetheless it would be useful to have this oracle and potentially practical method benchmarked in the experiments section.\n\n*Evaluation on far-OOD and OOD datasets with labels spaces different to the ID dataset*\n\nIdentifying OOD samples which are very far from the ID dataset, so far as not belonging to the same label space, is a key application of OOD detection metrics. Methods such as SNGP and DUQ have the advantage of being applicable to such scenarios. One major concern I have with the paper is that the OOD datasets are not very far from the ID datasets, for example ImageNet variants for ImageNet ID dataset and CIFAR variants for the CIFAR-10 training dataset. In such cases ID test accuracy has been shown to be a good predictor of OOD generalization Minderer, et al (2021).\n\nThe SoftmaxCorr metric could be computed on OOD datasets that do not share a label space with ID dataset. It is possible that SoftmaxCorr by approximating the confusion matrix is simply a good proxy for ID test set accuracy or similar metrics and that these metrics and SoftmaxCorr will not generalize to far OOD datasets. I would have liked the authors to have stress tested their method by evaluating it on datasets such as OOD=SVHN when ID=CIFAR-10 as done in Liu et al. (2020) and Van Amersfoort et al. (2020), and similarly more challenging label shifted datasets for the ID=ImageNet experiments.\n\n*Dataset level metric*\n\nThe SoftmaxCorr metric seems to be computed at the dataset level, this is quite a restrictive setting, meaning that individual samples cannot be scored and the metric cannot be directly applied to online prediction problems where inputs may come from a distribution shifting over time. I would be interested in the authors commenting on this disadvantage of the method. An interesting sensitivity analysis would be to progressively reduce the size of OOD dataset size N and measure the impact on the Table 1 results.\n\n\n\n\nLiu, Jeremiah, et al. \"Simple and principled uncertainty estimation with deterministic deep learning via distance awareness.\" Advances in Neural Information Processing Systems 33 (2020): 7498-7512.\n\nVan Amersfoort, Joost, et al. \"Uncertainty estimation using a single deep deterministic neural network.\" International conference on machine learning. PMLR, 2020.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and clear. To my knowledge the SoftmaxCorr is novel. The paper makes use of open source models and code, which indicates ease of reproducibility.",
            "summary_of_the_review": "The paper is well written and presents a clear, simple and effective metric for predicting OOD generalization performance. SoftmaxCorr compares well to other methods which only make use of the OOD softmax probabilities. However I have two major concerns: (i) I would have liked to have seen evaluation against other practical baselines which do not only use the softmax probabilities and (ii) I would have liked to see evaluation against far-OOD datasets and OOD datasets with different label spaces to the ID dataset, as there is existing evidence that ID accuracy is a good predictor of OOD performance for the near OOD datasets used in the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper876/Reviewer_8Aud"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper876/Reviewer_8Aud"
        ]
    }
]