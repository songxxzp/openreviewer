[
    {
        "id": "DBjMlgCEH8",
        "original": null,
        "number": 1,
        "cdate": 1666609815284,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609815284,
        "tmdate": 1670165089664,
        "tddate": null,
        "forum": "zgVDqw9ZUES",
        "replyto": "zgVDqw9ZUES",
        "invitation": "ICLR.cc/2023/Conference/Paper5681/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the infinite width limit of Multi-layer perceptrons, in particular the existence of feature learning. \nIn contrast to previous work, it studies adaptive optimizers instead of Gradient Descent, but results are roughly the same: NTK limit still results in lazy training (and no feature learning), while mu-parameterization supports feature learning.\nUnfortunately, no experiments on real (or even synthetic) data are provided in support of theoretical claims (It has been addressed in the rebuttal).\n",
            "strength_and_weaknesses": "Strengths:\n\n- Infinite width limit of MLP studied for the first time with adaptive optimizers.\n\n\nWeaknesses:\n\n- No empirical data is provided in support of theoretical findings.\n\n- NTK limit is provided (theorem 4.1) in memoryless case only, which is of very limited interest.\nThe authors state: \"we maintain that our analysis is applicable in the more general setting without introducing any additional arguments\", but no proof is provided as far as I understand.\n\n(It has been addressed in the rebuttal)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and contributions are novel.\n",
            "summary_of_the_review": "I would be much more convinced of the significance of this work if empirical validation of the theoretical results was provided (It has been addressed in the rebuttal).\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5681/Reviewer_8D7G"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5681/Reviewer_8D7G"
        ]
    },
    {
        "id": "QEyOx16ag9",
        "original": null,
        "number": 2,
        "cdate": 1666632490997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632490997,
        "tmdate": 1668197602158,
        "tddate": null,
        "forum": "zgVDqw9ZUES",
        "replyto": "zgVDqw9ZUES",
        "invitation": "ICLR.cc/2023/Conference/Paper5681/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work the authors tackle the question of the infinite-width limit of neural networks trained using adaptive optimizers (like ADAM). They show that if the step taken in the optimizer depends on a non-linear, scale-invariant function of the previous step, then the NTK can be computed by applying said function to $\\frac{\\partial f_{train}}{\\partial\\theta}\\nabla_{\\theta}\\mathcal{L}_{train} $, and then multiplying by $\\frac{\\partial f_{out}}{\\partial\\theta}$. An analogous result holds for the $\\mu$-parameterization (the unique family of infinite-width feature learning initializations).",
            "strength_and_weaknesses": "The arguments for the memoryless case are very clean. In addition, these additions to the TP framework make it more flexible and can give insight in many of the more complex practical cases which arise when training large models. The authors make a good mix of intuition-building arguments and links to the more rigorous ones.\n\nOne point of clarification: in the mean-field limit, does $\\epsilon$ have to go to $0$ to get all the scaling arguments right? Is there an intuitive reason that this doesn't mess up the Gaussianity arguments via divisors which are random variables which can have significant probability mass around $0$? The final limits seem to be $\\epsilon$-independent.\n\nOne of the big weaknesses is that the promise of extending the analysis to the case of an adaptive optimizer with memory (depending on gradients $g$ further back in time) is not clearly made. I believe without that, the result in the memoryless case is interesting but not sufficiently significant for an acceptance rating. It would also be good to have a discussion on how the total time $t$ must scale with $n$ for the arguments to hold.\n\nI think the paper would be significantly strengthened if there was an explicit formulation and proof of at least the NTK for the ADAM algorithm. From the comments in the paper these seem like results that are already completed, or easily derivable for the framework. This paper is positioned to become a standard reference for NTK/$\\mu$-parameterization for adaptive algorithms, and it would be strange for the most popular adaptive algorithm to not be explicitly covered.\n\nDue to the limits of my technical ability and the review timescale, I have not been able to rigorously check the main proofs in the TP framework. I followed along with the arguments in the main text and they seem sound. I believe that there needs to be numerical evidence that the proposed scaling limits hold - not to replace the proofs, but to supplement them and decrease the possibility that some key point in the complex machinery did not go awry.\n\nAs a small note, on page 3 it is unclear why the labeling of the gradients $g$ has even time indices only.\n\nUpdate: most of the concerns above have been addressed; my updated review score reflects those changes.",
            "clarity,_quality,_novelty_and_reproducibility": "Sections 1-4 are very clear and easy to read. Sections 5 and 6 are more difficult; however, I believe that this is due to the technical nature of the content, and not any fault of the authors who make an effort to build intuition for the proofs. The arguments are, to my knowledge, novel.",
            "summary_of_the_review": "The paper does an excellent job of building intuitive and rigorous arguments for memoryless adaptive optimizers. However, I feel that this is not a significant enough contribution on its own, and the case of non-memoryless optimizers like ADAM should be explicitly covered if they are in fact trivial extensions of the framework as suggested by the authors. I did not have the time nor expertise to rigorously check the proofs. I also suggest that numerical evidence of convergence to the theoretical limits would be very helpful to readers, especially those without strong theoretical backgrounds.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5681/Reviewer_k9hR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5681/Reviewer_k9hR"
        ]
    },
    {
        "id": "xP_d7SQdntd",
        "original": null,
        "number": 3,
        "cdate": 1666680380089,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680380089,
        "tmdate": 1666680744217,
        "tddate": null,
        "forum": "zgVDqw9ZUES",
        "replyto": "zgVDqw9ZUES",
        "invitation": "ICLR.cc/2023/Conference/Paper5681/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies two infinite-width limits (kernel and feature learning limits) of fully-connected neural networks (MLPs) under adaptive gradient-based optimization. Both results generalize that of MLPs trained by a non-adaptive way. In addition, the authors modify a framework (called Tensor Program) that allows to express the adaptive gradient processing as well as the convergence guarantee.\n",
            "strength_and_weaknesses": "Strength:\n\n- This paper extends two infinite-width limits of MLPs for adaptive gradient-based optimization, which have not been studied before. The proposed results can generalize both kernel and feature learning limits of non-adaptive settings. It is interesting that the adaptive NTK regime is still data-independent like the non-adaptive NTK.  \n\n- They provide a general Tensor Program framework for the adaptive optimization setting with O(1/sqrt(n)) convergence rate guarantees. \n\nWeakness:\n\n- Adaptive optimizations have various benefits over the SGD (e.g., fast convergence, better generalization, etc). Can this point of view be captured by the proposed adaptive infinite-width limits? It would be great if more clear motivations and meaningful results of adaptive infinite-width limits are provided. \n\n- The paper focuses on simple settings (e.g., memoryless adaptive gradient descents, update of parameters in a single layer). But, it is unclear whether the proposed results hold for more general settings. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality of this paper is quite poor and very informal. It should be improved by providing more comprehensive and rigorous flows. \n",
            "summary_of_the_review": "This paper extends infinite-width limits to the adaptive optimization setting, which has never been discovered before. But, it seems that a more in-depth comparison/analysis of the non-adaptive results would make the contributions much stronger. The writing quality needs to be improved for better readability.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5681/Reviewer_1g2Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5681/Reviewer_1g2Y"
        ]
    },
    {
        "id": "42xPUvJoxo",
        "original": null,
        "number": 4,
        "cdate": 1666689498095,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689498095,
        "tmdate": 1666690075027,
        "tddate": null,
        "forum": "zgVDqw9ZUES",
        "replyto": "zgVDqw9ZUES",
        "invitation": "ICLR.cc/2023/Conference/Paper5681/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This submission analyzes the dynamics of adaptive optimization of arbitrary neural network architectures in the infinite-width limit, and as a by-product,  a new tensor program framework is proposed.",
            "strength_and_weaknesses": "#### Strength\n- Close the gap between adaptive optimizer and NTK analysis.\n- Extend the guarantee in NTK and feature learning to the non-linear cases in the sense of the Tensor program language.\n\n\n#### Weaknesses\n- The adaptive optimizer considered in the analysis is memoryless and one-dimensional (Thm.4.1-4.2) given batch size is one, which is far from the practical case.\n- The theoretical results seem to be a generalized version of SGD, and we cannot see the difference between AGO and SGD from the theory. In fact, AGO and SGD behaviors differently during training, and we cannot treat SGD as a particular case.",
            "clarity,_quality,_novelty_and_reproducibility": "The submission extended the Tensor Programs framework to allow the expression of the computation graph involving adaptive optimizers. From this perspective, the novelty is limited.",
            "summary_of_the_review": "The submission adds some new tools in the previously developed framework (Tensor programs) for helping analyze the dynamic of $\\infty$-width neural network trained by adaptive optimizers. The only disadvantage is that the theory does not give more insight into the difference between SGD and adaptive optimizers (e.g., Adam).\n\nOverall, I still hold a positive score for this work since it is a necessary expansion of Tensor programs.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5681/Reviewer_7U4M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5681/Reviewer_7U4M"
        ]
    }
]