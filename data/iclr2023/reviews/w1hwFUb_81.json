[
    {
        "id": "l3-AEIOFl3K",
        "original": null,
        "number": 1,
        "cdate": 1666625875437,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625875437,
        "tmdate": 1668780796862,
        "tddate": null,
        "forum": "w1hwFUb_81",
        "replyto": "w1hwFUb_81",
        "invitation": "ICLR.cc/2023/Conference/Paper2877/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "While many MoE research focused on improving routing policies to encourage expert diversity and specialization, this paper studies another aspect of expert scalability, i.e., gradually increasing the portion of experts being activated per time. The authors show that by making MoEs smoothly scalable to the number of experts, it can turn into a new structured dropout approach that effectively trains larger transformer models in their full capacity. ",
            "strength_and_weaknesses": "First of all, it is very interesting to see the connection drawn between the modern MoEs and the \u201cold-fashioned\u201d dropout. The former typically refers to both sparse training and sparse inference (at the same or similar sparsity), while the later conducts sparse training yet dense inference in full model capacity. Despites their formation similarity, traditionally they pursue different goals: this paper makes a revisit of them and neatly bridges their gap.\n\nTheir proposed solution, SMoE-Dropout, simply re-parameterizes a transformer with MoE layers for their MLPs, and then use a fixed random router to activate experts with a \u201ccurriculum learning\u201d strategy, i.e., the number of activated experts gradually improved. Such training naturally yields a once-for-all in-situ trade-off between efficiency and performance when considering the deployment \u2013 a bonus that most current MoEs do not naturally enjoy. The method is also potentially generalizable to other non-transformer models.\n\nThe authors report very extensive empirical studies across various combinations of network architectures and datasets, as well as some transfer learning settings. They consistently demonstrate the significantly improved performance as well as training time savings from the SMoE-Dropout over a number of baselines. The experiment analyses are particularly well done and provide many useful insights on how SMoE-Dropout recipe works.\n\nThe following weakness points are identified, and clarifications are requested:\n-\tIs random routing really the major contributing factor in your recipe? What will happen if we apply the same curriculum learning of k to training learnable MoEs? Will they also become self-slimmable and suffer less collapse?\n-\tAnother main critique I hold against this work is that in many experiments, the empirical gains of SMoE-Dropout seem just marginal over other baselines, although the performance increase trend seems consistent as more experts are activated.\n-\t In Figure 1, it also appears that SMoE-Dropout performs worse than learnable MoE when the model is small.\n-\tI am also curious that, given the \u201cself-slimming\u201d property displayed on the same task, whether and how SMoE-Dropout could help address the compositional generalization challenge, that most MoEs nowadays can only solve with the assistance of clever prompting? For example, it would be very interesting to see the authors testing their trained models on multi-step commonsense or algorithm reasoning cases, perhaps with or without prompting.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing clarity and technical quality are great. The method is simple but revisits MoE/dropout from a novel angle.",
            "summary_of_the_review": "See the strength and weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2877/Reviewer_DuzK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2877/Reviewer_DuzK"
        ]
    },
    {
        "id": "v1J1mmpfGXq",
        "original": null,
        "number": 2,
        "cdate": 1666652724453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652724453,
        "tmdate": 1669465296945,
        "tddate": null,
        "forum": "w1hwFUb_81",
        "replyto": "w1hwFUb_81",
        "invitation": "ICLR.cc/2023/Conference/Paper2877/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Sparse Mixture-of-Experts (SMoE) have been widely used to increase the representation capacity of transformers with similar computational cost as dense transformers. SMoE consists of many experts, only few of which are activate for any given input example. This increased representational power comes with two issues: \n(a) Representation Collapse -- as the latent space representations get centered around experts leading to lots of redundancy and sub-optimal performance\n(b) Poor Scalability -- During inference and downstream fine-tuning, increasing the number of active experts does not increase the performance due to overfitting to the active experts during training\n\nThis work addresses these issues by introducing SMoE-dropout. It consists of randomly initialized and fixed router network that activates the experts. It gradually increases the number of experts during the training. This yields SMoE that have \"self-slimmable\" property, i.e., offering consistent boosted performance for transformers with an increase in activated experts during inference and downstream fine-tuning. Empirical evaluations show that SMoE transformers trained with SMoE-dropout yields consistent improvements over their dense counterparts on various reasoning tasks.",
            "strength_and_weaknesses": "\nStrengths:\n- \"Self-Slimmable\" -- the consistent boost in transformer performance with an increase in activated experts during inference and downstream fine-tuning\n- Extensive empirical evaluation that demonstrates the improvements on pre-training and reasoning tasks\n\nWeaknesses:\n- Does not explain the proposal in details (see below)\n- Limited novelty ( progressive activation of experts by https://arxiv.org/abs/2112.14397v1, other works have looked at progressive activation of kernels in CNNs such as Once-for-All networks  )\n\n\nQuestions for Authors:\n\n(1) SMoE-Dropout needs to be explained in more detail than what is presented in 3.2. What does it mean to say that the router network is \n - randomly initialized\n - and fixed \n - and implicitly optimized during training\n\n Either it is fixed or you train the network. May be elaborate any points that I have missed.\n\n(2) How do you integrate the Concrete-Dropout and DropBlock in the proposed SMoE-Dropout? A simple pseudo-code that shows these steps end-to-end would help a lot to clarify things. \n\n(3) Why is only the linear schedule used to gradually increase during the training? How about other ways to progressively activate the experts? How does dropout and expert activation schedule interplay?\n\n(4) How is Figure~1 generated? \n   - For SMoE-Dropout --> for the same SMoE, you increase the number of activate experts --> yielding in higher parameter count.\n   - How are green and black curves generated? \n   - Since black represents the dense network --> it should be a collection of different dense models\n   - Why is the proposed scheme worse in the low parameter count regime?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper would benefit from explaining the Dropout integration along with the expert activation schedule a bit more. It is unclear why the linear activation schedule would be the optimal in this setup. \n\nNovelty of this work is somewhat limited given that others have tried out similar notion of gradual expert activation (see https://arxiv.org/abs/2112.14397v1). \n\n\nNit-picks \n- Page 2, Para 3, \"... SMoE-Dropout ican be ... \"\n- Page 3, Para 4, \"... inferences. Meantime, the presented ..\"\n- Page 4, Eq.1, 'TopK( softmax(G(x), k) )' should change to 'TopK( softmax(G(x)), k )'\n- Term \"progressive\" might suit better in this context rather than self-slimmable\n\n\n\n\n",
            "summary_of_the_review": "Overall the self-slimmable property is indeed important for a sparse mixture of experts model and the proposed architecture helps in achieving the same. The paper is its current state needs to add more details to the method and explain some of the design choices. It is also lacking in novelty as mentioned earlier. \n\n\n----\n\n\nI've read other reviewer comments and author's response. I'll increase my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2877/Reviewer_qsko"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2877/Reviewer_qsko"
        ]
    },
    {
        "id": "38exqnP52iq",
        "original": null,
        "number": 3,
        "cdate": 1666662042075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662042075,
        "tmdate": 1668997067071,
        "tddate": null,
        "forum": "w1hwFUb_81",
        "replyto": "w1hwFUb_81",
        "invitation": "ICLR.cc/2023/Conference/Paper2877/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "SMoE-Dropout is motivated by the representation collapse and expert scalability issues observed in learned MoEs, by dynamically adapting number of activated experts as needed. It consists of a randomly initialized and fixed router network to activate experts and gradually increase their number as training progresses over time.",
            "strength_and_weaknesses": "Main Strength Points\n(1)\tSMoE-Dropout facilitates randomly and sparsely activated structure of network modules, playing an implicit regularization role similar to dropout \u2013 but more scalable to large transformer models. It has very low overhead and is very easy to use.\n(2)\tSMoE-Dropout naturally provides a \u201cself-slimmable\u201d property offering consistent performance boost for transformers with an increase in activated experts during inference and downstream fine-tuning. The transfer gains are especially obvious.\n(3)\tExperiments are very extensive and thorough, including a variety of tasks, architectures, datasets, and strong baselines beyond vanilla dropout. Both standard training and transfer learning are reported, accompanied with in-depth ablation studies. The experiment and analysis are very well organized and read convincing.\n\nWeakness and/or clarification:\n(1)\tDoes SMoE-training cost higher memory overhead, compared to dense training?\n(2)\tIt is very interesting to see that the SMoE-trained model can be distilled into a single-expert backbone. But details are lacked on how the distillation was done: is it common logit matching or something else? Also, how other MoE-variants will performance under distillation?\n(3)\tFor learnable MoE overfitting and collapse, it is an already well-known phenomenon that can also be mitigated by various diversity-regularized routers. It would be nice to compare SMoE-dropout with some of them.\n",
            "clarity,_quality,_novelty_and_reproducibility": "It seems the proposed method is not a new training recipe for MoEs, but rather a new way to train big dense models by leveraging MoEs as an auxiliary tool. This confused me initially. Clarifying this point better in abstract would help readers set their expectation. ",
            "summary_of_the_review": "The paper proposed a new method for dynamically adapting number of activated experts as needed and the overall quality is good. If the  questions as described above are well addressed, I tend to increase my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2877/Reviewer_3ZBm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2877/Reviewer_3ZBm"
        ]
    },
    {
        "id": "ED33CGo2sv",
        "original": null,
        "number": 4,
        "cdate": 1666662317438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662317438,
        "tmdate": 1668781476888,
        "tddate": null,
        "forum": "w1hwFUb_81",
        "replyto": "w1hwFUb_81",
        "invitation": "ICLR.cc/2023/Conference/Paper2877/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a novel plug-and-play strategy for training large transformer models, which leverages sparse MoEs in a dropout-like manner to scale transformers to better performance in their full capacity without collapse. The method is simple, and the experiments are thorough.",
            "strength_and_weaknesses": "Pros\n-\tSMoE-Dropout demonstrates an attractive \u201cself-slimming\u201d property during inference and downstream fine-tuning, which delivers a once-for-all in-situ trade-off between efficiency and performance\n-\tLike classical dropout, SMoE-Dropout is able to mitigate the representation collapse that standard MoEs usually suffer from, i.e., activating more experts do not improve or even hurt performance.\n-\tApplying SMoE-Dropout is extremely cheap as only random router is adopted\n-\tTransfer study is strong, which provides another evidence that more pre-training information is effectively retained by the full transformer capacity. It is further validated in ablation studies 4.4\n\nCons:\n-\tThe performance gain of SMoE-Dropout is sometimes marginal, such as on BeRT (section 4.1). Moreover, as seen from Figure 1 and Figure 3, it seems SMoE-Dropout does sacrifice performance at low parameter counts compared to Learnable MoE, why?\n-\tWhen breaking a single-stream models into MLP MoEs (the modularization step in section 3.2), how to decide the number of MLP experts needed, i.e., N? This seems to be an important hyperparameter but not discussed. Also, why not applying SMoE-Dropout to MLP layers but not self-attention? \n-\tFor training with Learnable SMoE, it is further unclear how the authors selected its k?\n-\tWhy training time is the same between SMoE-dropout k=N/2 and k=N, in Table 1?\n-\tTable 1 again: are all those numbers averaged across three datasets? The authors didn\u2019t explain\n",
            "clarity,_quality,_novelty_and_reproducibility": "All look good to me. Authors promised to release models and codes: it would help if the authors could respond to be clearer what they plan to release: all experiments versus some, pre-trained model versus training script, etc.",
            "summary_of_the_review": "Overall, I would tend to accept this paper. If the author can address my concerns. I would be more convinced.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2877/Reviewer_EKmy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2877/Reviewer_EKmy"
        ]
    }
]