[
    {
        "id": "6bhnqkS3R-m",
        "original": null,
        "number": 1,
        "cdate": 1666572445703,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666572445703,
        "tmdate": 1666572445703,
        "tddate": null,
        "forum": "1tHAZRqftM",
        "replyto": "1tHAZRqftM",
        "invitation": "ICLR.cc/2023/Conference/Paper1376/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the use of multi-task learning for GNN self-supervised learning and particularly uses a combined gradient based on pareto optimality. The model seems to improve upon existing ones, interestingly, especially also in terms of generalization capability w.r.t. heterophily/homophily.\n",
            "strength_and_weaknesses": "(+) The question represents an interesting topic to be investigated and the paper presents a reasonable approach which, to the best of my knowledge, is novel, especially the integration of the gradients.\n\n(+) The authors provide theory proving convergence. Please note that I did not check all that in full detail.\n\n(+) The evaluation considers various datasets, also challenging ones from OGB, tasks, and baselines. I am no expert in SSL to be able to judge if the most important competitors are considered.\n\n(-) Some statements seem unlucky/misleading to me:\n- The main question \"Does combining multiple philosophies enhance task generalization for SSL-based GNNs?\" sounds a bit too easy. \"Yes\" should be a very likely I would think. \n- Similarly, the authors claim throughout the paper that models focusing on a single task perform \"poorly\" (as narrow experts) and generalization beyond a single task is needed. I would think that these are too bold claims. If we developed perfect experts over time, this would not be a problem. Nevertheless, the fact that the paper shows that the generalization obtained helps improving over single-task-focused models is great. I just would frame it differently.\n\n(-) The paper is missing related work. For example, [1] propose to combine different self-supervised learning strategies and demonstrate that the combination is beneficial. Similarly, I would consider multi-task learning in GNNs as related work to be mentioned in more detail.\np.9 \"In these frameworks\" - if there are more, these should be referenced as well since this is closely related work.\n\n[1] Hu et al. STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS, ICLR 2020\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and to me seems to tick off the quality and novelty requirements. Code is also provided.",
            "summary_of_the_review": "Altogether, this work seems to be a solid contribution to me. I am too few expert of self-supervised learning to judge if it is really novel w.r.t. SOTA. If the other reviewers can confirm the latter, I would suggest acceptance.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1376/Reviewer_15ww"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1376/Reviewer_15ww"
        ]
    },
    {
        "id": "j8GrftA7AA",
        "original": null,
        "number": 2,
        "cdate": 1666622915637,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622915637,
        "tmdate": 1669458343725,
        "tddate": null,
        "forum": "1tHAZRqftM",
        "replyto": "1tHAZRqftM",
        "invitation": "ICLR.cc/2023/Conference/Paper1376/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a multi-task self-supervised learning framework namely ParetoGNN for node representation learning on graphs. The proposed method adopts five pretext self-supervised learning tasks and designs a multiple-gradient descent algorithm to reconcile different tasks by actively learning and minimizing conflicts. Extensive experiments over four downstream tasks and 11 benchmark datasets show that the proposed method can improve the model's task generalization ability.",
            "strength_and_weaknesses": "Pros:  \n[+] Multi-task SSL for node representation learning over graphs is a novel and important problem.   \n[+] This paper shows that task conflicts commonly exist in real-world graph datasets and theoretically proves that the proposed method can resolve task conflicts.  \n[+] This paper conducts extensive experiments over four downstream tasks and 11 benchmark datasets.  \n[+] The illustration of motivation, method design, and experiments are quite clear.  \n[+] Experiment details are contained in the appendix, including hyperparameters, settings, etc. Source codes are also included.  \n\nCons:  \n[-] It seems that the proposed method to resolve task conflicts is agnostic of domains, i.e., not specifically designed for graphs. It would make the paper stronger if further analysis of the difference between conflicts in graphs and other domains could be analyzed. For this reason, I think that the technical novelty is somewhat limited.  \n\n=== after rebuttal ===\nI have read the rebuttal and thank the authors for further clarifications. All things considered (including the quality of my other reviewed papers), I'd like to keep a slight positive score as 6.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and reproducibility are good, while the novelty is somewhat limited. ",
            "summary_of_the_review": "See above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1376/Reviewer_WfQN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1376/Reviewer_WfQN"
        ]
    },
    {
        "id": "1hKkiskOcA",
        "original": null,
        "number": 3,
        "cdate": 1666699179320,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699179320,
        "tmdate": 1666699179320,
        "tddate": null,
        "forum": "1tHAZRqftM",
        "replyto": "1tHAZRqftM",
        "invitation": "ICLR.cc/2023/Conference/Paper1376/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose PARETOGNN, which is a multi-task SSL framework using the concept of Pareto optimality.\n\n\nThe self-supervised learning (SSL) for multi-task is still challenging especially over graph-structured data.\n\n\nIn this environment, the SSL frameworks for the graph that adhere to just one objective might perform poorly to the others.\n\n\nTo improve task generalization, PARETOGNN is simultaneously trained on various SSL tasks while promoting Pareto optimality using the Multiple Gradient Descent Algorithm(MGDA).\n\n\nIt allows the model to capture inherent patterns that are relevant to several pretext tasks while reducing possible conflicts.\n\n\nAs a result, the model learns different patterns transferable to multiple tasks on graphs.",
            "strength_and_weaknesses": "**Strengths**\n\n\n- The paper including the appendix is well-written and organized. \n\n- The authors compare several features of the existing methods with the proposed method and provide a detailed discussion.\n\n- The results on benchmarks are comparable to or better than most baselines.\n\n---\n\n\nThere has been papers that explore Pareto optimality for SSL or MTL [1,2].\n\nWhat is the main contribution of your work compared to them?\n\nOf course I could have noticed, but I'm asking for the benefit of other readers.\n\nBecause it might seem to just simple extension of these series of works into graph version.\n\nIn Pareto MTL [2], the authors claim that the multiple gradient descent algorithm (MGDA) is not capable to incorporate different trade-off preference information as illustrated in Figure 2 of [2].\n\nDoes it mean ParetoGNN also can have the same weakness?\n\n---\n\n\n[1] Pareto Self-Supervised Training for Few-Shot Learning, Chen et al., CVPR 2021\n\n[2] Pareto Multi-Task Learning, Lin et al., NeurIPS 2019",
            "clarity,_quality,_novelty_and_reproducibility": "I asked novelty issue on the above section.",
            "summary_of_the_review": "I set the confidence of this review to a low score because I would change (or finalize) my decision after the discussion period (regarding the novelty issue).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1376/Reviewer_wz1H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1376/Reviewer_wz1H"
        ]
    }
]