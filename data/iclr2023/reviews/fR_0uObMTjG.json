[
    {
        "id": "fxbiBJb3QCs",
        "original": null,
        "number": 1,
        "cdate": 1666326135965,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666326135965,
        "tmdate": 1666326135965,
        "tddate": null,
        "forum": "fR_0uObMTjG",
        "replyto": "fR_0uObMTjG",
        "invitation": "ICLR.cc/2023/Conference/Paper2068/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates vertical FL on graph neural networks. A communication-efficient vertically-distributed training algorithm called GLASU was proposed, where the main idea for communication saving is lazy aggregation and stale updates. It was proved that the proposed algorithm can converge given that the loss function is smooth with Lipschitz gradient, lower-bounded and the sampling is uniform. Several experiments were conducted.",
            "strength_and_weaknesses": "Strength: This paper is written clearly and reader-friendly. The mathematical proof of the theorem looks solid.\n\nWeaknesses: The most severe weakness of the paper is that the theorem and experiment did not support the motivation of the algorithm.\n\n    1. Communication-efficient. The motivation of the GLASU algorithm is to save communication. However, neither the theorem nor the experiment discuss the communication cost, which undermines the contribution of the paper.\n    2. Privacy is an important aspect in federated learning. But privacy is not discussed in this paper.\n    3. The experiment results are not clear enough. For example, in Table 2 and table 4, it is hard to see how Q affects the results. In Table 3, it is hard to see how K affects the results. The experiment results are not strong enough to support the claims of the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly. The quality of the experiments can be further improved. The idea of the paper is somehow novel, but not very impressive. ",
            "summary_of_the_review": "This paper proposed GLASU, a communication-efficient vFL algorithm on graph neural networks. The convergence of GLASU is proved and some experiments are conducted. However, neither the theorem nor the experiment can prove that the communication cost is reduced. Also, the experiments are not strong enough to support the claim. Hence, the quality of the paper can be further improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_42nY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_42nY"
        ]
    },
    {
        "id": "TNT8k07-yEd",
        "original": null,
        "number": 2,
        "cdate": 1666656352894,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656352894,
        "tmdate": 1666656352894,
        "tddate": null,
        "forum": "fR_0uObMTjG",
        "replyto": "fR_0uObMTjG",
        "invitation": "ICLR.cc/2023/Conference/Paper2068/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The author works on the vertical federated learning for graph data. The author proposes a model splitting method to split the model to server and clients, and communication-efficient techniques such as lazy aggregation and stale updates for efficient training. Empirical results show similar performance to centralized training but a smaller wall-clock training time.",
            "strength_and_weaknesses": "Strength:\nThe proposed method reduce the communication time via the proposed communication-reduction technique.\n\nWeaknesses/questions:\n1. The model splitting method is very straightforward (including both the averaging and concatenation), and it can hardly be counted as a novel method to me. \n2. The proposed lazy aggregation and stale updates are not novel techniques. They have been commonly adopted in distributed training.\n3. Is there any further elaboration of the theorem1? How does it compared to centralized training, standalone training, or existing VFL methods? What is the theoretical benefits?\n4. 3 clients in HeriGraph is so small. Does the proposed method scale to more clients? What is the client settings in other datasets?\n5. What are the real applications of VFL on graph data? Please introduce it along with the applications of horizontal FL in the first graph in related works.\n6. Is there any memory concern when \"Each client m will store the \u201call but m\u201d representation\" ?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized and easy to follow. But the proposed techniques seem to be applying existing techniques in distributed training to VFL on graph data. Therefore, the novelty is incremental.\nNo codes are provided, so it could be hard to verify the reproducibility.",
            "summary_of_the_review": "The author proposes how to split model to server and clients for VFL on graph data, and proposes lazy aggregation and stale updates for communication efficiency. However, the novelty of those techniques is incremental, given the vast amount of communication-efficient works in distributed training.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_jr7W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_jr7W"
        ]
    },
    {
        "id": "WdPBDJwCwM",
        "original": null,
        "number": 3,
        "cdate": 1666670652685,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670652685,
        "tmdate": 1670207989279,
        "tddate": null,
        "forum": "fR_0uObMTjG",
        "replyto": "fR_0uObMTjG",
        "invitation": "ICLR.cc/2023/Conference/Paper2068/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on learning graph neural networks (GNN) under the federated (FL) scheme. As for FL under vertically distributed data, \"each client holds a subgraph of the global graph, part of the features for nodes in this subgraph, and part of the whole model; all clients collaboratively predict node properties.\" The paper proposes lazy aggregation and stale updates to reduce the communication frequency between clients and the server.",
            "strength_and_weaknesses": "Issues:\n1. The problem set up as it pertains to GNNs and vertical data is not well-motivated.\n2. The empirical evaluation is limited to one GNN model, GCNII (simple and deep convolutional network), and 3 clients of FL.\n3. The splitting of graph and features is not well explained.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read. The empirical evaluation is subjective.",
            "summary_of_the_review": "The novelty is lacking and the advantage of the proposed learning scheme is not clear.\n\nUpdate: I thank the authors' response. I will keep my initial review.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_q1q8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_q1q8"
        ]
    },
    {
        "id": "EdiMhJAxX33",
        "original": null,
        "number": 4,
        "cdate": 1666674483173,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674483173,
        "tmdate": 1666674483173,
        "tddate": null,
        "forum": "fR_0uObMTjG",
        "replyto": "fR_0uObMTjG",
        "invitation": "ICLR.cc/2023/Conference/Paper2068/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a method of communication-efficient federated learning framework for a vertically distributed graph. Specifically, a lazy aggregation rule is proposed to reduce the communication rounds. A new strategy called stale updates skips aggregation in specific iterations to reduce the cost during vertical training. ",
            "strength_and_weaknesses": "Pros:\n1.The paper proposes a communication-efficient GLASU algorithm for federated GNN in a VFL manner.\n\n2. The extensive theoretical analyses are derived to validate the performance guarantees of the proposed method.\n\n3. The algorithm is evaluated on a number of real-world graph datasets to demonstrate the effectiveness of the proposed approach for VFL on graph data.\n\n\nCons:\n1.\tDuring the training, the clients have to send the aggregated embeddings to the server, which might cause privacy issues, like inference attacks or recovery attacks. It would be better if the author could discuss this in more detail.\n2.\tBased on my understanding, each client needs to train a local W by minimizing the loss function, which means that each client should hold labels which is a very strong assumption cause in real-world cases, one client usually kept the labels.\n3.\tThe most significant advantage of GNN is that it can aggregate the features through neighbors. However, the proposed method directly concatenates the embeddings of the sub-layer, and the embedding is generated by local clients who keep only partial edges of the whole graph. I do not think the average or concatenation operation can replace or approximate the propagation rule in GNN.\n4.\tFor lazy aggregation, how to choose the indices of K. Furthermore, the embedding of each layer is dependent, and the gradients are calculated by following the chain rules. If skipping one layer, how to keep the consistency of the gradient updates is a big concern.\n5.\tThe paper lacks clarity on how stale updates work and why the proposed method can work without influencing performance.\n6.\tFor Theorem, when T approaches infinite, the gradient norm is still upper-bounded by the second term, and it can not show that the proposed method will finally converge.\n7.\tThe experiments have no running time and accuracy compared with SOTA methods. Moreover, figure 3 shows an unexplainable pattern. It would be better if the author could discuss it in more detail.",
            "clarity,_quality,_novelty_and_reproducibility": "It is understandable to a large extent, but parts of the paper need more work.",
            "summary_of_the_review": "In general, the studied problem is interesting and important. In addition, the methodology is principled with three major merits as discussed above. However, the work still has some unaddressed concerns to well justify its technical and empirical contributions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_Hn4F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_Hn4F"
        ]
    },
    {
        "id": "5Ogj743izrX",
        "original": null,
        "number": 5,
        "cdate": 1666694886100,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694886100,
        "tmdate": 1666696174420,
        "tddate": null,
        "forum": "fR_0uObMTjG",
        "replyto": "fR_0uObMTjG",
        "invitation": "ICLR.cc/2023/Conference/Paper2068/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the vertical Federated Learning (FL) scenario for graph-structured data. In particular, the authors first propose to update the node features calculated from every layer of Graph Neural Network (GNNs), distributed to multiple clients, simultaneously (i.e., at l-th layer, all clients share their node representations from the (l-1)-th layer, and then update the l-th layer node representations). After that, to make such the FL framework communication-efficient, the authors propose not only the lazy aggregation that shares node representations only at the particular layers, but also the stale update that receives node features of other clients from the server at the particular interval. The authors evaluate the proposed GLASU on the vertical subgraph-level FL scenarios, showing the effectiveness and the efficiency of GLASU.",
            "strength_and_weaknesses": "### Strengths\n* The tackled problem of vertical subgraph FL for graph-structured data is under-explored.\n* The proposed subgraph FL scheme with two algorithms for making it communication-efficient is novel, and interesting.\n* The authors make effort to theoretically analyze the convergence bound of the proposed GLASU, which is challenging since the estimated stochastic gradient is biased in vertical subgraph FL.\n\n### Weaknesses\n* The biggest concern is that every client participating in FL should be in the same stage, to share node representations of specific layers simultaneously between different clients, which is not realistic. Also, considering such the point, it is unclear whether the proposed GLASU, evaluated with 3 clients, is scalable to the larger number of clients (e.g., 10 or 30, which are not large though). I suggest authors to experiment with larger client numbers.\n* This paper is not positioned well against existing subgraph FL. In particular, there are several subgraph-level FL [1, 2, 3], and the considered vertical FL scenario is not the only subgraph-level FL. Therefore, I suggest authors to discuss clear differences between the previous subgraph-FL [1, 2, 3], and the targeted vertical subgraph FL. \n* The baselines are somewhat weak. The authors compare three baselines, and two (i.e., centralized and local training) of them are not the FL models. Also, the other compared baseline (i.e., Sim) is not directly comparable to the proposed GLASU, since they use different local subgraphs. I am wondering the authors can evaluate discussed methods (Zhou et al., 2020; Liu et al., 2022) by adapting them to the used experimental setups.  \n\n---\n\n[1] FedGNN: Federated Graph Neural Network for Privacy-Preserving Recommendation, KDD 2020.\n\n[2] Subgraph Federated Learning with Missing Neighbor Generation, NeurIPS 2020.\n\n[3] Personalized Subgraph Federated Learning, arXiv 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n* The proposed idea is clearly described, with nice illustrations in Figure 1 and Figure 2.\n\n### Quality\n* The technical quality of the proposed GLASU is reasonable, but seems problematic for real-world application. In particular, the idea of sharing layer-level node representations between clients is convincing for GNNs, and the two algorithms (i.e., lazy aggregation and stale updates) to make this idea communication-efficient are convincing as well. However, on the other perspective, the proposed GLASU is designed under the hard-constraint: every client should be in the same stage (See the first weakness above), which limits its real-world application.\n* The experimental quality is quite weak, since the authors experiment with the very small number of clients (i.e., 3) for FL, and compare with only one FL baseline. Therefore, more effort is required to show the efficacy of the proposed GLASU. \n\n### Novelty\nThe novelty is mild, for the following reasons:\n* The vertical subgraph FL is not entirely new.\n* Also, the idea of sharing layer-level information between client models is not new.\n* However, the authors combine and adapt existing schemes well, and also, with subtle node features updating tricks for FL, the authors make the proposed GLASU communication-efficient.\n\n\n### Reproducibility\n* The reproducibility is low, since it is difficult to articulate experimental setups for FL, however, the authors do not provide the source code.",
            "summary_of_the_review": "This is a borderline paper: the tackled problem is under-explored and the proposed method has a reasonable design choice; whereas, the experiment is weak, the novelty is mild, and the proposed GLASU might not be applicable to real-world scenarios.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_nqTt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2068/Reviewer_nqTt"
        ]
    }
]