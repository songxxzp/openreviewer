[
    {
        "id": "qILc3MkUi9m",
        "original": null,
        "number": 1,
        "cdate": 1666111196688,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666111196688,
        "tmdate": 1670604900647,
        "tddate": null,
        "forum": "s7gnrEtWSm",
        "replyto": "s7gnrEtWSm",
        "invitation": "ICLR.cc/2023/Conference/Paper1083/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an algorithm to determine a mapping between arbitrary densities. The algorithm is particularly conceptually simple compared to e.g. Langevin dynamics. The authors illustrate the algorithm on a variety of examples. \n",
            "strength_and_weaknesses": "**Strengths**.\nThe paper clearly proposes a novel method to map densities. The theoretical background is sound and rigorous. The authors also support their claims by the empirical evidence. As the new method provides a significant improvement against the baseline, this is a major contribution to the field.\n\n**Weaknesses**\nEven though theoretically the approach could work with any densities, it turns out only Gaussian source density work well in practice. The authors give some alleged reasons for this behaviour but this could have been investigated a little more. Perhaps this is a direction for future work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**. The paper is clear, nice to read, and highlights necessary takeaways while hiding away the details in the appendix.\nI have spotted a couple of minor typos:\n  - p. 2: \"the have outperformed\"\n  - p. 5: \"residual learning errors accumulates\"\n  - p. 6: \"makes the neural network approximates\"\n\n**Quality**. The research is sound. The authors give necessary background information, and provide proofs of their claims, both theoretical and empirical. See above re: performance on non-Gaussian source densities.\n\nI have a question though regarding the proof of the convergence theorem. It is stated that the finite variance is required. However, the proof seems to work even without this condition? Because the empiricial mean should converge to the expected value anyway. Please clarify if this is false.\n\n**Novelty**. As to my knowledge, the result is novel.\n\n**Reproducibility**. The authors do not provide source code but the experiment details are presented in the paper so should be easy to reproduce.\n",
            "summary_of_the_review": "To sum up, the paper proposes a new method to map between densities. The method works well in Gaussian-to-arbritrary density setting. The paper is clearly written. Overall, my recommendation is accept.\n\nUPDATE: in light of the other reviews, and in particular that the discussed paper can be seen as a a special case of the previous work [1], I would like to lower my score. This and the weaknesses I highlighted previously outweigh the strong points of the paper.\n\n[1] Peluchetti, Stefano. \"Non-Denoising Forward-Time Diffusions.\" (2021).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1083/Reviewer_XE6Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1083/Reviewer_XE6Y"
        ]
    },
    {
        "id": "rdCHvUWiieW",
        "original": null,
        "number": 2,
        "cdate": 1666212082906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666212082906,
        "tmdate": 1666288791760,
        "tddate": null,
        "forum": "s7gnrEtWSm",
        "replyto": "s7gnrEtWSm",
        "invitation": "ICLR.cc/2023/Conference/Paper1083/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The goal of this paper is to develop a deterministic denoising diffusion between arbitrary densities without relying on complex concepts like Langevin dynamics or score-matching. Thanks to Bayes' theorem, it shows that iteratively blending and deblending samples produce random paths between arbitrary densities and proves this converges to a deterministic mapping that can be learned to deblend samples. Here, blending means generating samples from a convex combination of the start and end densities, while deblending means generating samples from the posterior distributions of the start and end densities given the blended distribution and the blending parameter.\n\nThe equivalence to the DDIM in a special situation has been established and a more numerically stable sampling process is proposed. The main evaluations on image generation with Gaussian noise as the target density indeed demonstrate a favorable sample quality according to FID. On the other hand, disappointing generation results are also illustrated between different image datasets.",
            "strength_and_weaknesses": "Strengths:\n- The proposed iterative $\\alpha$-(de)blending (IADB) can be considered as a more general denoising diffusion process that can (in theory) work with arbitrary densities, which has the DDIM as one of special cases when the target density is Gaussian. The main idea is simple, and the nice thing about IADB is that the derivation does not rely on complex concepts which makes it more flexible. I really like the underlying motivation of the work to find a simpler approach to deterministic diffusion using only basic sampling concepts.\n- With the flexibility of the formulation, the authors are able to derive different learning variants where one of them is more numerically stable than others. The results on LSUN Bedrooms, CelebA and AFHQ Cats are favorable comparing to DDIM in terms of FID.\n\nWeakness:\n- The proposition on Page 3 and the proof in Appendix A could be improved. This proposition is a critical part in understanding the proposed method and the formula used for Bayes' theorem is not the famous one ($P(A |B) = P(B | A) P(A) / P(B)$), thus it deserves more discussion in the main text.\n- The experimental comparison between the proposed IADB and DDIM. It is nice that the authors try to control all the conditions so that the performance difference can be solely attributed to the different learning processes. But the resulting FID scores for the DDIM are considerably worse than those reported in the original paper, this may raise questions about the validity of the comparison.\n- The disappointing results with arbitrary image densities. As one of the main contributions of this work, neither the image generation nor restoration gives satisfactory results among arbitrary image densities. The authors argue that \"experimental set up that works well with Gaussian noise does not necessarily transpose successfully to other densities\". It would be much better if the authors could provide an architectural design choice that works. ",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed deterministic denoising diffusion between arbitrary densities and the alternative more numerically stable learning formulation for DDIM appear novel to me. The approach is relatively simple, and the paper is easy to read and understand. The illustrations in Figure 4, 5, 6 are very helpful in understanding the dynamics of the proposed method.\n\nOne question: in Section 6, it is argued that \"we regularize $f_0$ by applying a little amount of noise to the images. In theory, with this regularization, IADB is proven to produce a correct sampling of $f_1$ regardless of $f_0$\". But in the Theorem on Page 4 which validates the proposed algorithm, it requires both $f_0$ and $f_1$ to be Riemann-integrable.",
            "summary_of_the_review": "In summary, the paper provides a simpler but more flexible derivation of deterministic denoising diffusion models that works with arbitrary start and end densities and has the DDIM as a special case when the target density is Gaussian. The flexibility of the formulation also warrants different learning variants where one of them is more numerically stable than others and compares favorably with DDIM in terms of image generation quality measured by FID. Despite the disappointing results with arbitrary image densities, I think this work is a nice addition to the community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1083/Reviewer_j8qM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1083/Reviewer_j8qM"
        ]
    },
    {
        "id": "SPOs-Y8azy",
        "original": null,
        "number": 3,
        "cdate": 1666627819814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627819814,
        "tmdate": 1668375003500,
        "tddate": null,
        "forum": "s7gnrEtWSm",
        "replyto": "s7gnrEtWSm",
        "invitation": "ICLR.cc/2023/Conference/Paper1083/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new (bijective) transformation method between arbitrary pairs of distributions. The idea is to iteratively blend and de-blend the samples along the trajectories. Effectively, the direction in each iteration is the difference between the mean of the posterior samples in the two distributions. The method can be viewed as a generalization of DDIM, a deterministic sampling method for diffusion models. If the starting distribution is standard normal, they are equivalent up to the network parameterization. Experimentally, their method successfully transforms from one distribution to the other.",
            "strength_and_weaknesses": "### Strong points\n\n- The paper proposed a generalized version of DDIM that allows for bridging arbitrary distribution. The idea itself is interesting and provides an alternative perspective to the well-known deterministic sampling process of diffusion models.\n\n- The proposed training and sampling algorithms are straightforward to implement.\u00a0\n\n- Experimentally, their method outperforms DDIM on several image datasets when starting from Gaussian, and they showcase the generalization to arbitrary pairs of distributions.\n\n### Weak points\n\n- The paper lacks some important baselines that can also bridge two arbitrary distributions. For example, people can use the bijection map in [1] to first transform distribution A to a distribution on a large hemisphere by the forward-process, and then transform the distribution on hemisphere back to distribution B by a learned backward-process. The simple \"bridging\" process can also define a bijection between an arbitrary pair of distributions.\n\n- The experiments in this paper do not back up the paper\u2019s claim. In particular, the main claim of this paper is that the proposed method can generalize to a generative process between two arbitrary distributions (Section 6). However, also as the authors pointed out, in practice, the performances on these tasks are disappointing, and the positive results only come when fixing the prior sampling distribution to be a Gaussian one. The reviewer has one possible hypothesis: it's difficult for neural networks to learn the difference between two heterogeneous images, such as Leave and CelebA. Instead, the original approach of diffusion models (variant (b) in Table 1) would be better. Is it possible to verify the hypothesis and give the rationale behind the failure?\n\n- The dataset of pure image generation in Section 5 is easy -- they are all single-mode datasets. Could you provide experiments on more challenging datasets, such as CIFAR-10? In addition, it seems that the only difference between the proposed method and DDIM is the network target (or parameterization of the score function). Could you elaborate more on this part of the paper?\n\n- The second experiment in Section 6 is effectively an inverse problem. The author could resort to some manifold-constraint methods such as [2] to see if they help.\n\n### Writing suggestions / Minors\n\n- All theorems and propositions should be numbered.\n- The reviewer is confused about the listed baselines of DDIM. When sampling 128 steps, DDIM is shown to give 18.70 on CelebA $64^2$, while in the original paper, it is 6.53 with 100 steps. I am wondering why there is such a big discrepancy? If the reason is that the models have not yet been trained to convergence, I suggest the authors report the comparison at convergence.\n\n- The reviewer suggests changing the density notation from $f$ to $p$, as $p$ is more commonly used in the community.\n\n- In Section 1: (1) SDE (such as reverse diffusion) is not a variant of Langevin dynamics but a complementary method. (2) \" We derive a deterministic diffusion model ...\" The authors' model is more general than the \"deterministic diffusion model\".\n\n[1] Poisson Flow Generative Models, NeurIPS 2022, https://arxiv.org/abs/2209.11178\n\n[2] Improving Diffusion Models for Inverse Problems using Manifold Constraints, NeurIPS 2022, https://arxiv.org/abs/2206.00941\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written. The idea is interesting, but its practical value is unclear, as the proposed method cannot obtain high quality samples with non-Gaussian densities.",
            "summary_of_the_review": "Given the lack of basic baselines and the unclear practical value, the reviewer leans toward rejection for current version.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1083/Reviewer_W2Pp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1083/Reviewer_W2Pp"
        ]
    },
    {
        "id": "bvHN6jsL4mq",
        "original": null,
        "number": 4,
        "cdate": 1667598944597,
        "mdate": 1667598944597,
        "ddate": null,
        "tcdate": 1667598944597,
        "tmdate": 1667598944597,
        "tddate": null,
        "forum": "s7gnrEtWSm",
        "replyto": "s7gnrEtWSm",
        "invitation": "ICLR.cc/2023/Conference/Paper1083/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a formulation of discrete diffusion models that do not rely on the concept of score matching or stochastic differential equations. Instead, authors leverage an interpolation function for samples from two distributions, and show that an iterative interpolation ($\\alpha$-blending) and inverse interpolation ($\\alpha$-deblending) process can transport samples from one distribution to the other. Authors further provide training objectives for this process and prove that it is a generalization of conventional deterministic diffusion models, such as DDIM. Empirically, authors obtain better sample quality than DDIMs with fewer sampling iterations. Lastly, authors report experimental results of using non-Gaussian noise for sample generation.",
            "strength_and_weaknesses": "## Strength\n\n1. The formulation is simple yet effective. Writing is clear and easy to understand.\n2. The proposed method generalizes existing deterministic diffusion models (DDIM in particular), offering new insights into their inner workings.\n3. Negative results on image translation and non-Gaussian diffusion are thought-provoking.\n\n## Weaknesses\n\n1. The proposed idea is nice but not entirely new. The formulation can be reduced to a special case of the method in [1]. Specifically, one can set the SDE to $d x_t = (x_1 - x_0) dt + 0 dw$ (which actually becomes an ODE) and leverage Theorem 1 in [1] to derive the aggregated  ODE $d x_t = E[x_1 - x_0 | x_t] dt$, where the conditional expectation can be estimated by training a neural network $D_\\theta(x_t, t)$ with mean squared error minimization. This recovers the exact formulation in Algorithm 4, and yields the same exact training objective in equation (5) in this paper. Unfortunately, the work of [1] is not cited nor discussed in this paper.\n\n2. The connection between $\\alpha$-(de)blending and DDIM is implied in the work [2] (see Appendix D). Specifically, you can understand $x$, $\\epsilon$, $z_\\phi$ in [2] as $x_0$, $x_1$ and $x_\\alpha$ in this work. The (d) formulation in Table 1 amounts to $v$-prediction in [2]. In light of this connection, it is important to discuss and credit the work of [2].\n\n## References\n[1] Peluchetti, Stefano. \"Non-Denoising Forward-Time Diffusions.\" (2021).\n\n[2] Salimans, Tim, and Jonathan Ho. \"Progressive distillation for fast sampling of diffusion models.\" arXiv preprint arXiv:2202.00512 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "* Very clear writing. Theoretical analysis and empirical results are solid.\n* Originality of this work is less ideal, as some major ideas in this work actually existed before, yet authors failed to cite and compare with them.",
            "summary_of_the_review": "The idea is simple, intuitive, yet very effective. Experiments provide insights into when the method works better, and when not. However, this work lacks discussion of some previous papers that explored closely related ideas.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1083/Reviewer_rt47"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1083/Reviewer_rt47"
        ]
    }
]