[
    {
        "id": "ndrsA5dIEx",
        "original": null,
        "number": 1,
        "cdate": 1666166000695,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666166000695,
        "tmdate": 1669037169690,
        "tddate": null,
        "forum": "OKcJhpQiGiX",
        "replyto": "OKcJhpQiGiX",
        "invitation": "ICLR.cc/2023/Conference/Paper4621/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles a key problem in disentanglement - the independence assumption made by most methods does not hold in practice. Instead, the paper suggests requiring independent supports between factors which can be satisfied even when correlation exists. The paper proposes to measure support independence by the empirical Hausdorff distance, but find that doing so is infeasible. Instead, it measures the hausdorff distance on slices of the representations (specifically pairs of factors). While not powerful enough on its own, the authors show the Hausdorff loss term is very helpful when combined with existing approaches. Benefits are shown, particularly when strong correlations exist in the data. Good results are also shown for OOD generalization.    ",
            "strength_and_weaknesses": "+ Removing the assumption of independence between factors of variation in disentanglement models is of high importance.\n+ The approach of independent supports may be a promising direction for tackling this task and appears novel.\n+ The formulation of the loss term seems reasonable (but does not seem to be the entire story)\n+ Experimental results do seem to confirm the claims of the paper (but note weaknesses) \n\n- The loss term is only able to detect second order correlations, which is not quite the same as ensuring factorized support. While the authors explained why this did it, it would have been interesting to understand the performance loss due to this relaxation.\n- The experiments are somewhat self-serving in the sense the correlation is linear between pairs of terms, which is also what the loss term is optimising. How would it deal with higher order correlations? \n- The loss term, combined with reconstruction was insufficient and required the KL divergence with the normal distribution, which encourages no correlation. While the authors tried to address it in the paper, I wish that they justify this finding in a clearer way.\n- The datasets useed are somewhat easy. How does this method performance on the more challenging SmallNorb?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well written and clear.\nQuality: The main claims seem to be supported, the experimental results and ablation are well done.\nNovelty: I am not aware of previous approaches to unsupervised disentanglement based on support factoriization.\nReproducibility: Code was provided.",
            "summary_of_the_review": "Overall, I find the idea interesting and the experimental evidence positive. Please address the weaknesses above. I am leaning towards acceptance.\n\n#######################\n\nPost-rebuttal: The authors mostly addressed my concerns, I keep my rating and recommend acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4621/Reviewer_e6Ls"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4621/Reviewer_e6Ls"
        ]
    },
    {
        "id": "b4FywCpGfi",
        "original": null,
        "number": 2,
        "cdate": 1666682252415,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682252415,
        "tmdate": 1666682252415,
        "tddate": null,
        "forum": "OKcJhpQiGiX",
        "replyto": "OKcJhpQiGiX",
        "invitation": "ICLR.cc/2023/Conference/Paper4621/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To avoid the unrealistic assumption of factor independence in traditional disentangled representation learning, this paper proposes to relax this assumption to factorized support, and proposes a Hausdorff-distance-based regularization. The authors conduct experiments on 3 classical datasets to show the improved disentanglement brought by the proposed method. Additionally, ablation studies show that the proposed method has  promising potential on out-of-distribution settings. ",
            "strength_and_weaknesses": "Strength:\n1.\tThis paper is well motivated, aiming to tackle the problem of the unrealistic assumption of traditional disentangled representation learning.\n2.\tThis paper proposes a new and effective training criterion through a Hausdorff distance term, which can be easily combined with existing disentanglement methods.\n3.\tThe experiments show that the proposed method can improve the disentanglement and the out-of-distribution generalization ability.\nWeaknesses:\n1.\tSome intuitions can be further explained, e.g., in section 2.2 the situation that breaks the factorized distribution can have a factorized support. It will be more convincing to give an example which does not have a factorized support will fail to disentangle, more intuitively show the relationship between factorized support and disentanglement.\n2.\tSince this paper claims to aim at the realistic scenario of disentangled representation learning, it is better to conduct experiments on real world datasets instead of the synthetic datasets(at least for the out-of-distribution setting.).\n3.\tThe compared disentangled baselines seem to be out-of-date, it is better to incorporate the more recent disentangling methods. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is excellent, quality is good, novelty is good, reproducibility is good because the authors promise to release the code after review. ",
            "summary_of_the_review": "I have carefully checked the main body of this paper(including the introduction, method and experiments part.) I do not check the supplementary files.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4621/Reviewer_niK9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4621/Reviewer_niK9"
        ]
    },
    {
        "id": "1QpQGQlKkb",
        "original": null,
        "number": 3,
        "cdate": 1666800077991,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666800077991,
        "tmdate": 1670875090409,
        "tddate": null,
        "forum": "OKcJhpQiGiX",
        "replyto": "OKcJhpQiGiX",
        "invitation": "ICLR.cc/2023/Conference/Paper4621/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to relax the assumption of statistical independence used in many disentangled representation learning works. The paper proposes to only assume that the support of the latent factors\u2019 distribution factorizes, which is a weaker constraint than statistical independence. Specifically, the paper proposes a training criterion that minimizes a Hausdorff set-distance between the latents\u2019 actual support and its factorization. The paper contains extensive experiments on three datasets with varying degrees of correlation settings between ground truth generative factors. The proposed approach achieves better DCI-D scores as compared to previous works. The learnt representations from the proposed approach also achieve better classification performance in \u201cdistributional shift\u201d settings.",
            "strength_and_weaknesses": "Strengths\n- The proposed relaxed assumption of factorization of support instead of independence provides more flexibility, allowing correlations in the training data to be learnt.\n- The paper contains extensive experiments to support the proposed approach. Consistent improvements on Shapes3D, DSprites, MPI3D show the robustness of the proposed approach.\n- Improvements in classification performance in \u201cdistributional shift\u201d settings show that the proposed approach can facilitate generalization, especially in the case of strong correlation in the training distribution. \n- The paper is well-written and easy to comprehend.\n\n\nWeaknesses:\n\n- The paper describes a problem in previous works, proposes a solution and shows that it works. This usually covers the most important components to evaluate an approach but the paper could be much more useful for the research community if the paper can provide a higher-level picture of where the proposed approach fits in this research area, for example:\n    - A discussion on what different ways could provide the factorization of latents\u2019 support and why the proposed HFS criterion is a better choice among them.\n    - What are the drawbacks of the proposed approach? Should everyone replace the methods from previous works with the proposed method in all cases? If not, in what settings does the proposed approach fare better than others and in what settings it might be better to not use it? Does the proposed approach take more computational resources? Is it a more difficult objective to train? \n- The paper draws its motivation from the fact that real-world datasets contain correlations between generative factors and therefore, the independence assumption is not very suitable for these real-world settings. But, the paper doesn\u2019t include any experiments on uncontrolled real-world datasets, for example, a good first step could be the CelebA dataset.\n- The paper mentions that previous \u201csuccessful\u201d works enforce the independence assumption even more strongly when it doesn\u2019t hold for realistic data distributions. That should result in these approaches failing poorly on realistic datasets. Can the authors provide any findings (even from other papers) to support this argument? This again emphasizes the importance of at least one experiment with realistic data for this paper (where ideally, the previous works fail poorly, perhaps even more than vanilla VAE and the proposed approach works much better).\n- The paper only considers one disentanglement evaluation metric \u2014 DCI-D. Even according to the cited justification Dittadi et al. (2021), both DCI and MIG are considered to be the best. Why do the authors choose to focus only on DCI? What are the results in terms of MIG? Also, it would be better if the evaluation metric(s) being used in the paper is explained briefly.\n- Can the authors explain why HFS is working the best for no correlation setting? Shouldn't independence assumption work perfectly in this case and therefore, existing works have an advantage?\n- Similarly, why do HFS versions of beta-VAE and beta-TCVAE work better than HFS? If the assumptions in beta-VAE and beta-TCVAE are not suitable for correlated settings, their criterion should hurt and not facilitate disentanglement, right?\n- In Figure 2, I would have expected a consistent trend from top to bottom in the first few columns but there seem to be some training correlations in between (3rd, 7th, and 8th rows) that are better than the ones surrounding them. Why is that so? How exactly is the correlation made more severe in this case? \n- Minor:\n    - Definition 1.1 seems insufficient. This definition would be satisfied by the case when there is only one dimension that contains all the generative factors, the rest all being just noise. I think the authors meant one-to-one mapping between the latent dimensions and the generative factors (as mentioned in the next sentence). If so, this definition should be fixed accordingly.\n    - It would be easier for the reader to understand if the authors can explain in more detail why the independence assumption doesn\u2019t hold for the case in Figure 1, perhaps contrasting it with how the distribution would be in the case of independence.\n    - Table 1, column 2, HFS is second best (bold) instead of beta-TCVAE+HFS.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - The paper is clearly written and easy to read. Most of the decisions made are explained with justifications. Some of my concerns are mentioned in weaknesses above.\n\nQuality - The research in the paper is of high quality. The paper describes a problem in previous works, proposes a reasonable solution and shows with extensive experimentation that it works.\n\nNovelty - The proposed approach is the first to my knowledge to relax the independence assumption in previous disentangled representation learning works in a general manner (without explicit auxiliary variables or specific prior models).\n\nReproducibility - The authors provide a lot of their experimental details in the main paper and appendices. They also promise to release their code after publication.",
            "summary_of_the_review": "I think the paper tackles an important problem in disentangled representation learning research, provides a reasonable solution and supports their approach with solid experiments. I have some concerns about the reasoning behind some experimental results, the lack of any experiments with realistic data and providing a more balanced picture of the proposed approach that I have described in more detail in the weakness section. But overall, I think the paper would be useful for the research community and hence I am inclining towards acceptance.\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------\nThe authors' response satisfactorily answers all my concerns. Therefore, I am keeping my acceptance recommendation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4621/Reviewer_rYAe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4621/Reviewer_rYAe"
        ]
    }
]