[
    {
        "id": "L-x3w6s7BG",
        "original": null,
        "number": 1,
        "cdate": 1666634361234,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634361234,
        "tmdate": 1671174261042,
        "tddate": null,
        "forum": "15lSKp0wBnm",
        "replyto": "15lSKp0wBnm",
        "invitation": "ICLR.cc/2023/Conference/Paper5571/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is about dynamics prediction. Given multiple video streams taken from different view points of the same scene, for instance cubes floating in a liquid, the task is to predict future states of the scene. This is a very interesting problem. The proposed solution has a bit of an engineering approach but is reasonable. I generally enjoyed reading this paper and have only a few questions or suggestions how to improve the paper.",
            "strength_and_weaknesses": "This paper presents a consistent pipeline how to infer future states of physical systems given images only. The pipeline to rely on a NeRF model and uplift instance segmentations into 3D space is a neat idea.\n\nHow to infer dynamics from raw pixels is a problem not addressed by prior works.\n\nI have 2 questions remaining: In Sec 3.2, how do you obtain estimates for the velocities since there does not exist explicit correspondences between 3D points across frames? Secondly, what are the point attributes a_i^v? Are these the class labels from the instance segmentation? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and pleasant to read. It was the best paper on my stack - thank you.\n\nI agree with the other reviewers that the authors should be upfront with the limitations of their work.",
            "summary_of_the_review": "The proposed pipeline is reasonable and the problem addressed by the authors is very important and interesting. I am not very familiar with the topic itself, though, so my confidence is not high.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5571/Reviewer_TwJK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5571/Reviewer_TwJK"
        ]
    },
    {
        "id": "hYAK6Pbmmo",
        "original": null,
        "number": 2,
        "cdate": 1666650576445,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650576445,
        "tmdate": 1666650576445,
        "tddate": null,
        "forum": "15lSKp0wBnm",
        "replyto": "15lSKp0wBnm",
        "invitation": "ICLR.cc/2023/Conference/Paper5571/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a method for learning visual intuitive physics models solely from unlabeled multi-view images. Specifically, the method first leverages a conditional NeRF to learn the 3D geometry for the scene, which are then segmented and clustered to produce the 3D point cloud representation for the target fluid, rigid bodies and granular materials. A graph neural network is then applied to learn the dynamics of 3D points. The experiments demonstrate that the methods can produce good visual dynamics and generalize to simple unseen scenes.",
            "strength_and_weaknesses": "Strengths:\n- The paper provides a possible direction for learning visual dynamics by advocating explicitly utilizing the geometry information.\n- The experiments are clear. The proposed method outperforms existing methods.\n\nWeaknesses:\n- The proposed two modules are simple combination of existing works. The Perception Module uses the existing PixelNeRF to learn underlying geometry from sparse views. The dynamics simulator applies standard graph-based backbone. There are no much new in the network design.\n\n- The proposed methods use the color information to obtain the masks, which is a very strong assumption and not reasonable. Although the author claims it could also be solved with object segmentation, I think the problem of segmenting fluid, rigid bodies and granular materials is non-trivial (such as labeling, generalization etc) while the proposed method has a very strong reliance on it. This is even more severe when the author claims the methods can achieve \"strong generalization\". It would be better if the author can show the robustness of the proposed methods with different learned segmentation results. Otherwise I don't think the paper has enough contribution to the problem of learning visual dynamics. It's more like combining two existing methods with a strong and unreasonable assumption to solve a problem.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and interesting. I think it could be reproduced. However, I question the novelty of the work (see Weaknesses)",
            "summary_of_the_review": "I think the assumption of known segmentation is too strong in order to combine the two existing modules. It's unknown how the methods would perform without such an unreasonable assumption. I think more experiments (learned segmentation) and more analysis (robustness to worse segmentation) should be added. I think the paper is below the bar of ICLR.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5571/Reviewer_1Jtf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5571/Reviewer_1Jtf"
        ]
    },
    {
        "id": "5KwjL2YHIm",
        "original": null,
        "number": 3,
        "cdate": 1666721436122,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666721436122,
        "tmdate": 1666721436122,
        "tddate": null,
        "forum": "15lSKp0wBnm",
        "replyto": "15lSKp0wBnm",
        "invitation": "ICLR.cc/2023/Conference/Paper5571/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to learn a visual dynamics prediction model in the 3D space. The 3D point cloud representation is learned only from a few images from multiple views (with known camera pose). After that, they use instance segmentation to parse individual objects from the point cloud. Then it trains a graph-based prediction model on the 3D representations, without relying on the ground-truth correspondence, using the chamfer and spacing loss. The proposed method is evaluated using the prediction loss on a few simulation tasks including Pour, Shake, and Push.",
            "strength_and_weaknesses": "Strength:\n1) The idea of combining NeRF and graph-based dynamics prediction is interesting. Results are also visually good.\n2) The writing is pretty clear and complete. It\u2019s easy to read and understand the main message of the paper. The figures are also nice and well-designed.\n3) Code is provided. This could be a good asset to the community.\n\nWeakness:\n1) Most of components are adopted from previous works, including image-conditioned nerf and training dynamics prediction using chamfer distance. Getting instance point cloud from segmentation is new but this is relatively easy (color segmentation) and this is not as \u201cimperfect\u201d as the authors claimed in the abstract given the relatively simple scene. It remains unclear if this pipeline can be used in other complex scenarios or even in the real-world.\n2) The evaluation is purely based on visual quality while previous work (Li et al. 2021b) also evaluate on control accuracy.\n3) What is the merge loss in Figure 2? Does that mean the spacing loss?",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is well-written. Novelty may not be its advantage but there is still enough contribution to the community. The code is provided so I\u2019m not concerned about its reproducibility.",
            "summary_of_the_review": "In summary, I think some aspects of this paper is worth reading to the community. There are still some discussion and experiments missing as I pointed out in the weakness section and it may still need some work. I\u2019m also happy to discuss and increase my score if the above concerns are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not Applicable.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5571/Reviewer_z9jL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5571/Reviewer_z9jL"
        ]
    },
    {
        "id": "lAln6iVXZw",
        "original": null,
        "number": 4,
        "cdate": 1666780177773,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666780177773,
        "tmdate": 1666796501314,
        "tddate": null,
        "forum": "15lSKp0wBnm",
        "replyto": "15lSKp0wBnm",
        "invitation": "ICLR.cc/2023/Conference/Paper5571/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a pipeline for predicting the future steps of the physical simulation starting from the video input from multiple cameras. The model builds a Nerf model from several input frames, samples the particles from Nerf and runs the particle-based simulator using the extracted particle representation.\n",
            "strength_and_weaknesses": "\n## Strengths\n\nThis paper is one of the few approaches that are able to perform the dynamics from the video inputs. Unlike the previous approach Nerf-DY, this paper uses a more structured representation of the simulation. That allow the model to generalise to new sizes of containers and different amount of fluid, which is consistent with the findings in the previous papers. The paper connects the two approaches that are known to work well from the previous works: PixelNerf (for a high-res 3D model) and particle-based simulators (for fluid modelling and generalisation). The past works have already shown the strong generalisation to new scenes due to locality of the particle interactions, so it is not surprising to see the similar results in this paper too.\n\n## Weaknesses\n\nThere are a few weaknesses in the method that limit the applicability of the work to different systems.\n\nThe method requires a major knowledge about the underlying simulation states, which defeats the purpose of learning from video. Specifically:\n- Knowledge of the 3D meshes of the robot/container/pusher and their positions/velocities at every time step. This is a strong assumption, because these meshes constitute a large part of the state. The meshes are also notoriously hard to extract from the video in the real-world scenarios.\n- Knowledge of fluid density to set the minimum distance for the spacing loss.\n- The model relies on the fact that the fluid is coloured and is easy to distinguish from other objects only based on the blue color. Similarly, the objects need to be of different colors, always visible and are easy to track across time.\n\nAnother major drawback is that the approach still requires to have the particle simulation as the ground-truth. The mesh/particles and the initial state are notoriously hard to obtain, preventing the previous simulators to train on the real-world data. Therefore, learning directly from videos is desirable. However, the proposed model unable to learn entirely from the video data. Moreover, it requires a corresponding ground-truth particle simulation for every step. It defeats the whole point of learning simulation from videos. \n\n### Representing the correct fluid dynamics.\n\n*Supplementary Section B.4*: \u201cIf the particle belongs to the water, then we have no history states, so the input of Qv is all-zero. \u2026. Compared with human intuition, we can get an intuitive prediction of the movement of water by simply knowing the past movement of the cup without knowing the past movement of water\u201d\n\nI don\u2019t think this is true. Consider the case when the we start moving the cup back and forth, and we start recording the simulation when the cup is already moving. The fluid particles already have the prior velocity that is not the same as the velocity of the cup. It is not possible to infer the correct fluid dynamics without the history of fluid particles or velocities.\n\nAnother example from the paper is the fluid falling from one container to another.\u00a0 Velocities of the falling particles depend on gravity and the previous particle velocity. Here we might be able to infer the fluid velocities **only** if we assume that fluid velocities is zero in the beginning of the simulation and we have access to the **entire** history of states. In all other cases, it is not possible to represent the correct dynamics of the fluid.\n\nThe fact that the videos look pretty good without the velocity information may indicate that the model exploits some biases from the data that allow to roughly imitate the dynamics under the point-cloud Chamfer loss. Specifically, I suspect that the model misuses the heuristic that the fluid particles have roughly the same as the velocities as the pusher/constrainer. It is visible in the attached videos of the ablations with and without the spacing loss: the particles that do not interact with the pusher are moving with the same speed as the pusher itself. The spacing loss also introduces a strong inductive bias that the fluid density remains constant.\u00a0\n\nIt would be helpful to add the comparison of the dynamics component to the ground truth, starting from the known particle state and evaluated under the Chamfer distance, to compare the correctness of the predicted fluid dynamics. This should be possible to do, as the paper assumes having the ground truth particle simulation.\n\nAuthors should be honest about the very limited range of applicability of their model: the fluid velocity can be inferred from the velocities of the surrounding objects, which is a rare case. Even a simple case of the falling block of fluid cannot be represented by the current model. ",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of sampling the particle representation from the NeRF model is new. The paper is well-motivated, because being able to learn the simulation from the video data without knowing the ground-truth meshes/particles high desirable. However, the paper does not deliver on this motivation.  \n\nThe paper does not emphasise that the approach requires a lot of knowledge about the 3D simulation state. The paper also does not clearly mention that actually still requires the ground-truth particle simulations and compute the loss, and therefore the model cannot learn purely from image data. The simulation has to be specifically constructed, such that we can distinguish fluid from other objects (fluid is colored and objects that have to be of different colors)\n\nAdditionally, the paper has severe limitations in terms of the systems that it can represent. As stated in section B.4, the fluid particles do not have the history of states, and their velocity is set to zero, and thus the model relies on the *velocities of the objects* to infer the dynamics of the fluid, leading to the incorrect fluid dynamics, as mentioned above. It is unacceptable to put this detail into the last page of the supplementary. The authors should be honest about the applicability and limitations of their model and adequately assess whether the predicted dynamics is, in fact, correct.\n\nAdditional questions:\n\n1. Figure 5 shows the comparison of the Chamfer distance between Nerf-DY and the proposed method. How was the Chamfer (point-cloud) distance computed for Nerf-DY, if Nerf-DY performs the dynamics in the latent space and does not have the notion of particles?\n2. Table 1 and Figure 4 provide the comparison between the perception modules. Does \u201cOur model\u201d provide only the comparison to PixelNerf? Or are there other parts of the model besides PixelNerf that are included into the \u201cour model\u201d row?\n3. Is the particle simulator pre-trained or trained together with the model.\n4. Is the proposed model trained only on 1-step prediction or multi-step prediction?",
            "summary_of_the_review": "This paper is one of the few approaches that performs the simulation from the video inputs, which is novel.However, this approach is severely limited in the environments that it can model (namely, simulated environments with colored fluids and differently-coloured objects), assumes to know that full mesh and state of the robot/container and relies on the velocities of the known objects to infer the velocities of the fluid. The authors need to be clear about the limitations of their work and the environments where the method is applicable: because fluid particles do not have velocities or past history, it would not be able to produce the correct fluid dynamics. I suggest rejecting the paper due to severe limitations of the approach that are not clearly stated.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5571/Reviewer_E7oV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5571/Reviewer_E7oV"
        ]
    }
]