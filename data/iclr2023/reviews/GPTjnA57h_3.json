[
    {
        "id": "Uo0ALdTTKC",
        "original": null,
        "number": 1,
        "cdate": 1666359877920,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666359877920,
        "tmdate": 1668848450493,
        "tddate": null,
        "forum": "GPTjnA57h_3",
        "replyto": "GPTjnA57h_3",
        "invitation": "ICLR.cc/2023/Conference/Paper3679/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the domain adversarial training problem and propose to use label smoothing for domain discrimination. Specifically, motivated by the observation that different domains from VLCS dataset show small difference, thus a soft domain label should be applied to domain adversarial training instead of hard label. By simply rewriting the cross-entropy loss, the proposed ELS can be easily implemented. Extensive theoretical and empirical analyses are provided to support the proposed method.",
            "strength_and_weaknesses": "Strength:\n- This paper is well-organized and easy to follow. The general framework is pretty clear and follows rigorous logic.\n- The theoretical analysis shows many helpful properties of ELS, which are further validated by effective experimental results. To my opinion, this is a very solid paper.\n- The ELS method is very easy to implement. The details as well as codes are provided to ensure reproducibility.\n\nWeakness:\n- Although this paper is quite solid, I am still concerned about the motivation for introducing label smoothing. Indeed, the domains from the VLCS dataset are similar to each other. However, in other domain adaptation datasets containing significantly different domains (such as the Office-Home dataset), using smoothed domain labels might not improve the performance any further. As shown in the continuously indexed domain adaptation in Table 5, the improvement is 10%. Compared to tables 2 and 4, the improvement of ELS is less than 1%, especially in Table 4, ELS only surpasses the backbone method by 0.3%. So, is it possible that the proposed method is only effective on a dataset that has a small domain gap, and as the domain gap becomes smaller, the improvement would decrease?\n- When the encoder gets better, the generated features from different domains are more similar. However, a good encoder is also based on the quality of the discriminator. By using label smoothing, the discriminator can not be as discriminative as vanilla DAT which leverage hard labels, thus the performance of the encoder could doubtful. Is there any analysis of the learned discriminator?\n- Moreover, the improvements on the dataset Office-Home can be achieved by tuning the Hyper-parameter $\\gamma$, whose optimal value is not provided in the parameter analysis in Figure 4 (b). In my opinion, by tuning $\\gamma$, there is always a better classifier than the vanilla DAT.\n- In the introduction, the authors are motivated by the observation that VLCS has similar domains, thus label smoothing (smaller $\\gamma$) should be introduced, instead of the vanilla DAT ($\\gamma$=1). However, in Section 4.2, \u201cPACS can be easily discriminated\u201d, thus the optimal $\\gamma$ is smaller than VLCS. Is this a contradiction? Please give an explanation.\n- Additionally, the selection of $\\gamma$ is unclear to me. Do the authors conduct a grid search in the experiments? How to obtain the mean and std of $\\gamma$ to produce Figure 4 (b)?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is great in clarity and quality. The proposed ELS method is simple but the technical contribution is limited. Implementation details and codes are provided to ensure reproducibility.",
            "summary_of_the_review": "I have carefully read the whole paper. This paper is quite solid and has excellent completeness, however, there are still some concerns (see weaknesses). If the authors can address my concerns, I will consider raising my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethic concerns appear.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3679/Reviewer_kFo4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3679/Reviewer_kFo4"
        ]
    },
    {
        "id": "inkfs7C0QI",
        "original": null,
        "number": 2,
        "cdate": 1666576733271,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576733271,
        "tmdate": 1670256767008,
        "tddate": null,
        "forum": "GPTjnA57h_3",
        "replyto": "GPTjnA57h_3",
        "invitation": "ICLR.cc/2023/Conference/Paper3679/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Poor generalization on out-of-distribution data is a common problem among machine learning models. In classification tasks, one recently proposed solution is Domain Adversarial Training, which asks the model to predict the domain of the data sample in addition to its class, and encourages the model to learn domain-agnostic class-relevant features through an adversarial objective. However, this method is known to suffer from training instability. To that end, the authors proposed an extremely simple modification \u2014 label smoothing on the environment (i.e., domain) label \u2014 that improves not only the training stability but also robustness to label noise, convergence speed, and performance.",
            "strength_and_weaknesses": "Strengths\n1. In general, I am in favor of simple-but-effective improvements.\n2. The authors demonstrated via theoretical validations three advantages of the proposed method: (1) training stability, (2) robustness to label noise, and (3) non-asymptotic convergence speed. Further, the authors used empirical studies to complement the theoretical results. (1) is supported by Figure 2 and 5; (2) by Figure 4(a); and (3) by Figure 5.\n3. I particularly like section 3.4, where the authors explicitly faced the unrealistic assumptions in their theoretical validation and analyzed the \u201cempirical gap and parametrization gap\u201d.\n4. I am also glad about the fact that the authors intentionally organize the proofs and non-central results to the appendix, leaving the logic flow in the main text fluent and engaging.\n\nWeaknesses\n1. On the flip side of the first strong point I listed, one potential concern may be the lack of the so-called \u201cnovelty\u201d. I fully acknowledge the thoroughness and comprehensiveness of this work, yet the technical contribution (apart from the analyses and insights) is among the least \u201cinnovative\u201d I have seen. Afterall, label smoothing has been around for such a long time.\n\nMinor Things\n1. Would it be slightly more intuitive to write $D_S\u2019 = D_T + \\lambda(D_S - D_T)$ as $D_S\u2019 = \\lambda D_S + (1 - \\lambda) D_T$ instead?\n2. Typo. Page 2. \u201cphenomena\u201d instead of \u201cphenomenons\u201d.\n3. Minor grammar thing. Page 3, under Proposition 2. \u201cOne **may** argue that adjusting the tradeoff weight \u03bb can also balance AT and ERM**.** **H**owever, \u03bb can only adjust the gradient contribution of AT part, \u2026\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of high clarity and quality. Novelty is slightly questionable since the building blocks are not new. ",
            "summary_of_the_review": "This paper introduced a seemingly trivial modification, label smoothing on environment label, to a popular approach for out-of-distribution generalization. Notably, the authors showcased the advantages of the label smoothing in three different aspects through a combination of theorical and empirical analyses. Overall, I find this a very solid piece of work and would recommend for acceptance of this submission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3679/Reviewer_hUFN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3679/Reviewer_hUFN"
        ]
    },
    {
        "id": "EBe-5FLnaUa",
        "original": null,
        "number": 4,
        "cdate": 1667226474542,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667226474542,
        "tmdate": 1667226474542,
        "tddate": null,
        "forum": "GPTjnA57h_3",
        "replyto": "GPTjnA57h_3",
        "invitation": "ICLR.cc/2023/Conference/Paper3679/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a DAT label smoothing approach. The results are backed by the theoretical evidences. However, the main novelty is minimalistic.",
            "strength_and_weaknesses": "Strength:\n- The paper is well written.\n- The empirical results are backed by theoretical evidences\n- Simple solution\n\nWeakness\n- The paper used an already established idea with minor modifications and that is the major drawback of the work\n- The paper showed label smoothing approach but the question is wouldn't it attribute to the catastrophic forgetting of the discriminator. \nQuestions:\n1) The contribution seems marginal as most of the proofs (Theorem 1 and 2) and  can be directly inferred from previous GAN based works. \n2) Proposition 1, do we need both the $\\gamma(D_{S}-D_{T})$ and $\\gamma(D_{T}-D_{S})$ \n3) Proposition 2, would $p_{s^{'}}$ be $p_{t}$? If proposition 2 have issues for $p_{s}\\rightarrow 0$ then that would be similar for proposition 1, right?\n4) Proposition 3, for multilabel scenario would the $D_{Mix}$ provide any meaningful information? Though theoretically we could show the use of it but I am curious whether there would be any practical implication of that.\n5) Sec A.5, would the mixture provide any meaningful information or it would do only catastrophic forgetting to the discriminator?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper shared the code and seems like the results would be reproducible though I haven't checked all of them. ",
            "summary_of_the_review": "The papers proposes a label smoothing idea for DAT. The novelty is limited as the idea of label smoothing is already established in so many previous papers.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3679/Reviewer_LfGU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3679/Reviewer_LfGU"
        ]
    }
]