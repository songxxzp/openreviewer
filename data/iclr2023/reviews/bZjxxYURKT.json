[
    {
        "id": "QEJBzx21kq",
        "original": null,
        "number": 1,
        "cdate": 1666790770521,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666790770521,
        "tmdate": 1666790770521,
        "tddate": null,
        "forum": "bZjxxYURKT",
        "replyto": "bZjxxYURKT",
        "invitation": "ICLR.cc/2023/Conference/Paper1637/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes FedSpeed algorithm which has two components 1) prox-correction term and 2) extragradient perturbation where they relax a couple of assumptions made in prior work while achieving the same rate. \n\n",
            "strength_and_weaknesses": "Strength:\n\nThe experiment section is convincing to me. The way I see it, roughly speaking this paper uses both \u201cgradient tracking\u201d with additional \u201cregularization\u201d to deal with hetregenoty and improve generalization error which is interesting to me.\n\nComments:\n\nThere is no improvement in terms of achieved rates.\nAs mentioned on page 4, prox-term introduces an additional bias term. So, my question is do you show this extra gradient term can help to remove that bias?\nIn the title, it is mentioned \u201cHigher Generalization error\u201d, yet there is no theoretical proof for this claim (just verified through some experiments).\nAuthors keep mentioning that having larger local updates is a good property. However, there are a couple of papers such as [B] where it is suggested that there is no clear benefit to having a bigger local step size in FL, which contradicts the statement of this paper.\nMinor:\n\nThe following references [C] and [D] are using gradient tracking ideas (which show improvement over SCAFFOLD) so I would recommend authors compare their results with these references theoretically and empirically.  \n\n[B]: Woodworth, Blake E., Kumar Kshitij Patel, and Nati Srebro. \"Minibatch vs local sgd for heterogeneous distributed learning.\" Advances in Neural Information Processing Systems 33 (2020): 6281-6292.\n\n[C]: X. Liang, S. Shen, J. Liu, Z. Pan, E. Chen, and Y. Cheng. Variance reduced local sgd with lower communication complexity. arXiv preprint arXiv:1912.12844, 2019.\n\n[D]: Haddadpour, Farzin, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. \"Federated learning with compression: Unified analysis and sharp guarantees.\" In International Conference on Artificial Intelligence and Statistics, pp. 2350-2358. PMLR, 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is well-written.\n",
            "summary_of_the_review": "Please see the comments above!\nI will update my score based on the authors feedback.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1637/Reviewer_W3wF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1637/Reviewer_W3wF"
        ]
    },
    {
        "id": "b0kvu8jY-NZ",
        "original": null,
        "number": 2,
        "cdate": 1666807912474,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666807912474,
        "tmdate": 1666807912474,
        "tddate": null,
        "forum": "bZjxxYURKT",
        "replyto": "bZjxxYURKT",
        "invitation": "ICLR.cc/2023/Conference/Paper1637/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies federated learning for nonconvex optimization. The paper proposes a novel algorithm called FedSpeed, which applies a prox-correction term to reduce the bias and leverages gradient perturbation to improve the local generalization performance. FedSpeed achieves a fast convergence rate of O(1/T) and outperforms other baselines empirically.",
            "strength_and_weaknesses": "The paper is well-written and easy to follow. The proposed algorithm achieves a convergence rate of O(1/T) by incorporating prox-correction terms and gradient perturbation techniques. I have not checked the proofs in the appendix, though major arguments seem to be correct. \n\nMy concern is that the experiments only compare the test accuracy. The paper may also present the training losses or gradient norms to verify the theoretical analysis.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear. I appreciate the technical contribution of this paper, but it is hard for me to judge the originality since I'm not very familiar with federated learning.",
            "summary_of_the_review": "Overall, I think this is an interesting paper with solid contributions to federated nonconvex optimization.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1637/Reviewer_CRv7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1637/Reviewer_CRv7"
        ]
    },
    {
        "id": "jugtMd7gbO",
        "original": null,
        "number": 3,
        "cdate": 1667166497621,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667166497621,
        "tmdate": 1667170183553,
        "tddate": null,
        "forum": "bZjxxYURKT",
        "replyto": "bZjxxYURKT",
        "invitation": "ICLR.cc/2023/Conference/Paper1637/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a new federated learning (FL) algorithm called FedSpeed to mitigate the negative impacts of data heterogeneity and client drift problems in FL. The proposed method uses a prox-correction term on current local updates to reduce the bias introduced by the prox-term while retaining the benefits of the prox-term to maintain local consistency. The authors showed that FedSpeed has an $O(1/T)$ convergence rate to a constant error neighborhood and allows $O(T)$ local steps per communication round. The authors conducted experiments to compare FedSpeed with several existing FL algorithms.",
            "strength_and_weaknesses": "Strengths:\n1. This paper considered how to mitigate data heterogeneity and the resultant client drift phenomenon, which is an important problem in FL.\n2. The authors conducted a detailed theoretical convergence rate analysis\n3. The authors conducted comprehensive experiments with existing FL algorithms.\n\nWeaknesses:\n1. Although the proposed FedSpeed algorithm contains some interesting ideas (momentum plus prox) and is shown to have a strong convergence rate performance, the FedSpeed algorithm has a large number of hyper-parameters: $\\rho$, $\\alpha$, $\\eta$, and $\\lambda$, which could be quite difficult to tune in practice. Also, in the experiments, although the authors mentioned to some degree how to choose these parameters, it's not entirely clear from the paper how these parameters are related to each other.\n\n2. The $O(1/T)$ convergence rate in Theorem 4.4 seems questionable. Specifically, there is a constant $\\kappa>0$, which necessitates a very small local learning rate $\\eta_l$ and parameter $\\rho$. Furthermore, $\\eta_l$ depends on local steps $K=O(T)$. So in the end, the denominators of the first 3 terms on the right-hand side of Eq. (6) all have nontrivial dependence on $T$. Thus, it seems to claim $O(1/T)$ convergence rate is questionable (at least not holding without a careful justification).\n\n3. Remark 4.7 also seems misleading. It is actually not safe to the performance will improve as $K$ increases. For the same reason as above, in Corollary 4.5 where the authors plugged in values to derive the dependence of $\\Phi$ w.r.t. $\\Phi$, the dependence of $\\kappa$ on $T$ seems neglected. I would suggest the authors double-check all the calculations to make sure there is no error.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of this paper is clear and easy to follow in general. But there are quite a few typos and grammatical mistakes in this paper. The novelty of this paper is somewhat marginal since most of the proposed techniques exist in the literature and this paper tried to integrate these ingredients, although the integration is non-trivial. The reproducibility of this paper seems to be fine.",
            "summary_of_the_review": "This paper proposed a new algorithm called FedSpeed to mitigate the impact of data heterogeneity in FL and achieve a fast convergence rate under this setting. Although the problem is important and the proposed algorithm contains some interesting ideas, the algorithm has too many hyper-parameters to tune and could be hard to implement in practice. Perhaps also because of this reason, some of the convergence performance analysis seems questionable and may need more careful checking.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1637/Reviewer_P9p1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1637/Reviewer_P9p1"
        ]
    }
]