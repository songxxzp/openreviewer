[
    {
        "id": "FOn0m3Tcld",
        "original": null,
        "number": 1,
        "cdate": 1666580447590,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580447590,
        "tmdate": 1666580447590,
        "tddate": null,
        "forum": "663Cl-KetJ",
        "replyto": "663Cl-KetJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4019/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to select some graph data to improve the pre-training of GNNs. Existing works use all data to conduct pre-training, and it is observed that more data does not necessarily leads to better accuracy. The authors propose to use prediction uncertainty and graph properties to select data for training. Empirical results show that the proposed methods improve the accuracy of downstream tasks.",
            "strength_and_weaknesses": "Strength\n1. The observation that more data does not necessarily leads to better accuracy is neat.\n2. Using uncertainty and graph properties to select data makes sense.\n3. The empirical results are strong.\n\n\nWeakness\n1. Discussions of related works are shallow, which make the contributions unclear. For instance, in Section 5, the Pre-training in CV and NLP part is nothing more than saying that CV and NLP have used pre-training. However, the readers are interested in what methods they use for pre-training, especially for selecting data, and what are the differences from this work. The same applies to the Graph pre-training part. My suggestion is that when you discuss some closely related works, you should make the readers understand how these works on the methodology level, instead of only describing their targets. To my knowledge, uncertainty is widely used for sample selection is many areas. What are the differences from this work?\n\n2. Method design lacks explanation and does not make sense in some places. Eq. (4) calculates the mean of several graph property metrics (i.e., network entropy, density, degree). However, the appendix shows that these metrics differ in scale, which means that a metric may dominate the others when computing the mean. It also needs to be explained why we prefer larger values of these metrics (the network entropy part is clear). My guess that the intuition is that we prefer complex graphs that are likely to contain a diverse set of graph structures, and the proposed method tends to select graph with larger average degrees.\n\n3. Experiment and analysis need to be significantly improved. (1) What is the time used for data selection? Will it be a concern? (2) In Tables 1-2, different variants of APR perform the best on different datasets, what are the reasons? Is it related to the properties of the test graph? (3) Why not compare with some well-known GNN baselines, such as GraphSAGE, which can be trained in an unsupervised manner? (4) How does different graph properties affect accuracy? How does the weight of uncertainty and graph properties affect performance? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity needs to be significantly improved;\nQuality moderate;\nNovelty limited;\nReproducibility good as code is provided.\n",
            "summary_of_the_review": "This paper proposes to select data to pre-train GNNs. The idea makes sense but there are significant problems in method design and experiment analysis. Novelty is also unclear from the presentation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4019/Reviewer_pEvr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4019/Reviewer_pEvr"
        ]
    },
    {
        "id": "wgy2durrQn",
        "original": null,
        "number": 2,
        "cdate": 1666969429356,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666969429356,
        "tmdate": 1666969429356,
        "tddate": null,
        "forum": "663Cl-KetJ",
        "replyto": "663Cl-KetJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4019/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on pre-training graph neural networks (GNNs), aiming to identify a few most important examples for pre-training. For this purpose, a graph selector is designed, which uses the predictive uncertainty from a pre-trained GNN model as features for data selection. Extensive experiments on a few datasets prove the effectiveness of the proposed framework.",
            "strength_and_weaknesses": "Strengths:\n1. The problem of GNN pre-training is drawing growing attention in the graph machine learning field. Most works for GNN pre-training aim to collect more data or design better pre-training objectives to boost the performance. By contrast, this paper proposes a new idea, i.e., using data in a more efficient way by picking up those informative examples for training. Overall, I think the idea is interesting and novel.\n2. The proposed approach is quite intuitive.\n3. The experiment is extensive, where 13 graphs are used for pre-training. The results show that the proposed approach APT outperforms many existing methods.\n\nWeaknesses:\n1. In Sec. 3.1, this paper defines the predictive uncertainty by using the InfoNCE loss. Although this definition is intuitive, I feel like the design is heuristic and is lack of theoretical guarantee. It would be helpful to further discuss the advantage of the InfoNCE loss for measuring the sample importance.\n2. In terms of graph properties, this paper considers a few features of graphs, including network entropy, density, average degree, and, degree variance. Although they are very important to describe a graph, I feel like these properties are insufficient to yield a comprehensive characterization of the graph. I wonder whether it is possible to automatically learn some features as the structural properties of graphs.",
            "clarity,_quality,_novelty_and_reproducibility": "See above comments.",
            "summary_of_the_review": "See above comments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4019/Reviewer_z7Ev"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4019/Reviewer_z7Ev"
        ]
    },
    {
        "id": "PBJo84oCbfk",
        "original": null,
        "number": 3,
        "cdate": 1667176272134,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667176272134,
        "tmdate": 1667176272134,
        "tddate": null,
        "forum": "663Cl-KetJ",
        "replyto": "663Cl-KetJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4019/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a novel framework for cross-domain graph pre-training using fewer training samples. The paper first demonstrates the phenomenon of the \u201ccurse of big data\u201d - more training and graph datasets do not always lead to better performance in downstream node and graph classification tasks. Next, it presents a data-active graph pre-training (APT) framework that consists of two parts.  (1) The graph selector uses a loss that combines the predictive uncertainty of a data sample and its graph properties (entropy, density, etc.) to make an informed decision on the most instructive data samples for the model. (2) The pre-training model that learns from the graph selector and provides it guidance for better selection.  The paper implements the framework for different datasets and tasks to show that the proposed method improves performance for downstream tasks in different domains. The proposed method is compared with existing graph-based pre-trained models. The paper also includes results on the ablation of the different components of the graph selector, which is an important analysis for such a framework. ",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is overall well presented, and the methods are well described\n- It aims to tackle an important problem in the field and proposes an interesting strategy to do so\n- The experiments and results are thorough and demonstrate better downstream performance on a variety of tasks with fewer training samples\n- The presentation of the phenomenon of large training samples not adding to the performance is useful \n\nWeakness:\n\nIt would  be helpful if the following points regarding experimental settings, results, and implementation could be clarified:\n\n- Why were different models used for node (logistic regression) versus graph classification (SVMs)?\n- The ProNE model seems to outperform the APT model significantly for certain datasets. It would be helpful to discuss this result.\n- How is F (the maximal period of training one graph) different from the training epochs, or is F measured in seconds? \n- How sensitive is the model to the predictive uncertainty thresholds Tg and Ts (set to 3 and 2, respectively)? The decision choices for F, Tg, and Ts could be better described. \n- Are the baseline methods being re-trained on new datasets? If so, does it make sense to use the default hyperparameters for the same? \n\nMinor Comments:\n- I may have missed this, but what was the selection strategy for pre-training versus test data? \n- In appendix C, Table 3, the description of the \u2018msu\u2019 datasets implies it belongs to the protein category instead of the social category. The authors should make corrections where necessary.\n- In appendix C, specifically in Tables 3,4, and 5, it would be helpful to define the abbreviations |V| , |E|, and |G| in the captions. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The motivation, explanation, and results are clear. The clarity of the experiment section could be improved with reduced reliance on the Appendix. \n \nQuality: The paper is well-written, and the results support the claims. \n \nNovelty: Some of the ideas are present in the existing literature, but their implementation in the graph pretraining setting is new. The writing does a good job of placing the work appropriately with respect to existing literature.\n \nReproducibility: An anonymous code repository has been provided. \n",
            "summary_of_the_review": "The studied problem in this paper is important, the idea is new, and the experimental results are comprehensive. Specifically, it is demonstrated that the APT framework can achieve SOTA performance on downstream tasks by using less pre-training cross-domain data. The paper also shows this framework reduces the time complexity for pre-training. It would be great if some of the experimental choices and results could be described more clearly. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4019/Reviewer_xmXz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4019/Reviewer_xmXz"
        ]
    },
    {
        "id": "uRyqp6ZoTI",
        "original": null,
        "number": 4,
        "cdate": 1667186653922,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667186653922,
        "tmdate": 1667186653922,
        "tddate": null,
        "forum": "663Cl-KetJ",
        "replyto": "663Cl-KetJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4019/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies an interesting question in terms of which graph datasets should be selected for GNN pre-training tasks. The authors propose a novel graph selector that is able to provide the most instructive data for the model. The criteria in the graph selector include predictive uncertainty and graph properties (graph entropy, density, degree, etc). Besides, they propose a data-active graph pre-training (APT) framework, which integrates the graph selector and the pre-training model into a unified framework.\n",
            "strength_and_weaknesses": "**Strength**\n\n- Overall, the problem studied seems to be a novel angle in the context of GNN pre-training. Efficient pre-training is currently a new trend in other domains such as NLP and CVs with a focus on how to select informative training instances. It is important to shed some light in the context of how to better use the pre-training graphs data. \n\n- The core idea the author proposed intuitively makes sense. Indeed, graphs with certain characteristics such as higher network entropy, larger density, higher average degree, higher degree variance, or a smaller scale-free exponent will contain a larger amount of information. Even though the authors do not provide theoretical justification, empirically, this work does provide good insight into GNN pre-training. \n\n**Weaknesses**\n\n- The intuition figure 1 is a bit hard to fully understand. For the first row, when scaling up the sample size, does it mean we keep the same number of pre-train graphs but we vary the same portion of sample size from each pre-train graph dataset? For the bottom row, the total number of pre-train graphs in the paper is eleven and the number of graphs in the figure is only up to ten. The author should provide what is the benchmark results if we use all the possible pre-train datasets to properly compare \n\n- Directly using contrastive loss to define and measure predictive uncertainty is not questionable. \n\n- The notation of this paper is a bit messy. For example, both scaler and vector use non-bolded lowercase (degree vector and node degree) which is not conventional. \n\n- The selection mechanism is a bit complicated and heavily handcrafted. Currently, the graph selection policy uses a combination of 6 loss terms in total. Besides, loss terms work on a very different scale and are combined together with an additional time-adaptive parameter. I am not sure if this design is reasonable. Empirically, there is no proper ablation study on the choice of which graph property to be included in the pre-train graph selection criteria. \n\n- There is an additional proximal regularization term in the model design, which aims to better preserve the knowledge or information contained in previous input data when we train on new incoming data, a similar design component in the continue learning paradigm. I am not able to fully follow the rationale of this design. The pre-training problem is completely different from the catastrophic forgetting setting since we will normally shuffle the training order of the samples after each epoch. The claim of \"previous input data will be forgotten or covered by new incoming data\" is invalid if we shuffle the data training order. Besides, the $\\mathbf{\\theta}$ parameters learned from the first $j$ graphs, does it mean in terms of the memory complexity of the model will be $j$ times larger since we need to store the previous training iteration of the model?\n\n- Experimental results-wise, I have several concerns: 1) It seems the work is directly established on top of the GCC paper, but with completely different suits of pre-training datasets and downstream datasets. The choice of dataset section seems arbitrary and suspicious. Could the author directly work on the precious experiment setting in GCC? or what's the reason behind the complete switch of datasets? \n 2) The baseline uses are quite outdated, using baselines only coming before the year 2020. A lot of recent work in terms of GNN pre-training or graph data augmentation should be included [1] - [8].  3) One suggestion to the authors, since the paper uses a lot of downstream tasks and lots of numbers in the result table, a better way to present the results can be considered. For example, providing an average rank number across all the datasets for each method will be informative and helpful to deliver the message to readers.  3) I will suggest including the full pre-training dataset results in the experiment table to see the effectiveness of the pre-train graph selection scheme. Or should I interpret the GCC results as the full pre-training dataset results?  4) Can the author provide the training time comparison? It seems that to fully  \n\n- Can the author comment on the relationship of this work to some recent adaptive graph positive sample generation work? It seems they all achieve a similar end goal by finding the proper training samples [3] [5] [6] [7].\n\n\n[1] Hu, Ziniu, et al. \"GPT-GNN: Generative pre-training of graph neural networks.\" KDD 2020.\n\n[2] Xu, Dongkuan, et al. \"Infogcl: Information-aware graph contrastive learning.\" NeurIPS 2021.\n\n[3] Zhu, Yanqiao, et al. \"Graph contrastive learning with adaptive augmentation.\" Proceedings of the Web Conference 2021. 2021.\n\n[4] Zhu, Yanqiao, et al. \"An Empirical Study of Graph Contrastive Learning.\" NeurIPS 2021.\n\n[5] You, Yuning, et al. \"Graph contrastive learning automated.\" ICML 2021.\n\n[6] Lee N, Lee J, Park C. Augmentation-free self-supervised learning on graphs. In Proc. of the AAAI Conference on Artificial Intelligence 2022.\n\n[7] Han et al. \"G-Mixup: Graph Data Augmentation for Graph Classification.\" ICML 2022.\n\n[8] Hou, Zhenyu, et al. \"GraphMAE: Self-Supervised Masked Graph Autoencoders.\" KDD 2022.\n\n\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to Strength And Weaknesses.",
            "summary_of_the_review": "Please refer to Strength And Weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4019/Reviewer_ajow"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4019/Reviewer_ajow"
        ]
    }
]