[
    {
        "id": "pdeC8C5swi",
        "original": null,
        "number": 1,
        "cdate": 1666560434476,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560434476,
        "tmdate": 1666560434476,
        "tddate": null,
        "forum": "mWRngkvIki3",
        "replyto": "mWRngkvIki3",
        "invitation": "ICLR.cc/2023/Conference/Paper5919/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is about model distillation for dense prediction tasks, specifically for object detection and semantic segmentation. Prior works on this topic identify per-region distillation as an important ingredient to a successful knowledge transfer. This paper proposes a novel way to identify what regions should be distilled and with what importance (weight). These regions are modeled as masks, can be trained based on the respective task loss, and are not biased by the form of ground truth used (bounding boxes or segmentation maps). Given a teacher network, embedding vectors (one per region, the number of regions being a hyper-parameter) are learned that create a per-pixel similarity map with the feature maps, and then a (soft) masking. These embeddings (along with a corresponding weighting) are learned such that (a) the sum of all masked features can minimize the task loss, and (b) the masks are diverse. In a second phase, these masks are used for knowledge distillation into a student model. Additionally, the student also builds a mask (from the learned embeddings and the feature maps) and the distillation then becomes dependent on the agreement between teacher and student masks. Experiments on both object detection and semantic segmentation demonstrate consistent improvement over existing methods on multiple base models and backbones.",
            "strength_and_weaknesses": "Strengths:\n- The proposed method for generating masks is task-agnostic in a sense that no priors about the output space (bounding boxes or segmentation maps) are used.\n- It is good to see multiple detection and segmentation architectures and backbones evaluated.\n- The method is simple and appears to be consistently effective across model designs\n\nWeaknesses:\n- I found the motivation for identifying (un)important regions/pixels in a task-agnostic manner unsatisfactory. Most of the motivation builds on foreground vs. background objects. But this is only valid for object detection, not so much for semantic segmentation. The better motivation seems to be a \"balanced loss/impact\" for different regions when doing distillation.\n- I'm do not fully understand why it is \"optimal\" that only the pixels where both teacher and student masks agree should be distilled. This seems to be a guess (or a random justification) for something that works practically (i.e., the experiments show improvements).\n- Overall, I found the experimental setting to be insufficient.\n  - I'm missing more recent detectors like CenterNet [A] or DETR-based models like DINO [B-D].\n  - Choosing R101 and R50 as teacher and student, respectively, seems odd to me. The performance difference is not big between these two backbones (Table 1). Isn't the more practical situation for model distillation to have a very strong model distilling information into a much smaller one? I see such experiments for segmentation (with MobileNet backbones), but shouldn't this be the default for most experiments?\n  - Evaluations are only done on two benchmarks (COCO and CityScapes), although the method is advertised as general and task-agnostic. Given the rather small improvements over prior works across all experiments, I think additional evaluations on other datasets are needed to confirm consistent improvements of the proposed method over existing ones.\n- Significance of the improvement over prior works\n  - The improvement over FGD is consistent but minor in all results of Tables 1 and 2. This relates to my comment above about the experimental setting. With such consistent but only minor improvements, I find it necessary to provide more experimental evidence in the form of (a) additional datasets, and (b) reporting mean errors with standard deviation over multiple runs with different random seeds.\n  - I see similarly small margins in Table 4, where the proposed method outperforms randomly initialized tokens by only 0.6% AP. Again, are these improvements statistically significant?\n  - Why is the improvement much bigger for models trained from scratch (w/o ImageNet pre-training)? Is there any intuition for that? Also, are the teacher models also trained from scratch? If no, is this a practical situation where the teacher model can use ImageNet-pretrained weights, but the student cannot?\n- Table 1: Is there an explanation why the student model outperforms the teacher? The motivation and intuition of model distillation is to mimic the teacher's behavior. How does the model become better than the student?\n\nReferences:\n- [A] Objects as Points. Zhou et al. arXiv 1904.07850\n- [B] DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. Zhang et al. arXiv 2203.03605\n- [C] Dn-detr: Accelerate detr training by introducing query denoising. Feng et al. CVPR 2022\n- [D] DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. Liu et al. ICLR 2022",
            "clarity,_quality,_novelty_and_reproducibility": "- Overall, the paper is well written. The background and the proposed method are well introduced. Figures are also okay.\n- There is no mask weighting or refinement for semantic segmentation. This only becomes clear late in the paper. I think this distinction should be made clear earlier in the paper, when these methods are introduced.\n- How are the weights in Eq. 7 learned? Is it the same as the masks, with the task loss on a frozen teacher model? This is unclear in the paper.\n- Footnote 2 on page 4 does not say anything about \"easy to converge\", only that at convergence the original performance is almost fully recovered. The text says that mask tokens are easy and quick to train, but this is not apparent from the footnote.\n- The first paragraph on page 6 is written in a complicated way. It is assumed that a mask for the student can also be created, but this is not obvious. I would first state that the learned embeddings are used to create a student mask.\n- Table 4: What's the difference between the gray and the black marks (crosses and ticks)?\n- Figure 1: What activations are visualized here? Are these CAM activations?",
            "summary_of_the_review": "In summary, I think the experimental setting needs to improve to justify and solidify the improvement over prior works. I lean towards rejection of the submission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5919/Reviewer_EXB7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5919/Reviewer_EXB7"
        ]
    },
    {
        "id": "I6-j7n-h5w",
        "original": null,
        "number": 2,
        "cdate": 1666586864758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586864758,
        "tmdate": 1668824232481,
        "tddate": null,
        "forum": "mWRngkvIki3",
        "replyto": "mWRngkvIki3",
        "invitation": "ICLR.cc/2023/Conference/Paper5919/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a new distillation method that is more fine-grained than the previous bounding box distillation methods. It achieves performance improvement in both object detection and semantic segmentation.\n",
            "strength_and_weaknesses": "Pros:\n\n1. Figure 2 clearly illustrates the framework of the proposed method.\n\n2. This paper is overall well-written and easy to follow.\n\n3. This paper is novel. I have never seen similar works before.\n\nCons:\n\n1. For equation (7), how the weights of each mask are computed? What is the loss? Why this would not collapse to trivial solutions like emphasizing the masks that are easy to learn?\n\n2. In Tables 1 and 2, the improvement over FGD seems marginal (around 0.2-0.4), which may come from randomness.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Fair. (Though most parts are well-written, some parts are still confusing.) \n\nQuality: Good.\n\nNovelty: Good. (I didn't see this technique before)\n\nReproducibility: Fair. (Releasing the code can further improve the reproducibility)",
            "summary_of_the_review": "Though I still have some concerns, I think this paper is novel and well-written. Therefore, I would lean toward acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5919/Reviewer_mLef"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5919/Reviewer_mLef"
        ]
    },
    {
        "id": "MQ3JsJajEoH",
        "original": null,
        "number": 3,
        "cdate": 1666663154501,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663154501,
        "tmdate": 1666663195693,
        "tddate": null,
        "forum": "mWRngkvIki3",
        "replyto": "mWRngkvIki3",
        "invitation": "ICLR.cc/2023/Conference/Paper5919/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a knowledge distillation method for dense prediction tasks such as object detection and semantic segmentation. The method proposes to learn important regions (masks) adaptively for each image with task loss and mask diversity loss. Then the learned mask tokens, as well as mask weighting module, are adopted to distill teacher features to student. Results on COCO and Cityscapes datasets show the effectiveness of the proposed method.\n",
            "strength_and_weaknesses": "Strengths:\n1. The idea is novel and interesting. The mask tokens learned by minimizing the task loss can generate finer and task-related distillation regions, which are demonstrated through the visualizations and experiments in the paper.\n\n2. The proposed method is generic to various tasks, as it does not require ground-truth labels to select distillation regions as previous methods.\n\n3. The experiments on detection and segmentation tasks show that, MasKD achieves significant improvements compared to previous state-of-the-art methods. Sufficient ablation studies are provided to show the efficacy of the method.\n\nWeaknesses:\n1. The performance on ImageNet dataset is not state-of-the-art.\n2. How long does it take to learn the mask tokens for the teacher?\n3. Why the authors learn the mask tokens for only 2000 iterations? How about training it for more iterations?",
            "clarity,_quality,_novelty_and_reproducibility": "This work meets the bar of publication in this venue.",
            "summary_of_the_review": "I don't find major issues with this work. Please see my comments in strengths and weakness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5919/Reviewer_q4W5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5919/Reviewer_q4W5"
        ]
    }
]