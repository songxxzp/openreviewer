[
    {
        "id": "2xSs9sSrR6",
        "original": null,
        "number": 1,
        "cdate": 1665997676421,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665997676421,
        "tmdate": 1670828488351,
        "tddate": null,
        "forum": "UbH1jxLIPhb",
        "replyto": "UbH1jxLIPhb",
        "invitation": "ICLR.cc/2023/Conference/Paper4646/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel cost function for representation learning/stochastic embedding of data. Specifically, they adapt the information bottleneck functional, which aims at maximizing $I(Y;Z)-\\beta I(X;Z)$ by removing the term corresponding to the entropy of $Z$. Essentially, the authors result at maximizing $I(Y;Z)+\\beta H(Z|X)$, where the former term ensures that the representation $Z$ is useful for the downstream task, and where the latter maximizes the stochasticity of the representation given the input. The authors show that their approach results in a higher adversarial robustness and risk-controlled recognition performance.",
            "strength_and_weaknesses": "The paper is generally well-written and the math is easy to follow. The cost function is well motivated and its connection with existing methods (such as VIB) is explained. The paper is heavy on the experimental side, which I greatly appreciate. The experimental evidence is good (but see also below), and the supplementary material complements the paper nicely.\n\nRegarding the main weaknesses, I have to admit that the experimental setup and choices are not always entirely clear and that thus it is difficult to judge the significance of the experimental results. Further, some connections to the VIB and related methods are not fully clear and need to be expanded upon.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity and Reproducibility\nThe paper is generally clearly written, with a few exceptions that focus, unfortunately, on the experimental section. Training details are given in the supplementary material.\n- The fundamental difference to VIB is not entirely clear. Essentially, it is based on removing the term $H(Z)$ from $I(X;Z)$, maximizing only the conditional entropy rather than minimizing mutual information. (Matching the distribution of $Z$ to a standard Gaussian is only a consequence of the variational approach, not its goal.) Can you explain what the effect of this modification is? And how is this connected to the deterministic IB, where the term $H(Z|X)$ is removed? A better understanding of the effect these terms have on optimization would allow us to interpret MEIB better in the light of the literature.\n- On page 2, the first paragraph criticizes VAE and VIB to assign smaller variance to more uncertain inputs (see, e.g., the beginning of the next paragraph: \"lift such constraints\"). Indeed, it is later shown that MEIB also assigns smaller variance (lower entropy) to more uncertain inputs. Please explain how this is fundamentally different from VIB, and how MEIB now \"lifts a constraint\".\n- HIB by Oh et al. is not explained well. To better understand the effect of exchanging the VIB by the MEIB objective, their role in HIB should be clear.\n\n## Novelty\nThe MEIB objective is novel to the best of my knowledge, even if it is just a minor, and thus incremental change when compared to VIB or similar works.\n\n## Quality\nThe experimental evidence is large, covering various datasets, comparison methods, and hyperparameter settings. Results look very much in favor of MEIB. However, some of the choices made are not fully clear, and I would appreciate more information to accept the experimental evidence as fair.\n- Regarding the choice of QMNIST, it is not clear why a larger test set is required. Please comment and/or also add results about MNIST in the supplementary material.\n- The authors perform an ablation study in Appendix F, where it is shown that BN is essential to achieve good performance. Insights into why this is the case would be highly appreciated (and the authors themselves note that this is future work).\n- In Sec. 4.1, certain choices of $\\alpha$ were made and kept fixed for all experiments. However, there is an inherent trade-off between adversarial robustness and classification accuracy, and the authors themselves admit that VIB may be more robust for larger values of $\\alpha$/$\\beta$. The authors should either select $\\alpha$ differently for each task, or argue that MEIB is less sensitive to hyperparameter settings, which itself is a great advantage over VIB. \n- The results depicted in Fig. 4 will depend strongly on $\\beta$ (as shown in the original paper of VIB), and less on the variance $\\sigma^2$ of the Gaussian $r(z)$.\n- In Sec. 4.2, it is premature to talk about a trend, looking only at $D=2,3,4$. I would appreciate additional experiments with $D=100$ (or similar) to confirm the statements made in the paper.\n- In Fig. 9, should not at $\\alpha=0$ the performance of VIB and MEIB be identical? Can you explain the difference?\n\n## Minor\n- Why is the method called \"maximum entropy information bottleneck\" and not just \"maximum entropy bottleneck\"? After all, instead of (mutual) information, now the entropy is used for regularization.\n- On the first page, the authors write \"This [...] impels the embeddings to be close to a standard normal distribution, which is an explicit assumption that may not always hold true.\" Indeed, there are formulations of VIB that use more powerful priors $r(z)$, such as GMMs with learned parameters. Can you comment on how MEIB will perform in comparison to these?\n- Page 4: $\\sigma_\\theta^2 \\mathbf{I}$ is misleading, as it suggests that all elements on the main diagonal have the same variance.\n- Reference Thomas & Joy (2006) is broken.",
            "summary_of_the_review": "The paper is well-written and proposes an interesting adaption of VIB with small, but non-neglibible novelty. The experimental evidence is good, but not fully convincing because some of the experimental setups are not clear or well-justified. Regarding the theoretical argumentations, I would appreciate a more in-depth discussion of the effects of $H(Z)$, $H(Z|X)$, and $I(X;Z)$ as regularization terms, and how MEIB builds on them.\n\n*EDIT*: After discussion with other reviewers, I think that the paper is not sufficiently novel to merit publication. I lowered my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4646/Reviewer_PNBE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4646/Reviewer_PNBE"
        ]
    },
    {
        "id": "-UNRIwTfrt",
        "original": null,
        "number": 2,
        "cdate": 1666275458137,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666275458137,
        "tmdate": 1666275458137,
        "tddate": null,
        "forum": "UbH1jxLIPhb",
        "replyto": "UbH1jxLIPhb",
        "invitation": "ICLR.cc/2023/Conference/Paper4646/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents a Maximum Entropy Information Bottleneck (MEIB), which is a different take on the information bottleneck compared to the well-known variational information bottleneck.\n\n\n",
            "strength_and_weaknesses": "I liked very much the idea, both the simplicity and the motivation behind it. I also found very intriguing the property of MEIB being a lower bound on VIB. The experiments are extensive, combined with deep ablation studies; they convincingly show that models trained with the MEIB objective outperform existing methods in terms of regularization, perturbation robustness, probabilistic contrastive learning, and risk-controlled recognition performance.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written and of high quality in all respects. It is novel enough and totally reproducible.",
            "summary_of_the_review": "A high quality paper that may spur new interest in information bottleneck techniques for deep networks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4646/Reviewer_YBtS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4646/Reviewer_YBtS"
        ]
    },
    {
        "id": "CIv7Ql_qLRX",
        "original": null,
        "number": 3,
        "cdate": 1666734115730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666734115730,
        "tmdate": 1666734115730,
        "tddate": null,
        "forum": "UbH1jxLIPhb",
        "replyto": "UbH1jxLIPhb",
        "invitation": "ICLR.cc/2023/Conference/Paper4646/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This manuscript presents a new stochastic embedding method named Maximum Entropy Information Bottleneck (MEIB). Authors show that the well-known VIB is an upper bound of MEIB. Experimental results show that MEIB performs more robustly against adversarial attacks or other types of perturbation than VIB. The proposed method also outperforms the other four algorithms in the ReID missions on all three datasets. ",
            "strength_and_weaknesses": "Strength:\n1. A simple method but works well. It is also good that authors evaluated on a real ReID task. \n2. The manuscript is easy to follow. \n\nWeaknesses:\n1. In fact, it is easy to verify that the basic IB objective is also an upper bound of the so-called MEIB: \n$L_{IB}=I(Z,Y)-\\beta* I(Z,X)=I(Z,Y)-\\beta* [H(Z)-H(Z|X)]=[I(Z,Y) + \\beta* H(Z|X)] - \\beta* H(Z)= L_{MEIB}-\\beta* H(Z)>=L_{MEIB}$\nThe difference is just the absence of negative entropy of $Z$. \n\nAuthors have empirically demonstrated the effectiveness of the MEIB and its better performance of robustness as well as accuracy in specified scenarios. It would be more convincing if the authors present some theoretical explanations on why MEIB performs better than IB or VIB by just removing the regularization of H(Z) or the cross-entropy term. \n\nI can expect MEIB provide more accurate estimation on input uncertainty. However, it is hard for me to understand why MEIB is also robust to adversarial pertubations. Figure 3 is confusing to me, especially the VIB performs worst in adversarial robustness. This observation actually conflicts with previous literature (such as nonlinear IB or HSIC-bottleneck or even the classic VIB) which suggests that one can improve adversarial robustness by a regularization on H(X;Z) or even H(Z). \n\n2. The name (i.e., maximum entropy information bottleneck) of new method is also a bit confusing to me. I am not sure if the new method can be interpreted as a bottleneck or not. Usually, the IB or VIB involves a trade-off by maximizing a term, while also constraining another term. However, the new objective actually maximizes both I(Z,Y) and H(Z|X), i.e., there seems no bottleneck effect.\n\n3. In Section 4.2, can you validate that \"MEIB or its variant should perform better with much higher dimensional embeddings\"? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript is easy to follow. The method is also easy to be reimplemented, although authors did not attach the code. ",
            "summary_of_the_review": "A simple but effective method for stochastic embedding. Some results are not well justified. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4646/Reviewer_yBZK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4646/Reviewer_yBZK"
        ]
    },
    {
        "id": "9L4Kpc4Kva",
        "original": null,
        "number": 4,
        "cdate": 1667206920241,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667206920241,
        "tmdate": 1670515950061,
        "tddate": null,
        "forum": "UbH1jxLIPhb",
        "replyto": "UbH1jxLIPhb",
        "invitation": "ICLR.cc/2023/Conference/Paper4646/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Stochastic embeddings are explored to benefit its capability of associating uncertainty for robustness to noisy data. Instead of using a standard normal distribution prior for variational information bottleneck principle (that typically use KL to learn), this work uses maximum entropy as bottleneck objective to learn the embedding. The proposed approach outperforms prior based approach in terms of robustness.",
            "strength_and_weaknesses": "*Strength*\n- It is good to see the relationship between MEIB and VIB mathematically to understand the proposed approach better.\n- MEIB significantly improve the robustness of the embeddings compared to VIB, dropout and deterministic\n- MEIB performs much better than peer approaches in person re-id datasets\n\n*Weaknesses*\n- The proposed approach is incrementally novel from VIB. Please clarify the highlight of the motivation. Even mathematically, MEIB and VIB look similar, the motivation matters to justify the novelty.\n- Classification performance wise, MEIB is on-par with VIB in MNIST, though the robustness is much better, the trade-off is not discussed. Instead it seems a binary decision instead of a dynamic trade-off between error rate and robustness by tuning any hyperparameter. A suggestion is It might be nice to re-consider the equation (8) to add a (\\lambda * prior) item for an unmature example to mitigate this gap\n- More to see clarity section",
            "clarity,_quality,_novelty_and_reproducibility": "*Some clarification questions*\n- The motivation of using maximum entropy based approach can be described more clearly. Though the results showed better robustness results compared to VIB, it is unclear to me that what leads authors to use maximum entropy\n- In Figure 3(b), it is interesting to see when rejection rate > 90%, MEIB yields a larger error rate than other approaches which contradicts the  trend when rejection rate <70% (when > 70, MEIB already performs worse than dropout), any educated guess?\n- From figure 4, MEIB do cover larger area in embedding space than VIB. What if applying L2_NORM after last FC before logits layer. Then I assume both approaches perform similar? How to justify the effectiveness in this case? typically people use normalized embedding for retrieval as one use case.\n- Given MEIB is essentially increase \\delta in VIB adaptively, out of curiosity, whether a periodically update of \\delta based on VIB can perform similar, I believe there might be work investigating this.",
            "summary_of_the_review": "This work is self-contained and well written with sufficient discussion to support the claim, yet the novelty is limited and motivation is not very clear. Before the questions are answered, I would suggest a borderline reject.\n\nAfter discussion and reviewing authors feedback, the feedback answers most of my questions, but did not address the weakness part especially did not convince me of the sufficient novelty. Thus I'll clear update my recommendation as reject. This is a good paper, but not good enough.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4646/Reviewer_HaaB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4646/Reviewer_HaaB"
        ]
    }
]