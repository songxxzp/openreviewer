[
    {
        "id": "xz2qV7Uu8ST",
        "original": null,
        "number": 1,
        "cdate": 1666472598898,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666472598898,
        "tmdate": 1666472598898,
        "tddate": null,
        "forum": "cYijsVZhb5",
        "replyto": "cYijsVZhb5",
        "invitation": "ICLR.cc/2023/Conference/Paper3265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the transfer learning performance of vision language models (CLIP) in comparison with SimCLR. The primary motivation here is that SimCLR is essentially an image-image analogue of CLIP allowing for fair comparisons in performance while controlling for a variety of confounders. The paper claims three primary advantages of CLIP over VL. In terms of scaling, CLIP learns more transferrable representations when the data size increases. However, the required captions need to be descriptive. The authors leverage BLIP to score caption quality and show that datasets with higher caption quality lead to CLIP outperforming SimCLR as well as models trained with more low quality captions. The thirdly, the paper also provides experimental evidence that while variability in captions adversely affects VL training, this can be somewhat compensated by adding more data through text augmentations. ",
            "strength_and_weaknesses": "**Strengths**\n1. The paper is well motivated. It is of great importance to study the exact effects of vision-language training versus pure image based training. The idea of using SimCLR as a controlled baseline is inspired.\n2. The experiments have clear hypotheses and support the conclusions presented. Specifically, the use of multiple captions per image as text augmentations shows improvements over standard CLIP training, highlighting the importance of both scale and the variability of captions.\n3. The presented approaches to improve VL training for more transferable representations are simple and intuitive.\n\n**Weaknesses/Queries**\n1. The overall transfer accuracy reported for SimCLR is much lower than that reported in Chen et al's original work. The difference primarily seems to be an effect of the training dataset chosen (Imagenet in the original v/s COCO here). Could the authors comment on this? One suggestion could be to use Imagenet and Imagenet-captions (Fang et al.) to better understand the difference.\n2. Another improvement would be to study the effect of scaling up the architectures with more parameters (Resnet-2x, Resnet-4x) and perhaps with Transformer based architectures to understand if the conclusions hold across a variety of architectures/parameters.\n\nHowever, I understand that these studies are time-consuming, and the paper in its current form is a valuable study in itself.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and complete with references and experimental details. In terms of quality, as stated above, the experiments are well structured, and do address several open questions (with previously contradicting results) in a clear, concise manner. The ideas of using SimCLR, and BLIP based captioning to study the various effects of text supervision are novel and provide insight into the behavior of such models. The results are reproducible with the provided code. ",
            "summary_of_the_review": "Overall, this paper is a positive contribution the community with a well structured study comparing the effect of language supervision to that of image based contrastive training. The authors control for several of the confounders that previous studies lack, and provide actionable insights that will be useful for further understanding vision-language models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3265/Reviewer_G3mZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3265/Reviewer_G3mZ"
        ]
    },
    {
        "id": "TE3RRdYIMb",
        "original": null,
        "number": 2,
        "cdate": 1666551866290,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666551866290,
        "tmdate": 1666551866290,
        "tddate": null,
        "forum": "cYijsVZhb5",
        "replyto": "cYijsVZhb5",
        "invitation": "ICLR.cc/2023/Conference/Paper3265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the author investigates different popular datasets to explore the influence of the style, and scale of a dataset on vision-language representation pretraining task. The authors find that a dataset with sufficiently large and descriptive captions will be helpful in the vision-language representation learning and the representation learning can be negatively influenced by variability of captions. According to the findings, the author proposed data augmentation methods based on BLIP to boost the pretraining performance of the methods trained with different variants of datasets. ",
            "strength_and_weaknesses": "Strength:\nRaise an interesting topic that how many and what kind of captions are helpful for the vision-language pretraining.\n\nGive out some ideas of what kind of a vision-language dataset may help model to learn better representations.\n\nSufficient experiments on different kinds of setting and scale of image-caption training sets to show the influence of different aspects of captions on the representation learning.\n\nWeakness:\n\nSome phenomena can be further explained for the reader to get better understanding. For example,\nWhy cannot the \u201cmore descriptive\u201d BLIP-generated captions beat the performance of COCO dataset?\n\nThe paper evaluated the performance of representation learning only with classification-based transfer performance, while some other tasks can also be used to evaluate the capability of representations, i.e. image-text matching, and image textual grounding. The experiments from multiple different kinds of tasks can better support your claim.\n\nSome experiments can be more completed. i.e. CLIP_s 2 and 10 in the left table of Fig. 5.\n\nThe proposed data augmentation methods are based on BLIP model which is also a supervised trained with large scale datasets. I doubt if the learned knowledge will be leaked during the data augmentation process especially in the image captioning process mentioned in Section 3.3. This can be view as a kind of knowledge distillation of BLIP model which makes it less supportive for the claim.",
            "clarity,_quality,_novelty_and_reproducibility": "The problem raised in this paper, which to explore the influence of setting and scale of vision-language dataset on the representation learning, is that somewhat novel and interesting. \n\nThe paper is clear and easy to read. \n\nThe experiment is shown to be reproduced easily.",
            "summary_of_the_review": "In my view, this paper investigates an interesting topic and provide some preliminary rules of dataset design with some quantitative experiments for the vision-language learning.\nHowever, some of experiments are not so convincing and some of them need to be completed to support the claim better, like more differentiation downstream tasks. \nTherefore, I currently consider it marginally below the acceptance threshold. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3265/Reviewer_fwXk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3265/Reviewer_fwXk"
        ]
    },
    {
        "id": "WpSNaF7O28",
        "original": null,
        "number": 3,
        "cdate": 1666587341226,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587341226,
        "tmdate": 1669272157289,
        "tddate": null,
        "forum": "cYijsVZhb5",
        "replyto": "cYijsVZhb5",
        "invitation": "ICLR.cc/2023/Conference/Paper3265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper performs a thorough investigation of CLIP-like image representation learning.\n\nFirst, they try to compare the CLIP-like image-language contrastive loss and SimCLR-like image-only contrastive loss in a setting where many confounders are controlled. They conclude that adding language supervision can enable the model to learn more transferable representations than their image-only self-supervision method.\n\nThen, they investigate the effect of data properties on CLIP by training CLIP on three different datasets. They find that language supervision can hurt the model transfer performance in low-data regimes but can be helpful in large-scale settings. Also, \"descriptive\" captions and low variability of captions can improve the model transfer performance.\n\nFinally, based on their findings, they propose to filter non-descriptive captions by training a classifier to detect if a caption is COCO-like and mitigate caption variability by using GPT-J and COCO captions to generate new captions for existing image-caption datasets. They demonstrate the effectiveness of their proposed approach to CLIP.\n \n",
            "strength_and_weaknesses": "Strengths:\n1. The paper performs a relatively systematic study on how image-text contrastive loss is compared with an image-only contrastive loss and what properties of the training data can affect the performance of image-text contrastive loss. By drawing insights from their experiments, they can further demonstrate improvements.\n2. When comparing image-text contrastive and image-only contrastive losses, they do a good job of identifying potential confounders and carefully control them as much as they can, making their conclusions more convincing compared with several previous papers.\n3. Whether and to what extent language supervision can improve visual representation learning is an interesting and important topic. The conclusions of this paper can be helpful and inspire researchers in the future.\n\nWeaknesses:\n1. While image-text and image-only contrastive losses have some similarities, they are not mutually exclusive but can in fact be combined together [1]. Therefore, it would be better to see what are the fine-grained differences between the two paradigms and if combing them together can combine the best of them, instead of merely showing one is better than the other.\n2. They find that the \"descriptiveness\" of caption data can affect the model performance and define the term \"descriptiveness\" as the extent to which they refer to what is contained in an image, which is somewhat vague and it seems that \"descriptiveness\" is an antonym of the commonly used term \"noise\". They use a retrieval system to quantify the \"descriptiveness\" of a caption and find that COCO>CC>YFCC, which seems like they are just measuring the quality of the datasets. Because CC and YFCC are scraped from the web and the image-caption pairs are noisy and not well-aligned, it is normal and well-known that noisy data can lead to bad performance. A more precise definition and a more suitable quantification method are required so that the difference between their conclusion and the well-known fact that \"noisy data can be harmful\" is more clear.\n3. They choose to study the effect of caption variability on COCO, where the human-written captions are not quite diverse as noted in the paper. The Visual Genome dataset [2] can be used because it contains many more captions per image and each caption focuses on a specific part of its image, which can make their conclusions more convincing.\n\n\n\n[1] Mu, Norman, Alexander Kirillov, David Wagner, and Saining Xie. \"Slip: Self-supervision meets language-image pre-training.\" ECCV 2022.\n[2] Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A. and Bernstein, M.S., 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. It is unclear which specific BLIP checkpoints they use for measuring the descriptiveness of captions and image captioning.\n\nThe paper mainly presents some novel empirical findings and two practical ways of improving CLIP.\n\nThe code has been submitted and the hyper-parameters have been well-documented. ",
            "summary_of_the_review": "The paper focuses on a specific question (i.e. does language supervision lead to more transferable representations than using images alone?) and presents a comprehensive analysis to answer this question. While many of their conclusions are interesting and insightful, I do find some issues that can be improved as stated in the weakness part. I am willing to discuss and change my score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3265/Reviewer_Bcho"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3265/Reviewer_Bcho"
        ]
    },
    {
        "id": "tsBuh8RmuMa",
        "original": null,
        "number": 4,
        "cdate": 1667247841438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667247841438,
        "tmdate": 1667247841438,
        "tddate": null,
        "forum": "cYijsVZhb5",
        "replyto": "cYijsVZhb5",
        "invitation": "ICLR.cc/2023/Conference/Paper3265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a controlled empirical study that compares the capability of representational transfer between unsupervised image-only model (mostly SimCLR) and unsupervised vision and language models (mostly CLIP). The central question that the paper tries to answer is whether unsupervised vision-and-language models that exploit \u201clanguage information\u201d are richer than image only models.  Controls in the paper are related to experimental settings such as training data, training architectures, transformations. The primary observations emphasise that vision-and-language models are richer with caveats on the type of vision and language parallel data and scale. ",
            "strength_and_weaknesses": "Strength: \nThe paper contains an interesting exposition between single modality model  v/s multimodal model and the results confirm that multimodal models are indeed better at representational transfer (in general).\n\nWeakness:\n- The claim in the paper about \u201ctransfer of representations\u201d seems general (and tad too strong) however this is only evaluated over vision heavy transfer learning benchmarks. It is not clear if the behaviour would be similar for transfer learning on vision and language related benchmarks.   \n- The salient observations (relating to language - especially descriptiveness and variability) in the paper are perhaps fairly trivial considering the vast amount of similar work from the area of vision and language and semiotics [1, 2, 3, 4, 5 inter alia].\n- In general, the paper lacks empirical rigour, the paper contains a list of experimental interventions, but none are clear (expanding as the next point). \n\n[1] Automatic description generation from images: A survey of models, datasets, and evaluation measures. Bernardi et al. 2016\n\n[2] On the use of human reference data for evaluating automatic image descriptions. van Miltenburg. 2020\n\n[3] Ways of seeing. Berger. 2008 \n\n[4] Semiotics: the basics. Chandler. 2007\n\n[5] Underspecification in Scene Description-to-Depiction Tasks. Hutchinson et al. 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Section 3.2 lacks relevant citations from language and vision research where similar observations have been repeatedly made. Flickr captions are varied (I am also unable to relate the section to Grice, 1975, perhaps it should be to [6]). \n\n- The experimental settings and details in the paper are insufficient - such as: \n   - Section 3.3, it is not clear how the captions are designed and further: \n   - How should one quantify consistency? \n   - How should one measure completeness - is this based on ground truth COCO objects for a particular image? \n   - Without these details, it just seems arbitrary.  \n- Section 3.3 on how many captions are enough - what is \u201c(high-quality)\u201d captions - are the captions always factual? Nucleus sampling doesn\u2019t always generate factual captions. Perhaps the observations due to plateauing is due to irrelevant captions? \n\n[6] The construction of social reality. Searl 1995\n",
            "summary_of_the_review": "The current draft of the paper lacks experimental rigour, for an empirical paper most details are currently hand-wavy. The transfer based claims are perhaps supported for vision only transfer learning benchmarks. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3265/Reviewer_TB29"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3265/Reviewer_TB29"
        ]
    }
]