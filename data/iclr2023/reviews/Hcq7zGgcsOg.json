[
    {
        "id": "b-cXbsdTU1w",
        "original": null,
        "number": 1,
        "cdate": 1666591447498,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591447498,
        "tmdate": 1666591447498,
        "tddate": null,
        "forum": "Hcq7zGgcsOg",
        "replyto": "Hcq7zGgcsOg",
        "invitation": "ICLR.cc/2023/Conference/Paper5845/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper deals with NN models that are allowed to abstain from making a prediction whenever it is not sufficiently confident. These selective models are used to minimize risk of wrong prediction. Their model is based on SelectiveNet, which has 3 heads, one for minimizing the error for selected examples, one for deciding whether to select or reject examples and the last one for minimizing overall error rate. Given an order set of examples based on their difficulty scores, the curriculum-inspired training proposed by this work samples a mini-batch consisting of easy and hard examples to iteratively improve coverage and prediction accuracy of the NN model. They show improved performance of their technique over SelectiveNet. \n",
            "strength_and_weaknesses": "Comments:\n1. First paper to leverage the difficulty scores obtained by selective classification in curriculum learning framework.\n2. The contribution of this work is (1) defining the coverage term in the loss function. (2) use of mini-batch learning for curriculum inspired training. \n3. The experiments demonstrate the effectiveness of their technique over SelectiveNet. \n4. Have you tried different sampling strategies, instead of uniformly distributing easy and difficult samples in a mini-batch? Something like importance sampling based on the difficulty scores?\n5. Any theoretical guarantees of using this type of curriculum-inspired training? \n6. Any experiments/datasets where this technique failed to improve over vanilla learning?\n7. Any observations on getting worse results for this way of obtaining mini-batches? Some theoretical guarantees on convergence will be perfect but even empirical evaluation will be good. \n\nThis work is clearly presented and well written. The appendix also covers more details on ablation studies and convergence rates. I feel that this work is incremental and not very novel but useful nonetheless. \n",
            "clarity,_quality,_novelty_and_reproducibility": "<Covered in comments above.>",
            "summary_of_the_review": "This work is clearly presented and well written. The appendix also covers more details on ablation studies and convergence rates. I feel that this work is not very novel but useful nonetheless. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_F3no"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_F3no"
        ]
    },
    {
        "id": "6x5Kq8YpHQl",
        "original": null,
        "number": 2,
        "cdate": 1666595125463,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595125463,
        "tmdate": 1666595125463,
        "tddate": null,
        "forum": "Hcq7zGgcsOg",
        "replyto": "Hcq7zGgcsOg",
        "invitation": "ICLR.cc/2023/Conference/Paper5845/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies curriculum learning for selective neural networks. These are models that are allowed to reject examples if the confidence is low. The method proposed by the authors relies on carefully selecting the easy vs hard examples to be included in each mini-batch. Experiments are reported on CIFAR10, CIFAR100 and SVHN, comparing the proposed method with vanilla curriculum and non-curriculum baselines.",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-written and easy to follow.\n\n- The authors study an interesting topic.\n\nWeaknesses:\n- The level of novelty is limited in my opinion. The curriculum learning method is quite standard. The novelty consists in applying curriculum learning to selective nets, which does not represent a significant contribution.\n\n- The authors use the relative improvements in terms of error rate, which is deceiving. The relative gains in terms of accuracy would be very different. Nonetheless, the absolute gains seem rather small.\n\n- The authors use only small-scale and low-resolution datasets. It is thus unclear if the method would help on large-scale datasets where there are more examples per class available.\n\n- The authors should also relate and compare with curriculum methods not using example difficulty scores, e.g. [A].\n\n- In my opinion, the studied problem is related to the idea of dispatching test examples to different models based on image difficulty [B]. It would be nice for the authors to mention that hard examples could be processed by another (more confident) model instead of rejecting them.\n\n[A] Samarth Sinha, Animesh Garg, and Hugo Larochelle, \u201cCurriculum by smoothing.\u201d In Neural Information Processing Systems, pp. 21653-21664, 2020. \n\n[B] Petru Soviany, and Radu Tudor Ionescu. \"Optimizing the trade-off between single-stage and two-stage deep object detectors using image difficulty prediction.\" In International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, pp. 209-214, 2018.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, but the novelty is limited.",
            "summary_of_the_review": "My main concerns are related to the low-level of novelty and the rather small improvements on low-resolution datasets.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_oBun"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_oBun"
        ]
    },
    {
        "id": "5zOgtAo88cI",
        "original": null,
        "number": 3,
        "cdate": 1666630606367,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630606367,
        "tmdate": 1668991233302,
        "tddate": null,
        "forum": "Hcq7zGgcsOg",
        "replyto": "Hcq7zGgcsOg",
        "invitation": "ICLR.cc/2023/Conference/Paper5845/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work focuses on the training of selective networks, where the network knows then to reject to answer for unconfident examples. They propose a curriculum-inspired method to train selective neural works, where they (1) consider the target converge ratio when sampling batches, and (2) use the example difficulty score to to design the curriculum. They conduct experiments on CIFAR-10/100 and SVHN to demonstrate the usefulness of their method.\n",
            "strength_and_weaknesses": "The proposed idea is straightforward and makes sense to me. But I feel the empirical evaluation is not rigorous enough to validate the usefulness of the proposed method. Here are my concerns:\n\n(1) The improvement over the baseline SelectiveNet is quite marginal, e.g., ~0.5% error rate reduction on CIFAR-100 in Table 2.\n\n(2) There are no results on ImageNet. It is strongly encouraged to show ImageNet results for image classification. \n\n(3) There lacks diversity in network architectures. This work only shows results on a small CNN and VGG-16, both of which are probably far from the SOTA network architectures in computer vision, e.g., EfficientNet, FBNet, etc.\n\n(4) An existing work (\u201cWisdom of Committees: An Overlooked Approach To Faster and More Accurate Models\u201d ICLR 2022) shows that a couple of simple of metrics (e.g., maximum confidence, entropy, etc) can be used to make rejection decisions as well (although they use it in the context of model cascades). Due to the simplicity of those metrics, a comparison against them is necessary and we should expect an obvious improvement to justify the usefulness of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and reproducible. The ideas are new for selective neural networks but are very straightforward, so only contains modest novelty.",
            "summary_of_the_review": "The novelty of this work is not significant as the main ideas in this work are straightforward. In this case, I would like to look for rigorous empirical evaluation. At this point, I feel a ton of experiments need to be added to fully justify the usefulness of the proposed. The current performance improvement is marginal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_bj8f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_bj8f"
        ]
    },
    {
        "id": "JMq2L0cqYY9",
        "original": null,
        "number": 4,
        "cdate": 1666640893014,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640893014,
        "tmdate": 1666640893014,
        "tddate": null,
        "forum": "Hcq7zGgcsOg",
        "replyto": "Hcq7zGgcsOg",
        "invitation": "ICLR.cc/2023/Conference/Paper5845/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work introduces curriculum learning in training selective neural networks. \n- Selective Neural Network, also referred to as the abstaining classifier, have the option to reject an input, i.e., abstain from prediction. Typically, this abstention mechanism is learnt to be a measure of the difficulty of the input. \n- Curriculum Learning trains a standard neural network by sorting the inputs according to some difficulty / uncertainity measure and serving these data points in the order from easy to hard during training.\n\nThis work merges the above two ideas. Specifically, it trains a selective neural network using curriculum learning. Although one could leverage the vanilla curriculum learning strategy, the notion of rejection provides an opportunity to align the difficulty notion used in the curriulum learning and the selective neural networks. This work calibrates the input into easy and hard examples and serves each mini-batch to the abstaining network such that it can enforce a coverage constratint ( amount of examples that the network accepts for prediction ). Empirical evaluation shows that the proposed curriculum learning design for selective neural networks is better than vanilla techniques.\n",
            "strength_and_weaknesses": "\nStrengths:\n- Incorporating Curriculum learning in Selective Classification is indeed novel \n- Proposed carefully designed mini-batch design is indeed relevant to the coverage constraint being tackled in the Selective Classification\n\nWeaknesses:\n- Poor empirical performance (in many cases the improvement in error is in the range of 0.3-0.5 see Table~1, 2)\n- Lack of ablations that justify the design choices (selectivenet over other abstaining neural networks, pacing function, scoring function)\n- Does not address the computational overhead of the proposed method\n- It is unclear why this work chooses the proposed difficulty score estimator. It is by no means computational cheap as compared to simpler metrics such as entropy, softmax margin, etc. \n\nQuestions for Authors:\n\n- Choice of pacing function : For completeness, could you describe what the fixed exponential pacing function is? Did you try some other pacing functions? Since this problem ( CL + abstaining classifier ) is different than existing CL work, it is unclear why fixed exponential pacing function should be the de-fact choice in this scenario?\n\n- Choice of scoring function : Outlier detection algorithm ( Minimum Covariance Determinant (MCD) ) seems like an expensive way to measure the difficulty of examples. Is there a comparison on the training time split, i.e., how much time gets split between training the auxiliary classifier, how much goes in computing the MCD scores, how much in training the abstaining classifier? Have you tried other simpler scoring functions such as entropy, softmax margin, etc.?\n\n- How often is the MCD invoked? Are the scores refreshed every epoch in the training or only once in a while? \n\n- Could you elaborate the point regarding the use of inception network w.r.t. MCD?\n\n- Could you shed some lights as to why the gains are not impressive in Table 1 and 2?\n\n- Can you plot the hardness measure to see how well separated easy/hard examples are for various coverage levels?\n\n-  Is there a reason why SelectiveNet was used as the abstaining architecture? There are other superior networks than SelectiveNet, for instance, \n     - DeepGamblers : https://proceedings.neurips.cc/paper/2019/file/0c4b1eeb45c90b52bfb9d07943d855ab-Paper.pdf\n     - One-Sided Predictions : http://proceedings.mlr.press/v130/gangrade21a/gangrade21a.pdf\n     - Self-Adaptive Training : https://proceedings.neurips.cc/paper/2020/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf\n     - NNTD : https://arxiv.org/pdf/2205.13532.pdf\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is written in a simple manner and it is easy to follow. This work is indeed novel in that in integrates curriculum learning while training selective neural networks. It does not simply follow vanilla curriculum strategy but creates a mini-batch that respects the coverage constraint. \n",
            "summary_of_the_review": "This work is indeed novel in that in integrates curriculum learning while training selective neural networks. It does not simply follow vanilla curriculum strategy but creates a mini-batch that respects the coverage constraint. \n\nI had expected a bit more thoughts in the design choices such as scoring and pacing functions. Since these are very crucial to the success of proposed method. In addition, choice of selective net is also unclear. There are other abstaining classifiers such as DeepGamblers and NNTD with better performance. I would have expected a few ablations that justify the choice of selectivenet.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_xFiA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_xFiA"
        ]
    },
    {
        "id": "DrKnaLFBM2u",
        "original": null,
        "number": 5,
        "cdate": 1666784924995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666784924995,
        "tmdate": 1666784924995,
        "tddate": null,
        "forum": "Hcq7zGgcsOg",
        "replyto": "Hcq7zGgcsOg",
        "invitation": "ICLR.cc/2023/Conference/Paper5845/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a variation of SelectiveNet (Geifman & El-Yaniv, 2019) inspired by curriculum learning. The approach can be seen as a combination of selective classification and curriculum training. As illustrated in Figure 1, it aims to minimize a joint loss between the selective loss term constrained by the empirical coverage and standard loss. The authors provide several experimental results on benchmark datasets (CIFAR10, CIFAR100, SVHN) that their approach achieves lower error rates than the baseline (SelectiveNet). They also compare the performance with several variants of their methods to provide a rationale for their design choice.",
            "strength_and_weaknesses": "- Rationale: It is not clear why we need to combine the two approaches (selective classification and curriculum-inspired training). The authors need to clearly state a rationale for their choice (i.e., the combination of the two). It can be an argument that the two methods are complement each other, or extensive experimental evidence showing that the combination of the two outperform each one. \n\n- Ablation study: In addition to Inception (to provide a rationale for their difficulty score estimation) and Vanilla-Sekection (to provide a rationale for the mini-batch technique), another option can be added to provide a rationale for the selective classification technique in thier model: non-selective classification with the mini-batch technique.  \n\n- Hyperparameter choice: This approach uses several hyperparameters (e.g. coverage rates, alpha, lambda, etc.) that can impact the model performance. Therefore, several experiments are necessary to understand those. For example, all experimental results (Table1 - 4, Supp Table 6) show that the performance improves as it uses lower target coverage rates. Can you provide further experimental results to find what coverage rate is preferable for the model? Also, it would be good to see how the empirical coverage changes along with the target coverage in addition to the supplementary explanations.  \n\n- [Minor] Add missing references (see comment in Clarity - Minor 1)\n\n- [Minor] Make the main paper more self-inclusive (see comment in Clarify Minor 2)\n\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "### Novelty\nThe novelty comes from the idea that combines the selective classification problem and curriculum-inspired training. However, I evaluate it as a marginal novelty as it is not convincing enough why that combination is needed (as mentioned in Weakness - Rationale).\n\n### Clarity\nThere were no big obstacles to understanding its main idea and experimental designs. \n\nMinor)\n1. There are missing references for the sentence \"In many real life scenarios such as medical diagnosis, robotics and self driving cars ...\".\n2. If several tables (Table 1-4) are combined into one or two, the other interesting results can be moved from the supplementary to the main paper.\n\n### Quality\nThe quality of the paper needs to be improved by providing enough experimental evidence to support the approach and to understand the model's behavior (as mentioned in Weakness - Hyperparameter choice and Ablation study). \n\n### Reproducibility\nThis paper provides source codes for reproduction and uses publicly available datasets in their experiments. ",
            "summary_of_the_review": "Overall, this paper should be improved by \n(1) providing a good rationale for their design choice (i.e., why do we need to combine the selective classification and curriculum training), and\n(2) providing additional experiments to improve the quality and novelty of the paper.  \n\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_QZ16"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5845/Reviewer_QZ16"
        ]
    }
]