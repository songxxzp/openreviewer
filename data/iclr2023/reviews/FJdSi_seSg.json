[
    {
        "id": "mEk_xH5csp",
        "original": null,
        "number": 1,
        "cdate": 1666568510669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666568510669,
        "tmdate": 1670730872635,
        "tddate": null,
        "forum": "FJdSi_seSg",
        "replyto": "FJdSi_seSg",
        "invitation": "ICLR.cc/2023/Conference/Paper3687/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper makes the seemingly counterintuitive claim that the variance of losses should be rewarded, rather than penalized, in problems involving learning with noisy labels. Mathematically, the authors show that rewarding the variance of losses leads to parameter updates so that if the loss of the example is smaller than the average loss over the examples, the magnitude of the update is magnified, and vice-versa. This leads to an intuitive safeguard against the influence of points with noisy labels, which tend to have high loss. The authors present extensive empirical results that show that this type of variance regularization boosts the performance of existing methods for learning with noisy labels.",
            "strength_and_weaknesses": "## Strengths\n* Learning with noisy labels is a topic of high importance and interest to the wider ML community.\n* The proposed technique is novel and contrary to the traditional advice of penalizing the variance. The authors make a compelling case for the benefit of rewarding the variance in terms of attenuating the influence of noisy labels on the model during training.\n* The authors present extensive empirical evidence on a variety of scenarios that demonstrate the improved effectiveness of the method in boosting the performance of existing label-noise learning approaches.\n* Sensitivity analysis of the method\u2019s hyperparameter is provided in the supplementary that shows that it can be estimated using a validation data set\n\n## Weaknesses\n* Even though there are experiments with VRNL augmenting prior label-noise learning approaches, it would have been very compelling to include results with VRNL alone, without knowledge or reasoning about the transition matrix. For instance, a performance comparison of VRNL vs. no VRNL on a simple CIFAR10 example where, e.g., 10% of the labels are artificially made to be incorrect, would go a long way. \n* The method requires tuning of the hyperparameter $\\alpha$ and a wide range of values for $\\alpha$ is used for the various experiments on different scenarios in Sec. 4. From the plots in the appendix (Fig. 7), the method\u2019s performance is highly sensitive to the precise value of $\\alpha$. The authors did address that a validation data set can be used in a reliable way for CIFAR10, however, it is not clear whether this is always the case.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: the paper is well-written with a clear exposition\n* Novelty: the proposed approach is contrary to traditional wisdom and the authors make a compelling case for its benefits.\n* Quality: the empirical results provide support for the method when used with state-of-the-art label-noise learning approaches.\n* Reproducibility: the hyperparameters and the configurations used for the experiments are outlined in the results section (Sec. 4).\n",
            "summary_of_the_review": "This paper addresses a problem that is highly relevant to the ML community. It challenges the traditional wisdom that variance of losses should be penalized, and it instead makes a compelling argument for rewarding the variance. Even though a more direct comparison of VRNL would go a long way (see above), the authors present compelling empirical evidence on the benefits of the proposed method when used in conjunction with prior methods. In light of these considerations, I recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3687/Reviewer_VFMQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3687/Reviewer_VFMQ"
        ]
    },
    {
        "id": "XqQgBqG7lo",
        "original": null,
        "number": 2,
        "cdate": 1666572296171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666572296171,
        "tmdate": 1666918217693,
        "tddate": null,
        "forum": "FJdSi_seSg",
        "replyto": "FJdSi_seSg",
        "invitation": "ICLR.cc/2023/Conference/Paper3687/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper aimed to explore if the variance of losses should be always penalized when learning with noisy labels. The paper suggests adding a variance regularization to the loss, which will encourage the variance of losses. Consequently, when calculating the gradients of the proposed loss, if the loss of an example  is smaller than the expectation of the losses (which the authors assume is true in the case of examples with clean labels), the weight associated with the respective gradient will be increased and the example contributes more to the update of the parameter. If the loss of the example is larger than the expectation of the losses, the weight associated with its gradient is small and the example contributes less to the update of parameter. The assumption is that the latest will happen for noisy examples, because the network tends first to learn from clean examples and increasing the variance of losses would boost this memorization effect and reduce the harmfulness of incorrect labels. \nThe authors explain how to add the suggested regularization to several recent methods and show improvement. ",
            "strength_and_weaknesses": "Strength:\nThe overall idea of adjusting the weights, such that noisy samples influence weight update less, sounds like a good idea. The approach suggested to achieve that with addition of variance component to the loss is relatively simple (can be applicable as the authors show to several techniques) and yet relatively effective (improved results compared to the original method -- Tables 1 & 2).\n\nWeakness:\nThe paper mostly proves the effectiveness of the approach empirically, improving the results by some margin in case of the synthetic noise. The improvement on real noise ( Clothing1M  dataset) is relatively minor (Table 2 -- <0.5% improvement) and since there is no real theoretical proof that the suggested approach should work and under what conditions, the paper overall is not very convincing (in other worlds, that would not be my choice of an approach when dealing with noisy labels).\n In the absence of theory and given that the paper is not aiming to provide a SOTA technique, but discuss the loss variance penalization / encouragement, reorganization of the paper, e.g. :\n(1) bringing results from the appendix to the main body (e.g., uniting Table 1 and Table 4&5); (2) writing a paragraph with concrete conclusions and answer to the question posed in the title; (3)  clear conclusion on what types of noise, datasets  (balanced / not balanced) benefit from the approach, can help to strengthen the empirical contribution of the paper.    ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is mostly clear, however the experiments are scattered between the main body and appendix, which makes the reading harder.\n\nQuality:\nWhile the empirical validation is relatively thorough, the paper's writing quality can be improved.  \n\nNovelty:\nThe idea of exploiting early memorization of the network is not novel. Adding loss variance as a regularization to achieve better resilience to noise seems to be novel (to the best of my knowledge).  \n\nReproducibility:\nThe authors provide code as a supplementary material. ",
            "summary_of_the_review": "I have mixed feelings about this paper. I would suggest the authors reorganize the paper in a way that it provides some insights beyond just illustrating improvements to some methods (and providing the implementation details). \n1) The authors ask the question -- do we always need to finalize variances?  -- I could not find a clear to this question. It seems that the authors actually the opposite:  that the variances should be encouraged. So is the answer - always encourage losses?\n2) I am wondering why the authors did not provide the evaluation results of their method on WebVision dataset. Given relatively minor improvement on Clothing1M (< 0.5%), how will the results look like for WebVision? \n3) In my opinion, it's not clear why the authors push the results with little noise and strong noise to the appendix. It will be interesting to see those together with the results of Table 1 and maybe as a graph that shows the improvements (deltas)  as a function of noise. \n\nI will definitely reconsider my rating if the paper is reorganized and the main learning are clearly stated. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3687/Reviewer_zTmG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3687/Reviewer_zTmG"
        ]
    },
    {
        "id": "b_ppraBYYi7",
        "original": null,
        "number": 3,
        "cdate": 1667313740587,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667313740587,
        "tmdate": 1667313740587,
        "tddate": null,
        "forum": "FJdSi_seSg",
        "replyto": "FJdSi_seSg",
        "invitation": "ICLR.cc/2023/Conference/Paper3687/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "- the paper proposes a simple idea for robustness to label noise. The idea is to increase the variance of the loss function across training samples. A high variance could generally make sense since ideally the loss remains high for noisy-labelled samples during training while on the other hand the loss for correctly-labelled data is gradually decreased.\n\n- it further derives a form for the derivative of the proposed total loss w.r.t. the model parameters where the proposed loss can be seen as a reweighting of the gradient induced by individual training samples. It proportionally upweights the gradient of those samples with lower losses than the average loss and downweights the gradient of the samples with higher losses than the average loss.\n\n- it applies the method on top of three noise robustness techniques. Two of them are based on noisy label transition matrices, namely Forward and VolMinNet as well as a loss reweighting method.\n\n- the experiments show consistent improvement (although sometimes small) of the modified version of those three methods on different datasets, MNIST, CIFAR, Clothing1M) and noise types and rates (real, synthetic, symmetric, and asymmetric, 30% and 50%).\n\n- some additional studies are done to shed light on the inner workings of the proposed method. Interestingly, it is shown that the proposed method is more robust to erroneous estimation of the noisy label transition matrix.",
            "strength_and_weaknesses": "- the paper proposes a simple idea for robustness to label noise\n\n- some results are encouraging compared to the chosen baselines.\n\n- the method seems original in that it enforces high variance on the training data loss. However, quite a few methods that apply some form of selection/reweighting/distribution on the individual sample loss functions can be seen as increasing the variance of training losses. It is important to make such discussion in the paper and highlight the differences of the paper's proposal to such prior works both formally and empirically.\n\n- since the paper's claim on increasing variance for noise robustness is general, either the main experiments should be done on standard loss functions such as CE or the claims and theoretical motivation should be adjusted specifically to the case of noise transition matrices. \n\n- due to the simplicity of the approach (increasing loss variance) it is natural to assume complications might arise from the confusion of hard but correctly-labelled examples with noisily-labelled examples. Therefore, extensive discussions and experiments are required as to how such variance encouragement is theoretically or empirically unharmful for the difficult examples (e.g., in a scenario where the data is clean but quite complex to model).\n\n- different $\\alpha$s are used for different setups, what is the sensitivity of the method to $\\alpha$ (in each/some of those setups)? How is  optimized for each setup? Also, it seems like too-early or too-high enforcement of large variances can be detrimental to fitting the cleanly labelled data. Does the method require heavy hyperparameter tuning? If not, how robust is it to hyperparameters? Why is it robust?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "*Minor*\n\n- Fig 1: what dataset is the results reported on\n- page 4: \u201cnoise examples\u201d -> noisy\n- page 4: \u201cwhen work\u201d -> working\n- page 5 \u201clog determinate\u201d -> determinant",
            "summary_of_the_review": "The paper proposes a simple change of the loss function that encourages variance of the loss across training samples and shows this can bring robustness to training label noise in some scenarios. However, there are some important concerns with the empirical evidence that has been provided in support of the paper's proposal. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3687/Reviewer_JfaU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3687/Reviewer_JfaU"
        ]
    }
]