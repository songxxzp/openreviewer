[
    {
        "id": "Nki4nVDBoa",
        "original": null,
        "number": 1,
        "cdate": 1666393121592,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666393121592,
        "tmdate": 1666393121592,
        "tddate": null,
        "forum": "P4MUGRM4Acu",
        "replyto": "P4MUGRM4Acu",
        "invitation": "ICLR.cc/2023/Conference/Paper4892/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper defines three types of equivariance: correct, incorrect and extrinsic. The authors prove an upper bound on the accuracy when incorrect equivariance is used. Then the authors show empirically that using incorrect equivariance (e.g. using an equivariant neural network with incorrect labels) is worse than using a neural network, without the equivariant constraint. Finally, the authors shows that extrinsic equivariance can be helpful for policies in reinforcement learning. The authors experiment in two domains: 1) simple synthetic domain and 2) reinforcement learning tasks on robotic manipulation and DeepMind Control Suite.",
            "strength_and_weaknesses": "Strengths:\n\n1. Very important idea that the equivariance may be latent, and not necessarily captured in the input transformation.\n\n2. Useful definitions of correct, incorrect and extrinsic equivariances that capture the intuition of the problem.\n\n3. Really neat proof of Proposition A.1.\n\nWeaknesses:\n\n1. The authors address it in the end of their paper, but it would be nice to consider the method in other tasks / settings. I wonder if the power of the equivariant policy would be even further demonstrated in a realistic scenario, where the states are actual natural images. In that setting the extrinsic equivariances will be \"natural.\" Might be a cool experiment to try.\n\n2. Some minor comments: \n\n* It would be great if you could put Figure 2 on the first page to visualize what you mean in the discussion.\n\n* In Section 4.1. it is not clear what the relationship between $\\hat{\\rho}_x$ and $\\rho_x$ is. Can you give some examples for intuition early in the paper?\n\n* Small typo in the proof of Proposition A.1. \"labels and maximally\" <- \"labels are maximally\".\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is great. I understood the authors' message without much effort, and I do not expect most of the audience to be challenged with that. \n\nThe quality of the work is also very good: the experiments are extensive and they prove the point empirically. The theoretical result makes sense: it is simple, elegant and useful.\n\nI think the work is quite original, due to combining simple group theoretical arguments and strong experiments, and deserves its spot in the conference.",
            "summary_of_the_review": "I like the paper, because the problem of equivariance for neural networks is widely recognized, the proposed studies and theory are quite useful, and further research in that direction is promising, and may benefit from the findings in that paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4892/Reviewer_a4PJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4892/Reviewer_a4PJ"
        ]
    },
    {
        "id": "WJjrJ5wu6lo",
        "original": null,
        "number": 2,
        "cdate": 1666669874602,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669874602,
        "tmdate": 1666671360639,
        "tddate": null,
        "forum": "P4MUGRM4Acu",
        "replyto": "P4MUGRM4Acu",
        "invitation": "ICLR.cc/2023/Conference/Paper4892/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to learn *extrinsic* symmetries and equivariances in settings that have latent symmetries that are challenging to manually impose with standard group equivariance. The method is evaluated mostly on pixel-based settings with convolutional networks with a significant focus on pixel reinforcement learning tasks.",
            "strength_and_weaknesses": "Strengths\n+ The experimental evaluation is extensive and significantly improves upon established results for equivariant SAC/RAD (Fig 7, 9, 11) and DrQv2 (Fig 12).\n+ The appendix includes a significant amount of ablations and details, for example the tasks in Table 3 and Figure 27 on evaluating the DMC tasks with corruptions to the symmetry/camera pose\n\nWeaknesses\n+ While the paper is motivated by unknown latent symmetries, I thought it involve learning them. However, as reading the paper, I realized that the extrinsic equivariances still need to be manually specified as auxiliary equivariances that are simply other known transformations.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well-written. For one minor note, definition 4.3 seems incomplete and is missing the equivariance definition.",
            "summary_of_the_review": "I recommend to accept this paper as the idea of extrinsic equivariance is appealing and the experimental evaluation is convincing and improves upon many established results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4892/Reviewer_bQBQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4892/Reviewer_bQBQ"
        ]
    },
    {
        "id": "U-NgVFV4BZb",
        "original": null,
        "number": 3,
        "cdate": 1666870956638,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666870956638,
        "tmdate": 1668816429582,
        "tddate": null,
        "forum": "P4MUGRM4Acu",
        "replyto": "P4MUGRM4Acu",
        "invitation": "ICLR.cc/2023/Conference/Paper4892/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": " This paper explores the settings where an equivariant model can\n improve sampling efficiency and generalization by proposing a\n distinction between equivariant models that preserve the true latent\n symmetry and those which make it impossible to represent. These\n equivariant models that preserve the true latent symmetry even while\n not explicitly modeling it are called \"extrinsic\n equivariance\". Through a combination of theoretical and empirical\n results, this paper shows how extrinsic symmetries improve models. In\n fact, some of the improved generalization may come from\n \"extrinsically equivariant\" models transforming input data out of\n distribution in a way that non-equivariant models simply don't do.\n",
            "strength_and_weaknesses": " Overall, I view the paper as a useful contribution to the literature\n as it provides greater clarity for where equivariant models can really\n shine. While the paper does introduce an equivariant version of DrQv2,\n I feel the contribution is introducing the distinction between \"incorrect \n equivariance\" and \"extrinsic equivariance\".\n\n The main weakness of the paper is it doesn't introduce new models. All\n the theory and most of the experiments are done using existing equivariant\n models in the literature. Much of the benefits of equivariant models are\n already known as the related work highlights and there are already many\n methods for learning latent symmetries.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well-written and of high quality. I found it easy to follow and the experiments were well-chosen for demonstrating the different symmetries. The Appendix was detailed. I browsed the including software for the experiments but did not run them. I am reasonably confident that the work is reproducible. The work is novel, but it can feel a bit incremental as it's main contribution isn't a new model.",
            "summary_of_the_review": "A great contribution to the equivariant models literature, but the value is not in any one particular method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4892/Reviewer_6UwC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4892/Reviewer_6UwC"
        ]
    },
    {
        "id": "Nv-PuINy18",
        "original": null,
        "number": 4,
        "cdate": 1667006516328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667006516328,
        "tmdate": 1669437933336,
        "tddate": null,
        "forum": "P4MUGRM4Acu",
        "replyto": "P4MUGRM4Acu",
        "invitation": "ICLR.cc/2023/Conference/Paper4892/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work defines correct equivariance, incorrect equivariance, and extrinsic equivariance. Correct symmetry means that the model symmetry correctly reflects a symmetry present in the ground truth function, for which it is correct to enforce equivariance constraints. Extrinsic equivariance is when the equivariant constraint in the equivariant network enforces equivariance to out-of-distribution data. The authors theoretically demonstrate the upper bound performance for an incorrectly constrained equivariant model. In a supervised classification setting, they empirically show that a model with extrinsic equivariance can aid learning compared with an unconstrained model, especially in a low-data regime. Then, they explore this idea in a RL context and show that an extrinsically constrained model can outperform state-of-the-art conventional CNN baselines.",
            "strength_and_weaknesses": "Strength\n* This work empirically shows that extrinsic equivariance can still aid learning, which is very well-motivated contribution to model architecture with image transformation augmentation. \n* Empirical evaluations are strong and comprehensive, covering many kinds of transformations, supervised learning, and reinforcement learning.\n\nWeaknesses\n* The scale of the model and data used in these experiments are still considered small. Given the results in slightly larger data-regime, it's not clear how much it could be useful with large model and large data.\n* When the term correct, incorrect, and extrinsic equivariance/symmetry appear the first time in the paper, they are not well-explained. Even in the following, it's still unclear what the definition is and examples are somewhat unclear. I wonder if it would be useful to at least have section in appendix to clarify is the main paper space is too constrained.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I consider the work novel. However, as pointed out in the weaknesses part, there are still rooms to improve clarity.",
            "summary_of_the_review": "I lean to acceptance but would like the authors to improve the clarify and comment on effectiveness in large-data regime.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4892/Reviewer_ZXUL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4892/Reviewer_ZXUL"
        ]
    }
]