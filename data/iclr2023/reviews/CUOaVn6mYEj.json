[
    {
        "id": "0ENVSDdyppn",
        "original": null,
        "number": 1,
        "cdate": 1665751235144,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665751235144,
        "tmdate": 1665751235144,
        "tddate": null,
        "forum": "CUOaVn6mYEj",
        "replyto": "CUOaVn6mYEj",
        "invitation": "ICLR.cc/2023/Conference/Paper2215/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces the Hierarchical Sliced Wasserstein (HSW) distance that is a variant over the Sliced Wasserstein (SW) distance.\nIn a nutshell, the HSW distance improves on the computational complexity of SW in cases where $d \\gg n$ by lowering the burden related to the projections.\nTo do so, projections on subspaces of dimension $k$ is performed and the final $L$ projections are projections from this $k$-dimensional space to the real line.\nBoth properties of this HSW and experimental validation on generative modeling tasks are presented.",
            "strength_and_weaknesses": "* Strengths\n    * The method is sounded.\n    * The paper is well motivated. Notably, the introduction clearly states the contribution of the paper, and is easy to read despite the technicity of the proposition\n* Weaknesses\n    * The paper is already very dense (_eg._ parts of the experimental results discussed in the paper are only available as supplementary material) yet it raises several questions that would be worth investigating in the paper.\n\nIn more details, I really enjoyed reading the paper and believe the contribution is interesting.\nHowever, I have the following remarks / questions:\n* The paper is already dense, and I get that one has to make choices, but I believe an application of HRT on other sliced divergences would have helped illustrate the genericity of the proposition (though the matter is discussed in the paragraph \"Applications of HRT\").\n* Do we have any intuition for a link between HSW and SW (apart from the links exhibited in Proposition 3 between HSW and Max-SW -- resp. SW and Max-HSW)?\n    * When reading Proposition 3, it is unclear to me why Max-HSW would differ from Max-SW since, in spirit, they both correspond to taking the max over projections on a line. This is probably due to a misunderstanding on my side, but maybe adding a discussion on this point could help the reader better grasp the difference between those two.\n    * This is also related to the discussion on \"Distributions of final projecting directions in HRT\", which concludes:\n        > To our knowledge, the manifold $\\mathcal{S}$ has not been explored sufficiently in previous works, which may be a potential direction of future research\"\n        * I believe providing informed thoughts on properties of this manifold would be a real plus for this paper.\n* The section on the benefits of HSW when $d \\gg n$ is not very convincing since all complexities are said to be \"proportional to XXX\". Either the constant is the same for all and this should be stated clearly or a quantitative study of the running times as a function of $L$ would be welcome.\n    * This is also reflected in the experiments: could the \"computational complexity\" criterion be replaced by running times to make sure that in Table 1, for example (but this holds for other results too), each sub-group of methods indeed corresponds to cases where the HSW computation is cheaper and nothing is hidden in the $\\mathcal{O}(\\cdot)$ notations. The same applies to the \"Projection complexity\" criterion.\n\n\nI have a few extra minor comments:\n* \"which has columns are\" -> \"which columns are\"\n* [Titouan et al., 2019] -> [Vayer et al., 2019]\n* End of Section 3: the last sentence ends weirdly, probably a bad copy-paste\n* In Figure 3, using markers could help identify paired methods (which HSW variant corresponds, in terms of computation, to which SW variant)\n* \"In the Table 2, we reports\" (p. 8) -> report",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, of good quality and the information provided in the paper should be sufficient to reproduce the experiments, I believe.",
            "summary_of_the_review": "The proposed method is very interesting and the paper is well-written.\nI would appreciate more details in places (see previous section) and a comparison of complexity based on running times to make sure the comparison is fair.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2215/Reviewer_YGxc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2215/Reviewer_YGxc"
        ]
    },
    {
        "id": "pxMscYw8nB",
        "original": null,
        "number": 2,
        "cdate": 1666621957453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666621957453,
        "tmdate": 1666621957453,
        "tddate": null,
        "forum": "CUOaVn6mYEj",
        "replyto": "CUOaVn6mYEj",
        "invitation": "ICLR.cc/2023/Conference/Paper2215/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes an extension of the Sliced Wasserstein (SW) distance, denoted as Hierarchical Sliced Wasserstein (HSW) distance. At its core, for $d$ dimensional samples and $L$ slices, HSW first chooses $k$ bottleneck projections and then applies $L$ linear projections on the $k$ bottleneck slices, leading to slicing complexity of $\\mathcal{O}(k(d+L))$ compared to $\\mathcal{O}(dL)$ of the SW distance. The need for such hierarchical slices comes from deep learning applications where the number of samples $n$ (in mini-batch processing) is often smaller than their dimensionality $d$, and the computational complexity of the SW distance is dominated by the projection complexity. The more interesting part of this paper is the definition of Overparameterized Radon (OR) transform and its combination with the Partial Radon (PR) transform that results in the proposed Hierarchical Radon transform used to show that the proposed HSW is indeed a metric. Lastly, the authors demonstrate the application of HSW and compare it with SW for generative modeling on CIFAR10, CelebA, and Tiny ImageNet. They show boosts over FID and IS for different numbers of slices $L$ while demonstrating that HSW is faster than SW.  ",
            "strength_and_weaknesses": "### Strengths: \n\n* The paper is well-written and well-motivated.\n* The paper is theoretically sound, and the experiments support the authors' claims.\n* While conceptually simple and algorithmically trivial to implement, the approach is well supported by the theory of the proposed Hierarchical Radon transform, rooting the simple algorithm into the solid mathematical ground.\n* The ablation study in Table 2 is really great! \n\n### Weaknesses: \n\nI do not see any major flaws in this paper. However, below are some of the points that, in my opinion, could have improved the paper: \n\n* When defining the Max-HSW variation, the maximization could be performed on the $k$ bottleneck slices (I find this very interesting), on the $L$ linear combinations of the bottleneck slices, or jointly (as it is currently defined in the paper). It would have been exciting to see the effect of only maximizing over the $k$ bottleneck slices. Moreover, it seems that one should require the $k$ bottleneck slicers to be linearly independent; what enforces this constraint?\n\n* I am not sure if I follow these sentences: \"Since HGSW has more than one non-linear transform layer, it has the provision of using a more complex non-linear transform than the conventional Generalized sliced Wasserstein.  Compared to the neural network defining function of GRT, HGSW preserves the metricity.\" As far as I can see, if any of the projections (i.e., the $k$ bottleneck or the $L$ following slices) become nonlinear, then the metricity of HGSW would also depend on the defining functions and the injectivity of the (in this case) generalized hierarchical Radon transform. Could you please clarify this?\n\n* Currently Propositions 1 and 2 could read as if $HRf$ and $HRg$ are equal for **some** $\\theta_{1:k}$ and $\\psi$ then $f=g$, which I am sure we agree that is not true. I think what the proposition is saying is if $HRf(\\cdot,\\theta_{1:k},\\psi)=HRg(\\cdot,\\theta_{1:k},\\psi)$ for $\\forall \\theta_{1:k}, \\psi$ then $f=g$. Maybe rewording the propositions could increase the clarity. \n\n### Minor editorial: \n\nTypo - Page 3: \"the set of absolutely integrable function on\" should be \"the set of absolutely integrable functions on\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is well-written, and the concepts are crystal clear.\n\n* The paper is of high quality. \n\n* The proposed concept and the resulting algorithm are very simple and practical. The Overparametrized Radon and the Hierarchical Radon transforms are novel and could be of interest to the community. ",
            "summary_of_the_review": "Overall, I think the paper is well-written, and it addresses a valid challenge for the application of sliced-Wasserstein distance in training neural networks when using small mini-batches. The proposed method is algorithmically simple, yet it is mathematically rigorous, and it provides a significant improvement over the SW distance. \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2215/Reviewer_EUsv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2215/Reviewer_EUsv"
        ]
    },
    {
        "id": "a-0SRzkamSY",
        "original": null,
        "number": 3,
        "cdate": 1666641743397,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641743397,
        "tmdate": 1666641743397,
        "tddate": null,
        "forum": "CUOaVn6mYEj",
        "replyto": "CUOaVn6mYEj",
        "invitation": "ICLR.cc/2023/Conference/Paper2215/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduced a new variant of sliced Wasserstein (SW) distance, the hierarchical sliced Wasserstein (HSW) distance. The HSW distance is derived based on the proposed hierarchical Radon transform (HRT), which is a composition of partial Radon transform and overparametrised Radon transform. In the HSW, compared probability measures are first projected onto k bottleneck directions, yielding new probability measures $f_\\mu(t_{1:k}, \\theta_{1:k})$ and $f_\\nu(t_{1:k}, \\theta_{1:k})$, then the generated measure are projected onto a one-dimensional space using the partial Radon transform (PRT) in a $k$-dimensional subspace with L directions $\\psi$. The HSW is defined as the expectation of Wasserstein distances between the obtained one-dimensional probability measures over the distribution of $\\theta_{1:k}$, a product probability measure of k uniform distributions on d-dimensional unit spheres, and the distribution of $\\psi$, a uniform distribution on a k-dimensional unit sphere. In short, final projections in the HSW are linear combinations of the k bottleneck projections, and the weights of the k projections are given by L random samples drawn from a uniform distribution on a k-dimensional unit sphere.",
            "strength_and_weaknesses": "Strength:\n\nTheoretical properties of the HSW are derived in the paper, including its metricity, its link to SWD, max-SWD, approximation error of HSW, and computational complexity. Compared with the standard SW distance, the proposed HSW has the advantage of lower computational cost when both the data dimension and the number of final projections are much larger than the number of bottleneck projections k.\n\nThe proposed HSW is evaluated on a benchmark deep generative modelling experiments to compare it with other variants of SWDs. Experiment results show that models trained with HSW can produce images of higher qualities in terms of FID and IS scores. The HSW also demonstrated higher convergence rate with approximately the same computational costs than SWD.\n\nWeakness:\n\nMy major concern is on the experiment side. The main context of the proposed work is to improve computational efficiency of the optimal transport-based distances, in particular sliced-based ones, but the proposed HSW is only compared with the standard SWD, ignoring most of the recent efforts which were also discussed in the paper. The proposed method is also only evaluated on a generative modelling experiment, but other applications of slice-based Wasserstein distances can be found in other related papers, e.g., colour transferring and sliced iterative normalising flows [1][2].\n\n[1] Soheil Kolouri, et al. \"Generalized Sliced Wasserstein Distances.\" NIPS, 2019.\n[2] Biwei Dai, and Uros Seljak. \"Sliced Iterative Normalizing Flows.\" ICML, 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to read. The proposed method is novel and clearly explained.\n\nQuestions:\n(1) How are the computational complexity and project complexity are computed? What are the units of those complexities as reported in Tables 1 and 2?\n(2) The proposed HSW is constructed by recursively applying PRT and ORT to compared measures, is it possible to stack more PRTs and ORTs to construct SWD variants in the same way as in the HSW? Would they still be distance metrics? And what are their computational complexities compared to the HSW?\n(3) What is the performance of the HSW when H>1?\n",
            "summary_of_the_review": "The paper can be improved if more comprehensive experiments are included to support the effectiveness of the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2215/Reviewer_CrfW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2215/Reviewer_CrfW"
        ]
    },
    {
        "id": "Z3hEPjHfB8",
        "original": null,
        "number": 4,
        "cdate": 1666771089881,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666771089881,
        "tmdate": 1666771712334,
        "tddate": null,
        "forum": "CUOaVn6mYEj",
        "replyto": "CUOaVn6mYEj",
        "invitation": "ICLR.cc/2023/Conference/Paper2215/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Applications of Wasserstein distance to large-scale machine learning problems have been limited by its enormous computational cost. The Sliced Wasserstein (SW) distance and its variants increase computational efficiency using random projections but suffer from low accuracy if the number of projections is not large enough. In this work, the authors propose a new family of sliced-Wasserstein distance measures, called Hierarchical Sliced Wasserstein distances (HSWDs). \n\nHSWDs are based on projecting original measures into $k$ one dimensional projected measure via Radon transform with $k\\ll L$ and where $L$ is the number of projections. Towards this end, the authors define hierarchical Radon transform, which has an injectivity property. This latter yields the metricity property of HSWDs. Several numerical experiments in generative modeling are conducted on CIFAR10, CelebA and Tiny ImageNet datasets.",
            "strength_and_weaknesses": "### Strength ###\n- HWD: novel variant of sliced Wasserstein distance based on a hierarchal Radon transform.\n- HWD is a proper distance in the space of probability measures.\n- Equivalence between HWD, SWD, and Max-SWD.\n- Numerical experiments on generative modeling with HWD.\n\n### Weaknesses ###\n- In most experiments, the number of projections $L$  is taken significantly large. So, this affects badly the computational cost of calculating the linear mixing of the bottleneck projections. This mean, at last for me, HWD gains efficiency with a price of a large number of projections. At that time, one can consider vanilla SWD. I will appreciate it if the authors give more details about this point. \n- The paper lacks comparison with other approaches like augmented sliced Wasserstein (Chen et al. ICLR 2022) and distributional sliced Wasserstein (Nguyen et al. ICLR 2021).",
            "clarity,_quality,_novelty_and_reproducibility": "The code is attached to the supplementary materials. I thank the authors for that. Reproducibility of results is guaranteed.\nThe paper is easy to follow and well-explained. \n\n### Typos ###\n- Page 1: \"sample complexity\": it is a little bit confusing, since this term we use for the number of training samples that we need to supply to a learner algorithm. It will be better if the authors clarify this point as the bounding gap between the evaluation of divergence on two probability measures versus samples from these measures. \n- Page 5: \"success of overparametrization in DNN\" (Add a reference or details).\n- Page 7: \"while comparing HSW with the SW.e experiments while comparing the HSW with the SW\" --> \"while comparing the HSW with the SW\"\n- References: \"radon\" --> \"Radon\"\n",
            "summary_of_the_review": "the paper introduces novel variants of sliced Wasserstein distance to reduce the computational cost. The paper lacks comparison with other approaches like augmented sliced and/or distributional sliced Wasserstein distances.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2215/Reviewer_XSPt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2215/Reviewer_XSPt"
        ]
    }
]