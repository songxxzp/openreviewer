[
    {
        "id": "DkI8hK7lGjk",
        "original": null,
        "number": 1,
        "cdate": 1666646544724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646544724,
        "tmdate": 1666646544724,
        "tddate": null,
        "forum": "mQpmZVzXK1h",
        "replyto": "mQpmZVzXK1h",
        "invitation": "ICLR.cc/2023/Conference/Paper3443/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This submission proposes a new model-based RL method grounded in representation learning. It seems like the central idea is to impose linearity on the transition function, which is formulated as the dot product between $p(z|s.a)$ and $p(s'|z)$, which are both parameterized as neural networks.",
            "strength_and_weaknesses": "Honestly, I had a hard time reading this paper and believe that my understanding is somewhat limited (cf confidence score) due to scarce exposure to relevant prior work. I'll hence focus my review on the experimental section and am happy to adjust my score in the rebuttal if necessary.\n\nThe paper opens with a (quite bold) claim that the proposed algorithm remedies the computation difficulties of RL; in particular, the authors claim sample complexity (I assume this is what is referred to with \"statistical complexity\") independent of the number of states. While the theoretical analysis suggests that this is the case, I would have liked to see this demonstrated in experiments more clearly.\n\nThe experiments themselves are performed with common MuJoCo tasks, either as defined in gym or in dm_control. Generally, I think that learning curves for up to 1M steps should at least be shown in the Appendix; the plotted learning curves clearly indicate that further learning progress can be anticipated (according to the MBBL paper). The MBBL paper mentions that they limited their experiments to 200k steps since model-based methods are often slow to run and converge much earlier than model-free algorithms. What is the wall-clock time in your case? Besides, assuming that the authors ran the Dreamer and Proto-RL baselines themselves, could you add them to plotted the learning curves?\n\nFor Table 1, maybe the authors could provide more exhaustive evaluation by training on both ET and non-ET variants, for the sake of completeness?\n\nRe the dm_control experiments: my understanding is that `hopper_hop` and `humanoid_run` are not sparse-reward; the writing should be adjusted accordingly.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity (minor points):\n- Please add a citation when mentioning MBBL for the first time\n- How is the unsupervised pre-training phase for Proto-RL handled in the experiments?\n\nReproducibility:\n- A source code release would be great; based on the claimed contributions, easy reproducibility and re-use would be very beneficial to the RL community.",
            "summary_of_the_review": "As admitted above, this is a low-confidence review. However, I don't see the claims made in the introduction that suggest a fundamentally more efficient RL algorithm supported by the evidence presented in the experimental section. Yes, the learning curves are equal or better than SAC, but evaluation is limited to the first 200k environment steps. I think that the paper could definitely benefit from more exhaustive experimental analysis, even if it's placed in the Appendix.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3443/Reviewer_9PEm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3443/Reviewer_9PEm"
        ]
    },
    {
        "id": "7Ug3LGjHzC",
        "original": null,
        "number": 2,
        "cdate": 1666670227254,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670227254,
        "tmdate": 1666670227254,
        "tddate": null,
        "forum": "mQpmZVzXK1h",
        "replyto": "mQpmZVzXK1h",
        "invitation": "ICLR.cc/2023/Conference/Paper3443/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a representation view of latent variable models in linear MDPs, and further propose a computationally efficient algorithm to implement it for both online and offline RL. The authors also theoretically and empirically demonstrate the proposed approach.",
            "strength_and_weaknesses": "**Strength:**\n\n+ The paper is well organised and well written. \n+ The idea is reasonable and also well supported both theoretically and empirically.\n\n**Weaknesses:**\n\nI only have some minor concerns:\n\n+ Theorem 1 shows that the sample complexity largely depends on $|\\mathcal{A}|$. This seemingly implies that the proposed approach might struggle in the environments with continuous action space. Is it right?\n+ I understand that the factorization in (3) is consistent with the linear MDP setting. I am wondering how this could be extended to the nonlinear MDP setting? In my opinion, a more straightforward and general factorization should be $T^*(s'|s, a)=\\int_{\\mathcal{Z}}\\int_{\\mathcal{Z}} p^*(z|s)p^*(z'|z,a)p^*(s'|z')dzdz'$, which explicitly models an MDP over the latent space. Is there any insight over this?",
            "clarity,_quality,_novelty_and_reproducibility": "+ This paper is well clarified.\n+ To my knowledge, such a theoretical and methodological connection between latent variable models and linear MDPs seems new. This would be of interest to the community.\n+ The current version might be not sufficient to reproduce the results presented in the paper. ",
            "summary_of_the_review": "The paper builds a nice bridge between latent variable models and linear MDPs, with well supports on both theory and practice. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3443/Reviewer_F81s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3443/Reviewer_F81s"
        ]
    },
    {
        "id": "iJmfbWjTycG",
        "original": null,
        "number": 3,
        "cdate": 1666673107663,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673107663,
        "tmdate": 1666673107663,
        "tddate": null,
        "forum": "mQpmZVzXK1h",
        "replyto": "mQpmZVzXK1h",
        "invitation": "ICLR.cc/2023/Conference/Paper3443/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The article proposes a new RL algorithm where the idea is to model the dynamics of the system by using a latent representation space. Using a latent representation to capture the dynamics, it allows one to use\nvariational methods (and ELBO optimization) for doing representation learning over the MDP (Section 3). This is done by connecting latent representation models to Linear MDPs. In Section 3.1, the authors then use the maximization of ELBO approach to learning representations while simultaneously learning the policy (see Algorithm 1). The proposed approach is to alternate between representation learning steps and policy updates, these policy updates being made by planning on the learned model. One of the main points in the algorithm is to add an exploration bonus similar to the one proposed in REP-UCB. Section 4 provides a theoretical analysis of the algorithm. Section 5 describes experimental results obtained in the online RL setting over different classical control benchmarks. In that section, the proposed approach (with both discrete and continuous representations) is compared to classical RL (model-free and model-based) methods. It demonstrates a high efficiency on many of the tasks.\n",
            "strength_and_weaknesses": "First of all, the paper is very dense, providing both theoretical results and practical algorithms. As a consequence, it is sometimes difficult to understand exactly what the contributions presented are -- for instance in Section 4, the theoretical analysis is quite short (regarding the dozens of appendix pages) and it is hard to understand what to conclude exactly on the approach from the provided theorem which is very hard to catch without providing more explanations. Similarly, when describing the algorithms, many different aspects are still unclear to me (and not better detailed in the appendix). So I have mixed feelings about the article, and I think that the authors have to update the writing to make the paper more pedagogical and to clearly state the critical aspects of their approach. This is the type of paper that better fits in a journal than in a conference. \n\nHere are some concrete questions: \nPage 4: Connection to Ren: many different notions arise in this section like representation complexity, We understand that it is related to sample complexity (and Section 4). The last sentence \"The LV-Rep with....and learning\" is completely unclear at this moment in the article. I would suggest moving this paragraph into Section 4 instead since, at this point, it does not really bring relevant information to the reader, and make the message unclear. \nConcerning the ELBO approach which defines a variational encoder for the transitions, this approach has already been proposed in different papers like \"Temporal Difference Variational Auto-Encoder - Karol Gregor et al.\" for instance. Can you discuss the differences between this paper for instance ?  Is it really new ? \nOn page 5, the w^pi(z) notation appears without any clear definition. If we understand that it is a rewriting of the value function by using the latent representation, it would be nice to give a clear definition, and to comment a little bit more.\nThe use of random feature representation (described in the appendix only) is crucial to understand how you handle continuous representations. And the paper does not give clear definitions of this particular aspect. I would recommend providing more explanations about this point directly in the paper. It is not clear for instance how you choose the value of 'm', the distribution P(xsi_i) and what is the impact of these choices (Note that, in the experimental section, these choices are not detailed). \nEquation 8: the bellman equation is rewritten with an exploration bonus. This equation is connected to the notion of 'planning' in the paper and in the algorithm where (line 7) a new policy is obtained by planning using the exploration bonus. I don't understand exactly how this step is done. Moreover, in Section 6, you state that SAC is used as your planner in LV-Rep. Maybe it is just a matter of semantics, but in my mind, SAC is not a planning algorithm. Can you make this point more clear ?  I think that depending on the background of the reader, the use of the term planning can be misleading.\nSection 4 is actually too short to capture the theoretical contribution you are making in the paper. For a reader which is not specialized in theory, I would suggest better explain the content of Theorem 1, and providing a small insights about the steps to obtain such a theorem\nOn the experimental side, I would be happy to have more information about the impact of the representation size on the performance. Experiments with a high-dimensional space would also be interesting to evaluate if the approach is also able to capture a relevant representation space when observations are in high dimension -- this is not a critical remark but would strengthen the paper. \n",
            "clarity,_quality,_novelty_and_reproducibility": "(see general comments)",
            "summary_of_the_review": "In conclusion, the paper explores the use of a VAE as a representation model learned together with the policy. One of the interests of the VAE is that it allows the definition of an exploration bonus to guide the learning, which is certainly the critical reason for the good performance of the approach. The paper is interesting to the community, with theoretical and practical contributions (I did not check the proofs), but the writing makes the paper quite hard to follow. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3443/Reviewer_Y1QJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3443/Reviewer_Y1QJ"
        ]
    },
    {
        "id": "xGNKB4YiLQ6",
        "original": null,
        "number": 4,
        "cdate": 1667105779257,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667105779257,
        "tmdate": 1667105779257,
        "tddate": null,
        "forum": "mQpmZVzXK1h",
        "replyto": "mQpmZVzXK1h",
        "invitation": "ICLR.cc/2023/Conference/Paper3443/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a provable reinforcement learning algorithm for low-rank MDPs. The key difference from past work on this literature is that the transition model $T(s' \\mid s, a)$ has a decomposition $<\\mu(s'), \\phi(s, a)>$ which can be an infinite-dimensional inner product, however, it is required that $\\phi$ and $\\mu$ are both probability measures. This is viewed as a latent variable model where $\\phi(s, a)$ denotes the vector $(p(z \\mid s, a))_{z}$ and $\\phi(s')$ denotes the vector $(\\mu(s' \\mid z))_z$. The transition dynamics can then be expressed as $\\int p(s' \\mid z) p(z \\mid s, a) dz. \n\nIt is claimed that the above decomposition, in addition to more flexibility due to infinite-dimensional $z$, offers a flexibility of computationally-efficient maximum-likelihood estimation through ELBO and a simple-sampling scheme for $T$ obtained via $z \\sim p(\\cdot \\mid, s, a)$ and $s' \\sim p(\\cdot \\mid z)$. However, infinite-dimensionality of $z$ complicates things as well, in particular, requiring $Q$ function to be approximated using a kernel (or a two-layer neural network).\n\nThe presented algorithm LV-REP follows previous work (REP-UCB, FLAMBE), and estimates the model based on data collected so far, and then plans with an optimism-based elliptic bonus to find the next policy to collect data from. PAC results are presented for online learning (and a similar result for the offline case). Experiments are presented on Mujoco tasks showing comparable performance to prior work, while the presented approach also has theoretical guarantees. ",
            "strength_and_weaknesses": "Strength:\n\n1. Generalization to infinite-dimensional $z$ adds more flexibility, however, there is also constraints on the decomposition restricted to probability measures.\n2. PAC guarantees\n3. Experimental results showing comparable results\n\nWeakness:\n\n1. The paper seems to make two wrong assertions regarding prior work:\n\n- _\"Our definition of LV-Rep is more general than the original definition in Agarwal et al. (2020), which assumes |Z| is finite.\"_ This seems wrong. While FLAMBE assumes a finite rank, the features are not restricted to probability measures, in particular, features can take negative values. They also argue that the non-negative rank can be much larger than the rank of $T(s' \\mid s, a)$. I would like to understand how this trade-off works.\n\n- Regarding block MDP _\"p\u2217(z|s, a) is a deterministic measure supported on the latent state that generates the next observation.\"_. This is not correct. The block MDP algorithms mentioned also handle stochastic transitions. I believe this is a typo and $p(z \\mid s, a)$ is a stochastic measure that generates the next state and $p(s' \\mid z)$ is the emission distribution for generating an observation $s'$ based on latent state $z$. \n\n2. It is not clear to me when is the ELBO approach more computationally-efficient than the MLE approach in FLAMBE, etc. that the paper criticizes. Doesn't the ELBO approach also require maximizing over $q$? What is the empirical optimization that is being performed in the code? If this can be described clearly in a pseudocode and the advantage over MLE is more clearly described, then it would be helpful.\n\n3. Representing the Q function requires approximation using kernel or neural network. This requires certain conditions to be satisfied that I am not sure, how easy, it will be to satisfy in practice. Experiments, however, show that this is possible at times which makes this less concerning.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the difference from past work is quite nicely explained. A pseudocode that more closely resembles the code that was run for the experiments, would significantly improve the clarity.\n\nThe paper is closely related to previous work on low-rank MDPs. In particular, the paper is very related to REP-UCB and FLAMBE algorithms in past work, that the paper cites and describes. The proof strategy is also closely related to REP-UCB. Therefore, the novelty is somewhat limited. However, this is fine as the impact of results is more important than the novelty of techniques. A key difference, however, is that the paper presents a nice set of experiments.\n",
            "summary_of_the_review": "I lean towards a weak acceptance. My two main concerns are (i) clarity on how and when is LV-REP MLE more efficient than FLAMBE, (ii) how the trade-off work with FLAMBE that assumes finite-rank but allows the decomposition into $\\mu, \\phi$ to take negative values, and (iii) the two wrong assertions stated above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3443/Reviewer_F7os"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3443/Reviewer_F7os"
        ]
    }
]