[
    {
        "id": "fv_zQ0Fml0",
        "original": null,
        "number": 1,
        "cdate": 1666482394911,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666482394911,
        "tmdate": 1669254952572,
        "tddate": null,
        "forum": "XYUaprBSDjp",
        "replyto": "XYUaprBSDjp",
        "invitation": "ICLR.cc/2023/Conference/Paper5837/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes an architecture that can leverage external knowledge to make better decisions in RL problems. The architecture implements an attention mechanism that dynamically chooses to attend to an internal knowledge mapping or external ones. It is trained end-to-end via the Gumbel-softmax trick. Experiments are conducted on grid-world and robotic tasks. ",
            "strength_and_weaknesses": "Strengths:\n* Incorporating external knowledge into RL policy is an important problem. \n* The method outperforms proposed baselines.\n* The paper is easy to read.\n\nWeaknesses:\n* The experimental settings and results do not support claims made by the authors. First, the author \"knowledge\" as \"*can be any commonsense, suggestion, guideline, principle, specification, rule, restriction, or previously learned knowledge described in formal or informal language*\", but the knowledge in the experiments is simply commands or high-level options. The authors only construct instruction-following tasks rather than knowledge-incorporating tasks, which are much broader in scope and more difficult to solve. \n* The proposed method is not novel. Attentional architecture is the de-facto choice for building instruction-following agents [1]. The authors simply train the agent end-to-end without proposing any specific techniques that help the policy incorporate the knowledge effectively. Knowledge such as constraints or environment specifications may require more special treatments than simple attention mechanisms [2,3]. \n* There are other ways to design the attention mechanism but the authors do not explain why they choose the presented one. They also do not compare with alternative designs in the experiments. For example, instead of mixing the policies, I can mix at internal representation level h = sum(w_i * h_i), where each h_i is the representation of a piece of knowledge. Then I construct a single policy based on the mixed representation. This avoids having to perform the gumbel-softmax trick. \n\n[1] https://arxiv.org/pdf/1711.07280.pdf\n[2] https://arxiv.org/pdf/2010.05150.pdf\n[3] https://arxiv.org/pdf/2101.07393.pdf\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper does not present a novel idea. The experiments do not reflect the broad range of problems claimed to be solved by the authors. The authors do not consider alternative architectures and justify why the proposed design was chosen. ",
            "summary_of_the_review": "The merits of the paper are marginal. I do think the idea presented would be significant for the community. I am learning towards rejecting the paper. \n\n=====After Rebutal=====\n\nI have read the authors' response and decided to keep my current score. My main point of rejection is that the paper over-claims the generality of their methods while not having experiments to back up and avoiding comparing with related work. The definition of knowledge, which can be \"commonsense, suggestion, guideline, principle, specification, rule, or restriction, described as a knowledge mapping\" is very vague without defining what the mapping actual is (e.g. how is that policy related to the optimal policy?). I suggest the authors revise the scope of their claims and provide a more specific specification of their setting. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_ce5j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_ce5j"
        ]
    },
    {
        "id": "ebjjzwUoBG",
        "original": null,
        "number": 2,
        "cdate": 1666653205279,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653205279,
        "tmdate": 1666653205279,
        "tddate": null,
        "forum": "XYUaprBSDjp",
        "replyto": "XYUaprBSDjp",
        "invitation": "ICLR.cc/2023/Conference/Paper5837/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces an agent architecture intended to facilitate the reuse of existing knowledge, called \u2018knowledge grounded RL\u2019. The knowledge grounding consists of a dictionary with randomly initialized learnable keys paired with hard-coded policies (these could possibly also be pre-trained, in principle) which is available to the agent. There is one additional policy in the dictionary which is not hard-coded or pre-trained, but trained online. Action selection happens by attending over this dictionary.\n\nThe proposed architecture is evaluated on six MiniGrid and two robotics tasks, with a specific set of hard-coded policies for each environment. With the right set of pre-baked policies, the proposed architecture outperforms the chosen baselines on both environments.\n",
            "strength_and_weaknesses": "- Weakness: experimental evaluation. The MiniGrid environment is clearly a toy one, which might be suited for a proof of concept, but is not sufficient for a serious evaluation. The two robotics tasks are quite elementary as well; more complex tasks exist, and would be very natural applications for the proposed method - a very simple example being stacking blocks.\n- Weakness: the method as it is used in the paper relies heavily on hand-crafted pre-baked policies. The paper does not show how agents learn policies that can be transferred to agents learning other tasks.\n- Strength: the related literature section is fairly comprehensive and covers a good number of related topics.\n- Strength: the proposed method is explained clearly.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: As stated in the \u2018strengths\u2019 section, the proposed method is explained well, and the relevant literature section makes clear where the paper is placed relative to the rest of the field. The title (which is used throughout the paper) is a bit misleading, in the sense that the \u2018knowledge\u2019 used to \u2018ground\u2019 the RL in fact consists of pre-baked policies. While it is technically possible to argue that this is a form of grounding RL in knowledge, it would be more accurate to phrase it in terms of something along the lines of \u2018sub-policy re-use\u2019.\n- Quality: As stated in the weaknesses, there are major gaps in both the usefulness of the method that is demonstrated, and the scope of the experimental evaluation. The idea of reusing policies has merit in principle, but it is also difficult to execute well, and this paper does not reach the bar of a convincing demonstration. To start with, the authors don\u2019t address the question of how to pick pre-trained policies for a particular task. They don't show how to automate a system based on re-using policies, how to create diversity in the learned policies, or how to deal with the probably very large dictionary of policies that agents would have to learn to navigate. As it is, the paper does nothing to convince the reader that the proposed method would go beyond toy tasks where it is easy to provide pre-baked policies.\n- Novelty: while the HRL and options literature is referenced, there is a degree of disconnect between the claims about the proposed method made in the section discussing HRL/options, and the experiments and implementation that is actually presented. Paraphrasing, the claim is made that re-used policies in KGRL can be much more varied than skills in HRL or options. That might in theory be true, but in practice the pre-baked policies provided to the agents in the paper are very good examples of skills or options.\n- Reproducibility: no concerns.",
            "summary_of_the_review": "The idea of re-using existing knowledge, or more specifically, of policies trained or defined previous to the agent's initialization, has merit. However, the implementation and evaluation of that idea in this paper falls well short of the standard for novelty and quality for an academic contribution. The method relies too much on hand-crafted solutions, and the evaluation tasks are too simple and too few and unvaried. I recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_uBBQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_uBBQ"
        ]
    },
    {
        "id": "EkkH4cyofmf",
        "original": null,
        "number": 3,
        "cdate": 1667120439207,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667120439207,
        "tmdate": 1667120439207,
        "tddate": null,
        "forum": "XYUaprBSDjp",
        "replyto": "XYUaprBSDjp",
        "invitation": "ICLR.cc/2023/Conference/Paper5837/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a framework for incorporating background knowledge into reinforcement learning, Knowledge-Grounded RL. The approach uses an embedding-based attention-mechanism model that learns to attend to external and internal knowledge based on observations. The proposed framework is agnostic to the policy training algorithm, and was demonstrated using PPO and SAC algorithms. \n\nThe proposed framework is evaluated on several tasks from two benchmarks: navigation tasks from the MiniGrid environment and pick-and-place manipulation tasks from OpenAI-Robotics benchmark. In both sets of tasks, the proposed framework superior or competitive performance in terms of sample efficiency. ",
            "strength_and_weaknesses": "Strengths\n\nThe proposed framework demonstrates superior performance in terms sample efficiency on tasks used and does so with different policy training algorithms. \n\nWeaknesses\n\nThe paper misses related work such as Kimura, Daiki, et al. \"Reinforcement learning with external knowledge by using logical neural networks.\"\u00a0arXiv preprint arXiv:2103.02363\u00a0(2021) and Murugesan, Keerthiram, et al. \"Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines.\"\u00a0AAAI. 2021.\n\nText-based RL benchmarks such as those used in the above works (e.g., Textworld, Zork, etc) are good candidates for this work. The proposed work would be much strengthened it also used these benchmarks and compared to SOTA methods there.\n\nAlthough the proposed framework is described generally, the external knowledge used in the experiments is task specific and could be learned and re-used by existing transfer techniques. Why shouldn't curriculum learning be equally successful in this setting? The experiments seem to be demonstrating a way of recombining/re-using previously learned knowledge, which could be learned internally only using e.g., some curriculum. \n\nThe implementation details are very general. I don't think it would be easy to reproduce the results. For example, details are provided (Appendix) about how to build embeddings for external knowledge but it's not clear how that external knowledge is encoded. This is supposed to be done by the external knowledge mappings but not details are provided for any of the experiments. \n\nLastly, it's not clear what the internal policy is learning. It seems to be that the internal policy is learning to explore the environment and relies on external knowledge for task execution. So, in my opinion, the internal policy is not learning the policy to complete the tasks but the whole model is learning how to use external knowledge without building any internal knowledge for reuse later.\n\nAlso, I'm not sure the learned policy is interpretable. As shown in the experiments, we can see when the agent chooses to use which piece of knowledge but the individual pieces of knowledge are not interpretable in and of themselves, which is generally the issue of interpretability.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The work is described clearly and is of good quality. The novelty is lacking as several literature work on reinforcement learning with external knowledge is missing (see comments above in Strengths and Weaknesses). Not enough details is provided to reproduce the experiments. For example, it's not clear how the external knowledge is mapped into executable actions. It's also not clear how to integrate other forms of knowledge in general. E.g., domain rules that could be represented in formal logic or even text. ",
            "summary_of_the_review": "The paper address an important aspect of AI: how to incorporate external knowledge into a learning agent. It proposes an embedding-based actor model that learns to attend to different sources of knowledge and evaluates this on several benchmarks. However, it misses important pieces of the literature on the same topic and doesn't evaluate against SOTA in the space of reinforcement learning with external knowledge. It also misses important benchmarks in the area. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_4vDB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_4vDB"
        ]
    },
    {
        "id": "aUroUA9HQR",
        "original": null,
        "number": 4,
        "cdate": 1667206791977,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667206791977,
        "tmdate": 1667206791977,
        "tddate": null,
        "forum": "XYUaprBSDjp",
        "replyto": "XYUaprBSDjp",
        "invitation": "ICLR.cc/2023/Conference/Paper5837/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper defines a knowledge-ground reinforcement learning (KGRL) problem that an agent learns to follow external guidelines and develop its own policy. It also provides a realization of this problem by using an attention-based actor model which can switch between either a learnable internal policy or external knowledge. Experiments on MiniGrid and OpenAI-Robotics show the KGRL agent's effectiveness.",
            "strength_and_weaknesses": "Strength:\n1. The idea that agents learn to follow external guidelines and develop their own policies makes sense.\n2. The related work description is very detailed.\n\nWeaknesses:\n1. As mentioned in Section 4.1, \"We learn a key for each external knowledge mapping...\", how to learn these keys? I did not find any descriptions.\n2. In Figure 1, $k_in$ is generated by the Inner Actor, which can be learned during the interaction. However, there is no input about $k_gi$. Is it missing? Or please give some more description about my misunderstanding.\n3. In the paper, PPO and SAC are used as the baseline algorithm respectively, and I hope that more baseline algorithms can be combined to illustrate the generality.\n4. At the beginning of Section 5, this paper claims to answer the question \"(1) Can our KGRL agent learn effective new knowledge by referring to an arbitrary set of external guidance?\". Does the term 'new knowledge' simply refer to the stringing together of old knowledge? If so, the use of the word \"new\" is not appropriate, and if not, could you please give some experimental examples?\n5. Since Atari games often appear in reinforcement learning algorithm articles as a basic verification environment, it is hoped that this paper can also do a brief algorithm verification in this type of environment.",
            "clarity,_quality,_novelty_and_reproducibility": "While the main idea of this work is straightforward and the English writing is OK, many important details of the proposed method are missing, making many parts of this work difficult to follow. ",
            "summary_of_the_review": "I like the main idea behind this work, and the proposed actor model architecture looks reasonable. However, since many important details of the proposed method are missing, and there is no source code provided to check how the proposed model is implemented, I thus can not recommend acceptance of this work in its current form. I strongly encourage the authors to re-organize the manuscript's content and re-write the whole paper for better understanding by the audiences.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_quir"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_quir"
        ]
    },
    {
        "id": "UFbsmSNvcW",
        "original": null,
        "number": 5,
        "cdate": 1667407384131,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667407384131,
        "tmdate": 1667407384131,
        "tddate": null,
        "forum": "XYUaprBSDjp",
        "replyto": "XYUaprBSDjp",
        "invitation": "ICLR.cc/2023/Conference/Paper5837/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Knowledge-grounded RL (KGRL) for the purpose of incorporating, reusing, recomposing and generalizing external knowledge in RL tasks. Taking a unified form of external knowledge as an external knowledge policy, this work proposes an actor model that adaptively weighs or activates different external knowledge policies and a learning inner policy. This is realized by a typical self-attention mechanism, with a state-dependent query, inner/external policy-dependent keys. Query, key embeddings and the inner policy are all learned in an end-to-end fashion from standard RL policy signals/gradients. The proposed approach is empirical evaluated in MiniGrid and OpenAI-Robotics benchmark, demonstrating the supriority of the proposed in learning efficiency and generalization ability, given a few pre-defined external knowledge policies.",
            "strength_and_weaknesses": "$\\textbf{Strengths:}$\n+ The writing and presentation of the proposed method is almost clear.\n+ The related work includes multiple related domains.\n+ The experiments are conducted from multiple aspects.\n\n&nbsp;\n\n$\\textbf{Weaknesses:}$\n- The major issue is on the central notion of this paper, i.e., Knowledge or KGRL. I think it is improper and kind of misleading to use the notion of knowledge through the paper. After reading the paper, I recommend the authors to use the notion e.g., Policy Reuse/Transfer, which is more accurate. \n  - The authors emphasize many times in this paper like the claim \u2018all knowledge can be arbitrarily recomposed, rearranged, and reused anytime in the learning and inference stages\u2019. However, the concrete form of external knowledge considered is the external knowledge policy.\n  - In other words, acctually the main content of this work is on how to incorporate, reuse and transfer existing policies in RL. For KGRL at least in my opinion, the key part is how different (or several common) forms of external knowledge are represented, after which how to use the knowledge is to be considered. However, this part is not included in this work and in the experiments, the external knowledge is implemented by the pre-defined policies.\n  - Given the external knowledge policies, the proposed method in Section 4 is not novel to me. The self-attention mechanism and the end-to-end update are straightforward.\n- The experiments provide few impressive results, especially when the pre-defined external knowledge policies are strong and the environments are not challenging.\n- Important related work is missing. For example, Knowledge Guided Policy Network (KoGuN) [1] propose a similar idea, where suboptimal external knowledge is represented by trainable module based on fuzzy logic and then is incorporated into PPO policy network serving as a refine module.\n\n&nbsp;\n\nReference:\n\n[1] Peng Zhang, Jianye Hao, Weixun Wang, Hongyao Tang, Yi Ma, Yihai Duan, Yan Zheng:\nKoGuN. Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge. IJCAI 2020\n\n\n\n&nbsp;\n\n$\\textbf{Questions:}$\n\n1) The authors claim \u201ccurrent learning-from-demonstration and knowledge-reuse approaches lack the flexibility to rearrange and recompose different demonstrations or knowledge, so they cannot dynamically adapt to environmental changes\u201d. Do the author mean the ability of zero-shot adaptation or generalization? To my knowledge, policy reuse and policy transfer are well-studied domain in this direction. Can the authors justify this claim?\n2) The authors claim \u201cThese two research directions do not enable incorporating external guidance from different sources into learning, so their transferred knowledge is limited to the one learned among similar environments\u201d and \u201cKGRL allows an agent to follow and reuse an arbitrary set of knowledge obtained from different sources in a new task, and these sources can be very different from the new task\u201d. What do the \u2018different sources\u2019 mean? External knowledge in different forms? It is confusing to me since this paper does not address how arbitrary forms of external knowledge are transformed exactly into the external knowledge policies. Instead, the external knowledge policies are given in the setting considered in this work.\n3) Using a state-dependent key for the inner policy makes sense to me. However, I recommend the authors to add a KGRL variant without state-dependent key as an additional baseline in the experiments for more complete evaluation and abalation.\n4) I also have concern on the claim \u201cwe disentangle the decision component (the query) and the knowledge representations (the keys)\u201d. Indeed, they are separated but they are updated in an entangled form regarding the policy gradients.\n5) I think there is a straightforward alternative to the gumble-softmax way to acting with the inner policy and multiple external policies. Concretely, to sample an action for each policy (with reparameterization) and then to mix them according to a softmax distribution of $w$. Why is this way not considered? Or what is the drawback of this alternative way?\n6) Are the external knowledge policies used in the experimenst stochastic or deterministic?\n7) It is strange to see the very small error bars in Figure 2a and 2b (in fact, I cannot tell if there is a error bar in Figure 2b).\n8) In Figure 2e, why do PPO-KGRL (KG3) and PPO-KGRL (KG1+KG2) perform worse than PPO while PPO-KGRL (all) outperforms PPO?\n9) In Section 5.2, it seems that KG1 is already optimal in Reach task and KG1 + KG2 is optimal in Pick-and-Place. Can the authors provide more explanation on this point? Similarly, KG1 + KG2 + KG3 is also optimal in 5x5 environments since the view range is also 5x5.\n10) The authors mention \u201cindicating that a KGRL agent learns to not only follow the guidance but also mimic its strategy. This imitation process allows an agent to develop an inner policy that outperforms external strategies\u201d. Can the authors provide more explanation on how such imitation happen under KGRL and why imitation can be better than the external policies?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing and presentation of the proposed method is almost clear. The central notion (i.e., knowledge) is kind of misleading and improper. The proposed method is lack of novelty. A few key claims and statements are not well supported or explained.\n\nFor reproductibility, some details on implementation and experiments are provided in the appendix. Some key details of the pre-defined external knowledge policies are missing (e.g., $\\epsilon$ in Section 5.2). The source codes are not provided.\n",
            "summary_of_the_review": "According to my detailed review above, I think this paper is clearly below the acceptance threshold mainly due to the improper claims on knowledge, the lack of novelty of the proposed method and insufficient experimental evaluation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_c6pa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5837/Reviewer_c6pa"
        ]
    }
]