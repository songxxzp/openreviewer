[
    {
        "id": "cG9mkkPz9r",
        "original": null,
        "number": 1,
        "cdate": 1666444664904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666444664904,
        "tmdate": 1666444664904,
        "tddate": null,
        "forum": "wPVw218szF",
        "replyto": "wPVw218szF",
        "invitation": "ICLR.cc/2023/Conference/Paper3961/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies linear regression from i.i.d. examples under $(\\epsilon, \\delta)$-differential privacy in the settings (i) where there is no adversary and (ii) when a fraction of response variables are adversarially corrupted. While there exists a clear understanding about the optimal (inefficient) private and robust algorithm that solves this problem (namely the HPTR algorithm), it is not known whether this optimal sample complexity can be attained by a computationally efficient algorithm. Hence, this paper aims to improve on the number of samples required by the previous best-known algorithm by Varshney et al. (2022). This improvement is obtained by DP-SGD with some additional ideas.\n",
            "strength_and_weaknesses": "At a technical level, the authors analyze DP-SGD for the linear regression problem (without an adversary) using two additional ideas. First, the work of Varshney et al. (2022) proposes a single pass approach while, in this work, a full-batch gradient descent algorithm is employed. This gives an improvement in the sample complexity. Second, adaptive clipping is used in order to ensure robustness against label-corruption. \n\nIn general, the paper is interesting and the result is nice. The  proposed private algorithm attains an improved sample complexity guarantee while being computationally efficient and robust to label noise. My main concern deals with the issue of technical novelty; the paper relies on standard techniques for achieving privacy (e.g., private histograms) and robustness (e.g., resilience). While the clipping idea is nice, it is essentially ad-hoc for label noise and cannot handle corruption in the covariates. Nevertheless, I believe that the contribution is sufficient for acceptance.\n\nThe paper is mostly well-written (there are various typos that can be fixed) and the results and previous work are clearly presented. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The result is clear and well-presented. There are various typos in the text which should be fixed.",
            "summary_of_the_review": "The authors provide an efficient private and robust algorithm for linear regression, improving on the previous sample complexity bound. I believe that the contribution is beyond the acceptance threshold. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3961/Reviewer_p2zY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3961/Reviewer_p2zY"
        ]
    },
    {
        "id": "V-quNTV2_p",
        "original": null,
        "number": 2,
        "cdate": 1666513658066,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513658066,
        "tmdate": 1666513658066,
        "tddate": null,
        "forum": "wPVw218szF",
        "replyto": "wPVw218szF",
        "invitation": "ICLR.cc/2023/Conference/Paper3961/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the problem of linear regression under DP constraint and adversarial corruption. The authors propose a kind of variant of DP-SGD with a full-batch GD to improve sample complexity and adaptive clipping to guarantee robustness. Their theoretical results can improve the SOTA sample complexity without adversarial corruption. Under label corruption, they give the first provably linear regression algorithm to guarantee both DP and robustness. They also give experimental results on synthetic data to verify their theoretical results.\n\n",
            "strength_and_weaknesses": "Strength:\nThe paper is well-motivated since privacy and robustness become more and more important in ML model. This is the first efficient linear regression algorithm to provably guarantee both DP and robustness. The authors propose a private norm estimator and a robust private distance estimator by using private histogram mechanism for adaptive clipping and also give theoretical results for this algorithm. Based on this algorithm, they propose their DP-SGD method. The authors give both theoretical sample complexity and experimental results for the method. The proofs seem to be correct, and the experimental results also accord with their analysis.\n\nWeaknesses:\n1. The lower bound in Section 3.3 is not clear.\n2.  The \u2018n\u2019 in equation (1) should be B_t.\n3.  The step 1 of algorithm 1 should be three subsets.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I found the problem statement of the paper interesting and important. The authors nicely combine literatures from the DP linear regression and Robust linear regression to produce useful algorithms for the proposed setting. The authors have adequately cited related work as well as honestly commented on differences and similarities with existing literature. I think the paper is well-written. It would be good to bring the experiments in the main body. The submission looks technically sound and makes a solid theoretical contribution.  I didn't completely go through proofs but the intuition of breaking gradient update into contraction, noise from data and DP, bias introduced by clipping and corruption makes sense. I think the algorithms and the provided guarantees are important because the work can be applied in fields where adversary emerges and where there is a also a need of privacy and robustness like networks and finance.\n\n",
            "summary_of_the_review": "The lower bound is not clear. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3961/Reviewer_gS7K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3961/Reviewer_gS7K"
        ]
    },
    {
        "id": "XsSGeeBvacg",
        "original": null,
        "number": 3,
        "cdate": 1666646383304,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646383304,
        "tmdate": 1666646383304,
        "tddate": null,
        "forum": "wPVw218szF",
        "replyto": "wPVw218szF",
        "invitation": "ICLR.cc/2023/Conference/Paper3961/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors study linear regression from a differential privacy perspective. They propose a variant of the differentially private stochastic gradient descent (DP-SGD) algorithm with two innovations: a full-batch gradient descent to improve sample complexity and an adaptive clipping to guarantee robustness. Some theoretical results and empirical evidence are presented.\n",
            "strength_and_weaknesses": "Strengths:\n\n\nAuthors have a good grasp of some of the challenges of differentially private computations in the linear regression context. The paper is well-written. Section~4 on adaptive clipping is an interesting contribution, where I think the results can be improved, but the approach of using a a robust approach and trying to enforce differential privacy on it is interesting. \n\n\nWeakness:\n\nA few of the weaknesses are technical and can possibly be easily corrected. However, some of the weaknesses are probably not adjustable with minor changes. \n\n1) The use of sub-Weibull distributions is more of a distraction. This is primarily because we would want the differentially private estimator to have consistency and other desirable statistical properties, for which sub-Gaussian conditions are often necessary. It is not clear what sub-Weibull adds to either the robustness or differential privacy components of this paper. Also, this assumption does not seem verifiable. \n\n2) The requirement that $\\bar{\\alpha}$ is known in Assumption 2, and the complex bound for it seem unverifiable and not very practical. \n\n3) For high-dimensional regression, it is not clear that the proposed scheme can achieve sparsistency. \n\n4) Along the same lines, it is entirely possible that the norm or non-zero elements of a sparse $w^{*}$ may not have any relation with the leading eigen values/vectors of $\\Sigma$. Hence it is not clear the bounds for the weighted loss function considered here relate to the sparsity of $w^{*}$ or the noise variance.\n\n4) Also, it is not clear what additional complexity is arising from the corruption in the labels arising from an adversarial framework. This would only make sense if an oracle knew the original observations (indexed by $S_{r}$), and the proposed algorithm achieved some kind of a risk bound with respect to this oracle. Since the corruption can be arbitrary, it seems a standard robust statistics approach would have been sufficient here. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, the quality of the empirical work seems fine but there is a lack of adequate theoretical developments. The algorithm may have some elements of novelty, and the results look reproducible.\n",
            "summary_of_the_review": "This is an interesting paper overall, but when we dig a little deeper there are several unanswered questions. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3961/Reviewer_bBtg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3961/Reviewer_bBtg"
        ]
    },
    {
        "id": "rAb3FdOqCP",
        "original": null,
        "number": 4,
        "cdate": 1666988887615,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666988887615,
        "tmdate": 1666988887615,
        "tddate": null,
        "forum": "wPVw218szF",
        "replyto": "wPVw218szF",
        "invitation": "ICLR.cc/2023/Conference/Paper3961/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of differentially private linear regression with potentially corrupted labels. To handle the corrupted labels, the authors proposed a new private adaptive clipping technique. The proposed method can deal with both privacy concerns and corrupted labels. The experiment results validate the effectiveness of the proposed method in the corrupted setting. Furthermore, the authors claim that the proposed method can also improve the sample complexity requirements to achieve a certain level of accuracy in the noncorrupted case compared with existing methods.",
            "strength_and_weaknesses": "The strength of the paper:\n1. The proposed method is able to handle corrupted labels.\n2. Experiments validate the effectiveness of the proposed method.\n\nThe weaknesses of the paper:\n1. In the noncorrupted setting, it is unclear to me why it can improve the sample complexity requirement compared with Varshney et al. 2022. For the minibatch method proposed by Varshney et al. 2022, it seems that the minibatch size is chosen in the order of the full batch size. Thus it is unclear to me why using the full gradient as suggested in this paper can improve the sample complexity? Whether the improvement comes from Algorithm 2? These need to be further clarified.\n2. The requirements on $S_1$, $S_2$ and $S_3$ are not clear.\n3. Why the requirement of $n$ in equation (5) is independent of $\\kappa$? According to the last inequality on Page 16, it seems that the $\\bar \\alpha$ depends on $\\kappa$, and thus $n$ should also depend on $\\kappa$.\n4. Although the proposed method is able to handle label corruption, the corruption level is so small that it should be less than the desired accuracy level. Can you relax this requirement?",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is OK, and the proposed method looks novel, but it needs further clarification on the improvements.",
            "summary_of_the_review": "The author proposed an efficient, robust, and differentially private algorithm for solving the linear regression problem. The proposed method seems to be promising. However, it suffers from the low corruption level and the improvements over the previous method in the noncorrupted setting are unclear.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3961/Reviewer_gANM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3961/Reviewer_gANM"
        ]
    }
]