[
    {
        "id": "tU_9DX21hXC",
        "original": null,
        "number": 1,
        "cdate": 1666514800165,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666514800165,
        "tmdate": 1666514800165,
        "tddate": null,
        "forum": "NTTc8wZktaT",
        "replyto": "NTTc8wZktaT",
        "invitation": "ICLR.cc/2023/Conference/Paper2140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces how to train GNNs using non-overlapping decomposed graphs,  in order to scale GNNs to larger graphs and to obtain better performance.",
            "strength_and_weaknesses": "Strengths:\n\n1. The implementation details are described clearly, providing better reproducibility.\n2. The presentation is clear.\n\n\nI have the following concerns about the paper:\n\n1. The novelty of the proposed method, i.e., use METIS to partition the graph then training GNNs over them, is a bit limited. e.g., [1] ClusterGCN shares similar core ideas.\n2. The technical depth is a bit limited. It would be better if the authors can provide better decomposition/partition method more suitable for GNN training, or provide better insights about the information loss or the usage of the `interfaces`.\n3. Although in the abstract the authors stated the motivation is to deal with large graphs, the experiments are limited to small-size graphs.\n\n\n\n[1] Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks, Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, Cho-Jui Hsieh, SIGKDD 2019",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned in the above section, though this paper presents clearly and has nice reproducibility, its novelty, technical depth, and experiment evaluation are a bit weak.",
            "summary_of_the_review": "In summary, the concerns about novelty, technical depths, and experiment evaluation outweigh the merits.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2140/Reviewer_Tixk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2140/Reviewer_Tixk"
        ]
    },
    {
        "id": "4floIP-5_m",
        "original": null,
        "number": 2,
        "cdate": 1666612906007,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612906007,
        "tmdate": 1666612906007,
        "tddate": null,
        "forum": "NTTc8wZktaT",
        "replyto": "NTTc8wZktaT",
        "invitation": "ICLR.cc/2023/Conference/Paper2140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a way to improve performance of GCN when decomposing graph. To reduce GCN complexity on large graphs, a graph can be decomposed into non-overlapping subgraphs. GCN is then can be trained only on these subgraphs, ignoring the \"interface links\" that connect between subgraphs. To reduce information loss due to ignoring the interface links, the paper proposes to train GCN on both subgraph and interface part (SS method). The output is then a weighted average of the node features obtained by subgraph and interface part.\n",
            "strength_and_weaknesses": "Strength:\n  - The paper is well-written with good notations, figures, and pseudo codes.\n\nWeaknesses:\n  - The paper only compares result of the new method (SS) against other learning scheme on decomposed graph (AD and AL). Both AD and AL ignores the links between subgraphs, so it's obvious for SS to have prediction performance advantage. The paper does not study the increase in computation and memory cost though. Saving computation and memory cost is the main reason why graph decomposition is needed in the first place. I believe SS method reduces that benefit significantly.\n  - It is unclear why the weighting formula is proposed that way. The author says to \"minimal communication and computation cost\" between subgraph and interface features, which I don't know what that actually means.",
            "clarity,_quality,_novelty_and_reproducibility": "The main benefit of training GCN on non-overlapping decomposed graph is to trading off model accuracy for less memory footprint and training time. The trade-off is unclear with SS method and is not well analysed or experimented. The memory footprint and training time benefits are supposed to be reduced significantly by the proposed SS method.",
            "summary_of_the_review": "The proposed SS method for training GCN on non-overlapping decomposed graph has unclear benefit.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2140/Reviewer_zYgm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2140/Reviewer_zYgm"
        ]
    },
    {
        "id": "aBIpUHC2JK",
        "original": null,
        "number": 3,
        "cdate": 1666811686184,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666811686184,
        "tmdate": 1666811686184,
        "tddate": null,
        "forum": "NTTc8wZktaT",
        "replyto": "NTTc8wZktaT",
        "invitation": "ICLR.cc/2023/Conference/Paper2140/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is concerned with the scalability of training GNNs on large graph datasets where there may not be sufficient memory to store the graph structure. The basic idea is to decompose the graph into smaller components. The paper builds upon the earlier works such as (Chiang et al., 2019), which uses METIS to decompose a large graph into non-overlapping components and employs the alternating method to train the graph components and combine the results. In this paper, the authors basically also utilize the \"residual\" graph component (after the decomposition), the so-called \"interface adjacency matrix\", and construct a new (normalized) graph for simultaneous training. Experiment results show that the proposed method performs better than existing methods. ",
            "strength_and_weaknesses": "Strength:\n\n  + Addressing scaling of GNN training on large graphs.\n\n\nWeaknesses:\n   - With the use of the so-called \"interface adjacency matrix\", albeit in general sparser than the original matrix, still requires O(n) memory to store it. If n is larger than the available  memory, the proposed method will not work, whereas the decomposition methods such as (Chiang et al., 2019) requires only O([n/N]^2) memory to store individual graph components.\n   - The proposed method is rather ad hoc. There is no attempt in mathematically justification the proposed method.\n   ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally easy to follow, although the problem could be better motivated, and some of the \"system\" assumptions could be explicitly stated. For example, do you assume that the system memory will be at least in the order of number of nodes, n?\n\nThe idea of utilizing \"residual\" graph component (after the decomposition), the so-called \"interface adjacency matrix\", for training is fairly straightforward, provided that the system memory is sufficiently large to hold the  \"interface adjacency matrix\". Otherwise, it seems to defeat the purpose.  I would have liked to see some analysis on the memory requirements and comparisons between your proposed method and other existing methods. Just because your method outperforms other methods on some datasets does not mean your method will be \"practical\". The trade-offs between performance vs. memory requirements, etc. should be analyzed and evaluated.\n\nOn an \"intellectual\" level, the proposed method is rather ad hoc. There is no attempt in making some mathematical justifications on the proposed \"substructuring\" method and GNN architectures used, for example, the specific forms of $\\hat{S}$, and the way the learning based on this \"graph structure\" and how it is combined with the learning based on the decomposed graph components.  \n\nMore generally, why is METIS is the right choice for graph decompostion, which is oblivious of node features on the graph? Why non-overlapping graph decomposition is the right choice?\n\n",
            "summary_of_the_review": "The paper builds upon the earlier works such as (Chiang et al., 2019), which uses METIS to decompose a large graph into non-overlapping components and employs the alternating method to train the graph components and combine the results. In this paper, the authors basically also utilize the \"residual\" graph component (after the decomposition), the so-called \"interface adjacency matrix\", and construct a new (normalized) graph for simultaneous training. While experiment results show that the proposed method performs better than existing methods, the proposed method is rather ad hoc without any mathematical justification. Perhaps more importantly, it requires a lot of more memory than existing methods based purely on  non-overlapping graph decomposition",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2140/Reviewer_TgT5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2140/Reviewer_TgT5"
        ]
    }
]