[
    {
        "id": "EzS0qwGK1i",
        "original": null,
        "number": 1,
        "cdate": 1666243537064,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666243537064,
        "tmdate": 1669129925879,
        "tddate": null,
        "forum": "2_I3JQ70U2",
        "replyto": "2_I3JQ70U2",
        "invitation": "ICLR.cc/2023/Conference/Paper5062/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose an asynchronous message passing (AMP) framework which is theoretically more powerful than the standard message passing framework. The experiments demonstrate its superior performance on synthetic datasets and moderate performance on graph classification benchmark datasets with small sizes.",
            "strength_and_weaknesses": "### Strengths\n\n- The idea of AMP is interesting, especially its various variants of designs.\n\n- The results on the synthetic datasets are impressive.\n\n- The provided experimental code is well-documented. The environment, package dependency, and the steps to reproduce the results are all stated.\n\n### Weaknesses\n\n- The clarity of the paper can be greatly improved. A pictorial illustration of the proposed framework would help a lot. I cannot understand how AMP works on the entire graph/graphs.\n\n- The clarity of the pseudo-code (Algorithm 1) can be improved. Please specify the input and output of the algorithms. Also, what is $n$ in the for loop? Does Algorithm 1 works for one node or the entire graph?\n\n- The running time of AMP is poor as it can only run on a single CPU. I do appreciate the authors pointing this limitation out.\n\n- The AMP performance on real-world graph classification benchmark datasets still falls behind the state-of-the-art methods by a large margin. Also, the reported results for baseline methods seem to contradict the previous works.\n\n### Detail comments\n\nWhile I really want to learn what AMP is and how it can separate any pair of graphs in the optimal case, the clarity of the paper prevents me to understand AMP thoroughly. For example, I can understand how the asynchronous works for an isolated node from Figure 1. However, I cannot understand how it works for the entire graphs. When the constant delay is used, does the initial message simply the node features for each node? Do these initial messages happens at the same time or do we start the propagation from one node? The description of the AMP framework is too vague for me to understand. I suggest the authors add a pictorial example of the AMP framework on a simple graph. On the other hand, Algorithm 1 should state the inputs, outputs and hyperparameters. I am not sure what $n$ stands for in the for loop, where I suspect it to be the current node. I hope the authors can elaborate more about the AMP framework. \n\nAlthough the authors show that AMP is very powerful and expressive, it does not have time complexity analysis nor demonstrates a reasonable running time. Note that the brute force method can also resolve the graph isomorphism test, it just takes beyond polynomial times. Hence, merely demonstrating the expressiveness of the method is not satisfactory, especially with the claim of being able to solve a notoriously hard problem (i.e. graph isomorphism test). I would suggest the authors report the running time compared with baseline GNNs. I also wish the authors can compare the time complexity of AMP against the standard message passing framework.\n\nRegarding the experiment section, I find it awkward that there is no single model under AMP framework that can simultaneously do well on all three experiments. In section 5.1 the authors compare AMP-C and AMP-R with baseline methods, where the tasks are synthetic node and graph classifications. However, the authors compare different variants AMP-RNN, AMP-ACT, AMP-Iter\u2026etc with baseline methods in section 5.2 when predicting whether the shortest path of the graph is even or odd. Then on real-world benchmark datasets, the authors only show the result of AMP-RNN. Why do we need so many variants? How do we know that the superior performance of the result indeed mainly comes from AMP instead of these sophisticated and specialized variant designs? Finally, according to the ESAN paper by Bevilacqua et al. 2022, the state-of-the-art results on five tested benchmark datasets are close to ESAN. However, the authors show that GCN can significantly outperform ESAN. I wonder why such an inconsistency exists.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper needs an illustrative example to explain how AMP works on the entire graph. The overall clarity can be improved.\n\n- Quality: The method seems to work with improved expressiveness against 1-WL test.\n\n- Novelty: The idea is novel.\n\n- Reproducibility: The authors provide their experimental code which is well-documented. I believe the results can be reproduced.\n",
            "summary_of_the_review": "The authors propose the asynchronous message passing (AMP) framework which seems to resolve many existing issues of standard message passing based methods. The experiment results on synthetic data demonstrate the superior performance of AMP and moderate performance on real-world benchmark datasets. My major concerns of the paper are its clarity and time complexity. Some other concerns pertaining to the experiments are also listed. I hope the authors can elaborate more about the AMP framework and address my concerns listed above. I am happy to raise my score if my concerns are well addressed.\n\n==================post rebuttal========================\n\nI thank the authors' rebuttal. My major concerns are addressed and hence I increase my rating from 5 to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5062/Reviewer_BeiX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5062/Reviewer_BeiX"
        ]
    },
    {
        "id": "uqh5skv2GAP",
        "original": null,
        "number": 2,
        "cdate": 1666626825600,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626825600,
        "tmdate": 1669110971075,
        "tddate": null,
        "forum": "2_I3JQ70U2",
        "replyto": "2_I3JQ70U2",
        "invitation": "ICLR.cc/2023/Conference/Paper5062/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to improve shortcomings of GNNS by updating the messages in an asynchronous manner. The paper shows how this relates to the standard GNN and argue how it can be used for testing graph isomorphism.",
            "strength_and_weaknesses": "The strength of the paper is that it proposes a potential remedy against certain problems of GNNs as over smoothing and underreaching. This is done by a relatively straightforward and intuitive algorithm\n\nYet, there are certain limitations to the work in its current form:\n* The literature is not conclusive and a more focused presentation would improve the paper overall (see clarity,... below)\n* As the authors say themselves, the proposed approach is not quite state-of-the-art. While this is not a problem as such (if the underlying idea is promising) it does not scale well because of the runtime. Overall, I have the feeling that AMP is a neat idea that, however, is not yet ready and would still require significant work.\n* The theoretical contribution is overstated; i.e., it is not clear that AMP is at least as powerful as a GNN. Just because it can represent the same things in principle, it is not clear if and how this representation can be obtained.\n* I really miss an honest and thorough discussion of the experiments and see this as a largely missed opportunity to understand AMP better. Many statements seem to come out of the air and are not corroborated. E.g., \n** It is claimed that AMP/R produces mediocre results because of unstable gradients. Why are the gradients unstable, what could be done against it, what is special for AMP in that respect?\n** It is claimed that further improvements in AMP will reach a competitive performance. How can you be so sure? Which aspects limit the performance right now?. How could these aspects be improved?\n** There are nice plots regarding the effect of underreaching and oversmoothing in the Appendix. Unfortunately, these sections do not actually analyze the experiments, but only state the results.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I find that the paper lacks clarity in that it discusses various arguments for asynchronous message passing at length (human communication, recurrent architectures, GNNs, etc.) without making a point and giving a clear and concise motivation. Therefore it is also hard to gauge the actual contribution. The fact that the proofs are only presented in the appendix (without any summary of the main proof idea) does not help either. Even going through the proof of Theorem 3.1 it does not become obvious which parts are actual contributions of the paper and which ones of Awerbuch (1985).\nIt also seems that the references are not comprehensive. Asynchronous message passing has for example been successfully applied in traditional message passing algorithms (see e.g., [1,2,3]). The work in [4] shows the benefits of asynchronous event-based GNNs and is closely related.\n\nMinor edit: \"We hypothesize lies in better algorithmic alignment\" is not a complete sentence\n\n[1] Elidan, Gal, et al. \"Residual belief propagation: Informed scheduling for asynchronous message passing.\" arXiv preprint arXiv:1206.6837 (2012).\n\n[2] Knoll, Christian, et al. \"Message scheduling methods for belief propagation.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2015.\n\n[3] Aksenov, Vitalii, Dan Alistarh, and Janne H. Korhonen. \"Scalable belief propagation via relaxed scheduling.\" Advances in Neural Information Processing Systems 33 (2020): 22361-22372.\n\n[4] Schaefer, Simon, Daniel Gehrig, and Davide Scaramuzza. \"AEGNN: Asynchronous Event-based Graph Neural Networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.",
            "summary_of_the_review": "The paper makes an interesting contribution from a conceptional point of view. It does not make a good argument for the proposed approach though, since a) the literature is not conclusive, b) it does not scale to larger graphs, c) the amount of theoretical contribution is not clear, and d) a thorough discussion and interpretation of the experiments (and thus the potential benefits and problems) is missing. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5062/Reviewer_oodJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5062/Reviewer_oodJ"
        ]
    },
    {
        "id": "CC_9qeePwx",
        "original": null,
        "number": 3,
        "cdate": 1666660671024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660671024,
        "tmdate": 1666660671024,
        "tddate": null,
        "forum": "2_I3JQ70U2",
        "replyto": "2_I3JQ70U2",
        "invitation": "ICLR.cc/2023/Conference/Paper5062/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new framework, asynchronous message passing (AMP), for graph learning. To be specific, each message passed along the edges is treated individually, which differs from the widely used message passing GNNs. Also, the expressiveness of AMP has been characterized. Experiments on synthetic datasets and real benchmarks are performed to evaluate AMP.",
            "strength_and_weaknesses": "\nStrengths:\n\n1. The motivation and analysis of proposing AMP is inspiring and novel.\n\n2. Treating messages individually is a novel and reasonable idea to go beyond the widely used message aggregation schema. This idea is novel and could be inspiring to the community.\n\n3. The expressive analysis of AMP can answer the question why we should use AMP compared to message passing GNNs.\n\n4. The empirical performance on the expressiveness synthetic benchmarks are compelling and can support the main claim well.\n\nWeakness:\n\n1. The empirical comparison on more and lager real benchmarks should be provided. In the current experimental results, only Table 3 is for real graphs. However, the size and number of the graphs are limited and cannot support the method strongly. I highly recommend performing more experiments on larger graph datasets.\n\n2. The proposed new framework seems to be inefficient. The theoretical and/or empirical analysis of the complexity should be included.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I found it a little bit difficult to follow the paper. It would be more easier to be followed if more intuitive description and direct mathematical equations can be provided.",
            "summary_of_the_review": "Overall, I think this work has merits in terms of the motivation and method. On the other hand, more analysis and experiments should be added.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5062/Reviewer_qHED"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5062/Reviewer_qHED"
        ]
    },
    {
        "id": "BGPM2APnwU",
        "original": null,
        "number": 4,
        "cdate": 1666666649228,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666649228,
        "tmdate": 1666666649228,
        "tddate": null,
        "forum": "2_I3JQ70U2",
        "replyto": "2_I3JQ70U2",
        "invitation": "ICLR.cc/2023/Conference/Paper5062/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose the asynchronous message passing (AMP) framework to enhance the expressiveness of graph neural networks. In the AMP framework, nodes receive messages from their neighbors individually. The authors try to show that AMP is more powerful than the 1-WL test and can even compute graph isomorphism. Experiments demonstrate that the AMP-based model outperforms powerful synchronous GNNs on several GNN expressiveness benchmarks in terms of accuracy.",
            "strength_and_weaknesses": "**Strengths:**\n1. This proposed AMP framework is interesting. \n2. Experiments demonstrate that the AMP-based model outperforms powerful synchronous GNNs on several GNN expressiveness benchmarks in terms of accuracy. \n\n**Weaknesses:**\n1. The authors may want to provide a rigorous proof for Theorem 4.7. Some weaknesses are as follows.\n    1. In the proof, the authors make the assumption of GNN's width and depth, which is not mentioned in Theorem 4.7.\n    2. Corollary 3.1 in [1] assumes that the width of GNNs is unbounded, which is not equivalent to the assumption of a sufficiently wide GNN.\n    3. The authors may want to provide the lower bound of the depth of GNNs.\n    4. The application of Corollary 3.1 is confusing. What is the definition of Turing computable function in Corollary 3.1 in [1]? If a GNN can compute any Turing computable function, how to show that it can separate any pair of graphs?\n2. From Table 3, we can see that the accuracy of AMP is clearly lower than existing powerful GNNs (e.g., ESAN). The authors claim that the low accuracy is due to little investigation into AMP architectures and hyperparameters, but it is not the key reason in my opinion, as more AMP architectures with more hyperparameters are available to ensure a fair comparison for the authors.\n3. The authors may want to compare the time complexity and runtimes of AMP with those of synchronous GNNs.\n4. The authors claim that AMP can propagate messages over large distances in graphs without the corresponding theoretical analysis.\n\n[1] Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International Conference on Learning Representations (ICLR), 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** This paper is hard to follow. \nI have some questions as follows.\n1. How does the reaction block distinguish the neighbors with the same node feature and rooted subtree? Assume that node $v_1$ receives the messages from its neighbors $v_2$ and $v_3$ with the same node feature and rooted subtree. For an asynchronous GNN, node $v_1$ first receives message $m_1$ from $v_2$ and then sends the same message $m_2$ to $v_2, v_3$ according to Steps 8,9 in Algorithm 1. The updated node embeddings of $v_2$ and $v_3$ (denoted by $z_2$ and $z_3$ respectively) are still the same due to the same received message. Next, nodes $v_2, v_3$ generate the messages $m_3$ and $m_4$ with $m_3=\\mu(z_2, m_2)= \\mu(z_3, m_2)=m_4$, where $\\mu$ is the message function (Step 9 in Algorithm 1). If node $v_1$ sequentially receives messages $m_3, m_4$ from $v_2, v_3$, how does the reaction block discard $m_3$ and keep $m_4$ in the proof for Theorem 3.1, such that AMP simulates synchronous GNNs, for which node $v_1$ receives and keeps a single message from $v_2, v_3$ respectively?\n2. What is the meaning of \"taking\" in Table 6? Is it a state or message?\n3. What is the expression of the delay distribution $\\mathcal{D}$ in the experiments? \n4. In the third paragraph of the proof for Theorem 3.1 (A.1), the number of messages that the node is waiting on may be \"w\".\n5. In the proof for Lemma 4.2 (A.2), \"FOUND-(j+k)\" may be \"FOUND-(j+i)\". \n\n**Quality:** The authors may want to provide a rigorous theoretical analysis.\n\n**Novelty:** This proposed AMP framework is interesting.\n\n**Reproducibility:** The authors provide the codes for reproducibility.\n",
            "summary_of_the_review": "I recommend weak reject due to the concern about the theoretical analysis, the experiments, and the writing (See Weaknesses and Clarity). If the authors can properly address my concerns, I am willing to raise my score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5062/Reviewer_6FHJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5062/Reviewer_6FHJ"
        ]
    }
]