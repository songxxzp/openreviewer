[
    {
        "id": "Yi90VQ0fVUv",
        "original": null,
        "number": 1,
        "cdate": 1666533339396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666533339396,
        "tmdate": 1666533339396,
        "tddate": null,
        "forum": "Ivkh2_UdL9O",
        "replyto": "Ivkh2_UdL9O",
        "invitation": "ICLR.cc/2023/Conference/Paper3949/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new way to perform hyper-parameter search based on tensor completion. The combinatorial space of hyper-parameters is formulated as a multi-way tensor, where the loss value of a performed trial is registered as tensor entries. By decomposing and reconstructing the highly sparse tensor, one could predict the loss value of unseen hyper-parameter settings. On a set of experiments, the proposed method has shown comparable results with SOTA algorithms. ",
            "strength_and_weaknesses": "This idea is quite intriguing and I\u2019ve been expecting some work in this direction for some time. \nMy major concerns are the following: \n\nWeaknesses: \n1. The step of narrowing the search space may result in finding a local optimum. In other words, the very first search is just crucial for the final result.\n2. As the first work of using tensor completion for hyper-parameter search, I think it is important to perform a more thorough study on the quality of the completion. For instance, one could simulate a full tensor as ground truth and show how well the proposed algorithm can find the global minimum. \n3. The presentation of the results: the results could have been more convincing if the authors could provide results with more than 2 choices of execution time. For instance, consider visualizing the loss values in y-axis against execution time in x-axis. \n4. I would also be interested in the memory consumption of the proposed approach in comparison with SOTA. \n5. Comparing different tensor decomposition methods could also be an interesting contribution. \n6. How does this work compare to Deng and Xiao: \"A New Automatic Hyperparameter Recommendation Approach Under Low-Rank Tensor Completion Framework\"? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well presented and easy to follow. \nThe proposed idea is novel enough to me. \nThe authors promise that the codes will be made available upon publication. \n",
            "summary_of_the_review": "In general this is a very interesting idea but the presentation could have been better, especially with respect to points 2 and 3 under weakness. I would like to see the work published but after a revision. It would be much more valuable if the community could have a sound and thorough analysis of this idea, even if it turns out to underperform some SOTAs, than to have some discrete numbers that are supposed to demonstrate the superiority of the proposed approach. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3949/Reviewer_BwZ7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3949/Reviewer_BwZ7"
        ]
    },
    {
        "id": "Cxyb7A5zjn-",
        "original": null,
        "number": 2,
        "cdate": 1666558142131,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558142131,
        "tmdate": 1666558245006,
        "tddate": null,
        "forum": "Ivkh2_UdL9O",
        "replyto": "Ivkh2_UdL9O",
        "invitation": "ICLR.cc/2023/Conference/Paper3949/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a HPO method using tensor decomposition. It uses tensor decomposition to predict hyperparameter performance, which is a very novel idea. The best hyperparameter after tensor completion is recommended as the next hyperparameters to try and the algorithm continues until a predefined budget exhausts. Experiments compared to common HPO methods on tuning 4 algorithms showed some potential of the methods.",
            "strength_and_weaknesses": "On the strength, the work proposed a very novel route for HPO through tensor completion and I found it very interesting.\n\n\nOn the weakness, I have major concerns on the technical quality and clarity, as well as in the experiments set up. I think many questions should be answered to show tensor completion is a promising route for HPO. The questions follows:\n\nOn the technical quality.\n* Why is the low rank assumption true? Any motivating examples? For example, on some small scale problem, evaluate all the HP configurations and show that the low rank approximation has good accuracy.\n* How good is the predicted accuracy based on tensor decomposition, giving more and more hyperparameter evaluations? Tensor completion with only a few entries could not end up with a good prediction. How does it compare to GP that is used in BO? This is not studied but critical for a convincing story in HPO.\n* The current algorithm only does exploitation but no exploration. This is surprising and why did the author design it in this way?\n\nOn the experiment set up.\n* In Section 5.2, the author wrote \u201cFor HOTC, T was achieved by varying the number of\ntensor completion cycles, resolution of the initial search space, maximum number of elements for grid-search and minimum resolution for real-valued hyperparameters.\u201d This is problematic because the search spaces (min, max, resolution) should be exactly the same for all the methods.\n* The narrowing down search space part to me seems a bit arbitrary. Do you have experiment results without it? I would highly recommend taking this component out and comparing the proposed algorithm  to other methods.\n* It can be hard to know the low rank number \u201cT\u201d in advance. Did you try some other values in the experiments and some tip on how to set T?\n\nOn the clarity:\n* If the tensor decomposition used in the paper is \u201cnoisy tensor completion with Cross measure-\nments (Algorithm 2, Zhang (2019))\u201d, then I would expect the authors to explain at least the core ideas.\n* Is it a hard requirement for the uniformness of the integer and real hyperparameters (Section 4.1)?\n* After reading Section 4.3, I understand why the grid search is used in Algorithm 1. It is because when the search space is small enough, one could afford a grid search and not miss any important HPs. But it will be very helpful to explain that while describing the algorithm, or at least mention that it will be explained later on.\n* The language needs to be improved for correctness and smoothness.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality is not matching the ICLR bar in my opinion, as already mentioned in the weakness section. The paper contains limited detail of the experiments and no code is available (\u201cremoved for double-blind review purposes\u201d). The novelty is good and I think the authors should make a major edit and give it another try in the future.",
            "summary_of_the_review": "The paper is not matching the ICLR standard due to the weakness that I have mentioned above.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3949/Reviewer_C2WJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3949/Reviewer_C2WJ"
        ]
    },
    {
        "id": "ouF9RHjaSR",
        "original": null,
        "number": 3,
        "cdate": 1666661686322,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661686322,
        "tmdate": 1666661686322,
        "tddate": null,
        "forum": "Ivkh2_UdL9O",
        "replyto": "Ivkh2_UdL9O",
        "invitation": "ICLR.cc/2023/Conference/Paper3949/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper applied tensor completion to search for the optimal hyperparameters for machine learning methods. Precisely, the combinations of parameter candidates are mapped on the tensor values with multiple modes. By the efficient sampling trick (Zhang, 2019) followed by tensor completion, the optimal parameters are selected from the minimum (or maximum) values of the completed tensor.",
            "strength_and_weaknesses": "**Strength:**\n\n1. As a new application of tensor completion, this work may be the first to apply tensor completion to NAS (as claimed in the paper), which might inspire more studies on this problem in the tensor community.\n2. Some tricks, such as the mappings from hyperparameters to tensor values and the ones for performance improvement, is interesting and might be applied to broader ranges of tensor applications.\n\n**Weakness**\n\n1. Several crucial problems are not convincingly discussed in the paper.\n2. The experimental results cannot sufficiently support that the proposed completion method outperforms SOTAs in practice since only several simple architectures are used for evaluation in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and quality**\n\nThis paper is relatively well written. The main idea is clearly explained and easy to be followed. However, there are several questions which are crucial to be answered but lacking in the paper.\n\n1. One is whether the low-rank assumption is reasonable for the hyperparameter selection task. Although the authors give an example at the beginning to show the similarity of the loss with varing different hyperparameters, this fact does not directly imply the low-rank structure. I think it is closer to the targeted tensor's smoothness rather than low rankness. It thus makes me doubt if the low-rank structure exists in real. Since the success of the proposed method is based on the low-rank assumption, this point is deserved to be carefully discussed.\n2. It is unclear how to dimension explosion issues when the combinations of hyperparameters are on a large scale. I have such concern since the Tucker model is used in the paper, but we all know that the size of the core tensor of the Tucker model will grow exponentially with the tensor order. It means that one must deal with a very high-dimensional core when the tensor completion is applied to high-order tensors. In the experiment, the authors only evaluate the proposed method on several classic machine learning models and a very shallow neural network. The experimental settings can not say the usefulness of the proposed method in the real world, where a large amount of hyperparameters needs to be tuned.\n\n**Novelty**\n\nThe application of tensor completion seems new in the NAS task.",
            "summary_of_the_review": "I agree that tensor completion methods might be useful for quickly predicting the optimal combinations of hyperparameters. But as mentioned, the usefulness is held if the low-rank assumption exists in practice in the incompleted tensor. However, this point is not clearly discussed and proved from either theoretical or empirical side.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3949/Reviewer_1YoG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3949/Reviewer_1YoG"
        ]
    },
    {
        "id": "oS1Crqlxpc",
        "original": null,
        "number": 4,
        "cdate": 1666763250740,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666763250740,
        "tmdate": 1666763250740,
        "tddate": null,
        "forum": "Ivkh2_UdL9O",
        "replyto": "Ivkh2_UdL9O",
        "invitation": "ICLR.cc/2023/Conference/Paper3949/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a new hyper parameter optimization method using tensor completion algorithm. The new algorithm consider a N dimensional tensor with its dimension representing tunable parameters and values representing feedbacks (e.g. validation loss). A Tucker factorization is used to for tensor completion in this paper.\n\nEvaluation with machine learning algorithm, including SVM, KNN, Random Forest, Neural Network shows a better selected parameters for optimize the validation losses in the given execution time, as compared to other Bayesian Optimization approach. ",
            "strength_and_weaknesses": "Strength\n[+] Using tensor completion to improve hyper parameter optimization is a new and innovative idea.\n\n\nWeakness\n[-] The motivation of assuming the optimized metrics satisfies the low rank properties seems weak.\nAlthough heuristically sound reasonable. Could the author help to provide more evidence on reasoning of the low rank assumption? Although in the introduction, the paper mentions close parameter provides similar performance, it does not seem to motivate the low rank assumption. Is it possible to have search space with all evaluated loss, and then using tucker factorization to quantify if the low rank assumption holds?\n\n[-]  Not all tensor completion algorithm can converge, which in general depends on the sampling pattern and sampling ratio from the tensor. In this paper, the sampling pattern is directly related to S (set of search spaces) in algorithm 1. Could the author help to clarify how to select  the search spaces to ensure the success of tensor completion algorithm here?\n\n[-] The numerical results seem weak. With only the final searching results, it is difficult to quantify the performance of the algorithm. Could the author provide the convergence of plot of the parameter optimization process to demonstrate either fast or better convergence?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the writing of the paper can be improved. The algorithm description in section 4.1 and 4.2 can be rephrased to increase the readability. ",
            "summary_of_the_review": "Overall, I feel the idea of using tensor completion to improve hyper-parameter tuning is an interesting and innovative idea. \n\nHowever, due the concerns from \n[1] The motivation of the paper seems weak and little evidence has been provided to prove the low rank property holds, let along why Tucker rank is selected in the paper.\n[2] There are some concerns on the main algorithm of the paper, particular from sampling patter and sampling ratio to ensure tensor completion converges. Some minor concerns are from the rank selection as well.\n[3] In sufficient numerical results to demonstrate the advantage of the new algorithms. \n\nThus, I feel the paper can be improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3949/Reviewer_PJTn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3949/Reviewer_PJTn"
        ]
    }
]