[
    {
        "id": "tnv5T9rIsa",
        "original": null,
        "number": 1,
        "cdate": 1666245594896,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666245594896,
        "tmdate": 1666245594896,
        "tddate": null,
        "forum": "4mFTFqOovux",
        "replyto": "4mFTFqOovux",
        "invitation": "ICLR.cc/2023/Conference/Paper1312/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to solve Node Number Awareness Issue (N2AI) and accelerate the inference speed in graph similarity learning.\nThey argue that existing graph learning methods tend to map graphs with similar number of nodes to similar embedding distributions, reducing the separability of their embeddings. They also use (GSL2) to accelerate similarity computation. They conduct extensive experiments to demonstrate their effectiveness.\n",
            "strength_and_weaknesses": "Strength:\n1. They found the existing graph similarity models have a relatively large error in predicting the actual similarity of two graphs with similar number of nodes, because of the global pooling function, which is interesting.\n2. They propose A novel GNN-based graph similarity model, named N2AGim.\n3. They propose the GSL2 to speed up the inference of graph similarity models.\n\nWeaknesses:\n1. The writing level of the paper needs to be improved, the expression is not organized, and there are problems with the writing structure. \n2. Lack of the survey for related work.\n3. Why we need to address to accelerate similarity computation by GSL2, and what is the connection of this strategy to N2AI issue?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Poor. This paper aims to address two kinds of issues, but do not give any connections between these two issues.\n\nQuality: Poor. The paper has technical flaws. For example, the proof of the main theorem is incorrect or the experimental evaluation is flawed and fails to adequately support the main claims.\n\nNovelty: Fair. The paper contributes some new ideas or represents incremental advances.\n\nReproducibility: Excellent. Key resource are available and key details are comprehensively described such that competent researchers will be able to easily reproduce the main results.\n",
            "summary_of_the_review": "The logic of the paper is not clear, the focus is not highlighted, and it is difficult to understand their innovation points and contributions.\nPlease refer to \"weakness\" above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1312/Reviewer_ngMc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1312/Reviewer_ngMc"
        ]
    },
    {
        "id": "1qSUg3KB-C",
        "original": null,
        "number": 2,
        "cdate": 1666430914600,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666430914600,
        "tmdate": 1668657468888,
        "tddate": null,
        "forum": "4mFTFqOovux",
        "replyto": "4mFTFqOovux",
        "invitation": "ICLR.cc/2023/Conference/Paper1312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work found that the existing graph similarity models have a relatively large error in predicting the actual similarity of two graphs with a similar number of nodes. This phenomenon is the so-called Node Number Awareness Issue (N^2AI). This problem is addressed by a novel GNN-based method, called N2AGIM. The authors also propose using GSL^2 to speed up the inference of graph similarity models. GSL^2 can take different graph similarity learning models as backbones, while it reaches the best performance with N^2AGIM. The experiments result demonstrates the superiority of the proposed methods.",
            "strength_and_weaknesses": "Strength\n- This paper proposes an interesting problem (N^2AI), which means graphs with a similar number of nodes would be mapped to similar embedding distribution.\n- The author proposes N^2AGIM to solve the found problem.\n- The author proposes GSL^2 to speed up graph similarity learning inference and it\u2019s plug-and-play.\n- The experiment results validate the effectiveness of proposed methods.\n\nWeakness\n- The motivation of some parts of the model design is unclear, e.g., why using GIN can effectively address N^2AI as mentioned in section 4.1(Multi-scale GIN layers). Have you tried using other GNN backbones? / Why do you use three GIN layers in N^2AGIM? (It seems this should be a hyperparameter.) If that is the case, then a parameter analysis regarding this parameter can be conducted.\n- More detailed inference time comparison is encouraged to be added. For example, as a plug-and-play module. How fast can it improve on the top of graph similarity learning models, e.g., SimGNN and GraphSim?\n- The experiment result is my primary concern. It seems it is not consistent with the results reported in other papers. For example, in H2MN, they show their MSE is 0.913 for AIDS, and 0.105 for LINUX respectively. Nevertheless, in your work, you show that their results for these two datasets are respectively 1.826 and 0.21. Is there any difference between your experiment setting and their experiment setting?\n- It is suggested that annotations errors and typos should be eliminated in the work. In section 3, $F_i$ and $\\mathcal F_i$ are both used for the pooling method. In addition, X and $X$ are both used for representing feature matrix. In Theorem 1, $i$ is used to denote which graph for $G_i \\in S$, while it is also used to denote $u_{G_{1i} $ (which is unclear what i means here).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is good except for some typos and annotation errors.\n\nThe quality and novelty of the paper are also good but there is a concern about the experiment result. Is it fair to compare with those baselines with the reported results?\n\nThe author has provided the open source code, which ensures the reproducibility.\n",
            "summary_of_the_review": "This paper proposes an interesting problem (N^2AI), which means graphs with a similar number of nodes would be mapped to similar embedding distribution. The author proposes N^2AGIM to solve the found problem. In addition, the author proposes GSL^2 to speed up graph similarity learning inference and it\u2019s plug-and-play. The experiment results validate the effectiveness of the proposed methods.\n\nHowever, the motivation of some parts of the model design is unclear, and the experiment result is inconsistent with results reported in other papers.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1312/Reviewer_qS53"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1312/Reviewer_qS53"
        ]
    },
    {
        "id": "cSORokfeJ7O",
        "original": null,
        "number": 3,
        "cdate": 1666691778807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691778807,
        "tmdate": 1666691778807,
        "tddate": null,
        "forum": "4mFTFqOovux",
        "replyto": "4mFTFqOovux",
        "invitation": "ICLR.cc/2023/Conference/Paper1312/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This papertargets two issues in graph similarity computation including the node number awareness issue and the inference speed issue. The authors first analyze and show that the underlying reason of the first issue is the graph pooling. Then, the Different Attention is proposed to get the model aware of the node number. Finally, Graph Similarity Learning with Landmarks is proposed to address the second issue on the computation speed. Experiments are conducted on multiple datasets and compared with multiple baselines.",
            "strength_and_weaknesses": "Strengths:\n\nThis paper has a clear motivation to address the two specific problems in graph similarity computation, therefore the main logic is easy to follow.\n\nTheoretical analysis is provided.\n\nThe experimental results are promising.\n\nThe paper is well formatted.\n\n\nWeakness:\n\n1. The inference time, which is one of the two main contributions of this paper, seems not improved significantly compared to the second best method EGSCS-F. Does this means that EGSCS-F is also a highly efficient method and does not suffer from the inference time issue?\n\n2. It is not clear whether the performance is brought about by better performance on the graphs with similar numbers of nodes. It would be better to show the performance of different methods on the graph pairs with similar number of nodes.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good, the paper has a clear logic.\n\nQuality is good, the paper also has theoretical analysis on their method\n\nNovelty is good.\n\nReproducibility should be good, the code repository provided looks well maintained.",
            "summary_of_the_review": "Overall, this paper has a clear motivation and a reasonable solution. The experiments are overall acceptable. I would recommend acceptance,",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1312/Reviewer_w3qo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1312/Reviewer_w3qo"
        ]
    },
    {
        "id": "qqGD09sRO6",
        "original": null,
        "number": 4,
        "cdate": 1666778713126,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666778713126,
        "tmdate": 1666778713126,
        "tddate": null,
        "forum": "4mFTFqOovux",
        "replyto": "4mFTFqOovux",
        "invitation": "ICLR.cc/2023/Conference/Paper1312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the task of learning graph similarity, i.e.graph edit distance (GED)  with Graph Neural Networks. The authors propose 2 algorithms: a) N2AGim is an adaptation of the architecture proposed in (Qin et al 2021) where the attention layer is now not anymore obtaine via concatenation but through the differences of the embedding resulting in a higher attention when differences are higher; b) GSL2 leverages the first algorithm by computing approximate GED thanks to N2AGim for a set of so-called landmarks and using these values as an embedding of the graph.\nThe paper finishes with some experiments showing that this method gives better performances.",
            "strength_and_weaknesses": "The motivation in the introduction is lost in the rest of the paper. The main claim in the introduction is that GNNs are not good for predicting GED when sizes of the input graphs are close but the empirical results do not show that the current algorithm is actually improving this situation.\n\nSimilarly, theoretical results about pooling in Section 3 are useless. Indeed, pooling is not discuss anymore in Section 4 describing the architectures used. In the experimental section (section 5), pooling is studied in the ablation study and there seem to be no clear conclusion. \n\nThe performances of GSL2 will depend on the landmarks chosen and in particular on the number M of landmarks. A discussion about the impact of M would have benn nice.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written but the figures are too small and not readable.\nThe code is provided but I did not run it.",
            "summary_of_the_review": "The contribution is very weak compared to (Qin et al 2021) and should be better discussed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1312/Reviewer_2mHs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1312/Reviewer_2mHs"
        ]
    }
]