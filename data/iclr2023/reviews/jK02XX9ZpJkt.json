[
    {
        "id": "Gu0hRPIBi5",
        "original": null,
        "number": 1,
        "cdate": 1666486129895,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666486129895,
        "tmdate": 1666486129895,
        "tddate": null,
        "forum": "jK02XX9ZpJkt",
        "replyto": "jK02XX9ZpJkt",
        "invitation": "ICLR.cc/2023/Conference/Paper2957/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors introduced the framework of Constraint Augmented Multi-Agent (CAMA) for solving constrained multi-agent reinforcement learning problems. Earlier Sootla et al. [2022] augmented the safety constrains into the cost function by defining a safety budget and panelizing the agent when crossing that budget. CAMA brings this trick into the multi-agent setting for both the centralized training and decentralized execution (CTDE), and individual learning (IL). The latter is pretty similar to the earlier work (single agent), while the CTDE required more expansion. Authors compared their algorithms against 4 approaches in 5 domains showing promising results. \n",
            "strength_and_weaknesses": "Strengths\n+ Bringing the constraint state augmentation into the multi-agent setting is appealing\n+ The approach can be combined with any method, as the result of CAMA formulation is a MARL problem.\n\nWeaknesses\n- The writing is not great. Several improvements are needed (see details below)\n- Certain definitions seems incorrect. For example why \\tilde R_i is assumed to be non-negative in Definition 1 given that k is often negative? (see details below)\n- The contribution seems incremental given the work of [Sootla et al. 2022].\n- Experimental results don't show a major difference with MAPPO-Lagrange approach. In the literature review section authors mentioned: \"Although these methods satisfy the constraints to a certain extent, a part of the reward performance is still sacrificed\". This is also the case for their approach.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is not well written (see details). The idea is incremental from the novelty standpoint. Authors provided the full list of their parameters in the appendix. Although I could not find the separate discount values they used. does gamma = .99 correspond to \\gamme_c or \\gamma_r or both?\n\nDetails:\n- \"Although these methods satisfy the constraints to a certain extent, a part of the reward performance is still sacrificed\" -> Whenever you want to enforce safety you have to sacrifice part of performance.\n- \"such asElSayed-Aly et al.\" -> Add space before citation\n- \"Recent works, such like\" -> Recent works like\n- Excessive use of \";\"\n- Subscript j and m^i is not defined in the definition of C^i_j\n- Why pi maps to [0,\\infty] instead of [0,1] shouldn't the output be a probability?\n- [Notation] The R symbol used to represent the CMMDP and the one later defined as R_i are different. The relation is not officially defined. I believe R_i is the i_th element of R defined in the tuple.\n- Is C^i = sum_j C^i_j? You have not defined it.\n- a n-agent -> an n-agent\n- \"showed that an algorithm solving this problem actually solves a safe RL with probability one constraints.\" -> please add \"under certain conditions.\"\n- Why \\tilde R_i is assumed to be non-negative?\n- What is the motivation behind defining h in formula 4? Why divide by the constant gamma? Is it for simplicity of calculation?\n- If k is negative, how do you ensure the assumption of \\tilde{R_i} to be non-negative as defined in Definition 1?\n- Why set k to -R(spa)? Wouldn't this avoid any negative penalty if R(s,a) = 0? I read the discussion in the appendix but not sure how the reasoning hold in border set of domains.\n- I don't see CAMA-MACPO in figure 2. Typo? Did you mean CAMA-MAPPO?\n- \"More Harder\" => More difficult\n- \"agent..\": remove one dot\n- \"to ensure maximum exploration efficiency\": What does it mean?\n- Did you mention the your gamma_r and gamma_c for your experiments? Are both of them .99 given table 3 in the appendix?",
            "summary_of_the_review": "The paper has great direction, yet I am not sure if the contributions of the paper is enough for a publication. Moreover the writing of the paper needs non-trivial amount of work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2957/Reviewer_nfFv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2957/Reviewer_nfFv"
        ]
    },
    {
        "id": "wNTo8CfF1y",
        "original": null,
        "number": 2,
        "cdate": 1666556379857,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556379857,
        "tmdate": 1666591401714,
        "tddate": null,
        "forum": "jK02XX9ZpJkt",
        "replyto": "jK02XX9ZpJkt",
        "invitation": "ICLR.cc/2023/Conference/Paper2957/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces CAMA, a safety approach for MARL agents based on constraint augmentation of the search space, and especially the reward function under the notion of a safety budget and hazard values.\nCAMA is integrated into two MARL paradigms: CTDE and IL.\nExperiments are performed to compare CAMA to baselines methods. ",
            "strength_and_weaknesses": "The approach of CAMA is compatible with many MARL algorithms and does not require too big changes to be introduced.\nExperiments show that the safety budget is better honoured than in other approaches, and that in some cases a better performance in terms of reward is achieved.\nAt the same time, the reward improvement is not consistent and in some environments CAMA performs worse (although honouring safety).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written.\nHowever, it was initially difficult to understand what the actual contributions of this paper are versus the used techniques from the existing literature and how the new paper adds to the existing body of work.\nThis makes it difficult (for me) to judge the novelty specifically.\n\nDetails for the implementation of the method are given, although no code release is available. For an expert in MARL it might be able to reproduce the results of the paper.",
            "summary_of_the_review": "The CAMA idea seems good and the results confirm that the safety of the agents is improved.\nI have some concerns regarding the degradation of performance in some settings and the actual novelty of the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2957/Reviewer_CFTd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2957/Reviewer_CFTd"
        ]
    },
    {
        "id": "d2NoTGkUbU",
        "original": null,
        "number": 3,
        "cdate": 1666675939542,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675939542,
        "tmdate": 1666676870829,
        "tddate": null,
        "forum": "jK02XX9ZpJkt",
        "replyto": "jK02XX9ZpJkt",
        "invitation": "ICLR.cc/2023/Conference/Paper2957/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors introduce the Constraint Augmented Multi-Agent framework \u2014 CAMA, which can serve\nas a plug-and-play module to the popular MARL algorithms. They propose an approach that represents the safety constraint as discounted safety costs, known as the safety budget and demonstrate in experiments that CAMA converges  quickly to policies with constraint satisfaction and have better return performance when compared with other state-of-the-art safety RL algorithms.",
            "strength_and_weaknesses": "Strengths:\nIn this work, the authors proposed the CAMA framework for safety-aware multi-agent RL problems.\nCAMA is quite modular and can be used in conjunction with different standard MARL algorithms (to make them safety-aware). \nEmpirically the authors showed CAMA beats most multi-agent safe RL SOTA in terms of safety and return maximzation.\n\nWeakness:\nThe idea of safety budget has been well studied by many previous paper, but mainly for single constraint safe RL problem. It seems besides the CAMA framework and experimentations, the main novelty is on using this safety budget for MARL setting, which is quite incremental.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, with both algorithms and experimental setup quite clearly explained. In terms of contributions,  the main novelty is CAMA, which is a flexible safe RL framework for multi-agent systems. CAMA\ncan work in both cooperative and competitive multi-agent games. Finally the authors evaluate the CAMA algorithm on \nmulti-agent control tasks in SMAMujoco and Gym Compete, which shows the algorithms' ability on constraint satisfaction and reward maximisation. Contribution is more on the empirical sense in which a new flexible framework is proposed to solve multi-agent safe RL. Algorithmically the novelty is relatively incremental (in terms of using a standard reward shaping technique to incorporate multiple constraints into the new reward).",
            "summary_of_the_review": "The work is quite neat in terms of addressing the safe RL problem in the MARL setting. The CAMA framework is also quite flexible and can be adapted to different MARL algorithms. Experiments potentially show the superiority of this algorithm in terms of enforcing safety for multi-agent systems as well as effective return maximzation.\n\nHowever, novelty wise I think the underlying technique is a direct extension of safety budget in standard safe RL, which is quite incremental.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2957/Reviewer_FCBX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2957/Reviewer_FCBX"
        ]
    },
    {
        "id": "Ts-IosXqp4",
        "original": null,
        "number": 4,
        "cdate": 1667362006027,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667362006027,
        "tmdate": 1667362006027,
        "tddate": null,
        "forum": "jK02XX9ZpJkt",
        "replyto": "jK02XX9ZpJkt",
        "invitation": "ICLR.cc/2023/Conference/Paper2957/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents CAMA, a framework for multi-agents that incorporates safety constraints into multi-agent reinforcement learning algorithms. The framework can be added to different multi-agent reinforcement learning algorithms as a plug-and-play method. The safety constraints are represented as the sum of discounted safety costs. Since there is a safety budget, it is easy to compute the hazard value, which is the remaining safety budget. The paper includes empirical results that show the effectiveness of the method.",
            "strength_and_weaknesses": "Strengths: \n- Having a modular component that can be added to multi-agent reinforcement learning algorithms. \n- Having a method that works both with centralized training with decentralized execution and with independent learning.\n- The experimental results are extensive. They have been obtained in multiple scenarios and comparing different algorithms.\nWeaknesses: \n- The contribution is valid but the novelty is limited.\n- All the proofs are in an Appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and with detailed and well-explained results.",
            "summary_of_the_review": "The paper presents a module that can be added to multi-agent reinforcement learning algorithms to enforce safety constraints. The method is supported by extensive experimental results obtained in simulation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2957/Reviewer_VkFF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2957/Reviewer_VkFF"
        ]
    }
]