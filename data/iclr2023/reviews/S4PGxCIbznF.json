[
    {
        "id": "I5-3mxIgH5",
        "original": null,
        "number": 1,
        "cdate": 1666198664265,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666198664265,
        "tmdate": 1668608446931,
        "tddate": null,
        "forum": "S4PGxCIbznF",
        "replyto": "S4PGxCIbznF",
        "invitation": "ICLR.cc/2023/Conference/Paper4374/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Federated domain generalization aims to learn a global model from various distributed source domains and generalize the learned model in completely unseen domains, while keeping the privacy of the source domains. This paper proposes client-agnostic learning with mixed instance-global statistics and zero-shot adaptation with estimated statistics to implement federated domain generalization. This paper augments local features via the global statistics in the global model's batch normalization layer. And local models learn client-invariant representations based on proposed agnostic objectives with the augmented data. A zero-shot adapter is utilized to help the learned global model bridge a domain gap between seen and unseen clients.",
            "strength_and_weaknesses": "\nStrengths\n1. The problem that this paper aims to solve is important for the real-world FL.\n2. The related work section well reviews the federated domain generalization and the problems of directly using conventional domain generalization methods into federated domain generalization. Especially the differences between the multi-source and single-source methods can well motivate the necessity of developing multi-source domain generalization methods with privacy protection.\n3. This paper has considered the privacy problem and the proposed technique does not introduce extra privacy leakage than FedAvg. The motivations of the proposed methods are clear. And the method shows some novelty.\n\nWeaknesses\n1. It seems that the client-invariant features are learned based on mixing local and global statistics. However, the global statistics do not include the un-seen clients. How to ensure the claimed client-invariant features can generalize well on un-seen clients.\n2. The experiment setting is limited. Only four clients are considered. The cross-silo FL may have much more clients.\n3. There is no theoretical analysis of the generalization performance.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThis paper is well-writen and clear.\n\nQuality:\nThe related work is conprehensive, the methods are illustrated well. But the experiment setting is limited as mentioned in weakness.\n\nOriginality:\nAs far as I know, the proposed method is novel. There is no other works that consider mixing the local and global BN statistics and the zero-shot adaptation.",
            "summary_of_the_review": "Overall, if authors can address my concerns of the generalization ability of client-invariant features and the experiment settings, I recommend to accept this paper.  The studied problem is important, and few current works have studied it. And the proposed method shows some novelty.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4374/Reviewer_LXCP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4374/Reviewer_LXCP"
        ]
    },
    {
        "id": "SsZJqFP4wwA",
        "original": null,
        "number": 2,
        "cdate": 1666400819843,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666400819843,
        "tmdate": 1666400819843,
        "tddate": null,
        "forum": "S4PGxCIbznF",
        "replyto": "S4PGxCIbznF",
        "invitation": "ICLR.cc/2023/Conference/Paper4374/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work targets federated domain generalization.  The solution is based on FedBN, which disentangles local model parameters into two groups: domain-specific and domain-invariant. Specifically, the authors designed two strategies: (1) client-agnostic learning with mixed instance-global statistics on BN parameters, as well as designed learning loss terms; (2) a zero-shot adaptor to estimate the interpolation parameter. ",
            "strength_and_weaknesses": "Strength:\n\n-- The designed two strategies are interesting and technically correct (especially the zero-shot adaptor). Most prior works conduct linear interpolation based on a pre-defined and fixed parameter, which is believed sub-optimal. While in this work, the authors designed an adaptive parameter estimation method to estimate the parameter.\n-- The writing is clear and easy to follow. I feel no difficulty understanding the technical details of this work. The experimental results are extensive.\n\nWeakness:\n\n-- The work is heavily dependent on FedBN. The main difference is that author of this work designed an adaptive interpolation parameter estimation method. This jeopardizes the novelty and technical contribution of the whole work.\n-- I am a little conservative about Eq. 4. If Eq. 4 stands, does that mean the u^l in Eq.3 tends to be 1?\n-- The improvement of the designed solutions in Table 5, is not significant on some datasets. For example, on OfficeHome, the CSAC achieves 64.35, and the proposed solution achieves 64.71, which is a marginal improvement.",
            "clarity,_quality,_novelty_and_reproducibility": "Generally speaking, this work is written with good clarity and quality. The novelty is fair and the originality is good.",
            "summary_of_the_review": "The ideas and designed solutions are interesting and technically correct. The experiments are extensive. The writing is clear.\nWhat concerns me the most is the technical contribution of this work as it is heavily dependent on FedBN. Please provide more discussion and comparison to highlight your contribution in the rebuttal. I am willing to change my score if I can receive convincing feedback.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4374/Reviewer_p4rT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4374/Reviewer_p4rT"
        ]
    },
    {
        "id": "WYmuhn8DqA",
        "original": null,
        "number": 3,
        "cdate": 1666455800710,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666455800710,
        "tmdate": 1668776733942,
        "tddate": null,
        "forum": "S4PGxCIbznF",
        "replyto": "S4PGxCIbznF",
        "invitation": "ICLR.cc/2023/Conference/Paper4374/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how to learn a global model in FL from multiple distributed source domains and generalize the model to new clients in unseen domains at inference time. The authors propose client-agnostic learning with mixed instance-global statistics\nfor local training along with  zero-shot adaptation with estimated statistics for inference, which can be viewed as a privacy-preserving multi-source domain generlaization method. To obtain client-invariant representations, the authors augment local features using data distribution of other clients via aggregated BN statistics from the global model.\n\n",
            "strength_and_weaknesses": "Strength: \n\nThe main strength of the paper is the experimental results, which are provided in an organized manner covering a broad range or DG benchmarks and comparing with a sufficiently number of baselines. However, the reviewer has major concerns regarding the problem itself, the proposed method, and some claims in the paper: \n\n\nWeaknesses: \n\nThis paper addresses adaptation to a new completely unseen test domain. The main goal of FL is to help clients who participate in training in terms of obtaining a global model, which addresses their own test error on their own test distributions. Otherwise, why should a client participate in training, which is aimed to optimize a model for a completely different test domain? \n\nThe introduction of zero-shot adapter for example increases computational costs for local clients while it does not directly benefit their own test error. I am not convinced that a client is willing to participate in such collaborative learning. For example, consider cross-silo FL where multiple hospitals plan to learn a model collaboratively. Their main motivation is to obtain a model that helps their own patients at the inference time. \n\n-----\n\nAnother major issue is that what if the model used to train does not have batch normalization layers. It may be a simple CNN. In particular, batch nomlization has been shown to suffer from 1) increasing  computational primitive, which incurs memory overhead 2) introducing hidden hyper-parameters that have to be tuned 3) breaking the independence between training examples in the minibatch (Brock ICLR21 and ICML21). \n\nAndy Brock, Soham De, Samuel L. Smith, and Karen Simonyan. \"High-performance large-scale image recognition without normalization.\" In International Conference on Machine Learning (ICML) 2021.\n\nAndy Brock, Soham De, and Samuel L. Smith. \"Characterizing signal propagation to close the performance gap in unnormalized ResNets.\" In International Conference on Learning Representations (ICLR) 2021.\n\n-----\n\nThe authors assumed that client $k$ has a single domain data. What if client $k$ has different domains across training/test time, which is a natural scenario.  \n\n-----\n\n\"local models are easily over-fitted to their domains\"\n\nThis happens only when a model is trained locally for several iterations. If the model is aggregated frequently, this will not happen in standard FL training. \n\nThe authors proposed to locally train $\\{\\theta_k^a,\\theta_k^s,\\phi_k\\}$ for long epochs. There are several work in distributed learning, which show that local updates are not quite useful both experimentally and theoretically (Karimireddy ICML20). It will be nice if the authors explain why local training for long epochs is useful in this problem? \n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. \"Scaffold: Stochastic controlled averaging for federated learning.\" In International Conference on Machine Learning (ICML) 2020.\n\n-----\n\nFor MixIG: clients still need to share BN statistics to be averaged so the claim that this method has the same privacy guarantees as FedAvg is not accurate. Indeed, in addition to sharing $\\theta_k,\\phi_k$, sharing those statistics leads to additional privacy leakage.  \n\n-----\n\nThe interpolation mean vector is drawn uniformly at random  from $(0,1)$. Why did the authors choose this specific distribution? It seems to me that these weights should be considered as additional hyperparameters. Another major issue is that whether those weights are client-specific or shared among clients? Based on current notation, it seems that all clients use the same weights for BN layer $l$. \n\nThis paper has two additional hyperparameters $\\lambda_1$ and $\\lambda_2$., which makes the solution less practical given the challenges of tuning these parameters. \n\n-----\n\nMinor comments: \n\nTypo in Section I: \"Af inference time\"\n\nThe first phrase in the second sentence of paragraph \"Challenges:\" is incomplete. \n\nTypo in the first sentence of paragraph: \"Mixing Instance and Global Statistics (MixIG):\"",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the paper should be improved. The idea is interesting, which is mainly built on FedBN (Li et al., 2021b). I think the authors sufficiently explained the experimental setup. ",
            "summary_of_the_review": "While the paper has merits mainly w.r.t. experimental results, there are major concerns regarding the problem itself, the proposed method, and some claims in the paper",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4374/Reviewer_RmwN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4374/Reviewer_RmwN"
        ]
    },
    {
        "id": "z9Jjywi6I0",
        "original": null,
        "number": 4,
        "cdate": 1666607535822,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607535822,
        "tmdate": 1666607734307,
        "tddate": null,
        "forum": "S4PGxCIbznF",
        "replyto": "S4PGxCIbznF",
        "invitation": "ICLR.cc/2023/Conference/Paper4374/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This article tackles the problem of federated domain generalization. The method includes two parts: the first part mixes the BN parameters of each local model with the global model, and performs feature augmentation to improve model robustness; the second part introduces a zero-shot adaptor for unseen client generation. The method is verified over three datasets and results look promising. ",
            "strength_and_weaknesses": "Strength:\n\n* The experiments are comprehensive and results are good. \n* The method is simple and lightweight. \n\nWeaknesses:\n\nFirst of all, I would say that the paper is not well written. Though the method is simple, it is not easy to understand all the components as a unified model. Below I give my comments as well as the parts that are not clear to me.\n\n* My major concern involves with the core model design. It is not reasonable for me that the method aims to keep two groups of BN parameters at each BN layer, at the same time to let the features generated by the two parameter groups be same by Eq.4. Would this make the BN parameters same? I am wondering how will a simplified model performs, e.g., by dropping all local BN, $\\mathcal{L}_{CE}$ in Eq.6 and the loss in Eq.4. \n* It is not clear to me how the  output features of the two BN layers are fused? Of if they are not fused, the model will cause the increase of computational cost in local clients and some analysis should be provided.\n* From a unified perspective, I am wondering is $\\alpha$ in Eq.7 equivalent to $\\mu$ in Eq.3, only that $\\alpha$ are dynamically generated rather than sampling $\\mu$ from a gaussian distribution. If it is this case, I would suggest to use the same symbol to ease the understanding.\n* The paper presents that \"a zero-shot adapter that is carefully designed to dynamically generate $\\alpha$ for each input in both seen and unseen domains\". Then I have two questions:\n    * The $L_A$ in Eq.8 requires label $y_{i,k}$ for loss computation. However, it is not clear where the labels are obtained for unseen domains since in Sec. 4.1, it mentions that the unseen domain is only used for evaluation, which means that the data are not labeled. \n    * Regarding the specific implementation in Eq.9, a MLP is used for generating $\\delta$ and $\\epsilon$. I am concerning whether the network design is promising. Is it necessary to constraint the range of the two values so as to avoid generating unexpected values (e.g., 10000).\n* The statement \"we use diverse augmented domain from other clients beyond regularization.\" is not clear. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is not good enough. Though I can understand the main idea of the paper, the details in model design are hard to understand. Based on current presentation, the reproducibility will also be very difficult. ",
            "summary_of_the_review": "The paper needs a significant revision to improve the presentation and clarify the rational of the model design. The current version is far from reaching the acceptance bar of ICLR. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4374/Reviewer_Rg7n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4374/Reviewer_Rg7n"
        ]
    }
]