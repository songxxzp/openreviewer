[
    {
        "id": "OR6RK2RWxq",
        "original": null,
        "number": 1,
        "cdate": 1666648361245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648361245,
        "tmdate": 1668969493703,
        "tddate": null,
        "forum": "q3F0UBAruO",
        "replyto": "q3F0UBAruO",
        "invitation": "ICLR.cc/2023/Conference/Paper1657/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a framework for human-agent collaboration in MOBA games. The framework consists in two main parts: i) a protocol of commands that human and agents use to communicate with each other, and that can be expressed as a tuple of \"what\" the sender asks the receiver to do, for \"how long\" has to be done, and \"where\" the action has to happen; and ii) a hierarchical RL approach the agents follow to learn what commands to send, and to which commands they should respond - the approach is hierarchical because the learning process is done sequentially in three independent stages: first, message generation is leant by cloning the behaviour of expert data; second, the low level actions required to accomplish a command are learnt by fine tuning a pretrained model, such that the agents learn to complete the tasks described by the commands without decreasing the win rate; third, the agents learn to select which message among all the messages received (e.g., one per human player). Experiments show the proposed framework results in increased win rates with respect to agents that do not communicate with other players (i.e., they only execute their own commands), and that human players prefer the proposed agents with respect to agents that do not communicate.\n",
            "strength_and_weaknesses": "### Strengths\n+ The proposed framework is conceptually simple and might be extended to other scenarios (beyond MOBA or even beyond gaming) where humans and agents communicate at the strategic level.\n+ The whole design is full of interesting design choices that will likely inspire future research, like the reward function to fine tuning the low-level actions network, the command representation as a grid that it can be learnt with a CNN, or the command selector architecture that combines the gating mechanism with attention\n+ The paper establishes a baseline for a challenging and relevant problem.\n\n### Weaknesses\n- Many implementation details are missing (see comments on reproducibility below).\n- Clarity could be improved (see comments below) ",
            "clarity,_quality,_novelty_and_reproducibility": "### Quality and technical details\n- The baselines used for the experiments are rather poor: \n1. In order to put the win rates in context, what are the win rates for 5A teams?\n2. MC Rand seems useful as a sanity check only, and MC Base agents do not communicate at all. Hence, the relative results in the subjective experiments are not very illustrative. Preference of collaborating with other human (general and high) players is required, in other words, do human players prefer MCC over other human players?\n- Shouldn't $G_t$ include the expectation over meta-commands?\n- Why is the message $m_t^{i+1}$ dependent only on the observation, $o_t^i$, and not on the selected message, $c_t^i$?\n- It is said \"MC-Base agent has the same capabilities as the WuKong agent\". However, the MC Base agents have been fine-tuned, could fine tuning have decreased the performance w.r.t. the original WuKong agent?\n\n## Missing reproducibility details\n- Please include details on the hand-crafted command converter function $f^{cc}$ and the hand-crafted command extraction function $f^{ce}$. Could you please be more explicit how these functions relate to Tables 4, 5 and 6? \n- Tables 5, 6 and 7 describe the data type and the dimensionality, could you be more specific on the values (for example, scalar value refers to the number of elements, is it a normalised value, etc.)?\n- Please provide details on the value networks $V_t^{int}(o_t, m_t)$ and $V_t^{ext}(o_t)$ used to train the meta-command conditioned action network.\n- Appendix B.5 explains high level details of the networks, but lower-level details are missing, like the number of hidden layers, their size and activation functions, the value of $k$ for the Top-$k$ SoftMax activation. In addition, explanations on the rationale for choosing these parameters would be much appreciated.\n- Please provide details on the model used for cloning the expert data (Sec. B.6.1 describes the loss function but not the model). Also, what is the total number of frames, $N$, per trajectory?\n- Which RL algorithm is used to train the Command Selector network? (PPO is mentioned for the low-level actions (MCCAN) but haven't found a similar discussion for the CS network).\n- Training details are also missing, for example B.6.1 mentions the initial learning rate, but says nothing about the learning rate schedule or the other parameters.\n- According to Figure 3, $V_t^{mc}$ and $\\{Q_t^{i}\\}$ seem to refer to multiple heads. Does $V_t^{mc}$ propagate to the shared CNN? Do the h$\\{Q_t^{i}\\}$ heads propagate to the gating and observation MLPs?\n- In Figure 4(b), Test II, there is a simulated human. How has this agent been trained\n- Is the number of steps at the human-agent collaboration stage, $n$, fixed? or is it dependent on the meta-command? Can a command be interrupted by another meta-command (e.g., when an agent executing \"kill the dragon\" that could take longer than 20 seconds receives other messages every 20 secs)?\n- How are the resources in each cluster used? Are the 63000 CPUs used to, e.g., run about 2.3 episodes each to feed the 560 GPUs?\n\n\n### Clarity\n- Notation is sometimes inconsistent. For example, is $V(h) = V_t^{mc}(o_t, C_t)$? Do we have one state-action value network per human and agent sending messages, right? Does $Q(h, m')$ refer to each $Q_t^{i}(o_t, C_t, m_t^{i})$ for $i=1\\ldots M+N$ or to a single loss after combining the different attention heads?\n- Over which distribution is taken the expected value in the definition of the value of the meta-command, $L^V$? In other words what are $S$ and $C$ in $V_\\omega^k(S,C)$ I understand $C$ is the meta-command candidate set, is $S$ the current state?\n\n\n\n### Minor comments\n- What does \u201cphysical\u201d in a \"physical computer cluster\" mean?\n- Could you give some insight on how the gating mechanism works for completeness? Does the attention mechanism establish which meta-commands are more relevant for the current observation?\n\n\n\n",
            "summary_of_the_review": "**Edit after the authors rebuttal**\n\nThe authors have addressed my concerns as discussed in the rebuttal thread. Although the proposed approach is MOBA specific, I think it might inspire extensions to other game and real scenarios - see my ethical concern below. Moreover, the results in terms of winning rate and subjective preference are impressive. I am raising my score to acceptance. However, as noted in the rebuttal thread, I agree with other reviewers there are some issues to be discussed.\n\n**Original review**\n\nThis is an interesting paper that tackles an important problem in a rather sophisticated manner. It has good ideas and can likely inspire future research. \n\nThe results are illustrative but not strong. Fortunately, win rates of 5A teams and preference over playing with other human players are strong baselines that would shed light on the performance of the proposed solution.\n\nIn my opinion, a major contribution of this work is to actually make this thing work, which includes multiple machine learning engineering decisions and engineering expertise. However, although some high-level ideas are given, many crucial details are missing, which could prevent reproducing the results, and hence reduce the potential impact of the work. \n\nIf the authors tackle these two concerns: stronger baselines and reproducibility, I will increase my recommendation score.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "The authors introduce a simple methodology with excellent results, both in terms of winning rate and subjective preference, and I am concerned this might potentially be used to collaborate with drones and other AI agents in real life combat scenarios, and wonder whether this should be reviewed by the Ethics Committee.\n",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1657/Reviewer_Usav"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1657/Reviewer_Usav"
        ]
    },
    {
        "id": "benKbn4Idp",
        "original": null,
        "number": 2,
        "cdate": 1667035260033,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667035260033,
        "tmdate": 1669570800824,
        "tddate": null,
        "forum": "q3F0UBAruO",
        "replyto": "q3F0UBAruO",
        "invitation": "ICLR.cc/2023/Conference/Paper1657/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "I found this paper fairly hard to understand, so I will write a summary that\u2019s fairly different from the author\u2019s presentation that would have been easier for me to understand, and the authors can tell me if my summary is inaccurate.\n\nThere have been several agents that play multiplayer online battle arena (MOBA) games, such as OpenAI Five (Dota), AlphaStar (StarCraft), and the WuKong model (Honor of Kings). These agents play _competitively_ against humans: 5 copies of the agent control the 5 heroes on the team. However, we would also like to have AI agents _collaborate_ with humans: we can test this in MOBA games by having the team of 5 heroes be controlled by a mixture of humans and agents.\n\nA key challenge in human-AI collaboration (HAC) is how to facilitate communication between the humans and the AIs. The core contribution of this paper is to propose such a method and demonstrate its utility in Honor of Kings.\n\nFirst, we design a structured message format that humans can understand, which is a triple <L, E, T>. L is the location, which is one of 144 grid squares on the map. E is the task to perform, such as \u201ckill the dragon\u201d; I assume it is selected from a predesigned set of possible actions, though the paper does not say. T is an integer that specifies a number of timesteps in which to complete the action. The overall message <L, E, T> is to be interpreted as \u201cin the next T timesteps, go to L and complete action E\u201d. Only these instruction messages are supported; there is no other kind of communication.\n\nThen, for a single timestep, our agents work as follows:\n\n1. From their input observation o (and history, perhaps), generate a candidate message <L, E, T> to send to the other team players (whether AI or human). This is done by a Command Encoding Network (CEN) \u03c0_\u03d5(m|o) (where m is the generated message / meta-commands). \n2. Collect messages { <L, E, T> } from other agents. Select which of the messages to follow from amongst these messages and the message you generated yourself. This is done by a Meta-Command Selector (CS) \u03c0_\u03c9(o, C) (where C is the set of received messages / meta-commands).\n3. Select an action to execute based on the chosen message and the input observation. This is done by the Meta-Command Conditioned Action Network (MCCAN) \u03c0_\u03b8(a|o, m), where m is the message / meta-command chosen in the previous step, and a is the action.\n\nThe human players play as normal, but are only shown one of the generated meta-commands. This is done by the CS but I am not clear how (see Q5 in my list of questions).\n\nTraining for these 3 models works as follows:\n\n1. The CEN \u03c0_\u03d5(m|o) is trained using supervised learning on an expert dataset that is automatically extracted from a dataset of expert gameplay using a hand-crafted function.\n2. I don\u2019t understand how the MCCAN is trained (see questions in the Strengths and Weaknesses section).\n3. The CS is trained after the CEN and MCCAN are already trained. We simply use the full agents in a self-play setting, and train the CS using deep RL. Instead of using the environment reward function over the entire game, we instead use only the rewards during the execution of the current meta-command, and we weight the rewards obtained after reaching the location L twice as highly as the rewards obtained prior. (We also use multi-head value functions that predict different aspects of the reward, e.g. gold obtained vs. enemies killed, as done in prior work.)\n\nThe resulting MCC agent is compared against various ablations that show the value of communicating meta-commands between agents in an all-AI setting. The agent and various baselines are also paired with humans; the MCC agent achieves the highest winrate and is also most preferred by the human expert players.",
            "strength_and_weaknesses": "Strengths:\n1. The paper tackles an important and significant problem domain \u2013 interpretable communication with humans in the service of human-AI collaboration.\n2. The experimental results are strong, showing both improvements on win rate for the game, as well as improvements in human preference over which agents to play alongside.\n\nWeaknesses:\n1. The paper is hard to understand; despite poring over the details I would not know how to reproduce the paper\u2019s results. See the list of questions below.\n2. The authors claim that the MC-Base baseline (where meta-commands are not communicated) can be considered SOTA since it has the same level of performance as the SOTA WuKong agent, citing an appendix which is not present. It is not clear why this claim is true \u2013 did the authors evaluate the win rate between these agents? If so, why not also evaluate the win rate between MCC and WuKong, if you have access to the WuKong model? Even if you have evaluated the win rate of MC-Base vs WuKong, it is still good to compare MCC to WuKong, as chances of winning are not necessarily transitive (i.e. even if MCC beats MC-Base and MC-Base beats WuKong, it could still be that MCC loses to WuKong, though it is not likely).\n3. The proposed method requires some domain-specific hardcoding (e.g. to define the message protocol, to define the set of possible events E, etc).\n\nNotes on clarity and questions for the authors:\n\n1. Which parts of my summary are incorrect?\n\n2. In a message <L, E, T>, does the action E have to be selected from some predefined list of actions? If not, what are the legal values of E?\n\n3. When describing the message <L, E, T>, in the example you suggest that T could be \u201cuntil the dragon is killed\u201d. However, in the experiments it sounds like T is always set to be some integer. Is it possible to have T be something like \u201cuntil the dragon is killed\u201d or does it have to be an integer? If it is possible, how is that implemented?\n\n4. Is it possible for an agent to choose not to send any message?\n\n5. In Figure 4(c), it appears that the AI agents must coordinate to send a single meta-command to the humans, which is done by using the meta-command selector \u03c0_\u03c9(o, C). What do you use for the input o in this case, given that there are multiple AI observations but the CS only takes one observation?\n\n6. I would assume that the CS should choose which meta-command to execute based on the probability of executing the meta-command successfully and the value of success for the rest of the game. This means that when training the CS, we need to give it a reward signal that captures how valuable it is to successfully complete the meta-command, which you would get from the environment rewards *after* the meta-command is complete. However, the CS optimization objective explicitly considers rewards only up to the end of the meta-command, rather than considering rewards after the completion of the meta-command. Why do you choose this instead of considering rewards up to the end of the game (or up to some fixed horizon length after the end of the meta-command)?\n\n7. For the MCCAN training, the authors say\n\n> We adopt an intrinsic reward r^{int}_t(s_t, m_t, s_{t+1}) $= |f^{ce}(s_t) \u2212 m_t| \u2212 |f^{ce}(s_{t+1}) \u2212 m_t|$ to guide the process of executing the meta-command $m_t$, where $f^{ce}$ is a hand-crafted command extraction function.\n\nI am confused about what is happening here. Presumably this intrinsic reward somehow guides the policy to take actions that would achieve the meta-command $m_t$, since nothing else in the MCCAN training gives it that incentive. But how does it do this? My best guess is:\n\n- We create $f^{ce}$, a hand-crafted function that \u201cguesses\u201d which meta-command is being executed based on the current state / observation\n- We want the agent to take actions that cause $f^{ce}$ to make the correct \u201cguess\u201d, since that corresponds to taking actions that complete the meta-command, so we want the agent to minimize the distance between the guess from $f^{ce}$ and the actual meta-command at the end of the meta-command time interval, and which suggests a reward of $- |f^{ce}(s_{T^{mc}}) - m|$\n- We apply potential shaping [1] over the time length of the meta-command to get the intrinsic reward above. (Though if this were the case there should be a discount factor in the reward.)\n\nIs this correct? If so, why don\u2019t you have the discount factor? Also, wouldn\u2019t this incentivize the agent to complete the meta-command on the last timestep $T^{mc}$, rather than completing it as fast as possible?\n\n8. For the MCCAN training, what is the training environment? The CS has not yet been trained so it cannot be used. Are you using a self-play setting where each of the 10 agents always executes its own meta-command generated by the CEN?\n\nOther notes:\n\nSection 4.4 (Collaborative Interpretability Analysis) would be improved by some quantitative metrics: for example, you could ask players to choose key moments during the game, and then report how often the Q-value ordering agrees with the expert player rank ordering.\n\n\n[1] Ng, Andrew Y., Daishi Harada, and Stuart Russell. \"Policy invariance under reward transformations: Theory and application to reward shaping.\" Icml. Vol. 99. 1999.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: As explained above this is the biggest weakness of the paper. I found it quite difficult to understand.\n\nQuality: This is somewhat hard to evaluate due to the lack of clarity though overall I found it to be high quality.\n\nOriginality / novelty: While interpretable human-AI communication has been studied before, this is (to my knowledge) the first case study with deep learning in a setting as complex as a MOBA game.\n\nReproducibility: Given the lack of clarity this paper would be hard to reproduce.",
            "summary_of_the_review": "I want to like this paper: it tackles an important problem, it proposes a high-level design that is relatively simple and has a clear reason why it would help, and it shows strong experimental results in a complex, high-dimensional setting.\n\nHowever, as the paper is currently written, (a) it was quite difficult for me to understand even the high-level design, though I did eventually figure it out, and (b) even after that I cannot understand the details of how the method actually works. This is a key requirement for a scientific paper, and so unfortunately I would recommend against acceptance.\n\n(Due to the lack of clarity I am also reducing my score for correctness, as there were some claims in the paper that I could not evaluate because the necessary information wasn\u2019t present.)\n\nI also have some other qualms (weaknesses 2 and 3 above) but they are minor in comparison and I would recommend acceptance if those were the only weaknesses.\n\nUPDATE: After the author response many of the necessary details have been added, and so I am raising my score. However, I still find the paper relatively hard to understand.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1657/Reviewer_peTd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1657/Reviewer_peTd"
        ]
    },
    {
        "id": "wwTlmMww_s",
        "original": null,
        "number": 3,
        "cdate": 1667515657229,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667515657229,
        "tmdate": 1667516699701,
        "tddate": null,
        "forum": "q3F0UBAruO",
        "replyto": "q3F0UBAruO",
        "invitation": "ICLR.cc/2023/Conference/Paper1657/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work investigates human-agent collaboration in Multiplayer Online Battle Arena (MOBA) games and presents a human interpretable communication framework through which humans and agents can communicate their respective strategies during the game. It introduces the concept of meta commands which acts as the common ground for communication of macro-strategies between the humans and the agents. Further, the framework includes a meta command selector which estimates the value of the meta commands and selects a reasonable meta command to follow. The results show that using such a Meta Command Communication (MCC) framework improves the overall team performance and beats the current SOTA in MOBA games. \n\nOn a broader level, MOBA games present an interesting set of challenges for Game AI, one of which is human-agent collaboration. This paper addresses this challenge and shows that having an interpretable communication framework between humans and agents can achieve effective human-agent collaboration. They evaluate the MCC agent on agent-only environments and on online experiments with humans of different levels. ",
            "strength_and_weaknesses": "Strengths/Interesting aspects\n1. Communication through a human-interpretable common ground: The MCC framework establishes a human interpretable communication framework where irrespective of what the agent\u2019s internal representations are, the communication happens in a symbolic form which is interpretable to humans in the loop. This also inclines with having symbols as the lingua-franca [1] with the meta-commands being the symbolic information that is used as a common ground in human-agent interaction. The winrates from the online experiments show the effectiveness of the MCC framework with different types of agents and levels.\n\n2. Human subject study for the performance of MCC teams: Even though the results show that as more humans are present in the team the win rate reduces against an all-agent SOTA method, in comparison to the other methods, human teams with MCC agents improve the overall team performance. Further, the response rates are also high in MCC-human teams.\n\n3. Comparison of Command selector and high-level participant\u2019s value systems - Although it doesn\u2019t give a complete picture, it provides a preliminary insight into how the CS meta command selections are consistent with the ranking results\n\nWeaknesses/Clarifications\n\n1. Meta command is defined as a three element tuple where the event E (what to do ) is also part of it. The paper states the following \u201cTo achieve the former, a hand-crafted command converter function f cc is used to generate L of meta-commands by extracting the location from explicit messages, such as signals, sent by humans. To achieve the latter, we use a Command Encoding Network (CEN) \u03c0\u03d5(m|o) to generate L of meta-commands.\u201d It is not clear as to how the event E in a meta command is incorporated (if needed) in the meta command execution if in the command conversion stage the output is just the locations extracted/generated from the explicit human commands/observations. Clarification on this aspect should be provided in the paper.\n\n2. As much as it extends the SOTA in Honor of Kings, the common ground here is game-specific. The meta commands seem to be specifically designed for Honor of Kings. It is not clear if the meta commands or techniques are generic to other MOBA games either. \n\n\n[1] Kambhampati, Subbarao, et al. \"Symbols as a lingua franca for bridging human-ai chasm for explainable and advisable ai systems.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 11. 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The description of the meta command framework is clear. The details of the compute requirements for the experiments, training time, batch size, and important hyperparameters are provided. They also point out the tricks used to enhance the training process of the command selector.\n\nQuality: Although the idea of having a common ground in human-agent collaboration might not be new, its instantiation in MOBA games is innovative. The evaluations (both on agent-only and human subject teams) are well described even though reproducing the results might be a hassle due to the extensive computational requirements. Analysis on objective performance and subjective preferences was provided. The paper also addresses the potential limitations of the work and provides additional insights through ablation studies.\n\n\nNovelty: This work, albeit being fiercely application-oriented, is novel in the sense that it shows the effectiveness of establishing a common ground in human-agent interactions instantiated in MOBA games. Even though the common grounding and techniques are specific to MOBA games, it could potentially inspire people in the AI community to have a human-interpretable communication framework for human-agent collaboration.\n\n\nReproducibility: Given the extensive use of compute resources, it might be difficult for many researchers to reproduce the results. Along with that, additional details on some of the blocks in each of the model structures (like the MLPs in CEN model structure or the LSTMs, CNNs in MCCAN structure) seem to be missing in order to replicate the results.\n",
            "summary_of_the_review": "Overall, this work provides a novel instantiation of a human-interpretable common ground in MOBA games. The results show that such a framework improves human-agent collaboration and thereby the overall team performance. The framework on the other hand seems to be specific to the game used as the test-bed, Honor of Kings. But the promising objective and subjective evaluation of the framework could potentially inspire people in the AI community to have a human-interpretable communication framework for human-agent collaboration. Hence, this paper can be accepted to ICLR 2023.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1657/Reviewer_Lfxb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1657/Reviewer_Lfxb"
        ]
    }
]