[
    {
        "id": "8anKelWLeR-",
        "original": null,
        "number": 1,
        "cdate": 1666532841399,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666532841399,
        "tmdate": 1669228168015,
        "tddate": null,
        "forum": "c9lAOPvQHS",
        "replyto": "c9lAOPvQHS",
        "invitation": "ICLR.cc/2023/Conference/Paper4052/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a general method to approximate natural gradient updates in a deep-learning context. The natural gradient is a way of second-order optimization. They apply the Legendre-Fenchel duality to learn a direct and efficiently evaluated model for the product of the inverse Fisher with any vector. Under some assumptions, they prove that their approximate natural gradient descent can converge to a local minimum including a global minimum. Several numerical experiments are shown to demonstrate the effectiveness of the algorithm. ",
            "strength_and_weaknesses": "Strength: The usage of Legendre and the auxiliary parameters plus meta-learning is an interesting idea. They have the ability to compute the inverse of the Fisher information matrix efficiently. \n\nWeakness: \n\n1. The authors clearly miss many important results on the approximation of the information matrix, using the Legendre duality and auxiliary variable. \n\nAffine Natural Proximal Learning. Wasserstein Proximal of GANs, Wasserstein natural gradient, etc. \n\n2. Does the authors have some analytical examples to demonstrate the effectiveness of algorithms? Some examples in Gaussian distributions could be useful.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clear in both math and numerics. \n\nNovelty: The usage of online learning and meta-learning in natural gradient is a new and good approach. \n\nReproducibility: Some analytical examples are needed. ",
            "summary_of_the_review": "The paper is written well with clear mathematics and many numerical experiments. However, the authors still require some direct analytical examples in Gaussian distributions to demonstrate their ideas. Much important literature is missed in the context. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4052/Reviewer_qrGz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4052/Reviewer_qrGz"
        ]
    },
    {
        "id": "etpsvVF46oa",
        "original": null,
        "number": 2,
        "cdate": 1666632330361,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632330361,
        "tmdate": 1669490736574,
        "tddate": null,
        "forum": "c9lAOPvQHS",
        "replyto": "c9lAOPvQHS",
        "invitation": "ICLR.cc/2023/Conference/Paper4052/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose an approximated natural-gradient descent for NN via the Legendre transformation. \nThe authors show that the proposed methods performs similarly as K-FAC and its variants  in some medium scale problems (FACES and MNIST).",
            "strength_and_weaknesses": "Strength:\n* Re-formulation of natural-gradient descent via the Legendre transformation.\n* Surrogate approximation Q (Sec 4.2) to handle the positive-definite constraint in the Fisher information matrix. \n* Some theoretical results of the proposed method.\n\nWeakness:\n---\n* Theorem 2 and Theorem 3 are weak/standard since the authors make **direct** assumptions of the matrix Q. Note that Q is an approximation of the Fisher information matrix. Thus, the authors implicitly make assumptions about the Fisher information matrix.\nMoreover, the impact of Eq 16 is unknown since the assumptions of the matrix Q bypass the convergence analysis of Eq 16 as shown in the proof of Thm 2 and 3. \n\n---\n* There is an implicit and key assumption in Theorem 1. Without addressing this issue, the theoretical results are pretty useless.\nMy main question is when/why  $- E_{p(\\mathcal{D})}[ \\nabla_\\theta^2 \\log p(\\mathcal{D|\\theta})] $ is positive-definite in NN cases, which often implies the NN is not over-parametrized. If $- E_{p(\\mathcal{D})}[ \\nabla_\\theta^2 \\log p(\\mathcal{D|\\theta})] $ is positive-definite, why is damping needed?  The authors should include a NN example to justify this assumption. \n\nAs mentioned in Eq (8), the H function must be **strictly convex** w.r.t. $\\delta$.\nAs shown in Lemma 1 (in the appendix), the assumption $p(\\mathcal{D} | \\theta) = p(\\mathcal{D} | \\theta+\\delta) $ iff $\\delta=0$, implies that Eq 24 holds. In other words, $- E_{p(\\mathcal{D})}[ \\nabla_\\theta^2 \\log p(\\mathcal{D|\\theta})] $ must be **positive-definite**.  This is a strong assumption for NN problems. \n\nThere are two types of Fisher estimators when it comes to approximating the expectation via (data) samples. \n1. $-E_{p(\\mathcal{D})}[ \\nabla_\\theta^2 \\log p(\\mathcal{D}|\\theta)]$  (As shown in Eq 2, this does not guarantee to be positive semi-definite. This work exploits this result by making this assumption in Thm 1:  $p(\\mathcal{D} | \\theta) = p(\\mathcal{D} | \\theta+\\delta) $ iff $\\delta=0$.  )\n2. $E_{p(\\mathcal{D})}[ \\nabla_\\theta  \\log p(\\mathcal{D}|\\theta)  \\nabla_\\theta^T  \\log p(\\mathcal{D}|\\theta)   ]$  (As shown in Eq 1, this is  always positive semi-definite. This result has been used in many existing methods such as KFAC)\n\n\n---\n* To avoid directly addressing the positive-definite assumption in Thm 1, the authors instead introduce a  positive-definite surrogate matrix Q. \nThis trick turns the one-step NGD into a two-step update (Eq 16-17). \nIn other words, the iteration cost can be higher compared to KFAC.\nThe authors should discuss the additional cost of this extra step. \nFigure 2 should be plotted in terms of the wall clock time instead of the number of epochs.\nSince the authors use a Kronecker structure in Q, a proper baseline in all experiments should be KFAC. Please report the wall clock time instead of the number of epochs. \nThe additional cost could be high if a very deep NN is used.\n\n---\n* Why is the one-step Adam update in Eq 16  good enough to approximate Eq 11? In optimization, we usually have to use a double-loop update (e.g., taking a few Adam steps) since the one-step Adam update is in general not good enough. Does taking a few more Adam steps in Eq 16 improve the performance?   The authors should discuss this point. \n \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The presentation can be improved.  Discuss the hidden assumptions and the hidden (time and space) cost of the proposed method.\n\nQuality: The theoretical results and empirical results are not strong. \n\nNovelty: It is interesting to use the positive-definite surrogate Q to approximate the Fisher matrix by introducing an additional cost.\n \n ",
            "summary_of_the_review": "The theoretical results are weak.\n* There is an implicit and key assumption in Theorem 1. \n* Theorem 2 and Theorem 3 are weak/standard due to the direct assumptions on Q, which bypasses the convergence analysis of Eq 16.\n\n\nWithout providing additional results, in the current form, the empirical results are also weak.\nI wonder if the one-step Adam update shown in Eq 16 is enough for more challenging problems. \nThe meta-learning approach is very similar to the auto-encoder task considered in the paper. In this case, the one-step Adam update may be good enough.\nMy question is whether the proposed method such as the one-step Adam update works well on other tasks such as CNN.\n\n\n \n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4052/Reviewer_QPAM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4052/Reviewer_QPAM"
        ]
    },
    {
        "id": "BKSbEeyXcu",
        "original": null,
        "number": 3,
        "cdate": 1666648820785,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648820785,
        "tmdate": 1666648820785,
        "tddate": null,
        "forum": "c9lAOPvQHS",
        "replyto": "c9lAOPvQHS",
        "invitation": "ICLR.cc/2023/Conference/Paper4052/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new optimization algorithm for deep neural networks that incorporates aspects of curvature information via meta-learning. In particular, authors model the inverse of the Fisher information matrix that appears in natural gradient descent via a set of additional parameters that are optimized jointly in an online manner. Authors empirically demonstrate that the proposed method outperforms first order optimization methods and is competitive with other higher order methods on an autoencoder benchmark. Authors also provide a proof of convergence for functions that satisfy the PL condition.",
            "strength_and_weaknesses": "Strengths:\n* Paper present empirical verifications on model and real benchmarks\n* Proposed method features convergence guarantees\n* Claims are clearly stated and accompanied by proofs\n\nWeaknesses:\n* Wall clock time is not compared directly with other second order approximations (KFAC, etc)\n* Limited guidance provided on the choices of parameterization of the inverse fisher information matrix. (might be common knowledge in the field)\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clearly written and relevant references are provided. It also provides a good account for related work, generally understandable by someone with general knowledge of optimization methods. Additional details are provided in the appendix and the codebase is made available on github with instructions to reproduce the results.",
            "summary_of_the_review": "Authors consider an optimization method inspired by natural gradient descent which is based on meta-learning an inverse of the Fisher information matrix that can be optimized online during training. The paper reads well and is understandable even for someone outside of the optimization algorithms subfield.\n\nTo be more suitable for a larger audience I think the paper would benefit from an additional paragraph discussing how this method could fit into common use cases (i.e. large parameter regimes) and if there are model-specific scaling expectations in terms of runtime (compared to SGDm).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Authors consider an optimization method inspired by natural gradient descent which is based on meta-learning an inverse of the Fisher information matrix that can be optimized online during training. The paper reads well and is understandable even for someone outside of the optimization algorithms subfield.\n\nTo be more suitable for a larger audience I think the paper would benefit from an additional paragraph discussing how this method could fit into common use cases (i.e. large parameter regimes) and if there are model-specific scaling expectations in terms of runtime (compared to SGDm).\n",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4052/Reviewer_q97H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4052/Reviewer_q97H"
        ]
    },
    {
        "id": "qZgxu-7RKV",
        "original": null,
        "number": 4,
        "cdate": 1666663275124,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663275124,
        "tmdate": 1666668062260,
        "tddate": null,
        "forum": "c9lAOPvQHS",
        "replyto": "c9lAOPvQHS",
        "invitation": "ICLR.cc/2023/Conference/Paper4052/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approximation to the natural gradient update by using Legendre-Fenchel duality to estimate the inverse Hessian with the convex conjugate of the entropy, which is summarized in Theorem 1. Section 4.2 goes on to parameterize a model $Q(\\lambda)$ to predict the solution to the conjugate in (11). The method is experimentally evaluated on deep linear networks (Section 5.1) and the auto-encoding benchmarks from Goldfarb et al., 2020.",
            "strength_and_weaknesses": "Strengths\n+ Using duality to estimate the natural gradient update is an appealing idea and theorem 1 and the proof are great contributions\n+ The results in Section 5.2 on the settings from Goldfarb et al. are convincingly executed and attain especially nice reliability and variability\n\nWeaknesses\n+ The comparisons on the deep linear networks benchmark in Figure 1 and on the wall-clock times in Figure 3 do not consider other natural gradient methods as baselines, only SGD with momentum and Adam\n+ The top part of Figure 2 shows that FishLeg does not significantly improve upon the overall training loss/test error of other methods.\n+ The approximation $Q(\\lambda)$ may not be very accurate, especially at the beginning of training. Can bad approximations to it cause bad updates to be performed? It could be interesting to note that sub-optimal predictions of $Q(\\lambda)$ could be improved by either running more updates to it or having an explicit fine-tuning phase to find a better conjugate.\n+ $Q(\\lambda)$ is the same shape as the inverse Fisher matrix and is thus an extremely high-dimensional prediction problem. The full (or Kronecker-factored) matrix is too memory intensive to compute for large models, so it seems challenging and computationally intensive to predict it for large models.\n+ How strong are the assumptions of Theorem 1?",
            "clarity,_quality,_novelty_and_reproducibility": "This is a well-written paper with a clearly-presented method and results.\n\nOn the related work, predicting the solution to the convex conjugate has also been explored in the optimal transport community for computing the continuous Wasserstein-2 dual in the following papers:\n\n+ [Three-Player Wasserstein GAN via Amortised Duality](https://researchmgt.monash.edu/ws/portalfiles/portal/291820310/291812754_oa.pdf)\n+ [Optimal transport mapping via input convex neural networks](https://arxiv.org/pdf/1908.10962.pdf)\n+ [Wasserstein-2 generative networks](https://arxiv.org/pdf/1909.13082.pdf)\n+ [On amortizing convex conjugates for optimal transport](https://arxiv.org/pdf/2210.12153.pdf)",
            "summary_of_the_review": "I recommend to accept the paper as it's a reasonable and well-executed contribution. I would be willing to raise my score after a discussion on not having comparisons to other natural gradient methods in some of the experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4052/Reviewer_ACHH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4052/Reviewer_ACHH"
        ]
    }
]