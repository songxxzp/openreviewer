[
    {
        "id": "j6jgAZU9LX",
        "original": null,
        "number": 1,
        "cdate": 1666597231253,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597231253,
        "tmdate": 1666597231253,
        "tddate": null,
        "forum": "qpeAhwxTopw",
        "replyto": "qpeAhwxTopw",
        "invitation": "ICLR.cc/2023/Conference/Paper2560/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to initialize each linear layer of a deep neural network to be identity so as to accelerate training.",
            "strength_and_weaknesses": "Strengths: \n1. The experiments are done on many datasets.\n2. The code is attached for reproducibility.\n\nWeaknesses:\n1. It has been shown (Thereom 5 in [1]) that the identity initialization may not lead to convergence for deep linear neural networks. \n2. For residual layer, this work proposes to initialize the weight matrix with tiny values so as to avoid the dead neuron problem. This is not sensible. Because of the shortcut connections, weights with zero initialization still receive non-zero gradients.\n3. The overall idea, identity initialization, is not novel. For example, see [2]. The proposed variant in this paper is technically trivial.\n\nReferences:\n[1] Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks, Peter L. Bartlett, David P. Helmbold, Philip M. Long, ICML 2018.\n[2] A Simple Way to Initialize Recurrent Networks of Rectified Linear Units, Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton, arXiv 2005.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is fine. Novelty is close to none. Reproducibility is good.",
            "summary_of_the_review": "This paper proposes identity initialization for deep neural networks to accelerate the training. I found the idea lack of novelty and the claim of solving the dead neuron problem insensible.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2560/Reviewer_st9W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2560/Reviewer_st9W"
        ]
    },
    {
        "id": "8tWPI2-j_L",
        "original": null,
        "number": 2,
        "cdate": 1666640129416,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640129416,
        "tmdate": 1666640129416,
        "tddate": null,
        "forum": "qpeAhwxTopw",
        "replyto": "qpeAhwxTopw",
        "invitation": "ICLR.cc/2023/Conference/Paper2560/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new initialization scheme for both vanilla convnets and convnets with residual structures. This initialization is a simple and special case to maintain dynamic isometry at initialization time. Specifically, they initialize the weight matrix to be identity. The authors also propose a spatial shift scheme to improve the representation learning ability. Using this initialization, they can train nine-layer vanilla convnets and 110-layer residual networks on CIFAR. It is also shown this initialization method is helpful for transformer training.",
            "strength_and_weaknesses": "Strength:\n1) This paper is motivated by the dynamic isometry formulation and propose a very simple initialization (identity initialization). The simplicity is good and empirically helpful on varies settings.\n2) The proposed two techniques on improving the feature diversity are interesting.\n\nWeakness:\n1) Missing Reference: This paper misses discussion with DiracNet [1] and ISONet [2]. DiracNet reparameterize the convolutional kernals and essentially initialize it to be an identity. ISONet explicitly initialize the conv kernels to be identity both in vanilla convnets and residual convnets. It is necessary to discuss the differences and advantages over these previous works.\n2) Some terms and examples are confusing.\n- the robustness in this paper actually means insensitivity to hyperparameters. However, this term in machine learning usually means the network is insensitive to input noise.\n- Table 1 is also confusing. Most of the entries in the \u201cw/ BN\u201d category should be put in the \u201cw/o\u201d because Zero \\gamma, Fixup, Skip Init, ReZero are designed for training residual networks without normalization.\n- Figure 7 can be replaced by a Table becuase the current figure does not have caption and it is not straightforward to get the point from the figure.\n- The experiment setting and conclusion of section 4.4 is also confusing. Is the proposed initialization has any randomness? It seems the initialization is deterministic. In this case, why the std is not 0 in Table 1?\n3) The central message of the paper seems to be unclear. The authors mix two information together: 1) the trainability of network without batch norm; 2) the proposed method improves over the original initialization. The first part is well-motivated because BN has many disadvantages. However, the proposed method only shows the trainability of a network with 100 layers on CIFAR. The ImageNet experiments are based on networks with varies normalization layers and the proposed method only has marginal improvement over the baselines.\n\n[1] Zagoruyko et al. DiracNets: Training Very Deep Neural Networks Without Skip-Connections.\n[2] Qi et al. Deep Isometric Learning for Visual Recognition. ICML 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of this paper can be improved a lot. I listed a few examples in the second point of the previous weakness section. I think the author should address them and further polish the storytelling and presentation. Besides, I feel I don\u2019t obtain much insight from reading this paper. The authors may also consider to visualize the distribution of the weight matrix, e.g. how the weight matrix value changes through the training.\n\nThe paper has its own novel part. However, the central identity initialization has been proposed an analyzed before (ISONet and DIractNet, listed in the weakness).\n\nIt provides code. I checked the relevant implementation. I think the proposed method is simple and also easy to reproduce.",
            "summary_of_the_review": "In summary, this paper is interesting as it shows simply initializing the kernels to be identity can make vanilla networks trainble. However, the writing, experiments, and presentation need to be improved a lot. I cannot recommend acceptance for this paper before this improvement.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not Applicable.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2560/Reviewer_QUve"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2560/Reviewer_QUve"
        ]
    },
    {
        "id": "-wUk1UcCWa9",
        "original": null,
        "number": 3,
        "cdate": 1666748285576,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666748285576,
        "tmdate": 1666748285576,
        "tddate": null,
        "forum": "qpeAhwxTopw",
        "replyto": "qpeAhwxTopw",
        "invitation": "ICLR.cc/2023/Conference/Paper2560/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The reviewed work proposes \"identical initialization\" as a novel initialization scheme for neural networks. The guiding principle of this initialization scheme is to ensure that each layer, at initialization, propagates activations identically.\n\nThe motivation behind this choice is that the resulting isommetry property of the network prevents vanishing or exploding gradients and thus allows for the stable training of deep architectures. \n\nNumerical experiments on classification for CIFAR10 and Imagenet show improvements due to the new initialization scheme. ",
            "strength_and_weaknesses": "### Strength\nThe main strength of the paper are the empirical improvements apparently achieved by the proposed initialization scheme.\n\n### Weakness\nThe first-order weakness of the paper is the poor quality of its writing. There are numerous grammatical mistakes and vague language such as \"Therefore, if all values are set to 0, it can obtain non-zero gradients of a suitable magnitude.\" Due to the above, despite numerous attempts, I am still not entirely confident if I have correctly understood the approach proposed by the authors. \n\nA second weakness (possibly related to the first) is that the motivation for the proposed approach over FIXUP and ZerO is not clear. In particular the fixup approach, that similarly initializes the residual branches as products of identities and zero seems closely related. \n",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above, the paper is poorly written, with many grammatical mistakes and vague language. This makes it difficult to discern the novelty of the proposed approach compared to existing ones. Given the authors' awareness of FIXUP and ZerO, the claim \"To our best knowledge, it is the first trial to put identical-like initialization into practice\" appears to be false.\nRegarding the numerical experiments, I am somewhat concerned by the low performance of the baseline in Figure 6. ",
            "summary_of_the_review": "In summary, the empirical results suggest that the authors may be up to an interesting result. However, the writing, presentation, and comparison to existing work need major improvement. In its present state, the paper is not ready to be published at a major ML conference.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2560/Reviewer_TjT6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2560/Reviewer_TjT6"
        ]
    },
    {
        "id": "72_rEjuIYzf",
        "original": null,
        "number": 4,
        "cdate": 1667035644412,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667035644412,
        "tmdate": 1667035644412,
        "tddate": null,
        "forum": "qpeAhwxTopw",
        "replyto": "qpeAhwxTopw",
        "invitation": "ICLR.cc/2023/Conference/Paper2560/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose Identical Initialization (IDInit) that initializes weights using identity matrix and its variants. They consider how to initialize non-square matrices, residual structures and convolutional operations using identity-like methods. Empirical evaluation demonstrates its good performance and fast convergence. ",
            "strength_and_weaknesses": "The paper is well-written and easy to follow. The authors also present extensive empirical evaluations to support their methods.\n\nHowever, I have a severe concern regarding the effectiveness of IDInit. The identical initialization has been well studied in ZerO paper [1], where the authors theoretically prove that a simple identical initialization leads to training degeneracy in practical networks. The major reason behind this training degeneracy is that for a dimension-increasing matrix (e.g., the $D_{i+1} > D_{i}$ case in the current paper), a simple \"identity + zero-padding\" causes the rank constraint ((ii) in Theorem 1 in ZerO). \n\nTo initialize dimension-increasing matrix, IDInit replicates the inputs to span over the output dimension. However, this causes a similar rank constraint (i.e., a symmetric problem) as each replica in the dimension-increasing matrix receives the same gradient (excludes some special cases). ConstNet paper [2] also shows the degeneration caused by insufficient feature diversity (i.e., the symmetry), which is related to the replication here. In addition, the authors only perform trainability analysis from a signal propagation perspective, without considering the potential degradation caused by the replication. \n\nI highly recommend the authors to empirically evaluate if IDInit can pass the simple rank verification in Figure 3 of ZerO paper. On the other hand, I suspect the surprisingly good empirical evaluation of IDInit comes from randomized operations (e.g., dropout) in networks, which directly breaks the training degeneracy. I suggest the authors to carefully consider this training degeneracy issue, as it affects the effectiveness and universality of the proposed method.\n\nOther comments:\n\n1. The need of zero preserving initialization (IDIZ) is unclear. The authors motivate IDIZ by arguing that Fixup or ZerO method will not work if the multiplier in batch normalization is initialized as 0. However, this is not a standard case because the multiplier in BN is usually initialized as 1. The motivation also requires a special case where BN is applied after the last conv layer. These reasons can not convince me why IDIZ is really needed. Do authors initialize the multiplier in BN as 0 in their empirical evaluation? If so, I suggest the authors to provide more details about this case.\n\n2. How does IDIC compare to the simple IDI in convolution case? The authors mention that IDIC is used to improve feature diversity but it's unclear why it is needed. ZerO paper [1] directly utilizes IDI in convolution (Algorithm 2 in ZerO) and it works well.\n\n3. Comparison to ZerO. Since ZerO is the most relevant work to IDInit I believe, I suggest the authors to empirically compare them, including both performance and determinacy analysis (since both methods claim the benefit as being deterministic).\n\n[1] https://arxiv.org/abs/2110.12661\n\n[2] https://proceedings.mlr.press/v119/blumenfeld20a.html",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear with high quality. However, I believe it lacks enough novelty. Identical initialization has been studied in ZerO paper [1], where IDInit and the proposed variant (Eq 14) in convolution are already considered. Its deterministic benefit is also studied in ZerO. It would be great if the authors can provide more details about the connection and difference between the two methods. Another concern is the reproducibility given the training degeneracy issue discussed earlier, which affects the effectiveness of the proposed method.",
            "summary_of_the_review": "Identical initialization is a very interesting topic and IDInit looks promising. My major concerns are its effectiveness (training degeneracy problem), necessity (motivations behind IDIZ and IDIC) and novelty (compared to ZerO). I would like to discuss these issues with the authors, and I am happy to raise my score if the authors can address my concerns.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2560/Reviewer_Wd9d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2560/Reviewer_Wd9d"
        ]
    }
]