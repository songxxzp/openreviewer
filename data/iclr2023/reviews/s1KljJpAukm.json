[
    {
        "id": "qGApnAJFgl",
        "original": null,
        "number": 1,
        "cdate": 1666392544138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666392544138,
        "tmdate": 1667848450948,
        "tddate": null,
        "forum": "s1KljJpAukm",
        "replyto": "s1KljJpAukm",
        "invitation": "ICLR.cc/2023/Conference/Paper2496/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper looks at data free techniques for quantization. The framework is to look for automorphisms on the (R,x) which the authors show is essentially power functions. Then exponential representation are optimized to minimize reconstruction error. Finally, experimental results are presented to show the usefulness of the work.",
            "strength_and_weaknesses": "Strengths: - The paper proposes a novel quantization technique based on power functions. - Some theoretical analysis is performed to describe the method. - Extensive experiments are performed to evaluate the method. Weaknesses: - The mathematical derivations are hard to follow. I think there is room for improvement in terms of clarity. ",
            "clarity,_quality,_novelty_and_reproducibility": "I understand this field is moving very fast and it is hard to keep track to the most recent related works. But most of the works that were compared to are already dated by a few years. Can the authors compare to more recent works such as [1]\n\n[1] Sakr, Charbel, et al. \"Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training.\" International Conference on Machine Learning. PMLR, 2022.",
            "summary_of_the_review": "The paper proposes a new quantization method, provides analyses and empirical evaluations. \n\nDuring the rebuttal and through discussions with the authors, I have raised my score to a 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2496/Reviewer_TJBp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2496/Reviewer_TJBp"
        ]
    },
    {
        "id": "qbKkIFr_1wm",
        "original": null,
        "number": 2,
        "cdate": 1666727218641,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727218641,
        "tmdate": 1668528625611,
        "tddate": null,
        "forum": "s1KljJpAukm",
        "replyto": "s1KljJpAukm",
        "invitation": "ICLR.cc/2023/Conference/Paper2496/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to transform weights with power operation before quantization to improve the quantized accuracy. The motivation behind this is the bell-shaped distribution of weights, indicating that fine-grained resolution should be allocated around zero. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method against several strong baselines. ",
            "strength_and_weaknesses": "### Strength \n* This paper is well-motivated. Bell-shaped distribution of weights is widely observed. Power transformation addresses this issue well. \n* Continuous automorphisms are highly appreciated by quantization and hardware implementation. This method leverages continuous automorphisms in a smart way. \n* Extensive experiments are conducted on various architectures and datasets with significant improvements. \n\n\n### Weaknesses\n* Is $W$ in Eq. 2 a matrix? What is the precise definition of $|\\cdot|^a$?\n* \"for instance, for both SiLU (EfficientNets) and GeLU (Image Transformers), the activations are bounded below\" Are they empirical observations? How many examples are tested?\n* Although the authors mentioned the overhead of power transformation (against uniform quantization) in Eq (5), it should be further analyzed in detail to help understand the trade-off. \n* Missing reference on power-based quantization [1] [2]\n\n[1] Zhang, Sai Qian, et al. \"Training for multi-resolution inference using reusable quantization terms.\" Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. 2021.\n[2] Li, Yuhang, Xin Dong, and Wei Wang. \"Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks.\" ICLR (2020).",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "Both methods and results are interesting and significant. I am looking forward to feedback from the authors on my questions. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2496/Reviewer_6h14"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2496/Reviewer_6h14"
        ]
    },
    {
        "id": "UPJyW1ApfaS",
        "original": null,
        "number": 3,
        "cdate": 1666837631747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666837631747,
        "tmdate": 1666837631747,
        "tddate": null,
        "forum": "s1KljJpAukm",
        "replyto": "s1KljJpAukm",
        "invitation": "ICLR.cc/2023/Conference/Paper2496/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a novel non-uniform data-free quantization method.\nThe main Idea (which is very interesting) is to find the best non-uniform quantization intervals for both weights and activations by minimizing the reconstruction error using the proposed quantization and de-quantization method. \nThe paper provides extensive experimentation on various datasets and tasks (including Image-classification and NLP) and archives promising results (outperforms  SOTA). ",
            "strength_and_weaknesses": "Strength: \n\n- Novelty \n- The claims are backed by mathematical explanation \n- promising results\n\nWeaknesses: \n\n- The paper is heavy and very hard to read. To someone like me who is a non-native English speaker, the paper looks like a poorly translated novel that consists of unnecessary fancy words and grammars. It was hard to keep reading the paper and not getting distracted. \n- Section \"3.3 FUSED DE-QUANTIZATION AND ACTIVATION FUNCTION\" needs more clarification. I could not understand how the quantization works for the activations. Do we calculate $x^{\\alpha}$ online during the inference for the activations? If yes (and I think the answer is yes) then how is it calculated? ",
            "clarity,_quality,_novelty_and_reproducibility": "Please read \"Strength And Weaknesses\" section ",
            "summary_of_the_review": "As mentioned in the Strength And Weaknesses section, I did not understand the quantization of activations. Hence, I need to rely on the answer from the authors and other reviewers comments. I am open to change my score in future. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2496/Reviewer_Htvb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2496/Reviewer_Htvb"
        ]
    }
]