[
    {
        "id": "yEWNJIhhK3",
        "original": null,
        "number": 1,
        "cdate": 1666289144713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666289144713,
        "tmdate": 1669653042547,
        "tddate": null,
        "forum": "E1_fqDe3YIC",
        "replyto": "E1_fqDe3YIC",
        "invitation": "ICLR.cc/2023/Conference/Paper3681/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the setting of unsupervised domain adaptation when the shift between the source and target is large and thus gradual domain adaptation is needed. They propose a gradual domain adaptation algorithm that creates intermediate domains for the gradual adaptation and then applies (standard) gradual self training on these created domains. Authors propose to create domains in extracted feature space (e.g. on the embedding level). They show that it does better than creating domains using the original input space. The idea of creating intermediate domains spans from theoretical bounds: synthetic domains minimize the average Wasserstein distance between the pairs of intermediate domains.  \n",
            "strength_and_weaknesses": "Strength:\n- clarity: well written and easy to follow\n- good idea inspired by the theoretical bounds - find the optimal intermediate domains\nWeaknesses\n- Experiments don't compare with UDA baselines\n- Experiments are not convincing when there are no intermediate domains provided. May be more intermediate domains needed? \n- The method is expensive",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: excellent. Really well written, provides enough background on all the methods needed\nQuality: theoretically justified idea\nNovelty: novel way of generating intermediate domains\nReproducibility: authors provide enough of the details to be able to reproduce",
            "summary_of_the_review": "I read authors response and am increasing my score to accept\nOverall very well written paper with somewhat weak experimental results. I am willing to be convinced otherwise if authors are able to provide additional results (please see below)\n\nMethod:\n1) Is the number of domains generated a hyperparameter? Why do you go only up to 4 (or rather 4*number of intermediate domains provided)? Can you instead find the optimal value of it using the bounds in (2)? (e.g. min over T (1/T\u00a0+ T/sqrt(n)\u00a0+ 1/sqrt(nT))?\n2) How many data points do you geenerate for each intermediate domain - is it the number of non zero elements in optimal transport matrix (so m+n approx, where m and n are source and target number of points?)\n3) This method sounds extremely expensive. Apart from OT which you can solve approximately, you train additional T models to convergence. Can you add wall time comparison to Tables 1 and 2, I am trying to understand the bulk part of it (is it 4 times slower for 4 intermediate domains? Or more?)\n\nExperiments:\n1) \u00a0A bit surprised that only two datasets are used, but seems that a lot of papers on GDA use those 2 datasets. Though this\u00a0popular paper https://arxiv.org/pdf/2002.11361.pdf also has experiments on Gaussian synthetic data. Do you have experiment results on such data by any chance?\n2) Why not to add UDA like DANN or MMD to Table 1 and 2. You seem to say that they don't make sense since the shift is large. Do they underperform the 0-self training entry?\n3) is in Table 1 and 2 \"0 domains\" entry correspond to self training? \nLooking at table 1 it seems that when there are 0 given intermediate domains, you don't imporve\u00a0over the self training at all apart from one case (portrait on 4). All drastic improvements are only when there are some intermediary domains already provided. Why is this?\n4) Also i do find that you report # of generated domains this way confusing - for 3 given domains and 3 generated, you actually generated 9 domains right?\n\nMinor:\n- (6) treats T delta as the length, but it was the AVERAGE (so 1/T) in (3). \n- intro: performance degrade => performance degradation\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3681/Reviewer_d4Li"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3681/Reviewer_d4Li"
        ]
    },
    {
        "id": "aTf9yOJ_U9G",
        "original": null,
        "number": 2,
        "cdate": 1666630217834,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630217834,
        "tmdate": 1666630217834,
        "tddate": null,
        "forum": "E1_fqDe3YIC",
        "replyto": "E1_fqDe3YIC",
        "invitation": "ICLR.cc/2023/Conference/Paper3681/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "When the distribution gap between the source domain and the target domain is very large, some intermediate domains are usually used to make the model gradually adapt to the target domain. The author proposes the GOTA method to generate intermediate domains based on Wasserstein geodesic. Empirically, this GOAT framework can improve the performance of standard GDA(Gradually Domain Adaptation) when the given intermediate domains are scarce, significantly broadening the real-world application scenarios of GDA",
            "strength_and_weaknesses": "Strengths:\n(1)\tThis paper gives a clearly theoretical analysis which explanation of the definition and generation method of the intermediate domains\n\nWeaknesses:\n(1)\tIs there some problem with Equation 10. How does it become the mixup of samples? please give a detailed proof of this problem.\n(2)\tThe experiment is insufficient. The authors mentioned [1] in related work, which also relies on intermediate domain generation, however, is compared in this work. \n(3)\tWhile the generated intermediate domains by the proposed GOAT method is demonstrate in Figure 2 for the rotated MINIST data, other more complicated scenarios are not shown. \n(4)\tIn Table, given domains=0, GOAT does not bring improvement, why? \n(5)\tSome papers have also achieved very good results in constructing intermediate domain at the input level [2,3]. Discussion and comparisons are needed.\n[1] Gong R, Li W, Chen Y, et al. Dlow: Domain flow for adaptation and generalization[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 2477-2486.\n[2] Na J, Jung H, Chang H J, et al. Fixbi: Bridging domain spaces for unsupervised domain adaptation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1094-1103.\n[3] Na J, Han D, Chang H J, et al. Contrastive Vicinal Space for Unsupervised Domain Adaptation[J]. arXiv preprint arXiv:2111.13353, 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and novelty are only marginally significant. Not sure if the paper results can be reproduced.",
            "summary_of_the_review": "The generation theory of the intermediate domain is interesting, but there are some issues with the derivation. In addition, the author's experiment is relatively simple and lacks justification for the existing DA datasets",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3681/Reviewer_3XRG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3681/Reviewer_3XRG"
        ]
    },
    {
        "id": "F-_Ne0s7A7",
        "original": null,
        "number": 3,
        "cdate": 1666678123863,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678123863,
        "tmdate": 1670128717746,
        "tddate": null,
        "forum": "E1_fqDe3YIC",
        "replyto": "E1_fqDe3YIC",
        "invitation": "ICLR.cc/2023/Conference/Paper3681/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to address the task of gradual domain adaptation (GDA) which aims to address large distribution shifts via adapting models along a sequence of intermediate domains. To overcome domain shifts (especially when the given intermediate domains are missing or scarce), the authors propose a generative gradual domain adaptation with optimal transport (GOAT). Specifically, the authors start by generating intermediate domains along the Wasserstein geodesic, then adapt the source-trained classifier to target by considering the intermediate domains. Experimental results demonstrate the effectiveness of the proposed GOAT. However, I still have concerns about this paper. My detailed comments are as follows. ",
            "strength_and_weaknesses": "*Positive points\n\nThe task, gradual domain adaptation, which the authors focus on, is truly important and practical in reality since it mitigates the limitation of UDA whenever the distribution shift between the source and the target is large.\n\nThe proposed GOAT adapts a progressive manner for shifting between different distributions along Wasserstein geodesic, which is guaranteed by a theory from wang et al[1].\n\nThe visualization results in figure.2 are intuitive for the semantic meaning of the generated intermediate domains, which helps readers to understand this paper better.\n\n\n*Negative points\n\nHow to decide the number of generated domains of the proposed method? More discussions are needed.\n\n1. In table 1, it is observed that the performance of GOAT suffers performance degradation with a large # Generated domains (i.e., # Given Domains=4, # Generated domains=4). Besides, when given 3 domains GOAT achieves the best performance with 2 generated domains, why is it the case? Why more intermediate domains would not boost the performance of GOAT? \n\n2. But in Figure 4 (b), with the increase of generated domains, the test accuracy tends to increase, including Random, Uniform and OT settings. It seems that the contradictions in the experimental results.\n\nIt is necessary for the authors to provide more comprehensive and fair results.\n\n1. The experiments are only conducted on two datasets (4 datasets are used in [1], 3 datasets are used in [2]), I would like to see the performance of the proposed GOAT in more datasets for further verification of the effectiveness of GOAT.\n\n2. This paper only considers one baseline (GST, published in 2 years ago) for the comparison experiments. More recent state-of-the-art such as [1, 2] should be considered. \n\n**Reference\n\n[1] Understanding gradual domain adaptation: Improved analysis, optimal path and beyond. In ICML 2022.\n\n[2] Kumar, A., Ma, T., & Liang, P. (2020, November). Understanding self-training for gradual domain adaptation. In ICML 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe Idea of generating intermediate domains, in my opinion, is good but requires more verification by conducting sufficient experiments. \n\nThe proposed method is verified on two datasets and compared with only one existing method, which seems insufficient.\n",
            "summary_of_the_review": "The idea of generating intermediate domains for unsupervised domain adaptation is interesting to me. Moreover, the proposed GOAT is backed by a theory guarantee. However, the datasets and baselines for verifying the GOAT are insufficient for me. Thus, I recommend the weak rejection of this paper. If the authors convince me of the effectiveness of the GOAT, I would be glad to change the reviews. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3681/Reviewer_tuhk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3681/Reviewer_tuhk"
        ]
    },
    {
        "id": "eqe7LjI-0M",
        "original": null,
        "number": 4,
        "cdate": 1666983341858,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666983341858,
        "tmdate": 1669643688546,
        "tddate": null,
        "forum": "E1_fqDe3YIC",
        "replyto": "E1_fqDe3YIC",
        "invitation": "ICLR.cc/2023/Conference/Paper3681/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the domain adaptation problem, i.e., transferring a model trained on a source domain to a new target domain for which unlabeled data is available. It does this using self-training (pseudo-labeling, noisy student) where the source-trained model is used to generate labels for the target domain. The paper builds on theoretical insights which suggest self-training works best when the input data shifts gradually from the source to target domain (gradual self-training). This theory suggests that the ideal case would involve walking the shortest path between all the joint data distributions of the intermediate domains.\n\nThe authors transform this theoretical insight into a practical algorithm by:\n\n* making a covariate shift assumption which allows them to operate in a latent space, rather than the joint distribution of the data\n* generating intermediate domains that lie close to the Wasserstein geodesic between the source and target domain\n* adding a regularization term to the optimal transport problem to reduce computational complexity, in combination with thresholding the values in the transport plan to minimize space complexity",
            "strength_and_weaknesses": "I like this paper.\n\nThe idea seems well-grounded in theory. It seems to me that the authors are minimizing a lower bound though (the right-hand side of equation 6). I am also curious to know what cost function ($c$ in proposition 2). Is it L1 or (squared) L2-distance? Did the authors consider evaluating these different cost functions?\n\nI appreciate the simplicity of the resulting algorithm: Pick a feature space to interpolate in, map both the source and target domain to this space, solve the optimal transport problem, and now just self-train your network as you slowly walk this path. This makes a lot of intuitive sense from the perspective of neural networks learning smooth \"manifolds\" of data, and GOAT exploits this well.\n\nExperimentally, one concern I have is that GOAT does not seem very effective when there are no intermediate domains available (which seems to me the most common setting in practice). In particular, in table 1 the best approach for going from 0 to 45 degrees rotated MNIST seems to be to not generate any intermediate domains, and the gain for portraits is within margin of error. On the other hand, these tasks seem small and artificial enough (especially rotated MNIST) that I don't want to draw too strong conclusions from it.\n\nSince GOAT does not require any intermediate domains, I would love to see GOAT evaluated using a few more domain adaptation datasets (e.g., the WILDS 2.0 dataset). This would give a much clearer signal as to how this algorithm performs in practice on real-world datasets. Is there a reason the authors did not do this? Does the EMD solver not scale to this size?",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is well written and clearly presented. As a reader I would have preferred a higher-level, intuitive description of the algorithm before being presented with equations such as 4 and 5 (especially given that many of the details of equations 4 and 5 don't seem necessary to understand the main idea). However, this is partly a matter of taste.\n\nOne minor issue: I appreciate the fact that confidence intervals were given for the numbers in table 1 and 2. However, it seems to me that there are several case in which multiple columns should be bolded, because they are within margin of error (e.g., between 74.0\u00b11.3 and 74.2\u00b12.5 it makes little sense to bold the latter as being the \"best\").\n\nThe method seems novel, although I would argue that it seems a relatively straightforward idea given the existing theory and algorithms out there.\n\nThe models used are simple and clearly described in the appendix, so I imagine this work should be straightforward to reproduce.",
            "summary_of_the_review": "I think this is a good paper. It's main shortcoming is the experimental evaluation, which is limited to very small networks on small and artificial datasets. If such experiments are added, I would be able to support acceptance of this paper more strongly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3681/Reviewer_SoJb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3681/Reviewer_SoJb"
        ]
    }
]