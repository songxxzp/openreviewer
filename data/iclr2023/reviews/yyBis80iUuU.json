[
    {
        "id": "QOIUyA3zUr",
        "original": null,
        "number": 1,
        "cdate": 1666186743295,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666186743295,
        "tmdate": 1668803509078,
        "tddate": null,
        "forum": "yyBis80iUuU",
        "replyto": "yyBis80iUuU",
        "invitation": "ICLR.cc/2023/Conference/Paper1923/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method to run fitted Q learning on both online and offline data. Theoretical analysis is provided under several assumptions and empirical improvements over certain baselines are observed on certain tasks.",
            "strength_and_weaknesses": "Strength: the theoretical analysis seems novel and the paper is easy to follow\n\nWeaknesses: \n1. The authors failed to properly place the algorithmic contribution in the literature. For Algorithm 1, I feel the idea is basically to fix the demonstration / offline data in experience replay and run some RL algorithm (e.g., FQI). This feeling is especially true when it is actually implemented with a discounted factor using a target network. This idea is not new, see, e.g., [x][y]. The authors failed to explicitly acknowledge the similarity between Algorithm 1 and existing works. The way to control catastrophic forgetting mentioned in Section 4 is also already done by [y] and the authors failed to acknowledge.\n2.  As a result of (1), the main contribution is the theoretical analysis. So I must hold a higher standard when evaluating the contribution of the theoretical analysis. I, however, believe that Assumption 1 is way too strong. Can the author actually give a concrete meaningful example beyond the tabular setting where this assumption holds?\n3. The selection of baselines is improper. The only baseline that is designed for both online and offline data is DQFD (Hester et al., 2018). However, it looks DQFD does imitation learning instead of RL for offline data. I believe using baselines methods that do RL for both online and offline data is more proper and more informative. In particular, the authors should compare with [x] and [y]. And **what is the fundamental difference between the algorithm that the authors actually implemented and [y]? The authors are using DQN and [y] uses DDPG. Other than this, I cannot see any major difference.** I feel the paper is quite far from making the following claim [z]\n4. The domains for comparison can be much improved. For now the two domains are quite similar and requires guided exploration. I agree this is an important area of RL but considering only such areas is not enough to support the claim [z]. I think the authors should also compare in the settings where the reward is not so sparse. The authors should also consider offline dataset of different quality. D4RL seems to be a good complementary benchmarks. Otherwise the authors should consider tone down their claims.\n\n[x] Lee, Seunghyun, et al. \"Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble.\" Conference on Robot Learning. PMLR, 2022.  \n[y] Vecerik, Mel, et al. \"Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards.\" arXiv preprint arXiv:1707.08817 (2017).  \n[z] On the empirical side, several works consider combining offline expert demonstrations with online interaction (Rajeswaran et al., 2017; Hester et al., 2018; Nair et al., 2018; 2020; Vecerik et al., 2017). In our experiments, we find that these methods perform poorly when the offline dataset also contains experience from low-quality policies, while our method is robust to the quality of the offline data.",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "See above\n\n==========================================\nThe revision cleared my major concern in credit assignment so I raised score accordingly.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1923/Reviewer_bRLc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1923/Reviewer_bRLc"
        ]
    },
    {
        "id": "3OhWr6rV5PY",
        "original": null,
        "number": 2,
        "cdate": 1666571219174,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666571219174,
        "tmdate": 1670378266423,
        "tddate": null,
        "forum": "yyBis80iUuU",
        "replyto": "yyBis80iUuU",
        "invitation": "ICLR.cc/2023/Conference/Paper1923/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers hybrid reinforcement learning, which is a setting where both offline data and online data are available. The paper proposes a corresponding algorithm to be computation-efficient and sample-efficient under such a setting. The theoretical sample complexity upper bound shows both the characteristics of offline and online RL. The paper also provides experiments result to show the strength of the proposed algorithm.",
            "strength_and_weaknesses": "Strength: The paper considers a less studied setting and provides complete theoretical analysis of the proposed algorithm. The theoretical bound seems to have the optimal rate.\n\nWeakness: The motivation of the hybrid RL setting is not very clear to me. In particular, the introduction part says that, to overcome the issue of sample inefficiency (of online RL), attention has turned to the offline RL setting. However, the information of each data point in online RL is surely much more than that of offline RL. Given the same sample size, we should always choose online RL over offline RL. The only setting that I can imagine that hybrid RL is useful is that there is already an offline dataset and we do not want to waste the data.\n\nAs for the theory part, similar confusion remains: the result in Theorem 1 seems similar to the convergence of offline RL, as it still requires the offline data set to have a good coverage condition number $C_{\\pi}$. it is also not clear how the result is better than online RL (e.g., Du et al 2021). I am also wonder if using pessimism or optimism (from usual offline and online RL algorithms) in the hybrid RL algorithm is helpful to improve the sample efficiency. Or if the algorithm deliberately choose not to use them to reduce the computation cost.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is clear and the problem setting and algorithms are novel.",
            "summary_of_the_review": "The paper provides solid analysis of a novel RL algorithm in a less studied setting, in both theories and experiments. However, the contribution of the result seems not very clear, and thus I believe the paper is a little below the acceptance threshold. If the authors can well settle the concerns stated above, I may raise my score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1923/Reviewer_mmKF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1923/Reviewer_mmKF"
        ]
    },
    {
        "id": "8Iu4HGlTC9d",
        "original": null,
        "number": 3,
        "cdate": 1666677672334,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677672334,
        "tmdate": 1670453615915,
        "tddate": null,
        "forum": "yyBis80iUuU",
        "replyto": "yyBis80iUuU",
        "invitation": "ICLR.cc/2023/Conference/Paper1923/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents Hybrid Q-Learning (Hy-Q), an algorithm for a hybrid RL setting, where the agent has both offline datasets and can interact with the environment in an online manner. Hy-Q learns a value function by fitted-Q iteration on both the offline dataset and the online experiences, where the online samples are collected by the greedy policy w.r.t. the learned value function. Theoretical analysis for the sample complexity of Hy-Q is provided. In addition, in the case of the linear Bellman completeness model, it can be shown that Hy-Q is a computationally efficient algorithm, which is in contrast to the existing methods that require solving NP-hard problems. In the experiments, Hy-Q outperforms both pure online and pure offline RL algorithms in Combination Lock and Montezuma's Revenge.\n",
            "strength_and_weaknesses": "[Strengths]\n1. The paper presents a principled hybrid RL algorithm, which is supported by theoretical guarantees.\n2. The algorithm is simple to implement and computationally efficient.\n3. Experimental results show that the proposed Hy-Q outperforms online/offline baseline algorithms.\n\n\n[Weaknesses]\n1. It would be great to compare with more baseline algorithms that deal with both offline datasets and online experiences in the experiments, e.g. [1,2,3]\n2. Page 4: There exists a gap between theory and practical implementation, i.e. using a fixed proportion of offline samples rather than forgetting them gradually, but I think this is minor.\n3. Some notations are used without explicit definitions/explanations. What is $m_{off}$ and $m_{on}$ in Theorem 1? Also, they do not appear in the statement.\n\n\n[Questions]\n1. In the case of using flexible function approximators such as neural networks, is $C_\\pi$ bounded?\n2. Can Hy-Q be extended to continuous action spaces?\n3. In Algorithm 1, the proportion of the offline dataset would decrease as the number of online samples increases. I am curious about the performance of this forgetting algorithm, rather than using a fixed proportion of offline samples.\n4. I was wondering if the combination of Hy-Q and exploration strategy (e.g. RND) can improve or degrade the performance in the experiments.\n5. The theoretical result was somewhat counter-intuitive to me. Hy-Q collects online samples via a 'greedy' policy w.r.t the learned value function; thus there is no exploration mechanism in the algorithm. Suppose an offline dataset was collected by a poor agent. Then, intuitively, exploitation alone w.r.t. the learned value function may be prone to get stuck in local optima and thus unable to learn a near-optimal policy. However, Corollary 1 is saying that Hy-Q can always learn a (near-)optimal policy with a sufficiently large number of samples. It would be great to provide some intuitive explanation about how 'greedy' data collection can always yield a near-optimal policy.\n\n\n[1] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020\n\n[2] Lee et al., Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble, 2021\n\n[3] Xie et al., Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning, 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and provides a non-trivial theoretical contribution to a hybrid RL.",
            "summary_of_the_review": "The paper presents a simple yet provably efficient algorithm for hybrid RL with theoretical guarantees, which I think is a novel and nice contribution. Still, in the experiments, comparisons with baseline algorithms that use both offline and online experiences are missing.\nLastly, one thing unclear to me was that Hy-Q uses a greedy policy for online data collection, but it can always learn a near-optimal policy without falling into some local optima, which is counterintuitive. I would like to raise my score if those concerns are addressed.\n== post-rebuttal\nThanks for your clarifications, which addressed most of my concerns. I raise my score accordingly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1923/Reviewer_WaHo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1923/Reviewer_WaHo"
        ]
    },
    {
        "id": "yEFH8y2QN",
        "original": null,
        "number": 4,
        "cdate": 1666695274061,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695274061,
        "tmdate": 1666695274061,
        "tddate": null,
        "forum": "yyBis80iUuU",
        "replyto": "yyBis80iUuU",
        "invitation": "ICLR.cc/2023/Conference/Paper1923/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a simple hybrid offline-online RL algorithm, and provides an extensive theoretical analysis to showcase its properties. The proposes algorithm is compared with baseline algorithms that fall into the categories of online, offline and hybrid RL, as well as \"learning by imitation\", in the \"combination lock\" and \"Montezuma's revenge\" problems.",
            "strength_and_weaknesses": "Strengths\n- The paper is well-written and the references appropriate.\n- The proposed hybrid offline-online RL algorithm is simple and effective, and an extensive theoretical analysis of its properties is provided.\n\nWeaknesses\n- The comparison with other algorithms is weak. Just two simple problems are used, and the experiments could better analyze the reason of the different performance of the compared algorithms.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n\nI think that the paper could better analyze  the reason of the different performance of the compared algorithms. For instance, If we analyze  DQFD  and Hy-Q, besides the difference in their losses, it seems that the main difference lies in how they sample offline and online data. I was wondering how DQFD would perform if it adopted the same scheme that the discounted Hy-Q algorithm follows to get batches from offline and online data (annealing beta), instead of forcefully maintaining offline data in its replay buffer. One could hypothesize that this might be the reason why DQFD consistently worsen its performance on Montezuma's revenge when deteriorating the quality of the offline dataset. Furthermore, I'm also wondering how a pretraining stage such as that proposed in DQFD would affect the discounted Hy-Q algorithm. I think that maybe these experiments would be beneficial to get a better grasp on how these two algorithms compare, and how these practical design decisions affect them.",
            "summary_of_the_review": "The paper proposes a simple hybrid offline-online RL algorithm, it provides an extensive  analysis to showcase its properties and a comparison with other RL algorithms in two problems. I suggest to improve the experimental part and to provide more extensive experiments in more challenging problems, as well as a deeper analysis of the obtained results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1923/Reviewer_SJM5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1923/Reviewer_SJM5"
        ]
    }
]