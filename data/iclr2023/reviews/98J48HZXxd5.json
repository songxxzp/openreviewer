[
    {
        "id": "GtC3NiMTh6G",
        "original": null,
        "number": 1,
        "cdate": 1666364180007,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666364180007,
        "tmdate": 1666364180007,
        "tddate": null,
        "forum": "98J48HZXxd5",
        "replyto": "98J48HZXxd5",
        "invitation": "ICLR.cc/2023/Conference/Paper4923/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed an autoregressive diffusion model for the graph generation. A node-absorbing diffusion process was introduced. For the forward diffusion a diffusion ordering network was suggested, and for the reverse diffusion a denoising network was designed. These two networks can be trained jointly with a simplified log-likelihood loss. Experiments have shown the competitive results of the proposed method when compared to several existing methods. ",
            "strength_and_weaknesses": "Strength:\n\nA new node-absorbing diffusion process has been introduced, and a data-dependent ordering can be learned via the optimization. The proposed method has been compared with several SOTA methods and shown its competitive results including the generation time.\n\n\n\n######################\n\nWeaknesses:\n\n. Section 2: The authors mentioned that the absorbing diffusion is the most promising generation method. Can you add some explanation on that?\n\n. Section 3.1: The diffusion ordering network produces the probability of the node at time t via equation (1). Can you explain why such an ordering would reflect the topology/regularities of the graph?\n\n. Section 3.3: The proposed training objective has ignored the KL-divergence term in equation (3). Can you evaluate such approximation error, ie. calculate the actual KL-divergence and check whether it indeed approaches zero?\n\nExperiments:\n\n. Table 1: The performance of the proposed GRAPHARM on Cora is not competitive. Can you explain that?\n\n. Effect of Diffusion Ordering: Can you illustrate the proposed ordering visually to give a sense that it does reflect the topology/regularities of the graph when compared to the random ordering? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality: The presentation of this paper is good.\n\nNovelty: An autoregressive diffusion model has been proposed for the graph generation, where a proper ordering can be learned via training. \n\nReproducibility: The authors have provided details about the training parameters and network architectures in the appendix. These would help to reproduce the experimental results.  ",
            "summary_of_the_review": "The authors proposed an autoregressive diffusion model for the graph generation. The analysis is solid and the proposed method has been compared with several SOTA methods and showed its competitive performance. The paper can be further improved by adding more explanations and discussions (see the comments in Weaknesses above). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4923/Reviewer_KoQP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4923/Reviewer_KoQP"
        ]
    },
    {
        "id": "ZhRr5YwYN_L",
        "original": null,
        "number": 2,
        "cdate": 1666672131912,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672131912,
        "tmdate": 1670344285459,
        "tddate": null,
        "forum": "98J48HZXxd5",
        "replyto": "98J48HZXxd5",
        "invitation": "ICLR.cc/2023/Conference/Paper4923/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a new graph generative model based on an autoregressive diffusion model. It shows good performances in a list of tasks. ",
            "strength_and_weaknesses": "Strength: \n\n+ the new model has a formulation of autoregressive diffusion model. \n+ the model is very efficient\n+ the model shows good generation performances in a list of tasks. \n\nWeaknesses\n\n- Though the formulation is motivated differently, it has a large overlap with (Chen et al., 2021). The diffusion ordering network is roughly the same as the variational distribution of node orders in (Chen et al., 2021). The variational lower bound is also very similar to (Chen et al. 2021). The difference is actually because this work neglects the difference between graph sequences and node orders: a graph sequence is not equivalent to a node ordering, which is analyzed by (Chen et al., 2021). \n- The generative procedure is also similar to the standard autoregressive generative procedure: every time adding a new node and the edges connecting to this new node. I don't see the benefit of describing this procedure using the diffusion process. \n- There are several flaws in the analysis. The first one is the difference mentioned above, then the variational lower bound is not exactly correct. The second one is the complexity of generation: the complexity of this model should still be O(n^2) (the analysis below equation 2). Anyhow the model needs to make a decision for each node pair. The running time is saved by reducing the number of calls of neural networks (from O(n^2) to O(n)). \n\n[After a careful study of the submission and a discussion with authors], I find a critical derivation error in the submission. In particular, the submission bases its derivation on equation 8, which is from (Chen et al., 2021), but the term $p(G_{1:n}, \\sigma_{1:n})$ in this submission is different from that in the previous work, so the derivation is incorrect.   ",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe description of the diffusion process can be clearer. For example, the definition of \"Absorbing Node State\" seems to indicate that a node enters the absorbing state after/because it is masked and connected to other nodes with masked edges. But actually, the diffusion process puts the node to an \"absorbing node state\" -- as a consequence, it is masked and connected to other nodes with masked edges. \n\n[After discussion], the notation system in this submission is not well defined. It should follow previous work and use adjacency matrices to derive probabilities. ",
            "summary_of_the_review": "The work devises a new graph generative model from the perspective of autoregressive diffusion models. However, it does not seem to add much to the original formulation. The model design is a contribution: it is efficient and shows good performance in several tasks. \n\n[After discussion:] the notation system of the submission is not well defined, and the derivation is problematic. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4923/Reviewer_dB1j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4923/Reviewer_dB1j"
        ]
    },
    {
        "id": "J6grcbPRTp",
        "original": null,
        "number": 3,
        "cdate": 1666717089027,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717089027,
        "tmdate": 1666717089027,
        "tddate": null,
        "forum": "98J48HZXxd5",
        "replyto": "98J48HZXxd5",
        "invitation": "ICLR.cc/2023/Conference/Paper4923/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- This paper proposed an autoregressive diffusion-based graph generation model. \n- Its performance outperforms SOTAs. \n- GRAPHARM also has low complexity.",
            "strength_and_weaknesses": "### Strength \n- This paper gives a new method for the graph generation.\n- The well-designed model obtains better performance.\n\n### Weaknesses\n- It does not well explain what autoregressive diffusion is.\n- Theoretical analysis is not sufficient",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality, Novelty And Reproducibility are good.",
            "summary_of_the_review": "- This paper gives a new method for the graph generation.\n- The well-designed model obtains better performance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4923/Reviewer_uUZJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4923/Reviewer_uUZJ"
        ]
    },
    {
        "id": "YPq6US0i5y1",
        "original": null,
        "number": 4,
        "cdate": 1666722579086,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666722579086,
        "tmdate": 1666722579086,
        "tddate": null,
        "forum": "98J48HZXxd5",
        "replyto": "98J48HZXxd5",
        "invitation": "ICLR.cc/2023/Conference/Paper4923/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an autoregressive diffusion model (ARM) that leverages the absorbing discrete diffusion as the diffusion process on graphs. Since node ordering is involved in the diffusion process, the generative process needs to learn the reversed node ordering for the graph generation. The paper exploits the objective function in the KL divergence and uses a reinforcement learning procedure to bypass the inference complexity. Empirical results demonstrate superior performance compared with baseline methods.",
            "strength_and_weaknesses": "1. This paper introduces a discrete autoregressive diffusion process on graphs that is exchangeable in node ordering. The number of steps required is proportional to the number of nodes, thus improving sampling efficiency over previous graph sampling methods.\n\n2. The paper resolves the node ordering inference problem involved in the inference process by exploiting the variational lower bound property. The method is similar to \"Order matters: probabilistic modeling of node sequence for graph generation.\" by (Chen et al. 2021). \n\n3. Empirical results demonstrate superior performance in generation quality and time complexity.\n\nWeaknesses:\n1. The paper does not clearly explain the \"optimal node generation ordering\" learned from the posterior. An example demonstrated in Figure 3 in the appendix shows that GraphRAM tends to generate one community and then generate the other community. But more insight is required to check why that ordering is preferred in GraphRAM.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written. The proposed autoregressive diffusion model is new, but the inference algorithm, especially the node ordering inference part, is similar to Chen et al. 2021.",
            "summary_of_the_review": "This paper proposes a diffusion process on discrete graphs that derives an autoregressive generative process with inferred node ordering. Empirical results are good, but my concern is its similarity with previous node ordering inference algorithms.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4923/Reviewer_EBGH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4923/Reviewer_EBGH"
        ]
    }
]