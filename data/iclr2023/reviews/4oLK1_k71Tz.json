[
    {
        "id": "nqaTffxbtos",
        "original": null,
        "number": 1,
        "cdate": 1666633993709,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633993709,
        "tmdate": 1666633993709,
        "tddate": null,
        "forum": "4oLK1_k71Tz",
        "replyto": "4oLK1_k71Tz",
        "invitation": "ICLR.cc/2023/Conference/Paper6118/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This is an interesting study that proposes novel ideas on how to incorporate contextual cues in the form of category-specific feedback signals into visual recognition algorithms. The proposed algorithm (mid-vision feedback, MVF) outperforms several baseline models in several relevant benchmark object recognition tasks. More generally, this paper introduces plausible mechanisms by which feedback signals (here in the form of expectations) could be incorporated into the traditional architectures for processing visual information.\n\n",
            "strength_and_weaknesses": "Strengths\n\nThe proposed architecture is quite simple and therefore elegant. \n\nThe proposed ideas about how to incorporate feedback are rather generic and can be directly applied not just for any one specific dataset or task, but rather across multiple different problems including cases of no contextual cues, cases where contextual cues are incongruent or unusual, cases where contextual cues are provided by task demands or by statistical correlations previously learned from images.\n\nWeaknesses\n\nSeveral of the points below are questions, rather than weaknesses.\n\nIt is true that the majority of connections are feedback rather than feedforward, but not because of the work of Kveraga et al. In humans, we do not know any details about anatomical connectivity. Probably the best work making this point is Markov et al Cerebral Cortex 2014. \n\nI find Figure 1 to be somewhat confusing. First, the authors state that they cropped the images to exclude context. But then they end up arguing that understanding the difference of context can help appreciate the low-level feature differences. Which one is it, is there context in these images or not? And does it help or not? \n\nFigure 2. What are the injection sites? What are the big blue and red \u201cA\u201d symbols, are those the affine transformations? Are L_O and L_0 the same thing? Where do the high-level contextual expectations come from? \n\nWhat are the x-axis and y-axis in Figure 5?\n\nWhile this is not the main point of this study, why is it that ViT generally performs below VGG in Tables 3 and 4?\nThe rationale for having two stages in training is not clearly explained. Why two stages as opposed to a single stage of training?\n\nRelated to the question above regarding Figure 2, what decides which representations should be orthogonalized? In the example of desk and horse, who in the model says that there should correspond to two different contexts and should be orthogonalized? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation format is rather strange, with a dump of figures and tables without any clear text accompanying them. Most of the tables and figures are not even cited, except for single lines in a 4-sentence discussion. Most of the paper is devoted to explaining MVF and basically a single page is devoted to the results, which is mostly a dump of tables and figures without any description or with minimal description.",
            "summary_of_the_review": "This is an exciting effort to bring ideas about how feedback signals can be incorporated to help incorporate contextual cues into visual recognition. Although the clarity of the presentation can certainly be improved quite a lot, the results and ideas are novel and help push the field forward.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6118/Reviewer_ym4H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6118/Reviewer_ym4H"
        ]
    },
    {
        "id": "uEF643I-XlX",
        "original": null,
        "number": 2,
        "cdate": 1666654769298,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654769298,
        "tmdate": 1670481701038,
        "tddate": null,
        "forum": "4oLK1_k71Tz",
        "replyto": "4oLK1_k71Tz",
        "invitation": "ICLR.cc/2023/Conference/Paper6118/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper provides a mid level vision feedback module as an Add-On for CNN's. Authors argue that such mid-level feedback properties improve CNN performance (though perhaps for the wrong reasons), and also argue that this idea is worth pursuing given neuroscience/perceptual psychology motivated ideas.",
            "strength_and_weaknesses": "See below for Main Paper Summary. TLDR: this paper studies mid-level vision as an improvement gateway for modern CNNs, however I do not think the baselines or experiments are well controlled enough to validate the statement. In addition it is not obvious what the contributions to performance are that stem from noise injections, recurrence and SSL-like learning when isolated, and not stacked together. In addition authors only focus on performance and not other (perhaps more interesting) tasks that extend object recognition such as common corruption robustness or adversarial robustness.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is a bit difficult to understand with regards to flow. I understand where the authors are going, but am not entirely convinced I got there in a clear way, so I perhaps could have missed something in my review that could change my mind. I'm also not sure why authors did not select neuroscience as the primary category for review and instead opted for \"Applications\"",
            "summary_of_the_review": "++ Additional Controls and tentative confounding variables:\n\nTables 2 and 3 at first glance seem quite exciting and one could reasonably say that feedback is playing a critical role here. However, how can we dissociate between the effects of feedback and the effects of SSL or noise injection in training when comparing to the \"vanilla\" VGG, CNN and Transforer models? If a feedback loop would be the only other mechanism added in training then these tables are good indicators for the feedback effects in the paper, but if not, then knowing the exact contribution to the differential offset is not clear. Consider the question: what would have happened if a regular feedforward (withour recurrence) CNN would have been trained with noise injections, and PCA? Would the authors still get those positive bumps in score?\n\n++ Several Missing References that hurt the paper in relation to where it lies on innovation:\n\nMid-level vision:\n* Long, Yu & Konkle : \"Mid-level visual features underlie the high-level categorical organization of the ventral stream\" (texforms). PNAS 2019.\n* Jagadeesh & Gardner. \"Texture-like representation of objects in human visual cortex\" . PNAS 2022.\n* Harrington & Deza. \"Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks\". ICLR 2022.\n\nRecurrence:\n* Tang et al.: \"Recurrent computations for visual pattern completion\". PNAS 2018.\n* Lotter, Kreiman & Cox: \"Pred-Net\". ICLR 2017.\n\nContrastive Learning:\n* Konkle & Alvarez. \"Instance-level contrastive learning yields human brain-like representation without category-supervision\". ArXiv 2020 & Nature Communications 2022.\n* Li et al. \"Contrastive Clustering\". AAAI 2021.\n\n++ Additional experiments:\n\nIn experiments suggested by Long et al, Jagadeesh & Gardner, & Harrington & Deza, authors played around with machine vision classifiers showing them \"texforms\", these stimuli are stimuli that were synthesized from noise that match the local texture statistics of an object, in addition to preserving it's coarse shape. I wonder, if the argument here is to be made that mid-level feedback will improve general consistency and image categorization removing higher-order contextual biases, then the MVF-based networks should show similar confusion matrices for such texform stimuli and also the original stimuli. \n\nWhile it may be too difficult for authors to add these experiments in the rebuttal, it would be nice to see these or some variation such as common corruption robustness (ImageNet-C) or adversarial robustness (either quantiatively or qualitatively : see for example Berrios & Deza, ArXiv 2022 on testing 'brain-alignment' of transformer based models). \n\n+ Final Thoughts:\n\nOverall, I don't think having a small one or 2 point percentage increase in classificatiion accuracy (albeit a very coarse control, unless I missed something) is enough to convince me that \"feedback is better/necessary for computer vision\". I am however on the author's side, and I believe their premise is true, but I don't think these experiments drive the point home completly.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6118/Reviewer_zASa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6118/Reviewer_zASa"
        ]
    },
    {
        "id": "0Mmm7PEjeS",
        "original": null,
        "number": 3,
        "cdate": 1666832288832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666832288832,
        "tmdate": 1671516885624,
        "tddate": null,
        "forum": "4oLK1_k71Tz",
        "replyto": "4oLK1_k71Tz",
        "invitation": "ICLR.cc/2023/Conference/Paper6118/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes that images will be better processed if the context is known. Figure 1 provides excellent motivation for this idea. The images are very similar in appearance, but correspond to very different classes (food vs. animals). Inspired by the well-known fact that there are as many or more feedback connections in the visual system as feedforward ones, the paper proposes the idea that this feedback from higher levels can provide context information, altering the features to be more aligned with that context. So the idea of the paper is to provide biasing feedback to the mid-level features of the network, emphasizing one context (stretching a vector representing that context, while shrinking another). In order to avoid too much interference between feature dimensions when this transformation is applied, the network first is trained with an orthogonality bias at the particular layer where the context transformation will be applied (termed the \"injection layer\").  Then it is trained to use a biasing context, which is actually a learned affine transformation of the features at that layer of the network, controlled top-down. Hence, if the context is known, this can be applied dynamically, depending on the context. This contextual \"hint\" improves classification performance of the network. They note that this context manipulation could come from a more symbolic level of processing, providing a link between convnets and neuro-symbolic processing, although that itself is not demonstrated. They also show that post-hoc filtering of the results (masking off animal outputs in a food context, for example) doesn't work as well as altering the feature dimensions at a hidden layer. They show that this manipulation is relatively generic, and can be applied to a vision transformer as well as a convnet, improving performance in both cases.\n\nI have read the authors' responses to my review, the other reviews, and skimmed the revised paper, and have bumped up my score by one point as a result.",
            "strength_and_weaknesses": "Strengths:\n\n+ This is a relatively novel idea about how to use top-down connections to improve performance in particular contexts. I have not seen any other work like this. \n\n+ The context manipulation almost always improves classification performance. \n\n+ It seems the context can be discovered automatically by the same network that is performing the classification, but the system usually works better if the ground truth context is known, which could come from some other process providing contextual feedback. \n\n+ The approach is applied to a cross product of three different datasets and three different network architectures (except one cell is missing for a reasonable reason...), and almost always improves performance. \n\n+ The approach is shown to work better than post-hoc filtering of the non-contextual outputs, at least when the ground truth context is known, but not when the context is predicted automatically. \n\nWeaknesses, with concrete, actionable feedback\n\n- The writing could be clearer\n\n- It isn't clear whether the number of parameters is matched between the networks with feedback (which adds the parameters of the affine transformation) and without feedback. I assume this would add n^2 parameters, if there are n units in the layer.\n\n- The context splits seem particularly arbitrary (e.g., streetcar and rocket are in one context?). It would be good use (for example) Figure 1 as a running example, and have a context that has food and animals or something similar and show that those images don't get confused in context.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing could be improved considerably. I provide a lot of typos and wording suggestions below, as well as a few slightly more substantive comments. Since you never talk about the contexts in the main paper, it would help a lot if you at least focused on one context split and explained what the context split was, and described how it helped classification. Figure 6 is a good start, but it would help to explain what the context is here. I see you say the first five and the last five are the two contexts, but that context split doesn't seem particularly well-motivated. Why is rocket with streetcar? Also, the \"Absent\" row and column is not explained, and it is apparently all 0's anyway, so I would remove it from the plot. \n\nQuality: The experiments seem competently done.\n\nNovelty: as noted above, I don't know of any other model that uses top-down feedback in this way; I think it is pretty novel and effective. \n\nFor reproducibility, I assume if this is accepted, the authors will provide their code, although this is not explicitly promised in the paper text. There are occasional contradictory statements about parameters and training set splits in the paper, which reduces reproducibility. \n\nWording suggestions, typos, minor comments:\n\nIn the abstract, you make the claim that \"applications [of this idea] range from image and video understanding to explainable AI and robotics.\" You demonstrate that it is applicable to images, but the rest seems like a stretch without more justification. I would remove this and if you have room, put some discussion of how it would apply to these domains in the discussion section (4.4).\n\np2: \"feedback naturally allows for detection of out-of-context objects\" I don't follow this. It seems like contextual feedback would prevent the recognition of objects that don't fit the context. Please explain your reasoning here. \n\nThis opens up a couple possibilities -> This opens up a couple of possibilities\n\np2, reference to Figure 7: It would be helpful to have some form of Figure 7, e.g., a smaller one, right here to illustrate the point, as readers may not search ahead for Figure 7, and it isn't very clear until one gets to Figure 3. \n\nAlso in this paragraph, don't start a sentence with a lower case abbreviation (\"w.r.t.\") Instead write \"Also, w.r.t. point #2,...\"\n\nIn this page and the next, you italicize \"injection level\" three times. Once is enough, as long as you define it the first time, which you do.\n\n\"The fact that these vectors have been trained with a bias towards orthogonality w.r.t. contexts means that, through affine transformations, characteristics associated with context presence or absence can be manipulated with less collateral impact on other features.\"\n\nreword as->\n\nIf these vectors have been trained with a bias towards orthogonality w.r.t. contexts allows for affine transformations to manipulate features associated with context presence or absence with less impact on other features.\n\n\"The fact that\" suggests we already know this fact. Here, you are trying to say what the implication of this training is.\n\n\"Orthogonalization bias introduces a tendency towards orthogonality...\" Of course it does, by definition! Say *why\" you are trying to orthogonalize. In particular, you want to say something like, \"The orthogonalization bias is introduced to increase the independence between contexts, so they can be manipulated without interference between them. [This point would be enhanced by having a version of Figure 7 here which shows the effect of this bias, making correlated context vectors (animate/inanimate, for example) more orthogonal.] You do say this, but this version makes the point immediately. \n\n\"This approach to incorporation of context expectations is controlled.\" Not sure what this sentence is saying. \n\nSection 2.2: \"course\" -> \"coarse\", and \"Additionallym\" -> \"Additionally\"\n\nThe paragraph just before the methods section reviews irrelevant literature. This isn't the kind of feedback you are talking about, so eliminate this paragraph. This will free up some space to add something like Figure 7 earlier. \nCaption of Figure 3: \"and the injection level feature representations for those contexts produced by the injection level.\" ->\n\"and the injection level feature representations for those contexts.\"\n\"After feature vector modulation through application of an affine,\" ->\n\"After feature vector modulation through application of an affine transformation\" (also in other places. \"affine\" is an adjective, but you keep using it as a noun.)\n\np6 of CIFAR100 or -> of CIFAR100 are\n\nTables 2,3,4: all tables: remove the percent signs and say they are all percents in the caption. Table 3, replace Pred Feedback with PF and explain in the caption; Table 4: Replace GT Masking and GT Feedback with GTN and GTF and describe in the caption; all tables: round to the 1st digit past the decimal, and you should be able to fit these in the width of the text. \n\nlambda in one place is changed to ILS in Figure 5. Also, in the caption to Figure 5, you say you use 0.075 in all of your experiments, but in the 3rd line of section 4.1 you say you use 1.0.\n\np8, section 4.1.3: third line: \"third last\" attention block?\n\nAppendix section B.1: \"For full class breakdown see Appendix.\" Since you're already in the appendix, give the section in the appendix.\n\nSection B.2:  \"we designate 80% of the images in each class for training, but only *2%* for testing due to the computational cost incurred by the high number of images and the need for frequent testing.\"\nSection F.1: \"Like the Imagenet dataset, we designate 80% of the dataset for training and *20%* for testing.\"\nNote the difference. \n\nAlso Section F.1: \"Splits based on migration behavior, splits based on trophic level () and splits based on primary lifestyle.\" \n\nFigure 7: Is this an artists conception, or based on data?\n\nFigure 8: It would be a bit more helpful to notate the injection level in the figure.\n\n\"Splits based on migration behavior, splits based on trophic level () and splits based on primary lifestyle.\" Can you please be more specific?\n",
            "summary_of_the_review": "This paper suggests a novel method for providing contextual feedback to a network that is performing categorization, inspired by the biology that there are many feedback connections in cortex. So the motivation is good. The results are that performance is almost always improved by context feedback. However, the actual implementation of this is a bit unclear. Are there two matrices for the affine transformation, one for one context and one for the other? I don't recall seeing this in the paper anywhere. It also isn't clear whether there is a matched number of parameters between the networks with and without contextual feedback. For these reasons, my enthusiasm for the paper is somewhat reduced.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6118/Reviewer_ZiEu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6118/Reviewer_ZiEu"
        ]
    }
]