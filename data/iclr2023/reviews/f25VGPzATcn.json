[
    {
        "id": "KQOx8KWJOCK",
        "original": null,
        "number": 1,
        "cdate": 1666630975726,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630975726,
        "tmdate": 1666630975726,
        "tddate": null,
        "forum": "f25VGPzATcn",
        "replyto": "f25VGPzATcn",
        "invitation": "ICLR.cc/2023/Conference/Paper1541/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on anomaly detection from time series. Specifically, this paper tackles this problem from a causal perspective. It first learns a causal structure from the data. The causal structure is assumed to be a directed acyclic graph (DAG) which captures stationary causal structure. The anomaly then can be defined as data points that do not follow the probability conditional on its causal parents. By learning the causal structure, the root cause of the anomaly can also be detected. Empirical evaluation with both simulation and public datasets shows that the proposed method obtains impressive results.",
            "strength_and_weaknesses": "Strengths:\n- Modeling the anomaly detection problem from the causal perspective is quite interesting and important. The detection of root causes also makes the model more explainable.\n- The performance obtained using multiple datasets is very impressive.\n\nWeaknesses:\n- I am not fully convinced by the assumption that the causal structure is a DAG. First, the DAG may not be able to capture the time lag of the causal relationship. Second, there could be positive/negative feedback loops over time. For example, in economics, lowering investment causes lowering income after a certain lag, and in turn, causes lowering investment after some lags. In the summary causal graph, there could be cycles. Could some patterns be captured and modeled as well?\n- It seems that the detection algorithm relies heavily on the preset threshold $\\lambda$. It is not clear how much it impacts the performance and how the users should select the best hyperparameter.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. In terms of novelty, approaching the anomaly detection problem from the causal perspective seems new to me, although the methods used to identify the causal structure are somewhat standard. The paper did not include many details about the implementation and hyperparameter settings, which could limit the reproducibility of the paper. I would like to encourage the authors to discuss more details about the implementation and the hyperparameter setting (e.g., $\\lambda$) and release the source codes to maximize reproducibility.",
            "summary_of_the_review": "Overall, this paper offers a new perspective to tackle the anomaly detection problem for time series data. The idea is reasonable. The proposed method is overall sound, although some parts of the methodological design need stronger justification and more explanations. The empirical evaluation shows that the proposed method is impressive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1541/Reviewer_oDPS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1541/Reviewer_oDPS"
        ]
    },
    {
        "id": "3KyPz8EOLuC",
        "original": null,
        "number": 2,
        "cdate": 1666752560169,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666752560169,
        "tmdate": 1666752560169,
        "tddate": null,
        "forum": "f25VGPzATcn",
        "replyto": "f25VGPzATcn",
        "invitation": "ICLR.cc/2023/Conference/Paper1541/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a two-step procedure to perform root-cause analysis (RCA):\n1. Learn a causal graph from the data,\n2. Identify the root causes based on the changes in the local mechanisms (conditional probabilities).\n\nThe paper is simple and easy to read. The experiments look extensive. There are doubts about the novelty of the idea though.",
            "strength_and_weaknesses": "### Strengths\n1. The idea is simple, makes sense, and should work. \n2. The paper is well-written and easy to follow.\n\n### Weaknesses\n1. The authors seem to miss a similar paper. Can you describe the relationship of the paper to the idea of the following paper?\n    * Budhathoki, K., Minorics, L., Bl\u00f6baum, P., & Janzing, D. (2022). Causal structure-based root cause analysis of outliers. In _International Conference on Machine Learning_.\n2. The authors should comment on the error propagation from the DAG learning step. They have a set of ablation experiments on this topic, but might not be convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper's writing is clear. The authors use the proper technical language.\n\nI need the authors' response to the paper mentioned in the previous section to finalize my evaluation of the novelty.",
            "summary_of_the_review": "The authors propose a two-step procedure to perform root-cause analysis (RCA):\n1. Learn a causal graph from the data,\n2. Identify the root causes based on the changes in the local mechanisms (conditional probabilities).\n\nThe paper is simple and easy to read. The experiments look extensive. There are doubts about the novelty of the idea though.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1541/Reviewer_UaBN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1541/Reviewer_UaBN"
        ]
    },
    {
        "id": "JKcibYtHxqh",
        "original": null,
        "number": 3,
        "cdate": 1666838754830,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666838754830,
        "tmdate": 1666838754830,
        "tddate": null,
        "forum": "f25VGPzATcn",
        "replyto": "f25VGPzATcn",
        "invitation": "ICLR.cc/2023/Conference/Paper1541/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper is based on the insight that anomaly detection can be treated as a causal discovery problem, to both improve detection and to facilitate root cause analysis. An anomaly is defined as a datum that does not fit the discovered data-generating process, in short an instance where the process is non-stationary. The graph decomposition of the timeseries by structure learning has heuristic value to pin-point the variable that causes the anomaly. The proposed method is demonstrated on simulation data and an extensive set of public datasets. \n",
            "strength_and_weaknesses": "The fundamental notion addressed by this paper is sound, and the conceptual approach is well thought out. This is a novel application that showcases the wide applicability of causal reasoning, both for discovering properties in data, and for understanding the real-world phenomena that the data represent. I tend to believe that this approach is better than the current methods. \n\nHowever the the paper does not elucidate a coherent method that fits into a mathematical model. What I see is a jumble of ideas, inconsistent and inaccurately presented. There are an overwhelming number of misperceptions that do not recommend the work:\n\n- p.2 p(z) = 1.695  THis is not a probability. Probabilities are bounded between 0 and 1\n-  p(y|x) cannot be considered a \"p -value\".  A p-value is a kind of tail probability of a statistical test.\n- In the probability p(y|x) x is the cause. Conventionally the influence goes from x, the cause, to y, the effect. \n- Why use a p-value test [ Heard & Rubin-Delanchy (2018)] when you have likelihoods, the proper Bayesian solution to this problem? This avoids the need to set a threshold on a value -- the likelihood of the anomaly can be reported instead. \n-The structure learning methods cited, although informally referred to as \"causal discovery\" methods are just structure learning (\"Markov factorization\") methods, and cannot generate causal claims without additional assumptions, or domain input. \n- The authors propose several ways to treat of temporal dependencies within one time-series, e.g. such as ARMA, without realizing that these dependencies fit into the structure learning paradigm used between timeseries. The autocorrelation within a time-series is just another dependency that, to make a consistent, comprehensive model should be incorporated into the learned network.  Then the method could detect changepoints -- something the current method appears to overlook. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to understand. It's lucidity reveals that many of the concepts introduced may not be well understood by the authors.  It would help if the paper would pick a consistent theoretical basis, as the Bayesian probabilistic nature of the methods used for causal discovery (e.g. Pearl's work) suggest.  WIthout revealing why the proposed method works  questions about reproducibility cannot be determined. \n\nAs for comparisons with previous work, there are a wide range of vector anomaly detection methods based on PCA.  One that is in widespread use that must be considered is:\nA. Lakhina, M. Crovella, C. Diot, \u201cDiagnosing Network-Wide Traffic Anomalies\u201d\nSIGCOMM\u201904, Aug. 30\u2013Sept. 3, 2004, Portland, Oregon, USA. \n",
            "summary_of_the_review": "The paper is a poorly developed approach to a good idea, that cannot support the empirical claims made by the paper. Without a better understanding, offered by a consistent mathematical argument why the results were obtained, there remain too many questions about the results, and detract from their credibility.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1541/Reviewer_FAmL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1541/Reviewer_FAmL"
        ]
    },
    {
        "id": "wVynptAlyD",
        "original": null,
        "number": 4,
        "cdate": 1670029130661,
        "mdate": 1670029130661,
        "ddate": null,
        "tcdate": 1670029130661,
        "tmdate": 1670029130661,
        "tddate": null,
        "forum": "f25VGPzATcn",
        "replyto": "f25VGPzATcn",
        "invitation": "ICLR.cc/2023/Conference/Paper1541/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In the present manuscript, author(s) offer a causality based solution to anomaly detection in multivariate time series data. Such data can be ubiquitously found in real world scenarios and one might be interested in finding any anomaly in this data and its cause. Existing set of solutions either perform separate univariate anomaly detection and ignore the dependences between the various dimensions or a direct multivariate anomaly detection which does capture the inter-dimensional dependencies however is still incapable of learning the underlying data generating process and thus require a separate root cause analysis for anomaly cause detection. The present work attempts to detect anomalies and their cause simultaneously while capturing dependencies between the dimensions of the time series data by factorizing the joint distribution (data) into simpler, local mechanisms which are modular. \n\nOn a high level, the proposed framework, from the training data first generates a causal graph using data appropriate choice of causal graph discovery algorithms (e.g. PC, FCI, GES, etc). The inferred causal graph is then used to factorize the joint distribution using the Markov factorization. These factorizations are conditional probabilities corresponding to local mechanisms, which can be computed using methods such as kernel density estimation. Now for the non-training input (multivariate time series data), the anomaly score is computed as an aggregation of individual p-values, one for each variable and its corresponding mechanism. Anomaly score greater than a certain threshold is considered as anomaly and the variable with highest root cause score is deemed the cause of anomaly. Authors follow existing approaches from Heard & Rubin-Delanchy (2018) compute the anomaly score and ranking based algorithm for root cause score.\n\nThe experiments are thorough but not exhaustive. Experiments with more complex data generating processes are not explored, which is where the causal graph discovery methods are put to test. \n",
            "strength_and_weaknesses": "Strengths:\n\n- Novel framework which can be enhanced with advances in causal discovery methods. \n- Authors provide the required background of causal inference, making this work accessible to readers new to the domain. \n- The manuscript is well-written with minimal to no clerical errors, accompanied with simple examples and plots which makes it a moderately easy read for non-experts. The structure of the paper is good and easy to follow.\n\nWeaknesses: \n\n- Method highly sensitive to causal graph discovery method, which in itself is sensitive to the quality of data (which in this case are anomalies). Some more discussion on this is needed.\n- Proposed approach uses a window of time to extract samples for analysis, this does capture the time dependences between data dimensions however, it cannot capture the temporal dimension beyond the window of time chosen.\n- There are some limitations related to the size of the window and the underlying assumption that the causality is localized within that window. What happens if the data has periodicity? What would be the appropriate window size in that case?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, the quality of the manuscript is high. The experimental procedure is clearly laid out so I am guessing the results could be reproduced.\n\nOn the novelty side, using casual models to explain and detect anomalies is interesting.  ",
            "summary_of_the_review": "This is a well-written paper with some clear advantages in terms of explainability and modeling of anomalies. However, there are limitations in terms of the size of the window, which are not discussed in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1541/Reviewer_8A4e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1541/Reviewer_8A4e"
        ]
    }
]