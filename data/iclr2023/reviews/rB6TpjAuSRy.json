[
    {
        "id": "X_tdlNG-kR",
        "original": null,
        "number": 1,
        "cdate": 1666506786360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666506786360,
        "tmdate": 1666506786360,
        "tddate": null,
        "forum": "rB6TpjAuSRy",
        "replyto": "rB6TpjAuSRy",
        "invitation": "ICLR.cc/2023/Conference/Paper4148/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an image-to-text generation model based on the autoregressive transformer architecture. The videos are flattened into a sequence of spatial-temporal patches, tokenized by a VQVAE, and concatenated with the text input. The authors propose several engineering improvements that lead to more effective training. The resulting CogVideo model outperforms previous methods on multiple benchmarks.",
            "strength_and_weaknesses": "Strengths:\n- Text-to-video generation is an emerging and exciting field. This paper is one of the first work to demonstrate the possibility of such a system.\n- CogVideo achieves good performance on several benchmarks, outperforming previous methods by a large margin.\n- The paper provides sufficient engineering details. The code and model is also open-sourced.\n\nWeaknesses:\n\n- Why must the videos be sampled into a fixed number of frames during training? This fixed-length restriction seems to be the fundamental motivation of the proposed multi-frame-rate training, but the reason of this restriction is not clear. An autoregressive transformer should be able to process variable-length input. I assume that the GPU memory constraint is the main reason? If so, there could exist better solutions as discussed below.\n\n- The proposed method flattens a video as a sequence of spatial-temporal image patches. While this seems to be a straightforward extension from a text-to-image autoregressive model, it incurs a large amount of computation cost. One alternative solution is to encode each frame (instead of each patch) into a latent vector, and uses another model to generate the frame from the latent vector. Could the authors discuss on the pros and cons on these two design choices?\n\n- Is there any positional encoding to indicate the spatial-temporal position of each patch? \n\n- One potential concern for the current model architecture is that the video generation process may rely too much on the video inputs and less on the text, as text only occupies a small part of the input sequence. Have the authors considered using cross-attention on text instead?\n\n- How important is text information for the second interpolation stage? Is it possible to train an interpolation model using only video data?\n\n- The proposed multi-frame-rate training method does not fundamentally address the misalignment between text and video. The misalignment occurs due to the weakly-annotated dataset. CogVideo is still trained on web datasets. Have the authors performed any data cleaning to ensure better quality than Howto100M? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper focuses on engineering improvements (which are valuable), hence the technical novelty is not signifiant. The paper is mostly well-written, with some arguments (e.g., motivation of multi-frame-rate training) not well-supported.",
            "summary_of_the_review": "The biggest value of this paper is that it is one of the first text-to-video generation models with reasonable performance. The proposed engineering improvements could be useful. However, I have some concerns regarding the motivation of the proposed method and its advantage over alternative model designs. Overall, I'm slightly leaning towards acceptance, and may change my score depending on the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_M4hz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_M4hz"
        ]
    },
    {
        "id": "QQ1LdAfCqw",
        "original": null,
        "number": 2,
        "cdate": 1666705250368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666705250368,
        "tmdate": 1666705250368,
        "tddate": null,
        "forum": "rB6TpjAuSRy",
        "replyto": "rB6TpjAuSRy",
        "invitation": "ICLR.cc/2023/Conference/Paper4148/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a large-scale video pretraining network, i.e., CogVideo, for text-to-video generation. The results are plausible. The technical contributions of the paper are 1) Multi-frame-rate training, and 2) dual-channel attention.",
            "strength_and_weaknesses": "Strengths:\n1) The paper presents a large-scale video pertaining network.\n2) The paper is well-written and easy to follow.\n\nWeaknesses:\n1) The results are not of high quality. From the anonymous web demo, the generated videos are blurry and not temporally consistent.\n2) The novelty is limited. The whole architecture is just a combination of two transformers, one for sequential generation and the other for recursive interpolation. Both of them are existing techniques. They are directly adapted from VQVAE-based image generation methods.\n3) The authors employ a temporal channel attention block to handle the temporal coherence. However, there is no ablation study on this. The authors only did ablation studies on fully fine-tuning them, partially fine-tuning them, or training from scratch. One possible ablation study is removing this module.\n4) The comparisons on the UCF-101 and Kinetics-600 datasets are not fairly as the dataset used in CogVideo is significantly larger than others.\n5) The improvements are mainly contributed to the 5.4 million text-video pairs. However, the authors seem not to have a plan to release the dataset. I understand the main focus of this paper is to provide a large-scale pertaining video generation network. However, the performance of this model is far from existing text-to-image models. Therefore, with this model but without data, other researchers still cannot follow this direction.\n6) From Fig. 5, it seems that the results of (a) and (b) are both reasonable.\n7) There is no qualitative comparisons with existing methods in the main paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good.\n\nQuality: Neutral. The performance of the model is not so good.\n\nNovelty: Limited. All of the proposed modules are existing techniques and are not very interesting.\n\nRepoducibility: Neutral. There is open-source code released. But the data is still private.",
            "summary_of_the_review": "The performance of the model is not so impressive. The proposed methods are not so interesting.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_5tA9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_5tA9"
        ]
    },
    {
        "id": "94CxIYcuPDg",
        "original": null,
        "number": 3,
        "cdate": 1666736564705,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666736564705,
        "tmdate": 1666736564705,
        "tddate": null,
        "forum": "rB6TpjAuSRy",
        "replyto": "rB6TpjAuSRy",
        "invitation": "ICLR.cc/2023/Conference/Paper4148/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents an autoregressive model for text-to-video generation. The model is based on a Transformer backbone, pre-trained on a text-to-image task, and has three key components: 1) multi-frame-rate training (i.e. during training, the model observes videos sampled at different frame rates so that it can capture both short and long-term context); 2) a frame interpolation model to insert transition frames; and 3) the use of dual-channel attention (i.e. spatial and temporal attention is done in two separate but parallel layers, and the outputs are interpolated with a learnable parameter). Both the machine evaluation (using the popular FVD and Inception scores) and human evaluation show that the proposed approach produces better quality videos than the considered baselines (7 baselines, several of which published in the current year, 2022, were considered).",
            "strength_and_weaknesses": "**Strengths**\n- The paper presents a thorough evaluation of their model, based on both automatic scoring (using FVD and Inception scores), as well as a human evaluation from 90 anonymous evaluators (that were economically. compensated for their work).\n- Some of the key (according to the authors) components were carefully ablated, i.e. the effectiveness of dual-channel attention.\n- The appendinx contains very useful details about the implementatin and the evaluation of the model. I was able to find the answer to many questions that I had in these, especially regarding human evaluation. Kudos to the authors.\n\n**Weaknesses**\n- Several ways of \"factorizing\" the Temporal and Spatial attention have been presented in the past, but the paper does not compare against these approaches. For example, see the factorized attention in the \"ViViT: A Video Vision Transformer\" paper or in \"VidTr: Video Transformer Without Convolutions\".\n- The effectiveness of the multi-frame-rate training is not ablated in the paper, which is a central contribution of the paper, according to the authors, there's merely a visual inspection of the type of videos generated conditioned on different frame rates (see figure 6).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and the quality and novelty of the work is very high. Regarding reproducibility, it seems that the authors have collected their own dataset, so unless they release it, it's going to be hard to reproduce the results from the paper. The proposed methods however, should be relatively easy to implement and tested in other datasets, given the details provided by the authors.",
            "summary_of_the_review": "I believe that the paper could have good chances to influence the community, except for the fact that several industrial labs have recently published text-to-video image generation models that seem to generate higher quality videos (although this is of course debatable, since there isn't an apples-to-apples comparison available). Regardless of this, I believe that the work is of high quality, the experimentation and evaluation is sound, and it should be accepted to the conference.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_kSBX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_kSBX"
        ]
    },
    {
        "id": "620ftPR1mu",
        "original": null,
        "number": 4,
        "cdate": 1667004598886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667004598886,
        "tmdate": 1668961129814,
        "tddate": null,
        "forum": "rB6TpjAuSRy",
        "replyto": "rB6TpjAuSRy",
        "invitation": "ICLR.cc/2023/Conference/Paper4148/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "CogVideo proposes a combination of models that can generate videos from a given description. The system uses two model that generate frames in a hierarchical way: the first model generates the \"key-frames\" while the other model interpolates between the generated key-frames. The authors also proposed \"a dual-attention mechanism\" which allows for mix video-image training. The model is trained on a non-public dataset of ~5-4M text-video pairs. The authors tested the quality of generated videos on Kinetics-600 and UCF-101 and also provided a human study on videos generated on random classes of UCF.",
            "strength_and_weaknesses": "===== Strengths\n+ The paper is well motivated. The authors enumerate the main issues that make text to video a hard problem and try to address them one by one. \n+ The evaluations are done using both human metrics and machine metrics (FVD).\n+ The authors open sources the code behind the paper which is always a plus.\n+ The authors provided a large number of videos in their demo website which is also appreciated.\n\n===== Weaknesses\n- Weak baselines: The main baselines for the paper (in human evaluation) is TGAN (published in 2017) and VideoGPT (2021). The first one is out-dated at this point and the latter generates videos at a low resolution (64x64 and 128x128) which can affect the human judgement. It's not clear if the authors reduced the resolution of their model for a more fair comparison. I also encourage the authors to provide a similar comparison with newer and higher resolution models such as  NUWA and Video Diffusion.\n- Other problems: following the previous weakness and given the fact that there are not that many text conditional video generation models out there, I suggest authors to compare their model with in other video problems (such as frame conditional video prediction) where there are more baselines to compare to.\n- Out of distribution results: While the videos generated by the model look impressive, it is not clear how the model performs for out of distribution queries. The paper only contains two \"new compositions\" that probably don't exist in the training set: \"A lion man is drinking water\" and \"A tiger is playing football\" while the rest of the examples probably exist in the training set. The demo website contains more such new compositions however the generated videos are not generate suggesting that the model is overfitted to the training data and is just playing videos from memory! Unfortunately, the training data is also private which makes such analysis even harder but I suggest authors to perform an analysis on this matter by generating examples which are more detailed and most likely non-existent in the training set (e.g. \"a cat in red jacket and blue sunglasses is drinking tea from a bowl\").\n- Reproducibility: While the authors provided the source code, the paper lacks details of the model and the used data is private which severely affects the reproducibility. The authors also did not provide any numbers that indicate the required computation to train a 9B model. This is interesting given the fact that the paper mentions computational cost as one of the main issues in text to video.\n- Writing. The quality of the writing can be improved. Particularly, I found the Section 3.2. to be poorly written, assuming that the reader is familiar with details of the previous work CogView2 and CogLM.\n \n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is mostly clear however some sections can be improved such as 3.2.\n- Quality: The quality of the provided results are great however it's not clear how the model will perform on new combinations.\n- Novelty: the provided method, particularly fps conditioning is new to the best of my knowledge.\n- Reproducibility: While the authors provided the source code, the paper lacks details of the model and the used data is private which severely affects the reproducibility.",
            "summary_of_the_review": "Overall, while the provided results look good, more analysis and comparison is required to show case the strengths and limitations of the model. Given more analysis the paper should have more impact on the field.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_9Xh6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_9Xh6"
        ]
    },
    {
        "id": "hDc074yVHC",
        "original": null,
        "number": 5,
        "cdate": 1667330213761,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667330213761,
        "tmdate": 1670352575479,
        "tddate": null,
        "forum": "rB6TpjAuSRy",
        "replyto": "rB6TpjAuSRy",
        "invitation": "ICLR.cc/2023/Conference/Paper4148/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces an extension to CogView2, a text to image model, to include a temporal dimension that allows for the generation of short video clips. The authors. The authors extend the architecture by including a temporal attention block in parallel to the spatial attention block pretrained on text->image data. Quantitative results are overall quite good and the human evaluation is very favourable. However, the paper misses some important comparisons and ablations. ",
            "strength_and_weaknesses": "## Strengths\n* Solid qualitative results\n* Human evaluation\n* Good approach aiming at reusing an existing image-to-text model and thereby reutilizing spent compute.\n* Simple extension to existing architecture\n* Good presentation \n\n## Weaknesses\n* Qualitatively the videos show strong artifacts between frames, i.e., unnatural changes between frames. The dynamics are somewhat natural but the consistency in color and form is not very good. This is likely due to the strong prior of the model to draw individual frames (images) because of pretraining. With longer training this might become more natural. Maybe it could even be enforced more strongly. The authors do not address this. \n* The model is still clearly limited in its ability to generate natural videos but the authors are not exploring failure cases at all. A chapter or paragraph on limitations would be great.\n* No ablation on impact of model and (pretraining) data size.\n* Lacking comparisons:\n  * No details about how the data was crawled.\n  * No comparisons in terms of data (quality/scale) with similar works like Ho et al\n  * No comparison to Ho et al on UCF101\n  * No human evaluation against Ho et al. which seems comparable in quality\n* Seems worse on some benchmarks to more recent work even though these do not have extra pretraining data on text-video.\n* Hard to pin-point where improvements come from. When comparing to prior work it is hard to figure out where the main gains come from. Is it the new pretraining dataset, is it the pretrained image-text model, is it the modeling innovations in this paper.\n\n## Questions & Comments\n* The finding of FVD on kinetics is interesting! It would be interesting to see the FVD of the reconstruction on the original as well. Could it be that VQ VAE used in this work is not working well?\n* From scratch training better than naive fine-tuning? That\u2019s odd. \n* Multi-frame rate training evaluation would be better with some quantification. Maybe one could look at perplexities when conditioning on a frame rate by using another ground-truth frame rate and look at the perplexity to see whether the models conditioning is really working.\n* Is alpha really important? Why not just taking the sum instead? I strongly suspect it doesn\u2019t matter.\n* Time and spatial subsampling is not completely new but was also used for instance in Weissenborn et al 2019. The adaptivity by conditioning is novel though.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and the authors plan to open source the code. Documentation on the creation of the dataset would be helpful, and maybe the dataset itself. Otherwise it will be hard to reproduce. The paper's novelty mainly lies in the scale of the model, the reutilization of the image-text model and the pretraining data. The architectural changes are rather straightforward (good!) but have been introduced in similar form in other works.",
            "summary_of_the_review": "The paper is a well written and has solid results. It is definitely a step in the right direction but leaves some open questions and seems to miss some important comparisons. I am nevertheless leaning towards accept.\n\nUPDATE: I read the rebuttal and am mostly happy with it. I think this paper deserves to be presented at ICLR. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "The paper relies on a new video-text dataset and the authors do not describe the data collection process in detail so it is hard to judge. The authors seem aware of ethical issues and point to potential solutions, only some of which seemed to have been implemented when collecting the data. ",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_Tzyn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4148/Reviewer_Tzyn"
        ]
    }
]