[
    {
        "id": "gpJCYLB8OD",
        "original": null,
        "number": 1,
        "cdate": 1666323927761,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666323927761,
        "tmdate": 1666323927761,
        "tddate": null,
        "forum": "zZhX4eYNeeh",
        "replyto": "zZhX4eYNeeh",
        "invitation": "ICLR.cc/2023/Conference/Paper3200/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studied the Constrained MDP (CMDP) problem, under the infinite-horizon average-reward setup of linear MDP. The model-free algorithm proposed in this paper has better regret bound in terms of total steps $T$ compared with the previous algorithm for tabular CMDP, under weaker assumptions. Under the same assumption as the tabular CMDP algorithm, the authors show the algorithm can achieve smaller regret with no violation of the constraint. The second result in this paper is a policy-based algorithm that works under less realistic assumptions: uniform ergodicity and explorative policies. This algorithm takes a different approach and can achieve regret and violation of order $\\sqrt{T}$.",
            "strength_and_weaknesses": "Strength:\n1. The performance guarantee is significantly better than previous works. Compared with previous works for tabular CMDP, this paper works under weaker assumptions and has better upper bounds for regret and violation.\n2. Linear function approximation is a highly noted topic in RL theory community. The constrained MDP problem is a meaningful extension and can be interesting to many.\n3. The paper is clearly written and easy to follow.\n\nWeakness:\n\n1. Compared with Ghosh et al. (2022) and Wei et al. (2021a), the technical contribution is not significant. The regret analysis relies on first reducing the infinite-horizon average reward regret to the regret of finite horizons, the same as Wei et al. (2021a). Then, results for episodic CMDP are invoked from Ghosh et al. (2022) to bound the finite-horizon regret. This makes the novelty of Theorem 2 not significant enough. While the difference from Ghosh et al. (2022) is discussed it just remains superficial: the paper claims a $\\sqrt{H}$ improvement, which is no surprise because it studies the homogeneous setting.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and I believe it is technically correct. As mentioned in the weakness, the technical novelty can be questionable given the two papers Ghosh et al. (2022) and Wei et al. (2021a). The former deals with episodic CMDP; the latter deals with infinite-horizon average-reward linear MDP.",
            "summary_of_the_review": "This is a technically clear and rigorous paper. The novelty and significance are slightly decreased due to the existence of two previous works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3200/Reviewer_ob3W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3200/Reviewer_ob3W"
        ]
    },
    {
        "id": "rEAZkynppUI",
        "original": null,
        "number": 2,
        "cdate": 1666590773952,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590773952,
        "tmdate": 1669692733668,
        "tddate": null,
        "forum": "zZhX4eYNeeh",
        "replyto": "zZhX4eYNeeh",
        "invitation": "ICLR.cc/2023/Conference/Paper3200/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes two approach, optimism-based algorithm and policy-based algorithm for infinite-horizon average reward constrained linear MDP under different assumptions and achieve better convergence upper bounds. ",
            "strength_and_weaknesses": "Strength.\n\nThis paper is sufficiently complete. It proposes new algorithms, builds a theoretically-improved convergence analysis on both regret and violation bounds, and empirically verifies the result on stochastic environments. And this work is purely model-free; it doesn't require any estimation of transition kernel.\n\nWeaknesses.\n\nI didn't find any major weaknesses.  ",
            "clarity,_quality,_novelty_and_reproducibility": "All statements are clear and well-supported. The whole paper is well-written. Main results are not very interesting but are the first time to be proposed and proved.",
            "summary_of_the_review": "I recommend to accept this paper since the author has presented a complete result, including new algorithm designs, theoretical analysis, and empirical studies. And these results are new.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3200/Reviewer_3YVd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3200/Reviewer_3YVd"
        ]
    },
    {
        "id": "IevM--xOX43",
        "original": null,
        "number": 3,
        "cdate": 1666676723623,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676723623,
        "tmdate": 1666676789390,
        "tddate": null,
        "forum": "zZhX4eYNeeh",
        "replyto": "zZhX4eYNeeh",
        "invitation": "ICLR.cc/2023/Conference/Paper3200/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the infinite horizon average reward constrained Markov Decision Process (CMDP) for the model-free linear CMDP setup. The paper proposes 3 different algorithms under different assumptions or computation efficiency. The first algorithm is based on fixed-point optimization with optimism, which is to solve a very complicated optimism under uncertainty problem for every time step, which as the author/s pointed out, is very inefficient, but enjoys the \\tilt{O}(\\sqrt{T}) bound guarantee for both the regret and constraint violation. The second algorithm is computationally efficient but has worse bound of \\tilt{O}(T)^{3/4}, which is an extension of the previous work LSVI-UCB algorithm from the finite-horizon setting to the infinite one with additional analysis on the horizon truncation impact on the regret and constraint violation. The third algorithm, a primal-dual adaptation of previous work's algorithm MDP-EXP2, has stronger assumptions than the first two algorithms, which is both efficient and having \\tilt{O}(\\sqrt{T}) bound guarantee for both the regret and constraint violation.",
            "strength_and_weaknesses": "Strength:\n  1. The problem setup considered in this paper is infinite horizon average reward CMDP with linear function approximation, which is meaningful and interesting.\n  2. The theoretical results in both regret and constraint violation have improvements over existing works in the above mentioned specific problem domain. For example, the efficient algorithm 2 has better bounds of \\tilt{O}(T)^{3/4 compared with known one.\n  3. The paper is well-written and easy to read.\n\nWeakness:\n  1. The algorithm 2 and 3 are direct extensions of the existing algorithms in the literature with incremental efforts on having improved results in both regret and constraint violation bounds.\n  2. For the algorithm 2, the main difference is to show how to bound the gap between the best finite horizon policy and the infinite horizon case, which is obtained by Lemma 3 and its proof in the paper. The additional effort looks very incremental.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please check my comments above.",
            "summary_of_the_review": "As I commented above, I think the paper tries to solve a meaningful problem in the RL area. But all the proposed efficient algorithms are heavily dependent on previous works and the theoretical results are incremental as well. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3200/Reviewer_2G2e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3200/Reviewer_2G2e"
        ]
    },
    {
        "id": "KW_Q9Onkgkr",
        "original": null,
        "number": 4,
        "cdate": 1667468035364,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667468035364,
        "tmdate": 1670920637486,
        "tddate": null,
        "forum": "zZhX4eYNeeh",
        "replyto": "zZhX4eYNeeh",
        "invitation": "ICLR.cc/2023/Conference/Paper3200/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the model-free linear constrained Markov Decision Process with infinite horizon average rewards. The goal is to learn a policy for selecting actions that minimize the regret (the difference between the maximum achievable average reward and the policy's total average reward) while keeping the total constraint violations as low as possible.\n\nThe authors propose four different algorithms (the last two are policy-based algorithms) and show their sub-linear regret and constraint violation upper bounds. ",
            "strength_and_weaknesses": "**Strengths of paper:**\n1. Authors first propose an algorithm with $O(\\sqrt{d^3T})$ regret and constraint violation upper bounds, which has the tightest regret upper bound, but the algorithm is computationally inefficient. So they propose a computationally efficient algorithm that uses the primal-duel adaption of the LSVI-UCB algorithm but has $O((dT)^{3/4})$ regret constraint violation upper bounds. \n\n2. Under strong assumptions (Assumption 4 and Assumption 5), authors propose policy-based algorithms that are computationally efficient and have $O(\\sqrt{T})$ regret constraint violation upper bounds (or even constant constraint violation bound). \n\n3. No simulator (a common assumption) is needed to show the proposed algorithms' bounds.\n\n**Weakness of paper:**\n1. The assumptions for policy-based algorithms are too strong, which can limit their practical use. \n\n2. To show any algorithm to be \"provably sample-efficient,\" we need to have a lower bound. I didn't find any discussion or comparison with the lower bound in the main paper.\n\n3. It is unclear when the primal-dual algorithm and fixed episode length (to deal with infinite horizon) are used; then, the learned policy is also optimal. If the learned policy is sub-optimal, it can lead to linear regret and constraint violations.\n\n4. Empirical evaluations are weak. Even regret shown in Figure 1 (left) looks linear. \n\n\n\n**Question and other comments.** \n\nPlease address the above weaknesses. I have a few more questions:\n1. Would you give a few motivating examples (with what reward and utility functions) for your problem setting?\n2. What is $\\pi_1$ in Algorithm 1?\n\n\nA minor comment:\n1. Page 3, second last paragraph, second line:$j \\rightarrow \\diamond$.\n\n\nI am open to changing my score based on the authors' responses.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\n The paper is well-organized and clearly written.\n\n**Quality:** \nThe paper appears to be technically sound. The proofs appear to be correct, but I have\nnot carefully check all details.\n\n**Novelty:** \nThis paper makes non-trivial advances over the current state-of-the-art.\n\n**Reproducibility:** \nThe code is unavailable, which makes it difficult to reproduce the empirical results. However, sufficient details are given to reproduce the main theoretical results.",
            "summary_of_the_review": "This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Since this work is a theoretical paper, I do not find any ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3200/Reviewer_65aT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3200/Reviewer_65aT"
        ]
    }
]