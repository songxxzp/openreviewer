[
    {
        "id": "fjOZQ4WSYUa",
        "original": null,
        "number": 1,
        "cdate": 1666119129550,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666119129550,
        "tmdate": 1670551490234,
        "tddate": null,
        "forum": "_5sMa2sdU4",
        "replyto": "_5sMa2sdU4",
        "invitation": "ICLR.cc/2023/Conference/Paper4772/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an interesting solution for overcoming the data heterogeneity of FedAvg algorithm. The theoretical result indicates that for a specific structure of neural network, FedAvg can achieve nearly zero loss at a linear convergence rate without making any additional assumptions on data distribution. The empirical result further validates this result. ",
            "strength_and_weaknesses": "***Strength***\n* It is the first convergence analysis of FedAvg over multi-layer neural network. And it is interesting to see it achieves linear convergence rate without additional requirements on data distribution.  \n\n***Weaknesses***\n* I feel that the linear convergence of overparameterized model in the federated learning scenario is trivial. See the following two references. The data heterogeneity is characterized by the gap between global minimum and average of client minimum [Li2019] or the gradient noise at the global minimizer [Khaled2019]. For overparameterized model, both of those value are zero. It generally says that for overparameterized model there is no \"data heterogeneity\". I would be expecting to hear how data heterogeneity is defined in this paper and how it is different from those existing definitions. \n\n*[Li2019] Li, X., Huang, K., Yang, W., Wang, S., & Zhang, Z. (2019, September). On the Convergence of FedAvg on Non-IID Data. In International Conference on Learning Representations.*\n\n*[Khaled2019] Khaled, A., Mishchenko, K., & Richt\u00e1rik, P. (2019). First analysis of local gd on heterogeneous data. arXiv preprint arXiv:1909.04715.*\n\n\n* I agree that Theorem 1 indicates that FedAvg-SG attains a linear rate. But according to Remark 8 in the appendix, it appears that the choice of learning rate is very sensitive, it must be of the order $\\Theta(\\epsilon^2)$. If it is smaller, for example, $\\eta =  \\Theta(\\epsilon^3)$, then the rate cannot be linear; and if $\\eta =  \\Theta(\\epsilon)$, the convergence is not guaranteed. I don't think this result is satisfactory. For other research on overparameterized models, the following two references [Du2018] and [Ma2018] show the linear convergence rate can be achieved with a step size independent w.r.t. $\\epsilon$. It is not clearly clarified in this paper why there is such uncommon constraints on the learning rate.\n\n*[Du2018] Du, S. S., Zhai, X., Poczos, B., & Singh, A. (2018). Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054.*\n\n*[Ma2018] Ma, S., Bassily, R., & Belkin, M. (2018, July). The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. In International Conference on Machine Learning (pp. 3325-3334). PMLR.* \n\n\n===================================\n\nUpdates after Rebuttal:\n\nI have read other reviews and the author's responses. I still tend to reject this paper. (1) As stated by the author in the rebuttal, this work uses a different definition of the data heterogeneity from [Li2019] and [Khaled2019]. So I don't think this result solves the standard heterogeneity issue in the federated learning. (2) Also, compared to prior over-parameterized model studies, this paper has non-standard constraints on the learning rate. I agree that multiple local updates will make -level learning rate invalid but it makes this paper meaningless: I can always run one-step local update at each edge device with a constant learning rate then applying [Du2018] and [Ma2018] to standard SGD with mini-batches; it also has a linear convergence rate. (3) Besides both above, I agree with Reviewer Er5f: there exists solid theoretical results showing that the minibatch SGD is always not worse than the federated averaging approach, so the motivation of sololy studying FedAvg is weak.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has high quality regarding its writing: it clearly defines every symbol, which is not easy especially when notations are very complicated; and there is an easy-to-understand proof sketch. But the contribution of this work might be limited as described in the weakness section. ",
            "summary_of_the_review": "This paper fills in the blank of applying overparameterized model to FedAvg algorithms and achieves the expected result: linear convergence rate. But I have two major concerns: (1) the unclear definition of data heterogeneity; this paper may fall into an existing scoop of federated learning research. (2) the uncommon constraint on the step size; the theoretical result doesn't match some existing overparameterized models research.  From these two reasons, I tend to reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4772/Reviewer_vggk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4772/Reviewer_vggk"
        ]
    },
    {
        "id": "b5VJkecblJF",
        "original": null,
        "number": 2,
        "cdate": 1666328234270,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666328234270,
        "tmdate": 1666328234270,
        "tddate": null,
        "forum": "_5sMa2sdU4",
        "replyto": "_5sMa2sdU4",
        "invitation": "ICLR.cc/2023/Conference/Paper4772/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proves under certain assumptions, FedAvg can converge linearly to the optimal solution even under heterogeneous clients. The convergence relies on specific network architectures and parameter initialization.",
            "strength_and_weaknesses": "Strength:\n1. A global and linear convergence result for FedAvg. In this sense, the result is new and interesting.\n\nWeakness:\n1. Compared with the result for centralized training (Nguyen and Mondelli, 2020), the technical contribution of this paper is questionable. The network architecture, the initialization, as well as most of the proof, are from Nguyen and Mondelli (2020). While in Remark 5, the technical novelty compared with it is discussed, either extending to federated training or dealing with stochastic gradient is not significant or novel enough.\n\n2. The analysis of federated training is likely to be superficial or unrealistic in that, the total update length within each communication round, decreases as the local training steps increases. In particular, the step size $\\eta \\asymp A^{-r}$ ($A > 1$) where $r$ is the number of local steps so in total, $\\eta r$ decreases exponentially. For one thing, this is never the real world case. Usually, each local client will train for several steps with constant step size and then aggregate on the server side. More locally training steps makes the averaging step harder, while in this paper's configuration, more local steps mean the total change of the parameter becomes even smaller, thus averaging also become easier. This lead to the theoretical aspect: this kind of configuration makes the total local updates within $r$ steps so small, that the first-order approximation is precise enough. This makes the $r$-step update FedAvg no harder than the $1$-step update FedSGD. It is then trivial to show that FedSGD indeed can handle arbitrary heterogeneity in the clients.\n\n3. The presentation needs further polishing. I found several typos and notation inconsistencies in the equations. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper should provide a more clear discussion on what is the technical contribution compared with previous works, Nguyen and Mondelli (2020). Currently, I have the impression that many contents in this paper highly overlap that of Nguyen and Mondelli (2020), including the network architecture, the initialization, and part of the proof. \n\nThe key weakness of the FedAvg setting is that the learning rate is set so that more local steps make the first-order approximation easier rather than harder. Thus the key difficulty of analyzing FedAvg, namely the averaging part, is circumvented. I think to make the analysis meaningful or significant enough, at least the total update $\\eta \\cdot r$ should be of the order constant.\n\nAlong with Nguyen and Mondelli (2020), the significance or novelty of these results is questionable.\n",
            "summary_of_the_review": "Due to the lack of technical novelty, I recommend rejection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4772/Reviewer_oX5J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4772/Reviewer_oX5J"
        ]
    },
    {
        "id": "m0kuj1HQmW",
        "original": null,
        "number": 3,
        "cdate": 1666379574143,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666379574143,
        "tmdate": 1666379574143,
        "tddate": null,
        "forum": "_5sMa2sdU4",
        "replyto": "_5sMa2sdU4",
        "invitation": "ICLR.cc/2023/Conference/Paper4772/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of federated learning when optimizing with overparameterized multi-layer neural networks. More specifically, the author shows that with a specialized initialization, FedAvg has a linear convergence rate when solving the federated learning problem with overparameterized multi-layer neural networks.",
            "strength_and_weaknesses": "The strength of the paper:\n1. It shows a linear convergence rate of FedAvg under certain conditions.\n2. Experiments validate the effectiveness of theoretical guarantees.\n\nThe Weaknesses of the paper:\n1. The presentation is not clear. There are lots of unclear definitions, which makes the paper hard to follow. (see detailed comments in next section)\n2. Assumption 3 seems to be a strong assumption on the activation functions. What are specific examples of this kind of activation function?\n3. It is unclear whether FedAvg is better than the naive mini-batch SGD baseline in this setting. For example, if we do not perform local updates and instead compute $r$ stochastic gradients at the same point and send the mini-batch gradients to the server and then update the model, whether FedAvg can outperform this mini-batch SGD baseline? According to Lemma 1, such a procedure can reduce $\\rho$, and it will also avoid the error introduced by local updates as in Lemma 2.\n4. The paper only shows the convergence guarantee of FedAvg. Therefore, it is unclear the generalization performance of FedAvg using overparameterized multi-layer neural networks. Note that most of the existing works consider a stochastic problem, and thus their convergence guarantees of FedAvg can serve as the generalization guarantees.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the current work needs to be improved:\n1. In equations (2) and (3), what is the dimension of $W_l$ and $\\theta$? In addition, what do you mean by $f_{L,k}(\\theta)$? Do you mean is a function of just the output is dependent on $\\theta$? Why do you have $F$ norm in equation (3)?\n2. In definition 1, change $x$ to $\\theta$.\n3. Regarding Assumption 2, is the requirement for each client's model or the stacked model?\n4. Why the condition in (7) depends on both $\\theta_1$ and $\\theta_2$ instead of just $\\theta$?\n5. There are lots of works that also make no assumptions on the data heterogeneity, e.g., Karimireddy 2020b, the authors should also discuss it.",
            "summary_of_the_review": "The problem, i.e., how the overparameterization will affect the performance of FedAvg, considered in this paper seems to be interesting. However, the results provided in the current paper are not strong enough. The main question I have is whether the FedAvg can outperform the naive mini-batch baseline in this setting. In addition, what is the generalization guarantee of the proposed method?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4772/Reviewer_Er5f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4772/Reviewer_Er5f"
        ]
    },
    {
        "id": "bn6ptYTYah",
        "original": null,
        "number": 4,
        "cdate": 1666894567088,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666894567088,
        "tmdate": 1666894567088,
        "tddate": null,
        "forum": "_5sMa2sdU4",
        "replyto": "_5sMa2sdU4",
        "invitation": "ICLR.cc/2023/Conference/Paper4772/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the convergence performance of FedAvg for training (pyramidal topology) overparameterized deep networks. Specifically, under certain assumptions, the authors design a special initialization strategy for FedAvg and prove linear convergence rate of FedAvg on training such overparameterized models without making assumptions on data heterogeneity. The authors also provide experimental results on MNIST and CIFAR10 under different data heterogeneous settings to study the effect of model size in FedAvg training.",
            "strength_and_weaknesses": "Strength:\n\n1. Given FedAvg is a widely used algorithm in FL and the interpolation regime is an interesting regime in FL. Built on the previous results on overparameterized models with pyramidal topology [NM2020], this paper makes important contributions to provide linear convergence rates of FedAvg in this setting.\n\n2. The theoretical results are derived without assuming heterogeneity conditions.\n\nWeaknesses:\n\n1. The setup of experiments in Section 5 do not align with the theoretical results. I agree it is interesting to investigate the convergence performance of FedAvg under the settings in Sec 5. I would like to see the empirical performance of FedAvg with the specified initialization strategy for solving a problem where all the assumptions (network architecture and activation), which could better corroborate the theoretical results. Moreover, this could be helpful for examining the tightness of the convergence analysis (e.g., the theoretical linear convergence rate v.s. the actual linear convergence rate).\n\n2. [minor] The non-iid setting considered in Sec 5 is not very challenging, it might be interesting to study the performance of FedAvg under more extreme data heterogeneity conditions, e.g., settings in Table III in [LDC+2021].\n\nQuestions:\n\n1. The training loss is pretty large in Figure 2 and Figure 3, is the training loss in those plot the summed loss or averaged loss? Also, it might be helpful to visualize the training loss in log scale to study the linear convergence of FedAvg.\n\n\nReferences\n\n[NM2020] Global convergence of deep networks with one wide layer followed by pyramidal topology. Q. Nguyen and M. Mondelli. NeurIPS 2020.\n\n[LDC+2021] Federated Learning on Non-IID Data Silos: An Experimental Study. Qinbin Li, Yiqun Diao, Quan Chen, Bingsheng He. https://arxiv.org/abs/2102.02079. ",
            "clarity,_quality,_novelty_and_reproducibility": "(Clarity) This work is well presented.\n\n(Quality) High quality work, makes important contributions.\n\n(Novelty) Novel.\n\n(Reproducibility) Good.\n\n\n===============================================================\n\nTypo: In Remark 1, 'Assumption 2 is required to establish a PL like property' -> 'Assumption 3 is required to establish a PL like property'.",
            "summary_of_the_review": "This paper studies an important problem in FL optimization (convergence rate of FedAvg under overparameterized regime) and obtains interesting theoretical results, I would recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4772/Reviewer_3BVM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4772/Reviewer_3BVM"
        ]
    }
]