[
    {
        "id": "IRPdz3ndsf",
        "original": null,
        "number": 1,
        "cdate": 1666605134936,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605134936,
        "tmdate": 1666605134936,
        "tddate": null,
        "forum": "zYWtq_HUCoi",
        "replyto": "zYWtq_HUCoi",
        "invitation": "ICLR.cc/2023/Conference/Paper2617/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on unstructured sparsity of vision transformer. The paper proposes a second-order pruner called oViT, and a set of general sparse fine-tuning recipes. The proposed oViT allows pruning in a single run. The experiments on ImageNet-1K show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n- The paper is well organized and easy to follow.\n- The observation on vision transformer pruning is interesting and provides much insight.\n- The proposed correlation-aware second-order pruning is reasonable.\n\nWeaknesses or questions:\n- Optimal Brain Surgeon [1] is a widely-used second-order technology and has been used in neural network pruning [2,3,4]. This paper is an extension of OBS for vision transformer. The novelty is limited. What's new in the proposed method?\n- Could the proposed method be applied on CNN pruning?\n- The proposed finetuing recipe is a set of tricks, which is more like engineering contribution.\n- The proposed finetuing recipe includes learning rate schedule, regularization, augmentation and an efficient pipeline for sparsity sweeps, The ablation study of these components are not fully conducted.\n\n[1] LeCun Y, Denker J, Solla S. Optimal brain damage[J]. Advances in neural information processing systems, 1989, 2.\n\n[2] Hassibi B, Stork D. Second order derivatives for network pruning: Optimal brain surgeon[J]. Advances in neural information processing systems, 1992, 5.\n\n[3] Hassibi B, Stork D G, Wolff G J. Optimal brain surgeon and general network pruning[C]//IEEE international conference on neural networks. IEEE, 1993: 293-299.\n\n[4] Dong X, Chen S, Pan S. Learning to prune deep neural networks via layer-wise optimal brain surgeon[J]. Advances in Neural Information Processing Systems, 2017, 30.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and easy to follow. The novelty should be clarified, compared to the existing OBS based pruning methods. The code is provided which makes the paper easy to reproduce.",
            "summary_of_the_review": "The paper is well written but the novelty should be further clarified, compared to the existing OBS based pruning methods. The experiments on finetuning recipe should be more comprehensive.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2617/Reviewer_rP9A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2617/Reviewer_rP9A"
        ]
    },
    {
        "id": "cXRqrLbdA3N",
        "original": null,
        "number": 2,
        "cdate": 1666656266719,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656266719,
        "tmdate": 1666656266719,
        "tddate": null,
        "forum": "zYWtq_HUCoi",
        "replyto": "zYWtq_HUCoi",
        "invitation": "ICLR.cc/2023/Conference/Paper2617/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Paper proposes a method to prune neural networks. Most experiment are shown for Transformers, results on CNN are presented in the Appendix. The method is based on second order statistics, Hessian. The method is inspired by Optimal Brain Surgeon (1993) and is the modification of it. Results are presented on Imagenet image classification. \n\n",
            "strength_and_weaknesses": "Strengths:\n\n- Transformers are popular and pruning them is important. The proposed method is generic and it is not clear how it is specific to transformers. \n- Working with saliency metric other than the magnitude is a promising direction. Specially because weight magnitude does not scale for global pruning and requires sensitivity analysis. \n- Taylor decomposition based techniques are demonstrated to be better for CNNs, it is good to see this works for transformers as well.\n- Proposed method computed saliency and the required weight update. \n\nWeakness/questions:\n\n- What is exact pruning setup? Is it global unstructured sparsity or uniform unstructured sparsity? For the the former one it is clear that GM will not perform well as weights are not scaled with respect to the layer number. \n- With all approximation, what is the final equation to compute saliency? authors mention eq 2 in Algorithm 1, but this is the start point not oViT. Sharing code will help to understand the algorithm. \n- In the paper authors state and try to answer why pruning transformers is hard. However, pruning of ViT should not be harder as transformers are overparametrized. [1] shows that Diet pruned for structured sparsity of 50% maintains accuracy. For N:M sparsity, for example 2:4, pruning with magnitude gives no accuracy drop. \n- Finetuning is an important aspect of the pruning method. Authors tune the recipe, what happens if the standard recipe is used.\n- Method is based on the Taylor expansion of the the loss function. The method is inspired by OBS and therefore should be compared directly even if compute time is significant. Having comparison with optimal brain damage will be helpful as well. \n- How would the method perform if we use diagonal approximation of the Hessian: $\\rho = w_i^2*\\Delta L(w_i)^2$ ? This is another popular saliency metric for pruning. \n- Transferability of the concluded insights: the experiments are conducted on classification tasks only. While the observations on different tasks maybe different, thus I am not sure whether the extracted insights can be transferred to other tasks or it is only a not general conclusion for classification tasks.\n- What is special in the proposed method for transformers? The method seem to be general but title specifically days \"for vision transformers\"\n- Minor: LeCun proposed optimal brain damage (OBD) while Hassibi proposed optimal brain surgeon. \n\n\n[1] Yang, Huanrui, et al. \"Nvit: Vision transformer compression and parameter redistribution.\" arXiv preprint arXiv:2110.04869 (2021).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - good, quality - good, novelty - good.",
            "summary_of_the_review": "Paper proposes a general framework for pruning neural networks. It is based on iterative saliency pruning. Novelty of the method is in the computation of the saliency metric that is based on Hessian inverse and a modification of OBS (1993). In my understanding, the paper proposes a way to simplify computation of the Hessian inverse, therefore novelty is questionable. Paper lacks comparisons with simpler techniques based on first order Taylor expansion methods that are simpler (gradient times weight), as well as full versions of OBS and OBD. Finally, the method is not specific to Transformers and more comparisons are required to the SOTA in CNN pruning. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2617/Reviewer_rRsE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2617/Reviewer_rRsE"
        ]
    },
    {
        "id": "ycvUqVLx6jK",
        "original": null,
        "number": 3,
        "cdate": 1666691010635,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691010635,
        "tmdate": 1669104102154,
        "tddate": null,
        "forum": "zYWtq_HUCoi",
        "replyto": "zYWtq_HUCoi",
        "invitation": "ICLR.cc/2023/Conference/Paper2617/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new weight sparsification for vision Transformer. It considers second-order information to recognize redundant weights. By investigating various strategies, the proposed method can acheive high sparsity levels with low impact on accuracy.",
            "strength_and_weaknesses": "\nStrength:\n\n-The authors conduct extensive experiments to validate the effectiveness of this method. The experiments are conducted with multiple backbones including ViT, XCiT, Swin-Transformer, e.t.c.\n\n-This paper is well-written and easy to follow.\n\nConcerns:\n\n-Using second-order information to prune neural network is a common strategy. It has been widely used in spare convolutional neural networks. The work brings these methods to vision transformer. Could the second-order method for CNN be used in vision transformer? The authors are required to clarify the contribution and novelty about the method transferring.\n\n- It is interesting that the unstructured pruning can achieve practical acceleration. However, only the speed of proposed method is shown in Table 5. How about the competeting method? Compared with other methods, could the proposed still show superiority with similar practical speeds?  Besides, it is better to illustrate how to achieve practical acceleration on the a CPU platform more detailedly.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "See Strength And Weaknesses",
            "summary_of_the_review": "See Strength And Weaknesses",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2617/Reviewer_TCn5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2617/Reviewer_TCn5"
        ]
    }
]