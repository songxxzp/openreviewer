[
    {
        "id": "KTl8ftUh_u",
        "original": null,
        "number": 1,
        "cdate": 1666545105222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666545105222,
        "tmdate": 1666545162409,
        "tddate": null,
        "forum": "5R96mIU85IW",
        "replyto": "5R96mIU85IW",
        "invitation": "ICLR.cc/2023/Conference/Paper4986/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper uses several ideas to improve on the privacy-utility trade-off of DP-SGD using public data, beyond the traditional approach of pre-training on public data:\n1) use data augmentation to pre-train on augmented data (setting called \"warm\" in paper)\n2) include public data in training data (setting \"warm-aug\")\n3) at each DP-SGD gradient calculation step, before clipping and noise addition, recenter the gradient vector using a gradient estimate learnt from public data (technique called DOPE-SGD in paper)\n4) ensembling (I seem to understand that means \"using majority voting over n checkpoints\" on CIFAR-10 experiments)\n\n\nExperiments mainly on CIFAR-10, but also on language modelling are shown. Ablation and analytical experiments are conducted. The combination of methods improves on the state of the art.",
            "strength_and_weaknesses": "Strengths:\n- convincing demonstration of superiority\n- good set of experiments\n\nWeaknesses:\n- exposition unclear, verging on obscure\n- experimental settings insufficiently documented\n",
            "clarity,_quality,_novelty_and_reproducibility": "#Quality\n\nThe paper describes, justifies, implements and evaluates idea 3 thoroughly. Research questions are well chosen. The contribution is relevant to current research and is useful to practitioners. \n\n#Clarity\n\nThe paper is unclear and does not explain experimental settings well at all. Here are questions that it leaves unanswered.\n- does setting \"warm\" table 1 undo the augmentation applied in warm-aug ?\n- which hyperparameters are fine-tuned in each experiment (cf. claim in section Settings)\n- the text seems to use \"generated data\", \"synthesized data\", \"augmentation\" interchangeably in various places, which is confusing, but doesn't say this explicitly -- is this correct?\n- does setting \"extended\" use augmentation or not?\n- which ensembling technique is applied in table 1 experiments?\n- how does majority voting apply to language modelling? It makes sense in classification, but in language modelling do you apply renormalization after probability averaging?\n- does table 2 apply ensembling for LM ? It seems to apply it to CIFAR-10 since I see 75.1% like in table 1. \n- what does Fed-SGD refer to exactly? maybe cite the work which is followed\n- table 3, are \"methods\" respectively Amid+ 2021 and Li+ 2022?\n- table 3, does Augmentation cover \"warm-aug\" or anything else too? I would recommend using a codename for settings, like warm-aug, defining it in one place, and then avoiding the use of periphrases to refer to it. \n- where is out of distribution data used at all? why is it defined in section 4?\n- in the Abstract, where does the number 68% come from? Is it the 68.1% in table 2 ?\n\n#Originality\n\nUsing the numbering from my summary, ideas 3 and 4 seem novel, especially idea 3. It is most similar to Li+ 2022 where gradients are re-scaled instead of re-centered. Idea 1 is the standard way of using public data, idea 2 (data augmentation) seems straightforward, I find it hard to qualify it as novel. Idea 4 is surely a good idea that deserves being applied systematically, but is known to apply quite generally.\n\n\n#Typos and errors\n\n- Table 5, column eps=2, 40k: the figure should be 75.1% I think?\n- conclusion, first sentence is ungrammatical\n- \"why this method works\" is vague\n- fig 2 caption: \"the noise added by the Gaussian mechanism\": do you refer to the Gaussian noise term used in DP-SGD? clarify\n- As we increase the privacy budget...: sentence very imprecise (my suggestion: use commas more often)\n- section Comparison: first 6 lines are incoherent",
            "summary_of_the_review": "The paper's central claim of beating SOTA seems supported by experimental evidence on both CIFAR-10 and language modelling, in different settings. The description of experimental settings and protocols is unclear and leaves much underspecified. I see no major flaw in the work and it has potential to be improved. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4986/Reviewer_uKD4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4986/Reviewer_uKD4"
        ]
    },
    {
        "id": "YRnbENwLLq",
        "original": null,
        "number": 2,
        "cdate": 1666668583669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668583669,
        "tmdate": 1669218311392,
        "tddate": null,
        "forum": "5R96mIU85IW",
        "replyto": "5R96mIU85IW",
        "invitation": "ICLR.cc/2023/Conference/Paper4986/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies multiple ways to incorporate public data to help private training,  (1) use pre-training to improve the initial point of private training, (2) use a novel DP-SGD algorithm during training, (3) post process on the private models. The contribution is complete, tackling aspects before/during/after the training. Ablation studies are done on CIFAR10 and Wikitext to show the performance.",
            "strength_and_weaknesses": "Strength: This paper is well-written and the methodology is comprehensive, including different aspects of using public data (or public gradient) in the DP training. The experiments are carefully conducted and I appreciate the ablation study in Table 1. I think the new algorithms are interesting with good motivation.\n\nWeaknesses: 1. I think Algorithm 1&2 fall in the category of weighted sum between public and private gradients. Let's denote $g_i$ as per-sample grad, $\\hat g$ as public gradient like in Line 7 of Algorithm 1. Then the clipped residual is $c_i*(g_i-\\hat g)$ and Line 9 becomes $(1-c_i)\\hat g+c_i g_i$. Then the summed clipped gradient is $\\sum_i (1-c_i)\\hat g+\\sum_i c_i g_i$ and the noise is added. Clearly, this is a weighted sum that has been also studied in https://arxiv.org/abs/2111.00115 and several other works. So I think they should be discussed. 2. I wonder do the authors experiment with the average angle between clipped per-sample gradients and the public gradient at each iteration? This could be very helpful to convince that your new clipping helps reduce the bias (also, only the angle bias not the magnitude matters, as the latter can be recovered with adjusting learning rate). 3. The experiments do not show significant improvement but the algorithms are seemingly over-complicated.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is on the borderline of acceptance and rejection. I am happy to discuss more and to consider raise my score if my concerns are addressed.",
            "summary_of_the_review": "I think the paper is well-written but the experiments seem thin and the results aren't very exciting.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4986/Reviewer_Y2rR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4986/Reviewer_Y2rR"
        ]
    },
    {
        "id": "jpNR4r0Q3Q",
        "original": null,
        "number": 3,
        "cdate": 1666673099854,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673099854,
        "tmdate": 1666673327431,
        "tddate": null,
        "forum": "5R96mIU85IW",
        "replyto": "5R96mIU85IW",
        "invitation": "ICLR.cc/2023/Conference/Paper4986/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes new techniques for using public data in differentially private machine learning and makes a significant improvement over the state-of-the-art solution. More specifically, they propose to use synthesized data provided by a generative model trained by the given public data, and a new gradient clipping mechanism that achieves higher accuracy with the help of public and synthesized data. ",
            "strength_and_weaknesses": "Strength:\n- The techniques are well-motivated and presented. The intuitions behind the proposed techniques are well explained. A solid theoretic backbone is also provided for their claims.\n- Those techniques are novel and not very complicated. They also provide good experiments demonstrating the impacts and improvements in different settings and parameters. \n\nWeakness:\n- It's better to have proofs of proposition 2 and 4 in the appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly presented with a comprehensive analysis. ",
            "summary_of_the_review": "Overall I found no flaw in this paper and would recommend it. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4986/Reviewer_d916"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4986/Reviewer_d916"
        ]
    },
    {
        "id": "8a2C2YtqJ_",
        "original": null,
        "number": 4,
        "cdate": 1666681987072,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681987072,
        "tmdate": 1666681987072,
        "tddate": null,
        "forum": "5R96mIU85IW",
        "replyto": "5R96mIU85IW",
        "invitation": "ICLR.cc/2023/Conference/Paper4986/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the DP model training with public data. They utilize the public data in three phases:  pretraining, better optimization process and a post-process ensemble of private models. The experimental results show the significant improvement of their approach compared with the SOTA.",
            "strength_and_weaknesses": "**Strength**\n1. This paper is written very clear and organized.\n2. It clearly explains the intuition why the new proposed clipped gradient would be potentially better than clipping to the origin. The approach is overall well-motivated.\n\n**Weaknesses**\n\nThe experiments are mostly well-designed. However, it might miss one fair baseline, which is training only with (augmented) public data, including the new generated synthetic data. This baseline can justify the improvement from the second phase (new clipping method) and third phase (model ensemble) in the approach.\n\nMinor issue:\n1. Notation of *L* is re-used as both loss function in Algorithm 1&2 and the Lipschitz factor.\n2. \"Out-of-distribution public data\" is introduced in the dataset descriptions but is not mentioned in the later results in the main text. (I checked that it appears in the experiments in Appendix).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and the method is novel.",
            "summary_of_the_review": "The results of proposed approach are very promising. The missing baseline (see weaknesses) leads to a some uncertainty that how much the DOPE-SGD helps, which will influence the justification of significance. Thus I give the score of 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4986/Reviewer_UAzg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4986/Reviewer_UAzg"
        ]
    }
]