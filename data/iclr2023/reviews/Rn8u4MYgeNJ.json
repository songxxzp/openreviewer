[
    {
        "id": "phCj4QiyG5y",
        "original": null,
        "number": 1,
        "cdate": 1666681711822,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681711822,
        "tmdate": 1669798837558,
        "tddate": null,
        "forum": "Rn8u4MYgeNJ",
        "replyto": "Rn8u4MYgeNJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6179/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper suggests modifying the variational objective in mixture-based multi-modal VAEs by including both a uni-modal as well as a multi-modal reconstruction term. The authors also aim to present a unified perspective on mixture-based multimodal VAEs by showing that methods motived by a Jensen-Shannon-Divergence or Total Correlation objective can be viewed as mixture-based VAEs under some assumptions. Empirical results indicate the proposed approach improves the tradeoff of generative quality versus cross-coherence for different multi-modal VAEs.\n\n\n---\nPost-rebuttal comments:\nHaving read the other reviews and the authors' response, I feel that the submission is stil borderline.\nThe authors response clarified the comments that I had previously.\nThe presentation in the updated version has been improved.\n\n---",
            "strength_and_weaknesses": "Strengths:\n- The idea to include the uni-modal reconstruction terms appears well motivated.\n- The empirical performance suggests that the new approach yields better generative quality (e.g., FID scores) or cross-coherence (e. g., classification accuracy) than most previous approaches.\n- The paper adds some interesting connection between MVTCAE and JMVAE and mixture-based multi-modal VAEs.\n\nWeaknesses:\n- While the suggested approach improves the cross-coherence compared to previous work, the generation quality does not improve necessarily compared to some methods.\n- The algorithmic change to include the unimodal reconstruction terms is somewhat small. \n\nComments:\n- To clarify Figure 2, do the different number of modalities mean that a different generative model is trained for different number of modalities, or is this the same generative model that has a different number of missing modalities when encoding the latents?\n- Does Lemma 1 assume a MoE approximate posterior aggregation as in Lemma 2?\n- Does the approach give good accuracy to classify the labels for PolyMNIST/Translated-PolyMNIST in the latent space?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written. The presentation is largely clear. However, it would be clearer to summarize exactly what constitutes the method CRMVAE: Eq. (5) that replaces the reconstruction term with eq. (7), and a MoE aggregation? \nRelevant prior work is cited appropriately.\n",
            "summary_of_the_review": "The suggested approach of including both uni- and multi-modal reconstruction terms is novel as far as I am aware. It appears to be a rather small adjustment algorithmically compared to previous work yet it can yield improved performance, particularly with respect to the cross-modal coherency.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6179/Reviewer_Jrda"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6179/Reviewer_Jrda"
        ]
    },
    {
        "id": "dTRtrszLTA",
        "original": null,
        "number": 2,
        "cdate": 1666694930515,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694930515,
        "tmdate": 1666695629509,
        "tddate": null,
        "forum": "Rn8u4MYgeNJ",
        "replyto": "Rn8u4MYgeNJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6179/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper deals with multimodal VAEs. In particular it tries to mitigate the current limitations of models in this class. After highlighting the shortcomings of mixture-based approaches, which have been uncovered in previous work, the authors investigate the relationship between two previously proposed product-based multimodal VAEs, namely MVTCAE and MMJSD, and categorize them as coordination-based approaches. Coordination-based multimodal VAEs assume a product-of-experts joint posterior to have high-quality generated samples, and introduce specific terms in the objective to bring unimodal and joint encoders closer together, to obtain coherence between modalities. The authors propose CRMVAE, by extending the MVTCAE to include unimodal reconstruction terms in the objective, and show empirical results in multiple experiments indicating an overall improvement over existing approaches in terms of coherence and quality of generation.",
            "strength_and_weaknesses": "**Strengths**: \n- The paper investigates a relevant problem, trying to overcome the limitations of existing multimodal VAEs.\n- The paper is well-written and well-structured.\n- The Experiments section shows the authors have made an effort to evaluate their proposed method on a wide range of datasets, and empirical results indicate a clear improvement over previous methods.\n\n**Weaknesses**:\n\n- The main contribution of the authors when proposing the CRMVAE is introducing unimodal reconstruction terms in the objective. However, the rationale behind this choice seems to be rather based on intuition and lacks theoretical grounding, leaving some open questions. It is e.g. unclear why optimizing for unimodal reconstruction of each modality, i.e. reconstruction of each modality given the latent code inferred from the corresponding unimodal encoder, would improve coherence for cross-generation.\n- The experimental results are extensive in the sense of models being evaluated on a wide range of datasets, but there would be additional results that would give important insights on whether certain current limitations of multimodal VAEs are truly overcome. For example, how do the models compare on generation performance, when the joint latent code is sampled from a prior? This is an important aspect when trying to learn a joint distribution of all modalities [1-2], and I think coherence across generated modalities in this setting would show if bringing unimodal and joint posteriors closer together effectively leads to encoding shared content effectively in the latent space.\n- From my understanding it is unclear if the equality in section 3.4 below equation 7, central to deriving the ELBO, is justified for the considered setting. Could the authors elaborate on that? Also, it seems like the assumption could be stated more clearly: Are the integrals actually necessary for the left-hand side, or is it equivalent to assuming that the joint posterior is equal to each unimodal posterior? Can the assumption be stated in terms of a (conditional) independence relation? \n- Another important point where this work can in my opinion be improved is reproducibility. Being quite familiar with the mentioned models and experimental settings, I would have liked to replicate some of the results shown in the paper, but the authors did not share their code, which makes it hard to do so. In addition, I tried to reproduce the results for MVTCAE in the CUB Image-Captions experiment, using the public repo for the original paper, but I was not able to obtain the same performance shown in the paper for cross-generation, despite using the same settings. I think it would be helpful to see the code to understand the implementation better and reproduce the results. \n- [Minor] I believe a logarithm is missing in the RHS of the equation in Proof 1. \n\n**References**\n- [1] Yuge Shi, et al.: Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models. NeurIPS 2019.\n- [2] Thomas M. Sutter, et al.: Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence. NeurIPS 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: the flow and exposition of the paper are clear. However, it would be preferable to have more results (e.g. in the Appendix) for a clearer understanding of model performance in the experiments (e.g. generation when latent code is sampled from a prior, image-to-caption generation). \n\n**Novelty**: This work analyzes the relationship between MVTCAE and MMJSD, and categorizes them as coordination-based approaches. Then proposes CRMVAE, which extends the MVTCAE by adding unimodal reconstruction terms. The authors stress that their addition is beneficial for model performance and this is confirmed in the showcased experimental results, but the introduction of these terms is backed only with an intuitive argument, and more empirical results would be helpful to see if limitations of existing multimodal VAEs are fully addressed (see point 2 Weaknesses).\n\n**Reproducibility**: The authors did not share their code and the paper alone does not provide sufficient details, which makes it hard to reproduce their results. \n\n**Quality**: There are some aspects of high-quality work, such as the inclusion of a variety of experimental settings, but also some aspects in which quality could be improved, such as reproducibility.\n",
            "summary_of_the_review": "The paper is well-written, deals with a relevant problem, and presents promising empirical results. The authors propose a model in the class of coordination-based multimodal VAEs that extends MVTCAE, by adding unimodal reconstruction terms in the objective. However, behind this addition, there only seems to be an intuitive argument. While empirical results are promising, reproducibility can be improved, and more empirical results could give a clearer picture on whether limitations of existing multimodal VAEs are fully addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6179/Reviewer_P3oY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6179/Reviewer_P3oY"
        ]
    },
    {
        "id": "zIJtdir1vi",
        "original": null,
        "number": 3,
        "cdate": 1667023203958,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667023203958,
        "tmdate": 1667023203958,
        "tddate": null,
        "forum": "Rn8u4MYgeNJ",
        "replyto": "Rn8u4MYgeNJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6179/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper describes recent development in Multimodal VAE in extensive details. \nSpecifically, versions of the ELBO where the latent models , based on subset of modalities, are forced to be similar to\nthe latent model that is estimated with all the modalities is explained as Product of Experts like model. The paper goes on \nto propose a weighted sum of reconstruction loss together with Product of Experts like posterior model.\n  ",
            "strength_and_weaknesses": "\nThe paper is very well written and easy to follow. I have the following comments and suggestions.\n\nComments:\n- How is the unimodal posterior $q_\\theta(z|X)$ computed and sampled from? The main motivation of mixture models is to be scalable, i.e., they can be used for large number of modalities just as effectively. The equations described in (5) and (7) look like something that is not going to scale very well with the number of modalities. Is it possible to show how the approach scales as the modality increase, empirically? \n- The paper reads more like a survey. It would have been much better if the Sections from Section 2. up to Subsection 3.3 are condensed and the contributions, Section 3.3 and Section 3.4, are expanded upon.\n- Related works is not specialized to this work-- there is a paragraph about transformer based multimodal models.\n\nSuggestions:\n- Please include what $H(\\cdot)$ actually is, this is below Eq(4). It is much better to define it there than forcing the reader to scroll through the reference. \n- Please improve result reporting in Table-1. The abbreviated dataset description and their categorization is not clear.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear. Novelty is not sufficiently argued for (I am referring to the weighted reconstruction loss).",
            "summary_of_the_review": "The paper is an informative piece of work on multimodal VAE. Although, it reads like a survey paper with added suggestions. My main concern is the scalability of the approach which mixture models are meant to be efficient on.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6179/Reviewer_venE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6179/Reviewer_venE"
        ]
    },
    {
        "id": "XBZ7TPoYF0",
        "original": null,
        "number": 4,
        "cdate": 1667525210302,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667525210302,
        "tmdate": 1667525210302,
        "tddate": null,
        "forum": "Rn8u4MYgeNJ",
        "replyto": "Rn8u4MYgeNJ",
        "invitation": "ICLR.cc/2023/Conference/Paper6179/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "It is challenging to infer a joint representation from arbitrary subsets of multimodalities, and the state-of-the-art approaches (mixture-based multimodal VAEs) attempt to accomplish this by training to generate all modalities from a joint representation inferred from missing modalities, but the quality of modality generation is lower than that of unimodal VAEs, and this limitation is theoretically unavoidable.  Therefore, the authors propose a coordination-based model that brings the representation inferred from each modality closer to that inferred from all modalities. Experiments with diverse and challenging datasets show the advances of the proposed method.",
            "strength_and_weaknesses": "\nStrength:\n\n- The authors point out why these existing coordinate-based models perform poorly on cross-model generation, and propose a novel model to fix the issue.\n- The proposed model can mitigate the limitations in multimodal VAEs and performs well in both cross-coherence and generation quality.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and the proposed model is novel.",
            "summary_of_the_review": "Unfortunately, this paper lies outside of my field of expertise, therefore, my comment may be biased. Area chairs are suggested to seek opinions from other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6179/Reviewer_DC9h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6179/Reviewer_DC9h"
        ]
    }
]