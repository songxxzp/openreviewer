[
    {
        "id": "yCrWkP5o2_",
        "original": null,
        "number": 1,
        "cdate": 1665932488956,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665932488956,
        "tmdate": 1665932488956,
        "tddate": null,
        "forum": "3KHzMQUOH4x",
        "replyto": "3KHzMQUOH4x",
        "invitation": "ICLR.cc/2023/Conference/Paper1463/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for enhancing the depth perception of an image. The proposed method first embeds content-independent depth perception of a scene using the visual representation learning technique, and then trains a controllable depth enhancer network based on a parametric feature rotation block (PFRB). Some visual results are shown to verify the proposed method. \n",
            "strength_and_weaknesses": "Strength\n- This paper proposes a strategy to learn the visual representation space of style disentangled from image contents.\n- A controllable neural network with a single control parameter is proposed to enhance the depth perception of the input image.\n- A parametric feature rotation block (PFRB) is introduced to enable continuous modulation of feature representation.\n\nWeaknesses\n- It is unclear to me the advantage of this method over existing methods utilizing explicit depth information. In fact, the ground-truth depth-enhanced images are just generated using the estimated depth maps of the input image. Also, the controllable factor (i.e., sigma) can be utilized in methods using explicit depth as well. \n- Lack of comparison with other depth enhancement methods.\n- There is no numerical result. I understand that quantitative evaluation is difficult, but a user study might be the least that can be provided. \n- The qualitative results are not visually pleasing and unrealistic. There are many obvious errors in the generated images, e.g., in Fig.7(c) regions with different depths have the same blur effects, and regions with similar depths have different blur effects.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is overall easy to follow. The originality is marginal.",
            "summary_of_the_review": "Given that the idea is not very new, the reported results are not visually pleasing, and the evaluation is not sufficient, I would like to give a reject rating at the current stage.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1463/Reviewer_MFV5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1463/Reviewer_MFV5"
        ]
    },
    {
        "id": "pJaHZkT-M3u",
        "original": null,
        "number": 2,
        "cdate": 1666342662296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666342662296,
        "tmdate": 1666342662296,
        "tddate": null,
        "forum": "3KHzMQUOH4x",
        "replyto": "3KHzMQUOH4x",
        "invitation": "ICLR.cc/2023/Conference/Paper1463/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the DEpth Enhancement via Adaptive Parametric feature Rotation (DEEAPR) method to modulate depth information with a single control parameter. They first use visual representation learning to embed content-independent depth perception of a scene, then train a controllable depth enhancement network with a novel modulator (parametric feature rotation block, PFRB), which is for continuous modulation of a representative feature. They verify the effectiveness of proposed components and the whole method via ablated studies and comparisons.",
            "strength_and_weaknesses": "Comments.\n\nThe paper lists three contributions. The first one is a new method to learn style disentangled visual representation space of image contents. This mostly relates to visual representation learning, but the difference between the proposed method and existing methods is not explained. Sec. 2.1 mentions that it is similar to (Chen et al., 2020b) and is motivated by DASR (Wang et al., 2021), but it is not clear what the exact similarities and differences are. Hence, it is very hard to evaluate this contribution. \n\nBesides, it is not clear how to define the \"content-independent depth perception of a scene\" and why \"content-independent\" relates to this task. How can the combination of (Hadsell et al., 2006) and (Balntas et al., 2016) embed the content-independent depth perception of a scene onto a representation space, and why this bridges the image space and the control parameter axis? Why choose these two methods? How to define the control parameter axis? What is the \"direction\" of depth enhancement?\n\nThe second contribution is a controllable neural network that enhances the depth perception of an image with a single control parameter. Sec. 2.2 says the main differences to existing methods are that DEEAPR uses a single parameter and does not rely on explicit depth information. However, it seems that (Dutta et al., 20) also do not rely on explicit depth information. So the key difference is about using a single parameter. It is not explained why the authors propose to use a single control parameter. Is a single control parameter enough and why?\n\n\nThe third contribution is the parametric feature rotation block (PFRB) for continuous modulation of feature representation. Its motivation and novelty are not explained clearly. For example, (Wang et al., 2019a) have a tuning network to modulate the feature of the main branch. How does the proposed DEEAPR differ from this?\n\nAlmost all methods included for discussion and comparisons are published before/in 2021. More recent works should be included.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper will need a clear clarification of its key contributions and a clear explanation of its motivations. The paper provides a clear description of network details for reproduction but it is not clear whether the code & data will be released to the public.",
            "summary_of_the_review": "The paper lists three contributions but I find them not convincing. This reflects in how they explain the motivations and discuss their differences with existing methods. Hence, at this point, I am on the negative side.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1463/Reviewer_mEsF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1463/Reviewer_mEsF"
        ]
    },
    {
        "id": "ch34VKWE40",
        "original": null,
        "number": 3,
        "cdate": 1666667684894,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667684894,
        "tmdate": 1666672336745,
        "tddate": null,
        "forum": "3KHzMQUOH4x",
        "replyto": "3KHzMQUOH4x",
        "invitation": "ICLR.cc/2023/Conference/Paper1463/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors proposed a network-based approach to control the depth-of-field (DoF) effect of photographs post-capture. While there are many previous works in artificial Bokeh generation, the authors claimed their novelties lie in 1) learned representation space for the DoF effect, 2) controllable network for the strength of the DoF effect, and 3) a new network block called PFRB that can preserve the norm when modulating the features. ",
            "strength_and_weaknesses": "### Strengths\n1. Controllable network for different DoF effect levels (though seen in previous work).\n2. Network architecture and PFRB module seem novel.\n### Weakness\n1. Given Bokeh's image formation model and the DoF effect's strength are clearly defined, I have doubt whether learning a latent space for rendering is necessary. \n2. The term \"depth perception\" is ambiguous, but rather DoF is the good latent space that models the background/foreground blur/sharpness contrast. \n4. Lack of results to show the robustness of the proposed network. Only very few images (I counted 4 throughout the paper and supplementary material) are used for visual evaluation. \n5. The shown results image has unnatural artifacts. For example, the bridge image, different portions of the bridge at the same depth are exhibiting different levels of blur.\n5. The depth-based contrast model $O_{contrast}$ in section 4.1, though already redefined from the haze/fog model from prior, is still heavily haze-oriented. For general photos taken under good weather conditions, the proposed method may introduce an unwanted or exaggerated hazy effect.  The plant image in the supplementary material proves this point: as $\\sigma$ increases, the further away background (maybe ~50$cm$) is becoming hazy, whereas the depth-dependant haze effect should hardly be seen in the real world for an object about this distance. \n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe term *depth perception* used throughout the paper is vague and really is majorly DoF effect + depth-dependent haze effect at its core given the authors' results and depth-based model. The authors could just clear it upfront.\n### Novelty\nNetwork architecture and the proposed PFRB module seem novel\n### Reproducibility\nIt may be difficult to reproduce the authors' results if the authors don't open-source their code, since the architecture and training setup is novel.",
            "summary_of_the_review": "While I appreciate the effort the authors carry out in designing the network and module for the specific problem of post-capture DoF effect rendering which is very useful in computational photography and has a wide-range impact in real life. \n\nMy biggest concern is whether the authors' fundamental motivation, that we need a learnable latent representation space for the DoF effect, is valid. \n\nUsually learning an implicit latent space for image manipulation or enhancement is crucial when the problem can not be easily and explicitly defined by physics-based models or the underlying physical model is too complex or ill-posed  (i.e. manipulating the age of the person in a given image, image style transfer, etc.). However, in the case of DoF or Bokeh effect rendering, the problem is very well and explicitly defined. The DoF is clearly defined as a function of the depth ($u$), camera focal length ($f$), aperture size or F-number ($N$) and circle-of-confusion ($\\mu$), $DoF = \\frac{2 \\mu^2 N C}{f^2}$. Shall or deep DoF (or \"depth perception\" per this paper) can be nicely controlled and rendered with the camera parameter factored into $O(x)$, defined in equation 4.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1463/Reviewer_3SWe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1463/Reviewer_3SWe"
        ]
    },
    {
        "id": "MapIlFKGs7",
        "original": null,
        "number": 4,
        "cdate": 1666930106298,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666930106298,
        "tmdate": 1666930106298,
        "tddate": null,
        "forum": "3KHzMQUOH4x",
        "replyto": "3KHzMQUOH4x",
        "invitation": "ICLR.cc/2023/Conference/Paper1463/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a novel method for depth enhancement for single image. They used a single hyperparameters to control the synthetic defocus blurriness. ",
            "strength_and_weaknesses": "Strength:\n+ The output image can be controlled by adjusting the input hyperparameters\n\nWeakness:\n- The paper is poorly written and extremely hard to follow. The countless grammatical errors, notions used without any references,  and bad paper organization make it almost impossible for readers to understand the paper content. The following listed ones are just a few: What is query, positive sample and hard negative sample mean (Sec. 3.1 and Fig.3)? What is depth representation means? What's the difference between global/local depth representative feature (Sec. 3.2)? How you can rotate the parametric features in PFRB?\n- The technical contribution is marginal. I didn't find significant difference between images of different \\sigma.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the paper is low.",
            "summary_of_the_review": "In summary, even though the paper shows that they can achieve depth enhancement effect in a single image by varying value of a hyperparameter, there's still a very large space for this paper to improve w.r.t. writing and performance. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1463/Reviewer_4wXg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1463/Reviewer_4wXg"
        ]
    }
]