[
    {
        "id": "BSt1VEsAUOH",
        "original": null,
        "number": 1,
        "cdate": 1666641137017,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641137017,
        "tmdate": 1666641137017,
        "tddate": null,
        "forum": "lH1PV42cbF",
        "replyto": "lH1PV42cbF",
        "invitation": "ICLR.cc/2023/Conference/Paper5444/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper propose BINDER, a training-free neural-symbolic framework that (1) maps the task input to a program using a pre-trained language model, where the model has to decide which part in the input can be converted to a target programming language and corresponding tasks API calls for further extension; (2) adopts an LM as both the program parser and the underlying model called by the API during the execution, where a language model serves to realize the API calls given the prompt and the return values feed back into the original programming language; (3) derive the final answer via a deterministic program interpreter executing the program without API calls. BINDER achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, where previous work is all fine-tuned on a large-scale training set.",
            "strength_and_weaknesses": "Strength:\n- The system does not require extra training but can achieve better performance compared to fine-tuning settings.\n- The in-depth analysis validates that BINDER can have significantly better performance on program-unsolvable questions.\n\nWeakness:\nNo obvious weakness was observed.",
            "clarity,_quality,_novelty_and_reproducibility": "The BINDER system can benefit lots of semantic parsing tasks since it uses uniformed underlying API, i.e., SQL and Python, and requires on further training process.",
            "summary_of_the_review": "- This paper is clear and well-organized. Lots of complementary materials are also provided in the Appendix to help us understand the technical details better.\n- The methods can be applied to a broad range of semantic parsing tasks since it use uniformed underlying APIs, i.e., SQL and Python, and require no further training process.\n- The experimental results are impressive. It outperforms previous work which typically use finetuning settings based on large training set.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5444/Reviewer_S9TA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5444/Reviewer_S9TA"
        ]
    },
    {
        "id": "LLUpQGiEnq6",
        "original": null,
        "number": 2,
        "cdate": 1666670972357,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670972357,
        "tmdate": 1666670972357,
        "tddate": null,
        "forum": "lH1PV42cbF",
        "replyto": "lH1PV42cbF",
        "invitation": "ICLR.cc/2023/Conference/Paper5444/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a training-free two-stage method based on Codex model for QA tasks. The first stage uses few-shot learning to prompt Codex and converts a natural language question to a SQL or Python program, where difficult-to-resolve parts are represented as API calls. In the second stage, Codex is prompted again to generate answers to each said API call, and then the whole program is executed to obtain the answer. Experimental results show new state-of-the-art on WikiTQ and TabFact datasets, with particular performance gains on questions that are unsolvable by natural-language-to-SQL translation.",
            "strength_and_weaknesses": "This paper is a creative usage of a code generation model. It also represents an interesting new approach to neural symbolic: code generation models can learn to divide a task into a high-level symbolic logical program and some low-level components for neural inference.\n\nThe experimental results are promising. With a pre-trained Codex model, without fine-tuning and with a small number of annotated examples, the proposed method is able to achieve state-of-the-art on two QA datasets. It achieves the desired behavior of a neural symbolic method, including better interpretability and answering some difficult questions.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good.\n\nQuality is good.\n\nNovelty is good.\n\nReproducibility is good as code is attached.",
            "summary_of_the_review": "The main idea is good and well executed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5444/Reviewer_8V6D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5444/Reviewer_8V6D"
        ]
    },
    {
        "id": "OaFocbMmEb",
        "original": null,
        "number": 3,
        "cdate": 1666839451324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666839451324,
        "tmdate": 1668537516963,
        "tddate": null,
        "forum": "lH1PV42cbF",
        "replyto": "lH1PV42cbF",
        "invitation": "ICLR.cc/2023/Conference/Paper5444/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "1) The paper proposed BINDER to incorporate symbolic component into large language models, and the main benefit of the method is not requiring any fine-tuning.\n2) BINDER outperforms state-of-the-art results on WIKITABLEQUESTIONS and TABFACT datasets \n3) useful analysis is presented to help readers understand the proposed approach.",
            "strength_and_weaknesses": "Strength:\n1. Incorporating symbolic component into end2end learning systems is always an interesting idea\n2. The proposed approach achieved state-of-the-art results\n\nWeakness:\n1. The writing is not clear throughout.  Most importantly, the paper directly jumps in talking about the proposed method without first clearly introducing the problem setup.\n2. Again, presentation is not clear.  The main figure is confusing.  I do not understand how the prompting part is employed exactly in the system.  In particular, it is not clear how the prompting results are combined with the parsed results to feed in the program interpreter.\n3. The results are a bit over sold.  Even though the fine-tuning methods require more task annotations, but GPT3 codex is pretrained on many more data.  The improvement is not that surprising.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity needs to be improved. Quality, Novelty And Reproducibility seem to be good.",
            "summary_of_the_review": "The proposed idea is exciting, the results are fairly strong, but the writing needs to be improved.\n[Edited: The author has improved their writing to make the paper much clearer.  I have thus raised my score from 6 -> 8.]",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5444/Reviewer_Pz8D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5444/Reviewer_Pz8D"
        ]
    }
]