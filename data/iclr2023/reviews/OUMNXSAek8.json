[
    {
        "id": "9Lugb-wKM8",
        "original": null,
        "number": 1,
        "cdate": 1666464034694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666464034694,
        "tmdate": 1670872343204,
        "tddate": null,
        "forum": "OUMNXSAek8",
        "replyto": "OUMNXSAek8",
        "invitation": "ICLR.cc/2023/Conference/Paper198/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new talking head generation method with memory compensation. The proposed method is novel with good experiment performances. The claimed implicitly scale condition is confusing, however, it could be addressed by editing the paper.",
            "strength_and_weaknesses": "Strength:\n- The proposed method is novel in terms of using the meta memory bak to transfer representative facial representations. Compared to the previous works [19], the main novelty comes from the memory module and compensation module. The two modules are reasonable to me and the author has clearly demonstrated their implementation of them. \n\n- The method is well demonstrated. The author has provided three figures Fig. 2, 3, 4 to demonstrate the architecture of the proposed method with detailed text. The proposed method is a bit complex, and I hope the author could release their source code upon acceptance. The author provides the visualization of the meta memory to show the learned memory content.\n\n- The proposed method achieves the best performance on two datasets for both same-identity reenactment and cross-identity reenactment experiments. The author also provides a detailed ablation study to show the benefit of meta-memory learning, scale-aware memory learning etc.\n\nWeakness:\n- It is not clear to me why the author claims \"scale\" conditioned. First, the author motivated the scale is important because adults and children have different face scales (beginning of section 3.2). However, there are no children in the data that they tested on. I do not understand why the author cares about the scale in this case. Second, there is no evaluation to show that the proposed method indeed implicitly learns the \"scale\" although the ablation study shows the scale module can improve the performance. I do not think the author could claim they have the \"implicit scale representation\". Last, the implementation of the scale is scaling of the convolutional weights, which is not the scale of the face. I do not think scaling the weights has the semantic meaning of scaling the face. Taken together, I am confused why the author focuses on the scale in this proposed method rather than the meta memory.\n\n- There are many parts of the method without an explanation of why the author does that. For example, the multi-layer generation in section 3.3 is upsampling operations that claim to \"preserve the details of the face\", which is not clear to me why and how. The Wrapped facial feature compensation has similar issues that I do not see why the author has this specific architectural design. Is that the author designed the module from the scratch by themselves, or did the author borrow this module from previous works (for which I did not see the citation)?\n\n\nMinor comments:\n- The images in Fig. 7 are too small to see and zooming in just ends up with blurry images. I suggest the author reduce the number of meta memory in Fig. 7 to have a better visualisation.\n- The author writes \"Specifically, compared with FOMM and DaGAN, which adopt the same motion estimation method as ours ...\". Does it mean the author takes their proposed method as a motion estimation method? I think the meta memory is the main character of the method.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing is clear, although there are some minor parts that could be improved. The proposed method is novel. Although the author provides a detailed description of the method, I think the source code is still needed to re-produce the same results.",
            "summary_of_the_review": "The proposed method is novel and achieves the best performance on two datasets. There are some issues in this paper that could be addressed by re-writing part of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper198/Reviewer_StKu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper198/Reviewer_StKu"
        ]
    },
    {
        "id": "YX9atOIdxt",
        "original": null,
        "number": 2,
        "cdate": 1666597449102,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597449102,
        "tmdate": 1666597449102,
        "tddate": null,
        "forum": "OUMNXSAek8",
        "replyto": "OUMNXSAek8",
        "invitation": "ICLR.cc/2023/Conference/Paper198/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new video-driven talking head generation method based on memory band learning and implicit scale representation learning. The proposed model, named as MCNet, consists of:\n1) a keypoint detector and the dense motion network that predicts keypoints from source and driving frame and then estimates optical flow between them,\n2) an implicit scale conditioned module ISCM that encodes implicit scale representation from warped feature and source keypoints, which is then conditioned on the query of the meta memory to produce identity-independent scale-aware memory band,\n3) a memory compensation module MCM that utilizes a dynamic cross-attention mechanism to spatially compensate the warped features, which is finally produced into final image by decoder.\nThe MCNet learns global facial meta memory bank to obtain global facial priors on spatial structure and appearance from all available training faces and compensate for dynamic facial synthesis, addressing the ambiguities in face generation especially in situation of large head pose change.\n",
            "strength_and_weaknesses": "Strengths:\n+ The paper proposes to learn a global facial meta memory bank to transfer representative facial representations to handle the appearance and structure ambiguities caused by the highly dynamic generation from a still source image\n+ The paper proposes an implicit scale conditioned memory module (ISCM) to learn implicit scale representation from warped feature and source keypoints and addresses the problem that different source faces contain distinct scales.\n+ The paper proposes a memory compensation module (MCM) to compensate and refine the warped feature map, which utilized the learned global prior facial information to generate better results.\n+ The qualitative and quantitative comparisons to STOA in both same-identity reenactment and cross-identity reenactment show the effectiveness of the proposed method.\n+ Ablation studies are conducted to visualize the learned meta memory and show the effectiveness of each module.\n\nWeaknesses:\n- The method is proposed as a talking head generation method, but many compared SOTA methods such as FOMM, MRAA and TPSN are general image animation methods capable of animating images from different domains, which makes the comparison with these methods in face image domain somehow less convincing.\n- The proposed implicit scale conditioned memory module (ISCM) learns implicit scale representation which is hard to visualize or compare to show its effect, and the quantitative results of the ablation study show that the ISCM module has only very slightly improvement on the evaluation metrics.\n- The evaluation datasets are not sufficient. It\u2019s better to perform experiment on some new datasets such as VoxCeleb2 and high-resolution dataset HDTF.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work has high quality and the idea and framework are clear. It is innovative and is able to be reproduced according to the details provided in the appendix.",
            "summary_of_the_review": "This paper proposes a novel method based on good observation of human face and effectively improves the performance of facial reenactment.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper198/Reviewer_gGqi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper198/Reviewer_gGqi"
        ]
    },
    {
        "id": "WE40hFj23v",
        "original": null,
        "number": 3,
        "cdate": 1666658979460,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658979460,
        "tmdate": 1666658979460,
        "tddate": null,
        "forum": "OUMNXSAek8",
        "replyto": "OUMNXSAek8",
        "invitation": "ICLR.cc/2023/Conference/Paper198/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presented a method for learning talking head generation/reenactment. Specifically, the paper proposed a memory bank based architecture to address the inconsistency caused by varying shape/appearance of source and driving signals. A learnable external prior is introduced to learn and store the shape/appearance coupling. Some other modules and architectural designs are also proposed in the paper to improve the performance of the method, specifically handling the scale difference between source and driving signals. The paper provided qualitative and quantitative comparisons with some existing approaches, including, FOMM, DaGAN, TPSM etc. The proposed method achieves better performance on image quality and identity preservation, albeit with relatively small margin. The paper also provided quantitative ablation study to demonstrate the effectiveness of each component.",
            "strength_and_weaknesses": "Strength:\n\n1. The problem that the paper is aiming to address is interesting and is an essential challenge in face reenactment work. Keypoint and warping based method for transferring dynamic facial appearance often suffers from inconsistency between the driving signals and target image, especially in terms of shape and scale of the images.\n\n2. The approach is novel to me by leveraging a memory bank as a global face appearance representation and use it for a face reenactment task.\n\n3. The experimental results demonstrated the advantage of the proposed method. The qualitative results in the paper and supplemental video showed clear advantage over previous work. The quantitative performances on image quality and identity preservation evaluation also outperformed previous work.\n\n\nWeaknesses:\n\nNo major concern from me except a few minor questions:\n\n1. Can this method scale to very high resolution images? Would the facial details be challenge or too expensive to learn with this method?\n\n2. The method seems still have issue with identity preservation, which is a common issue in such tasks. Since the method is learning a memory bank from training data, would it make any difference if the testing identity is a part of the training corpus? Perhaps an ablation study on identity preservation could help explain.\n\n 2. Ablation study. The network design is quite complicated, which is my major concern with this paper. I'm not sure how much of them could be simplified. It is good that the author provided the ablation study to show the accumulating benefit of each component, but it's a little disappointing that each of which doesn't seem to contribute much upon the baseline. More importantly, there seem to be lack of qualitative comparisons for the ablation study. How much difference can we see in the results by adding the proposed architectural designs?\n\nSome inconsistency or typos:\n\n- In section 3.1, the paper refer Ms as \" identity-independent scale-aware memory bank Ms\". But in section 3.2, last paragraph, the paper stated \"person-dependent scale-aware memory Ms\" which is the direct opposite of the early statement. Is there a typo or am I missing something?\n\n- Section 3.2 last paragraph, \"scale-dependent scale-aware memory bank Ms\" -- why mention scale twice? Perhaps a typo?",
            "clarity,_quality,_novelty_and_reproducibility": "The work seems to be original and the paper is overall easy to read. The architecture design and the motivations of it are both well documented and explained in the text. The experiments are also clear to me. Overall I think the quality and clarity are good.",
            "summary_of_the_review": "I appreciate the motive, method, and results of the paper. I have some complaints about the method being complicated and might be challenge to reproduce (I hope the code and model will be published alongside the paper). I also have some minor questions/suggestions about the paper as mentioned above. Overall I'm positive on this work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't have any new ethics concerns about the technique. Face reenactment is a sensitive application so the work inherits the existing concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper198/Reviewer_dF7D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper198/Reviewer_dF7D"
        ]
    }
]