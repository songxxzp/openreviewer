[
    {
        "id": "nMGWUC20NSn",
        "original": null,
        "number": 1,
        "cdate": 1666093567377,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666093567377,
        "tmdate": 1666093567377,
        "tddate": null,
        "forum": "a18z-D9l763",
        "replyto": "a18z-D9l763",
        "invitation": "ICLR.cc/2023/Conference/Paper3785/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a customized poisoning attack in the context of continual learning to stealthily promote catastrophic forgetting.\n",
            "strength_and_weaknesses": "Strengths:\n\n- Trendy topic\n- Interesting idea\n\nWeaknesses:\n\n- Writing needs to improve\n- More experiments are needed\n\n\nComments for the authors:\n\nIn this paper, the authors exploit the intriguing property of generative models, i.e., they cannot well capture input-aware triggers, to promote catastrophic forgetting, while retaining high accuracy on the current task and evading the defenders to some extent.\nThe idea of leveraging input-aware triggers in this specific scenario is intriguing, as they are more likely to be overlooked and disregarded by the DGR. Thus, the replayed examples no longer carry the triggers, but they still carry the flipped labels.\nMore interestingly, the replayed examples cannot be easily filtered out by v-SVM as label-flipped poisons because the input-dependent triggers introduce additional complications to the DGR.\n\nHowever, I do have the following concerns.\n\n- The paper is a bit hard to follow, so I would suggest the authors re-organize the paper structure. Moreover, as the poisoning attack in the context of continual learning is more complex than common scenarios, it would be much better to provide more visual explanations about the attack process.\n\n- In Section 4, the authors mention they use both Neural Cleansing and v-SVM as defenses. However, only experiments related to v-SVM appear in the evaluation. It would be better to conduct some experiments about Neural Cleansing.\n\n- The attack performance on the FashionMNIST and permuted-MNIST is far less significant than on the MNIST dataset. Thus, I would like to know what will happen if the authors conduct experiments on more complex datasets, e.g., CIFAR100 and CelebA.\n\nMinor:\n\n- In Section 2, \"The DRG model\" -> \"The DGR model\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "good",
            "summary_of_the_review": "see strength and weakness",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3785/Reviewer_U8Zv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3785/Reviewer_U8Zv"
        ]
    },
    {
        "id": "XBfbRa-yCV",
        "original": null,
        "number": 2,
        "cdate": 1666485393619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666485393619,
        "tmdate": 1666485393619,
        "tddate": null,
        "forum": "a18z-D9l763",
        "replyto": "a18z-D9l763",
        "invitation": "ICLR.cc/2023/Conference/Paper3785/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This submission presents a data poisoning attack against continual learning. Specifically, continual learning systems that utilize generative algorithms to remember and replay data from previous tasks, are attacked in this work. The attack inserts poisoned data (containing backdoored data and flipped labels) that later breaks the generator when it replays samples.",
            "strength_and_weaknesses": "The proposed attack is interesting and overall executed well, and the authors spend a good amount of effort on motivating and deriving it in detail. From the experiments, I particularly like analysis in sections 5.2 and inclusion of baseline defenses in 5.3. The experimental section is somewhat limited by the small scale of experiments investigated here, it would have been interesting to see whether this attack is also possible if the dataset (and by extension the generator) have to model more complex behavior, e.g. when looking at variations of ImageNet for continual learning. \n\nI've listed some weaknesses of the submission in its current state below:\n\n* The submission contains motivation and discussion of the cross-trigger examples, but I could not find an ablation detailing the effectiveness of this modification and effectiveness in relationship to cross ratio described in the experimental evaluation.\n\n* Further, what about baseline attacks? A simple one that comes to mind is simply maliciously flipping the labels of 40% of the data, e.g. by  predictably flipping between the classes in each task.\n\n* I wonder whether the authors can comment on the necessity of dirty-label backdoors in this system. Is the proposed attack contingent on having access to label information on poisoned samples? It would be great if this point could be clarified somewhere in the submission.\n\n* Finally, if I understand it correctly, the attacks described in Sec. all assume that 25%+15% of the data can be poisoned. What is the attack performance as these numbers are increased or decreased? It would be great if the whole range could be plotted.",
            "clarity,_quality,_novelty_and_reproducibility": "Concerning Related Work:\n\nWhat is the relationship of this work to previous attacks in Umer et al. 2020, \"Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks\" and follow-up work in Umer 2021 and especially Umer 2022 \"False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger\"? It would be great if the submission could discuss and distinguish the proposed attack from related work on poisoning attacks in continual learning. Somewhat related is also Li and Ditzler \"Targeted Data Poisoning Attacks Against Continual Learning Neural Networks\".\n\nConcerning the Title:\n\nI believe the current title is not a great fit for this work. While generative models are indeed poisoned to promote catastrophic forgetting, this is not a generic attack against generative models, but an attack that happens only in the context of generative models used as replay systems in continual learning. I specifically do not think it a bad thing that the attack targets this threat model, just that this is not well reflected in the title. A suggestion could be \"Poisoning Generative Replay in Continual Learning to Promote Catastrophic Forgetting\".\n\nConcerning Clarity of Writing:\n\nThe submission is overall well-motivated and introduced concepts are well-explained. However, the presentation is held back a bit by a smaller stream of typos and grammatical errors. For example, on the first pages:\n* \"...urges investigation of poisoning attacks on it in a broader range of learning paradigms\"->\"...urges the investigation of poisoning attacks in a broader range of learning paradigms\"\n* \" and work in batch.\" -> \"and work in batched learning scenarios\"\n* \"a novel customization of dirty-label input-aware backdoor \"->\"a novel customization of dirty-label input-aware backdoors\"\n* \"The poison should sustain solid defense deployed by\"->\"The poisoned data should be robust to solid defenses deployed by\"\n* \" the transiency of data stream\" -> \" the transiency of data streams\"\n* \"The DRG models can be simplified\" -> \"DGR models can be simplified\"\n*  \"we are now missioned to\" -> \"We are now tasked to \"\n\nMy review is not changed by these problems with the writing, but I do think this work would be more impactful if its another pass over the text would be made with particular focus on the writing, especially in earlier sections.\n",
            "summary_of_the_review": "Overall I like the idea described in this submission, but some open questions concerning baselines, ablations and related work remain for me, as outlined above. I would like for the authors to comment on and clarify these points before recommending acceptance of this submission.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3785/Reviewer_rvck"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3785/Reviewer_rvck"
        ]
    },
    {
        "id": "eL9lKyLLQb",
        "original": null,
        "number": 3,
        "cdate": 1666842121913,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666842121913,
        "tmdate": 1666842121913,
        "tddate": null,
        "forum": "a18z-D9l763",
        "replyto": "a18z-D9l763",
        "invitation": "ICLR.cc/2023/Conference/Paper3785/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a poisoning attack on the generative model that is used to generate past data in the continual setting. ",
            "strength_and_weaknesses": "I found that the problem setting is very narrow and has limited applicability.  My major concerns are as follows:\n\n1. It seems like one can easily defend the proposed attack simply by not solely relying on a generative replayer. Instead, all you need is to keep a small dataset for each task in addition to the generative model, and the attack scenario (and the proposed algorithm) becomes invalid. More specifically, a small dataset for each task can be used to check the validity of the generated data.\n\n2. The proposed approach does not have anything to do with \"promoting forgetting.\" What it does is inject wrongly labeled inputs that cannot be generated by a generative replayer. It's unclear to me why one needs to backdoor training samples to achieve their goal. Indeed, arbitrary random inputs with random labels might be sufficient to achieve the same goal. Note that both attacks are easy to detect if someone inspects the generated data and compare it with labels before using it for training. \n\n3. Experimental results are highly limited. It will be great if the authors can use standard benchmark datasets for continual learning.\n\nIt would be great if the authors could correct my misunderstanding if any. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear to read.",
            "summary_of_the_review": "The problem setting is not clearly motivated, and the proposed solution's validity is unclear. See my major concerns above. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3785/Reviewer_YjaT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3785/Reviewer_YjaT"
        ]
    },
    {
        "id": "twOzbLdimq",
        "original": null,
        "number": 4,
        "cdate": 1667169440016,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667169440016,
        "tmdate": 1669145541797,
        "tddate": null,
        "forum": "a18z-D9l763",
        "replyto": "a18z-D9l763",
        "invitation": "ICLR.cc/2023/Conference/Paper3785/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method to poison the generative models used in DGR in an effort to degrade continual learning methods using DGR.",
            "strength_and_weaknesses": "### Strengths:\n\n1. I agree with the authors that the poisoning attacks on continual learning systems are understudied (especially with the overlap with FL here). \n2. The high-level idea of somehow poisoning the generative model for continual learning seems quite interesting.\n\n### Weaknesses:\n1. The paper is poorly written, and clunky to read. I would highly recommend the authors do a more thorough read-through of the work to make it more easy digest. Note - I added this comment as a weakness, but it did not factor into my recommendation.\n2. The threat model is quite loose - plenty of modern poisoning attacks work in the clean-label regime, as this is far more convincing as a threat, and I am wary of any attacks that leverage manipulated labels. Also, the assumption that the attacker can query the victim model's exact gradient seems unrealistic. At least for some of the works that the authors reference as making similar assumptions, these claims are erroneous and in fact no such assumptions are made.\n3. It's unclear what exactly are novel contributions of the paper - the poisoning method seems to just reuse IAB... From my understanding, the paper's novel contributions are the application of this to the setting of continual learning, which is interesting, but somewhat minor.\n4. It's unclear to me why a trigger is necessary at all? Why not just insert mis-labeled data, with the hope that the generative model produces incorrect data for a certain label during a future replay?\n5. There are a lot of assertions that at best have no justification, and often just seem flat out wrong intuitively. For example: \n    * \"If the trigger is a constant small white square at the image center, most generative models will preserve it\"\n    *  \"Since the mislabeled examples for the current task all carry an input-aware trigger, [...] it predicts accurately on pristine test examples for task t which carry no trigger\"\n6. The experiments are on very simple datasets.\n7. I'm not an expert on DGR, but it would be nice if experiments were done with more commonly used diffusion-based generative models.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the writing could use improvement. The paper seems somewhat novel.",
            "summary_of_the_review": "Because of the strong attack assumptions, and the unsubstantiated claims in the paper, I do not vote for acceptance at this point. However, I think the core idea is quite interesting, and very much encourage the authors to correct the writing errors, address reviewer concerns, and resubmit in the future.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3785/Reviewer_jjXs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3785/Reviewer_jjXs"
        ]
    }
]