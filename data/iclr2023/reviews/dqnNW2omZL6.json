[
    {
        "id": "HGwDhYzILA",
        "original": null,
        "number": 1,
        "cdate": 1666636640715,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636640715,
        "tmdate": 1669277896533,
        "tddate": null,
        "forum": "dqnNW2omZL6",
        "replyto": "dqnNW2omZL6",
        "invitation": "ICLR.cc/2023/Conference/Paper2803/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents an analysis targeting the generalization in graph neural networks. The key idea is to decouple the node's information processing from the message passing operations. To this aim, it is introduced a new neural network paradigm, called PMLP, such that the weight values of a GNN are trained in the MLP form, and then only at inference stage the message passing operations are introduced. The approach is assessed in several tasks for node classification. To shed light on the reasons behind the good performances found in practice, the authors elaborate on the connections between MLP, PMLP, GNN and their respective neural tangent kernels equivalent.\n\nUpdate after rebuttal: I thank the authors for addressing my concerns on the submitted version of the paper. I am happy to confirm my positive score.",
            "strength_and_weaknesses": "Strengths\n* Novelty: the paper brings a new perspective in the analysis of GNN architectures (to the best of my knowledge)\n* The mathematical analysis and the links to the NTK seem sound and interesting\n* The experimental analysis seems comprehensive and relevant for the purposes of the paper\n\nWeaknesses:\n* Despite the paper is clear and the analysis interesting, I find surprising that the weights trained in the MLP work so well in most cases. Does this aspect mean that essentially to achieve a good classification it is sufficient to consider local nodes' information, and structural information encoding can be somehow neglected. It would be interesting to see a discussion on this aspect.\n* Model selection and fine-tuning is not completely clear. In particular, it is not clear to me how the number of layers and neuron per layers are chosen, and if the hyper-parameters are tuned (on the validation set?) individually per each model and per task. Please clarify on this aspect for the sake of reproducibility of the results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is sufficiently clear, and novel.\nA few more information on the experimental settings are needed.\nThe paper presents a few grammar typos.\nI found it strange that a citation to the original GNN work (Scarselli, Franco, et al. \"The graph neural network model.\" IEEE transactions on neural networks 20.1 (2008): 61-80.) is missing in the work.\n",
            "summary_of_the_review": "In my view, the paper seems novel and interesting enough.\nA few more details on the experimental settings (as specified above) can confirm the relevance of the achieved results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2803/Reviewer_JEyU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2803/Reviewer_JEyU"
        ]
    },
    {
        "id": "WQuWuEkIMH7",
        "original": null,
        "number": 2,
        "cdate": 1666699179215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699179215,
        "tmdate": 1666699239521,
        "tddate": null,
        "forum": "dqnNW2omZL6",
        "replyto": "dqnNW2omZL6",
        "invitation": "ICLR.cc/2023/Conference/Paper2803/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper claims the statement that using message passing after propagating MLP can achieve comparable performance to using ordinary graph neural networks. By showing various empirical results, using P(ropagational) MLP and GNNs shows similar performance, and using PMLP is much more efficient than using GNNs. Furthermore, by proposing the theorems about the behavior of PMLP and MLP, authors claim that the ability of extrapolating the samples in PMLP can be treated as a useful property.",
            "strength_and_weaknesses": "**Pros:**\n\nP1. Using PMLP which is much more efficient than ordinary GNNs achieves comparable results, and carry out extensive experiments.\n\n**Cons:**\n\nC1. The proposed methods (PMLP) is already used in APPNP. PMLP might not be a novel approach, and it could be just an extended version of APPNP from transduction to inductive setting.\n\nC2. Since the performance of PMLP is similar to ordinary GCNs, the advantage of using PMLP is not significant. It would be better to specify much more persuasive property of the advantage of using PMLP.\n\nC3. The propose theoretical results is far from the practical scenarios, and the claims in the theorem is ambiguous. Especially, I wonder it is effective to claim the extrapolation behavior of PMLP in inductive setting.\n\nC4. In my knowledge, the authors does not specified how may ML operations are performed during the propagation steps. Because of the over-smoothing problem which is a well-known problem in GNN community, it would be better to specify the numbers. Furthermore, in my opinion, showing the results of using MP operations with residual connection would be important.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. But the proposed approach seems not a novel method.",
            "summary_of_the_review": "I vote to reject this paper because of the novelty of the proposed methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2803/Reviewer_Hjsm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2803/Reviewer_Hjsm"
        ]
    },
    {
        "id": "UfFF3psQ4x",
        "original": null,
        "number": 3,
        "cdate": 1666850336370,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666850336370,
        "tmdate": 1669010336336,
        "tddate": null,
        "forum": "dqnNW2omZL6",
        "replyto": "dqnNW2omZL6",
        "invitation": "ICLR.cc/2023/Conference/Paper2803/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an efficient model, PMLP, which achieves the GNN-level generalization and accuracy at the cost of training an MLP (which is much cheaper than training a GNN). Specifically, the training of PMLP is identical to that of a normal MLP, where edge information in the graph is completely ignored. During inference, PMLP adds the message passing operation to the MLP model obtained during training. Thus, the inference of PMLP is similar to that of a normal GNN. The authors empirically show that PMLPs achieve similar test time accuracy as GNNs. PMLPs even achieve better robustness than GNNs when introducing structural noise to the graph. Theoretical analysis is also presented by analyzing the NTK of MLP, PMLP and GNN. It is shown that the message passing operation makes PMLPs and GNNs generalize differently from MLPs on out-of-distribution test data: while all models degrade to linear extrapolation on OOD data, PMLPs and GNNs have their linear coefficients depend on the node degree to preserve more information from the data. ",
            "strength_and_weaknesses": "## Strengths\n+ The proposed is a simple model capturing some fundamental aspects of the learning process. The model computation process is very easy to understand and efficient to execute. Surprisingly, it achieves very promising empirical results. The fact that we can understand its extrapolation behavior under the lens of NTK is also very interesting. \n+ The paper is novel. While there have been a few tricks in the literature to push GNNs towards MLPs by decoupling the message propagation from the feature transformation, most of them still require the message passing operation during training (e.g., by treating it as preprocessing). PMLP is different from them as the training is equivalent to a vanilla MLP. Theoretical analysis on generalization via NTK also brings some new aspects. \n+ The paper is clearly written and I enjoy reading it. It shows a good mixture of empirical and theoretical studies. \n\n## Weaknesses\n- The proof can be written with more care. I have read most of the proof in detail. I do find some key steps hard to process during my first pass, due to missing details. I still have the following questions:\n    - What does the superscript in $w^{(k)}$ mean? I suppose it denotes the dimension of $w$, and when $k\\rightarrow \\infty$, $w$ corresponds to an infinitely wide network. Is this correct?\n    - What is the dimension of $\\phi^{(1)}$? From the NTK definition, I think it should be a vector so that $(\\phi^{(1)})^T \\phi^{(1)}$ is a scalar. However, from Equation 33, it seems $\\phi^{(1)}$ is a matrix (if $[\\cdot,\\cdot]$ denotes concatenation). \n    - What does $...$ mean in the square brackets of Equation 33? In general, can you please clarify how to derive Equation 33 from Equation 30?\n- It seems Theorem 4 and 5 apply to both PMLP and the original GNN. Therefore there is still a gap to explain why PMLPs achieve performance close to GNNs. Specifically, I think it is an important question why replacing the GNN weights with MLP weights does not hurt accuracy. This question is not addressed by Theorem 4 and 5 \u2013 basically they don\u2019t care about what the weights look like. \n- The setup of the theoretical analysis does not describe a realistic graph OOD scenario. When the test node $x_0$ is far away from the training nodes ($t\\rightarrow \\infty$), it likely won\u2019t have any direct connections with the training nodes. i.e., it may have some multi-hop connections to the training nodes via other test nodes not too far away from the training nodes. In summary, I agree that the linear coefficient of extrapolation will depend on graph structure, but I don\u2019t think $\\tilde{d}\\cdot \\tilde{d_i}$ will be the realistic coefficient. Note that such concern does not apply to Theorem 3 on MLPs, since edges don\u2019t exist in that case. \n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarify\n\nOverall I think the paper is very well written with a good mixture of motivating examples, empirical results and theoretical insights. However, I suggest the authors make a careful pass on the proofs since I find some parts written unclearly (see above). \n\n## Quality\n\nI think the overall quality is high. The proposed model is novel, simple and efficient. The experiments are well designed. The theoretical analysis shows useful insights. \n\n## Novelty\n\nI think the novelty is high. The model itself demonstrates a new training and inference strategy. The theoretical analysis on generalization from the NTK perspective is also interesting. \n\n## Reproducibility\n\nThe experimental setup and the hyperparameter searching procedures are described with sufficient details. Due to the simplicity of the model design, I think others can potentially reproduce the results. \n\n",
            "summary_of_the_review": "Overall, I think this is a good paper. It is a simple model both revealing useful insights and enabling efficient practical deployment. The theoretical results seem correct (need some clarification from the authors). Yet there are some limitations of the analysis as I mentioned above. I vote for acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2803/Reviewer_nT54"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2803/Reviewer_nT54"
        ]
    }
]