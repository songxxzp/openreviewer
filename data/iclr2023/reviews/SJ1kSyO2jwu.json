[
    {
        "id": "hDnr8qUG3M",
        "original": null,
        "number": 1,
        "cdate": 1666501878024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666501878024,
        "tmdate": 1669606738555,
        "tddate": null,
        "forum": "SJ1kSyO2jwu",
        "replyto": "SJ1kSyO2jwu",
        "invitation": "ICLR.cc/2023/Conference/Paper698/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a diffusion based motion modeling method that uses transformers a backbone for the diffusion process. The authors train a general conditional motion generation method using classifier free guidance which allows them to use the same model for conditioned and unconditioned generation. In experiments, the authors perform favorably against other motion generation methods in the tasks for unconditional generation, text-conditioned generation, action-conditioned generation, in-betweening, and motion editing.",
            "strength_and_weaknesses": "Strengths:\n+ New diffusion architecture that combines transformers and diffusion\n+ Quantitatively and Qualitatively outperforms baselines in most of the evaluation tasks.\n+ Nice visualizations of the diffusion process and results\n\n\nWeaknesses:\n\n- Comparison between simply U-Net and the proposed backbone.\nGiven that the architectural novelty is the use of transformers instead of U-Net, I suggest the authors provide an ablation comparing the two design choices. The ablation should compare performance, inference time, and maybe any analysis showing differences of one versus the other. I feel like this is something critical that is missing from the paper, and would be good to have.\n\n\n- Inference time?\nGiven that diffusion models are known for being slow, it would be good to provide the time it takes for inference in comparison to other models. This would show how much room for improvement there is in this aspect of the method, and the tradeoffs of using the proposed method versus the performance gain over the baselines.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and the presentation and results are of high quality. The novelty of the paper seems to be there in mixing transformer backbones with a diffusion process. I don't think there is enough detail to reproduce the work. Things such as number of attention heads, hidden dimension, and other details of the transformer backbone are missing.",
            "summary_of_the_review": "All-in-all, this is a nice paper with good results and nice presentation. However, I am slightly learning towards acceptance due to the concerns presented in the weakness section, and lack of detail for reproducibility. Nevertheless, I am looking forward to the author's response and am willing to increase my score depending on it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper698/Reviewer_Eywo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper698/Reviewer_Eywo"
        ]
    },
    {
        "id": "NR8Becevw1",
        "original": null,
        "number": 2,
        "cdate": 1666634143519,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634143519,
        "tmdate": 1666634143519,
        "tddate": null,
        "forum": "SJ1kSyO2jwu",
        "replyto": "SJ1kSyO2jwu",
        "invitation": "ICLR.cc/2023/Conference/Paper698/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces a denoising diffusion model for both conditional and unconditional human motion (pose sequence) generation. Except for a couple of concurrent arXiv papers, this is the first time diffusion models have been applied to full-body human motion. The paper proposes a transformer-based model that can be conditioned on a text or action embedding in order to synthesize specific motions. The proposed approach makes several design decisions that make the diffusion model amenable to human motion including predicting clean samples rather than noise at each step and applying additional losses to improve motion quality. The model is evaluated on a large variety of tasks including text/action-conditioned generation, unconditioned generation, in-betweening, and motion editing, showing strong performance both qualitatively and quantitatively.",
            "strength_and_weaknesses": "\nStrengths:\n* The adaptation of the diffusion framework is done in a principled way, with reasonable design choices suitable for human motion modeling. For example, predicting the final clean motion from the network instead of the noise at each step is a subtle but important choice, which allows using geometry penalties during training and easy editing through overwriting at test time. Also using classifier-free guidance to control the influence of conditioning. Though no component is particularly novel (i.e. they are adapted from other domains), their combination gives a strong first pass at diffusion for human motion.\n* The approach is novel in the sense that it is the first diffusion model for full body motion (besides concurrent arXiv papers which show diffusion being used in a more limited capacity). \n* The model is demonstrated on a wide range of tasks (text-conditioned, action-conditioned, unconditioned, in-filling, editing). This large task variation really shows the capabilities of diffusion for human motion which may push the community to creatively continue in this direction. \n* Quantitively, the model is competitive or better than several relevant baselines across the evaluated tasks. Qualitatively, the results are quite convincing. Overall, the evaluation is quite thorough.\n* Code to be published.\n\nWeaknesses:\n* A couple components of the architecture were not clearly justified. First, the use of a transformer rather than the usual U-Net. The intro says the transformer \u201cbetter fits the temporal and non-spatial nature of motion data\u201d but a U-Net baseline is not compared against in experiments.  Second, why is CLIP is used rather than some other language-only model (as in TEMOS) to get the text embedding? Is the \u201cvisual\u201d nature of the CLIP latent space actually helpful for human motion?\n* Sec 3 could use a bit more detailed intro to diffusion models for those not familiar, especially since this model is new to human motion.  For example, before Eq (2) it is stated that the model predicts the clean samples rather than the noise, but doesn\u2019t explain that these are equivalent formulations.\n* Eq. 7 combines the clean motion predictions from the network for classifier-free guidance. This is a bit different from the original formulation of Ho & Salimans, which combines the noise prediction from the model. Are these formulations equivalent? (perhaps this doesn\u2019t matter in practice).\n* For the user study in Sec 4.1. it\u2019s not clear what the users were asked to evaluate. Is it the motion quality? Is it how well the motion matches the text? Ideally both should be evaluated.\n* Sampling from the diffusion model is quite slow (~1min for a sequence) as discussed in the conclusion. However, there has been much recent work on speeding up sampling lately, so this can be improved in future work.\n\nQuestions, comments, and clarifications (not considered in rating):\n* How many parameters are in the architecture? Is the transformer considered \u201clightweight\u201d because it is faster or smaller than U-Net?\n* Intro: saying \u201cnon-spatial nature of motion data\u201d is a bit confusing. Joints are positions which are by definition \u201cspatial\u201d. Maybe the authors meant the data is less structured, but even so joints are part of the kinematic tree structure.\n* Related work: Gu et al [Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion, CVPR 2022] use a similar diffusion transformer for pedestrian trajectory prediction.\n* In practice, how long can generated sequences be before motion quality starts to degrade? Or can it truly handle \u201carbitrary\u201d lengths?\n* If both joint positions and rotations are available in the pose, which are actually used as the output?\n* Since there are so many evalution metrics, it would be good to define them in the appendix for easy access.\n* It would be interesting to see how classifier guidance works for text and action-to-motion instead of having to re-train separately for each task. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written despite a few missing details (mentioned above), and I think the quality of the work is high considering the principled design taken for the proposed diffusion model and extensive experiments. As mentioned above, the work is novel in how it combines previous diffusion components for human motion and I think it will inspire many followup works.\n\nBetween the paper details and code (promised to be released), the paper is reproducible.\n",
            "summary_of_the_review": "Overall, the paper clearly introduces and thoroughly evaluates a strong diffusion model for human motion. Despite some small weaknesses, I think the qualitative results and flexibility of the model make it very useful for the community. Therefore, I recommend accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper698/Reviewer_W5bV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper698/Reviewer_W5bV"
        ]
    },
    {
        "id": "XxrfeoOUo3d",
        "original": null,
        "number": 3,
        "cdate": 1666675199324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675199324,
        "tmdate": 1666675199324,
        "tddate": null,
        "forum": "SJ1kSyO2jwu",
        "replyto": "SJ1kSyO2jwu",
        "invitation": "ICLR.cc/2023/Conference/Paper698/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a diffusion model for synthesizing human motion parameters. The proposed model consists of a diffusion model adapted to the human motion data format, plus task-specific geometric losses. The proposed model is applied to multiple scenarios such as text-to-motion, action-to-motion, motion editing, and unconstrained synthesis. Extensive quantitative and qualitative evaluations show the promising performance gain achieved by the proposed model.",
            "strength_and_weaknesses": "Paper strengths:\n- The paper presents a novel method, which is among the first to apply diffusion models to human motion synthesis. The method is well-thought, containing multiple non-trivial design choices to make the method work well on human motion data.\n- The paper is well-written and easy to follow. The demo is high-quality and well-made.\n- The experimental evaluations are thorough and convincingly demonstrate the effectiveness of the proposed model.\n\nPaper weaknesses:\n- Intuitively, the synthesized human motion should be varying-length. However, this does not seem to be addressed in the paper. I assume the clip length was fixed and sequences were temporally stretched/squeezed to meet the specified length. It would be great if additional discussions can be added.\n- It would be great to see a more detailed analysis of each particular geometric loss term and the influence of its weighting.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and has relatively high quality. The presented work appears original, although its arxiv version can be easily located via a search engine, I tend to believe the submission is not a duplicate work if the author lists are the same.",
            "summary_of_the_review": "Overall I think this is a strong paper, which presents the first steps of applying diffusion models to human motion synthesis. The presented method is technically sound and contains multiple insightful details. The paper's evaluation is comprehensive and its demonstration is well-executed. My concerns are minor and can be easily addressed. Therefore, my initial recommendation is to accept the submission.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper698/Reviewer_Cmzb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper698/Reviewer_Cmzb"
        ]
    },
    {
        "id": "7bYOy0LUe9F",
        "original": null,
        "number": 4,
        "cdate": 1666690833457,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690833457,
        "tmdate": 1666690833457,
        "tddate": null,
        "forum": "SJ1kSyO2jwu",
        "replyto": "SJ1kSyO2jwu",
        "invitation": "ICLR.cc/2023/Conference/Paper698/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the Motion Diffusion Model (MDM) for multi-modality human motion generation, such as text-to-motion, action-to-motion, and unconditioned generation, etc. MDM is transformer-based and adopts an existing diffusion method that predicts the signal itself at each timestep instead of the noise so that geometric losses could be used. MDM achieves impressive results on several motion generation tasks, and requires lower computation resources at the same time.",
            "strength_and_weaknesses": "Pros\n- A simple and general framework, along with some good-quality generation results. \"simple\" is for the model simplicity, and \"general\" is for the ability to enable different forms of condition (text, action, editing...) \n- Lightweight compared to other multi-modality motion generation works.\n- Easy to read and understand.\n- Rich demos.\n\nCons\n\nThe necessity of the diffusion model is not clearly presented, except for adding geometric losses. The motivation for using diffusion models which predict the signal itself instead of the noise still needs a discussion. If the reason is that geometric losses could be added, given that we supervise each MDM with the GT $x_0$, it seems that there is no need to stack MDM for the diffusing process. \n\n- What will the result be when $T=1 or  2$ both for training and testing, compared to $T=1000$?  If there is just a small difference, it seems that the good performance is contributed to the transfomer encoder and CLIP, which has been widely used in many works.\n\n- Another concern is about the low diversity caused by predicting the signal at each step, which seems opposite to the original goal of diffusion models.\n \n- Have the authors tried to predict $\\epsilon$ and not use geometric losses in MDM? \n\nAblation experiments on diffusion models are appreciated and needed since they could help readers understand the motivation for introducing the diffusion model better. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and easy to understand, but the motivation for using diffusion model (predicts signal) is not clear yet.\n\nThe quality is good shown in the demos. However, some ablation experiments are missing (stated above), which is important.\n\nThe proposed method is somewhat novel and the code will be published. ",
            "summary_of_the_review": "Overall, I lean to accept this paper at this time, since it is a simple and general framework with promising results in several motion generation tasks. However, I'm still confused about the necessity of introducing diffusion here, especially in predicting the signal itself. My rating may change depending on the experiment results I mentioned above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper698/Reviewer_Lxvz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper698/Reviewer_Lxvz"
        ]
    }
]