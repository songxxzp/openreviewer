[
    {
        "id": "LeSZiHJauT",
        "original": null,
        "number": 1,
        "cdate": 1666251583774,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666251583774,
        "tmdate": 1666251583774,
        "tddate": null,
        "forum": "Q0XkE_srKnG",
        "replyto": "Q0XkE_srKnG",
        "invitation": "ICLR.cc/2023/Conference/Paper3316/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an adversarial loss function between image and video domain to unsupervisely learn the fine-grained representations from labeled images and unlabeled videos for video segmentation. The authors show that their proposed method can improve the performance of both the CNN and transformer based methods on video segmentation.",
            "strength_and_weaknesses": "Strength:\nThe idea of narrowing representation variance between image and video data is interesting.\n\n\nWeakness:\n1.The experimental results are not convincing.\n1)Selection of the dataset is strange. The authors require large-scale video segmentation dataset with common classes in COCO, but they use the subset of the DAVIS 2019 dataset and the small-scale FBMS dataset, ignoring large ones like the Youtube-Objects Dataset which would be better qualified.\n\n2)Performance for baseline methods on the DAVIS dataset is rather low, comparing with state-of-the-art results, e.g. 65.6% J mean on 2019-val by [1]. Effectiveness of the proposed loss function on such algorithms does not guarantee the effectiveness on top-performing ones.\n[1]Luiten, J., Zulfikar, I.E., & Leibe, B. (2020). UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking.\u00a02020 IEEE Winter Conference on Applications of Computer Vision (WACV), 1989-1998.\n\n2.The authors claim that \u201dSome video artifacts such as motion blur are most apparent around moving object boundaries. When applied to tokens at the global level, the adversarial loss operates over the entire spatial resolution and may not target these features well. To address this we place our adversarial loss directly after the patch embedding layer to operate over low-level features containing detailed spatial information. We apply the loss per-token right after the patch embedding layer.\u201d in Section 3.2, but I cannot find the demonstration of this statement.\n\n\n3.Experiment, performance not convincing.\nThere are many typos in this manuscript, some frequent errors are:\n1)inconsistent verbal forms, e.g. sometimes \u2018labelled\u2019, sometimes \u2018labeled\u2019;\n2)lack of commas, e.g. \u201cTo prevent the loss of discriminative semantic class information  (short of a comma) we apply our ...\u201d in the abstract section, \u201cTo deal with this problem  (short of a comma) researchers often use ...\u201d in the introduction section;\n...",
            "clarity,_quality,_novelty_and_reproducibility": "The ideas are clear, novel and reproducible.",
            "summary_of_the_review": "The idea of this paper is interesting, but it is not well demonstrated.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3316/Reviewer_KyqK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3316/Reviewer_KyqK"
        ]
    },
    {
        "id": "wSc-yq7QOd",
        "original": null,
        "number": 2,
        "cdate": 1666588937145,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588937145,
        "tmdate": 1670487581469,
        "tddate": null,
        "forum": "Q0XkE_srKnG",
        "replyto": "Q0XkE_srKnG",
        "invitation": "ICLR.cc/2023/Conference/Paper3316/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method to improve semantic segmentation performance on single images from videos, from a network trained on still images. The argument is that videos typically have several distribution shifts from standard images, such as framing and motion blur. Therefore, applying a pre-trained segmentation network trained on still images results in sub-par performance on videos. The proposed method is to apply an adversarial loss on the features from the segmentation network, where the discriminator tries to discern whether the input is from a still image or a video. This allows the network to learn the image distribution of videos. Experiments are performed with networks trained on COCO, and tested on the Davis dataset, using both CNN and transformer architectures. Ablations also show that the improvement is not evident when training and testing on still image datasets.",
            "strength_and_weaknesses": "Overall, the motivation of this work is quite interesting, and the idea that we can improve inference on videos due to the distributional shift is quite novel. The overall method is quite simple, at least in theory, although adversarial losses can be difficult to implement and train. The experiments show strong improvements over the baseline, and also demonstrate that this is not a general method for all segmentation networks (there must be a distributional shift), via the ablation.\n\nOverall, this would be more compelling if more datasets were evaluated on. In addition to number, there are also 'video' datasets such as Cityscapes, where the images were recorded in a video manner, but do not seem to exhibit the same issues one might see in Davis. Would we expect to see similar improvements for those datasets? Or is there more clarification needed on the exact class of issues that we would expect this method to improve upon?\n\nAnother topic of interest is that this does not seem like a method that is limited to image -> video transfer, but should, in theory, work on any two sets of inputs from different domains (E.g. daytime -> night time). Would this method generalize to those cases?",
            "clarity,_quality,_novelty_and_reproducibility": "The method is well motivated and very clear. The motivation is perhaps a little unintuitive at first, but the method is simple (not a bad thing), and the results defend the story well.",
            "summary_of_the_review": "Overall, this is a nice simple method which shows strong improvements on a perhaps novel problem. I would be interested in knowing if this method would generalize to other distributional shifts (and perhaps more interested in why not, if so).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3316/Reviewer_79eQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3316/Reviewer_79eQ"
        ]
    },
    {
        "id": "0-_EpLVn1_i",
        "original": null,
        "number": 3,
        "cdate": 1666894883754,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666894883754,
        "tmdate": 1666895331794,
        "tddate": null,
        "forum": "Q0XkE_srKnG",
        "replyto": "Q0XkE_srKnG",
        "invitation": "ICLR.cc/2023/Conference/Paper3316/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper performs video segmentation via labeled images and unlabeled videos, in which models in the image domain is adapted to the video domain via an adversarial loss. Ablations are conducted to determine at which level(s) the loss can be added to optimize the performance.",
            "strength_and_weaknesses": "Strength:\n\n+ The paper is in general easy to understand, despite the ambiguity in task definition (see Clarity).\n+ Ablations are performed and interesting founding is presented.\n\nWeaknesses:\n\n- Using adversarial losses to adopt image model to video domain is not new. The idea of clustered loss is also incremental.\n- The claim on most self-supervised video representations focus on the recognition level, isn\u2019t accurate. There are quite a lot of self-supervised video correspondence learning works that highly related to video segmentation task (i.e., most evaluate on the video segmentation task). [1] leverages self-supervised video correspondence technique to semi-supervised VIS. However, all above is not mentioned at all.\n- Seems no STOA approaches are compared, and the baseline's performance is rather low.\n- Davis is not a ideal dataset for semantic segmentation since most of the sequences contain very few objects, and almost one single object per category.\n\n[1] Fu et al. Learning to Track Instances without Video Annotations. CVPR 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & Quality: The description is unclear at many points. 1. The problem definition is not well presented. It took me a long time to figure out whether the task is for semantic, or instance segmentation (i.e., it is figured out from (3)). This ambiguity leads to many questions when reading this draft: for instance segmentation, transferring from image to video domain needs to consider instance association, which is not mentioned anywhere; for semantic segmentation, using DAVIS is not ideal.  2. Many figures are blurry with too tiny texts, figure 1 is almost uninformative. \n\nNovelty: This paper falls into the large category of adopting an adversarial loss to mitigating image-video domain gaps, that is investigated by tons of works in recent years. The idea is not new.\n\nReproducibility: the work seems reproducible according to the description.",
            "summary_of_the_review": "Based on the weaknesses and the various concerns discussed in the Clarity, Quality, Novelty And Reproducibility, I would not recommend acceptance per its current states.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3316/Reviewer_AueS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3316/Reviewer_AueS"
        ]
    },
    {
        "id": "LHDPV_sc__z",
        "original": null,
        "number": 4,
        "cdate": 1666981853039,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666981853039,
        "tmdate": 1666981853039,
        "tddate": null,
        "forum": "Q0XkE_srKnG",
        "replyto": "Q0XkE_srKnG",
        "invitation": "ICLR.cc/2023/Conference/Paper3316/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approach for training a video segmenter by combining image segmentation datasets with unlabeled video datasets. The main innovation is to treat the problem effectively as a domain adaptation problem. As such, the paper uses an adversary to ensure that features from images and features from video frames are indistinguishable. An additional innovation is to perform this adversarial loss on combined clusters of features from videos and images so that the differing frequency of categories in image and video does not cause a problem",
            "strength_and_weaknesses": "Strengths:\n- The idea of using clustering of features to avoid penalizing differing class frequencies in the two domains is a clever one. This is a nice idea for other domain adaptation problems as well.\n- The end result is quite good performance on video segmentation.\nWeaknesses:\n- The novelty of this work is somewhat limited. It is mainly about applying existing domain adaptation approaches to this problem. This might be enough if the application itself was innovative or surprising, but I am not sure that is the case; the motivation seems very much to be that video frames and images are different domains.\n- Is video segmentation only about image frame segmentation? I feel that motion-based cues should also play a role, and the fact that they don't seems to be more an issue with the dataset rather than the problem. In any case, I find the prospect of framing video segmentation as solely a different domain for image segmentation somewhat limiting.",
            "clarity,_quality,_novelty_and_reproducibility": "As above, I think the novelty is limited. On the positive side it is quite clear.",
            "summary_of_the_review": "The paper proposes to use domain adaptation for video segmentation. While it has some clever ideas, I don't find this framing of video segmentation as domain adaptation particularly compelling or interesting enough to justify acceptance.\n\nAs a recommendation, I would suggest that the authors consider expanding the idea of clustered adversarial loss to more domain adaptation applications. I think that might make for a more valuable contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3316/Reviewer_SNu6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3316/Reviewer_SNu6"
        ]
    }
]