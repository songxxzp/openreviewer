[
    {
        "id": "Wgtbk-W6nA",
        "original": null,
        "number": 1,
        "cdate": 1666516715085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666516715085,
        "tmdate": 1668755650916,
        "tddate": null,
        "forum": "ULnHxczCBaE",
        "replyto": "ULnHxczCBaE",
        "invitation": "ICLR.cc/2023/Conference/Paper4752/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the convergence of AdaGrad and its variants over the convex objectives. Specifically, the authors first prove the convergence of AdaGrad-Norm both in the deterministic case and in the stochastic case. Compared to existing results, the derived results remove the bounded domain assumption. The authors then propose two types of variants of AdaGrad. The first type can ensure the last iterate convergence in the deterministic case and the second can achieve a faster convergence rate.",
            "strength_and_weaknesses": "**Strength**\n\n1. The presentation of this paper is clear and it is easy to understand the results of this paper and the proof technique.\n\n**Weakness**\n\n1. I am not sure whether understanding the convergence of AdaGrad in the convex regime is an important question. First, to the best of my knowledge, AdaGrad is seldomly adopted in machine learning practice, especially in deep learning. Second, I think that it is more important to study the non-convex setting.\n\n2. The current logic and organization of this paper need to be improved. The current organization of this paper seems to \"collect\" all the results. For example, AdaGrad-Norm Last is proposed to ensure the last iteration convergence of AdaGrad-Norm. But does AdaGrad-Norm really fail to converge in terms of the last iteration or it is just due to the limitation in proof? Discussion is needed here.\n\n3. While removing the bounded domain assumption, the result for the stochastic case is still somewhat restricted. For convex objectives, SGD is known to converge with the bounded noise variance assumption, which is strictly weaker than the sub-gaussian noise assumption. Can similar results be derived for AdaGrad-Norm?\n\n4. While Section 1.1 claims \"show an explicit non-asymptotic convergence rate of AdaGrad-Norm and AdaGrad on $R^d$ in the deterministic setting\", I only find such a result for AdaGrad-Norm (Theorem 3.1). Did I miss something?\n\n5. I feel that the authors need to include some experiments to justify the usefulness of the proposed AdaGrad-Norm variants. For example, does AdaGrad-Norm-Acc really converge faster than AdaGrad-Norm over convex objectives? Also, what is the advantage of AdaGrad-Norm-Acc over Nesterov's momentum given that they can both achieve the lower bound of the convergence rate?\n\n6. There are missing related works. Faw et al. (2022) derive the convergence of AdaGrad-Norm in the non-convex setting with the least assumptions. They also provide detailed references to the works studying the convergence of AdaGrad.\n\n**References**\n\nFaw et al., The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance, 2022",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe writing of this paper is clear and easy to understand.\n\n**Novelty**\n\nTo the best of my knowledge, there are no such results in the existing literature. However, the novelty can be further improved as discussed in the \"Strength And Weaknesses\" part.",
            "summary_of_the_review": "Despite that the results do improve those in the existing literature, I find that the studied problem is less interesting, the theoretical results may be further improved, and the proposed algorithms need to be justified in more detail. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4752/Reviewer_qVCv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4752/Reviewer_qVCv"
        ]
    },
    {
        "id": "atuT1NekulR",
        "original": null,
        "number": 2,
        "cdate": 1666666013359,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666013359,
        "tmdate": 1668793881834,
        "tddate": null,
        "forum": "ULnHxczCBaE",
        "replyto": "ULnHxczCBaE",
        "invitation": "ICLR.cc/2023/Conference/Paper4752/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces some new analysis technique of the convergence of AdaGrad on the unbounded domain, in the smooth convex optimization setting. The authors prove that several variants of AdaGrad can converge (in both average and the last literature sense) to the optimum without the need of additional assumptions. ",
            "strength_and_weaknesses": "Strength:\n  1. The proposed analysis techniques are, as far as I am aware, novel and interesting\n  2. The authors have formulated the analysis in a way that is easy to understand their contributions\n\nWeaknesses:\n\n  Before I start talking about the weakness, I want to emphasize that, as a theory person, I highly appreciate the authors for trying to figure out different ways to provide the convergence analysis of algorithms. Although I don't really have enough time to read all the proof details, the analysis that the authors try to provide are very interesting and potentially impactful. \n\n  However, I have the feeling that the paper is a bit over-selling, or the results presented are not really matched with the authors' claims. Most of the paper is concentrated on a variant of AdaGrad, AdaGradNorm, which I admit that I am not familiar with. I think it is not very popular, and the analysis techniques that the authors have used for this algorithm, maynot be easily transferrable to the vanilla AdaGrad algorithm. I tried to read some parts of the Appendix as well, and it seems that the only setting that the authors have provided analysis on AdaGrad is, the deterministic setting (Sec A.3). The stochastic version of AdaGrad is not analyzed.\n\n  Moreover, the nice properties and the variants are all proposed based on the AdaGradNorm algorithm, instead of the original AdaGrad algorithm. Can we propose similar variants for AdaGrad and get similar properties? This makes it also hard to interpret the results as a more advanced/detailed analysis of AdaGrad, but rather AdaGradNorm. Therefore, I feel the results in this paper are important, but the way the authors sell them are not completely aligned with the results.\n\n  Finally, the results are established under the quasar-convex setting, and I wonder whether they can be further extended to the nonconvex setting. (I have a very strong feeling that this can be done easily since in the nonconvex setting, similar things such as the telescopic sums in Sec 3.1 appear in the analysis too). Some (simple and synthetic) experimental results that support the correctness of the theorems will be highly appreciated.\n\n\n*After Rebuttal*\n\nI thank the authors for a nice rebuttal. I have increased my rating.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is a little unclear, very novel, and reproducible",
            "summary_of_the_review": "In summary, I do like the idea of the paper as a theory person. However, I just feel like it should be further improved in its current form. \n\nI highly appreciate the authors for trying to find new ways to prove the convergence of AdaGrad, and its variants, and I believe the results could be very significant. Extending the current results to, e.g., stochastic AdaGrad, last iterate convergence of AdaGrad would be much more interesting than dealing with the variant AdaGradNorm. Or extending the results to the nonconvex setting would be very interesting. \n\nThat being said, the above suggestions may make the paper a bit too-long and too technical for a (mostly deep-learning) conference like ICLR, I would suggest re-submitting to ML journals like JMLR, or theory conferences like COLT.\n\nI am happy to change my rating if the authors can convince me of their contributions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4752/Reviewer_sbsF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4752/Reviewer_sbsF"
        ]
    },
    {
        "id": "Oo4GJlIOx4B",
        "original": null,
        "number": 3,
        "cdate": 1666757942922,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666757942922,
        "tmdate": 1666757942922,
        "tddate": null,
        "forum": "ULnHxczCBaE",
        "replyto": "ULnHxczCBaE",
        "invitation": "ICLR.cc/2023/Conference/Paper4752/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a theoretical analysis of AdaGradNorm which is a variant of AdaGrad in the unconstrained domain. As much as it is known this is the first analysis that extends the convergence analysis of Adagrad to the unconstrained domain. They analyze both deterministic and stochastic settings. The analysis is provided for the quasar-convex that is bigger than the class of convex functions and includes it. Finally, they also propose accelerated variants of AdaGradNorm in the deterministic setting with convergence analysis which indicates the acceleration occurs compared to Non-accelerated variants. \n",
            "strength_and_weaknesses": "As much as it is known this is the first analysis that extends the convergence analysis of Adagrad to the unconstrained domain. ",
            "clarity,_quality,_novelty_and_reproducibility": "Although your title is about analyzing AdaGrad the analysis is actually for AdaGradNorm. I suggest either changing the title or extending your analysis for the vanilla/general AdaGrad. ",
            "summary_of_the_review": "\nComments: \n1- As an empirical justification, it would be useful to include some empirical results for your accelerated variant in the paper. \n\n2- Although your title is about analyzing AdaGrad the analysis is actually for AdaGradNorm. I suggest either changing the title or extending your analysis for the vanilla/general AdaGrad. \n\nMinor comments: \n1- Adding numbers for your important equations would be helpful. \n2- In the 2nd last eq in page 6, I guess a \\sum is missing. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4752/Reviewer_iRiy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4752/Reviewer_iRiy"
        ]
    }
]