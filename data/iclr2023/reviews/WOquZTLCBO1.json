[
    {
        "id": "ayACPlVnYS",
        "original": null,
        "number": 1,
        "cdate": 1666415904970,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666415904970,
        "tmdate": 1666415904970,
        "tddate": null,
        "forum": "WOquZTLCBO1",
        "replyto": "WOquZTLCBO1",
        "invitation": "ICLR.cc/2023/Conference/Paper4525/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel approach to pessimism-based offline RL. Instead of the standard practice of explicitly constructing a lower confidence bound for value functions, this new approach uses perturbed rewards to implicitly quantify the training uncertainty and construct lower confidence bounds for the ground truth. Such an approach is argued to improve the practicality of pessimistic offline RL especially in situations where the value functions are approximated by large neural networks, whence the large scale model elucidates traditional theoretical analysis. The theoretical property of this approach is studied in overparametrized neural networks with gradient descent training. Empirical experiments are conducted to show the favorable performance of the proposed approach. ",
            "strength_and_weaknesses": "Strengths:\n\n1. A novel algorithm to pessimism. \n\nThis work contributes to the literature of offline RL by proposing a solid and practical approach to constructing confidence lower bounds. This idea is clever and refreshing. It is both practically useful and theoretically interesting. \n\n2. Solid theoretical results.\n\nThis paper provides solid theoretical analysis. It is quite invovled but I think it will be useful for future works in the literature. \n\n\nWeaknesses:\n\n1. Clarity of results. \n\nA drawback (which I think can be resolved to some extent) is that although the overall idea is clean, the presentation is way too overwhelming. For example, it will help the readers if important steps in the algorithms can be explained / highlighed / commented, or given more wording in explaining their roles. Also, Theorem 1 poses a huge challenge to the whole flow as the conditions are too complicated. I will suggest replacing it with an informal theorem, and put all these conditions to the appendix. \n\n2. Relation to the literature. \n\nAlthough the authors generally did a good job in relating to the literature, more discussion on the novelty of this idea can further improve this work and posit this paper appropriately in the literature. For instance, 1) how does the construction of uncertainty quantification differ from that in the online setting (e.g., Jia et al. 2022)? Is there special tricks for dealing with distribution shift? 2) how does the analysis of 2-layer neural networks rely on existing ones, and which parts are specific to the new offline setting? etc.\n\nQuestions:\n\n1. Since $\\mathcal{Q}^*$ is used for fitting the networks, I am curious about the definition of $\\mathcal{Q}^*$. Is it equivalent to the class of 2-layer neural networks with ReLU activation, or is it a superset of it? \n\n2. What is the relationship between $\\mathcal{H}_{ntk}$ and the class of 2-layer NN with ReLU activation? Please clarify this to help the readers.\n\n3. The conditions in Theorem 1 are too lengthy. Is it possible to reduce it to a cleaner version (potentially with some harmless relaxation)?\n\n4. It will help if there is a sketch or overview of the key theoretical techniques in Theorem 1. For example, why do the randomized rewards lead to a valid uncertainty quantifier. \n\nMinor issues: \n\n1. The last sentence in Section 4 is difficult to read. What does it mean by `` with a provable implicit uncertainty quantifier and $\\tilde{\\mathcal{O}}(1/\\sqrt{K})$''?\n\n2. Remark 1 is a bit difficult to read. What does it mean by ``data-dependent quantity that measures the number principle dimensions over which the..\"?\n\n3. What does $\\sigma'$ mean in Definition 1? Please provide a formal definition.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is of high quality in general. The theoretical analysis and empirical results are solid. \n\nThe paper is clearly written and properly posited in the literature. \n\nThough the idea of randomized rewards and pessimism have appeared in the literature, the idea of combining them and the theoretical / practical considerations therein are fresh. ",
            "summary_of_the_review": "I find this to be a strong paper. The idea of using randomized rewards in pessimism-based offline RL is clever, and the theoretical analysis is nontrivial and solid. Empirical studies are conducted to support the arguments. However, I think the presentation of the results can be improved, and more discussion on the relation to the literature is need. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethics concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4525/Reviewer_EhDB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4525/Reviewer_EhDB"
        ]
    },
    {
        "id": "zbFpx_5OhOH",
        "original": null,
        "number": 2,
        "cdate": 1666598127239,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598127239,
        "tmdate": 1666598127239,
        "tddate": null,
        "forum": "WOquZTLCBO1",
        "replyto": "WOquZTLCBO1",
        "invitation": "ICLR.cc/2023/Conference/Paper4525/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the PEturbed-Reward Value Iteration (PERVI) which combines the randomized value function idea with the pessimism principle. PERVI only needs $O(1)$ time complexity for action selection while LCB-based algorithms require at least $\\Omega(K^2)$, where $K$ is the total number of trajectories in the offline data. It proposes a novel data splitting technique that helps remove the potentially large log covering number in the learning bound. PERVI yields a provable uncertainty quantifier with overparameterized neural networks and achieves an $\\tilde{O}(\\frac{\\kappa H^{5/2}\\tilde{d}}{\\sqrt{K}})$ sub-optimality. statistical and computational efficiency of PERVI is validated with an empirical evaluation in a wide set of synthetic and real-world datasets.",
            "strength_and_weaknesses": "Strength: This paper combines the randomized value function idea and the pessimism principle. In addition, this paper proposes a novel data splitting technique that helps remove the dependence on the potentially large log covering number in the learning bound. The authors empirically corroborate the statistical and computational efficiency of our proposed algorithm in a wide set of synthetic and real-world datasets. \n\nWeakness: 1. Your assumption 5.1 requires for any $H+1$-bounded function, Bellman update maps it to $Q^*$. What will happen if your function has $\\epsilon$-misspecified error, i.e. $\\inf_{V}\\sup_{||V'||_\\infty\\leq H+1}\\|\\|B_h V-V'\\|\\|_\\infty \\leq \\epsilon$, how will the $\\epsilon$ model error affect your results? You don't need to derive the result for this case but explain in a few sentences is fine.\n\n2. While Appendix B.1 already provide a nice comparison with Xu&Liang,2022, they consider a different setting where only trajectory reward is observed. If the per-step award is aware, how would their result be like and how would it compare to PERVI? I am asking this since in your setting per-step reward is aware, so I am wondering which of the two methods would be better under this case.\n\nMinor: The recent paper https://arxiv.org/pdf/2210.00750.pdf also considers the pessimism offline RL with parametric function class. How is your result compared to theirs since both results contain the similar measurement $\\sum_{h=1}^H ||g(x;W_0)||_{\\Lambda_h^{-1}}$ in the suboptimality bound? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper is clear and the quality of this paper is high. ",
            "summary_of_the_review": "This paper overall has a high quality and provides complete study of neural network approximation for offline RL. If the authors can address my concern in above, I am happy to keep my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4525/Reviewer_Svop"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4525/Reviewer_Svop"
        ]
    },
    {
        "id": "wDeIGH3AelQ",
        "original": null,
        "number": 3,
        "cdate": 1666633079777,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633079777,
        "tmdate": 1666633079777,
        "tddate": null,
        "forum": "WOquZTLCBO1",
        "replyto": "WOquZTLCBO1",
        "invitation": "ICLR.cc/2023/Conference/Paper4525/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper looks at the problem of reinforcement learning from offline data.\nThey authors introduce PERVI, which uses \"randomized value functions\" to generate an approximate posterior distribution over value functions, and then acts pessimistically with respect to those estimates for safety.\nThe authors support their new algorithm through an analysis in tabular MDPs, as well as more empirical evaluation with neural network function approximation.",
            "strength_and_weaknesses": "There are several things to like about this paper:\n- The problem of RL and decision making with offline data is an important one for the community.\n- The PERVI algorithm passes a \"sanity check\" intuitively... by this I mean that it's not just an algorithm for a proof... but you have a sense this is something close to something someone would actually want to use.\n- The quality of the writing and presentation in the paper overall is very high.\n- The paper has a progression of intuition, to hard theoretical guarantees in simple settings, to empirical success in more complex settings. I like the way of analysing problems with overparameterized NTK!\n- Discussion of related work and the key intuitions for the approach appear to be pretty comprehensive for a short paper, although I am likely missing important pieces.\n\nThere are some places where the paper probably could be further strengthened:\n- In some sense, many of the results and analyses are sort of incremental. The application of randomized value functions plus pessimism has existed before, but this is a slightly new twist on that as opposed to a \"game changing\" new perspective.\n- Some of the ways the Theorems are presented are really messy... you need to read through lines and lines of bizarre constants/terms to even get to the result!\n- Something must be missing (at a high level) from these theorems, since they don't really expose clearly a dependence on the *quality* of the offline data... an algorithm should be able to learn very differently when the demonstration data is very good, versus when it is very bad... and a good algorithm should be able to kind of work that out and leverage it.\n- Why do you use the term \"SubOpt\" instead of regret?\n- Does PERVI (pervy?) raise some of the issues that NeurIPS (nips) had to deal with?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall I think the paper scores highly across these attributes.",
            "summary_of_the_review": "This paper presents PERVI, a combination of randomized value functions with pessimism for offline reinforcement learning.\nThe paper takes mostly-existing concepts but combines them in a more elegant and coherent framework than previous papers I have seen.\nThe resultant algorithm is sane and sensible, and the progression of support from intuition to analysis to empirical results is comprehensive.\n\nIn terms of \"the grand challenge\" of offline RL I think this algorithm leaves potentially a lot of value on the table.\nHowever, I do think this is a valuable piece of the literature and will be useful to the conference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4525/Reviewer_69m1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4525/Reviewer_69m1"
        ]
    },
    {
        "id": "owI-jYTEZ6",
        "original": null,
        "number": 4,
        "cdate": 1666634631653,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634631653,
        "tmdate": 1666712993201,
        "tddate": null,
        "forum": "WOquZTLCBO1",
        "replyto": "WOquZTLCBO1",
        "invitation": "ICLR.cc/2023/Conference/Paper4525/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies offline RL problems with perturbed rewards. In particular, the Q function will be parametrized as the minimum of $M$ neural networks, trained on $M$ perturbed datasets. The benefit of the proposed algorithm, compared to the UCB-based method is reducing the time complexity of action selection. On a technical side, the authors propose a data splitting analysis technique to improve dependence on log covering the number in the sample complexity result.",
            "strength_and_weaknesses": "Strength:\n\nThe proposed neural network-based offline RL algorithm is new. In appendix B3 the authors compare with existing literature on the sample complexity results. \n\nWeakness:\n\nI did not spot any major errors in the paper. However, I have several minor technical questions:\n\nQ1. In line 10 of Algorithm 1, shouldn't we take argmax over action space?\n\nQ2. A related question is, how is the argmax implemented in code since we are considering the large space-action space?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written with sufficient novelty.",
            "summary_of_the_review": "The idea of learning Q functions from the perturbed datasets and thus implicitly implementing the pessimism principle is neat. Theoretical analysis and preliminary experiments illustrate the potential usefulness of the proposed algorithm. I suggest a marginally accept. However, I'm not familiar with the main related papers cited in this work, so I choose a confidence score of 2.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4525/Reviewer_NEMf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4525/Reviewer_NEMf"
        ]
    }
]