[
    {
        "id": "SAgdxxDphZ",
        "original": null,
        "number": 1,
        "cdate": 1666568151684,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666568151684,
        "tmdate": 1666572289526,
        "tddate": null,
        "forum": "_d2f3hRn0hT",
        "replyto": "_d2f3hRn0hT",
        "invitation": "ICLR.cc/2023/Conference/Paper1331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a novel generative model GA-NTK that mitigates the drawbacks of traditional GANs trained with the alternative SGD method. This is achieved by using a closed-form discriminator based on a neural tangent kernel (NTK) instead of a neural network. The author proved the convergence of the proposed model during the training phase. Also, numerical experiments are conducted to show the efficacy of the proposed method.",
            "strength_and_weaknesses": "*Strengths*\n- The motivation of the proposed method is clearly stated. The mathematical settings of the proposed method are clearly stated. The overall writing is fluent and easy to understand.\n- Numerical experiments of generating new samples with the method are conducted. Also, the result is consistently evaluated against state-of-the-art methods.\n\n*Weaknesses*\n- The statement of the method could be made better if the training process of the proposed method is more clearly stated. In particular, I'm confused how the proposed discriminator is trained (or not trained) during the overall training process, e.g. what are the variables to be updated during training? \n- Since the main advantage of the proposed method is to substitute traditional neural network-based discriminator with kernel methods, it would be plausible to include the comparison of the discriminator performance between the two methods. \n- The scalability of the proposed method is one of the drawbacks of the proposed method. In particular, the proposed discriminator has a space complexity of O(n^2) where n is the number of data points. \n- The paper makes the assumption that mode collapse is solely caused by min-max training regime in traditional GAN methods. But mode collapse can be caused by other factors as well, such as the structure of generators. A more thorough study of the causal relationship between min-max training and mode collapse could be plausible to provide.\n- In the experiment part, generated images and their most similar images from the dataset are provided. It looks to me that the GA-CNTK method tries to memorize the training samples and the pixel level and not able to learn high-level semantics from the images.\n- typo on page 4: \"my be desirable\" -> \"might be desirable\"",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper's writing has good clarity. The method is novel as it provides a different training regime by introducing a new class of discriminators. \n- code and network architectures are provided to improve reproducibility.",
            "summary_of_the_review": "Overall the paper provides a new angle of view for the GAN models. But there are still some points the paper needs to address to meet the bar of acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1331/Reviewer_qHF8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1331/Reviewer_qHF8"
        ]
    },
    {
        "id": "0ABMJSIcS5T",
        "original": null,
        "number": 2,
        "cdate": 1666923932002,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666923932002,
        "tmdate": 1669981909228,
        "tddate": null,
        "forum": "_d2f3hRn0hT",
        "replyto": "_d2f3hRn0hT",
        "invitation": "ICLR.cc/2023/Conference/Paper1331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new framework, GA-NTK, of Generative adversarial networks (GANs) by constructing the discriminator based on the neural tangent kernel (NTK). The NTK allows to describe the discriminator output in a compact and simple function of the given data X and the auxiliary variables Z, and hence many computational issues involved in the GANs training, such as convergence problem, mode collapse, and gradient vanishing, are greatly suppressed. The proposed method's experimental results succeeded in producing artificial images of comparable qualities with other standard methods such as WGAN and SNGAN.\n",
            "strength_and_weaknesses": "Strengths:\n- The core part of the idea is novel. \n\n- The computational cost of the proposed method is significantly smaller than other comparable methods.  \n\nWeaknesses:\n- From a statistical viewpoint, there is no sufficient theoretical background for the proposed method, I think. The details are described in the Quality section below.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The presentation is clear and the manuscript is easy-to-read. But there are still minor flaws. The following list of the spots I realized:\n******************************\nPage 9: What is ``FNN''? \n******************************\n\nQuality: Although the presentation is good, I think the proposed method is not well supported by statistical background. The meaning is as follows. In the standard GANs framework, the generator tries to approximate the data distribution P_{data}. A common way to do so is [1] generating a hidden variable (noise) as z~P_Z (P_Z is usually chosen as Gaussian); [2] updating the generator G so that the distribution of G(z) is as close as possible to the data distribution. The distribution of G(z), P_g, resultantly approximates P_{data}. Hence we can use P_g as a generative model. The proposed method, however, has no such distribution. The author(s) do not discuss this point at all. Actually, the proposed way of generating artificial data is to find the minimizer of the loss defined by eq. (4); if the global minimizer of this minimization problem is obtained, then the generated artificial data is uniquely obtained as a function of the data and has no fluctuation, implying that it is not possible to generate diverse artificial data. Still, from another viewpoint, it is possible that a variety of artificial data can be obtained from optimization issues such as splitting the data into mini-batches and the local minimums of the loss. But the distribution of the variety caused by these issues is basically uncontrollable and cannot be expected to approximate the data distribution.\nHence, in the standard statistical meaning of the generative model, I think the proposed method does not provide any generative model. Due to this reason, I think the quality of the paper is not high.\n\nNovelty: The base idea is novel and interesting, though its statistical justification is absent as I criticize above.\n\nReproducibility: Although I did not try to reproduce the result by myself, the algorithm is explained well and thus one can reproduce the result in principle. Hence there is no serious problem in reproducibility.\n",
            "summary_of_the_review": "The core idea of the paper is novel and interesting but its statistical meaning is not clear as mentioned above. Taken as best one can, it can be said that the authors proposed an optimization framework of generative models rather than the statistical one. However, it is not possible for me to take it so favorably, and hence I vote for a rejection side about this paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1331/Reviewer_ihk5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1331/Reviewer_ihk5"
        ]
    },
    {
        "id": "HLlECLgvv3v",
        "original": null,
        "number": 3,
        "cdate": 1666973035171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666973035171,
        "tmdate": 1666973446330,
        "tddate": null,
        "forum": "_d2f3hRn0hT",
        "replyto": "_d2f3hRn0hT",
        "invitation": "ICLR.cc/2023/Conference/Paper1331/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel NTK-based GAN named GA-NTK. Specifically, the authors use an infinite-wide linear neural network (i.e., NTK model) as the discriminator for GAN, which thus enables one to directly obtain a closed-form solution for the inner maximization problem in GAN training. Compared with vanilla GAN, GA-NTK enjoys better convergence and less mode collapse and can avoid gradient vanishing. Experiments are conducted on both synthesized and real-world datasets to justify the effectiveness of the proposed method.",
            "strength_and_weaknesses": "In general, the paper is well-written and the proposed GA-NTK method is very interesting and novel. Other strengths are listed below:\n\n1. GA-NTK converts the original bilevel optimization problem of training GAN to a single-level problem, which significantly reduces the hardness of training GAN.\n\n2. Experiment results on real-world datasets show that GA-NTK can generate images that align well with human perception.\n\n3. Experiment results on synthesized data are strong enough to justify that GA-NTK can avoid mode collapse.\n\n\nOther comments / questions:\n\n1. I wonder the relationship between the batch size and the generative performance of the proposed method. For example, based on the FID results in Table 1, for GACNTKg, one can find a positive correlation on MNIST, a negative correlation on CIFAR-10, and no correlation on CIFAR-100. Any insight about that?\n\n2. What is the time usage for training GA-NTK and its variants?\n\n3. In the second paragraph of page 2, I disagree with the claim that gradients of $\\theta_g$ can not be back-propagated through the inner maximization problem. Actually, once you find the solution of the inner maxization problem in which we denoting it as $\\theta_D^*$, then the gradient for the current $\\theta_g$ can be written in an analytical form with respect to $\\theta_D^*$.\n\n4. The comparison between GA-NTK and existing approaches may be a little unfair, as GA-NTK does not use the same model architectures as that in existing approaches. A more fair comparison for me may be comparing existing approaches with its GA-NTK version in which the finite-wide discrimators are replaced with their corresponded infinite-wide linear conterparts.\n\n5. It would be interesing to compare the convergence speed of GA-NTK with previous GAN training algorithms.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the proposed GA-NTK method is novel.",
            "summary_of_the_review": "The proposed GA-NTK is novel and the experiment results are strong enough to justify the effectiveness of GA-NTK. Besides, the paper also extends the application domain of NTK and opens up new research direction of GAN training. Therefore, I suggest to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1331/Reviewer_rAgg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1331/Reviewer_rAgg"
        ]
    },
    {
        "id": "SW1AGo-WHHN",
        "original": null,
        "number": 4,
        "cdate": 1667115102464,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667115102464,
        "tmdate": 1670599473696,
        "tddate": null,
        "forum": "_d2f3hRn0hT",
        "replyto": "_d2f3hRn0hT",
        "invitation": "ICLR.cc/2023/Conference/Paper1331/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose generative adversarial neural tangent kernel (GA-NTK) and its variants.\nThey model the discriminator as a Gaussian process whose mean (and covariance) is governed by NTK.\nAlso, they can be performed via a single-level training process as a whole, because the training dynamics of their discriminators can be evaluated in closed form.\nThe authors claim that this property allows them to avoid the training difficulties often encountered with GANs, and experimentally confirmed this claim and their effectiveness for data synthesis.",
            "strength_and_weaknesses": "I place special emphasis on comments with the mark *.\n\n---\n\nStrength:\n\n[1*] As the authors also wrote at the end of Section 1, the main significance of this study is not performance improvement but novelty of the research direction.\nThis study is an interesting attempt that uses an interesting idea to solve the problems of GANs, which are the motivation for the study.\n\n---\n\nWeakness:\n\n[2] There are writing misses.\n\n[2-1] You should use \\citep as well as \\citet.\nFor example, you can write \"theoretical interpretations Arjovsky & Bottou (2017).\" in p.3 as \"theoretical interpretations (Arjovsky & Bottou, 2017).\" by using \\citep.\nAlso, \"Franceschi et al. Franceschi et al. (2021)\" in p.3 is strange for me; you can write \"Franceschi et al. (2021)\" by using \\citet only.\nPlease look up the use of \\citep and \\citet and rewrite the relevant parts.\n\n[2-2] In l.12 in p.4, value each element -> value of each element.\nCheck your writing (including other parts) again.\n\n[2-3] In eq.(4), $\\arg\\min\\_{Z^n} L(Z^n)=\\arg\\min\\_{Z^n} ||\\cdot||_2$ -> $\\arg\\min\\_{Z^n} L(Z^n)$, where $L(Z^n)=||\\cdot||^2$.\nYou should define $L(Z^n)$ explicitly, and unify $||\\cdot||\\_2$ and $||\\cdot||$ into one if they are the seme (see also norm in other parts).\n\n[2-4] The $f$-divergence appears in places, but what function was used for $f$?\n\n[2-5] In Figure 3, title is partially obscured.\n\n[2-6] In Figure 5, what is \"real\" in the legend?\n\n[2-7] In the last equation in p.14, absolute value symbol is missing:\nfor example, $\\nabla_{z_j} L(Z^n)\\le$ -> $|\\nabla_{z_j} L(Z^n)|\\le$.\n\n[2-8] You should centralize Figure 14.\n\n[2-9] In Section 10, ?? -> 18.\n\n---\n\nQuestion:\n\nRegarding the following points, it may be that the authors' description is appropriate and there is no problem, just because I have not understood it correctly.\nI do not reflect these points in my current recommendation score.\nDepending on the authors' response, I may change my recommendation score.\n\n[3] Is it necessary to solve an optimization problem like eq.(4) every time to generate pseudo examples?\nIf so, that is a demerit compared with GANs, which can generate an infinite number of pseudo examples without additional training from a trained model.\nThis disadvantage should be written more clearly.\n(Even so, I will not lower the score.)\n\n[4*] From the experimental results (see especially Figure 3) and my understanding of the formulas, it seems to me that GA-NTK that is sufficiently trained with a large $t$ would simply return an training example (or mixture of training examples) as a pseudo example.\nIs this question correct?\nIf correct, such data synthesis would have low practical value.\nIt does not seem to me that the use of a generator model adequately solves such a problem.\nAlso, the authors should explain when (i.e., at what $t$) to stop learning.\n\n[5] This question relates to [4*].\nI am interested in the relationship between the batch size $b$ and the novelty of the pseudo example (dissimilarity to the training examples $X^{b/2}$).\nI suspect that if $b$ is small, each generated pseudo example will be fairly close to one of the training examples $X^{b/2}$.\nIs this question correct?\nIf correct, I think that the authors should mention this issue.\n\n[6*] It is difficult for me to understand the whole algorithm.\nI could not understand the relation between the index $t$ in $\\lambda=\\eta \\cdot t$ in eq.(3), the index $j$ in Theorem 3.1, \"Epochs\" in Figure 3, and \"Iterations\" in Figure 5, and the evolution of $Z^n$ and $Z^{n,(j)}$.\nPlease write a pseudo code (or modify the text) so that the relation of these objects become clear.\n\nAlso, how does $t$ change when $Z^n$ changes?\nI think this question is relevant to the validity of applying NTK theory.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nI do not understand the points [3]--[6*].\nOther parts have a good clarity.\n\n---\n\nQuality:\n\nI think that the quality of the paper is not low, but I do not understand the points [3]--[6*].\nSo I will reserve a definite evaluation until the response.\n\n---\n\nNovelty:\n\nAs I wrote in [1*], I give a good evaluation regarding the novelty.\n\n---\n\nReproducibility:\n\nI took a glance of GitHub repository.\nI think that experimental part of this study is reproducible.\nHowever, I want you to cope with [6*], to improve the understandability (a basis of the reproducibility).",
            "summary_of_the_review": "I give this study a positive score now, on the basis of the novelty and interest of the idea [1*].\nHowever, the description of the proposed method is insufficient and not clear [3]--[6*].\nThese points could be improved!\nI will change the score up or down, depending on the responses to [3]--[6*].\n\n---\n\nI changed the score from 6 to 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1331/Reviewer_LpjQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1331/Reviewer_LpjQ"
        ]
    }
]