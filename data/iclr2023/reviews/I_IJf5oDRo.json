[
    {
        "id": "veoX4daBpRy",
        "original": null,
        "number": 1,
        "cdate": 1666172414044,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666172414044,
        "tmdate": 1666172414044,
        "tddate": null,
        "forum": "I_IJf5oDRo",
        "replyto": "I_IJf5oDRo",
        "invitation": "ICLR.cc/2023/Conference/Paper2044/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new framework called GIANT (for Geometric InformAtioN boTtleneck) to provide results\u2019 interpretations/explanations of a collaborative -filtering-based recommendation system. \n\nThis framework requires first the computation of a user-item interaction graph. \nEach node is represented by LightGCN encoding of:\n- for a user node: all text reviews the user wrote, \n- for an item node: all text reviews posted for this item.\n\nUser and item nodes are then clustered via K-means based on Radial Basis Function kernel for distance. The distance between a user or an item to the cluster centroid vectors is used as the probability to be assigned (soft assignment) to the cluster and the corresponding distribution is used as prior to learn user and item latent vectors from a Variational AutoEncoder (VAE).\n\nFor a pair of user-item taken from the interaction graph, user and item representations are fed to a VAE trained so that obtained latent vectors of the pair are used to predict the rating from that user for this specific item. \n\nFinal loss of the network does not take into account only the regression loss (for rating prediction) but also regularization terms:\n-one for constraining the learning of the latent vector in text modality by imposing the prior knowledge from the graph modality (information bottleneck)\n-one for preventing learned distribution of latent vector to be chaotic (prior-centralisation)\n",
            "strength_and_weaknesses": "Strengths:\n\nThe paper is quite clear and enjoyable to read. \nThe approach is clearly stated with the different steps:\n-interaction graph computation\n-encoding of the nodes\n-clustering\n-VAE: prior / posterior\n-Final objective function\n\nThe framework is supported by multiple experiments on 3 public datasets to evaluate quantitatively accuracy of the recommendation but also importance of each block of the loss by appropriate ablation study. Finally there is also a quantitative analysis of the interpretability of the recommendations based on the cluster analysis: as each dimension of the latent variables from the VAE encoding part are associated to a cluster, we can interpret the recommendation of a given item by looking at the the most frequent words associated to reviews from clusters associated to the highest values of coefficients of the latent vector of the item.\n\nWeaknesses / questions:\n\nI have several questions:\n\nQ1: Why not compare GIANT with Wasserstein VAE and only StandPrior for interpretability evaluation?\n\nQ2: What is the architecture of the predict head? Simply dot product of the latent vectors from the VAE?\n\nQ3: How are the text reviews treated before LightGCN? Is it the same treatment as for the input of the VAE encoder?\n\nQ4: Eq. (1). Which value do you take in practice for $I_c$?\n\nQ5: Is K-means applied on all types of nodes at the same time or two applications on user and items separately? I think there is only one application but p.4 the differentiation of $C^u_k$ and $C^i_k$ is a bit confusing. \n\nQ6: None of the methods described in the Related work section for explainable recommenders have been tested against GIANT. Am I mistaken?\n\nQ7: Did you try the Wassertein metric instead of KL divergence to calculate the distribution discrepancy between posterior and prior? (to make WassersteinVAE benefit from a better prior). \n\nMinor (typos):\n-p.3 text before equation 1, extra comma to remove after decode arrow\n-p.16 missing space between citation and Nadaraya-Watson estimator + bracket typo is conditional probability\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is rather clear and well written.\nThe combination of all the blocks seems to be novel. The use of VAE or user-item interaction graphs to learn embeddings in recommender systems is not new. What is novel here is the proposition of regularization terms from graph modularity to text modularity. The proposed prior for VAE is more sophisticated than for StandPrior. \nExperiments have been made on publicly available benchmarks and the authors provide the set of hyperparameters and chosen optimizer to ensure reproducibility of the results.  \n",
            "summary_of_the_review": "The approach is showing on the 3 public benchmarks best performance so the method seems promising. Moreover the qualitative evaluation shows indeed interpretation associated is more valuable than the one associated to StandPrior. Why not compare with WassersteinVAE which was giving the second best performance in terms of rating prediction?\nMy score is currently a weak reject, score that can be increased when my questions (in weaknesses part of the review) are answered. \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2044/Reviewer_fUWF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2044/Reviewer_fUWF"
        ]
    },
    {
        "id": "OLY6VuT4TH",
        "original": null,
        "number": 2,
        "cdate": 1666243542839,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666243542839,
        "tmdate": 1666243542839,
        "tddate": null,
        "forum": "I_IJf5oDRo",
        "replyto": "I_IJf5oDRo",
        "invitation": "ICLR.cc/2023/Conference/Paper2044/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a review-based explainable recommedation model that predicts user preference and gives explanations based on user/item clusters. The core idea is to learn two sets of clusters, one based on text reviews and the other based on user/item interaction information. This explicitly ensures that information about similar users from the aspect of collaborative filtering can facilitate explanation generation for each other. Mathematically, this is achieved by using information bottleneck: the authors maximize the information between two sets of clusters, while ensuring compression (deriving compact clusters) and reconstruction also through information-based perspective.\nContribution: \n1. The idea to build a connection between multi-modal data (text and interaction) through mutual information is quite interesting and is novel to explainable recommendation according to my knowledge.\n2. The summarization of existing issues in explainable recommendation is good, especially the difficulty in getting ground-truth labels for explanations and the low quality for reviews. ",
            "strength_and_weaknesses": "Strength:\n1. The overall idea (use mutual information to connect multi-modal data) is interesting and novel, and it could be useful for explainable recommendation. \n2. The introduction includes a good summarization of existing issues in explainable recommendation, especially that the reviews usually are not good ground-truth labels for explanations, and that getting the true ground-truth is very difficult.\n\nWeakness:\n1. The major weakness is about the explanation introduction, quality, and evaluation.\n(1) Method: I cannot find a clear and formal introduction about how explanations are generated in the method section. The introduciton seems to talk about explanation generation with one sentence: \"and we extract the representative words in each dimension to offer recommendation explanations.\" According to this sentence and also Fig. 4, I think the explanations are actually many words in some clusters that are activated for a user-item pair. This is different from the example in Fig. 1, which contain only a few words. I suggest the authors more clearly introduce their explanation generation method, since explainability is a core contribution here.\n(2) Explanation quality: When borrowing words from similar items/users, how can you ensure that the selected words are relevant to the item and faithful (not misleading)? Also, the current methods only ensures similarity between input and decoded embeddings. Can this ensure that the decoded words are still relevant with the input words? Even if we can correctly decode a review, how can we ensure that the decoded words serve as good explanations? Can you help explain more about these questions?\n(3) Evaluation: No formal comparison with baselines in terms of explanation quality (e.g., whether the explanations are correct or useful). For example, what are the example explanations that you will show to users, and any comparions with baselines to prove your explanation quality? Also, any ablation study on whether the regularization really helps in improving explanation quality?\n2. Some descriptions about related works are not rigorous\n(1) \"However, the aforementioned approaches suffer from two major\nissues: they rely heavily on the accuracy of sentiment analysis tools (Guan et al., 2018); they tend\nto ignore implicit interactions not expressed directly in the individual review text.\" I do not think all mentioned related works rely on sentiment analysis tools? What do you mean by ignore implicit interactions? I think most methods consider user-item interaction information in the model (e.g., as model outputs)?\n(2) NAREE should be NARRE.\n3. The theory part is unclear to me.\n(1) Why the second term in Eq. (3) is I(X_t, Z_g) rather than I(X_t, Z_t)? Why the first term considers only Z_t but not Z_g? Do you use Eq. (2) to dervie Eq. (3)? Can you give a step-by-step derivation here? For example, can you first replace X, Z in Eq. (1) with Xt, Xg and Zt, Zg and gradually derive each equation?\n(2) The proof in Section B is also difficult to understand. I am an expert in explainable recommendation but I am not an expert in information bottleneck. Every derivation step needs to be clearly written for me to carefully check the correctness of the proof, e.g., what is the chain rule, how you apply it twice. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, quality and reproducibility could be improved (see weaknesses 1 and 3).\nNovelty is okay (see strength 1).",
            "summary_of_the_review": "The idea is novel and interesting, and the summarization about current issues in explainable recommendation also provides interesting insights. However, the explanation generation method is not clearly introduced and discussed, and the explanation quality is not well evaluated. Moreover, the related theories and equations can be better introduced to ensure readability. In its current state, it is difficult for me to judge whether the method has issues.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2044/Reviewer_Scqk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2044/Reviewer_Scqk"
        ]
    },
    {
        "id": "wqDpowv3zs",
        "original": null,
        "number": 3,
        "cdate": 1666649325585,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649325585,
        "tmdate": 1666649325585,
        "tddate": null,
        "forum": "I_IJf5oDRo",
        "replyto": "I_IJf5oDRo",
        "invitation": "ICLR.cc/2023/Conference/Paper2044/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes an explainable recommender system based on variational autoencoders (VAE). The main idea is to model user/item textual reviews with an autoencoder whose latent space is regularized by user/item cluster-membership distributions. The latter are derived from the user-item interaction data by first clustering users and items, and then estimating a user or item distribution based on its distance to each cluster centroid using a Gaussian Kernel. The authors argue that such regularization would enable the use of collaborative signals from similar users and items while generating explanations for a given user-item pair. Experiments are curried out on three real-world datasets. Various aspects of the proposed method are evaluated, including rating prediction and latent variable interpretability.       ",
            "strength_and_weaknesses": "Strengths.\n- The paper proposes an interesting idea to improve the interpretability of user/item latent factors in a recommender system by leveraging knowledge from both textual reviews as well as user/item clusters derived from user-item interaction signals.  \n- The proposed method offers competitive performance in terms of rating prediction as measured by MAE and RMSE metrics. Some qualitative/quantitative results also show that the proposed method seems to learn more coherent and meaningful latent representations and clusters. \n\nWeaknesses. \n- Writing requires some important improvements. Section 3 and 4 describing the proposed method are hard to follow. \n(I) The notations are a bit confusing. I would recommend using \u201ct\u201d and \u201cg\u201d to denote text and graph related quantities as superscript instead of subscript.\n(II) The transition from eq. 2 to eq. 3 is not obvious. An alternative is to introduce eq. 4 first, which corresponds to a Beta-VAE with a cluster-membership distribution as a prior, and then provide a connection to the IB objective as a supportive analysis for eq. 4. \n(III) Eq. 8 regarding the KL-term is not obvious either, is p(Z_t|x_n,e) Gaussian? \n- The focus of this work is on explainability, however the results regarding this aspect are weak. I would recommend including some human evaluations (user study) to better assess the quality of the explanations generated by proposed method compared to the baselines. \n- The performance of the proposed recommender system is measured using prediction metrics such MAE and RMSE. In general, MAE and RMSE do not necessarily reflect the quality of item recommendation. Reporting retrieval measures such as Precision and Recall would be more convincing.  \n\nAdditional comments.\n- The clustering component is central in the proposed method. It would be useful to have more experiments regarding this aspect. For instance, what is the impact of number of clusters on the performance of the proposed method. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the strength and weaknesses section above.  ",
            "summary_of_the_review": "Please refer to the strength and weaknesses section above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2044/Reviewer_JQQm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2044/Reviewer_JQQm"
        ]
    }
]