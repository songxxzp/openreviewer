[
    {
        "id": "YzrjKafDdcG",
        "original": null,
        "number": 1,
        "cdate": 1666573034957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573034957,
        "tmdate": 1666573034957,
        "tddate": null,
        "forum": "78xgBm6ckZr",
        "replyto": "78xgBm6ckZr",
        "invitation": "ICLR.cc/2023/Conference/Paper2053/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors propose a method to 'translate' a Decision Tree (DT) into a Multi layer perceptron (MLP). They use this method as the basis for a Neural Net (NN) weights' initialization technique that allows them to considerably boost NN's performance on tabular (unstructured) data. Authors provide a thorough set of experimental tests where they compare the performances of their proposed approach to that of standard (random) NN init, as well as against the performances of different DT-based methods and another NN architecture aimed at  tabular data. The results obtained back the validity of their method.",
            "strength_and_weaknesses": "- The paper is very well written in structure, language and style, with only minor issues here and there.\n\n- The proposed method is very interesting, although remarkably simple. Authors do a very good job explaining it, nonetheless.\n\n- Experimental evidence provided is comprehensive, and provides good empirical support for the proposed approach.\n\n- Results are highly encouraging to continue this line of research, where different ML algorithms combine and blend, in simple and very effective ways.\n\n- Authors also do not fall short in providing discussion regarding all aspects of their works (inner workings and behavior of their methods, results, consequences and possibilities resulting from their  work).",
            "clarity,_quality,_novelty_and_reproducibility": "These are some suggestions that I'd like make to the authors in order to further improve their draft:\n\n- In Related Works section, please make emphasis and/or describe briefly how your approach herein proposed for transforming a DT into a MLP differentiates from methods previously found in literature. In this section you mention that these _translations_ have been proposed as early as the 90s; since this is such a critical part of your approach (and also because is a fundamental good writing style) I think you should make clear as soon as possible how this new translating mechanism differentiates from other approaches previously proposed. Otherwise, novelty might be put into question.\n\n- It would also be good if you discuss some of the motivations that led to your research. From the wording in the abstract and intro, I got the feeling that you consider neural nets shall \"reign supreme\" overall other ML methods, and if they lose against any other ML paradigm in certain areas of application, then we shall bring elements of such other family of methods to deep learning, until they are \"the one ML method to rule them all\"... that'd be a weird line of thought. Think of the following two questions => \"Why cannot NN be as good, or even better, than tree methods in this or that kind of problems?\" vs \"How can we enhance the performance of NN by taking elements of other ML paradigms?\" which one sounds more natural to you? Just a bit of rephrasing in abstract and intro would do the trick in my opinion.\n\n- Did you consider discussing the implications of your research regarding the capabilities and limitations of gradient methods and the overall optimization complexity of neural nets? I think you might be sitting on something beyond just an initialization technique, but rather further evidence that gd is not as good as previously thought, which has been a recent line of research (e.g. lottery ticket hypothesis, etc.).",
            "summary_of_the_review": "In general, I think this paper is very good. My only doubt is how similar is this DT-MLP transformation technique to approaches previously proposed in the field. Other than that, I think the paper is well written and presented, experimental evidence and results check, and novelty and significance are good enough for ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2053/Reviewer_mdb9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2053/Reviewer_mdb9"
        ]
    },
    {
        "id": "X3SsQujIwUv",
        "original": null,
        "number": 2,
        "cdate": 1666656540979,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656540979,
        "tmdate": 1666656586781,
        "tddate": null,
        "forum": "78xgBm6ckZr",
        "replyto": "78xgBm6ckZr",
        "invitation": "ICLR.cc/2023/Conference/Paper2053/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to initialize MLP neural networks when working with tabular data. The method uses tree-based techniques to initialize the weights of the NN. The authors argue that using this initialization in the first layer of an MLP is sufficient to improve its performance (the initialization of the other layers is random).",
            "strength_and_weaknesses": "+ The proposed approach is interesting and presents promising results. \n+ The idea of using decision trees to capture relevant features and their interactions and define a mapping to encode extracted relationships into a neural network is not new. Still, the authors presented a focused approach and interesting experiments to justify its use.\n\n- The paper suggests the proposed approach operates in an implicit regularization during the NN training. However, I\u2019m not convinced that it was entirely demonstrated in the experiments, results, and analysis.\n- The authors compared the proposed NN initialization approach with random weights initialization. However, this is an active research area, and classical approaches should be considered for comparison:\n   - Understanding the difficulty of training deep feedforward neural networks, Glorot and Bengio, 2010\n   - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, Saxe et al., 2013\n   - Random walk initialization for training very deep feedforward networks, Sussillo and Abbott, 2014\n   - Data-dependent Initializations of Convolutional Neural Networks, Kr\u00e4henb\u00fchl et al., 2015\n   - All you need is a good init, Mishkin and Matas, 2015\n   - Fixup Initialization: Residual Learning Without Normalization, Zhang et al., 2019\n   - The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, Frankle and Carbin, 2019\n- The authors do not consider the computational cost of training the tree-based methods to initialize the NN model. \n- What is the performance of the proposed approach on deeper models or using different activation functions? Please provide more comments on that.\n- The authors did not discuss the limitation of the proposed method. Therefore, it will be meaningful to discuss the gap between the experiments in the current version of the paper and the real-world applications.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and logically structured. Unfortunately, the code is unavailable, making it difficult to reproduce the experiments.\nThe idea of using tree-based methods to initialize NN models is simple but seems effective considering the paper's results. However, the idea is not entirely new, and the authors should compare the proposed model with other approaches and with other initialization methods. Therefore, the comparison with random weight initialization seems not fair. \n",
            "summary_of_the_review": "The proposed paper presents a promising approach with interesting results and analysis. The authors focus on comparing the proposed method with random initialization methods but should consider other initialization approaches to compare. The author also should discuss the proposed approach's limitations.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethical concerns about the paper.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2053/Reviewer_znja"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2053/Reviewer_znja"
        ]
    },
    {
        "id": "xvXV9ln5ih",
        "original": null,
        "number": 3,
        "cdate": 1666770989266,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666770989266,
        "tmdate": 1666770989266,
        "tddate": null,
        "forum": "78xgBm6ckZr",
        "replyto": "78xgBm6ckZr",
        "invitation": "ICLR.cc/2023/Conference/Paper2053/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a new method to initialize neural networks using pertained tree based models for tabular related data. The new proposed framework includes two components: \n- [1] As proposed by the prior work 'Neural random forest', 2019, tree based models can be converted into a 3-layer MLP.\n- [2] Design the non-linear function with proper selection of parameter to fit into gradient descent training framework.\n\nNumerical result shows that MLP with tree based initialization method converges better than random initialization. Parameters distribution analysis also shows model trained with tree based initialization converges to a solution as compared to random initialization.",
            "strength_and_weaknesses": "Strength\n[+]  Paper is well organized and well motivated. Literature is comprehensive and overall paper is well organized.\n\nWeakness\n[-] Novelty seems limited. The algorithm and contribution of the paper is mainly on developing a schema with neural network to handle tabular data and the major source of improvement is from tree based initialization and fine tuning process, which mostly seems to be obtained from Biau et al. (2019).\n\n[-] Some details might need more clarification. \nCould the author provides an example or demo graph to illustrate more clearly on what the initialized the neural network look like. Figure 1 is obtained from the original paper to show how tree based model can be converted to neural nets in a intuitive way. Could some demo example listed here to more precisely show the initialization step? Same applies to section 2.3 on the details to convert the model to allow gradient descent training.\n\n[-] Experimental results seem weak.\nFrom the results in table 1, it seems MLP initialized with GBDT still lower perform than GBDT along. Is there any analysis on the source of changes, considering the neural networks is fine tuned on top of GBDT models? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, quality and reproducibility are good. While clarity and novelty of the work can be improved. ",
            "summary_of_the_review": "The paper considers an interesting and important topics on building a neural network schema to handle tabular data. Initialization with tree based model with designed fine tuning process is reasonable sound. However, the novelty of the paper seems limited since majority of the work seems to be inherited from Biau et al. (2019). Experimental results seem weak as compared to the baseline models. Although there are some interesting findings on the converged models, while there is not theoretical analysis on those behavior. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2053/Reviewer_c3Rb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2053/Reviewer_c3Rb"
        ]
    }
]