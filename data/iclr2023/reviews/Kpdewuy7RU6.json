[
    {
        "id": "HJDgwIgcUN",
        "original": null,
        "number": 1,
        "cdate": 1666649177490,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649177490,
        "tmdate": 1669064450719,
        "tddate": null,
        "forum": "Kpdewuy7RU6",
        "replyto": "Kpdewuy7RU6",
        "invitation": "ICLR.cc/2023/Conference/Paper5698/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to rescale gradient components based on spatial mutual information when optimizing convolutional neural networks. They show that the proposed spatial gradient scaling is equivariant to previous work on structural re-parameterization (which uses a multi-branch topology network during training and re-parameterize back to the original convolution for inference). They compare their approach to previous work using structural re-parameterization for classification tasks on the CIFAR and ImageNet datasets and demonstrate that the proposed technique improves performance at a lower computational cost.\n",
            "strength_and_weaknesses": "**Strength:**\n1. This paper is relatively well-written and most technical parts are clear to me. The experimental details are also sufficient.\n2. The connection between spatial gradient scaling and parallel convolution parameterization is a good observation. It shows that previous work on muti-branch reparameterization is unnecessary for achieving better performance.\n\n**Weaknesses:**\n1. The proposed approach is heuristic and lacks theoretical foundations. The use of mutual information for gradient rescaling is intuitive but it is hard to justify from a theoretical perspective. \n2. There are useful tricks in deep learning which do not have a theoretical explanation in the beginning. But in that case, the empirical study needs to be more convincing. It may not be enough to only apply their technique to classification tasks and compared only to previous work on structural re-parameterization. In my opinion, it would be convincing if they could show that their performance improvement can not be possibly achieved by using other optimization methods with adaptive gradient (e.g. AdaGrad), and their empirical gain is robust across tasks such as object recognition, image segmentation, and image generation, and across a wide range of base CNN architectures. Of course, I am not asking the authors to run these experiments during the rebuttal period. These are just suggestions to make a strong paper in the future. That level of empirical study is usually not necessary, but for a more heuristic technique, it might be good to demonstrate the robustness of the performance gain.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\n\nThe paper is well-paced with good explanations and illustrations. The paper contains sufficient technical details.\n\n**Quality:**\n\nI am convinced that their proposed spatial gradient scaling is equivalent to branched reparameterization in previous work without introducing structural changes. I have no questions regarding this contribution and I think their experiments also support that claim well.\n\nHowever, I do not understand why structural reparameterization is an important and promising direction. As shown by this paper, previous work on structural reparameterization can just be seen as gradient rescaling. Can this line of research be seen as designing data-dependent optimizers dedicated to convolutional neural networks? If that is the case, can we find other optimization methods with adaptive gradients such that gradient rescaling is no longer essential? I think the baselines should not be previous work on structural reparameterization, because we already know they are equivalent. The paper should try to demonstrate why structural reparameterization/spatial gradient scaling is an attractive technique and the practitioners should be recommended to use it. If this cannot be justified theoretically, then more empirical studies need to be conducted to show that performance improvement is robust and can be used on a variety of tasks/architectures, and no other techniques (e.g. adaptive gradient optimizers) can not achieve similar improvement. \n\n**Reproducibility:**\n\nThe paper provided enough experimental details. The code is currently not provided.",
            "summary_of_the_review": "I am leaning toward weak rejection but I think this paper still has merits. \n\nTheir claim that the proposed spatial gradient scaling is equivalent to branched reparameterization in previous work without introducing structural changes is correct and it is also supported by their experiments. \n\nHowever, the proposed technique is too heuristic with weak theoretical justification. In experiments, they only compare to branched reparameterization in previous work  (which has already been shown to be equivalent to their proposed approach). So I am not convinced that this technique can bring robust performance gains that cannot be achieved using other optimization algorithms with adaptive gradients. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no echical concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5698/Reviewer_xBEc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5698/Reviewer_xBEc"
        ]
    },
    {
        "id": "zNaERHRicF",
        "original": null,
        "number": 2,
        "cdate": 1666678025938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678025938,
        "tmdate": 1670305130550,
        "tddate": null,
        "forum": "Kpdewuy7RU6",
        "replyto": "Kpdewuy7RU6",
        "invitation": "ICLR.cc/2023/Conference/Paper5698/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides a gradient scaling approach to replicate structural reparametrization that improves training. The main idea is based on the linearity of convolutions and the observation that the effect of reparametrization can be achieved by simple gradient scaling. The results show the benefits of the approach on multiple vision datasets.",
            "strength_and_weaknesses": "## Strengths\n1. The idea is simple and elegant. It replaces structural reparametrizations which require high memory and computational overhead during training with a simple gradient scaling that performs similar to such methods.\n2. The paper is well-written, and the idea is backed by the proofs and experimental results.\n\n## Weaknesses\n1. It is not clear why mutual information is a good measure to obtain gradient scales. Specifically, it is not clear to me why one should weight a filter coefficient higher if it is similar to the center pixel in a convolution filter. Even though the results show the benefits it would be good to motivate it intuitively. Please comment on this.\n2. Even though the idea is simple, there were several implementation details (eg., the last para on page 6) required to make it work in practice. I think reproducibility might be a concern due to this.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, the idea is good and overall the paper is of high quality. Reproducibility might be a concern so I would encourage the authors to release the code.",
            "summary_of_the_review": "Overall, the idea is nice and it is backed by theory and experiments.\n\n## Post rebuttal\nI thank the authors for the response. I would recommend adding a paragraph about the intuition behind using mutual information for gradient scaling. Additionally, the extensive experiments across various settings show the practical benefits of the method. I was already positive and I would like to see the paper accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5698/Reviewer_v3Lx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5698/Reviewer_v3Lx"
        ]
    },
    {
        "id": "39pRB_3qW8",
        "original": null,
        "number": 3,
        "cdate": 1666717916621,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717916621,
        "tmdate": 1666717916621,
        "tddate": null,
        "forum": "Kpdewuy7RU6",
        "replyto": "Kpdewuy7RU6",
        "invitation": "ICLR.cc/2023/Conference/Paper5698/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to scale the gradients in convolutions through learning pixel/feature dependencies",
            "strength_and_weaknesses": "Strengths:\n- The paper seems novel.\n- Derivations seem correct\n\nWeaknesses:\n- Figures seem to be in low resolution\n- Table 1 evaluation is only with VGG (quite old), not sure why it did not include ResNet or more recent networks.\n- Results on CIFAR 100 are quite low compared to the state-of-the-art\n- I missed a comparison with other reparameterization methods, e.g. Salimans and Kingma (Neurips, 2016) and others.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is written clearly with graphs that help understand the methodology. \nReproducibility: I did not find a link to the code",
            "summary_of_the_review": "Paper seems interesting, although there are a few improvements that would benefit the paper",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5698/Reviewer_ahFw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5698/Reviewer_ahFw"
        ]
    },
    {
        "id": "omZw4u9d_L",
        "original": null,
        "number": 4,
        "cdate": 1667166173284,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667166173284,
        "tmdate": 1667170733900,
        "tddate": null,
        "forum": "Kpdewuy7RU6",
        "replyto": "Kpdewuy7RU6",
        "invitation": "ICLR.cc/2023/Conference/Paper5698/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to scale the components of the gradients during back-propagation to achieve the same results as branched reparametrization.",
            "strength_and_weaknesses": "The paper is well-written. It has a coherent story and it is interesting to read. The related work section is properly organized. As I was not aware of some of the papers mentioned there, I found it extremely useful.\n\nI find the idea of the paper elegant. Although it is simple, it demonstrates that a simple modification of the learning process can achieve the same result (or better) than a modification of the architecture.\n\nI didn't notice any significant issues. However, I would appreciate if the authors explain why they use mutual information for gradient rescaling. Why not some other function? And if it is not limited to mutual information, would the results differ if the method relies on some other measure for spatial correlation?",
            "clarity,_quality,_novelty_and_reproducibility": "The text is well-written. The source code is not shared. However, it seems possible to reimplement the method myself. ",
            "summary_of_the_review": "The paper proposes a novel and elegant idea. I think it is worth sharing with the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5698/Reviewer_SeDu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5698/Reviewer_SeDu"
        ]
    }
]