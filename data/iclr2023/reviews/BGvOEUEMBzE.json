[
    {
        "id": "OTGyAuNLZT",
        "original": null,
        "number": 1,
        "cdate": 1666593931840,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593931840,
        "tmdate": 1668153964185,
        "tddate": null,
        "forum": "BGvOEUEMBzE",
        "replyto": "BGvOEUEMBzE",
        "invitation": "ICLR.cc/2023/Conference/Paper2412/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores the active learning problem under OOD data scenarios and incorporates AL and OOD objectives within a multi-objective optimization framework to balance their conflict. Specifically, they propose a Monte-Carlo Pareto optimization mechanism to enable efficient Pareto optimization, which selects optimal subsets of unlabeled samples with fixed batch size from the unlabeled data pool. The proposed framework is flexible and can apply to various combinations of AL and OOD sample selection methods, extensive experiments on ML and DL tasks demonstrate its effectiveness.",
            "strength_and_weaknesses": "Strengths:\n1. This paper analyses how OOD data affects the effectiveness and efficiency of the active learning methods, and points out the conflict between AL sample selection objective and OOD detection objective.\n2. This paper casts the AL sample selection under OOD data scenarios as a multi-objective optimization problem, which can automatically balance the conflict between AL and OOD objectives without tuning hyper-parameters. Experiments show its superior to other optimization strategies. \n3. This paper proposes an efficient Monte-Carlo Pareto optimization algorithm for fixed-size batch-mode AL, which avoids searching a non-fixed-size Pareto Front and saves computation cost. Besides, this paper additionally adopts a pre-selection technique and an early-stopping strategy for large-scale datasets to improve efficiency. \n4. The proposed optimization framework is flexible, experiments of POAL incorporated with multiple AL methods on ML and DL datasets validate its effectiveness and generality. \n\nWeaknesses:\n1. The proposed Monte-Carlo Pareto optimization mechanism for select Pareto subsets is somewhat similar to the Pareto embedding mechanism in [a], the authors can make some comparisons and analysis.  \n2. How to deal with the selected OOD samples? Will they be recognized by the Oracle/Annotator and then be discarded, or be wrongly annotated?\n3. It will be better to visualize the selected samples on one dataset or a toy example to verify that they are truly Pareto subsets.\n4. How MC POAL performs on large-scale experiments?\n\nRefs:\n[a] Karlson Pfannschmidt and Eyke H\u00fcllermeier: Learning Choice Functions via Pareto-Embeddings. KI 2020: 327-333\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written nicely and is easy to follow.\n\nQuality & Novelty: As discussed above, sufficient novelty is contained in the proposed method.\n\nReproducibility lacks sometimes.",
            "summary_of_the_review": "Generally, I find this paper tackled an interesting problem with a multi-objective optimization framework for active learning under ODD setting. Yet, it still requires clarification and some solid empirical support before warranting acceptance of this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2412/Reviewer_6RQ4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2412/Reviewer_6RQ4"
        ]
    },
    {
        "id": "ztlAahoLFVO",
        "original": null,
        "number": 2,
        "cdate": 1666627933520,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627933520,
        "tmdate": 1669771596499,
        "tddate": null,
        "forum": "BGvOEUEMBzE",
        "replyto": "BGvOEUEMBzE",
        "invitation": "ICLR.cc/2023/Conference/Paper2412/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studied active learning under out-of-distribution data, which could be unreliable with the existence of OOD data. This paper proposed a multi-objective loss to simultaneously control the data uncertainty and simultaneously filter the OOD data. Empirical results justified the proposed approach\n",
            "strength_and_weaknesses": "### Pros\n- This paper considered a new setting in active learning, when unlabeled data contains out-of-distribution data (or distribution shift), the uncertainty or diversity based idea could mostly capture this unreliable data, leading to a significant performance drop.\n- Empirical results and ablations support the method.\n- In general, this paper is clearly written.\n\n### Cons:\n- [Main concern] I do think this paper has a clear misconception of out-of-distribution data in the context of active learning, where a rethinking on the problem setting would be important.  \n- About related work. In fact, active learning under distribution shift has been recently discussed in a recent deep active learning framework. Surprisingly, there is even no discussion on these related works.\n\n### Comments on cons\n\n1.  I do think the out-of-distribution data in active learning is vaguely defined. The problem settings in the paper assume that unlabelled data could contain different distributions than labelled data (or OOD data). Thus we need to filter these query. However, this is not necessarily correct in active learning. \n- (1) In real-world active learning, the label annotation by **human** could provide the information of OOD data. For example,  labeled dataset = digits, and OOD data is classification cat/dog. Well, when machine query the image of cat, **human** will return cat rather than digits information. From this perspective, there is no need to conduct the pre-selection. If the data is indeed OOD (with different semantic information), human will return ground truth label, then we could easily detect outlier. \n- (2) If the OOD is defined similar semantic information (e.g, cat/dog classification in-doors and out-doors), querying these points could be beneficial or even **improve** the robustness of the prediction (see comment 2, related work section). I would think this is the problem of vague definition of OOD data, what is the formal definition in OOD data in active learning? OOD data share the same semantic information or not ? since in active learning, the data is assumed to ask **human** to query the ground truth label, there are no similar issues in the conventional out-of-distribution data generalization/detection. \n\n2. Another important aspect is the lack of related work. As far as I know,  the distribution shift and active learning has been recently empirical and theoretically studied. For example paper [1-3] studied active learning as a distribution shift problem (labelled data distribution and unlabelled data distribution). Specifically, they assume the unlabeled data shared similar semantic information (e.g, always dog/cat but different background rather than digits). Related theories/practice are developed. I do think these are important to discuss and compare.  \n\n[1] Deep active learning: Unified and principled method for query and training. Aistat 2020\n\n[2] Discrepancy-Based Active Learning for Domain Adaptation. ICLR 2022\n\n[3] Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach. ICLR 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general this paper is clearly written and very easy to follow. The paper studied a novel setting in active learning, the reproducibility seems fine. \n",
            "summary_of_the_review": "This paper studied active learning under out-of-distribution data, which could be unreliable with the existence of OOD data. The setting is quite novel for me and extensive empirical evaluations are done. However, this reviewer believes that there are several fundamental misconceptions in the problem setup. Besides, important related works are not discussed/compared, making this paper fall short of the acceptance bar. \n\n-------------------------------------\n### Post rebuttal after discussions\n\nI would like to appreciate the effortful responses by authors. I further spend some time to further re-read the manuscripts and related work. The following is my additional feedback.\n\n**About robust active learning.** In general, this paper aims to solve robustness in the context of active learning, where unlabelled data could be unreliable. However, checking the traditional active learning papers, these issues have been theoretically or practically discussed such as paper [1-4]. I should say, at least this setting is not entirely novel in active learning. \n\n**About theoretical contribution** This paper aims to propose a novel practical method in active learning. This part is not applicable. \n\n**About methodology and empirical contribution** Since this paper aims to propose a novel practical method, the requirements in this part should be significantly strong to achieve the acceptance bar. Unfortunately, after rechecking the paper, I would feel this is insufficient. In terms of practice, this paper evaluated several standard and relatively simple benchmarks such as tabular data and CIFAR100. However, recent **practical** active learning papers generally should evaluate very large scale, high-dimensional and complex datasets such as ImageNET or Open Images v6 such as [5-6]. Given the current paper is purely empirical, I do think such kinds of experiments are required. As for methodology, I would think the current version consists of different known components in different domains such as Pareto Optimization, out-of-distribution detection, etc. Why choose a specific approach in these domains? Why not other well-known methods in OOD detection? For addressing these, additional ablation studies on large-scale data such as ImageNET is necessary. \n\nOverall, since this paper aims to propose a novel practical method, however, the empirical/practical are not sufficient to achieve the acceptance bar. Without strong, sufficient and convincing empirical results in a very-large, complex and challenging dataset, it is quite hard to convince the community to adopt it practically. \n\nI hope my additional notes could further improve the paper quality.\n\n[1] Robust Active Learning Strategies for Model Variability\n\n[2] Corruption Robust Active Learning. Neurips 2021\n\n[3] Robust Interactive Learning. JMLR 2012\n\n[4] Active Learning with Logged Data. ICML 2018\n\n[5] Towards Robust and Reproducible Active Learning using Neural Networks CVPR 2022\n\n[6] Batch Active Learning at Scale. NeurIPS 2021\n\n\n[Please note that I am not saying CIFAR 10/100 is not useful. If the paper mainly proposes novel theories in understanding robust active learning, it is perfectly fine to merely evaluate these simple datasets as a proof of concept. In contrast, if this paper is empirical or methodological, strong empirical validations on challenging dataset is required in modern active learning.]  \n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2412/Reviewer_SDAU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2412/Reviewer_SDAU"
        ]
    },
    {
        "id": "oUaicojIKS",
        "original": null,
        "number": 3,
        "cdate": 1667002191748,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667002191748,
        "tmdate": 1668699750617,
        "tddate": null,
        "forum": "BGvOEUEMBzE",
        "replyto": "BGvOEUEMBzE",
        "invitation": "ICLR.cc/2023/Conference/Paper2412/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "When the unlabeled data pool contains out-of-distribution (OOD) samples, active learning (AL) becomes challenging as OOD samples may often be confused with informative in-distribution (ID) samples.\nThis paper aims to address this issue by proposing a novel AL scheme that can outperform existing schemes under OOD scenarios.\nFor this purpose, they propose a Monte-Carlo Pareto Optimization for Active Learning sampling scheme, called (POAL).\nPOAL aims to batch-select an effective subset of samples from the unlabeled data pool, thereby efficiently improving the model accuracy.\n",
            "strength_and_weaknesses": "Considering that the goal of AL to select informative unlabeled data points for labeling and the goal of OOD data point detection may conflict with each other, POAL takes a multi-objective optimization scheme to identify and select data points that are located on the Pareto frontier such that they jointly maximize the AL score (for being informative) and the confidence score for being an ID data point (hence less likely to be OOD data points).\n\nIn this work, the authors take the maximum entropy (ENT) approach for assessing the informativeness of a data point and use the negative Mahalanobis distance to measure ID confidence.\nFor computationally efficient Pareto optimization and batch selection, the paper proposes Monte-Carlo POAL for fixed-size batch selection, where Monte-Carlo sampling iteratively generates a candidate solutions at random and compares them against the current Pareto set to efficiently update the set of Pareto optimal data points.\n\nThe performance of POAL has been evaluated for a traditional ML model as well as a deep learning (DL) model, where performance evaluation results show somewhat modest yet consistent improvement over a number of existing AL schemes.\n\n\n1.  As a performance bound, the authors show the AL performance that can be attained by oracle + ENT, where only ID data points are selected based on maximum entropy principles.\nHowever, since AL selection and OOD detection conflict with each other and can confound the performance evaluation results, it would be important to show how the AL schemes are also combined with oracle OOD detection.\nThis would provide insights into what performance improvement may be attained by improving OOD detection and to what extent the AL performance can be enhanced by selecting a specific AL scheme.\n\n\n2. While the paper proposes Mont Carlo POAL to make the Pareto optimization computationally more tractable, there are no results showing how scalable the proposed scheme is.\nThere should be a comprehensive evaluation of POAL in comparison with other AL schemes in terms of their scalability.\n\n\n3. There should be further investigation and discussion regarding the OOD detection performance and the overall AL performance in OOD scenarios.\nFor example, while SIMILAR sometimes outperforms PAL in terms of OOD detection, its overall AL performance appears to fall behind POAL.\nSometimes POAL outperforms SIMILAR in terms of OOD detection, which again leads to better AL performance.\nThere should be ablation experiments to decouple these confounding factors arising from the conflict between AL selection vs. OOD detection, which will be informative for understanding the merits of the proposed method POAL and what factors contribute to its improved performance.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the proposed method POAL is well-motivated and the paper is written in a clear manner that is easy to understand.\nThe proposed approach is moderately novel, but it has been shown to lead to consistent improvement over other existing AL schemes under various OOD scenarios.\nWhile the performance improvement is generally modest, the evaluation results in the paper show that POAL may significantly enhance the AL performance in some cases.\n",
            "summary_of_the_review": "This paper proposes POAL, a Pareto optimization scheme for active learning under out-of-distribution data scenarios.\nWhile the proposed idea is moderately novel and leads to modest AL performance improvement in the presence of OOD data in the unlabeled data pool, it is shown to consistently outperform a number of baseline AL schemes under OOD scenarios.\n--------\nEvaluation scores have been updated after reviewing the authors' response.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2412/Reviewer_hK87"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2412/Reviewer_hK87"
        ]
    }
]