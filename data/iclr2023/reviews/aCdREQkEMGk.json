[
    {
        "id": "Agc4e11Fi_",
        "original": null,
        "number": 1,
        "cdate": 1666637998756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637998756,
        "tmdate": 1669767471412,
        "tddate": null,
        "forum": "aCdREQkEMGk",
        "replyto": "aCdREQkEMGk",
        "invitation": "ICLR.cc/2023/Conference/Paper3615/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "For self-supervised learning with contrastive loss, they proposed Synchronized Contrastive Pruning (SyncCP) that have two encoders (online and offline/momentum) with different sparsity, but the difference in sparsity is maintained at $\\Delta_s$.  They also propose Contrastive Sparsification Index (CSI) that determines when to start sparsification.  CSI calculates the difference in the pruning masks (M) as inconsistency (I).  When inconsistency reaches the sparsity gap $\\Delta_s$, CSI activates the sparsity increment. \n\nThey compare their proposed approach with two existing sparsification approaches using 3 SSL approaches (MoCo, BYOL, Barlow Twins) and 2 datasets (CIFAR 10 and 100).   With Imagenet, they only use BYOL in SSL.   Generally, the results indicate that the proposed approach compares favorably.\n\n",
            "strength_and_weaknesses": "Strengths:\n\n1.  They compared SyncCP with two existing sparsification approaches on 3 datasets.  Generally, the results indicate that SyncCP compares favorably.\n\n2.  Motivation on the disadvantage of sparsification inconsistency in asymmetric encoders in SSL.\n\nWeaknesses:\n\n1.  Some parts of the approach could be explained further (see next section)\n\n2.  An analysis of how $\\Delta_s$ affect performance could be studied.\n\n3.  Additional downstream tasks would help illustrate the quality of learned representations.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity could be improved:\n\n1.  Eq 10-12: Some of the variables and superscripts could be defined.  What is the motivation for the form in Eq 10 and 11, particularly the cubic exponent?\n\n2.  What is the difference between the two activations: \"sparsity increment\" and \"sparsification process?\"   They seem to have different conditions to be activated.    Why is \u03c4 independent of $\\Delta_s$?\n\n\"CSI activates the sparsity increment when I equals to \u2206 s , and this moment is defined as the CSI checkpoint\"\n\n\"the sparsification process is activated when I is less than a pre-defined threshold \u03c4 (e.g., \u03c4 = 0.9)\n\n\nReproducibility could be affected by some parts that could use further explanation.",
            "summary_of_the_review": "The authors propose SyncCP to synchronize sparsity in asymmetric encoders in SSL.   SnycCP constraint the difference in sparsity to $\\Delta_s$.   Empirical results indicate SyncCP compares favorably with 2 other sparsification approaches.  Additional analysis on $\\Delta_s$ and downstream tasks can benefit the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_EBsm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_EBsm"
        ]
    },
    {
        "id": "aSdrdlxyHK",
        "original": null,
        "number": 2,
        "cdate": 1666653493179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653493179,
        "tmdate": 1666653493179,
        "tddate": null,
        "forum": "aCdREQkEMGk",
        "replyto": "aCdREQkEMGk",
        "invitation": "ICLR.cc/2023/Conference/Paper3615/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- This paper explores the problem of inducing sparsity during training for self-supervised learning.\n- The authors discuss challenges with applying existing sparse training techniques to self-supervised learning: sparsity-induced asymmetric non-identical encoders and sparsifying the model with frequent architectural changes both lead to degraded performance. \n- The authors then present SyncCP, which sparsifies both encoders in an SSL setup while ensuring that the two encoders maintain a consistent gap between their sparsity levels and that their architectural difference is mainly due to this difference in sparsity levels rather than the disjointness of which weights to prune. The sparsity levels start gradually increasing once the architectural difference criterion is met.\n- Experiments demonstrate improvements over other sparse training techniques and training speedups.",
            "strength_and_weaknesses": "Strengths:\n- The paper walks you through the challenges of inducing sparsity during SSL training and then presents a method that addresses those challenges. I quite like the overall presentation.\n- The proposed method is novel and well-motivated.\n\nWeaknesses:\n- The results for baseline BYOL on ImageNet with ResNet-50 (66.16) are very low compared to BYOL's reported numbers (74.3). Why?\n- The presented method is only for momentum encoder-based SSL, which is not clear upfront. For instance, there is an introductory discussion about SimCLR and SDCLR, but the method is not actually applicable to SimCLR. Although the authors talk about generality to \"other SSL methods,\" it is not clear which classes of SSL methods are included or excluded.\n- Section 3 provides little context on the modeling decisions being made, such as why the online network is the one to be sparsified or why the online and offline encoders are pruned to different sparsity levels.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty:\n- The presented method is novel and interesting.\n\nClarity and Quality:\n- It is a bit hard to understand what the authors are trying to do without making one pass through the paper. I would suggest the authors present their contributions and motivations more clearly/explicitly in the introduction. While I liked the gradual flow of the paper, I do want the spoilers right in the beginning.\n- Table 7 does not indicate which dataset these numbers are for.\n- The notation in equations 5, 8, and 9 makes it look like the output of the encoder is sparsified via multiplication with a mask. This does not apply any sparsity to the actual encoder parameters and thus does not bring the efficiency improvements the paper aims for.\n- Typo: In paragraph above eq 7, \"quantifying\" -> \"quantify\"\n\nReproducibility:\n- Hyperparameters and algorithms are presented in the Appendix.",
            "summary_of_the_review": "In general, I like this paper. It describes a problem worth solving and presents a method backed by a discussion of the challenges it aims to fix. However, I have some concerns with the current state of the paper (see Weaknesses and Clarity), which I believe need to be addressed before it can be ready for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_NxVm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_NxVm"
        ]
    },
    {
        "id": "jHkPFIGkGYP",
        "original": null,
        "number": 3,
        "cdate": 1667146113423,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667146113423,
        "tmdate": 1667146113423,
        "tddate": null,
        "forum": "aCdREQkEMGk",
        "replyto": "aCdREQkEMGk",
        "invitation": "ICLR.cc/2023/Conference/Paper3615/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new self-supervised learning framework (SSL), which aims to transfer the supervised sparse learning to SSL and reduce the required computational resources during the pretraining stage. Specifically, this paper investigates the correlation between training sparsity and SSL, which embraces the benefits of contrastiveness while exploiting high sparsity during SSL training. Experiments results show that this work surpasses the prior work on multiple datasets.",
            "strength_and_weaknesses": "**Strength** \n1. The overall idea is novel and interesting.\n2. The theoretical analysis of this work is promising.\n\n**Weaknesses** \n\nI am worried about the experimental results of this paper., although the performance of this work surpasses the prior work.\n\n1. Obviously, in table 6, the linear probing performance is significantly lower when the training flops are reduced to 0.64\u00d7 or 0.58\u00d7 (64.89 vs. 66,16 and 63.76 vs. 66.16). I am wondering, in the case of reducing how many flops, this work can get the on-pair performance with the default setting,\n\n2. Self-supervised learning aims to transfer the learned representations or whole network parameters into various downstream tasks. However, I do not see any transfer learning experiments in this paper. Could you provide more transfer learning experiments, for example, linear evaluation and fine-tuning in fine-grained classification tasks, semi-supervised learning, and object detection/segmentation?",
            "clarity,_quality,_novelty_and_reproducibility": "I do not think I can reproduce this work unless the author shares the code.",
            "summary_of_the_review": "See Strength and Weaknesses above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_inij"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_inij"
        ]
    },
    {
        "id": "61svxBvi92P",
        "original": null,
        "number": 4,
        "cdate": 1667200118719,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667200118719,
        "tmdate": 1667200118719,
        "tddate": null,
        "forum": "aCdREQkEMGk",
        "replyto": "aCdREQkEMGk",
        "invitation": "ICLR.cc/2023/Conference/Paper3615/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a new sparse SSL approach by exploring the correlation between in-training sparsity and SSL. This paper investigates the challenges of the sparsity-induced asymmetric SSL (a.k.a prune-and-regrow) and proposes synchronized contrastive pruning approach. The proposed approach is adaptive to various granularities of sparsity.\n",
            "strength_and_weaknesses": "Strength\n1. The paper presented a detailed analysis of the existing sparse self-supervised learning approaches.\n2. The proposed approach is novel in solving the sparsity in SSL and also improve the model performance.\n\nWeakness\n1.The paper did not give a sufficient introduction of its baseline, SDCLR. It is hard to follow the ideas without that background.\n2. The paper did not evaluate the inference complexity, e.g. inference FLOPs, latency, memory cost, etc. Without that, it is hard to justify the effectiveness of its sparsity.\n3. The proposed approach is contrastive learning focused. If the paper is scoped to the contrastive SSL, the title needs to be confined. Otherwise, the paper needs to further evaluate its performance on MIM SSL frameworks.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper proposed a novel synchronized contrastive pruning approach to solve the problems of the existing sparse SSL approaches.\n2. The paper is clearly presented in general. However, given the paper is an improvement of the existing SDCLR work, missing a brief introduction of the core framework of SDCLR makes the paper hard to follow.\n3. The paper provided the details about the experiments. A minor issue is it did not clearly present the dataset used for pretraining and linear probe.\n",
            "summary_of_the_review": "The paper presented a novel synchronized contrastive pruning approach in sparse SSL. As its main contribution, the work did not evaluate the inference efficiency which is hard to justify the effectiveness of its sparsity. The paper can be further improved by introducing the core idea of its baseline to make the paper easy to follow.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_19nX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_19nX"
        ]
    },
    {
        "id": "vDSZOEycQyx",
        "original": null,
        "number": 5,
        "cdate": 1667241681818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667241681818,
        "tmdate": 1667241681818,
        "tddate": null,
        "forum": "aCdREQkEMGk",
        "replyto": "aCdREQkEMGk",
        "invitation": "ICLR.cc/2023/Conference/Paper3615/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper enables model pruning to accelearate contrastive self-supervised training. Compared to the exsited method, this paper tries to sparsify online decoder and offline decoder simultaneously. To stabilize the sparse training. The authors propose Contrastive Sparsification Indicator (CSI) to guide the model pruning.\n\nTo evaluate the effectiveness of SyncCP, the authors conduct experiments on various classification datasets with unstructured sparsity, N:M sparsity, and structured pruning.",
            "strength_and_weaknesses": "Strength:\n\nThe paper is well written.\n\nThe experiments including various different sparse types (unstructured, N:M, structured) are persuasive.\n\nThe performance (accuracy and efficiency) improvement is significant compared to the existing method [1].\n\n\n\n\nWeaknesses:\n\nThe self-supervised learning method usually trains on large-scale datasets, but this paper doesn't show any training GPU hours improvement on large-scale datasets. I feel confused about the motivation.\n\nThe NVidia GPUs only support 2:4 sparsity, can you explain the detail implementation of inference gain about 1:4 sparsity in Table5.  \n\n\nThe BYOL and MOCO-v2 are not SOTA SSL methods, the recent SOTA methods are more convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "Na",
            "summary_of_the_review": "Na",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_5j3N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3615/Reviewer_5j3N"
        ]
    }
]