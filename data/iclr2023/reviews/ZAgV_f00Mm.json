[
    {
        "id": "RL-LwFVlxM",
        "original": null,
        "number": 1,
        "cdate": 1666640873274,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640873274,
        "tmdate": 1666640873274,
        "tddate": null,
        "forum": "ZAgV_f00Mm",
        "replyto": "ZAgV_f00Mm",
        "invitation": "ICLR.cc/2023/Conference/Paper4760/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an adaptive structured dropout method, ProbDropBlock, which drops contiguous blocks from feature maps with a probability given by the normalized feature salience values. The authors evaluate ProbDropBlock on both vision and language tasks.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper proposed an adaptive structured dropout method that is beneficial for both vision and language tasks. It indicates the potential of using structured dropout in Transformer-based models.\n2. The paper provided an thorough investigation of existing structured dropout methods.\n\nWeakness:\n\n1. The experimental settings and the performance gains are insufficient to demonstrate the effectiveness of the method. In Table 2, I am not quite convinced by the RoBERTa results, as unstructured dropout can usually achieve some gains for Transformers/ViTs. The authors may present results on more models/tasks to show that structured dropout works better in Transformers/ViTs. For a fair comparison, the authors may compare the proposed adaptive structured dropout to adaptive unstructured dropout. In Table 3, the gains are marginal (typo: the gain on QNLI should be $0.15$). \n\n2. The motivation of assigning higher drop rate to activations with higher magnitude is unclear. More ablations and analysis are needed to understand its influence to the training. It seems to impose a strong regularization but may also prevent the model from seeing important features. \n\n3. Expanding blocks based on the sampled $A_{i,j}$s may be problematic as the surrounding activations may not have high magnitude on average.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. the writing is generally clear and understandable.\n2. The idea is simple and novel. The technical contribution maybe limited: it is a combination of structured and adaptive dropout. Using magnitude as saliency score is also adapted from model pruning. ",
            "summary_of_the_review": "The paper proposed an adaptive structured dropout method that is beneficial for both vision and language tasks. It indicates the potential of using structured dropout in Transformer-based models. However, the experiment results are not sufficient to demonstrate the effectiveness of structured dropout and the proposed adaptive idea, especially in Transformers. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4760/Reviewer_zLHu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4760/Reviewer_zLHu"
        ]
    },
    {
        "id": "hgalpS4Yfk_",
        "original": null,
        "number": 2,
        "cdate": 1666671812953,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671812953,
        "tmdate": 1666671812953,
        "tddate": null,
        "forum": "ZAgV_f00Mm",
        "replyto": "ZAgV_f00Mm",
        "invitation": "ICLR.cc/2023/Conference/Paper4760/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes application of structured dropout methods (e.g. drop block) to transformers.  It also proposes an enhancement to drop block which utilizes activation values to assign dropout probability to nodes in feature maps as opposed to random selection of blocks.  Empirical results on language and vision tasks demonstrate that structured dropout is advantageous for transformer models and the proposed modification to drop block methods leads to an improvement in accuracy for language and vision tasks.\n",
            "strength_and_weaknesses": "Pros:\n* Exploration of structured dropout for transformers\n* An enhancement to drop block that shows better empirical performance as compared to the baseline approach\n* Combination with linear dropping schedule gives further gains, reconfirming the value of linear dropout schedule\n\nCons:\nN/A",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: very clear\nNovelty: sufficiently novel\nReproducibility: fairly straightforward, should be reproducible",
            "summary_of_the_review": "see comments above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4760/Reviewer_Fidd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4760/Reviewer_Fidd"
        ]
    },
    {
        "id": "-f33Upka7OY",
        "original": null,
        "number": 3,
        "cdate": 1666855636797,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666855636797,
        "tmdate": 1666855723092,
        "tddate": null,
        "forum": "ZAgV_f00Mm",
        "replyto": "ZAgV_f00Mm",
        "invitation": "ICLR.cc/2023/Conference/Paper4760/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes ProbDropBlock, an adaptive version of DropBlock in which the drop probability of the mask is defined to be negatively correlated to the absolute value of the centroid entry. Experiments are conducted on 3 NLP datasets (MNLI, QNLI, RTE) and 3 CV datasets (Cifar-10, Cifar-100, Imagenet).",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is easy to follow\n\n2. The idea of investigating both unstructured and structured dropout methods on SOTA models w.r.t. both NLP and CV tasks is well motivated and beneficial for advance of these two research tasks.\n\nWeakness:\n\n1. Experiment results of baseline method are not SOTA. For example, the (top-1) accuracy of Resnet50 on Imagenet reported in Table 5 is lower than that in Ghiasi et al. (74.22 vs 76.51). Other examples include the accuracy of WideResNet28 on CIFAR100 (which is lower than that reported in the original WRN paper). This weakens the claim that made in the introduction section about testing dropout methods on current SOTA models. Some SOTA results can be found in https://paperswithcode.com/sota/image-classification-on-cifar-100\n\n2. Lack of explanation and analysis about the adaptive dropout ratio, which is negatively correlated to the absolute value of layer output activations. It is hard to understand why the proposed method performs better than existing methods. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to follow. The proposed method about adaptive dropout ratio needs further analysis (theoretically or experimentally). Experiment settings can be found in the paper.",
            "summary_of_the_review": "In general, this paper is well motivated and focuses on an important problem which will be beneficial for future studies in both NLP and CV communities. However, both theoretical and experimental analysis are not thorough and do not support the claims well.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4760/Reviewer_EuKt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4760/Reviewer_EuKt"
        ]
    },
    {
        "id": "gccwMTRHLMT",
        "original": null,
        "number": 4,
        "cdate": 1667541398904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667541398904,
        "tmdate": 1667541398904,
        "tddate": null,
        "forum": "ZAgV_f00Mm",
        "replyto": "ZAgV_f00Mm",
        "invitation": "ICLR.cc/2023/Conference/Paper4760/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Structured dropout are methods that determine probability of deactivation based on certain inductive priors such as neighborhood or location, which are important in architectures like CNN.  \nThe author surveyed existing structure dropout method and developed improved version based on block dropout called PROBDROPBLOCK. which assign different deactivation probability based on activation levels. It is interesting to see these method work well in transformers. ",
            "strength_and_weaknesses": "strength: clear motivation ,simple method, wide impact \n\nweakness: this manuscript serve well as a systematic review article , but lack of novel in method as a original research article by ICLR standard. The motivation or theoretical analysis of why choosing to conduction stochastic dropout in PROBDROPBLOCK is unclear. ",
            "clarity,_quality,_novelty_and_reproducibility": "very  clear, reproducible, good writing quality but lack of novelty in method ",
            "summary_of_the_review": "Although lacking novelty in method, due to the importance and popularity of dropout in machine learning and the well designed experiments, I think this could bring values to the community  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4760/Reviewer_osqf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4760/Reviewer_osqf"
        ]
    }
]