[
    {
        "id": "pzqXOqy71V",
        "original": null,
        "number": 1,
        "cdate": 1665584946232,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665584946232,
        "tmdate": 1665584946232,
        "tddate": null,
        "forum": "jpsw-KuOi7r",
        "replyto": "jpsw-KuOi7r",
        "invitation": "ICLR.cc/2023/Conference/Paper3397/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the sample complexity of reward-free learning in low-rank MDP and achieves sharp upper and lower bounds. The learned representations can also be reused in a different MDP.",
            "strength_and_weaknesses": "Pros:\n-The paper achieves the SOTA upper and lower bounds for reward-free learning in low rank MDP. This enhances the understanding of low rank MDP of the RL theory community.\n\nCons:\n-I think the authors may want to emphasize more on the difference between different setup in function approximation. For example, citing the Bellman rank paper [1] for the setting of low-rank MDP may not be very accurate. As I understand low-rank MDP is a special case of low Bellman-rank MDP? As a result, comparing results in different setting may involve further complications. The authors may want to elaborate that a little bit.\n- The table given is very clear. It would be even better if the sample complexities of algorithms in the normal learning setting can also be compared with the proposed algorithm. This will be helpful as it showcase the difference between the normal setting and reward-free setting.\n- It's not clear technique-wise what's the new ingredient that makes the proposed algorithm performs better then the existing ones. The novel termination rule is explained but I believe it is only part of the story. More concretely, it makes me think of the algorithm is a combination of the technique from zero-reward exploration in tabular MDP ([2][3]) and the MLE techniques in FLAMBE. Am I missing anything important here?\n- There is some concurrent work also using an optimistic MLE-based approach for different settings (POMDP), such as [4][5]. I know this is not necessary, but it would be a plus if you could also compare it with the algorithmic design and proof technique in these papers.\n\n[1] Contextual Decision Processes with Low Bellman Rank are PAC-Learnable\n[2] Task-agnostic Exploration in Reinforcement Learning\n[3]  A Sharp Analysis of Model-based Reinforcement Learning with Self-Play.\n[4] Optimistic MLE\u2014A Generic Model-based Algorithm for Partially Observable Sequential Decision Making\n[5] Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms",
            "clarity,_quality,_novelty_and_reproducibility": "The exposition is clear and the results look sound to me.\nThe novelty of this paper may require more support, as a I pointed out in the weakness part.",
            "summary_of_the_review": "The paper achieves the SOTA upper and lower bounds for reward-free learning in low rank MDP. This enhances the understanding of low rank MDP of the RL theory community. Certain clarification of technical contribution will be super useful to further improve the quality of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3397/Reviewer_kHZp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3397/Reviewer_kHZp"
        ]
    },
    {
        "id": "aq2pMcJOaH",
        "original": null,
        "number": 2,
        "cdate": 1666558172359,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558172359,
        "tmdate": 1666558172359,
        "tddate": null,
        "forum": "jpsw-KuOi7r",
        "replyto": "jpsw-KuOi7r",
        "invitation": "ICLR.cc/2023/Conference/Paper3397/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work focused on reward-free RL with low-rank MDP. The author proposed a new model-based reward-free (RAFFLE) algorithm and provided a theoretical guarantee with polynomial sample complexity. In addition, the author offers a lower sample complexity bound for any algorithm within the low-rank MDP. Finally, the author shows it is possible further to learn a provably near-accurate representation of the transition kernel.\n\n",
            "strength_and_weaknesses": "Strength:\n\nThe author proposed a novel model-based reward-free (RAFFLE) algorithm and improved the sample complexity in low-rank MDP. In addition, the theoretical lower bound also supports the efficiency of the RAFFLE algorithm.\n\nWeakness:\n\n1. The lower bound on sample complexity (Section 5.2) ignores an essential requirement that $d\\ge K$. More specifically, according to figure 1 in Appendix C, the first layer have $L/K$ different states, and the second layer has $L$ states. In this case, the number of different states is at least $K$ since the first layer should have at least one state. Thus, the dimension $d=L\\ge K$ and the hard-to-learn instance only hold when $d\\ge K$.\n\n2. Based on weakness 1, the claim that reward-free RL under low-rank MDPs is strictly harder than linear MDPs is incorrect. To make this claim, it is necessary to create an instance that shows the lower bound for low-rank MDPs is strictly larger than the upper bound for linear MDPs. However, according to weakness 1, the lower bound only holds for $d\\ge K$ (large $d$ regime), and the lower bound does not contradict the $O(d^3H^3/\\epsilon^2)$. With a similar argument, we can easily extend the lower bound of linear MDP $O(d^2/\\epsilon^2)$ to $O(d|S|/\\epsilon^2)$ with the regime $d\\ge |S|$ and mentions \"linear MDPs is strictly harder than low-rank MDPs since the lower bound to have an additional dependency on $|S|$.\"\n\n3. The comparison with Uehara et al. (2022b) is incorrect. Uehara et al. (2022b) calculate the sample complexity with the number of steps, while this work measures the sample complexity with the number of trajectories. There always exists a gap of H between trajectories and steps. Thus, the RAFFLE algorithm only improves a factor of $H$ rather than $H^2$.\n\n4. It would be better if the author could provide more intuition on why Lemma 2 holds. The author mentions it is an extension of Lemma 12 in Uehara et al. (2022b) to episodic MDPs. However, in Uehara et al. (2022b), both the triple at stage $h-1$ and state $h$ are used to estimate the model (MLE). It would be better if the author could provide more intuition as to why only using the data at stage $h$ can maintain a similar performance or the uniform choose action $a_{h-1}$ is unnecessary.\n\n5. It is strange why the RAFFLE can always find the correct representation $\\phi^*$ in Theorem 4. More specifically, even for a known transition kernel $P$, there may still exist multi-representation for $P$ in the lower-rank MDP. For instance, if we switch the first and second dimensions of $\\phi_1,\\mu_1$ and obtain $\\phi_2,\\mu_2$, then two representations will lead to the same transition kernel. Under this situation, it seems impossible to learn the actually representation $\\phi^*$ from $\\phi_1$ and $\\phi_2$.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow.",
            "summary_of_the_review": "This work proposes a novel algorithm (RAFFLE) for learning reward-free low-rank MDPs and improving the sample complexity. However, it seems that the lower bound ignores an essential requirement, and the author makes a wrong claim, \"reward-free RL under low-rank MDPs is strictly harder than linear MDPs is incorrect.\" as a contribution. In addition, I am concerned about why it is possible to distinguish two representations in Theorem 4, when they correspond to the same transition kernel $P$.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3397/Reviewer_rgZQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3397/Reviewer_rgZQ"
        ]
    },
    {
        "id": "W64xDbIUQJ",
        "original": null,
        "number": 3,
        "cdate": 1666599421195,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599421195,
        "tmdate": 1666599421195,
        "tddate": null,
        "forum": "jpsw-KuOi7r",
        "replyto": "jpsw-KuOi7r",
        "invitation": "ICLR.cc/2023/Conference/Paper3397/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new model-based algorithm, coined RAFFLE, and show that it can both find an $\\epsilon$-optimal policy and achieve an $\\epsilon$-accurate system identification via reward-free exploration, with\na sample complexity of $\\tilde{O}(H^3d^2K(d^2+K))$, where $d,H$ and $K$ respectively denote dimension, episode horizon, and action space cardinality. This significantly improves the sample complexity in Agarwal et al. (2020) for the same learning goals.\n\nWe further provide a sample complexity lower bound that holds for any reward-free algorithm under low-rank MDPs, which matches our upper bound in the dependence on $\\epsilon$, as well as on $K$ in the large $d$ regime.",
            "strength_and_weaknesses": "Strength: propose a new model-based reward-free RL algorithm under low-rank MDPs. The algorithm can both find an \u03b5-optimal policy and achieve an \u03b5-accurate system identification via reward-free exploration, with a sample complexity of $\\tilde{O}(H^3d^2K(d^2+K))$. A lower bound on the sample complexity that holds for any reward-free algorithm under low-rank MDPs.\n\nWeakness: \n\nThe main weakness of this paper is the algorithm seems close related to [1]. Note reward-free multi-task is a more general setting than the current study and the exploration of RAFFLE seems to be very similar to REFUEL in [1] (with $T=1$). In addition, the main contribution of this paper seems to have overlap with [1]. For instance, (6) of Theorem 4.2. is a system identification result and (7) is the reward-free learning result. At this moment, I suspect the result of the current paper might be a corollary of [1]. It is likely I might be wrong, so please correct me if I misunderstood something.\n\n[1] Provable benefit of multitask representation learning in reinforcement learning, NeurIPS 2022\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written overall.",
            "summary_of_the_review": "Please answer my major concern.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3397/Reviewer_2hcZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3397/Reviewer_2hcZ"
        ]
    },
    {
        "id": "ATK1Q2M729",
        "original": null,
        "number": 4,
        "cdate": 1666636532384,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636532384,
        "tmdate": 1666647100972,
        "tddate": null,
        "forum": "jpsw-KuOi7r",
        "replyto": "jpsw-KuOi7r",
        "invitation": "ICLR.cc/2023/Conference/Paper3397/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper combines the REP-UCB and RF-UCRL to obtain an improved sample complexity for reward-free reinforcement learning under the structural assumption on Low-rank MDPs.\n",
            "strength_and_weaknesses": "### Strength:\n* The idea is simple and easy to follow.\n* The interpretation of the lower bound is interesting.\n\n### Weakness:\n* The proof of the lower bound lacks some rigor.\n* Section 5.3 is not well-motivated under the context of RL. In other words, I don\u2019t quite understand how this divergence will help to analyze the performance of reinforcement learning.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear with limited novelty.",
            "summary_of_the_review": "For me the combination of REP-UCB and RF-UCRL is not so striking. The most interesting part for me is the lower bound, where the authors argue that we cannot avoid the dependency on the number of actions if we need to learn the representation. However, as the proof lacks some rigor, I'm not sure if the proof is correct. For me, intuitively the lower bound is correct, as for the instance the authors consider, if we know the exact feature, we already know which action is different. I believe the authors need to make the proof of the lower bound much more formal. The analysis in Section 5.3 should be much more well-motivated.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3397/Reviewer_exQF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3397/Reviewer_exQF"
        ]
    }
]