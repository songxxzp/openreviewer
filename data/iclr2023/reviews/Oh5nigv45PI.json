[
    {
        "id": "eg-sE9is3C4",
        "original": null,
        "number": 1,
        "cdate": 1666313007592,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666313007592,
        "tmdate": 1666313007592,
        "tddate": null,
        "forum": "Oh5nigv45PI",
        "replyto": "Oh5nigv45PI",
        "invitation": "ICLR.cc/2023/Conference/Paper4881/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper starts with a variant of accelerated gradient flow considered in [Luo & Chen 2021], and added noise in a degenerate, underdamped Langevin alike fashion. The resulting SDE is then discretized semi-implicitly, resulting in a \u201cstochastic gradient\u201d optimization algorithm. \u201cConvergence analysis\u201d is conducted on quadratic loss. The method is then empirically tested, and performances comparable to existing optimizers were demonstrated on a collection of important downstream tasks.",
            "strength_and_weaknesses": "Strength: \n\n(1) The Gauss-Seidel splitting and semi-implicit discretization is nice. It is great to see important knowledge in numerical analysis applied to optimization / machine learning problems.\n\n(2) I also like the diversity of empirical tests.\n\nWeakness:\n\n(1) Since I could have missed the point, could the authors please explain again why the new method is important, if it produces comparable performance to existing approaches?\n\n(2) Stochastic Gradient is not the same as (deterministic) batch-gradient plus constant covariance Gaussian noise. While there is still active research studying whether the noise of SG is Gaussian or heavy-tailed (e.g., [G\u00fcrb\u00fczbalaban et al. The heavy-tail phenomenon in SGD. ICML 21]), at least its covariance should be state-dependent (e.g., [Li et al. Stochastic modified equations and adaptive stochastic gradient algorithms. ICML 17]). The assumption in this paper, namely an isotropic additive noise, is unrealistic. Normally I would suggest to soften the claim of \u201cSGD\u201d, but since an SGD version of the previous result in [Luo & Chen 2021] is the main point, I am a little confused about the contribution.\n\nIn addition, the derivation up to equation (15) is based on isotropic additive Gaussian noise, denoted by $\\eta_k$, but this noise disappeared in Algorithm 1. I can\u2019t find $\\eta$ or even its amplitude $\\sigma$ and became puzzled. My guess is $\\nabla f$ in Algorithm 1 is no longer the deterministic full-batch gradient used before, but a stochastic gradient based on mini-batch. But if this is the case, the derivation is a little disconnected from SG (see my above point(2)).\n\n(3) I wish the theoretical part could be stronger. The current status of the field is well beyond convergence analysis for quadratic objectives (Thm.1). In fact, if I didn\u2019t misunderstand, Thm.1 is for a deterministic setup. Appendix B empirically investigated the stochastic setup, based on a statement that the stationary distribution of (11) remains unknown (and thus implying theoretical analysis is out of reach). But it is a standard procedure to characterize the stationary distribution: (11) is a linear SDE and its stationary distribution is just a Gaussian, whose covariance can be solved for algebraically via Ricatti equation. Besides, I missed why the part of Appendix B in page 29 discussed overdamped Langevin instead of equation (11).\n\nBack to the convergence analysis of the full-blown nonlinear SDE, could the authors please explain why tools in, e.g., [Ma et al. Is there an analog of Nesterov acceleration for gradient-based MCMC? Bernoulli 21] and [Li et al. Hessian-free high-resolution Nesterov acceleration for sampling. ICML 22] cannot be used to quantify the speed of convergence in a more general setup?\n",
            "clarity,_quality,_novelty_and_reproducibility": "I thank the authors for good clarity. I would be able to better assess the originality and quality once the aforementioned weaknesses are discussed.",
            "summary_of_the_review": "The Gauss-Seidel splitting and semi-implicit discretization is definitely an interesting idea, and I strongly encourage the authors to explore it a bit more, possibly with improved theory so that the benefits could be made quantitative. For now, I\u2019m not very sure about either its algorithmic or theoretical impact, and thus unable to provide a very positive recommendation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4881/Reviewer_ksyw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4881/Reviewer_ksyw"
        ]
    },
    {
        "id": "8vJ-ktFZVt",
        "original": null,
        "number": 2,
        "cdate": 1666577767300,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577767300,
        "tmdate": 1666577767300,
        "tddate": null,
        "forum": "Oh5nigv45PI",
        "replyto": "Oh5nigv45PI",
        "invitation": "ICLR.cc/2023/Conference/Paper4881/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the NAG-GS optimizer. The algorithm is designed by considering the dynamics of an accelerated systems of SDE discretized via Gauss-Seidel (GS) splitting. NAG-GS is theoretically analyzed on quadratic problems. Numerical experiments, including logistic regression, finetuning RoBerta on downstream tasks, and image classification via ResNet, are carried out to illustrate the usefulness of the proposed method.",
            "strength_and_weaknesses": "**Strength:**\n- (S1) Applying GS for discretizing SDEs is interesting and worth careful investigation.\n- (S2) Theoretical analysis, though limited to quadratic problems, is still insightful to understand the behavior of NAG-GS.\n\n**Weakness:**\n\nThe main weakness of this work lies in the numerical experiments. Confined by computational resources, the results do not support the efficiency of proposed method well. In particular, in many test cases, NAG-GS does not catch up with AdamW or SGDm. And sometimes the gap is not small. In particular, \n- (W1) it can be beneficial to run tests on quadratic functions to support the theoretical findings.\n- (W2) The experiment setups for logistic regression in Section 3.1 is not clear. In addition, it is more clear if the test accuracy can be summarized in a table for comparison. Moreover, when the learning rate is small, SGDm seems to have better performance over NAG-GS. For the relatively simple logistic regression problem, it does not bother too much for hyperparameter tuning. In other words, a better test accuracy is more important.\n- (W3) In section 3.2, the authors write \u201cAdamW step size is $10^{-5}$, which is much smaller than $10^{-2}$ of NAG-GS\u201d. In practice, it is common for Adam type algorithm to use a smaller learning rate than SGD. More importantly, on more than half downstream tasks, NAG-GS fails to catch up with AdamW.\n- (W4) It is unclear for the reason of switching acc@1 to acc@5 in section 3.3. Once again, NAG-GS does not offer competitive results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The theory part is well presented, but not the numerical results. \nThe algorithm is novel, but not well supported in experiments.\n",
            "summary_of_the_review": "This paper puts more weight on the theoretical side, but the performance in typical machine learning tasks confines its practical potential.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4881/Reviewer_Dog8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4881/Reviewer_Dog8"
        ]
    },
    {
        "id": "Vm_MUw-xXbj",
        "original": null,
        "number": 3,
        "cdate": 1666646538962,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646538962,
        "tmdate": 1666646538962,
        "tddate": null,
        "forum": "Oh5nigv45PI",
        "replyto": "Oh5nigv45PI",
        "invitation": "ICLR.cc/2023/Conference/Paper4881/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers a class of accelerated algorithms and provides some analysis for noisy inputs to the algorithm. It is shown that the algorithms will converge even when presented with gradients corrupted with gaussian noise on quadratic losses. Experimental results on deep learning benchmarks show competitive performance with AdamW.\n",
            "strength_and_weaknesses": "\nThe semi-implicit algorithm is general well-motivated and presented. However I am confused on a couple points: the equations 14 do not appear to match exactly the pseudo-code Algorithm. For example, should we not set $x_{k+1} = (1+\\alpha_k)^{-1}(x_k \\alpha_k v_k)$ rather than $x_{k+1} = (1-\\alpha_k) x_k + \\alpha_k v_k$? Similar issue may be involved with the $v_k$ update.\n\n\nIn regards to the theoretical convergence results, these seem to be asymptotic statements that the algorithm will eventually converge on quadratic losses with gaussian noise in the gradients. This seems rather weak given the current state of the art in acceleration under unbiased gradient noise (e.g. see https://proceedings.mlr.press/v119/joulani20a/joulani20a-supp.pdf), which establishes non-asymptotic convergence rates for general convex smooth function under arbitrary unbiased gradient noise with bounded variance without requiring knowledge of several problem parameters.\n\nIn regards to the experiment, it seems to be slightly worse than SGD on computer vision tasks, and comparable to AdamW on transformer fine-tuning tasks. All in all, this does not seem convincing that we should switch from AdamW. Is it possible perhaps to make an argument that this method requires less tuning, as was hinted at in the text?\n",
            "clarity,_quality,_novelty_and_reproducibility": "There are a few typos, but no significant concerns.",
            "summary_of_the_review": "The theoretical results here seem to be in a somewhat preliminary state and the empirical results do not seem strong enough to justify acceptance on their own.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4881/Reviewer_AnLK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4881/Reviewer_AnLK"
        ]
    },
    {
        "id": "DPu7D7OYGu",
        "original": null,
        "number": 4,
        "cdate": 1666673082031,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673082031,
        "tmdate": 1670713061142,
        "tddate": null,
        "forum": "Oh5nigv45PI",
        "replyto": "Oh5nigv45PI",
        "invitation": "ICLR.cc/2023/Conference/Paper4881/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": " This paper presents the NAG-GS algorithm, a Gauss-Seidel type discretization of the Nesterov-like SDE. NAG-GS maintains the acceleration nature while improving the robustness to the section of learning rate. Numerical simulations are implemented to show that NAG-GS is competitive with state-of-the-art methods. ",
            "strength_and_weaknesses": "## Strength\n- This paper is well-organized and relatively easy to follow. The proofs seem to be theoretically sound. The theoretical background motivates the algorithm smoothly, and the proposed method is achieving great empirical performance.\n\n## Weaknesses\n- Although simulations are implemented for various settings, the theory only holds for the quadratic functions.\n- The theory only shows NAG-GS is convergent (for quadratic functions). Other properties such as the convergence rate and improvement of the range of learning rate are missing. \n\n===========================\nI have read other reviewers' responses and authors' comments. I basically agree with other reviewers. Although applying semi-implicit discretization to SDE of NAG is novel and worth more future exploration, the current theoretical results provided in this paper are preliminary. Also, the experiments cannot convince me one should switch to NAG-GS in practice in sufficiently many settings. Hence, I'll keep my score. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this work is clear and well-organized and the novelty looks good to me. ",
            "summary_of_the_review": "In summary, I think this is nice work with good intuition and theoretical background that motivates the final algorithm smoothly. I hope to see more detailed theoretical results such as convergence rate and the improvement of the learning rate, as these two properties are stated as the main advantage of the proposed NAG-GS algorithm. It would be fantastic if the theory could be extended to the more general problems such as optimizing functions in $\\mathcal{S}_{L, \\mu, \\mu}^{1,1}$.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4881/Reviewer_DiaF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4881/Reviewer_DiaF"
        ]
    }
]