[
    {
        "id": "nGOJB9_5Gec",
        "original": null,
        "number": 1,
        "cdate": 1666618979348,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666618979348,
        "tmdate": 1666618979348,
        "tddate": null,
        "forum": "hFCUPkSSRE",
        "replyto": "hFCUPkSSRE",
        "invitation": "ICLR.cc/2023/Conference/Paper4504/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the method of few-shot prompting for internet-augmented language models consisting of three steps: 1) retrieval, which searches top 50 paragraphs using Google search, 2) prompting, where four prompts are applied on each of top 50 retrieved paragraphs to generate 200 candidate answers in total, 3) reranking, which reranks these candiate answers. In particular, authors present various methods for the \u201creranking\u201d step \u2013 RAG-style inference, noisy channel inference, and PoE. Experiment results on NQ, HotpotQA, FEVER, and StrategyQA show that the proposed few-shot internet-augmented LMs make further improvements, comparing to the closed book model, Gopher 280B, under both no-reranking and reranking settings. When evaluating the performance with respect to inference time, the proposed few-shot prompting produces better performances than the closed book model, when using similar inference time (i.e. FLOPS). \n\nThe key contribution of the paper is the simple but novel few-shot prompting for using internet search to improve the large language model under the autoregressive Transformer architecture, thereby confirming that that retrieval-augmented models are more promising and effective than the huge closed book model (i.e., Gopher 280B). \n",
            "strength_and_weaknesses": "Strengths\n\n- This paper is well written and easy to follow the major idea and content. \n- The experiment results are solid, showing consistent improvements comparing to the huge closed book model. \n- The proposed methods are simple but valuable to be explored, particularly presenting a new approach for the few-shot \u201cin-context\u201d learning in retrieval augmented language model under the autoregressive transformer. \n\nWeaknesses\n\n- Only google search results are used for retrieval. It is not clear whether the improvement come from the high quality of google\u2019s retrieved contents or from the retrieval-augmented method. Other collection sources for retrieval such as Wikipedia or the training corpus of Gopher 280B also need to be used for evaluation, in addition to Google\u2019s results on internet. In other words, the authors further need to whether the improvements over CB (in Table mainly come from by keeping or perfectly memorizing the corpus used for pretraining CB, or whether it comes from the use of new but well-matched corpus that is NOT seen when pretraining CB. \n- The proposed method is applied to the autoregressive transformer using the \u201cin-context\u201d learning. But, it is not clear how the method is extended to the encoder-decoder model like T5. \n- Evaluation datasets are limited. KILT or other knowledge-intensive datasets need to be further used for evaluation. \n- What is the advantage comparing to other few-shot learning of retrieval-augmented models, such as:\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, Few-shot Learning with Retrieval Augmented Language Modelsm 2022:\n\n   How the proposed method is complementary applied to the above few-shot learning method? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The most parts of the paper are clear, and easy-to follow given its well-written style. But, an exception is that the method of product-of-experts (PoE) is clearly described. Also, the equation for direction inference needs to be checked: should the max be used instead of the summation? \n\nThe proposed method of the few-shot \u201cin-context\u201d learning for internet-augmented model in the GPT3-style autoregressive language models is novel, which has not been explored yet. \n",
            "summary_of_the_review": "Overall, this paper is well-written and easy to follow. The proposed idea of few-shot prompting for internet-augmented language models is novel, interesting, and timely proper in the literature of the autoregressive language model. Experiment results are solid, showing improvements over the baseline huge Gopher 280B, and Gopher\u2019s performance can be achieved by much smaller model. The concluded direction of reconsidering towards huge pretrained models reasonably run from the experiments in the paper.\n\nFurther comments: \n\n- In the right of Figure 3, CB 280B is only plotted. But, it would be better or required to plot the results of other smaller CB models.\n- In Table 1, the results may rely on the performance of the retrieval. In addition to using other resources like Wikipedia, can we control the retrieval performance by randomly putting noise irrelevant passages and check the degree of the performance from which the final QA performance start to decrease?  \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4504/Reviewer_rQxP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4504/Reviewer_rQxP"
        ]
    },
    {
        "id": "JVSQ1E-IK0",
        "original": null,
        "number": 2,
        "cdate": 1666650190530,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650190530,
        "tmdate": 1666650190530,
        "tddate": null,
        "forum": "hFCUPkSSRE",
        "replyto": "hFCUPkSSRE",
        "invitation": "ICLR.cc/2023/Conference/Paper4504/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper mainly tackles open-domain question answering. It uses Google search to obtain web information to prompt large-scale language model for answer generation.",
            "strength_and_weaknesses": "Strength:\n1. This paper utilizes web-scale information through google search for downstream tasks like open-domain QA.\n2. It proposes 4 methods to rerank the generated answers through LM produced probability without fine-tuning.\n3. The proposed methods achieve effective results on 4 downstream datasets. This paper also provides analysis on the model scale and inference time, and the ability to answer questions grounded in different dates.\n\nWeakness:\n1. The method is only tested on the Gopher model. It would make this paper much stronger if it's also tested on other large-scale LMs such as OPT and GPT-3.\n2. More experiments should be done: How does the number of shots k affect the model performance? Can we improve the results by simply increasing k? Another baseline is to simply combine multiple paragraphs into one paragraph and pass into the model instead of generate different answers using each paragraph separately. In this case we hope the LM can produce more accurate answers by joint inference over multiple paragraphs. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the novelty and quality of this paper are good. But the clarity about different reranking methods is a concern. This paper needs to elaborate more on each reranking method and the difference between them. Currently there's just one-sentence description for each method. Especially for Product-of-Experts (PoE), which is the most effective one. How to combine those probabilities is not clear.\nMoreover, the reproducibility is limited since it's only tested on the Gopher model, which seems not publicly available either through API or model weights. ",
            "summary_of_the_review": "Although this paper has weaknesses on insufficient experiments, it has contributions in pushing the research about large-scale LM inference and its combination with web-scale knowledge. Thus my recommendation is marginally above the acceptance threshold. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4504/Reviewer_YZVR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4504/Reviewer_YZVR"
        ]
    },
    {
        "id": "CkN8TsjoIP8",
        "original": null,
        "number": 3,
        "cdate": 1666688998759,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688998759,
        "tmdate": 1666936952253,
        "tddate": null,
        "forum": "hFCUPkSSRE",
        "replyto": "hFCUPkSSRE",
        "invitation": "ICLR.cc/2023/Conference/Paper4504/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to improve the use LLMs on open-domain QA tasks, via searching up-to-date information from internet, few-shot prompting, and inference-time computing with the searched-out information. Specifically,\n\n(1) Retrieve information from Google Search.\n\n(2) Input a few existing examples as the prefix to prompt LLM.\n\n(3) In inference time, re-rank the answers with different retrieved paragraphs as evidence.",
            "strength_and_weaknesses": "Strengths:\n* The paper is well-written and easy to understand.\n* Plenty of empirical discussion and study.\n\nWeakness:\n* The motivation of this work is unclear. It reads like the most significant contribution is using Google search as the information retriever compared with previous work using other search engines on Wikipedia, etc.\n* The technical contribution is incremental, without significant novelty. The 3 proposed methods can all be summarized as hyperparameter/model tuning on the retrieval-based LMs, i.e., tuning the search engine, input prompting, and ensemble strategy.\n* Although the authors have a lot of discussions on their empirical results, they don't finally deliver impressive insights or conclusions, besides using Google search and prompting performs better.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good clarity and easy to understand, but the motivation is unclear and the novelty is limited.",
            "summary_of_the_review": "Although the paper is well-written and with plenty of experiments and discussions, the proposed approach is not well-motivated and doesn't have significant originality and looks just like training a model towards the benchmark by tuning some modules in a retrieval-based LM and using a very large LM as the backbone.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4504/Reviewer_z3mQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4504/Reviewer_z3mQ"
        ]
    },
    {
        "id": "GD-pXcWYA1",
        "original": null,
        "number": 4,
        "cdate": 1667073158389,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667073158389,
        "tmdate": 1670227802139,
        "tddate": null,
        "forum": "hFCUPkSSRE",
        "replyto": "hFCUPkSSRE",
        "invitation": "ICLR.cc/2023/Conference/Paper4504/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a simple semiparametric few-shot prompting model for open-domain question answering. For the retriever component of the model, the model uses google search instead of Wikipedia. To account for the long length of documents, each doc is broken into a list of paragraphs where each paragraph is a sequence of 6 sentences. Next, the paragraphs are re-ranked based on Tf-idf similarity within the document. \n\nThe prompts for each task are static (i.e. does not change for each query) and consist of 15 (evidence, question, answer) tuples. No parameter training is required. \n\nAnother contribution of the paper is to demonstrate that performance of these models can be improved by increasing computing at inference time, i.e., by sampling multiple answers from the model and re-ranking them by the probabilities computed by the model. The paper explores three kinds of ranking - (i) directly estimating p(a|q) by marginalizing an evidence latent variable, (ii) estimating the joint probability of answer and question given the evidence, and (iii) Product-of-Experts which combines (multiplies?) different probabilities from the model. All the probabilities are estimated directly from the model except for p(evidence | question) which is estimated by normalizing the tf-IDF score. The paper should definitely write more to motivate each of the re-ranker, currently, the writing of this part is too short, and especially this is one of the salient contributions of the paper.\n\nThe underlying LM used is Gopher LM with varying number of model parameters (44M, 117M, 400M, 1B, 7B, 280B).\n\nThe model is tested on 3 open-domain QA datasets (NQ, HotpotQA, StrategyQA)  and a fact-verification dataset (Fever). The proposed semiparametric model outperforms its corresponding closed-book counterparts. However, the best performance of the models is far behind the state-of-the-art (which is surprising) and I thought the paper did a bad job of explaining this difference in performance and what could be done to fix it.",
            "strength_and_weaknesses": "**Strengths**\n\n- The proposed approach of the paper is simple. I also agree with the conclusion that instead of racing towards building the largest model, we should carefully explore the few-shot capabilities of mid-sized models.\n- Apart from the short motivation of the re-ranking methods, the paper is well-written and readable\n\n**Weaknesses**\n\n- My main concern about the paper is its very limited technical novelty. It uses Google search for retrieval and a set of static prompts (per dataset) to query the LLMs. It proposes three re-ranking methods (without much motivation). Using Google search for retrieval and static prompting has been explored before. There also is a lot of work that explores re-ranking. Although the paper claims that obtaining probabilities/scores from LMs have not been explored, there exist works such as Izacard and Grave 2021, ART (Sachan et al 2022) that use scores from LMs.\n- The paper also does not explore the easy-to-try extension of prompting. For example, retrieving similar questions w.r.t a given question. This is easy to try and might work better and I would argue is a more practical prompting strategy than the current strategy of designing dataset-specific prompts.\n- Another concern regarding the paper that I have is that most of the results are pretty well-known - e.g. performance of open-book models are better than parametric models, and Google search is better. One interesting result was Gopher 7B + retrieval was better than Gopher 280B without retrieval for NQ and HotpotQA. However, this result does not transfer to reasoning benchmarks, so I am not sure if this result is widely applicable.\n- Can you explain the product-of-experts and noisy-channel re-ranking in more detail? It would be also good to know the motivation behind applying this.\n- Even though open-book models perform better than closed-book models the overall results when compared to SoTA performance is quite lacking. For example (55.9 v/s 38.4 in NQ, 65.2 v/s 30.3 in HotpotQA). Although it is not required for a paper to achieve state-of-the-art for publication, but I couldn't find any discussion of this wide gap in results and what could be done to fix it. I think the paper would benefit a lot if a section is added which discusses how can this gap be possibly closed.\n- Table 1 shows the retrieval performance of Google search and the text in section 5.1 compares that to DPR and MDR. Can you also report the retrieval performance wrt more recent (and better) retrievers such as contriever, etc.\n- [Minor] The abbreviation LSLM is first introduced in Section 3 and used in a few places later, but it is unclear what it stands for. Large Semiparametric Language Models?\n- [Minor] The second paragraph of Sec A.4 seems to be out of place.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: I think the paper is easy to understand with the exception of an important section (3.3) where the re-ranking is discussed. I think the paper could also benefit from a discussion on how to close the performance gap.\n\nQuality: The paper does a good job in the experimental setup to show us that open-book models is better than closed-book models. Although useful, this is well-known in the community. I still think this is a high quality work with very limited novelty\n\nNovelty: The novelty of the paper is very lacking. Using google search, static prompting are things that has been explored. I also think the paper could have tried other dynamic prompting approaches. The re-ranking section was not motivated well and the re-rankers itself is not novel. \n\nReproducibility: Unfortunately, very few institutions have the resources to reproduce results on 280B LLMs. However, I appreciate the experiments on the open-source OPT-6B models.",
            "summary_of_the_review": "Given the lack of novelty, and the wide gap in performance w.r.t SoTA and also the lack of discussion addressing that result, I am unfortunately tending towards rejecting the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4504/Reviewer_H18i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4504/Reviewer_H18i"
        ]
    }
]