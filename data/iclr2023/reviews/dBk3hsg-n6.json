[
    {
        "id": "ppuAdPUMcn",
        "original": null,
        "number": 1,
        "cdate": 1666582370104,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582370104,
        "tmdate": 1666582370104,
        "tddate": null,
        "forum": "dBk3hsg-n6",
        "replyto": "dBk3hsg-n6",
        "invitation": "ICLR.cc/2023/Conference/Paper5379/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends the context-dependent gating (XDG) approach to avoid catastrophic forgetting in life-long learning, i.e. learning a new task might overwrite the weights learned for another task. XDG randomly disabled 20% of the neurons in each layer to ensure the creation of distinct neural ensembles for different tasks. This paper proposes to learn these input-dependent gating masks using a neural network. The authors showed that such a network yields superior performance for solving a  continual learning benchmark such as permuted MNIST and rotated MNIST in object recognition. ",
            "strength_and_weaknesses": "Strengths: The paper addresses an important problem, and the results represent a significant advance relative to Masses, Grant and Freedman's PNAS paper.  The results, relative to EWC and no constraint are compelling. \nWeakness: It is not clear this is the state of the art in continual learning, which is a very active area of research in ML, and has a leaderboard with many different tasks and many algorithms.  This paper does not address any of those tasks. The permuted MNIST and rotated MNIST might be too simple. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clear enough. ",
            "summary_of_the_review": "The paper proposes an extension of the XDG approach to alleviate catastrophic forgetting in life-long learning, with compelling results.  Using a neural network to learn the context dependent gating is new in this context. However, the algorithm has not been rigorously compared with other state of the art algorithms and against more challenging benchmark in  the field of continual learning.  It has promise but the  technical innovation and implications currently are quite limited. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5379/Reviewer_6wb8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5379/Reviewer_6wb8"
        ]
    },
    {
        "id": "iLb4Isn9zC",
        "original": null,
        "number": 2,
        "cdate": 1666650275550,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650275550,
        "tmdate": 1666650275550,
        "tddate": null,
        "forum": "dBk3hsg-n6",
        "replyto": "dBk3hsg-n6",
        "invitation": "ICLR.cc/2023/Conference/Paper5379/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Context-Dependent Gating (XDG), proposed by Masse, Grant, and Freedman, showed promise in dealing with catastrophic forgetting. In that work, gating is randomly chosen before training. This work follows up with the obvious next question: how to learn gating for different tasks. Authors show Learned Context Dependent Gating (LXDG), combined with Elastic Weight Consolidation (EWC), outperforms existing methods on permuted MNIST and rotated MNIST datasets. ",
            "strength_and_weaknesses": "Strengths:\nThe paper systematically compares the new method to alternatives and provides clear evidence of superiority.\n\nWeaknesses:\nI see no significant weakness.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and of expected quality. While learning of gating functions is an obvious thought, someone needed to properly explore and implement it. The design of appropriate regularization terms for the gating function is novel to me. Since this work is mostly experimental in nature, reproducibility is hard for me to judge.",
            "summary_of_the_review": "This work is a natural extension of XDG, well executed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5379/Reviewer_LEqE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5379/Reviewer_LEqE"
        ]
    },
    {
        "id": "X-mMEwF0KK",
        "original": null,
        "number": 3,
        "cdate": 1666657479683,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657479683,
        "tmdate": 1666657479683,
        "tddate": null,
        "forum": "dBk3hsg-n6",
        "replyto": "dBk3hsg-n6",
        "invitation": "ICLR.cc/2023/Conference/Paper5379/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the problem of catastrophic forgetting in deep neural networks by introducing a complementary network for gating the neurons. This second network is trained based on the same input to produce task-aware gates which are sparse and regularized to be similar within each task while orthogonal between the tasks. The authors test their architecture on permuted- and rotated-MNIST tasks and show that their model, with no supplied labels, exhibits a performance close to that of the existing models which required task labels.",
            "strength_and_weaknesses": "This work contributes to the field of continual learning by introducing a context-aware mechanism of gating the network neurons while learning.\n\nAmong the strengths of the work are:\n\n-A nice and simple model that learns to identify the tasks to for the gates (I am surprised that it hasn\u2019t been done yet!)\n\n-High performance, matching that of the previous approaches, but with no need to supply task labels\n\n-I enjoyed the analysis which shows what similar tasks (i.e. MNIST data rotated to small angles) lead to similar gates.\n\nWith that said, I believe that additional analysis could showcase the abilities of this model and thus strengthen this contribution. It looks like the content-aware gating which generates similar gates for similar tasks should be more parsimonious than the existing approaches and thus should be able to pack more tasks in the same neural network. Can the Authors please comment on that proposition and/or run such an experiment? It shouldn\u2019t take long.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The text is well-written and the experiments are well-documented making the study reproducible.",
            "summary_of_the_review": "The paper adds a simple yet powerful tweak to the existing continual learning approaches allowing to omit task labels at the evaluation time. I believe that the model can be further explored to assess its other properties such as a potential increase in the number of tasks learnable with a single network. Such properties \u2013 if shown \u2013 may strengthen the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5379/Reviewer_wV5E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5379/Reviewer_wV5E"
        ]
    },
    {
        "id": "01K-bNku104",
        "original": null,
        "number": 4,
        "cdate": 1666661161566,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661161566,
        "tmdate": 1666661161566,
        "tddate": null,
        "forum": "dBk3hsg-n6",
        "replyto": "dBk3hsg-n6",
        "invitation": "ICLR.cc/2023/Conference/Paper5379/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper develops a method of adding gates to hidden units in order to protect their weights from modification during training on new tasks. The goal is to prevent catastrophic forgetting. The method, LXDG, extends a prior method XDG, by (among other things) removing the need for a task label.",
            "strength_and_weaknesses": "\n(strengths)\n\nThe background context is clear.  The paper is well-scoped, and the paper delivers incremental but significant improvement on prior work. \n \nThe findings are convincing.\n\nThe work has significant value. \n\nMiscellaneous comments:\n\nPunctuation errors: 2 commas near top of page 2, period in first line of page 5.\n\nPlease number equations.\n\nRe lambda_gate: better might be to give different lambdas in the equations, then in the first paragraph of 2.2 note that they were all set equal. Alternately (and more standard I think), withhold the lambdas from the individual loss equations, and include them in the collected Loss eqn at top of pg 5.\n\nI did not get the last paragraph of 3.1. Is this a meaningful observation? ie, is the network in the paper similar enough to bio networks to draw any conclusions?\n\nIn section 3.2: Did the value of beta_c (in L_change) change since the tasks were so closely related?\n\nFig 5 and 7: please specify the task type (permuted or rotated) in the caption title.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well-written and easy to read. \n\nThe work is well-scoped and novel in an appropriate, measured way.\n\nI am not versed in the literature of catastrophic forgetting, so I cannot confirm originality.\n\nI did not see mention of an open codebase.",
            "summary_of_the_review": "I loved this paper. Well-written, interesting, well-scoped, with convincing and valuable findings.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5379/Reviewer_jDUA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5379/Reviewer_jDUA"
        ]
    }
]