[
    {
        "id": "XFUZ7J7kqA",
        "original": null,
        "number": 1,
        "cdate": 1666643315457,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643315457,
        "tmdate": 1666643315457,
        "tddate": null,
        "forum": "TJ2nxciYCk-",
        "replyto": "TJ2nxciYCk-",
        "invitation": "ICLR.cc/2023/Conference/Paper2045/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work demonstrates the natural emergence of sparsity in commonly used Transformer models. \n\nThis paper proposed  Top-k thresholding to enforce sparsity, which brings robustness of training with erroneous annotations, less sensitivity to input noise/perturbation, and better confidence calibration of the predictions.",
            "strength_and_weaknesses": "Strength:\n(1)This paper observed the sparsity in Transformers and this sparsity is emergent without any explicit design.\n\n(2) This paper found that sparsity in Transformers is a prevalent phenomenon, and can improve efficiency, robustness and calibration.\n\n(3) This paper further presented Top-k Transformer, a simple modification to Transformer architecture that allows control on the sparsity level for all model inputs throughout training.\n\nWeakness:\n(1) This paper only shows sparsity on the classification task. Is the sparsity existing for downstream tasks? such as detection and segmentation.\n\n(2) Tab 1 shows that Top-128 ViT is on par with ViT for natural accuracy while is significantly better for model robustness and calibration. How about the other Top-k results? (please move some experiments from the appendix). \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper's motivation is based on statistical observation. The analysis is insightful and well supported by some experiments.\nAs T5 and ViT are public available models, this paper is reproducible.\n",
            "summary_of_the_review": "This paper observed the sparsity in Transformers and proposed Top-k thresholding to enforce sparsity. Based on the proposed method, the efficiency, robustness and calibration are improved. However, this paper only considered the classification task, and I'd like to see the sparsity existing in the downstream tasks. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_HpEq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_HpEq"
        ]
    },
    {
        "id": "ouT42gU34G",
        "original": null,
        "number": 2,
        "cdate": 1666754327895,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666754327895,
        "tmdate": 1666754327895,
        "tddate": null,
        "forum": "TJ2nxciYCk-",
        "replyto": "TJ2nxciYCk-",
        "invitation": "ICLR.cc/2023/Conference/Paper2045/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the role of sparsity in deep neural networks that are based on transformers. In particular, the authors show that for a trained transformer only a small percentage of the hidden neurons in Multi-Layer Perceptrons (MLP) are non-zero. Motivated by that they propose a variant of the transformer networks, for a MLP layer only the top k hidden neurons are kept. They conduct numerical experiments for two instances of transformer networks for images and one for natural language processing. When comparing their version with the base models, they show that this model achieves comparable metrics, when the top 128 neurons (k=128) are kept. They further investigate the robustness of this model by introducing noisy labels and adding noise to the inputs. Overall, this new model appears to be outperform the baseline in this noisy conditions.   ",
            "strength_and_weaknesses": "Strengths:\n\n(a) The work presents an interesting application that can be easily applied to all transformer based models.  \n(b) The paper is in general well-written and easy to follow. \n(c) The authors conduct additional experiments on the robustness of the models. \n\nWeaknesses:\n\n(a) From Fg. 6 I see that in quite a few cases using this \"sparse\" model does not lead to improvement in inference time. For instance for T5 Large all the numbers are negative. This undermines the main goal of this model, which was to improve running time. \n(b) On page 6, the authors discuss the possibility of using approximate algorithms for finding the top K values with sublinear running times in hidden dimension. However, it is not clear to me of the authors actually used this approximate method or not. \n(c) Minor comments/questions:\n    (c1) Are the results in Fig. 1 for train set or test set? \n    (c2) Is dim in the legend of Fig. 4 the same as dff?\n    (c3) What is dff in Fig. 5? \n    (c4) On page 9, change \"many work...\" to \"many works...\"\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors propose interesting insights into the sparsity patterns of  transformers. However, the contributions of the paper seems to be limited to only proposing to take the top k values in hidden layers of MLPs. In particular, from Fig. 6 we see that this does not necessarily lead to running time performance. Moreover, the in Fig. 5 we see that the baseline networks slightly outperform the proposed method in most cases. ",
            "summary_of_the_review": "Overall, the authors show intersting insights into the transformers sparsity patterns. However, I believe that the the current numerical results are not convincing to immediately adopt this variant. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_x5F1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_x5F1"
        ]
    },
    {
        "id": "nmEJY_5N7_j",
        "original": null,
        "number": 3,
        "cdate": 1666778646772,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666778646772,
        "tmdate": 1669738398530,
        "tddate": null,
        "forum": "TJ2nxciYCk-",
        "replyto": "TJ2nxciYCk-",
        "invitation": "ICLR.cc/2023/Conference/Paper2045/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work shows that sparsity is prevalent in transformer models, including T5 model for NLP and ViT model for vision with extensive experiments. Such a discovery might be suggesting that the law of parsimony is playing a role in Transformers even though they are not explicitly designed so. Furthermore, a top-k transformer is proposed by using top-k thresholding, which achieves good generalization.",
            "strength_and_weaknesses": "Strength:\n\n(1)  The authors show the natural emergence of sparse activation in commonly used Transformer models.\n\n(2)  Inspired by the experiments on sparsity, a top-k transformer is proposed to obtain good performance and generalization.\n\n(3)  Extensive experiments are conducted to support their arguments.\n\n\nWeaknesses:\n\nThis work shows the emergence of sparse activation in Transformer models. I have some questions as follows. \n\n(1)  In Sec. 1.3, the authors state that they use ReLU as the activation function instead of GeLU.  However, what is the performance gap between ReLU and GeLU? If GeLU is employed, Does sparsity still exist? Furthermore, the authors also propose a top-k transformer. The authors should list the specific performance with the top-k transformer and the original GeLU for comparisons. \n\n(2)  The authors choose the top k value for the transformer. However, it might affect the data parallelism thereby reducing the inference speed. The authors should provide a reasonable comparison, e.g., throughput. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This work has high quality and high clarity. This work has originality in analyzing the sparsity of transformer models based on my knowledge. The results should be reproducible because the authors provide amounts of experiments.",
            "summary_of_the_review": "To sum up, the work shows the emergence of sparse activation in transformer models with amounts of experiments and proposes a top-k transformer based on this discovery, which is promising. Therefore, I tend to vote accept. Some experiments for reasonable comparisons should also be conducted.\n\n\n-------- After rebuttal --------\n\nI have read the authors' responses and other reviewers' comments. I keep my 'accept' rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_KPJJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_KPJJ"
        ]
    },
    {
        "id": "wxIupT5NMe",
        "original": null,
        "number": 4,
        "cdate": 1666814205149,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666814205149,
        "tmdate": 1666814205149,
        "tddate": null,
        "forum": "TJ2nxciYCk-",
        "replyto": "TJ2nxciYCk-",
        "invitation": "ICLR.cc/2023/Conference/Paper2045/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study sparsity in the MLP portion of transformers as a means to speed up inference time computation. They also show that sparsity enables robustness to input noise. \n",
            "strength_and_weaknesses": "+ Reducing the number of operations in a dense matrix multiplication makes sense as a route to speedups.\n+ The writing is easy to understand.\n+ Exhaustive empirical evaluation\n\n- I'm not convinced by reading the paper that sparsity it the solution to more computationally efficient models. Right above section 3.3, the authors claim a 10% wall-time reduction. Is this significant? I wonder if it's worth retraining a model to be sparse for only a 10% wall-time reduction.\n- I'm curious why the authors mainly studied the activations of the MLP portion of transformers and not so much the activation maps prior to the MLP.\n- Section 3.3 contains interesting information but feels rushed and too short. I was hoping the authors could expand on how much sparsity improves robustness to noise and confidence calibration.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, but I'm not sold on the novelty. I don't know what the takeaway of this paper should be. The authors have run many experiments but it's hard to decipher a core message.\n",
            "summary_of_the_review": "I am on the fence about accepting this paper. It contains a lot of interesting experiments but is lacking a strong message. I hope the authors can address some of the weaknesses: I am open to increasing my score. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_cu8i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_cu8i"
        ]
    },
    {
        "id": "FKLzZAIZZN",
        "original": null,
        "number": 5,
        "cdate": 1667225489180,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667225489180,
        "tmdate": 1667225489180,
        "tddate": null,
        "forum": "TJ2nxciYCk-",
        "replyto": "TJ2nxciYCk-",
        "invitation": "ICLR.cc/2023/Conference/Paper2045/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper experimentally discusses the emergence of sparse activations in the feed-forward layer of Transformer neural networks. The authors highlight that sparse activations emerge for a variety of tasks and architectures. They then discuss how sparsity can be taken advantage of to reduce FLOP count and the implications of sparsity for robustness under different sources of noise.",
            "strength_and_weaknesses": "Strengths:\n- Many experiments emphasize that this phenomenon happens for a variety of architectures and tasks. \n- The appendix offers some very good insights. The relevance of the optimizer and the residual connections are the first candidates that fall in mind when first reading the paper and could be emphasized/briefly mentioned in the main text as well.\n- It is nice to see that depending on the task and the rank of the representation required, different sparsity patterns with layer index can be observed. \n\nWeaknesses:\n- As mentioned, time-wall performance is not guaranteed apart from unbatched greedy decoding for larger models.\n- Would be beneficial to discuss more (dis)similarities with sparsity in the attention matrix. I suspect similar results to Table 1 would come out if the Top-k operator was applied to the attention matrix.\n\nQuestions/Remarks:\n- Would be interesting to see what happens when residual connections weights are learned, as e.g. in ReZero [1].\n- The Role of the optimizer is discussed extensively in the Appendix. [2] might offer an alternative to train Transformers via SGD, which would be a valuable comparison.\n- It is not clearly described what the legend in Figure 5 means. I guess (Train) indicates that the model was trained from scratch with a fixed k? If Top-k Transformer models are trained from scratch, it would be interesting to know if more steps are required to reach convergence.\n- Studying the relationship with ResNets is not fair, as ResNets have batch normalization layers before the activations. Comparison with MLP should also include residual connections to be fairer.\n\n[1]: Bachlechner, Thomas, et al. \"Rezero is all you need: Fast convergence at large depth.\" Uncertainty in Artificial Intelligence. PMLR, 2021.\n[2]: Noci, Lorenzo, et al. \"Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse.\" arXiv preprint arXiv:2206.03126 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. Appendix includes all details to reproduce the results.",
            "summary_of_the_review": "All in all the paper offers some valuable insights on the activation sparsity in (Transformer) networks. Plethora of experiments demonstrate that this phenomenon is present across architectures and tasks. Claims regarded FLOP efficiency are briefly mentioned, but not adequately backed. A new Top-k layer is proposed, with limited insights given regarding drops in expected accuracy and expected gains in wall time inference. Robustness claims are also linked to previous work. The authors do make attempts to explain possible causes (mainly in the appendix). Contributions are incremental and not totally clear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_urTS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2045/Reviewer_urTS"
        ]
    }
]