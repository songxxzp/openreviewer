[
    {
        "id": "0VyfhWKZab2",
        "original": null,
        "number": 1,
        "cdate": 1666472199479,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666472199479,
        "tmdate": 1666472199479,
        "tddate": null,
        "forum": "ueEMZjY9WiM",
        "replyto": "ueEMZjY9WiM",
        "invitation": "ICLR.cc/2023/Conference/Paper6319/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes the SFW algorithm to solve a constrained optimization framework. The proposed method can result in well-performing models that are robust towards convolutional filter pruning as well as low-rank matrix decomposition. Experiment results also show that the proposed method has better \u201caccuracy vs sparsity\u201d performance than existing approaches. ",
            "strength_and_weaknesses": "Strength:\nOverall, the paper is well-organized and easy to follow. This paper studies two optimization problems: convolutional filter pruning and low-rank matrix decomposition. For the first problem, the author uses the group-k-support norm ball to constrain the optimization problem to compress the model instead of the existing k-sparse approach. To solve this problem, the author proposes to use the SFW algorithm. For the second problem, the spectral-k-support norm is used. Experiments results on those two problems show the advantage of the proposed method. Specifically, the accuracy of the proposed method is more robust than the existing methods when compressing the model.\n\nThis work also empirically shows that the robustness of SFW can largely be attributed to the usage of the gradient rescaling of the learning rate. To justify the usage of gradient rescaling theoretically, the convergence of SFW with batch gradient dependent step size in the non-convex setting is established.\n\nWeaknesses:\n1.  In the abstract, the author said that compression-aware training could obtain state-of-the-art dense models which are robust to a wide range of compression ratios using a single dense training run while also avoiding retraining. But I believe the proposed framework still needs to run several times independently to obtain models with different compression ratios. That is, to obtain a model with different compression ratios, you have to run SFW another time from the beginning. Therefore, I am not sure why this method can avoid retraining.\n  \n2. I notice that In Figure 2, the performance of ABFP is comparable to or even better than SFW. Could the author show the advantage of SFW compared to ABFP?\n\n3. Although Theorem 3.1 shows the convergence of SFW with batch gradient dependent step size in the non-convex setting.  However, the assumption of the objective function seems to be too strong (L-smooth and L-Lipschitz). I believe neither of the two studied problems would satisfy these assumptions. Please correct me if I am wrong.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality and clarity are good. Originality is minor.",
            "summary_of_the_review": "The experiment result is convincing. I also like the idea of combing the SFW and the group-k-support norm constraint together, but I didn\u2019t find much novelty in the theory and algorithm respects.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6319/Reviewer_v7Db"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6319/Reviewer_v7Db"
        ]
    },
    {
        "id": "kJ178g9l2A",
        "original": null,
        "number": 2,
        "cdate": 1666528499203,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666528499203,
        "tmdate": 1666528499203,
        "tddate": null,
        "forum": "ueEMZjY9WiM",
        "replyto": "ueEMZjY9WiM",
        "invitation": "ICLR.cc/2023/Conference/Paper6319/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors adopt SWF (stochastic Frank-Wolfe) algorithm to perform compression-aware training. Prelimary experiments demonstrate the efficacy of the proposed approach. ",
            "strength_and_weaknesses": "The authors adopt a classic optimization algorithm (SFW) to perform structure compression-aware training. Before this work is accepted, several concerns should be addressed:\n(i)\tThe first concern is about the novelty. This work heavily depends on the unstructed compression-aware training by extending the unstructured pruning to structured pruning setting. The novelty is discounted. \n(ii)\tThe authors claims that the adopted SFW methods achieve significant speedup compated with nuclear-norm regularization based approach. In general, for nuclear norm regularized optimization, several SVD-free methods have been proposed [1]. We require the authors compare their proposed approach with more advanced optimization methods for solving nuclear norm regularized problem to show its effecgtiveness for a fair comparision. Furthermore, the real speedup ration should be reported to show the efficacy of the proposed algorithm. \n(iii)\tThe current experiments restrict to CIFAR-10, CIFAR-100, Tiny-ImageNet. In general, ImageNet-1K is a standard benchmark to verify the effiectiveness of the compression methods. We recommend the author provide more experents on ImageNet-1k. \n(iv)\tAbout Theorem 5.1. Why does the term E[g*||Grad{L}||] indicate the convergence of the proposed approach? THe authors should gives more discussions on this convergence criteria. \n\n\n[1] SVD-free Convex-Concave Approaches for Nuclear Norm Regularization, IJCAI, 2017.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well writen",
            "summary_of_the_review": "See the comments above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6319/Reviewer_XfiW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6319/Reviewer_XfiW"
        ]
    },
    {
        "id": "kkpY9s6ENoY",
        "original": null,
        "number": 3,
        "cdate": 1666656422024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656422024,
        "tmdate": 1670818900757,
        "tddate": null,
        "forum": "ueEMZjY9WiM",
        "replyto": "ueEMZjY9WiM",
        "invitation": "ICLR.cc/2023/Conference/Paper6319/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a constrained optimization framework based on a versatile family of norm constraints and the stochastic FrankWolfe (SFW) algorithm. The proposed method apply on benchmark image-classification architectures and datasets, and it yields competitive results, often outperforming existing compression-aware approaches. ",
            "strength_and_weaknesses": "Strength:\n\nThe proposed method applied on benchmark image-classification architectures and\ndatasets outperform the existing compression-aware approaches.  In the case of low-rank\nmatrix decomposition, the proposed method can require much less computational resources\nthan nuclear-norm regularization based approaches by requiring only a fraction of\nthe singular values in each iteration.\n\nWeakness:\n\nThe novelty of this paper is very limited. This paper basically combines \nthe existing k-support norm regularztion (https://arxiv.org/pdf/1204.5043.pdf, \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952587) and the existing stochastic \nFrank-Wolfe methods (https://arxiv.org/pdf/1607.08254.pdf). ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing can significantly be improved. \nThe novelty of this paper is very limited. This paper basically combines \nthe existing k-support norm regularztion (https://arxiv.org/pdf/1204.5043.pdf, \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952587) and the existing stochastic \nFrank-Wolfe methods (https://arxiv.org/pdf/1607.08254.pdf). ",
            "summary_of_the_review": "The proposed method applied on benchmark image-classification architectures and\ndatasets outperform the existing compression-aware approaches.  In the case of low-rank\nmatrix decomposition, the proposed method can require much less computational resources\nthan nuclear-norm regularization based approaches by requiring only a fraction of\nthe singular values in each iteration. \n\nSome Questions:\n\n1. Why the proposed method can require much less computational resources\nthan nuclear-norm regularization based approaches by requiring only a fraction of\nthe singular values in each iteration ? It would be great if the authors would detail it.\n\n2. In the experimental results, we hope to see the efficiency of the proposed methods. \nPlease give the results on test accuracy vs time.\n\n3. There will be a strict upper limit of 9 pages for the main text of the submission, \nwith unlimited additional pages for citations (https://iclr.cc/Conferences/2023/CallForPapers). \nThis paper maybe not fit for this request.\n\n\n\n\n--------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------------\nThe authors still did not solve my main concern- the limited novelty of this paper. \nSo I support to reject this paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6319/Reviewer_gmRx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6319/Reviewer_gmRx"
        ]
    },
    {
        "id": "mByKxvZi00",
        "original": null,
        "number": 4,
        "cdate": 1666854003118,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666854003118,
        "tmdate": 1666854003118,
        "tddate": null,
        "forum": "ueEMZjY9WiM",
        "replyto": "ueEMZjY9WiM",
        "invitation": "ICLR.cc/2023/Conference/Paper6319/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel framework for compression-aware training of neural networks. The proposed method uses norm constraints, for two types of pruning (1) convolutional filter pruning (2) low-rank matrix decomposition, expressed via updates of the Stochastic Frank-Wolfe (SFW) algorithm efficiently. ",
            "strength_and_weaknesses": "Pros: \n- The proposed framework is interesting and beneficial to the community. The authors provide sufficient intuition and motivation behind the use of the sparsity-inducing norm constraints and how they can be effectively realized via SFW. The presentation is clear and the math is easy to follow. \n- All the claims are supported well by empirical studies on benchmark datasets. The baselines seem sensible; although I must admit that I'm not too familiar with the related works in the compression-aware setting\n\nComments on the robustness study:\nOne of the interesting sections in the paper is the study on the robustness of the pruned model. The experimental study and the authors' discussion on the benefits of using the rescaled learning rate are insightful, especially at higher compression rates. With gradient scaling, the authors are able to show the 1/\\sqrt(T) convergence result (FW gap) for SFW. However, they still collectively don't provide enough convincing arguments for the robustness claims, in my opinion.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and the ideas are easy to follow. The authors have agreed to release the code if accepted. ",
            "summary_of_the_review": "Overall, I think this is a good paper. The ideas presented are novel and are well-supported by ample experimental evidence and some theoretical results. The proposed method and discussions are relevant and useful to the community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6319/Reviewer_RYk5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6319/Reviewer_RYk5"
        ]
    }
]