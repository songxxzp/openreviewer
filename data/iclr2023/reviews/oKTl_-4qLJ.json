[
    {
        "id": "zct_0f1WCZe",
        "original": null,
        "number": 1,
        "cdate": 1666617026048,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617026048,
        "tmdate": 1666617026048,
        "tddate": null,
        "forum": "oKTl_-4qLJ",
        "replyto": "oKTl_-4qLJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5058/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a study of a reinforcement learning setting consisting of a task-agnostic data collection phase and a task-aware offline optimization phases, on a set of continuous control tasks. Comparing the performance obtained when using different exploration criteria (combined with a new MPC-based method) in the data collection phase, as well as different dataset sizes, this work tries to draw conclusions about the features we should expect to find in offline RL datasets for a downstream algorithm to perform well.",
            "strength_and_weaknesses": "Strengths:\n- The problem addressed in the paper is of paramount importance: increasing our understanding of the kind of datasets that are needed to reach good performance with offline reinforcement learning has the potential to both shape the design of future offline RL algorithms and to have practical implications;\n- The proposed MPC-based exploration method, which could be of independent interest, stands out as a clean way (probably cleaner than the reactive counterpart) of comparing the different exploration criteria.\n- Despite being probably already known to some researchers in the field, having clearly written down some of the conclusions about the influence of dataset size and reward distribution could be useful to practitioners.\n\nWeaknesses:\n- The paper only uses the CRR offline RL algorithm for evaluating the downstream effects of the construction of different types of datasets. Despite this is a start, the actual outlook of offline RL algorithms used in the community is very vast, and there is really no baseline as established as for in the online RL case. This severely limits the scope of the work, since the reader cannot know a priori whether the conclusions of the study only hold for that specific offline RL algorithm or are indeed more general than that;\n- The paper determines dataset size is possibly the most important variable to predict the performance for a downstream offline RL algorithm. However, this could be a bit misleading: one could imagine having the same trajectory over and over again in the dataset and clearly increasing the dataset size without any increase in the downstream performance. Despite this is a trivial counterexample, I think it is really epistemologically important to pin down the effect of diversity, more than size, on the resulting policy, because that one is more likely to be the underlying cause of the observed performance increase.\n- The comparison with previous and parallel work is not very insightful from the perspective of the reader. For instance, it would be nice to understand more the relationship with (Yarats et al., 2022), instead of just briefly mentioning its existence. Moreover, there is some work similar in flavour about pretraining (https://arxiv.org/abs/2106.04799): it seems important to have a discussion on how the study presented in this work is different compared to that.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, albeit some of the conclusions could be presented more directly.\n\nThe quality of the work could be improved (are the results only relevant for a single offline RL algorithm? Is dataset size really the importance variable?)\n\nThe work is relatively novel, despite it misses important discussion with respect to previous and parallel work in unsupervised exploration.\n\nThere seem to be no apparent reproducibility issue (3 seeds are not great, but I understand the possible computational constraints the authors might have).",
            "summary_of_the_review": "Overall, I appreciate the direction presented in this work, as well as the general methodology that has been attempted. However, I believe there are some serious drawbacks that might overly reduce the scope of the conclusions drawn in the paper, and thus for now I tend to recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5058/Reviewer_58zj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5058/Reviewer_58zj"
        ]
    },
    {
        "id": "y316O2cJpnO",
        "original": null,
        "number": 2,
        "cdate": 1666619174114,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619174114,
        "tmdate": 1666619174114,
        "tddate": null,
        "forum": "oKTl_-4qLJ",
        "replyto": "oKTl_-4qLJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5058/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies offline RL from a less-studied perspective - collecting informative experiences. The authors investigate the task-agnostic setting based on curiosity-based intrinsic motivation methods. The authors propose the Explore2Offline framework, and conduct an extensive empirical study to investing the effect of data collection strategies. ",
            "strength_and_weaknesses": "### Strengths\nThe paper is clearly written and easy to follow. The authors also conduct extensive experiments to investigate data collection in offline RL based on a number of intrinsic motivation based exploration algorithms to reveal interesting findings. \n\n### Weakness\nMy main concern for the paper is its novelty. Although I acknowledge that the paper studies offline RL from a less-studied perspective (data collection instead of policy learning that aims to address extrapolation error) which is very interesting, it seems that it is very related to [1] without enough discussion about the differences. Specifically, [1] also studies data collection in offline RL, and it is worth discussing the differences between them.\n\n[1] Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don\u2019t change the algorithm, change the data: Exploratory data for offline reinforcement learning, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see my comments in the above section.",
            "summary_of_the_review": "The paper studies offline RL from an interesting and less-studied perspective (data collection instead of policy learning), and propose a new exploration agent, Intrinsic Model Predictive Control that have strong performance. The authors also conduct extensive and in-depth analysis and investigation in standard offline RL tasks with different qualities. My main concern is its novelty, which is quite related to [1], but without enough discussion about their differences.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5058/Reviewer_BjG9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5058/Reviewer_BjG9"
        ]
    },
    {
        "id": "n-bVbhyQZy",
        "original": null,
        "number": 3,
        "cdate": 1666708470599,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666708470599,
        "tmdate": 1666708725193,
        "tddate": null,
        "forum": "oKTl_-4qLJ",
        "replyto": "oKTl_-4qLJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5058/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores how to collect informative data for offline RL methods. Many curiosity-based methods are considered to explore the environment. Intrinsic Model Predictive Control (IMPC) approach is proposed to improve the performance.",
            "strength_and_weaknesses": "The idea and setting are novel and interesting. However, the method part is a little difficult to follow. the description of the IMPC is not clear.\n1. What are the input and output of IMPC? How do you train it? What is the main contribution?\n2. The experiments only choose MPO to verify the quality of the samples, which is not convincing enough. Since you propose a new framework and use the offline RL methods as a mechanism for evaluating exploration performance, I think more than 2 most popular methods should be used,\n3. The paper does not give a deeper analysis of the data requirement of the offline RL. In other words, in order to get better performance, which kind of data is required by the offline RL methods? How to construct such a dataset?\n4. There are some papers focusing on offline training and online fine-tuning to improve performance via fewer data. They also use exploration methods to guide online data collocation. Their methods can also be compared after slight modification.",
            "clarity,_quality,_novelty_and_reproducibility": "Code is not provided and detailed implementation is not very clear.",
            "summary_of_the_review": "Please see my questions above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5058/Reviewer_QZYQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5058/Reviewer_QZYQ"
        ]
    }
]