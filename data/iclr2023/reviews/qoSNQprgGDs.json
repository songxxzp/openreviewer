[
    {
        "id": "dbOL6sIS2yS",
        "original": null,
        "number": 1,
        "cdate": 1666484003519,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666484003519,
        "tmdate": 1666484003519,
        "tddate": null,
        "forum": "qoSNQprgGDs",
        "replyto": "qoSNQprgGDs",
        "invitation": "ICLR.cc/2023/Conference/Paper1484/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work attempts to establish general geometric statistics of deep learning representations (Manifold graph metrics---MGMs) that can be useful for quantifying various properties of deep learning models. According to the paper, these can be viewed as measurements of desirable qualitative features of the resulting models. The paper makes statements about the relationships between the self-supervised models (SSL) based on the statistics developed. Specifically, the authors claim that SSL models can be clustered usefully by these geometric properties, and that they are a strong indicator of transfer learning capabilities. They do so by analyzing clustering of the SSL models on both the MGM space as well as the accuracy space (across a range of tasks). \n\nTo create a set of clusterable features, the authors consider the space of MGMs applied to manifolds resulting from neighborhoods derived from 5 generating operations: semantic category, augmentations, crop, colorjitter, and rotations. For each of these they compute the mean and spread of: equivariance-invariance (diameter of the polytope resulting from NNK performed on the set generated by the mechanism) and affinity (the angle between the natural and augmented manifolds resulting from the NNK procedure) as well as the cardinality of the NNK neighborhood (which is a surrogate for dimension of the underlying manifold). \n\nFrom these sets of features cluster and correlation analysis is applied. The authors compare the clusters resulting from features comprised by accuracy across a variety of tasks to that of the MGM features. They claim that this shows that the there is an intrinsic connection between the geometric properties of the SSL model (MGMs in this context) and the transfer learning performances. The authors close with a claim that the MGMs could be used for improved SSL.",
            "strength_and_weaknesses": "The main strength of the work is that the objective of the paper seems novel and is clearly intelligible. \n\nThere are several weaknesses in my opinion. First of all, these choices of metrics are arbitrary: while the authors claim the diameter and affinity metrics have semantic meanings in terms of equivariant and intrinsic dimension, this is not corroborated by evidence. The minimal evidence provided is in the terms of post-hoc reasoning about the nature of the models on some datasets, in light of their MGM values. This does not hold up under scrutiny, since e.g. the effectiveness of the diameter metric at capturing equivariance will vary from model to model and from training strategy to training strategy. This is because these networks have high dimensional encodings and so the encodings can be far apart geometrically, but look similar to the self-supervised decision heads (e.g. the momentum encoder weights or the prototype weights, etc.).  There would need to be more analysis specifically on the MGMs looking at the uniformity of these metrics across a range of networks and training regimes to a establish the significance of them in the proposed analysis. Are the MGMs robust to hyperparameters or small changes in the SSL methods? How about other deep learning representations? There is not enough analysis dedicated to the value of metrics independent of the clustering and correlation analyses and that leads to the another weaknness. \nAll of the statements in this work rely on some kind of dimensionality reduction or summarization of the aggregated statistics of the MGMs across a large input set (Imagenet validation). For me, it is hard to take this kind of analysis seriously given that you are reducing very large and complex collections of weights and data (generally hundred of megabytes of networks, plus gigabytes of images) into dendrograms and x-y plots. Given the lack of analysis on the MGMs themselves, I do not feel I have any intuition on the significance of this kind of analysis or can infer whether it is \"right\". Slink is a nonparametric clustering approach, but the linkages are across different algorithms represented by MGMs. Again, this leaves the reader wondering what the significance of the linkages is---beyond a degree (distance in the MGM space) or a linkage (the closest neighbor under the slink algorithm used here). ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow, though some of the technical details are hidden and organization could be improved. As a reviewer, I can understand the methods for evaluation and the intuition behind their metrics. To improve the organization a table of the MGM descriptions, equations, and method of calculation would better organize the paper and make for easier digestion. This paper has a lot of evaluation settings as well, and the authors should consider whether organizing these into a table for ease of understanding would be beneficial as well. These tables could be placed in the appendix. \n\nThe value of the metrics is not entirely obvious, beyond the analysis provided in the paper. The conclusion that their metrics motivate the design of new SSL approaches begs the question: why did they not test this conclusion? The paper offers no concrete evidence that these metrics can be used directly for self-supervision. Furthermore, it is unclear to me why the authors limited the application to self-supervised learning. Why would these geometric features not be useful for other models? E.g. as regularization for semi-supervised models or even supervised models.\n\nThe metrics introduced seem to be novel. However, as I mentioned above, the degree to which they express the qualitative features indicated by their names in the networks is not clear. The evaluation work is certainly novel, however I again question the value of this analysis. Clustering networks based on these metrics involves a lot of simplification. For instance, why should the properties studied here by isotropic? They could certainly vary throughout the input space. This seems to limit the value of the uniform analysis applied for the entire network as if the metrics were not spatially varying in the input space(since the averages and spread are computed over a large collection of samples). The implication is that these are uniform behaviors across input space, but this is clearly not the case (imagine images from different domains with varying complexity). \n\nThe authors did not include a reproducibility statement in their work, however, assuming they used open source models/training code for the SSL methods here reproducing their work should be possible by a motivated scientist.",
            "summary_of_the_review": "This work proposes some novel descriptor of deep learning representations viewed through augmented inference. The authors then apply these descriptors to understanding the geometry of self-supervised deep learning representations as applied to computer vision. This aspect of the work is novel. The authors do not provide in depth analysis of their proposed descriptors, convincing the reader of their value and stated significance. By directly applying correlation and clustering analysis to these descriptors without providing compelling evidence or intuition of the meaning, the authors leave the reader uncertain of the actual result here. More in depth understanding of their proposed descriptors and whether they can actually provide the evidence of the claims made in the paper should be performed. Finally, the authors allude to the value of these descriptors for self-supervised learning. But they do not conduct any experiments towards this end. It is worth noting that conducting such experiments would go a ways towards resolving the complaint of the lack of evidence of the value of the descriptors noted earlier.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1484/Reviewer_HJu3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1484/Reviewer_HJu3"
        ]
    },
    {
        "id": "6Aig3DiKBWt",
        "original": null,
        "number": 2,
        "cdate": 1666554676262,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666554676262,
        "tmdate": 1666554676262,
        "tddate": null,
        "forum": "qoSNQprgGDs",
        "replyto": "qoSNQprgGDs",
        "invitation": "ICLR.cc/2023/Conference/Paper1484/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work empirically showed that the geometry of SSL models can be efficiently captured by leveraging graph-based metrics. The work demonstrated that the proposed geometrical metrics are able to capture the transfer learning capability of many different SSL models. The analysis provides insights into the landscape of SSL algorithms, highlighting their geometrical similarities and differences. ",
            "strength_and_weaknesses": "#strength: It provides new metrics for evaluating the representations learned vis SSL with geometrical insights on feature similarities. \nThe work empirically demonstrates the effectiveness of these metrics on various SSL models and encoder network architectures.\n\n#weakness: The current study focuses on existing network architectures and SSL paradigms. It is unclear how the metrics can lead to better SSL paradigms or representation designing methods, with better representation qualities.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is quite clear. The empirical results are interesting.\n",
            "summary_of_the_review": "Overall, this work is well written and well presented.\nIt provides new metrics for evaluating the representations learned vis SSL with geometrical insights on feature similarities. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1484/Reviewer_w8qW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1484/Reviewer_w8qW"
        ]
    },
    {
        "id": "foOlcBpgP0",
        "original": null,
        "number": 4,
        "cdate": 1666644167807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644167807,
        "tmdate": 1666644167807,
        "tddate": null,
        "forum": "qoSNQprgGDs",
        "replyto": "qoSNQprgGDs",
        "invitation": "ICLR.cc/2023/Conference/Paper1484/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The core questions in the paper are \n1) What is the geometry of representations learned by contrastive learning with data augmentation? \n2) What are the determinants of good generalization in downstream tasks?",
            "strength_and_weaknesses": "Strengths:  \nA set of intuitive descriptors \"manifold graph metrics\" is proposed and evidence is presented that the can inform pre-training choice for given downstream tasks.  \nExperimental evidence is quite impressive across imagery and audio signals\n\nWeakness:   \nSome unclear points in the statistical analysis of experiments (figure 6., control for multiple comparisons?)",
            "clarity,_quality,_novelty_and_reproducibility": "Well-written contribution.  Novel descriptors that combine manifold measures (local dimension, curvature etc). Detailed experimental analysis in supplement.",
            "summary_of_the_review": "An interesting meta-learning analysis with new intuitive descriptors, evidence is presented that they can inform choices in self-supervised learning",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1484/Reviewer_teRg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1484/Reviewer_teRg"
        ]
    }
]