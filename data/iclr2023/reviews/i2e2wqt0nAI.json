[
    {
        "id": "8CQY6yPcDT",
        "original": null,
        "number": 1,
        "cdate": 1666037836220,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666037836220,
        "tmdate": 1668625689075,
        "tddate": null,
        "forum": "i2e2wqt0nAI",
        "replyto": "i2e2wqt0nAI",
        "invitation": "ICLR.cc/2023/Conference/Paper3665/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper propose a new symbolic regression (SR) dataset for scientific discovery (SRSD) that is a modification of the Feynman symbolic regression database (FSRD) that fixes the following issues present in FSRD: 1) no physical meaning for a number of the datasets, 2) an oversimplified sampling process, 3) distinct equations that are marked as duplicates, and 4) inappropriate or incorrect formulas. The proposed SRSD datasets treats constants (e.g., lightspeed) as constants, samples values for variables with appropriate ranges using experimental observations, and partitions the 120 datasets into easy, medium, and hard sets based on formula operation length and variable domain range. The authors also propose using normalized edit distance to assess the similarity of the structures between the predicted equation models and their respective ground-truth models.\n\nThe authors evaluate 5 established and well-known SR methods and one transformer-based SR model on the proposed SRSD datasets and measure performance using the proposed normalized edit distance, accuracy (which captures instances where the predicted and true model differ by a constant or scalar), and solution rate. Overall, the authors find the transformer-based approach outperforms all other methods on normalized edit distance, but performs worse than all other methods in terms of accuracy and solution rate; this suggests the transformer-based approach provides more structurally similar equation models than existing methods.",
            "strength_and_weaknesses": "Strengths\n---\nThe proposed SRSD datasets address issues present in the FSRD database in order to provide datasets that better align with the goals of symbolic regression for scientific discovery.\n\nThe proposed SRSD contains 120 datasets, partitioned into three different sets of varying difficulty level.\n\nThe proposed metric of normalized edit distance allows for a more fine-grained analysis into the structural differences between the predicted and true equation models.\n\nThe paper is relatively well-written and easy to read and follow.\n\nWeaknesses\n---\nIt is not clear to me how much insight is gained using the normalized edit distance (NED) over existing metrics measuring the success of symbolic regression models. The results suggest that the transformer-based model achieves slightly better NED over existing methods, but it is difficult to judge if this improvement is meaningful from a scientific discovery point of view. Even if two equation models are mostly structurally similar (as measured by NED), they may differ by one or two elements that could perceivably result in a very different equation model not representative of the actual ground truth equation.\n\nThe experimental results are not very thorough, and only provide results for a small number of metrics for a small number of baselines. Due to the difficulty evaluating the merit of equation-based scientific discovery, perhaps a metric based on manual inspection with domain experts could give some insight into whether or not the proposed metric (and existing metrics) are well-aligned with how humans judge how close a generated equation is to representing an actual scientific equation.\n\nWhen proposing a new dataset, the authors should consider documenting their dataset thoroughly using, for instance, \"Datasheets for Datasets\" (Gebru et al. 2021) or a dataset nutrition label (Holland et al. 2018).\n\nMinor Weaknesses\n---\nConsider using \"\\citet\" when using citations as a noun.\n\nThe solution rate metric is not clearly described.\n\nWhy is accuracy and NED not shown in the comparison between the FSRD and SRSD dataset (Table S16)?",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed SRSD datasets expand the FSRD database and appear novel and potentially useful to the scientific discovery via symbolic regression community, however the clarity of the paper could be improved somewhat.",
            "summary_of_the_review": "Although the paper proposes a potentially valuable collection of SRSD datasets and a new evaluation metric measuring normalized edit distance, the dataset is not well-documented and it is unclear whether the new evaluation metric provides significantly meaningful insights in terms of scientific discovery compared to existing evaluation metrics.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3665/Reviewer_CRUL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3665/Reviewer_CRUL"
        ]
    },
    {
        "id": "lHkZ-6R5AY",
        "original": null,
        "number": 2,
        "cdate": 1666271486539,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666271486539,
        "tmdate": 1666271486539,
        "tddate": null,
        "forum": "i2e2wqt0nAI",
        "replyto": "i2e2wqt0nAI",
        "invitation": "ICLR.cc/2023/Conference/Paper3665/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new benchmark set for symbolic regression. Starting from the Feynman symbolic regression dataset (FSRD, Udrescu et al), they propose a new dataset of 120 problems, together with rules for sampling variables and parameters. They introduce a new metric for comparing symbolic functions, based on the edit distance between the trees representing the simplified skeletons of the functions. Finally, they provide baseline comparisons between different popular models (and one introduced by the authors).",
            "strength_and_weaknesses": "**Strengths**\n\nThe discussion of the limitations of current benchmarks is interesting, and the improvements proposed by the authors (e.g. keeping constants constant, introducing physics-informed ranges for the parameters and variables) makes a lot of sense. Baseline evaluations are provided.\n\n**Weaknesses**\n\nThe dataset remains very small, and most if it is a new curation/annotation of an existing benchmark (FRSD). This limits the novelty and impact of this paper. To me, the main problem of symbolic regression benchmarks is the small size of the datasets, and their lack of diversity. The paper does not address these questions.\n\nThe edit distance is an interesting idea, but needs a lot of improvements before it can be used as a reliable criterion. Two limitations spring to mind.\n\n* Some weighting of the operators is probably needed. Take f(x) = cos(ax+b), g(x)=sin(ax+b), h(x)=cos(ax), k(x)=exp(ax+b). The edit distances are edit(f, g)=1 edit(f, k)=1 and edit(f, h)=2, yet g is equivalent to f, h is a phase shift away from f (i.e. very close), but k has very different mathematical properties from the three others.\n* The magnitude of the constants should be taken into account. For small a, sin(ax) is essentially the same as ax, for large a, the functions are very different. Also, suppose you compare functions of the form f(x)+a.g(x), and u(x)+b.v(x). If a<<1 and b<<1, the edit distance between f and u is probably a much better metric than the edit distance between f+ag and u+bv. This problem will appear every time a function is represented as a sum of terms of decreasing magnitude (a very common situation in science).\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and its quality is good.\n\nThe novelty is very limited (see above). ",
            "summary_of_the_review": "The dataset proposed is interesting and welcome addition to current benchmarks and test sets. However, it is very small, and mostly amounts to a new curation of an existing dataset. The edit distance is an interesting idea, but would need significant work before it can be used as a measure of accuracy of symbolic regression. \n\nOverall, my feeling is that the contribution is very marginal, and not novel enough to feature in a venue like ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3665/Reviewer_2CZT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3665/Reviewer_2CZT"
        ]
    },
    {
        "id": "BA8Fp-rQ6Z",
        "original": null,
        "number": 3,
        "cdate": 1666984492665,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666984492665,
        "tmdate": 1666984492665,
        "tddate": null,
        "forum": "i2e2wqt0nAI",
        "replyto": "i2e2wqt0nAI",
        "invitation": "ICLR.cc/2023/Conference/Paper3665/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents the design of\ndatasets and benchmark of symbolic regression for scientific\ndiscovery. Authors point our some limitations for existing datasets and\ndesign their own dataset categorizing subsets of small, medium and\nlarge complexity and develop a new metric based on tree distance\nbetween the true and the predicted equations.",
            "strength_and_weaknesses": "The main contribution of this work lies in the design of\ndatasets and benchmark of symbolic regression for scientific\ndiscovery. Authors point our some limitations fo existing datasets and\ndesign their owm dataset categorizing subsets of small, medium and\nlarge complexity and develop a new metric based on tree distance\nbetween the true and the predicted equations.\n\nWhere is I in the examples of Table 1? (It is mentioned in the caption\nof this table)\n\n\"we preprocess equations by 1) substituting constant values e.g., \u03c0 and\nPlanck constant to the expression...\" --> isn't the idea to learn these\nexpressions from data? Do you mean that you *postprocess* the learned\nformulas?\n\n\"coefficient values themselves (e.g., value of C1 in Fig. 2) should\nnot be important\" --> I don't get the idea here. I'd think that\nconstants can differentiate one expression from another when modeling\nreal systems (e.g., physics, math etc).\n\n\"such true equations will not be available in practice\" --> but don't\nyou generate data from these equations in order to create the\ndatasets? How do you expect to validate learning models if you don't\nhave the ground truth? Besides, don't Feynman lectures present the\nequations?\n\nDo you have any explanation of why ST results in 0% R2 for all\nproblems? It seems strange.\n\nOther minor comments:\nboth the metrics --> both metrics\nTable 4 show --> Table 4 shows\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nNovelty: good\nReproducibility: authors provide various items that may allow the work to be reproduced",
            "summary_of_the_review": "The main contribution of this work lies in the design of\ndatasets and benchmark of symbolic regression for scientific\ndiscovery. Authors point our some limitations fo existing datasets and\ndesign their owm dataset categorizing subsets of small, medium and\nlarge complexity and develop a new metric based on tree distance\nbetween the true and the predicted equations.\n\nWhere is I in the examples of Table 1? (It is mentioned in the caption\nof this table)\n\n\"we preprocess equations by 1) substituting constant values e.g., \u03c0 and\nPlanck constant to the expression...\" --> isn't the idea to learn these\nexpressions from data? Do you mean that you *postprocess* the learned\nformulas?\n\n\"coefficient values themselves (e.g., value of C1 in Fig. 2) should\nnot be important\" --> I don't get the idea here. I'd think that\nconstants can differentiate one expression from another when modeling\nreal systems (e.g., physics, math etc).\n\n\"such true equations will not be available in practice\" --> but don't\nyou generate data from these equations in order to create the\ndatasets? How do you expect to validate learning models if you don't\nhave the ground truth? Besides, don't Feynman lectures present the\nequations?\n\nDo you have any explanation of why ST results in 0% R2 for all\nproblems? It seems strange.\n\nOther minor comments:\nboth the metrics --> both metrics\nTable 4 show --> Table 4 shows\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3665/Reviewer_V7wv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3665/Reviewer_V7wv"
        ]
    }
]