[
    {
        "id": "QVdeGw_Gat",
        "original": null,
        "number": 1,
        "cdate": 1666275497445,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666275497445,
        "tmdate": 1666275497445,
        "tddate": null,
        "forum": "LcQ3aRCEuKK",
        "replyto": "LcQ3aRCEuKK",
        "invitation": "ICLR.cc/2023/Conference/Paper3633/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose an SO(3) equivariant variational autoencoder, acting on steerable features. Using standard datasets, they examine the accuracy of reconstruction, and whether the latent representations of objects are useful for object classification and regression tasks.",
            "strength_and_weaknesses": "I think the idea of the paper is that using steerable features and equivariant models for unsupervised learning should yield good representations for downstream prediction tasks.  It is a reasonable hypothesis, but it wasn\u2019t clear to me if the results really support it. Of the tasks presented, which ones are already better solved in simpler ways? (I\u2019m not familiar with literature on unsupervised classification of 3D shapes).\n\nE(3) equivariant autoencoders already exist, for example in Satorras et al. (2022) https://arxiv.org/pdf/2102.09844.pdf, but I have not previously seen examples using steerable features.  Introducing spherical harmonics etc. adds complexity, which ideally would be justified by results.  Maybe the cited paper by Brandstetter et al. could be a source of ideas about when the extra complexity is worth it.",
            "clarity,_quality,_novelty_and_reproducibility": "As far as I can see, code is not provided.\n\nI greatly appreciate the effort the authors have made to explain how the steerable features and equivariant transformations are constructed.\n\nSome language could be more precise. For example, in 2.1 I think \u2018data that is distributed in 3D space\u2019 means \u2018functions on R3\u2019 (with point clouds being sums of delta functions). In 3.3, \u2018point clouds of amino acids\u2019 means \u2018single amino acids represented as point clouds of atoms\u2019. Use of the word \u2018hologram\u2019 seems gratuitous: trying to relate the model to physical holograms (i.e., interference patterns used as diffraction gratings) only confused me.\n\nIn 5.2 what are the 6 channels? I expected 3 channels for R, G, B (I'm not familiar with this dataset).\n\nEquation (3): is c channel? Is every h positive? Should each h be squared? I don\u2019t know the term \u2018total norm\u2019, is it standard? Can you just use layer norm (Ba et al. 2016)?\n\n3.4, 3.5 MSE loss and cosine metric: I do not understand the claim that you cannot measure goodness of reconstruction using MSE and that the problem is fixed by looking at cosines instead. 2.1 says the Zernike polynomials are a complete orthonormal basis for functions on R3, so MSE in truncated ZFT seems like an OK measure of reconstruction accuracy (it's just MSE ignoring the higher-order Zernike components).\n",
            "summary_of_the_review": "The model is new, and I believe it may prove to be useful, but I would like the usefulness to be shown empirically. I would like the authors to go beyond 'let's use spherical harmonics in a VAE' to 'here's what it can do that a simpler E(3) equivariant VAE can't do'.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3633/Reviewer_Wgbf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3633/Reviewer_Wgbf"
        ]
    },
    {
        "id": "F1occn1NwV",
        "original": null,
        "number": 2,
        "cdate": 1666329054206,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666329054206,
        "tmdate": 1666560451101,
        "tddate": null,
        "forum": "LcQ3aRCEuKK",
        "replyto": "LcQ3aRCEuKK",
        "invitation": "ICLR.cc/2023/Conference/Paper3633/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work presents H-VAE, an end-to-end SO(3)-equivariant VAE.  The input is a radial signal in $R^3$ called $\\rho(r,\\theta,\\phi)$ which is mapped via the Zernike Fourier Transform to a set of coefficients of Zernicke polynomials which are products of spherical harmonics and radial basis functions.  The SO(3)-equivariant encoder and decoder both operate in Zernike Fourier space, using block diagonal linear maps, tensor product, and Clebsch-Gordon decomposition to achieve equivariance.  The latent space is formed by a single element of SO(3) and an invariant vector.  The authors demonstrate in experiments that this results in the model learning a canonical frame for inputs.  That is, inputs are essentially rotated to be aligned to a canonical frame and the rotation is stored along with an invariant encoding of the canonically rotated input.  The authors demonstrate their method in 4 experimental domains, spherical MNIST, SHREC 17 3D model classification, Amino Acids, and Protein neighborhoods.  The evaluation shows the model achieves good clustering by class and meaningful alignments.  The model also has good reconstructions and can be sampled from to produce reasonable outputs.  ",
            "strength_and_weaknesses": "## Strengths\n- The model is well-described and reasonable.  In contains block diagonal linear layers acting in Fourier space (as found in other SO(3)-equivariant methods, e.g. Thomas '18), tensor product (ETP) non-linearities (Kondor et al., '18) instead of radial non-linearities.  The authors introduce Signal norm to stabilize learning.  The latent space uses a single SO(3) element constructed using Gram-Schmidt and an invariant vector, a design inspired by Winters '22.  While most of these elements originate elsewhere I think they make for a reasonable design here which, so far as I can tell, is the first end-to-end SO(3)-equivariant VAE.     \n- The part of the experiments I found most interesting was the idea to insert the identity into the decoder and thus see the output in canonical orientation.  From the results, it does seem the model learns to align inputs leading to a disentanglement of frame and identity which is quite interesting and could have potential applications in 3D alignment or classification problems in which it is difficult to classify objects without disentangling pose.  \n\n## Weaknesses\n- The motivation seems a bit weak to me.  The experiments appear to use relatively small point clouds or spherical signals at a single radial distance.  The latent space embeddings do not seem to lead to better performance in downstream tasks.  They do seem better clustered, have nice interpolations, and can be easily sampled, but why is this desirable and is it not true of previous methods?  I would wager there are good answers to these questions, but I didn't get a strong impression from the paper.  In particular, it would nice to see applications in which orientation-identity disentanglement is critical.\n- The paper could use an ablation study.  How do features such as the Fourier space, the Zernicke basis, ETP, signal norm, and a small degree latent space contribute to success?  In particular, most SO(3) methods use simple radial basis functions localized at different radii.  Why is the Zernicke basis superior? Does orthogonality contribute so useful property?  \n- The paper could use more baselines.  There are many VAEs which could applied in the experimental settings as well as other generative models.  Could some of the claimed features (such as frame-aligned latent vectors), better clustering, etc. be realized by models with different generative paradigms or without equivariance or without equivariance but trained with data augmentation or using some form of canonicalization?  My hypothesis would be the proposed method is better at disentangling the frame, but this could be demonstrated.  \n- While the theory is sound, the equivariance of the model should be empirically confirmed to verify correct implementation and quantify any error arising from approximation.\n- I have a concern/question about the decoder and the size of the latent space.  Given that the input has features with high frequency $l_{max}$ it seems a bit strange to me go down to just an element of SO(3) and invariant features.  The reconstructions do seem to show this is adequate and since the encoder and decoder and complex non-linear functions, I don't think there is any theoretical problem, but it still strikes me as an unusual design choice.   Other works use higher-dimensional mixtures of higher frequency harmonics to encode latent representations of spherical signals.  I'm not sure this is the wrong choice, but I'd like to see the comparison.  \n- I'm not sure, but here is one version of my concern: the decoder takes $g \\in SO(3)$ and $z \\in \\mathbb{R}^n$ invariant, so $d(g,z) = g d(I, z)$.  However, this uses up the equivariance constraint, so that $d(I,z)$ is unconstrained.  It could thus be a simple MLP with g acting at the end and have the same equivariance property.  So what is gained by using the given architecture? \n\n\n## Questions\n- I'd like a proof signal norm is equivariant.  It seems like summing the components of the vectors would not be, but maybe I've misunderstood the notation.  \n- Is it possible to use a quantitative measure of for the quality of the random samples from the prior?  It may make it easier to compare to other work. \n- Is it not possible to fill in some missing values in table 1 in some way? Why do the supervised methods lack class. Acc.? ",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarify\n\nThe paper is very clearly written.  I would have liked some more experimental details in some cases.  \n\n## Quality\n\nThis seems like a relative high-quality paper, but the experiments seem incomplete to me.  I would like to see more motivation and comparisons to other methods. \n\n## Novelty\n\nGiven the number of SO(3)-equivariant methods to date, I was a bit unsure about the claim this is the first SO(3)-equivariant VAE.  Some searching shows that while there are many equivariant generative models, they are mostly not end-to-end equivariant autoencoders (some use diffusion, flows, or are autoregressive).  The one example I found (https://arxiv.org/abs/2205.07309) is conditional and point cloud based making it different from this method.  The method is indeed novel. \n\n## Reproducibility\n\nI did not run the code, but the model description is quite clear and the data appears to be publicly available. \n\n",
            "summary_of_the_review": "This paper contains a promising approach and is well-written but the motivation and experiments seem incomplete to me.  My score is tentative and I willing to adjust it based on discussion.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3633/Reviewer_FEZB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3633/Reviewer_FEZB"
        ]
    },
    {
        "id": "0Y-_SOignV",
        "original": null,
        "number": 3,
        "cdate": 1666559814264,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559814264,
        "tmdate": 1670297741233,
        "tddate": null,
        "forum": "LcQ3aRCEuKK",
        "replyto": "LcQ3aRCEuKK",
        "invitation": "ICLR.cc/2023/Conference/Paper3633/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an end-to-end $SO(3)$-equivariant (V)AE. The idea is to learn, in the spherical Fourier space, an dis-entangled embedding of the input datum -- one corresponding to the invariant embedding describing the datum in a \"canonical\" orientation, and the other being the equivariant frame describing the datum's orientation. Experiments have been conducted trying to verifying the authors' claim that the proposed model can achieve good unsupervised clustering and classification results on spherical images and protein structures.",
            "strength_and_weaknesses": "**Strength**\n1. Inspired on the prior work by Winter et al. (2022), the idea of learning disentangled representations corresponding to the group-invariant embedding of the data as well as its equivariant frame describing its $SO(3)$ rotation is interesting. \n2. The authors have tried to conduct extensive experiments to verify the usage of the proposed model.\n\n**Weakness**\n1. The writing and structure of the paper can be significantly improved to make the paper clearer. For example, most of the technical terms introduced in Section 2 need further explanation for clarity, and many definitions and explanation of the appendix A.1 should be moved to the main text.\n2. Since the theoretical contribution of the paper is limited, one would expect the authors to better explain the architectural design of the proposed model. However, most of the implementation details are buried in the appendix, which makes Section 3 of the main text especially confusing.\n3. The authors claim that they achieved state-of-the-art performance in unsupervised clustering and classification. However, the only method to which the paper compared is an arxiv paper by Lohit & Trivedi, 2020. This makes the claim less compelling.\n4. Even though the authors explained why MSE is not a \"good\" measure of the reconstruction, the models are still trained using MSE. It is thus reasonable and fair to compare the results using MSE.\n5. It is interesting to see whether the model indeed learns disentangled representation. An easy experiment to conduct is to change the frame in the latent space with a fixed $z$. I am wondering whether this will generate a rotated copy.\n6. In table 1, the previous work by Lohit & Trivedi achieves the best result when measured in \"purity and V-means\", while it achieves the worst result when measured in \"classification accuracy\". This makes me wonder whether the valuation metric proposed by the authors are convincing.\n7. Since Shrec17 is 3D data to begin with, what is the rationale of projecting it to a sphere?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper could be significantly improved. The idea of disentangled equivariant representation learning is interesting, but not novel.",
            "summary_of_the_review": "Although the idea is interesting, there is a lot of room for the paper to improve.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3633/Reviewer_nuL1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3633/Reviewer_nuL1"
        ]
    }
]