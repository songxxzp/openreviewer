[
    {
        "id": "Xy3jBMwLbn",
        "original": null,
        "number": 1,
        "cdate": 1666086740758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666086740758,
        "tmdate": 1669796934822,
        "tddate": null,
        "forum": "70BaDC5ceIO",
        "replyto": "70BaDC5ceIO",
        "invitation": "ICLR.cc/2023/Conference/Paper1989/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The manuscript studies the approximation properties of solutions of variational problems with Barron functions. In particular, this includes solutions of nonlinear PDEs like the p Laplace equation. It follows recent works on approximation properties for solutions of linear PDEs and extends this analysis to general variational problems. More precisely, it considers a preconditioned gradient descent of the variational energy in the Sobolev space $H^1_0$, which converges linearly, and bounds the Barron norm of the iterates, which relies on an assumption on the variational energy. Overall, this provides an estimate on the Barron norm required for the approximation of the solution of the variational problem up to a given accuracy. By standard arguments, this also implies upper bounds on the complexity of shallow networks with sigmoidal or ReLU activation for these target class and provides way of analysing the dependence of the convergence properties in dependence of the dimension of the problem.",
            "strength_and_weaknesses": "The paper addresses the timely and important question of the approximation properties of neural network related function classes of solutions of variational problems. I believe that the extension to general variational problems is a valuable contribution. Main weaknesses of the manuscript include:\n* The work follows existing works on linear problems very closely. \n* The consequences of the main theorem are not well elaborated. For example, no theorem bounding the Barron norm or the complexity of a shallow network required for eps approximation is given. Although these steps seem straight forward, they would be important to be carried out, especially in light of Theorem 4 the error does not decay to zero as discussed by the authors. \n* The paper is currently not very well written and exhibits a lot of spelling and grammatical mistakes, which and should be addressed.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the manuscript provides a clear overview of the proof techniques used for the main theorem. Unfortunately, the paper is currently not very well written and exhibits a lot of spelling and grammatical mistakes, which make reading less smooth and can lead to some confusion. Examples for this are in Theorem 4, where it is not clear, whether the constant $R$ can be chosen or is only guaranteed to exist and in Theorem 1, whether the complexity bound holds for $\\epsilon\\to0$ or $d\\to\\infty$. \n\nThe proof techniques are very similar to previous works, where the difference is to go from a quadratic energy to an energy, which is strongly convex with upper bounded second derivative, i.e., admits upper and lower bounds of the squared norm. Hence, the techniques appear to be not novel, that being said, this is a normal way to go, when generalizing result for linear to non-linear problems. Thus, I don\u2019t think this is a reason to reject this paper. ",
            "summary_of_the_review": "The paper addresses the timely and important question of the approximation properties of neural network related function classes of solutions of variational problems. I believe that the extension to general variational problems is a valuable contribution. That being said, I believe that the manuscript in its form is not ready for publication. My main concerns are the following:\n\n* *Implications of results:* The actual implications are not well developed. In particular, it would be important to elaborate the consequences of Theorem 4. In particular, it is not very clear, whether Theorem is able to establish estimates for $\\varepsilon\\to0$. Interesting implications include a precise formulation of the Barron norm required for $\\varepsilon$ approximation of a solution of a variational problem and complexity bounds for neural networks for $\\varepsilon$ approximation. This would could greatly improve the insights of the results and would make the results more applicable for future works. I know that to some extend this is done in Remarks 1 and 2, but it is not clear to me, whether this holds for arbitrary $\\epsilon$ and if so how to proof this.\n* *Inhomogeneous problems:* Can you also treat inhomogeneous variational problems? This seems to be crucial for the following reason: Consider the Laplace equation, then solution of the homogeneous problem are harmonic hence smooth and can be approximated at a dimension independent rate, where the constants might depend on the dimension (follows from standard arguments like Yarotsky).\n* *Clarity and quality of writing:* Unfortunately, the paper is currently not very well written and exhibits a number of spelling and grammatical mistakes as well as unintuitive formulations, which make reading less smooth and can lead to some confusion. I attached a list of specific observations below.\n\nSpecific questions:\n* Can you provide a theorem that bounds the Barron norm and complexity of a network required for $\\epsilon$ approximation? \n* Why do you state your results in the abstract and introduction for $L^2$ and not for $H^1$ approximation?\n* In Assumption 1: Is this supposed to hold for some $\\epsilon_L<\\lambda$? Then I would formulate it that the assumption is that $\\epsilon_L := \\sup \\dots < \\lambda$. \n* In Theorem 4: it is not clear to me what \u201econsidering a constant $R \\le \\dots$ \u201c means. Can this be chosen? If so, can it be set to 0? If it can not be chosen, then I would maybe say \u201e$R = \\dots$\u201c.\n* Can you treat non-homogeneous problems? \n* What is the benefit of working with $\\tilde L$ and not with $L$?\n\nSpecific comments:\n* The formulation \u201eneurally simulating (preconditioned) gradient descent\u201c is confusing to me. Although, it sounds very fancy, I do not understand what it means, since form my understanding you use this for preconditioned gradient descent in a Sobolev space without having a direct connection to neural networks nor to simulation of something. When using such a terminology it should be better explained.\n* In the introduction, Sirignano&Spiliopoulos (2018) is given as a reference for a work minimizing the variational formulation of a PDE. However, they minimize the residual energy, which is fundamentally different to the variational energy.\n* It would make sense in Lemma 1 to say that (3) is to be understood in the weak sense.\n* In Theorem 4: I wonder whether $\\tilde\\epsilon$ is a good symbol, since epsilons are usually going to zero, which this quantity is not. \n* You claim to \u201esubsume and substantially generalize\u201c prior works. In general, I would be very careful with judging the impact of any work, but this claim seems not entirely correct. From what I can see you extend their analysis to general variational problems, but only study homogeneous equations. Hence, prior results on linear but inhomogeneous problems are not special cases of your results.\n* Further references of interest: E&Wojtowytsch (2022), Grohs&Herrmann (2022)\n* Regarding Yarotsky: Yarotsky provides an approximation rate of $O(N^{-r/d})$, where $r$ is the regularity of the target function; if the solutions of the PDEs are smooth, which is for example the case for harmonic functions, then approximation with a power law with exponent independent of the dimension is possible; hence I disagree with your statement that the networks must by exponentially large; further, I do not know what you mean by Yarotsky's results being non-parametric\n* It could be a consideration to include the appendix in the normal PDF rather than the supplement.\n\nThroughts regarding formatting and writing:\n* In the list of references, the titles should be case sensitive, for example \u201ePDEs\u201c instead of \u201epdes\u201c.\n* Try to avoid double brackets.\n* On page 1: it is stated that \"... is at least as expensive as the neural network required to represent it.\". I think this is not very clearly written, maybe it would be better to say \"In general the complexity of fitting a NN grows with its size\"?\n* Section 2: \n    * Should be $\\Omega\\subset\\mathbb R^d$ and $\\partial\\Omega$\n    * Generally better to use $u\\colon\\Omega\\to\\mathbb R$ instead of $u:\\Omega\\to\\mathbb R$ (\"\\colon\" instead of \":\")\n    * Below Definition 1 it would be nice to add in what space the problem is well posed\n    * In Lemma 1 it should be \"the minimizer $u^\\star$\n* Section 3: \n    * In the first sentence it should b \"... been a growing line of works that utilize neural\"\n    * Regarding \"This is a great and promising direction...\" I would in general shy away from judging the quality of other research in a research paper\n    * It should be \"equation and Lu & Lu (2021) extend their analysis\"\n    * It should be \"Schr\u00f6dinger\" (\\ \"{o})\n    * When using a bibtex style that gives the authors names, I would not write \"in [authors names]\" but simply \"[authors names]\" (e.g., \"In Marwah et al. (2021)\")\n* Section 4: \n    * I would try to not start a sentence with a symbol (in your case $C^\\infty$)\n    * In Definition 3 it should be $g\\in L^2$ and I would rather write $\\nabla g\\in L^2$\n    * In Theorem 3 I would introduce $\\sigma$ before the statement.\n* Section 6: \n    * \"neurally unfolding\": As said before, I do not understand what is meant with this; I think the description as preconditioned GD in a Sobolev space is perfectly clear.\n    * Lemma 4: there should be a full stop after \"Definition 3\"\n    * The reference to Equation 28: is this where it should refer to? If so, referencing to something burried in the appendix for a high level idea is not ideal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1989/Reviewer_AUeL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1989/Reviewer_AUeL"
        ]
    },
    {
        "id": "LpUs6x2eXl",
        "original": null,
        "number": 2,
        "cdate": 1666168465212,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666168465212,
        "tmdate": 1668343061418,
        "tddate": null,
        "forum": "70BaDC5ceIO",
        "replyto": "70BaDC5ceIO",
        "invitation": "ICLR.cc/2023/Conference/Paper1989/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the representational power of neural networks for approximating solutions to nonlinear PDEs. They focus on a specific class of nonlinear elliptic variational PDEs. They show that a 2-layer neural network can be used to approximate the solution. Thus showing neural networks can evade the curse of dimensionality. This is a theoretical paper; perhaps the application aspect can be strengthened.",
            "strength_and_weaknesses": "Strength\n\nWhile most previous approaches have studied linear equations, this paper considers a nonlinear family of PDEs and studies nonlinear variational PDEs. \n\nThe results go beyond the typical non-parametric bounds on the size of an approximator network that can be easily shown by classical regularity results of the solution to the nonlinear variational PDEs and universal approximation results.\n\nWeaknesses\n\nThere is no obvious application for this paper.\n\nThis paper considers only the elliptic equations, but the equations that evolve over time are also important.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear, but there is no summary of the limitations.\n\nThe methodological innovations are mainly at the theoretical level and hardly appreciated by ICLR readers.\n",
            "summary_of_the_review": "This paper presents a theoretical work on neural networks for nonlinear elliptic equations. I am not a mathematics major, and the mathematical proof is difficult for me to understand. It would be more solid to show some applications.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1989/Reviewer_23dt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1989/Reviewer_23dt"
        ]
    },
    {
        "id": "eCfTzwGlsT2",
        "original": null,
        "number": 3,
        "cdate": 1666604021892,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604021892,
        "tmdate": 1666604021892,
        "tddate": null,
        "forum": "70BaDC5ceIO",
        "replyto": "70BaDC5ceIO",
        "invitation": "ICLR.cc/2023/Conference/Paper1989/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors consider the potential approximability of solutions to PDEs by neural networks. This topic has attracted significant interest in the past few years and a key question, as noted by the authors, is to understand which class of PDEs give rise to solutions that can be efficiently represented by a neural network without the curse of dimensionality, i.e. when the network size doesn't grow exponentially with the spatial dimension. The authors expand upon the previous works on linear PDEs by consider nonlinear variational PDEs of the form $-\\nabla\\cdot(\\nabla L(\\nabla u))=0$ and show that the solution has low complexity when $L$ does. Here, the complexity is formulated in terms of the Barron norm of the solution, which is a standard tool in the deep learning and PDE literature.",
            "strength_and_weaknesses": "### Major comments\n\n1. The paper is well-written and nice to read. However, its current organization makes Section 2 not really rigorous (essentially because some notions are introduced later in Section 4). Additionally, the literature review is splitted in two parts between the introduction in Section 1 and related works in Section 3. I suggest a re-organization of Section 1-4 to merge all the literature review into Section 1 by adding:\n  - Subsection 1.1 Related works, which includes all the literature review.\n  - Subsection 1.2 Contributions, summarizing the essential contributions informally like Theorem 1 but without the notations and definitions introduced later.\n2. The authors show the existence of solutions to variational PDEs of the form of (1) with small Barron norm, motivated by a result connecting Barron norm and approximability by a neural network. They also mention in the conclusion that an interesting question is the number of parameters by a neural network like Marwah et al. (2021). However, the main question considered in the introduction (and most interesting due to its connection with the practical works on solving PDEs with neural networks) concern the representability of solutions to PDEs by a small neural network. While the authors show a small complexity of the solution, it would be much more interesting (and impactful) to state a theorem in terms of neural network size after proving Theorem 4.\n3. The authors vaguely mention in Section 1 the type of problems modeled by the nonlinear PDEs they consider. It would be good for the clarity of the paper to expand on this by giving precise examples with explicit functions $L$.\n4. Would the approach considered by the authors naturally extend to time-dependent PDEs like nonlinear parabolic equations of the form $du/dt - div(grad L(grad u))=0$?\n5. The proof structure seems very similar to (Marwhah et al., 2021; Chen et al., 2021), could the authors add some clarifications on the novelty of the proof techniques with respect to these prior works due to the nonlinear class of PDEs considered?\n\n### Minor comments\n\n1. First line of Section 2, $\\Omega \\subset R^d$, the Poincare constant is not defined. I would write \"is such that the Poincare constant Cp satisfies $Cp\\geq 1$\" and add a footnote defining Cp. [I understand it is defined later in Theorem 2, which is why I suggest a re-organization of the paper].\n2. Footnote 2 requires a dot at the end of the sentence.\n3. Definition 1, the function space for u is not defined, what is meant by \"L is smooth\"? In which class of functions does L belong to?\n4. Definition 2, the max should be over $1\\leq i\\leq d$.\n5. Theorem 2, add a reference for Poincare inequality?\n6. Definition 4 should be \"Let F be the set...\" or \"We define F to be the set...\".\n7. Definition 5, \"Let Gamma be a set...\".\n8. Theorem 3, would this work for any non-polynomial activation function like the assumption of the universal approximation theorem?\n9. Could you comment on the difference with the paper by De Ryck, Jagtap, and Mishra \"Error estimates for physics informed neural networks approximating the Navier-Stokes equations\", which is analyzing solutions to the Navier-Stokes equations?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, however, I would suggest a re-organization of the first two sections (see major comment 1) to merge the literature review in the introduction and avoid presenting the main results before the introduction of essential notations. While I am aware of the works on linear PDEs, mentioned by the authors, I have not seen similar attempts on nonlinear PDEs.",
            "summary_of_the_review": "The authors generalize existing results on the approximation of solutions to elliptic PDEs by neural networks to nonlinear variational PDEs. The paper is well-written and a good addition to the literature on this topic but would benefit from a re-organization of the structure and clarification of the novelty of the proofs compared to prior works. In addition, the authors show that solutions to the nonlinear PDEs have low-complexity but do not continue to derive a result on the size of the neural networks needed to approximate them. I would recommend the paper for publication after a revision addressing the major and minor comments listed above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1989/Reviewer_N9yD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1989/Reviewer_N9yD"
        ]
    },
    {
        "id": "RQCYZJTiE45",
        "original": null,
        "number": 4,
        "cdate": 1666788061805,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666788061805,
        "tmdate": 1666788061805,
        "tddate": null,
        "forum": "70BaDC5ceIO",
        "replyto": "70BaDC5ceIO",
        "invitation": "ICLR.cc/2023/Conference/Paper1989/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper is fundamentally a PDE analysis result involving the regularity of solutions to (nonlinear) variational problems with respect to the Barron norm when the function involved in the variational problem (to within an epsilon approximation in the H^1 seminorm) satisfies a Holder-like condition (with respect to the Barron norm).  The proof is via analysis of an iterative approximation scheme (preconditioned gradient descent).  This is relevant for variational approximation schemes using neural networks as a trial space, because bounds on the Barron norm of the solution correspond to bounds on the size of a two-layer neural network needed to represent the solution to a given accuracy.",
            "strength_and_weaknesses": "The paper is a nice result, clearly written.  The proof tools are standard for this type of PDE analysis, but they are well used here.\n\nThe result does not tell us anything about deeper neural networks, nor does it seem to tell us how to find the optimal approximating networks that satisfy the bound -- we just know that such networks must exist.\n\nI would like to see this work out in the literature somewhere, and it is certainly relevant to people trying to build neural network schemes for the solution of PDEs arising in science.  But I admit that if I was looking for a result of this form, I would probably usually start in a journal on approximation theory or PDE analysis rather than in ICLR or similar conference proceedings.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear, at least for someone who is familiar with this sub-area.  The main argument is in the supplementary materials rather than the main body, and I have not checked all the details.  Nonetheless, this seems like a fairly standard type of argument for this flavor of results, and I believe that it is probably correct, and a nice result.  Extending these types of analyses from linear to nonlinear PDEs is generally a useful thing to do, and I am not aware of other results of this form for this case.  The paper is purely theoretical, so there are no issues of reproducibility here.",
            "summary_of_the_review": "This is a nice result, and I would like to see it published somewhere, though it would not be obvious to me that an ML conference is the venue where it would have the best reception.  Complexity of approximation is important for understanding numerical schemes for solving PDEs generally, and there are relatively few such results around approximation schemes that involve neural networks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1989/Reviewer_4yVB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1989/Reviewer_4yVB"
        ]
    }
]