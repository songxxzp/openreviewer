[
    {
        "id": "7nJR3COJk8T",
        "original": null,
        "number": 1,
        "cdate": 1666410076431,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666410076431,
        "tmdate": 1668830471825,
        "tddate": null,
        "forum": "PQOlkgsBsik",
        "replyto": "PQOlkgsBsik",
        "invitation": "ICLR.cc/2023/Conference/Paper1282/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a unified model for multi-modal retrieval. The proposed method consists of two techniques, the universal embedding optimization strategy for contrastively optimizing the embedding space, and the Image verbalization method for bridging the modality gap.",
            "strength_and_weaknesses": "The main strengths of this paper can be concluded as follows:\n\n1. This paper proposes a unified model for multi-modal retrieval, which demonstrates that universal multi-modal search is feasible compared with the divide-and-conquer pipeline with a united model, and also benefits cross-modality tasks.\n\n2. The proposed method bridges the modality gap by optimizing the vision-language embedding space using hard negatives, and aligning the semantics of image captions and figure pixels, which achieves state-of-the-art performance on WebQA datasets.\n\nThe main weaknesses of this paper are as follows:\n\n1. There is still a gap between the proposed method and SOTA method in some evaluation tasks on the WebQA Dataset. The authors should provide more corresponding analyses.\n\n2. An important assumption in this paper is that the universal embedding optimization strategy can enable the optimization of the universal embedding space with modality-balanced hard negatives. The authors should provide a mathematical explanation to make the assumption more convincing.\n\n3. Relevant works such as [a] [b] should be compared to make this paper more comprehensive.\n\n[a] Effective Conditioned and Composed Image Retrieval Combining CLIP-Based Features. CVPR2022.\n[b] Visual\u2013Textual Hybrid Sequence Matching for Joint Reasoning. TCVB2021.",
            "clarity,_quality,_novelty_and_reproducibility": "The structure of this paper is complete, but more details of the Image verbalization method need to be provided, and the rationality of the Universal embedding optimization strategy needs to be more fully verified.",
            "summary_of_the_review": "This paper is marginally above the acceptance threshold of ICLR2023.\n\n=======================================================================================================\n\nI have checked the comments of the other reviewers and the feedback from the authors carefully, and I would like to keep my score. The authors have answered my concerns, especially the mathematical explanation of the universal embedding optimization strategy. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1282/Reviewer_vCSr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1282/Reviewer_vCSr"
        ]
    },
    {
        "id": "A6s-LiCPtqY",
        "original": null,
        "number": 2,
        "cdate": 1666431901432,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666431901432,
        "tmdate": 1666431901432,
        "tddate": null,
        "forum": "PQOlkgsBsik",
        "replyto": "PQOlkgsBsik",
        "invitation": "ICLR.cc/2023/Conference/Paper1282/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents universal vision-language dense retrieval model, which builds a unified model for multi-modal retrieval. The proposed model encodes queries and multi-modal resources in an embedding space for searching candidates from different modalities. To learn a unified embedding space for multi-modal retrieval, this work has come out with (1) universal embedding optimization strategy, which contrastively optimizes the embedding space using the modality-balanced hard negatives; (2) image verbalization method, which bridges the modality gap between images and texts in the raw data space.",
            "strength_and_weaknesses": "**Strengths**\n\n(1) The paper addresses a very interesting tasks of visual language research.\n(2) The paper is reasonably well presented and written in good English.\n(3) Experimental results are quite encouraging.\n\n**Weaknesses**\n\n(1) The novelty is not clear. I was expecting it to be mentioned or specified in the introduction of the paper for better understanding on the contribution of the paper. Furthermore, I am aware of the following existing papers which are also considering multi-modal embedding for different tasks, such as text [1], sketch [4] and both [3,5] based image retrieval, text and image topic modelling [2] etc.\n\n(2) It is not very clear why a simple summation of the representation of image caption and image feature works well for combining two very different types of modalities. I wonder if any other combination (concatenation, outer product etc.) has been ablated or could be interesting to try.\n\n(3) Currently a lot of progress has been made on text, sketch and both based image retrieval which should also be included and discussed within the literature of cross-modal retrieval.\n\n(4) It is not clear why [CLS] and [SEP] tokens special and different.\n\n(5) I think it is worth defining universal representation learning and image verbalization procedure.\n\n(6) In the experimental results table, citation of the baselines and SOTA methods should be given for readability.\n\n[1] Mai et al., Spatial-Semantic Image Search by Visual Feature Synthesis, CVPR, 2017.\n[2] Gomez et al., Self-supervised learning of visual features through embedding images into text topic spaces, CVPR, 2017.\n[3] Dey et al., Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval using Text and Sketch, ICPR, 2018.\n[4] Dutta and Akata, Semantically Tied Paired Cycle Consistency for Any-Shot Sketch-based Image Retrieval, IJCV, 2020.\n[5] Sankgkloy et al., A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch, ECCV, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably clear. It is well written in good English and the reported results are encouraging. The authors have also provided the codes in the supplementary material which I expect to be useful for it reproducibility. However, I haven't found much originality in the core work, which I commented in the weaknesses section.",
            "summary_of_the_review": "Taskwise, the paper is interresting. It is well presented and also the reported results are also interesting. Howevere, I have found the novelty to be limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1282/Reviewer_qnyp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1282/Reviewer_qnyp"
        ]
    },
    {
        "id": "ufUAFlpqi8",
        "original": null,
        "number": 3,
        "cdate": 1666653422150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653422150,
        "tmdate": 1669849441615,
        "tddate": null,
        "forum": "PQOlkgsBsik",
        "replyto": "PQOlkgsBsik",
        "invitation": "ICLR.cc/2023/Conference/Paper1282/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a universal vision-language dense retrieval with two techniques, using modality-balanced hard negatives for optimization and bridging the modality gap by the image verbalization method.  Experiments are conducted on the built open-domain dataset from WebQA and compared to existing models for single-modality, divide-and-conquer, and multi-modal retrieval and ablation studies.",
            "strength_and_weaknesses": "Strength:\nReasonable motivation and a combination approach. \n\nWeakness:\nThe proposed image verbalization method simply adopts the existing approach to generate matched captions or queries according to pictures. The performance of the combined method may be bounded by the methods of proposal generation and caption generation models. Is there a strong reason the authors choose VinVL (Zhang 2021) and MLM (Devlin 2019)? Or, it can be any arbitrary methods to achieve this.  In addition, what is the definition of hard negatives,  some visualization examples may help? What will it happen if the proposed method uses non-hard negatives compared to other methods? Are those good results from the hard negatives? \nCan the authors explain why to choose NDCG@10, instead of @1, @5, as we know that most of the time the top retrieved results are the most important to queries?  When good results on MRR@10 and @20, why BM25 & Clip-DPR Rec@20 is better a lot in Table 2? Does that imply the UniVl-DR + BM25 method can provide even better? ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written. It's clear and easy to follow, but the novelty may be limited.",
            "summary_of_the_review": "The paper proposes to build a universal embedding space for single and cross-modality matching for retrieval tasks with a multi-modality training strategy for queries and documents and try to mitigate the modality boundary between vision and language. I have some concerns (see weakness). If my concerns are addressed, I will consider increasing the score appropriately.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1282/Reviewer_FZm3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1282/Reviewer_FZm3"
        ]
    }
]