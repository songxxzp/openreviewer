[
    {
        "id": "FMKItGfCo5",
        "original": null,
        "number": 1,
        "cdate": 1666204891683,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666204891683,
        "tmdate": 1669578297583,
        "tddate": null,
        "forum": "8HRvyxc606",
        "replyto": "8HRvyxc606",
        "invitation": "ICLR.cc/2023/Conference/Paper948/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work cautions the application of CKA similarity measure in comparing neural representations in practice and points out a few issues including 1) its sensitivity to the family of the subset translations where certain transformations do not change the functional behaviors of the model; 2) its value can be directly manipulated without significant changes in the model behaviors.",
            "strength_and_weaknesses": "On the strengths,\n- The authors provide a formal characterization of the CKA sensitivity to the family of subset translation transformations (i.e., the translation of a subset of the representations) and demonstrate special cases of this family can preserve \"functional behavior\" of the learned neural representations. Thus, an ideal similarity measure should be insensitive to this family, but the CKA fails to do so.\n- CKA is a family of measures so merely showing the linear case is sensitive is not very conclusive. It is nice that the authors not only conduct analysis on linear CKA but also nonlinear CKA with RBF kernels and show that the nonlinear ones still suffer from the same sensitivity.\n- The demonstration of the CKA map can be manipulated into any target map without changing the model performance is quite interesting and reminds me of the adversarial examples. This result questions the validity of many existing conclusions regarding the similarity between different models and their behaviors.\n\nOn the weaknesses,\n- My biggest concern is about the \"similarity\" advocated in this work. Informally, the similarity assumption is that models that get similar accuracies have similar representations. In order to make this more robust, the authors add more constraints including same linear separating hyperplane and same margin. Even with these additional constraints, I do not think this assumption is a good one. Models learned with different random seeds have almost identical in-distribution accuracies, but vastly different out-of-distribution accuracies. In an analogy, this suggests that two distributions having the same first-order moment should be the same distribution. But we only consider two distributions same if they share the same moments in all orders. Therefore, unless the fixed models have similar accuracies in *all* datasets might we decide them to have similar representations. Indeed, similar assumptions have been considered in the literature, but they are used in a different way. For example, (Ding et al., 2021) uses this as an audit. That is, representations having different in-domain and/or out-of-domain accuracies should be different. This is very different from what used in this work.\n- Some of the conclusions are not well supported. For example, it is argued throughout the paper that the subset translation can *naturally* happen in training neural networks but fails to provide any concrete example. In Section 4.1, the conclusion of \"considerably high CKA similarity values for early layers, does not necessarily translate to more useful, or similar, captured features\" is based on the visual differences of the learned filters without considering the \"functional behaviors\" advocated in this work. It is possible that they are *functionally* similar even though they are visually different.",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper is nicely written and easy to follow, and contains the necessary background information of different neural representation similarity measures to be self-contained. \n- There are conclusions that are not well-supported, including the natural occurrence of subset translation in normal neural networks training and inconsistent ways to judge the similarity of representations (visually or functionally).\n- (Kornblith et al., 2019) showed that the CKA similarity is invariant to orthogonal transformations which preserve scalar products and Euclidean distances between examples. This implies that CKA is sensitive to changes in the Euclidean distances between examples. So the analysis in Section 3 is not quite surprising since the subset translation changes the Euclidean distances. That being said, the manipulation of the CKA map into any target map without impacting the model performance appears novel to me and is definitely an interesting contribution. A natural question here is: is this manipulation only applicable to CKA, or any existing similarity measure is susceptible as well?",
            "summary_of_the_review": "The CKA similarity has been widely used to draw conclusions in comparing neural representations across model architectures and learning paradigms, thus it is critical to better understand its strengths and weaknesses. I agree with the authors that any similarity measure should not be blindly applied, and multiple measures should be considered to cover multiple notions of similarity. Even though the CKA map manipulations is quite interesting, I am concerned with the similarity assumption advocated in this work: models with similar accuracies (in one dataset) should have similar representations. Since the main part of this work builds on this assumption, I am leaner towards the negative side.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper948/Reviewer_wkkL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper948/Reviewer_wkkL"
        ]
    },
    {
        "id": "GaFDpGNggSk",
        "original": null,
        "number": 2,
        "cdate": 1666665969747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665969747,
        "tmdate": 1669401436048,
        "tddate": null,
        "forum": "8HRvyxc606",
        "replyto": "8HRvyxc606",
        "invitation": "ICLR.cc/2023/Conference/Paper948/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors inspect the robustness of centered kernel alignment (CKA) as a measure of representational similarity. They analytically derived the limiting behavior of CKA as a subset of points are translated infinitely far away. They suggest that CKA is sensitive to a small number of \"outlier\" values. They empirically back up their theory by showing it is possible for most networks to achieve a variety of across-layer CKA maps without hurting performance. Further, CKA similarity between networks memorizing and generalizing on CIFAR-10 images was surprisingly high. Thus, their empirically results show that representational similarity as quantified by CKA can be decoupled from network function.",
            "strength_and_weaknesses": "I liked the motivation of the paper. CKA is an influential tool in the field and it feels important for us to closely examine its failure cases. I am unaware of any paper within the ML literature which takes a close look into these issues. Figure 2 and Figure 8 show interesting empirical results which I feel are highlights of this work.\n\nOn the downside. I did not feel like the main theorem proved in this paper gave me much intuition, and in fact some of the statements made by the authors were quite confusing to me. For example, at the top of page 5, the authors say \"$\\Vert \\mathbb{E}[x] \\Vert^2  / \\mathbb{E}[ \\Vert x\\Vert^2]) \\sqrt{\\text{dim}_\\text{PR}(X)}$ will be of relatively small value in practice so the whole expression in Eq.\n2 will be dominated by $\\Gamma(\\rho)$.\" Maybe I'm missing something very basic but if $\\Vert \\mathbb{E}[x] \\Vert^2  / \\mathbb{E}[ \\Vert x\\Vert^2]) \\sqrt{\\text{dim}_\\text{PR}(X)} \\approx 0$ then you'd have $\\text{CKA} \\approx 0$ always in the limit of $c \\rightarrow \\infty$? Even when $\\rho = 1/2$? More generally, I think that the limit of very large $c$ isn't too intuitive or sensible to think about (e.g. if some form of weight decay or other regularization is applied). It would be more useful to understand the rate at which the CKA decreases (for finite $c$) in Figure 4. Perhaps the authors can clear up their intuitions in the rebuttal period.\n\nI also think that the results in this paper could be made more rigorous. For example, Figure 2 intrigues me, but I would love to see how much CKA varies across different random seeds. In particular, I would like to see additional lines for `Generalized vs Generalized` and `Memorized vs Memorized` and `Randomized vs Randomized` across random seeds. Also, error bars indicating standard deviations.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the results are mostly novel, although they build off of previous works. One of the main conclusions, that CKA is sensitive seems to have been mentioned in passing by Nguyen et al. (2022), as the authors acknowledge. Further, the main take home message of Maheswaranathan et al. (2019) is that representational geometry is not one-to-one mapped to algorithms or functions computed by a network. However, the experiments (and to a some extent the theorems) in this paper make these observations more precise.\n\nI also have to admit that I'm somewhat lukewarm about a few observations in this paper. For example, the result in Figure 4b seems pretty obvious to me, but the trick of translating points like this without effecting network function only works for the last layer in the network and so this doesn't end up being very insightful.\n\nThe theory portion of the paper is could be improved for clarity (see above).\n\nThe experimental portion of the paper is reasonably clear, though could also be improved. I recommend changing the colormaps on the bottom row of Figure 6 to match the colormap of the original network CKA map shown on the rightmost panel. This would help compare the bottom row across the different CKA targets.",
            "summary_of_the_review": "Overall, I find myself on the fence. I think in its current form the paper doesn't yet meet the bar for acceptance, but I am open to raising my score if the authors provide further empirical detail on Figure 2A (see comments above), and if they are able to revise their theory section to impart more intuitive insight. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper948/Reviewer_UWFM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper948/Reviewer_UWFM"
        ]
    },
    {
        "id": "fpyQdF-9fjY",
        "original": null,
        "number": 3,
        "cdate": 1666725554385,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725554385,
        "tmdate": 1666725554385,
        "tddate": null,
        "forum": "8HRvyxc606",
        "replyto": "8HRvyxc606",
        "invitation": "ICLR.cc/2023/Conference/Paper948/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on understanding Centered Kernel Alignment (CKA), a common representational similarity metric. In particular, the paper shows that CKA is overly sensitive to a large class of \"simple\" representation-space transformations, which do not change the functional behavior of the trained models.  The theoretical analysis shows that CKA is sensitive to subset translation---a transformation that shifts a subset of data points in representation space. This finding sheds light on why CKA is empirically sensitive to outliers (\"dominant data points\") and to transformations that preserve linear separability.  The paper also shows that one can explicitly optimize the CKA map to match a comical target map without really changing the model behavior (e.g. accuracy) by much. They also show that the block structure phenomenon can be reproduced with this explicit optimization approach. Taken together, these results show that the CKA is unreliable and can easily manipulated without substantial changes to the functional behavior of the models",
            "strength_and_weaknesses": "Strengths \n\n- The theoretical analysis on subset translations is insightful because it sheds light on why CKA is empirically sensitive to outliers (\"dominant data points\") and to transformations that preserve linear separability. \n\n- The empirical results on explicitly optimizing CKA maps show that CKA is overly sensitive and may not be a useful metric to draw conclusions about general model behavior. \n\n- The empirical findings cast doubt on the \"block structure phenomenon\" that is considered to distinguish representations of wide and narrow networks. \n\n\n Weaknesses:\n\n- Specifics of Theorem 1 is a little hard to interpret.   A more intuitive description of these quantities after stating the theorem (e.g., eigenvalues of cov(X), rho, dim_PR etc) is missing. \n\n- I think section 4.1 needs more work, or can be removed entirely. I am not sure how it relates to subset translation (the class of transformations studied in this paper) and the comparison is weak---comparing weight matrices visually does not make sense (even it \"looks different\")",
            "clarity,_quality,_novelty_and_reproducibility": "Please see other sections of the review. ",
            "summary_of_the_review": "I vote to accept this paper. The paper is easy to read and well organized. The paper focuses on an interesting problem and shows that the CKA metric is not a good proxy for functional behavior of models. Most findings in the paper are original/novel and the practical implications of the theoretical analysis is insightful. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper948/Reviewer_U5e1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper948/Reviewer_U5e1"
        ]
    },
    {
        "id": "pkQxv9J9TXw",
        "original": null,
        "number": 4,
        "cdate": 1666904188376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666904188376,
        "tmdate": 1669583132827,
        "tddate": null,
        "forum": "8HRvyxc606",
        "replyto": "8HRvyxc606",
        "invitation": "ICLR.cc/2023/Conference/Paper948/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission discusses some pitfalls of using CKA as a similarity measure. In particular, the authors show that CKA is sensitive to outliers and some transformations that preserve linear separability and margin.",
            "strength_and_weaknesses": "The most interesting point about this paper is characterizing the sensitivity of CKA to outliers. Although sensitivity of CKA to large transformation to a few data points is not surprising to me, characterization of this sensitivity can be helpful for future work.\n\nMajor comments:\n\n1. My main concern is that the submission evaluates reliability of CKA for comparing representations without a clear regard for the exact purpose for using this measure. A measure of (dis)similarity like CKA captures certain information about the representation and is invariant to other transformations. Whether an invariance is a strength or weakness for this measure depends on the context and the purpose for using the similarity measure. For example, Kornblith et al. use CKA for gaining insights about the training process of neural networks (see section 2.1 in their paper) while Ramasesh et al. use CKA to measure representation drift in the context of catastrophic forgetting. Kornblith et al. argue that a proper measure of similarity in their study needs to be invariant to rotation. In the context of Ramasesh et al., the similarity measure should not be invariant to rotation, since a rotated representation will have a different separating hyperplane and will result in forgetting if the classifier is not adjusted for this rotation. The submission would have been stronger if, before stating that a property of CKA is a weakness, it clearly and explicitly stated a context where CKA is used in previous work and discussed why this property of CKA is a weakness in this context.\n\n2. The finding that representations with different separating hyperplanes may have a CKA close to 1 is well-known. Konblith et al. already show that CKA is invariant to orthogonal transformations (which clearly can change the separating hyperplane) and, as I said above, argue that this is a strength of CKA is their context.\n\n3. Corollary 2 states that Theorem 1 holds for values of rho close to 1. In this case, Gamma will tend to infinity. Which factor in Theorem 1 will cancel out this large value of Gamma and keep CKA in [0,1]?\n\n4. It looks like the definition of S has changed in Corollary 3. In theorem 1 it is the set of unmodified data points but corollary 3 it is the set of modified data points.\n\n5. In the experiment for Figure 4a, the points from the second cube are translated in a random direction. The text claims that the cubes are still separated after this transformation. This is not true if the translation direction happens to be towards the first cube.\n\nMinor comment: Please explain in the caption for Figure 3 how the plots are generated and what the colors represent.",
            "clarity,_quality,_novelty_and_reproducibility": "See the previous section",
            "summary_of_the_review": "My main concern is that the submission mentions certain properties of CKA as weaknesses without discussing the context in which CKA is used and why these properties are weaknesses in these context. I also have some doubts about the correctness of some theoretical results and novelty of some empirical results that I elaborated on in the strengths and weaknesses section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper948/Reviewer_YJ8i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper948/Reviewer_YJ8i"
        ]
    }
]