[
    {
        "id": "yDsQ796QNeJ",
        "original": null,
        "number": 1,
        "cdate": 1666672533502,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672533502,
        "tmdate": 1666672533502,
        "tddate": null,
        "forum": "U_T8-5hClV",
        "replyto": "U_T8-5hClV",
        "invitation": "ICLR.cc/2023/Conference/Paper4084/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the primal-dual framework to derive popular self-attention mechanisms. The framework bases on a support vector expansion by solving a support vector regression problem. The paper shows that several popular self-attention mechanisms can be derived under the framework. It also invents two new self-attention mechanisms by employing the batch normalization and different amounts of training data, respectively. Experiments demonstrate the advantages of the proposed two self-attention mechanisms by comparing with the softmax self-attention. ",
            "strength_and_weaknesses": "Strength:\nThe paper presents a framework to derive the popular self-attention mechanisms, which is based on support vector expansion. Then one may develop self-attention mechanisms that do not rely on heuristics and experience. Notably, the proposed framework could derive many popular self-attention mechanisms. Also, it invents two new mechanisms. These demonstrate the effectiveness of the proposed framework.\n\nWeaknesses:\nThe main weakness is that the proposed framework lacks practical and theoretical guidance. \nFirst, what is the motivation of the used function in (4)? Why is the function \\Phi(x) divided by the vector-scalar function h(x)? \nSecond, are there any rules for choosing \\Phi(x) and h(x)? Can any function be used as \\Phi(x) or h(x)? The invented two new self-attention mechanisms still from the existing experiences.\nThird, what makes one self-attention mechanism performs better? Could this be explained by the support vector expansion?\nForth, the paper shows that some self-attention mechanisms can be combined together. Can the proposed framework derive these combinations?\n\nOther comments:\nIt is better to include the linear, sparse, multi-head self-attentions in all the comparisons. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. It is easy to understand the contributions of the paper. Based on support vector expansion, the proposed framework could derive many popular self-attention mechanisms. To further demonstrate the effectiveness, the paper develops two new self-attention mechanisms, which perform better than the softmax self-attention mechanism. However, the theoretical guidance of the proposed framework needs more elaborations. ",
            "summary_of_the_review": "The paper presents a framework to derive some popular self-attention mechanisms. It also invents two new self-attention mechanisms, which outperform the softmax self-attention. These two contributions are novel and well-supported. However, the proposed framework lacks practical and theoretical guidance, which needs to be further strengthened.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper does not involve ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_ZNRr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_ZNRr"
        ]
    },
    {
        "id": "fM_uQXKSZRR",
        "original": null,
        "number": 2,
        "cdate": 1666715238873,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666715238873,
        "tmdate": 1669343645664,
        "tddate": null,
        "forum": "U_T8-5hClV",
        "replyto": "U_T8-5hClV",
        "invitation": "ICLR.cc/2023/Conference/Paper4084/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a primal-dual interpretation between neural network layers and self-attention through support vector expansion from a Support Vector Regression (SVR) problem. Based on this insight, the paper derives well-known attention mechanisms such as linear, sparse, and multi-head attention. The paper also proposes two new attention methods, 1) Batch Normalized Attention (Attention-BN) and 2) Attention with Scaled Head (Attention-SH), leveraging the SVR interpretation. Experiments on UEA Time-Series, Long Range Arena, and ImageNet benchmarks show that the proposed methods show improvements in performance as well as efficiency compared to the baseline softmax attention.",
            "strength_and_weaknesses": "### Strengths\n\n- The paper presents an interesting discovery that connects neural network layers and self-attention via a primal-dual formulation of the SVR problem.\n- A wide range of experimentation has been conducted across multiple well-known benchmarks, together covering various input modalities such as time-series and images. \n- The advantage of proposed methods is also supported with comprehensive analyses measuring the empirical efficiency as well as redundancy across attention heads.\n\n### Weaknesses\n\n- The motivation is a bit unclear: the introduction mentions that \"most attention layers are developed based on intuitions and heuristic approaches\", but the SVR formulation and presented experiments do not provide much further insight, as to whether utilizing the dual-formulation of a neural network layer can bring significant benefit towards better attention-based model diversity and/or performance.\n- Related to the previous comment, the new attention mechanisms, Attention-BN and Attention-SH, somewhat overlap with existing methods to be considered empirically novel. For instance, Attention-SH with sequence subsampling is similar to existing work on convolution-augmented attention [1,2]. Further utilizing the different design choices in the SVR formulation (e.g. choosing a different normalization $h(x)$) could bring a larger impact and increase the value of the primal-dual interpretation.\n- Experimental results are slightly lacking as it does not show results across all 30 and 5 tasks available in UEA Time-Series and Long Range Arena benchmarks, respectively. Is there a specific reason why experiments were run on subsets from the two benchmarks instead of the entire set of tasks? If not, could the comprehensive results from all tasks be shared as well?\n\n[1] Gulati et al., 2020. Conformer: Convolution-augmented Transformer for Speech Recognition.  \n[2] Wu et al., 2021. CvT: Introducing Convolutions to Vision Transformers.\n\n\n### Other questions/comments\n\n- What are the hyperparameter setups tested for 1) the mean weighting factor $\\beta$ and 2) downsampling scales for Attention-SH/SH+BN? For better reproducibility, what were the actual values used for the results presented?\n- Is there a reason behind having the weighting factor $\\beta$ as a constant given as hyperparameter? In order to have Attention-BN/BN+SH \"flexibly adjust the effect of normalization\", letting $\\beta$ be a trainable parameter similar to the scale and shift parameters in BatchNorm seems more suitable than fixing it as constant.\n- Given that all experimental results are averaged over 5 runs with different seeds, could the standard deviations be added in Tables 1, 2, and 3?\n- How is the head redundancy measured with average $\\mathcal{L}_2$-distance? What do the \"distances between heads\" exactly measure? \n- In Equation 12, gathering all the $D^{1/4}$ terms in the numerator results in $D^{j/4}$, which doesn't lead to the $\\sqrt{D}$ term in Equation 14. Is there an additional step that is missing?\n- Duplicate indexing by $j$ in Equation 11.\n- Typo in Remark 1: \"As a results\" &rarr; \"As a result\"\n- Typo after Equation 24: \"... less and vice versus.\" &rarr; \"... less and vice versa.\"\n- Typo in Table 1 (Attention-BN+SH on HeartBeat): Should it be 76.26 instead of 72.26?\n- Typo in Section 4: \"... more efficiency than the baseline.\" &rarr; \"... more efficient than the baseline.\"\n- Typo in Appendix A: \"... recentering keys and values alone ...\" &rarr; Should it be \"... recentering queries and keys alone ...\"?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is great, with clear organization and well-written mathematical derivations. The technical derivation from a SVR model to self-attention seems novel, but the motivation and significance of this derivation is unclear at the moment. Specifically, the two newly proposed attention mechanisms derived from the primal-dual framework does not seem novel compared to existing work. Reproducibility is good as experimental code has been shared alongside the main draft. Adding information on the hyperparameter tuning procedure (as mentioned in the previous block) would make it even better.",
            "summary_of_the_review": "The paper proposes an interesting direction of interpreting self-attention as a dual formulation of the SVR problem on a neural network layer. However, the newly proposed attention mechanisms as well as presented experiments are not convincing enough to fully assert the significance of the primal-dual framework.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_2btF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_2btF"
        ]
    },
    {
        "id": "lbsW1-eAupq",
        "original": null,
        "number": 3,
        "cdate": 1667059263156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667059263156,
        "tmdate": 1667059263156,
        "tddate": null,
        "forum": "U_T8-5hClV",
        "replyto": "U_T8-5hClV",
        "invitation": "ICLR.cc/2023/Conference/Paper4084/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper first proves that the self-attention mechanism in neural networks is a special form of support vector regression. Based on this conclusion, the authors proposed a principle to develop attention and designed the Attention-BN and the Attention-SH for improving accuracy and efficiency. Experimental results demonstrated that their methods have better performance than conventional attention methods.",
            "strength_and_weaknesses": "Strength: This paper gave detailed mathematical proof of the relation between the attention mechanism and the support vector regression, which inspired me a lot because most previous attention methods were designed based on intuition and experiences. This paper also proposed a framework for how to develop attention that is meaningful for future research.\n\nWeaknesses: The motivation and intuitive interpretation of Attention-BN and Attention-SH should be discussed more. I can not fully understand what inspired you to design these two novel attention methods. More insightful discussion can be provided. And one thing comes to my mind, can these two attention methods benefit some practical vision problems, such as semantic segmentation?  Can the proposed framework help develop some CNN-related attention modules, such as CBAM?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written,  and the mathematic interpretation is of high quality. The introduction can be longer to discuss more motivation of this work.\n",
            "summary_of_the_review": "This paper gives me a new perspective to understand attention. This research is meaningful to deep learning, and my score is 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_fLYg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_fLYg"
        ]
    },
    {
        "id": "Hgpa_EzWZL",
        "original": null,
        "number": 4,
        "cdate": 1667272989236,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667272989236,
        "tmdate": 1667272989236,
        "tddate": null,
        "forum": "U_T8-5hClV",
        "replyto": "U_T8-5hClV",
        "invitation": "ICLR.cc/2023/Conference/Paper4084/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides a primal-dual formulation to derive self-attention as a solution to the support vector regression problem having the primal formulation in the form of a neural network layer. The formulation allows the authors to derive various types of attention: linear, softmax, sparse, and multi-head attention using different design choices/ constraints. Two new attention mechanisms are then proposed in the paper. Performance is evaluated on 3 benchmarks with small (seemingly statistically significant) improvements in accuracy and impressive improvements in efficiency.\n",
            "strength_and_weaknesses": "## Strengths\n- The paper makes novel connections between attention mechanisms in DNNs and SVR - support vector regression. It shows that formulating the network layer as a solution to the ridge regression convex optimization problem, the support vector expansion has the form of self-attention.\n- It shows that several different attention types can be derived from different design choices in the SVR formulation thus unifying the approach to designing different types of attention.\n- It then proposes two new attention mechanisms - one inspired by batch normalization, and another with scaled (multiresolution) attention heads.\n- The paper has a potential for high technical impact. \n- It evaluates the performance of the new attention mechanisms (and a combined one) on three benchmarks, demonstrating improvement in both accuracy and performance (efficiency). The performance evaluation is convincing. \n\n## Weaknesses\n- __(W.1)__ While it is clear that different design choices lead to different kinds of attention, the paper doesn\u2019t provide intuition or meta-level criteria that could be used to make these design choices. It would be great if the authors provide a discussion of these.\n- __(W.2.1)__ The paper mentions that the results are averaged over 5 runs. It is recommended that the spread around the mean is also added to the performance tables to allow the assessment of statistical significance of the gain in accuracy.\n- __(W.2.2) UEA TMC__: In Table 1, (a) Most results (~50%) are within one percent of each other. Is this statistically significant? Kindly explain using the spread over 5/ 10 runs. (b) For the HeartBeat dataset, the Attention-BN+SH model is not only significantly worse than the Softmax attention baseline but also Attention-BN and Attention-SH which are both better than the baseline. Kindly explain the anomaly.\n- __(W.2.3) LRA__: In Table 2,  only results on the Retrieval dataset seem to be significantly better (I\u2019m assuming significant if performance improvement is > 1%). Kindly explain the results using the spread over 5/ 10 runs. \n- __(W.2.4) ImageNet__: In Table 1, (a) Results are within one percent of each other. Is this statistically significant? Kindly explain using the spread over 5/ 10 runs. (b) For Top-5 accuracy, the Attention-BN+SH model is not only significantly worse (almost 20% in absolute terms) than the Softmax attention baseline but also Attention-BN and Attention-SH. Kindly explain the anomaly.\n- __(W.3) Combining Attentions__: Explain the intuition behind combining Attention-BN+SH with Linear Attention and why it\u2019s expected to perform better.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The paper is written well and is easy to understand.\n\n**Quality**: The quality of the paper is high. There is a potential for high impact. \n\n**Novelty**: I score the paper high on technical novelty and technical significance. The empirical performance is convincing but currently, only marginally significant. \n\n**Reproducibility**:  Should be reproducible.\n",
            "summary_of_the_review": "The paper self-attention as a solution to the SVR problem which unifies several attention mechanisms in a single optimization framework. The contribution is novel and has a potential for high technical impact. Two new types of attention are proposed using this framework along with combined attention which shows modest improvement in accuracy but with significant efficiency gains. The quality of submission is good. Overall, it is a good contribution to the field.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_W46E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_W46E"
        ]
    },
    {
        "id": "uPFdNFNsbY",
        "original": null,
        "number": 5,
        "cdate": 1667772270210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667772270210,
        "tmdate": 1669333527709,
        "tddate": null,
        "forum": "U_T8-5hClV",
        "replyto": "U_T8-5hClV",
        "invitation": "ICLR.cc/2023/Conference/Paper4084/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper shows that self-attention mechanisms can be derived by finding the support vector expansion of a support vector regression (SVR) problem. The key contribution of this paper is the framework proposed in Section 2, which links attention mechanisms to support vector regression problems, and provides a different way of deriving attention mechanisms. The paper re-derives softmax, sparse, and multihead attention mechanisms using their proposed framework, and derives two new attention mechanisms: batch normalized and multiresolution head attention.",
            "strength_and_weaknesses": "This paper has several strengths:\n\n1. It is very well written: The problem is well motivated, the background on self attention is useful for optimization researchers who are not familiar with transformers, and the notation and theoretical results are clearly described.\n\n2. The problem considered is important: Since self attention plays such an important role in the performance of transformers, new and more principled ways to discover better attention mechanisms will further improve this performance.\n\n3. The theoretical results are novel: I am not aware of the connection between support vector expansions and transformers derived in this paper, and I believe this is a clever observation that is backed up theoretically.\n\nHowever, it has a few weaknesses:\n\n1. Is the proposed framework really principled? Deriving new attention mechanisms using the proposed framework involves changing the the SVR problem and the primal neural network layer. However, *how* to change the SVR problem and the primal neural network layers still appears to involve intuition and heuristics. In Section 2.3, the paper derives 2 new attention mechanisms, but there is nothing to foretell which of these attention mechanisms is superior to the other (without empirical evaluation). As such, I am not sure whether the proposed framework (in its current form) is really more principled than current practice.\n\n2. The empirical evaluation of the new attention mechanisms is not conclusive: Based on the results in Table 1, it is unclear whether Attention-BN or Attention-BH is to be preferred overall, which undermines the goal of this paper of providing a principled way to design attention mechanisms. The baselines are also fairly sparse (just one attention mechanism) and arbitrary subsets of public datasets were used for evaluation (without justification).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the theoretical results are novel to the best of my knowledge.",
            "summary_of_the_review": "Overall, I believe this paper proposes a new \"interpretation\" of self-attention by theoretically linking it to support vector expansion, and provides empirical evidence that new attention mechanisms derived using this link perform well. However, I believe the proposed framework still involves the usage of heuristics and intuition to derive new attention mechanisms. Nevertheless, the framework paves the way towards a more principled way of deriving new self-attention mechanisms in the future.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_ipYM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4084/Reviewer_ipYM"
        ]
    }
]