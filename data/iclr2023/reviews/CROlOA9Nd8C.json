[
    {
        "id": "qjIfgE202ah",
        "original": null,
        "number": 1,
        "cdate": 1666562294471,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666562294471,
        "tmdate": 1666669774362,
        "tddate": null,
        "forum": "CROlOA9Nd8C",
        "replyto": "CROlOA9Nd8C",
        "invitation": "ICLR.cc/2023/Conference/Paper4333/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a retrieval-based LM that can generate phrases. Their base model is a standard transformer, and the retrieval component uses BERT. Their model is trained using in-batch positives / negatives. Retrieval is actually done at document-level, then phrase extracted using a segmentation algorithm. For evaluation, rather than focus solely on language modeling, they evaluate in a text generation setting. They primarily evaluate with MAUVE but also report other metrics useful for analysis. The results are strong compared to baselines, but text generation is very hard to evaluate and I have some questions / suggestions about this.",
            "strength_and_weaknesses": "# Strengths\n\n- Strong results using MAUVE in all settings (although I find it hard to interpret MAUVE here, and there should be more effort to help readers calibrate their understanding of the reported metrics).\n\n- The method can generate phrases instead of only single words (although I have questions about efficiency).\n\n- The retrieval component uses rich context information.\n\n# Weaknesses\n\n- It is not ideal that COPY uses BERT for retriever and kNN-LM uses vanilla transformer trained from scratch. To me this is a major confounder, and maybe having a strong retrieval component (i.e. BERT) is more important than using phrase-based vocab.\n\n- The rep-N metric is very strict (measures contiguous duplicate n-grams), and perhaps is useful for debugging, but I imagine it does not tell use much about generation quality. It would be more useful to include distribution novel n-grams with respect to ground truth. Can review RAVEN paper for guidance (McCoy et al., 2021. How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN).\n\n- The results are very reliant on the MAUVE metric, but for wikipedia text, factual correctness is very important and I am not sure how will this is reflected by MAUVE. Although there is much discussion about perplexity being a poor measure of text generation, here, I think it would have been helpful since it could reflect factual correctness to some degree. Unfortunately, authors mentioned that PPL is not reliable because of the phrase-based vocabulary --- I wonder if it would be possible to still compute PPL in some settings if not all (certainly the baselines support PPL at least). In any case, some human eval may be appropriate here so we can better calibrate the interpretation of MAUVE in this setting.\n\n- It would help considerably to have other baselines. This might alleviate concerns about MAUVE too.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and the idea is clear. I am not sure of other methods that so simply enables phrase-based vocabulary. I have some questions about method and efficiency. My major concerns are about evaluation with MAUVE which I mentioned as weaknesses.\n\nIs phrase encoder updated every batch?\n\nHow large is the \"effective\" vocab during training vs test time? I suppose this is function of the number of retrieved documents, k = 50.\n\nHow do you handle duplicate phrases in the softmax?\n\nI am surprised to hear COPY is faster than vanilla transformer. Sure, can generate more tokens at once, but don't these tokens still need to be processed by the LM after generation? Also, I imagine the size of the softmax is much larger in the COPY approach, although I am not sure. I suppose with K = 50 documents, if there are only a few thousand phrases added to the vocab then maybe it is not such a problem. Along these lines, what size is the word-level vocab used here?\n\nHow is it possible to retrieve phrases from local context (like in Fig 2, \"Boulter is an\")? This was not explained in the main text AFAICT.\n\nOne minor comment about the intro... It should be more clear in the intro that Wikitext-103 is not being used in the standard way, and in fact, is not being used as a language modeling benchmark since there is no perplexity eval.\n\n\"average inference time cost\" --- is it average over words in the test set?\n\ntypo: \"We denote all the e\"\n\ntypo: \"i is the maximum valid value\", I think \"i\" is not supposed to be the max value here.\n\nnit: Can be confusing that top-k is used in so many different ways. Maybe k with different subscripts will help?",
            "summary_of_the_review": "The work is very interesting and relevant to the community. The main issue is the authors have taken a language modeling task for Wikipedia and converted it into an unconditional text generation task. In unconditional text generation, there are many valid and diverse continuations of text. But in Wikipedia especially I believe that factual information is important to get right, making it hard to interpret some of the evaluation. Human evaluation is one way to immediately address this concern, but perhaps other evaluations besides MAUVE could be useful too. I mentioned perplexity, but maybe BLEU with small sliding window could help understand factual correctness of the model AND leverage the phrase-based aspect.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4333/Reviewer_pM7y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4333/Reviewer_pM7y"
        ]
    },
    {
        "id": "s_DCrayTGWX",
        "original": null,
        "number": 2,
        "cdate": 1666704032714,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704032714,
        "tmdate": 1666704032714,
        "tddate": null,
        "forum": "CROlOA9Nd8C",
        "replyto": "CROlOA9Nd8C",
        "invitation": "ICLR.cc/2023/Conference/Paper4333/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The proposed Copy-Generator architecture is an attempt to depart from the classical probabilistic token-by-token language modelling perspective for language generation. Instead, generation is framed as gluing together phrases that are retrieved from a collection of documents. Searching for phrases at inference time is implemented in a heuristic coarse-to-fine-grained fashion to reduce the computational cost. The issue of computing a partition function over a potentially ginormous set of phrases is side-stepped by using an NCE loss in training, and either using greedy decoding or nucleus sampling (normalization over top-k items) for inference.",
            "strength_and_weaknesses": "I think that the idea is a reasonable follow-up to the kNN-LM approach of Khandelwal et al. The ability of adding in-domain data without retraining is very appealing. But there are some practical disadvantages compared to vanilla LMs that are brushed over to some extent. First, Copy-Generator does not define a probability distribution over a fixed vocabulary (as the authors admit in Sec. 4.3), and that limits its applicability to pure generation - it does not work in combination with other models (e.g. Bayesian, (re-)scoring...). More importantly, the idea of retrieving phrases from the training data contradicts the trend towards using bigger and bigger training sets. In fact, while latency is reported on Wiki-103 (which has 100M tokens, so rather small by today's standards), it is not reported on the En-Wiki corpus. But even En-Wiki with 3B tokens is not outrageously large nowadays.\n\nA potential way around this latency concern would be a separate treatment of the training set and the document collection used at inference time. A good data point would be the En-Wiki Copy-Generator with an inference-time document collection trimmed down to match the latency of the baselines. More details about the sensitivity regarding the size/domain of the inference-time phrase table would be interesting. I think the note made in Sec. 5.3 \"we can use learn COPY-GENERATOR with a smaller corpus but leverage\nadditional information in a larger corpus in a plug-and-play fashion\" misses the point: we want to train with a large corpus, and plug in a small corpus at for inference. \n\nA better sense for the importance of the phrase table would also be provided by reporting Wiki-103 performance of the Copy-Generator with Law-MT phrase table. Is Copy-Generator more/less prone to catastrophic forgetting?\n\nMinor comments:\n- It is nice to know that the average length of phrases is 3.95, but more details (variance, full histogram..) would be useful.\n- Phrase extraction by segmentation seems limiting as it does not allow any overlap between phrases.\n- Why is the Copy-Generator latency for nucleus smaller than for greedy\n- Abstract: coping -> copying \n- Sec. 3 \"We denote all the e all the phrases\"\n- Sec. 3 \"all phrases are context-sensitive\" clarify that they depend on the source document context, not on the target prefix context at inference time.\n- Sec 4.2: \"does not suit work well\"",
            "clarity,_quality,_novelty_and_reproducibility": "Novel architecture, good and clear writing style.\n",
            "summary_of_the_review": "The idea of tackling neural language generation on the phrase-level is neat, but the practical limitations (latency, sensitivity regarding the size and content of phrase table) are not fleshed out enough in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4333/Reviewer_LpvV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4333/Reviewer_LpvV"
        ]
    },
    {
        "id": "W6mCzW4inK",
        "original": null,
        "number": 3,
        "cdate": 1666931841878,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666931841878,
        "tmdate": 1666931841878,
        "tddate": null,
        "forum": "CROlOA9Nd8C",
        "replyto": "CROlOA9Nd8C",
        "invitation": "ICLR.cc/2023/Conference/Paper4333/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper formulates text generation as copying from existing text segments. The authors decompose text generation as a series of copy-and-paste operations by seeking text spans from existing articles and copying them as the generation result at each step. Experimental results on WikiText-103 show that the proposed method can achieve higher MAUVE scores and also support effective domain adaption and better inference efficiency.",
            "strength_and_weaknesses": "Strengths:\n\n1) Reformulating text generation as a series of actions that fully depend on copying is an interesting idea, which may work well in some NLG scenarios.\n\n2) From the experimental results, copy-generator can achieve better generation quality while having lower latency compared with Transformer-based baselines, which shows its effectiveness.\n\n3) This paper is well-organized and easy to follow.\n\nWeaknesses:\n\n1) The idea which combines vanilla token-level generation and phrase copying has been already discussed in the existing works such as [1]. In my view, the main difference only falls into the format of knowledge sources (unstructured documents in this paper and structured knowledge in [1]). Thus, the authors should further clarify the difference between copy-generator and existing works to show their novelty.\n\n2) The metrics used in the experiment are somewhat questionable. First, this paper lacks the fluency metric such as perplexity. From my experience, if copy operations dominate the generation process compared with token-level generation, the fluency of the whole sentence may be degraded. Then, MAUVE as a distribution-based metric may be biased to evaluate copy-generator because copy-generator largely copies source texts. If the distributions of source texts and the test set are similar enough, it\u2019s not surprising that copy-generator can achieve a high MAUVE score.\n\n3) The latest baseline selected in this paper is in 2020. The authors should add state-of-the-art baselines about retrieval-augmented language models such as [2].\n\n4) Since the experiment is mainly conducted on WikiText-103, I wonder whether there is a principle to collect the source texts for a specific downstream task / dataset, such as machine translation, text summarization or dialogue generation. It\u2019s important because the generation quality of copy-generator largely depend on the quality of source texts.\n\n5) Typo: \u201call the e\u201d in the second paragraph of Section 3 should be removed.\n\n[1] Latent Relation Language Models. AAAI 2020.\n\n[2] GNN-LM: Language Modeling based on Global Contexts via GNN. ICLR 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of this paper is OK. But the authors should further clarify the differences between the proposed method and existing works to highlight their novelty. The reproducibility may be degraded due to the lack of codes.",
            "summary_of_the_review": "Although generating texts via a series of copy operations is interesting, I\u2019d recommend the authors to solve the concerns about the novelty and experimental settings before making this paper ready for publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4333/Reviewer_8gPK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4333/Reviewer_8gPK"
        ]
    },
    {
        "id": "EHkxFkQa83",
        "original": null,
        "number": 4,
        "cdate": 1667074806714,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667074806714,
        "tmdate": 1667074806714,
        "tddate": null,
        "forum": "CROlOA9Nd8C",
        "replyto": "CROlOA9Nd8C",
        "invitation": "ICLR.cc/2023/Conference/Paper4333/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a language model that can autoregressively choose tokens from a fixed vocabulary, or multi-token phrases from a pre-encoded corpus. The paper builds on previous work on phrase encoding for question answering, but extends this work to freeform text generation. The model (Copy-generator) is evaluated on three separate language modeling tasks. Two that use Wikipedia text and one that involves domain transfer to a legal domain. When compared to a vanilla LM and a KNN-LM that retrieves memories of individual tokens, Copy-generator performs better according to the recently proposed MAUVE metric. It does less well, in comparison to KNN-LM, according to diversity and Rep-N metrics on some tasks, though. Traditional perplexity scores are not particularly meaningful here because the model can choose different segmentations of the text than those used by the baselines.\n\nThe phrase encoder is trained along with the language model in the following fashion. First a greedy segmentation algorithm is run to choose a single segmentation of each training example into phrases that exist elsewhere in the training corpus. Then, for each phrase, a piplined retrieval system is run. First a document retrieval system retrieves a small set of documents that are similar to the prefix. Then, a phrase search is applied over all phrases in this retrieved set of documents. Phrases are represented using encodings of their start and end position, following previous work on phrase retrieval. The phrase retrieval model and language model are trained using a combination of InfoNCE loss over phrase candiates, as well as language model loss. The document retrieval model is fixed (as far as I can make out).\n\nAt inference time, phrases or individual tokens are selected from the union of all phrases in a large retrieval corpus and the fixed length vocabulary. Examples show that Copy-generator often chooses to select long phrases instead of individual tokens (I would like to also see statistics illustrating how often different length n-grams occur), and the generated text is deemed significantly better than the baseline systems according to the automatic MAUVE metric, which has been trained to correlate with human judgements. An analysis of performance shows that quality increases with the number of documents in the phrase corpus, and this increase is particularly pronounced when greedy search is used during decoding. \n",
            "strength_and_weaknesses": "#### Strengths\n- This is a novel and interesting architecture that is significantly different to previous work.\n- Copy generator performs well in comparison to reasonable baselines.\n- While the model is only applied to language modeling tasks, I can imagine extensions to other forms of generation where retrieving and copying text is likely to be a good strategy (e.g. multi-document summarization).\n\n#### Weaknesses\n- The paper relies heavily on the recently proposed MAUVE metric. It would be great to also have some human judgement of quality, no matter how small, to get more intuitions of where Copy-generator does better or worse than the baselines.\n- Copy-generator makes use of context-dependent phrase encodings. However, the examples in Figures 2 and 5-8 seem to show that large phrases are being drawn from very different contexts to the target context. It'd be good to see some analysis of how important the is here.\n- It would be interesting to see a comparison to retrieval augmented language models (e.g. RETRO, ATLAS) which also include document retrieval at inference time (but also have a greater inference-time computational cost than copy-generator).\n",
            "clarity,_quality,_novelty_and_reproducibility": "#### Clarity\nIf I understand this paper properly, the pipelined approach presented in Section 4.2 as 'implementation details' are essential during both inference and training. In particular, I believe that the BERT phrase encoder parameters are only ever updated for the small set of retrieved documents, and there is no need to re-index the phrase table during training (the phrase representation is operating more like a re-ranker). If my understanding is correct, then I suggest this detail is moved out from a paragraph focusing on 'inference efficiency' into the description of the core approach. Without this detail, the description of the training procedure is very hard to understand.\n\nOtherwise, this paper is well written and easy to follow.\n\n#### Reproducability\nMore should be said about the construction of the InfoNCE loss. How exactly are the in-batch negatives constructed? Are these all phrases in all retrieved documents for all prefixes in the batch? Also, what happens when there are multiple separate 'correct' representations of the target phrase $p_k$ coming from different contexts in the retrieved documents?\n\n\n#### Novelty & Quality\nPhrase encoding and retrieval has been studied before, as has contextualized token representation retrieval for language modeling. But, I believe this is the first work to do phrase retrieval for language modeling, or any type of long-form generation. That seems significant.\n",
            "summary_of_the_review": "This paper presents a novel approach to language modeling, that can copy large phrases from a retrieval corpus rather than decoding them one token at a time. The performance on three language modeling datasets are positive, according to the MAUVE metric and in relation to the similar KNN-LM baseline that performs retrieval of single token encodings. It would be nice to see some more analysis of the outputs, and how the copy mechanism is being used during inference time. It also would be very interesting to see how well this model does on a wider variety of text generation tasks. However, the current paper is sufficiently novel and interesting for acceptance at ICLR.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4333/Reviewer_siHy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4333/Reviewer_siHy"
        ]
    }
]