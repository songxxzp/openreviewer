[
    {
        "id": "a-GVDvCnCu",
        "original": null,
        "number": 1,
        "cdate": 1666634108572,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634108572,
        "tmdate": 1669248659696,
        "tddate": null,
        "forum": "cJWxqmmDL2b",
        "replyto": "cJWxqmmDL2b",
        "invitation": "ICLR.cc/2023/Conference/Paper3650/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper theoretically studies the notion of neural collapse (NC) -- the penultimate layer representations of all the examples in a specific class collapse to a single representation, and separate from other classes. Specifically, this paper studies this notion under uniform label noises, and proposed a model called memorization-dilation (M-D), in which it shows labels smoothing leads to less memorization and better generalization.",
            "strength_and_weaknesses": "**Strength**\n1. This paper theoretically studied neural collapse (NC) with a definition that describes more precisely the empirically observed phenomena in practice comparing to other papers.\n2. This paper formally study the NC under label noises with a memorization-dilation model, and make connection between memorization and NC.\n3. The theoretical model is motivated from empirical observations with deep neural networks on real data.\n\n**Weakness**\n1. The theoretical setup for studying neural collapse is quite different from what a real neural network behave in practice. In particular, it assumes \"infinite expressivity\" and allow the representation of each example to be freely trainable parameters. The resulting formulation looks like a matrix factorization problem with a classification loss. I think the most interesting part of neural network representation learning (and collapsing) would mostly happen jointly in the layers below the final classifier layer, and how those layers share weights when jointly computing the representations for all the training examples. This is especially important for the topic of this paper, where label noise is introduced to study memorization. In this case, the lower layers are forced to build different representations for visually similar inputs in some cases when they are assigned different training labels. But this interactions would be completely missing in the model proposed here.\n\n2. I appreciate that the empirical studies in this paper uses deep neural networks. However, since the theoretical models are so different, I would like to see empirical studies with a similar setup, using networks with approximately infinite expressivity. Or better, simply using a model where the representations are directly optimizable free variables, and see if the empirical observations are still similar and equally well for motivating the M-D models. One question here is how to get the representations for the test examples for computing Eq (3) in this case.\n\n3. The analysis is limited to binary classification problems.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of this paper is relatively easy to follow. I do have a nitpick request: I think a more common convention is to use n, N for example indices and k, K for class indices. This paper does the opposite and confused me a number of times during the reading. ",
            "summary_of_the_review": "This paper studies the phenomenon of neural collapse (NC) under label noises using a theoretical model to characterize the relation between memorization and NC. One of my major concern is that the theoretical model is too different from a real deep neural network, which is especially crucial for the topic of this paper (label noises and memorization). \n\n-------\nPost rebuttal: thanks to the authors for clarifying my questions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3650/Reviewer_xSrW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3650/Reviewer_xSrW"
        ]
    },
    {
        "id": "gF2uYh3kpHn",
        "original": null,
        "number": 2,
        "cdate": 1666755336849,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666755336849,
        "tmdate": 1666793733155,
        "tddate": null,
        "forum": "cJWxqmmDL2b",
        "replyto": "cJWxqmmDL2b",
        "invitation": "ICLR.cc/2023/Conference/Paper3650/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the phenomenon of neural collapse (NC), where representations of multi-class examples tend to collapse to the mean representation in a structured fashion.  The authors argue that the commonly used layer peeling model for NC is overly simplistic; in particular the assumption of \"infinite expressivity\" does not hold well with networks in practice.  They argue instead for a refinement of the layer peeling model that takes the signs of the features into account, and limits the expressiveness of the model to represent transformed inputs.  They use this model to study the interplay of memorization (which they define as the deviation from the expected NC structure of data with injected label noise) and dilation (which they define as deviation of the collapsed within-class mean structure that defines neural collapse) on the generalization ability of networks trained on both cross entropy and label smoothing.  \n",
            "strength_and_weaknesses": "### Strengths\n- The authors are attempting to make the study of neural collapse applicable to more real-world models, by refining the layer-peeling model to align with the properties of many commonly used DNNs (e.g non-negativity of features).  This is important work needed to understand the training dynamics of modern networks.\n\n- Though this paper unavoidably introduces a lot of notation, the authors do a good job in guiding readers thorugh its deployment in service of their arguments.\n\n- The authors also explain the properties of NC very clearly, and do a good job of succinctly summarizing the development of tools for the study of neural network optimization.  Though I would add I think the study on memorization by Feldman and Zhang (NeurIPS 2020) merits mentioning here, as they tackle long tailed natural data distributions (which combined with the rigidity implied by NC means that memorization would be necessary to achieve low training error). \n\n### Weaknesses\n\n- In section 1, the given definition for **NC2** has a typo.  I believe it should be that the inner product between any *different* orthonormal class means (centered at $\\mathbf{h}$) approaches $- \\frac{1}{N - 1}$.  As written, it suggests that $<x,x> \\rightarrow - \\frac{1}{N - 1}$\n\n- In section 3 just after the definition of $\\mathcal{P}_{\\alpha}$, it looks as if $\\mathbf{y}_{n}^{(\\alpha)} is defined by label smoothing, but it might be good to remind readers here, as there is a lot of notation introduced in this section.\n\n- Could the authors clarify element (iiii) of definition 3.1?  Definition 3.1 by inspection seems to agree with the original definition of NC properties,  with the exception of (iii).  Perhaps to aid the reader here, each component of Definition 3.1 could be annotated with the NC property that it entails?  E.g  component (i) seems to entail NC1\n\n- The first sentence in the last paragraph of section 3 is awkward (and contains a repitition \"in in\"). In the sentence, do the authors mean that def\u2019n 3.1 (that includes condition (iii)) allows for Theorem 3.1 in Trier & Bruna 2022 to be established?   Could the authors please clarify.  I read this as suggesting \"Definition 3.1 and the orthogonal frame result in Theorem 3.1 in \u2026\"\n\n- The definition of the dilation quantity $\\mathcal{NC}_1$ could use more motivation.  Why are we examining the trace of this product?  What intuition does this give us about NC test data? Zhu et al (which the authors cite as providing the definition) explain that $\\mathcal{NC}_1$ is intended to measure the within-class variability collapse.  I presume that this is because the trace of $\\Sigma_W$ will vanish as NC takes hold, while $\\Sigma_B$ will approach a constant.  Perhaps a footnote could help guide the reader in the absence of Zhu et al\u2019s explanation? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is for the most part written quite clearly.  It requires no small effort to make work on the structure of neural network optimization accessible to a wider ML audience.  Bravo :)\n\nOn the novelty of their work, though Zhou et al (**Are All Losses Created Equal: A Neural Collapse Perspective**, to appear in NeurIPS 2022) also examine Neural Collapse through the lens of several losses including label smoothing, this is close enough to co-publishing that I'm willing to give the authors credit here.  In addition, this work takes a different basis for interrogating the effect on how label noise that perturbs NC structure affects generalization, which to my mind is a new (and worthy) line of inquiry.\n\nI have an extended note about the authors' definition of memorization (defined as equation 3 in section 4.1). This concept of memorization is not unrelated to the concept by Feldman and Zhang.  Here, you are creating elements of the long tail of an anti-causal representation by adding label noise.  Your approach seems equivalent to drawing a label, then drawing observable features for this label from among the distributions of the data from differing labels, thus creating extremely improbable observations in the tail of the conditional distribution of observable features given the label.  \n\nI would ask the authors to reconsider how they define the corrupted dataset, and ask themselves if this measure defined in (3) really measures memorization.   I can see an argument where in fact a low value for ` mem` would mean that the network has memorized that the label corrupted instances should be mapped to $\\mathbf{h}_{n}^{*}$, since this is the only way to achieve low training error.  \n\nA third consideration is that if the authors desire is to measure the effect of label corruption on the displacement of the data point from the class mean NC structure, it might be more informative to measure the relative distances between $\\mathbf{h}_{n}^{*}$ and $\\mathbf{h}_{orig}^{*}$, the mean of the true label instance.    \n\n",
            "summary_of_the_review": "The authors present a model that extends the Layer-Peeled model, introducing positivity conditions on the feature representations.  They develop the notions of memorization and dilation, which they use to explain the improved generalization of the label smoothing loss as compared to standard cross-entropy.  \n\nWhile i have some reservations about the wording of the definitions, and a more philosophical difference of opinion about memorization, I do not think either of these are serious impediments to the merit of the paper.\n\nI will say that I hope the authors do not stop at studying the effect of label noise on NC structures. If we are to consider the penalty to generalization that NC imposes on models that train until NC onset, then surely more common issues in generalization (e.g distribution shifts) should be considered as the centre of future work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3650/Reviewer_ajzH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3650/Reviewer_ajzH"
        ]
    },
    {
        "id": "8NN-arSInw",
        "original": null,
        "number": 3,
        "cdate": 1666769815452,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666769815452,
        "tmdate": 1670890476098,
        "tddate": null,
        "forum": "cJWxqmmDL2b",
        "replyto": "cJWxqmmDL2b",
        "invitation": "ICLR.cc/2023/Conference/Paper3650/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This study gives the solution structure of the label smoothing (LS) loss with positivity constraint, which is an orthogonal variant of the original neural collapse configurations. The paper further explores why LS loss has a better generalization than CE loss when label corruption emerges based on the memorization-dilation model. It reveals that LS loss induces less memorization, so leads to less dilation, which explains its better generalization.  ",
            "strength_and_weaknesses": "Strengths:\n\nThe study gives the solution structure of the LS loss, whereas previous studies mainly focus on the original CE loss. \nThe study gives an explanation of the better generalization of LS loss than CE loss under label corruption. \n\n\nWeaknesses:\n\n- Misleading description. The authors claim to address the model with noisy data. However, they actually only consider label corruption. Noisy data does not equivalently refer to label noise. A more precise description should be adopted. \n\n- The positivity constraint is confusing. Although the last layer will have positive feature when ReLU is performed, a neural network does not necessarily end with a ReLU activation in most cases. If an identity connection or a BN layer is appended, we can easily get rid of the positivity constraint. More importantly, I do not see a necessary connection between the positivity constraint and the later analysis of label corruption. Is your result (LS loss shows less memorization and dilation) valid only when the positivity constraint is accompanied? \n\n- The claim that less dilation leads to better generalization lacks rigorous support. As indicated by your definition, dilation reflects the compactness and separation of test features. But generalization ability seems to be more related to loss and accuracy on test set. So, a more rigorous relation between \u201cdilation\u201d and the \u201cgeneralization\u201d in your context should be constructed. Otherwise, the claim seems to be groundless. \n\n- The memorization-dilation model is confusing. First, it only considers two classes, which is unrealistic. Why do the authors only consider two classes? I do not think it would be a simple extension by generalizing the two-class result into multiple classes. Besides, why are H and W fixed as the optimal solution in the memorization-dilation model? Its motivation and rationality are unclear and need more discussion. \n\n- The second equation in (NC2) in page2 is wrong. \n\n- I suggest that the authors consider more cases in Theorem 3.2. It would be better if the authors could first give the solution structure of the LS loss without positivity constraint, and the CE loss with the positivity constraint, and then deal with the LS loss with positivity constraint as stated in Theorem 3.2.\n\n\n\n------ After rebuttal\n\nThe authors address most of my questions and concerns. I increase my score to 6.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The study focuses on an important problem. The result is somewhat inspiring and interesting. The clarity of this paper needs to be improved, especially for the structure. ",
            "summary_of_the_review": "The paper suffers from some issues, as listed in the weaknesses. Most importantly, the main claim lacks rigorous supports. Some settings in the model are confusing. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3650/Reviewer_ww8W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3650/Reviewer_ww8W"
        ]
    },
    {
        "id": "egvPmzCA1h",
        "original": null,
        "number": 4,
        "cdate": 1667142687879,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667142687879,
        "tmdate": 1667142687879,
        "tddate": null,
        "forum": "cJWxqmmDL2b",
        "replyto": "cJWxqmmDL2b",
        "invitation": "ICLR.cc/2023/Conference/Paper3650/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study the phenomenon of neural collapse (NC) under several variants of the layer-peeled model.  Since features from modern networks are the outcome of some non-negative activation functions, such as ReLU, the paper first considers the case of non-negative features and shows that label smoothing also produces NC solutions in this case. The authors then propose a new memorization-dilation model on test data with randomly label-corrupted training samples, which shows a linear relation between NC degree (dilation) and overall distance between test samples and their corresponding class-mean (memorization).  Finally, in the label-corrupted setting, they formally prove the advantage of label smoothing over cross-entropy in binary classification tasks, and show some supporting experiments.",
            "strength_and_weaknesses": "## Strength:\n- This paper formally shows NC solutions for label smoothing and cross-entropy (CE) under nonnegative features. Nonnegative features are only considered for MSE loss in the previous work. \n- An interesting label-corrupted experiment is proposed, and the linear relation between dilation and memorization when corrupted levels are not large is very interesting. \n- The proposed memorization-dilation model may provide further insight into the connection between neural collapse and generalization.\n\n## Weakness:\n- As the nonnegative features model has already been studied in Tirer & Bruna (2022), though for the MSE loss, the authors may highlight the technical challenges in extending the results to label smoothing. \n- This paper shows that on training data with label noise, label smoothing could be more robust to label noise and achieve better performance on the original testing data. Based on this, the paper claims in many places that label smoothing leads to improved generalization in classification tasks. How does this better performance under label noise translate to better generalization in the standard case without label noise? \n- Following the above point, the recent work [A] shows that label smoothing and CE indeed produce similar performance when the network is sufficiently large and trained sufficiently long in the standard way without label noise. Since label smoothing and CE produce similar NC features under the unconstrained feature models, is the better performance of label smoothing on label noise because it has a different convergence speed than CE? It will be of interest to perform experiments with more iterations and see whether the results are the same. \n- It is interesting to note that MSE has better testing performance in Figure 1. Could the authors provide some comments on this? \n- Does the memorization-dilation model only consider one mislabeled training sample per class?\n- Could the memorization-dilation model be extended to the multi-class case?  \n\n[A] Zhou et al., Are All Losses Created Equal: A Neural Collapse Perspective; arXiv preprint arXiv:2210.02192; 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-organized and well-written. The presentation of section 4.2, particularly Denifition 4.1, could be improved. For example, $u_1$ and $u_2$ could be introduced right before Definition 4.1. The results on nonnegative features extend previous work on MSE to label smoothing. The memorization-dilation model is new and could provide further insight into the connection between neural collapse and generalization.",
            "summary_of_the_review": "This paper is well-organized and provides interesting theoretical and empirical results on neural collapse with label smoothing. The memorization-dilation model could provide further insight into the connection between neural collapse and generalization, though it is not clear how the performance with label noise can be used to understand the generalization performance in the standard setting without label noise. I look forward to the authors\u2019 response to revise my decision. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3650/Reviewer_7FKE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3650/Reviewer_7FKE"
        ]
    }
]