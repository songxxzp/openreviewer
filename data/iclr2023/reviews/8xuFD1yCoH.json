[
    {
        "id": "gU6jTbH3gL",
        "original": null,
        "number": 1,
        "cdate": 1666240694444,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666240694444,
        "tmdate": 1666240694444,
        "tddate": null,
        "forum": "8xuFD1yCoH",
        "replyto": "8xuFD1yCoH",
        "invitation": "ICLR.cc/2023/Conference/Paper3353/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a curriculum learning strategy, named TUNEUP for GNN, which aims to improve the performance of GNNs at the tail nodes. TUNEUP first trains a GNN that produces good performance on head nodes, then synthesizes tail nodes by dropping the edges of head nodes, and then fine-tunes on these synthesized tail nodes. The authors conduct three types of experiments: semi-supervised node classification, link prediction and recommender systems. Extensive experiments demonstrate the effectiveness of this method.",
            "strength_and_weaknesses": "Strength:\n1. The authors performed three different types of GNN applications, all with improved results.\n2. It is significant to improve the performance of the tail node.\n\nWeakness:\n\nThe author's approach is based on three very strong assumptions, which I disagree with, as follows:\n1. The first one is \"Tail nodes can be synthesized by dropping edges from head nodes\".\nWhy the DropEdge can synthesize the tail node from the head node?\nI think in most cases\uff0c dropping edges from head nodes would give synthetic tail nodes that have different characteristics from real tail nodes\nThe authors did not prove this experimentally or theoretically, so I don't think this assumption is convincing enough.\n\n2. The second one is \"Target supervision on head nodes can be reused for synthetic tail nodes\".\nThe author points out that the degree of a node is not related to its properties, which I think is incorrect.\nIn most cases, the degree of a node is strongly related to its own properties.\nFor example, in a social network, celebrities will have many people to follow, thus resulting in a high degree.\nI think the authors should also give explanations from an experimental or theoretical point of view.\n\n3. The third one is \"Edge information helps for GNNs to make prediction\".\nIn many cases, more edges are not always better.\nReal-life graph data has many heterogeneous connections, and these edges can lead to performance degradation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is built on three suspicious assumptions, please refer to \u201cweakness\u201d above.",
            "summary_of_the_review": "I think the authors should give theoretical or experimental explanations and clarifications for these three assumptions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3353/Reviewer_1BhX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3353/Reviewer_1BhX"
        ]
    },
    {
        "id": "rpA8omftfk",
        "original": null,
        "number": 2,
        "cdate": 1666601327381,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601327381,
        "tmdate": 1666601327381,
        "tddate": null,
        "forum": "8xuFD1yCoH",
        "replyto": "8xuFD1yCoH",
        "invitation": "ICLR.cc/2023/Conference/Paper3353/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a two-stage curriculum learning strategy -- TuneUp --  to improve GNN's performance on tail nodes of small node degrees. The paper is motivated by the observation that GNNs perform worse on nodes with smaller degrees. \n\nIn this first stage, TuneUp conducts standard training without augmentations. In the second stage, TuneUp trains GNNs with DropEdge augmentation. The paper argues that the DropEdge augmentation in the second stage can synthesize tail nodes from head nodes, thus providing more tail nodes for training.\n\nTuneUp is evaluated in three tasks (semi-supervised node classification, link prediction, and recommendation). When the task is semi-supervised node classification, TuneUp also uses pseudo labels in the second stage of training.",
            "strength_and_weaknesses": "**Strengths:**\n\n* The paper is clearly written.\n* The studied problem of generalization to tail nodes is important for recommender systems and social network analysis.\n* The proposed model is tested on extensive benchmarks from various domains. \n\n\n\n**Weakness**:\n-  The technical novelty is weak. \n   1. The methods of DropEdge and Pseudo Labeling are well explored in previous works, as already cited in this paper. \n   2. The proposed TuneUp method is a straightforward adaptation of the \"from easy to difficulty\" idea from Curriculum Learning. \n\n- The experimental results are insignificant. \n   1. In Table1, *TuneUp w/o pseudo-labels* underperforms *DropEdge*. The result shows that the proposed two-stage training method is no better than a single-stage training with augmentation. Although using pseudo-labels can improve performance, pseudo labeling is proposed by previous works and does not align with the motivation of this paper. \n   2. In Table2, TuneUp's improvement over DropEdge is marginal. The result shows that the improvement over the base model is mainly attributed to the DropEdge augmentation rather than the proposed method.\n\n- The experimental settings need more careful treatment. \n   1. In sec4.2, the experimental setting of the link prediction task follows that of two papers from a different domain -- recommender system -- instead of link prediction. The authors are encouraged to follow the experimental setting of papers from the same domain for appropriate evaluation. \n   2. It is weird to use an ogb**n** (**n** for node) dataset for link prediction evaluation, since the ogb benchmark has multiple ogb**l** (**l** for link) datasets designed specifically for link prediction evaluation.\n   3. I also notice that the experimental settings of semi-supervised node classification and recommendation are new and different from existing works. The authors are encouraged to report the performances under the same evaluation protocols of existing works. ",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity:** The presentation is clear. \n* **Novelty:** The nolvety is weak since the employed DropEdge, Pseudo Label and Curricum Learning method are well explored in previous works. See the 1st weakness in the previous section.\n* **Reproducibility:** The reproducibility is in concern due to the abnormal experimental setting. See the 3rd weakness in the previous section.",
            "summary_of_the_review": "The paper presents a two-stage curriculum learning method to improve performance on tail nodes of small degrees. The novelty is weak and the experimental results are insignificant. The experimental settings need more careful treatments. The paper is not ready for publication at the current stage. Hence, I am leaning on the negative side.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3353/Reviewer_Yj9L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3353/Reviewer_Yj9L"
        ]
    },
    {
        "id": "fGr7rqUOs4",
        "original": null,
        "number": 3,
        "cdate": 1666670966690,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670966690,
        "tmdate": 1666670966690,
        "tddate": null,
        "forum": "8xuFD1yCoH",
        "replyto": "8xuFD1yCoH",
        "invitation": "ICLR.cc/2023/Conference/Paper3353/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a two-stage curriculum learning process for GNNs to ensure better performane in a variety of tasks. Specifically, the paper argues that low-degree nodes are the hardest to make predictions for and focus on improving this.",
            "strength_and_weaknesses": "Strengths:\n- Clear communication, easy to understand\n- Discusses the assumptions that enable the method to perform better.\n- General method, i.e. applicable in many areas\n- Good dataset selection to evaluate empirical performance\n- Covers both transductive and inductive settings, as well as cold-start nodes\n- Ablation studies done are meaningful and reinforce the points of the paper\n- Many other graph augmentation methods are compared against\n\nWeaknesses:\n- The GNN architecture used in evaluation is old and a narrow choice. The most standard choice would be an MPNN and then checking with the 3 most common aggregators, which I suspect would have a large impact on how the node degrees affect prediction. So I think the information provided by the empirical evaluation may not be useful to the community as GraphSAGE is simply not the default choice anymore. GCN is even older. The authors should rerun at least the largest dataset in each task (node classification, link prediction, recommender system) with the three MPNN (max, mean, sum aggregators).\n- The evaluation does not do enough to convince the reader that this is useful across GNN architectures, for that MPNN with different aggregators are the minimum.\n- The deviation in the evaluation protocol for node classification, while sensible, is unfortunate in that it prevents easy comparison for past, concurrent, and future work as it is not standard.\n- For link prediction there is no test set, which I find problematic and is not an acceptable protocol for a top machine learning conference.\n\nMinor:\n- The standard deviation numbers in a lot of tables (especially Table 2.) are too small to be readable, the authors would be better off reporting them in the appendix and removing them from the main paper. It harms the readability too significantly.\n\nQuestions:\n- You claim in section 2.4 that links in heterophilieous graphs would necessarily harm predictive performance in node classification. I can't imagine that is true, consider graph colouring, where adjacent nodes needs to have different colours and we are given an initial colouring for some of the nodes. This is a node classification problem in a heterophelic graph, link information is informative, in fact necessary for prediction. Am I missing something or are heterophileous graphs not a good example of the point you are trying make?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- Clear.\n\nQuality:\n- Good\n\nNovelty:\n- Small\n\nReproducibility:\n- Good",
            "summary_of_the_review": "The paper is clear and well written. The method is a small technical innovation that seems applicable across a wide variety of tasks. However, the empirical evaluation needs to be improved, in particular the GNN architectures considered as well as using a test set for link prediction.\n\nIf these concerns are addressed, I am willing to raise my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3353/Reviewer_xBXx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3353/Reviewer_xBXx"
        ]
    },
    {
        "id": "HCShzSheg1",
        "original": null,
        "number": 4,
        "cdate": 1666685571691,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685571691,
        "tmdate": 1666685571691,
        "tddate": null,
        "forum": "8xuFD1yCoH",
        "replyto": "8xuFD1yCoH",
        "invitation": "ICLR.cc/2023/Conference/Paper3353/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a training strategy for GNN, TuneUp, that applies curriculum learning with node synthesis and label generation. It is evaluated on multiple modalities and outperforms multiple data augmentation baselines.",
            "strength_and_weaknesses": "### Strengths\n1. The paper introduces a training strategy, TuneUp, that organically combines multiple data augmentation methods for GNN.\n2. The proposed TuneUp strategy is intuitive and performs well on multiple types of prediction tasks.\n3. It also considers an interesting cold-start scenario that may be useful for real applications.\n\n### Weaknesses\n1. The paper claims the TuneUp strategy can improve the performance of the GNN training. However, it is only tested on the basic GNN models, GraphSage and GCN. It would be interesting to see other GNN architectures that appear in the ogbn leaderboard to show the generality of the method.\n2. It would be interesting to see some examples of nodes and targets that are generated from the synthesis stage to give more insights into the method.\n3. The method adopts pseudo labels to improve performance. It would be also great to have some label-smoothing baselines.\n4. The paper made several assumptions for the augmentation techniques used in the algorithm. It can be more convincing to show some statistics of the graphs in different datasets, such as how many pseudo-labels are correct, and the accuracy for the head nodes vs tail nodes for the base model",
            "clarity,_quality,_novelty_and_reproducibility": "The writing conveys the ideas of the authors, but the organization of the paper is not very good. There are a few assumptions not well supported by experiments.",
            "summary_of_the_review": "The paper proposes a training strategy for GNN that combines multiple data augmentation methods together and improves the performance of two basic GNN models. It is not well-written and several assumptions are not well supported.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3353/Reviewer_khNG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3353/Reviewer_khNG"
        ]
    }
]