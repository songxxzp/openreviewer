[
    {
        "id": "DlfXgjf0Mvi",
        "original": null,
        "number": 1,
        "cdate": 1666504389476,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666504389476,
        "tmdate": 1666640385203,
        "tddate": null,
        "forum": "oze0clVGPeX",
        "replyto": "oze0clVGPeX",
        "invitation": "ICLR.cc/2023/Conference/Paper4516/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work conducted experiments on privately fine-tuning GPT-3 using group-wise gradient clipping and successfully trained a model performing well with relatively small privacy budget.",
            "strength_and_weaknesses": "The main strength is the successful training of GPT-3 to reasonable performance with relatively small privacy budget, which is achieved by device-wise gradient clipping and parameter tuning.\n\nWeakness is mainly on novelty, the work is just applying existing DP training algorithms to training GPT-3, with some hyperparameter tuning.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality is good, novelty is not strong, reproducibility is fair since code is not provided.",
            "summary_of_the_review": "I feel it is interesting to see GPT-3 can perform quite well event with privacy budget as small as 1, but I feel novelty might be a concern for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4516/Reviewer_FnUx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4516/Reviewer_FnUx"
        ]
    },
    {
        "id": "DmqjymIctnC",
        "original": null,
        "number": 2,
        "cdate": 1666626301570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626301570,
        "tmdate": 1666626301570,
        "tddate": null,
        "forum": "oze0clVGPeX",
        "replyto": "oze0clVGPeX",
        "invitation": "ICLR.cc/2023/Conference/Paper4516/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explored fine-tuning extremely large models with DP-SGD and proposed methods to reduce the memory and improve the efficiency of the per-example gradient clipping. The authors demonstrated that per-layer gradient clipping with adaptively tuned clipping bounds can be as efficient as non-private training and achieve similar utility to flat clipping. The methods were compared with prior works and evaluated on multiple standard benchmark datasets and models, and for the first time, on the GPT-3 model. \n",
            "strength_and_weaknesses": "Strengths\n\n1. For the very first time, demonstrated how to train DP-SGD on GPT-3 which is already memory consuming for non-private learning. The engineering efforts that made DP-SGD training working on such a large model are incredible. \n2. The paper is well written and the methods are clearly explained. The idea of using per-layer clipping to improve efficiency is novel and 3. not explored in prior works. The intuition of using adaptive clipping to improve the utility is also well-explained.\n3. The experiments are comprehensive and convincing. Multiple baselines are used to compare with the proposed methods.\n\nWeaknesses\n\nNot sure how easy it is to reproduce the implementation of the per-layer clipping and parallel training of the large model. There is no discussion on open sourcing but I hope the author would share their implementation for the benefit of the research community.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly with good quality. The exploration of using per-layer clipping and the engineering efforts for large model training is novel. Pseudocode and hyper-parameter are given in the paper but reproducing the implementation and the experiments can be hard.\n",
            "summary_of_the_review": "I support accepting this paper given its strong empirical contribution.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4516/Reviewer_QMKV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4516/Reviewer_QMKV"
        ]
    },
    {
        "id": "GwcNIs2VZna",
        "original": null,
        "number": 3,
        "cdate": 1666658934161,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658934161,
        "tmdate": 1666660494224,
        "tddate": null,
        "forum": "oze0clVGPeX",
        "replyto": "oze0clVGPeX",
        "invitation": "ICLR.cc/2023/Conference/Paper4516/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel adaptive per-layer clipping and shows superior performance compare with classical flat clipping in GPT-2 and GPT-3 to protect the privacy while maintain a better task performance. ",
            "strength_and_weaknesses": "Strength: \n1. The propose work is addressing a fundamental problem in privacy-aware large scale deep learning models in NLP.  \n2. The proposed method is easy to follow and very intuitive, and quite reasonable. \n3. The experiments are pretty solid. \n\nWeakness: \n1. The efficiency of the proposed strategy is mentioned in introduction but not evaluated in experiments. \n2. The epsilon value is set in relatively large values (3 and 8) in several experiments. ",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed work is novel and the paper is well-written. ",
            "summary_of_the_review": "The paper proposes a novel adaptive per-layer clipping and shows superior performance compare with classical flat clipping in GPT-2 and GPT-3 to protect the privacy while maintain a better task performance. \n\nStrength: \n1. The propose work is addressing a fundamental problem in privacy-aware large scale deep learning models in NLP.  \n2. The proposed method is easy to follow and very intuitive, and quite reasonable. \n3. The experiments are pretty solid. \n\nWeakness: \n1. The efficiency of the proposed strategy is mentioned in the introduction but not evaluated in experiments. \n2. The epsilon value is set in relatively large values (3 and 8) in table 3 and table 4 without explanation. \n\nI recommend an acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4516/Reviewer_CEDx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4516/Reviewer_CEDx"
        ]
    }
]