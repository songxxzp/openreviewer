[
    {
        "id": "U9a2SOrrc7J",
        "original": null,
        "number": 1,
        "cdate": 1666638706262,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638706262,
        "tmdate": 1668287843383,
        "tddate": null,
        "forum": "2W6ExpOzWGV",
        "replyto": "2W6ExpOzWGV",
        "invitation": "ICLR.cc/2023/Conference/Paper4764/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes an approach to learning distributions on the SO(3) manifold via an autoregressive factorization of the distribution on quaternions, wherein each conditional and the first marginal are approximated by a mixture of uniform distributions.\n\nThe paper provides a comparison to the prior work of Murphy 2021, and shows some empirical advantages over that approach.",
            "strength_and_weaknesses": "Strengths:\n* The method is relatively simple and very computationally efficient as compared to the approach of Murphy.\n* The experimental results in figure 7 are illustrative.  This is a very nice choice of visual.\n\nWeaknesses:\n* The method is not very clearly motivated relative to other possibilities.  E.g. why use quaternions as the underlying representation on which to apply the autoregressive decomposition? And why use a mixture of uniforms rather than something else with a computable likelihood?\n* The empirical component of the validation is very limited.  The authors could strength the paper with comparisons to additional methods.\n* The experimental results in figure 7 A appear to be a negative result.  There is much probability mass spread throughout the space, not just on the four possible orientations given the top view of the \u201c5\u201d. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The choice to use a mixture of uniform distributions was surprising and a bit odd to me.  One would expect that problems like the ones examined would exhibit some amount of local smoothness that is not easily captured with the mixture of uniform construction.  Another option could have been, for example to use a mixture of Gaussians for each conditional (as, for example, in RNADE MOG https://www.pure.ed.ac.uk/ws/portalfiles/portal/11874029/1068.pdf).  Can the authors speak to this choice? Have they considered a mixture of Gaussians for the conditionals with appropriate rescaling (e.g. via softmax) to the constrained space?\n\nA chief advantage of the approach is said to be the computational speed-up relative to Murphy 2021.  Is computation cost a limiting factor in the applications to which this method is applicable?   \n\nThe authors describe their method and characterize it relative to the work of Murphy 2021.  Why do the authors choose this as the base of comparison? I was not previously familiar with this approach so was surprised to see it treated as a gold-standard baseline to beat.  Without a compelling explanation for the choice of this baseline the empirical results do not stand on their own as impressive.\n\nThere has been quite a lot of recent work in this space.  Can the authors comment on advantages and disadvantages to previous work including:\n\nDe Bortoli, Valentin, et al. \"Riemannian score-based generative modeling.\" arXiv preprint arXiv:2202.02763 (2022).\n\nFalorsi, L., de Haan, P., Davidson, T. R., and Forr\u00e9, P. (2019). Reparameterizing distributions on lie groups. In International Conference on Artificial Intelligence and Statistics, pages 3244\u20133253.\n\nLeach, Adam, et al. \"Denoising Diffusion Probabilistic Models on SO (3) for Rotational Alignment.\" ICLR 2022 Workshop on Geometrical and Topological Representation Learning. 2022.\n\n---\n\nThe writing in the paper is generally very clear.",
            "summary_of_the_review": "The paper describes a new approach for modeling distributions on the SO3 manifold.  I recommend rejection because of little motivation for the choice of the method relative to other possibilities, and because of a lack of substantial positive empirical results to demonstrate that the method performs well.\n\nUpdate: I have changed my score from 3 to 5.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4764/Reviewer_WNVz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4764/Reviewer_WNVz"
        ]
    },
    {
        "id": "fLmyHFwRP2",
        "original": null,
        "number": 2,
        "cdate": 1666771734054,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666771734054,
        "tmdate": 1666771734054,
        "tddate": null,
        "forum": "2W6ExpOzWGV",
        "replyto": "2W6ExpOzWGV",
        "invitation": "ICLR.cc/2023/Conference/Paper4764/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper uses a quaternion model for capturing rotational distributions. The approach is branded as a new 'method' even if it appears more like a model construction. The efficiency over a baseline is demonstrated on simple data sets.\n",
            "strength_and_weaknesses": "*Strengths*\n\n1. Quaternions are the standard way of modelling rotations in tracking, position estimation, and pose modelling, and thus most likely a sensible way of modelling them also for capturing complex rotational distributions in general.\n\n2. The paper is interesting, and could topic-wise be a good fit for the conference.\n\n*Weaknesses*\n\n3. The originality and novelty of the approach is questionable: This appears more like a standard model design choice for the particular application (you know that the phenomenon has rotational symmetry, and thus you incorporate this as prior knowledge in your model), and not as a new 'method'.\n\n4. The presentation could be improved. The text is wordy and could be stating things more directly and clearly. The abstract alone almost fills the first page.\n\n5. The experiments act as proof of concept. Additional experiments would have strengthened the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The originality and novelty of the approach is questionable: This appears more like a standard model design choice for the particular application (you know that the phenomenon has rotational symmetry, and thus you incorporate this as prior knowledge in your model), and not as a new 'method'. The presentation could be improved. The text is wordy and could be stating things more directly and clearly. The paper has been written in first person (\"I introduce...\") which is non-standard and sounds a bit weird (this is a matter of taste, of course).\n\nThe author(s) have not shared code for replicating their experiments, but they provide pseudo-code in the appendix.\n",
            "summary_of_the_review": "This paper is interesting but appears to have multiple flaws.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4764/Reviewer_oE5t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4764/Reviewer_oE5t"
        ]
    },
    {
        "id": "7kP4vywTAcb",
        "original": null,
        "number": 3,
        "cdate": 1667518621314,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667518621314,
        "tmdate": 1669509076283,
        "tddate": null,
        "forum": "2W6ExpOzWGV",
        "replyto": "2W6ExpOzWGV",
        "invitation": "ICLR.cc/2023/Conference/Paper4764/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a novel method (AQuaMaM) for 3D quaternion orientation estimation from potentially ambiguous 2D images. It employs a Transformer architecture to learn sequences of quaternion parameters representing distributions over the SO(3) group of 3D rotations, treating them as language tokens. \n\nThe proposed architecture is able to efficiently learn multimodal distributions over SO(3), allowing to represent multiple legitimate candidate rotations corresponding to a given ambiguous 2D image.\n\nThe approach is validated on a toy dataset and a 3D die orientation estimation dataset, demonstrating higher accuracy and higher training and inference time efficiency with respect to a strong SoTA baseline (IPDF).",
            "strength_and_weaknesses": "Strengths\n---------\n- To my knowledge, the proposed approach is novel. It seems to be the first successful application of a Transformer autoregressive model for multimodal distribution estimation on SO(3).\n- AQuaMaM is compared against a strong baseline (IPDF), demonstrating:\n   1) Significantly faster convergence time\n   2) Faster inference, since it requires few forward passes with respect to the $N$ required by IPDF\n   3) Higher accuracy\n   4) Higher reliability (see Fig. 6)\n- The paper is generally well-written. The introduction is particularly pleasant to read and positions the paper very well in the literature.\n\nWeaknesses\n------------\n- As detailed in the following, there are several points in which clarity shall be improved. This would especially help readers who may be familiar with the orientation estimation problem, yet not necessarily accustomed to language modeling and Transformers.\n- The die experiment is convincing and allows for detailed analysis in a controlled way. Still, an additional comparison on a more challenging dataset (e.g., the one employed in the original IPDF paper) would strengthen the paper by validating AQuaMaM on an additional, possibly harder problem.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n-----\n- Writing is in general very good. Section 1 is particularly well-written\n- Some statements and technical steps were not clear or simple to understand:\n   1) \"IPDF is trained with Ntrain \u226a Ntest which can make it difficult to reason about how the model will behave in the wild\" --> I don't see how the number of employed test points and its relationship with the number of training points could impact performance expectations on unseen data.\n   2) In Sec. 2.2, I found it hard to understand the role of the unit 3-ball. While reading, it was non-trivial to grasp why \"Therefore, there is a one-to-one mapping f : B3 \u2192 H1\". There seemed to be a gap w.r.t. the previous paragarph: it was not immediately obvious how the remaining quaternion parameters $q_x, q_y, q_z$ map to $B^3$\n   3) In Eq. (1), the reasoning leading from the second expression (with the summation) and the third one is not clear. It goes from a summation of weighted uniform distributions to a single term weighted by $\\pi_k$.\n   4) After Eq. (2), the derivation of the conditional distribution term $p(q_z \\| q_x, q_y)$ is not provided. It may possibly be trivial (or not), but in my view it should be at least reported explicitly in the Appendix.\n   5) The origin of $q_w$ at the numerator in (4) is not clear. What is its relationship with $s_q$? I'd suggest to make the passages explicit.\n- Some important points refer the reader to prior works. It would help a lot to provide a synthetic description of relevant tools in the main paper or possibly in the appendix to facilitate understanding. In particular:\n   1) The partially causal attention mask\n   2) The claim that \"models using mixtures of uniform distributions that partition some domain can be optimized solely using a \u201clanguage model loss\u201d\"\n- Not clear whether inference requires a single pass as stated in the abstract or three passes (although optimized via caching) as mentioned at the end of Section 3.2\n- In general, figures help a lot in grasping some of the most complex passages. Still, possible improvements may include: \n   1) Extending the scheme in Fig. 4 or adding a new one to illustrate the steps outlined in the last paragraph of Sec. 3.1, namely the generation of the transformed tokens and the classifier head; \n   2) The description below the underbrace in Fig. 4 may possibly be wrong. The first tokens should be patch embeddings, while the last 3 should be the position embeddings as far as I understood; \n   3) In Fig. 6b, the Category 4 subfigure should probably have 16 rotations, while only 3 are shown.\n- There is some notational confusion and redundancy between $\\pi_k, \\pi_{q_c}, c, k$ and between $\\omega_c$ and $\\hat b_i$ from Figure 2\n- Define $\\pi_{q_{d,c}}$ and explain the difference from $\\pi_{q_{c}}$\n- Consider making the model parameters explicit in the NLL loss definition, i.e., $\\mathcal{L}(\\pi_{q_{d,c}}, \\mathcal{X})$\n\nQuality\n----\n- The quality of the work is generally high. The proposed architecture seems to be suitable for this kind of problem and is able to overcome some limitations of IPDF on well-designed experiments.\n\n- Only one baseline is compared against (IPDF), but this choice appears justified by its strength with respect to other candidates (although on a different dataset).\n\n- Training time to convergence is significantly faster for the proposed method w.r.t. IPDF, as shown in Fig.  5, supporting one of the main claims of this work \n\n-  I agree on the fact that the proposed die experimental scenario is more controllable and allows to better study some key aspects of the method and the baseline, as stated by the author in Sec. 4. Still, an additional comparison on the symmetric solids dataset employed in the original IPDF paper may be informative to evaluate the comparative performance of AQuaMaM on a more challenging task.\n\nNovelty\n---\n- The approach appears indeed innovative. I am not aware of methods employing Transformer architectures to tackle this problem\n\nReproducibility\n----\nThe approach seems to be described in sufficient detail for possibly reimplementing it. Implementation details and code snippets are reported in the appendix.",
            "summary_of_the_review": "Overall, in my view the paper makes a significant algorithmic contribution by presenting a novel Transformer-based approach to complex multi-modal distribution learning on SO(3). The proposed approach is demonstrated to be faster and more accurate and reliable than a strong baseline on two clear and controllable experimental setups.\n\nThe paper is generally well-written and pleasant to read. Some improvements would be required in terms of clarity, as detailed above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4764/Reviewer_KBHk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4764/Reviewer_KBHk"
        ]
    }
]