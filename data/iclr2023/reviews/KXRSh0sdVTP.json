[
    {
        "id": "RmuUpXufG43",
        "original": null,
        "number": 1,
        "cdate": 1666492587062,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666492587062,
        "tmdate": 1667994321290,
        "tddate": null,
        "forum": "KXRSh0sdVTP",
        "replyto": "KXRSh0sdVTP",
        "invitation": "ICLR.cc/2023/Conference/Paper932/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new meta-learning framework formulated as a bilevel optimization problem. The implicit function theorem is utilized to efficiently compute the gradient of the bilevel optimization problem. It is shown that in the context of deep kernel learning the proposed framework encompasses DKL and DKT as extreme cases. In the wide range of models under this framework, the author proposes ADKF-IDF which meta-learns the parameters of the feature extractors and fine-tunes the parameters of the base kernel. This specific design choice avoids the computational hassle in the gradient computation and induces a natural interpretation of deep kernel learning in meta-learning. The ADKF-IDF is extensively compared to existing methods on few-shot learning tasks predicting molecular properties and on Bayesian optimization of molecular properties.",
            "strength_and_weaknesses": "### Strengths\n- The paper proposed a meta-learning framework that is formulated via bilevel optimization and optimized using the implicit function theorem with general applicability.\n- In its application to deep kernel models, the authors propose a deep kernel model with a meta-learned feature extractor and fine-tunable GP parameters. This not only circumvents the computational hassle of the proposed but also gives a quite interpretable hierarchical structure. While including existing methods, DKL and DKT as special cases, this specific model is a balanced interpolation of both under the proposed meta-learning framework.\n- The proposed ADKF-IFT is extensively compared with existing methods on molecular property prediction few-shot learning. \nEspecially, in the experiments on FS-MOL benchmarks, it seems that much effort and resources have been put into the experiment, which provides also good baselines for others as the authors benefited from the baseline results in MoleculeNet experiments.\n- In addition to its strengths in meta-learning, the authors also conduct Bayesian optimization experiments which supports the good quality of learned representation in the proposed meta-learning framework and the well-calibrated uncertainty of ADKF-IFT.\n\n\n\n\n### Weaknesses\n- Is a random initialization of $\\psi_{adapt}$ enough to guarantee the below?\n$$\nIF(\\psi_{adapt} | \\psi_{meta}, S_T) = \\frac{\\partial L_{train} (\\psi_{meta}, \\psi_{adapt}, S_T)}{\\partial \\psi_{adapt}} = 0\n$$\n    - In Appendix B, it says that in ADKF-IFT, the fine-tuned parameter $\\psi_{adapt}$ is initialized using the median heuristic.\n    - It seems that such initialization does not guarantee $IF(\\psi_{adapt} | \\psi_{meta}, S_T) = 0$.\\\nDoes the median heuristic generate a point $\\psi_{adapt}$ such that $IF(\\psi_{adapt} | \\psi_{meta}, S_T) = 0$?\\\nIs there a mechanism that drives a point $\\psi_{adapt}$ such that $IF(\\psi_{adapt} | \\psi_{meta}, S_T) \\neq 0$ to a point such that $IF(\\psi_{adapt} | \\psi_{meta}, S_T) \\neq 0$?\n- How is it guaranteed that $\\psi_{adapt}^*$ is a minimum?\n    - From Thm.1, it seems that when the implicit function theorem is used, implicitly defining function is \n$$\nIF(\\psi_{adapt} | \\psi_{meta}, S_T) = \\frac{\\partial L_{train} (\\psi_{meta}, \\psi_{adapt}, S_T)}{\\partial \\psi_{adapt}} = 0\n$$\nThis implies that for a given $\\psi_{meta}$, $\\psi_{adapt}$ satisfying the above equation is a maximum, a minimum, or a saddle point.\n    - I was wondering how $\\psi_{adapt}$ is guaranteed to be a minimum.\\\nEven if the initialization guarantees $IF(\\psi_{adapt} | \\psi_{meta}, S_T) = 0$ or the gradient descent eventually makes the point to satisfy $IF(\\psi_{adapt} | \\psi_{meta}, S_T) = 0$, \n    - It seems unlikely that the minimization of upper validation loss is obtained with the maximization of lower train loss. \\\nThis may imply that some care is necessary for the choice of two losses in this bilevel meta-learning framework.\\\nI agree that the specific instantiation of ADKF-IFT seems reasonable also in this respect.\\\nStill, I am curious about the authors' thoughts and opinions on this.\n- Sensitivity to the initialization\n    - Even if it is guaranteed that $\\frac{\\partial L_{train} (\\psi_{meta}, \\psi_{adapt}, S_T)}{\\partial \\psi_{adapt}} = 0$, it is imaginable that, in some case, $\\psi_{adapt}$ is initialized to a point of a connected component of $\\\\{\\psi_{adapt} | \\frac{\\partial L_{train} (\\psi_{meta}, \\psi_{adapt}, S_T)}{\\partial \\psi_{adapt}} = 0 \\\\}$ which consists of maximum points.\nConsidering this case, I am curious about the sensitivity of the method to the initialization or the choice of hyperparameters (some hyperparameters may exclude such bad cases).\n\n\n\n\n\n### Minor points\n- It seems that this somewhat seemingly relevant work is missing\n    - Few-Shot Bayesian Optimization with Deep Kernel Surrogates; ICLR 2021\n    - I am not asking to include this as an additional baseline. \n    - The comparison with this seems to be able to further highlight the benefit of ADKF-IFT.\n- Significance of the differences in the ablation study.\n    - I can check visually the consistency of the superiority of ADKF-IFT in Figure 3. But in many cases, the confidence intervals (I guess they are from repeated experiments with some randomness, it is reasonable to call them confidence intervals) overlap. Maybe providing some simple statistical results may strengthen the argument of the ablation study.\n- Duplicated names\n    - In the paper, ADKF-IFT stands for two things, 1) meta-learning framework formulated by a bilevel optimization problem and trained using the implicit function theorem, 2) deep kernel GP model with meta-learned feature extractor and fine-tuned GP parameters using the proposed meta-learning framework\n    - Saving ADKF-IFT to the proposed specific deep kernel methods, it may be less confusing to use another name for the proposed meta-learning framework reflecting its generality, for example, Meta-BIFT (meta-learning formulated via BIlevel optimization and optimized via Implicit Function Theorem)\n",
            "clarity,_quality,_novelty_and_reproducibility": "Except for the questions in 'Weaknesses', the paper is well-written in general. Its technical novelty efficiently combines existing methods and tools under a new meta-learning framework proposed by the authors. The empirical contribution of the paper seems quite significant by providing other few-shot learning baselines on FS-MOL for various support set sizes. It seems that implementation details are provided at a reasonable level of detail. Still, the reproducibility can be improved by disclosing the code.",
            "summary_of_the_review": "The paper proposes a new meta-learning framework and a specific instance of it that is shown to be effective in few-short learning and out-of-sample prediction demonstrated via Bayesian optimization experiment. With a good combination of many technical tools under the new meta-learning framework, the paper has good empirical contributions. As long as my concerns mentioned in 'Weakness' can be addressed, I am willing to increase my score and support the acceptance of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper932/Reviewer_fnkh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper932/Reviewer_fnkh"
        ]
    },
    {
        "id": "SND64PGki8F",
        "original": null,
        "number": 2,
        "cdate": 1666671597001,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671597001,
        "tmdate": 1666671597001,
        "tddate": null,
        "forum": "KXRSh0sdVTP",
        "replyto": "KXRSh0sdVTP",
        "invitation": "ICLR.cc/2023/Conference/Paper932/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In order to fit molecular property prediction models that give good uncertainty estimates but also overcome issues associated with neural network methods that either overfit or underfit Gaussian Processes, the authors propose Adaptive Deep Kernel Fitting with Implicit Function Theorem (ADKF-IFT), a generalization of Deep Kernel Learning and Deep Kernel Transfer that meta-learns a subset of parameters (usually those associated with a feature extractor) across many tasks, and transfer learns the rest (usually those associated with the base kernels of Gaussian Processes). The authors propose a bilevel optimization method that avoids costly inner optimization gradient calculations by invoking Cauchy's Implicit Function Theorem. The authors demonstrate how ADKF-IFT outperforms a large variety of existing methods of molecular property prediction tasks in MoleculeNet and few-shot prediction tasks in FS-MOL. They also show how the feature representation learned by ADKF-IFT can be used with Gaussian Process surrogate models to provide optimal or near-optimal results in out-of-domain molecular optimization tasks like molecular docking and antiviral drug design. Using ablation studies, the authors also show that the bilevel optimization is what gives the reported performance gains.",
            "strength_and_weaknesses": "Strengths:\n- ADKF-IFT provides the means to effective training of Gaussian Processes that overcomes the shortcomings of the two methods it generalizes, DKL and DKT while achieving state-of-the-art predictive power.\n- ADKF-IFT achieves state-of-the-art results on many molecular property prediction benchmarks, including few-shot tasks.\n- ADKF-IFT appears to learn informative molecular features that perform well for entirely different tasks.\n\nWeaknesses:\n- The authors do not include any examples that demonstrate that ADKF-IFT is neither over- or underfit, nor do they demonstrate that the uncertainty estimates derived from the learned GPs are better than those of GPs learned in other ways.\n- MoleculeNet is somewhat out-of-date - the authors should consider the superset of tasks proposed in the Therapeutics Data Commons (https://tdcommons.ai/) database.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to understand. The work appears to be high-quality and well-executed, though no code has yet been shared.\n\nGeneralizing the DKL and DKT methods into a bilevel optimization problem in order to overcome the shortcomings of both is an original and important contribution.",
            "summary_of_the_review": "This paper makes a significant contribution to Gaussian Process training methods by proposing a novel bilevel optimization method that achieves state-of-the-art molecular property prediction. Such models could have significant impact on the work of researchers applying ML to drug discovery, or other related topics.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper932/Reviewer_sNf8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper932/Reviewer_sNf8"
        ]
    },
    {
        "id": "9X_Q3OcoiVV",
        "original": null,
        "number": 3,
        "cdate": 1667369273739,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667369273739,
        "tmdate": 1667369273739,
        "tddate": null,
        "forum": "KXRSh0sdVTP",
        "replyto": "KXRSh0sdVTP",
        "invitation": "ICLR.cc/2023/Conference/Paper932/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents ADKF-IFT -- a new method to train deep kernel GPs using meta-learning. This method splits the parameters into two sets, where only one set is adapted to each task. This leads to a bi-level optimization problem, that the authors solve using the Implicit Function Theorem. The number of parameters which are adapted can help in balancing between over- and under-fitting, which helps the model perform well on tasks with small training datasets. The paper also shows that prior methods for training deep kernel GPs, DKT and DKL, are special cases of the proposed method.\n\nExperimental results from chemistry show that the model generally performs competitively on small data domains, and also perform well on OOD tasks.",
            "strength_and_weaknesses": "### Strengths\n* The paper presents a novel method for training deep kernel GPs that is competitive on small data domains.\n* The proposed method can help balance over- and under-fitting.\n* The paper demonstrates that the proposed method subsumes the existing DKT and DKL methods as special cases.\n\n### Weaknesses\n* It would be instructive to understand how the model performs on tasks with larger training datasets. I understand that is not the goal of the paper, but understanding the failure modes of the method (if there are any) can be useful.\n* How sensitive is the method to the choice of \\Psi_{adapt}, the parameters that are adapted to each task? Providing a discussion on this would be useful\n* Provide more details about the hyperparameters & architectures for reproducing the results.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe writing is clear and easy to follow. The paper provides enough background to understand the paper.\n\n### Novelty\nThe primary contributions of the paper are the interpolation between meta-learning & conventional deep kernel learning, and the use of implicit function theorem to solve the resulting bi-level optimization problem. These are both novel and interesting.\n\n### Reproducibility\nThe paper does not provide all of the hyperparameter and architecture details to reproduce the results. However, the authors promise to make the code available upon publication.",
            "summary_of_the_review": "The paper presents a novel method for training deep kernel GPs which obtains strong results on small datasets. The proposed method is shown to be a generalization of existing methods. Overall, I find this work to be sufficiently novel and useful for many chemistry problems to justify acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper932/Reviewer_BY35"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper932/Reviewer_BY35"
        ]
    }
]