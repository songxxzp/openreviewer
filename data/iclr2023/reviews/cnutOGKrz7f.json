[
    {
        "id": "nPklaMDFUgg",
        "original": null,
        "number": 1,
        "cdate": 1666576549886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576549886,
        "tmdate": 1666647284805,
        "tddate": null,
        "forum": "cnutOGKrz7f",
        "replyto": "cnutOGKrz7f",
        "invitation": "ICLR.cc/2023/Conference/Paper4591/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method called \"Soft Sampling\" which is a data selection method for training of deep learning models. They also show some theoretical convergence guarantees and experimentally show that their method achieves similar accuracy as GradMatch (a SoTA method) while using less resources (as measured by wall-clock training time). The datasets used are CIFAR10 (image classification), Librispeech (audio to text), and their in-house Payload dataset (audio to text).\n\nThe way I understand it, Soft Sampling simply selects a random subset with replacement given the budget at each epoch, as such the subset is not pre-determined/fixed.",
            "strength_and_weaknesses": "Strengths:\n- training time improvements on the baseline (GradMatch) are significant (e.g. when comparing GM_R1 and SS_R1 on CIFAR10)\n- theoretical guarantees are provided\n\nWeaknesses:\n- the method name is very confusing. Looking at algorithm 1, I would simply call this method a variant of random sampling. I can understand that the \"soft\" part is trying to show that the data selection is not fixed (i.e. hard), so \"soft random sampling\" (with replacement) could be a better name\n- given the above, I see no novelty in this method. Random sampling* is one of the first and most important baselines for data selection. As such, I'm surprised to see this being proposed as a novel method. The strength of this baseline has also been reported in multiple earlier works, for example, DeepCore (Chengcheng Guo et. al.) concludes: \"... although various methods have advantages in certain experiment settings, random selection is still a strong baseline.\". I believe this is a fatal flaw.\n\n\n*: I understand that \"random sampling\" can refer to different but related methods: hard vs soft (or online vs offline) as well as with/without replacement.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: I believe the method is clear, though the name is confusing.\n\nQuality: Experiments seem sound and show clear improvement on multiple datasets, but only one baseline is used. Admittedly, I did not verify the theoretical proof (I saw little point in doing so).\n\nNovelty: I see no novelty in this method.\n\nReproducibility: the experiments section mentions the hyper-parameters and the GPU used. However, no code seems to be available.",
            "summary_of_the_review": "Unless I'm missing something, which I'm hoping the authors can point out, the proposed method is not novel and has been one of the baselines that most data selection methods are always compared to. The strength of this baseline has also been observed many times before (e.g. DeepCore). As such, even though I believe the problem (data selection) to be highly important and knowing that I believe the method to be valuable, I must reject this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4591/Reviewer_BYaz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4591/Reviewer_BYaz"
        ]
    },
    {
        "id": "fTxA9PjPWVf",
        "original": null,
        "number": 2,
        "cdate": 1666647157864,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647157864,
        "tmdate": 1666647157864,
        "tddate": null,
        "forum": "cnutOGKrz7f",
        "replyto": "cnutOGKrz7f",
        "invitation": "ICLR.cc/2023/Conference/Paper4591/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\n1. The paper argues in favor of random sampling (termed as soft sampling) over other methods - specifically coreset based. The argument is well understood and to summarize \n   a. The advantage of coresets in accuracy over random sampling is limited in some (maybe authors want to argue over many / most? But the evidence is provided on 3 datasets, 1 of which is very large) large scale datasets. \n   b.  On the other hand, the computational cost of coresets is usually very high which is well known and accepted. \nTo extend the argument, the time spent in coreset computation can actually be used to train models for longer times with random sampling \n\n2. The analysis seems fairly straightforward which is not a bad thing in my opinion. However, it definitely requires more scrutiny. Specifically, how is the convergence analysis different from mini-batch SGD analysis with bounded variance and smooth and lipschitz continuous ? More specifically, what new thing do we learn here ? Also, sample coverage and occupancy computations are important to understand the dynamics of random sampling in the context of training. However, to get fair understanding of the contribution, you might want to compare against a mini-batch SGD and talk about number of batches vs occupancy / coverage. (instead of epochs)\n\n3. In the results, authors use same number of epochs across different methods. However, when sampling, the number of epochs really loose their meaning. Instead, we can keep number of iterations the same . Because it is difficult to compare between, say, 10% and 20% sampling rate if the 20% is runs twice as many iterations. \n\n4. The argument that random sampling is competent for a lot of cases is a well accepted belief in the community. I am not sure if there has been a extensive study on the topic though. However, it should be noted that there will be cases where coresets will strongly outperform random sampling w.r.t accuracy and showing that random sampling beats coresets on a handful of datasets does not support using random sampling is better than coresets for training. In order to make a strong argument, i would suggest the following two directions - \n\na. Use a lot of real datasets from different domains with learning and do an extensive study confirming what people believe. Having such a study would be very useful.\nb. Else, maybe you want to make an argument in favor of random sampling even in adversarial cases. In such a case, hand design data distributions where random sampling is bad (so in terms of #iterations, coresets beat random sampling) . But in training scenarios, using the advantage of fast sampling random sampling is competent with coresets in terms of #seconds to converge. \n\n5. Also, it is not clear what the novelty of the paper is specifically because random sampling for training is the norm. It would be useful if authors would elaborate on the novelty. One interesting direction that i see in random sampling itself is this trade-off between soft-hard sampling. For example, the following two are natural extremes in random sampling, \n[softest] you pick each sample with replacement and form batches from these samples\n[hardest] you shuffle the entire dataset and make batches from this shuffled dataset. This is also what is current practice.\nYour approach is then somewhere in the middle, where you select a subset (which obviously has no repetitions) and then make batches from the subset. However the two different subset selections can have overlaps. \n\nYou can analyse effect of this granularity that you introduce w.r.t occupancy, coverage and variance of gradient estimate for convergence (in theory and simulations) and in training convergence performance (# iterations required to converge). \n\n",
            "strength_and_weaknesses": "The paper is well written. However, it has several weaknesses. Please see the points 2, 3, 4, 5",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity in paper is very good. However the novelty seems limited see 5 above",
            "summary_of_the_review": "The major concern is that the paper does not provide clear evidence to the effect that random sampling should be always preferred in training neural networks.  See points above in summary",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4591/Reviewer_BCTj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4591/Reviewer_BCTj"
        ]
    },
    {
        "id": "yPS-lm-6h3",
        "original": null,
        "number": 3,
        "cdate": 1666676216063,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676216063,
        "tmdate": 1666676216063,
        "tddate": null,
        "forum": "cnutOGKrz7f",
        "replyto": "cnutOGKrz7f",
        "invitation": "ICLR.cc/2023/Conference/Paper4591/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyze the soft sampling method for training DNNs, which selects a subset uniformly at random with replacement from the full data set in each epoch. Analysis of convergence rate and coverage are conducted.",
            "strength_and_weaknesses": "Strength:\n1. The theoretical analysis of convergence and coverage of the sampling method could be a useful supplement to the methodology.\n\nWeaknesses:\n1. There is no major technical contribution in this paper. Algorithm 1 seems to be a standard practice of DNN training.\n2. The theoretical results of convergence might be useful. It is not clear how the analysis coverage and occupation could be useful.\n3. The evaluation part is weak. A larger scale dataset, like ImageNet is required for evaluation for validating the effectiveness.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to read. ",
            "summary_of_the_review": "As the technical contribution of this paper is weak, I recommend reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4591/Reviewer_u569"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4591/Reviewer_u569"
        ]
    }
]