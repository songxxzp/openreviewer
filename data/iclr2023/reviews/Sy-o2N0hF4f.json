[
    {
        "id": "svhHord_ir-",
        "original": null,
        "number": 1,
        "cdate": 1666367017252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666367017252,
        "tmdate": 1666367017252,
        "tddate": null,
        "forum": "Sy-o2N0hF4f",
        "replyto": "Sy-o2N0hF4f",
        "invitation": "ICLR.cc/2023/Conference/Paper1352/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper targets the low sample efficiency of current reinforcement learning approaches. It focuses on a central question: can we leverage pre-trained data without actions to achieve a good initialisation? The experiments on Atari show consistent improvements using the proposed pretraining strategy that is based on cycle consistency.",
            "strength_and_weaknesses": "## Strengths\n\n* The paper tackles an important problem - sample efficiency of RL agent training. The approach is based on learning a representation through unsupervised pre-training that can then easily be adapted when training the agent. The topic of the paper should be of interest to a large audience.\n\n* The experimental evaluation is thorough and shows excellent results, improving over SGI that uses action labels during pre-training, which are not as easy to obtain as simple replays.\n\n* The method is simple and general and should be easily adaptable to other tasks and methods.\n\n\n## Weaknesses\n\n### W1 Pretraining Dataset\nThe main motivation of the paper is that for pre-training it is easier to collect unlabelled videos than state-action pairs. The evaluation does not fully show the full extend of this, as the data comes from training an agent on the environment. Pragmatically, at that point training a second agent is not useful, as the data collection already provides a trained agent. It could be helpful to analyze this dependency by varying the data-collection method. One could use human data, youtube videos or a random agent and measure the effect on the performance. \n\n### W2 Generalization\nFrom the paper it is not clear if the model is pre-trained for each environment individually, or if the pre-training happens across all environments and only the agent training is done separately. The second scenario is of course a much stronger showcase of the benefits of the approach. \n\n### Minor:\nEq 2: when $\\beta=1$, the stop-gradient operator should be superfluous as the loss then has the same gradient as simply $\\Vert z_e - e \\Vert^2_2$. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, clear and easy to follow. The method is simple and leads to good performance. The paper uses standard benchmarks and promises to release code upon acceptance. ",
            "summary_of_the_review": "The paper addresses an important problem and proposes a simple method that achieves excellent results. There are some weaknesses in the evaluation, which are however overall outweighed by the strengths, as the evaluation still demonstrates the applicability of the method sufficiently. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1352/Reviewer_Krh3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1352/Reviewer_Krh3"
        ]
    },
    {
        "id": "dYQBgBQCdV",
        "original": null,
        "number": 2,
        "cdate": 1666616965179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616965179,
        "tmdate": 1666616965179,
        "tddate": null,
        "forum": "Sy-o2N0hF4f",
        "replyto": "Sy-o2N0hF4f",
        "invitation": "ICLR.cc/2023/Conference/Paper1352/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approach to pretrain a network to learn features for RL from videos of the task without actions. The network uses a reconstruction loss and a cycle-consistency loss to learn this. The approach is evaluated on several atari game tasks.",
            "strength_and_weaknesses": "Efficient RL learning is an important task, and this approach shows benefit over random initialized training. The ablations show that some of the components, e.g., reconstruction loss, are useful. However, there are no comparisons to any previous works, which makes it hard to know how well or efficient this approach is compared to others. The approach is only evaluated on relatively simple videos of atari games, how would this approach work for more complex or real videos? How much of the approach relies on the pretraining data? For example, if it was purely random trajectories vs. top human player trajectories, how would that influence the pretraining? In Table 2, it seems that EZ-L is usually worse, which seems to contradict the writing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly well written, and would likely be able to be reproduced from the provided details. The approach is , which would further aid the reproducibility. However, the approach is not especially novel, it is largely a combination of existing ideas, such as cycle-consistency and VQVAEs, but applied to a new task of pretraining RL networks.",
            "summary_of_the_review": "The paper proposes a method to pretraing RL networks. The approach is straightforward, though not especially novel. The experiments show the benefit over random init, but are lacking comparisons to other works.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1352/Reviewer_wEqK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1352/Reviewer_wEqK"
        ]
    },
    {
        "id": "jlhHDUHEClQ",
        "original": null,
        "number": 3,
        "cdate": 1667051719691,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667051719691,
        "tmdate": 1667051719691,
        "tddate": null,
        "forum": "Sy-o2N0hF4f",
        "replyto": "Sy-o2N0hF4f",
        "invitation": "ICLR.cc/2023/Conference/Paper1352/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Paper proposes a framework which can use action-free videos (instead of action-tagged videos) to train RL models by a 2-phase pipeline. In phase 1, the approach infers the hidden action embedding from the videos, pre-train the visual representation and the predicted next frames. A vector-quantization step was also introduced to prevent the model from learning the shortcut mapping (identity function). For the down-stream tasks, the model uses an approach termed Action Adapter to map the action embedding to the actual actions.\n\nEmpirical experiments outperformed the SOTA methods such as EfficientZero (without pre-training) and SGI (pre-training with actions tagged videos) for the custom Atari 50k dataset. The proposed method was pretrained with 1M transition of replay frames from EfficientZero model.\n\nFurther experiment was done to pre-train a single model for 6 different games to demonstrate that the model can quickly adapt to different rules with a single pre-trained model. In this setting, the \"unified\" model outperformed EfficientZero (without pre-training) for 5 out of 6 games.\n\nAblation studies were also done to demonstrate the importance of the various sub-components (differences reconstruction, vector quantization).",
            "strength_and_weaknesses": "Strengths\n1. Novelty, contributions and experimental evidence are well-linked.\n2. Paper is clear and concise.\n\nWeaknesses\n1. Novelty is quite limited in the sense that pre-training is well-established concept for other learning paradigm. However, to my knowledge, it was not applied to RL as the paper proposed.\n2. It is not clear why paper only experimented on Atari 50k when Atari 100k is the standard benchmark. Given the efficiency of the proposed method, it should not be difficult to also extend the experiments to 100k. This will also allow direct comparisons for other SOTA methods.\n3. Seo et al was cited as a comparable method. But no experimental was done to compare. Why?",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity and quality are high.\n2. Novelty is medium.\n3. Reproducibility is high as important details were provided. Moreover, source codes will be released upon acceptance.\n\nOther suggestions\n1. Visualization of the action embeddings through clustering (e.g. t-SNE with colored points of the dictionary mapping) may be useful for authors and readers to understand if the mapping are indeed correct/useful.\n2. Minor. It is unconventional that authors choose to put Related Works in the 2nd last Section. Conventionally, it is in Section 2 as the readers can have a more thorough understanding of the problem and the proposed solution before they proceed to read on the other sections. As the paper is well-written, this is not a big issue. But I still suggest that it be moved to Section 2.\n",
            "summary_of_the_review": "1. Good paper with significant scientific and empirical contributions.\n2. However, some experimental setups appear to be incomplete.\n3. Overall, the work is of interests to other researchers in the RL and should be accepted for ICLR.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1352/Reviewer_1ysS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1352/Reviewer_1ysS"
        ]
    },
    {
        "id": "MthNKrP6KYD",
        "original": null,
        "number": 4,
        "cdate": 1667148282517,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667148282517,
        "tmdate": 1667148282517,
        "tddate": null,
        "forum": "Sy-o2N0hF4f",
        "replyto": "Sy-o2N0hF4f",
        "invitation": "ICLR.cc/2023/Conference/Paper1352/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a pre-training mechanism that improves sample efficiency for model-based reinforcement learning. They propose to use raw videos by extracting action embeddings to pretrain their representations and dynamics. The pretrained networks are then finetuned with limited task data.\n\nIn summary, the authors study how pre-training from raw videos can improve model-based RL, propose a method based on cycle-consistency to empower pre-training, and achieve SoTA performance on benchmarks related to Atari games. Emphasis is made on sample efficiency and large improvements over EfficientZero.",
            "strength_and_weaknesses": "## Strengths\n- The paper is clear and understandable\n- The propose approach is based on a simple concept, yet appears to generate large benefits when compare to previous techniques.\n- The reconstruction losses used in conjunction with FICC seem pertinent.\n- The performance increase in the experiments shown is laudable.\n\n## Weaknesses\n- Some claims are unclear or not well defined. For example:\n     a. unrolling the losses, end of Sec. 3.1 -> how was this implemented?\n     b. latent action z has too much information, first paragraph, Sec 3.2 -> can you give information on how you determined this?\n- Loss combinations: $L_{cc}$ and $L_{vq}$ appear quite different and could have different scales that can impact training. There is, however, no controlling factors in the final loss. How was this analyzed?\n- Writing: some sections could be improved in terms of clarity and grammar, e.g. 4.2, 4.3\n- Comparisons: the technique is mainly compared to EfficientZero, but more comparisons to other state of the art techniques would be pertinent.\n- Section 4.3 is unclear: are we still building an action adapter M? If not, how do you adapt to the different amount of actions in each environment?\n- Ablation studies: there is no ablation showing the individual performance delta of each reconstruction loss (difference reconstruction and full reconstruction) - we only see an ablation related to difference reconstruction\n- shortcut learning ablations: there is no analysis showing how reducing the dimensionality of $z_e$ impacts performance. It would be nice to motivate the use of vector quantization instead of just simple dimensionality reduction with quantitative experiments. \n- Figure 6 is compelling, but could be much clearer. Labeling each column and highlighting the displacement in the breakout bar would improve its clarity. \n\n\n## Questions\n- Is the Vector quantization enough to stop FICC from converging to trivial solutions? What is the dimensionality of $z_e$ and $z_q$? There could be a world where these vectors, if large enough, still encode enough information to trivially reconstruct $s_{t+1}$, particularly when finetuned on downstream tasks\n- Are the action embeddings $e_k$ trained during finetuning? It's clear that R and D are finetuned with learning rate $l_f$, but what happens with $e_k$ is unclear",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nThe paper is generally clear and explains the concepts in a well distilled manner. There are certain sections and figures that are unclear and could be fleshed out., like sections 4 and 5, as well as the discussion paragraph. Figures 5 and 6 could be much clearer.\n\n## Quality\nThe general quality is satisfactory. The analysis are correctly performed, the algorithms are explained and the general concept of the paper appears to be strong. To increase quality, it would be ideal to augment the set of comparisons, flesh out a few missing ablations and improve the final Figures.\n\n## Novelty\nThe concept introduced appears to be novel. The different moving parts have been seen before, but the combination and subsequent performance improvements appear to be new.\n\n## Reproducibility\nThe paper is sufficiently clear to be reproducible. Certain details still need to be fleshed out, such as details on loss unrolling and details related to finetuning. More information about how EZ-L was evaluated (and how actions are chosen) would also be ideal.",
            "summary_of_the_review": "The paper proposes a novel idea that shows performance improvements and efficiency gains. In general, the ideas are clearly transmitted and the concept seems simple but powerful. The paper does lack more in-depth comparisons to state of the art methods, and could be clearer in certain sections, but the general idea is sound and interesting. As it stands, the paper is above the acceptance threshold, and could be improved if certain sections are re-written and figures replaced. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1352/Reviewer_HyxY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1352/Reviewer_HyxY"
        ]
    }
]