[
    {
        "id": "6v2J-bVxRS",
        "original": null,
        "number": 1,
        "cdate": 1666625355028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625355028,
        "tmdate": 1666625355028,
        "tddate": null,
        "forum": "pKu077C57fH",
        "replyto": "pKu077C57fH",
        "invitation": "ICLR.cc/2023/Conference/Paper4858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper applies a large language model, Codex to the autoformalisation of proof statements and of proofs. Two main contributions resulting are: (1) translating theorem statements of a form similar to docstrings of mathlib to theorems, and (2) translating (outlines of) NL proofs to Lean proofs.\n\n\n\n",
            "strength_and_weaknesses": "Strengths:\nThe paper is very interesting and has good novelty. \n\nWeaknesses:\nThe main drawback is that the article is very hard to read. The paper is written in a very loose style with almost no technical basis on which to understand what is going on. No model for a mathematical statement is provided, or of the technical basis for the rewriting. It is fully an application of existing tools to a rewriting process, with no details of what the technical novelty is. ",
            "clarity,_quality,_novelty_and_reproducibility": "The results are hard to understand. For this most part, the description is too high-level to comprehend. The descriptions of results are frustratingly vague: for example \"Our chosen proofs are much longer than a typical theorem statement and we didn\u2019t observe Codex outputting a completely correct proof. We instead relaxed the requirement of autoformalisation to produce a (faulty) proof that is easy to repair for humans, saving time and effort compared to formalising from scratch.\"\nHere, there is no notion of key concepts, such as \"much longer than a typical theorem\" (what is the metric for length?); produce a (faulty) proof (what is faulty?); \"easy to repair for humans\" (what is easy/hard?).\n\nAs such, it is virtually impossible to measure progress in this paper.\n\nDetailed issues:\n\nprompts: there are many questions about this aspect, such as\n\n--what is the formal basis?\n--when are prompts too strong/weak? (Example 1 seemed like a very strong prompt, but there is no basis to estimate that)\n--can one learn prompts? This seems like a natural sub-task of this work.\n\nSection 3 is very hard to understand. What is the scope of allowable prompts? Figure 1 \"Example of a prompt\" seems far beyond the scope of the original text.\n\nThe data used is not the clearest. \n--The definition of data sets is far too abstract to understand what they contain.\n\n--Data in tables 1 and 2 is far too abstract to understand what they contain.\n\nThe authors proposed a classification of proofs, but the main body did not contain this material, or a summary of results by proof type.",
            "summary_of_the_review": "This is a paper of high novely, yet the results are hard to understand. Further, the technica basis of the approach is lacking.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4858/Reviewer_Pspz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4858/Reviewer_Pspz"
        ]
    },
    {
        "id": "_311U4ntPd6",
        "original": null,
        "number": 2,
        "cdate": 1666685000148,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685000148,
        "tmdate": 1666685000148,
        "tddate": null,
        "forum": "pKu077C57fH",
        "replyto": "pKu077C57fH",
        "invitation": "ICLR.cc/2023/Conference/Paper4858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper shows empirically that Codex LLM is able to generate mathematical formalizations in a recently developed proof assistant Lean from natural language using prompting. The results include formalization of the relatively short theorem statements with a modest accuracy of 75%, and attempts to formalize short proofs. The overall results are mixed, with proofs requiring significant post-processing and manual fixing. ",
            "strength_and_weaknesses": "Strength:\n\n+ The paper builds on a recent line of promising work on using LLMs for mathematical reasoning task - the focus of this paper on autoformalization. \n\nWeakness: \n\n- Why only limit the work to Lean theorem prover? If the Codex LLM is capable of autoformalization, it should be able to work with Coq, Isabelle and PVS - theorem provers which have been around for decades and are widely used in academia and industry. They also have fairly large libraries that can be used for evaluation. The current simple theorems are not representative of typical need for formalization. \n\n- Some of the prompt examples appear to be imprecise. For example, the natural language talks about vector space and the formalization uses division_ring and module. While one could argue that this is okay because the formal statements are correct but nonetheless the English statement was more specific and one would expect formalization to stay at the same level. Given this example, the reviewer fears that the paper is using a rather liberal notion of \"correct\" statement. This is particularly concerning given the very simple nature of this \"theorem\" statements, which aren't really the challenging aspects of formalization in proof assistants. \n\n- There appears to be significant shift in technique and results for theorems and proofs. Input-dependent prompting is not feasible for proofs. Elaboration is also not feasible. The results for proofs appear to be far worse and no correct proof were generated. The notion of identifying \"easily repairable\" proof is unclear and rather vague. Looking at figure 3, the error in constructed proof were significant. The expansion of absolute value would have created branches and led to severe difficulty in the proof. It is much better not to expand before applying triangle inequality.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper does not have a novel contribution from the ML point of view. Its contribution in demonstrating utility of LLMs to proof assistants is marginal. Please see weaknesses for specific limitations. ",
            "summary_of_the_review": "In its current form, the draft appears to be a work in progress. The focus of the work is empirical demonstration of LLM for autoformalization and the draft falls far short of presenting enough empirical evidence. Expanding to other proof assistants (Coq, PVS etc) and using their large corpus of theorems and proofs to test whether Codex LLM can complete these would be useful. Also, given the lukewarm results, perhaps, prompt engineering can help improve results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4858/Reviewer_jvt2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4858/Reviewer_jvt2"
        ]
    },
    {
        "id": "KdEmoQ7TBm",
        "original": null,
        "number": 3,
        "cdate": 1666691829859,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691829859,
        "tmdate": 1666691829859,
        "tddate": null,
        "forum": "pKu077C57fH",
        "replyto": "pKu077C57fH",
        "invitation": "ICLR.cc/2023/Conference/Paper4858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the application of language models to interactive theorem proving by doing a qualitative case study on input-dependent prompt engineering. It shows that Codex is capable of producing partially correct formal proofs (in Lean 4) that can be turned into correct proofs with a moderate amount of modification (possibly done by a human user).",
            "strength_and_weaknesses": "Strength:\n\n+ The qualitative study is interesting. There hasn't been much work in the field of ML for interactive theorem proving that includes a case study like the one in this paper.\n\nWeakness:\n\n- The contribution looks a bit pale. While observing the consequence of prompt engineering is interesting, prompt engineering itself and the particular methodology (i.e., input-dependent prompts by similarity) adopted in this paper are not new.\n\n- The postprocessing of a completion produced by Codex is claimed as part of the contribution. However, if I understand it correctly, the elaborator is technically a procedure that tranlates expressions in Lean 3 to their counterparts in Lean 4 while making sure that the translated expressions type check.  I appreciate the engineering efforts here, but this part is more of a Lean-specific tweak required to make things work, and does not seem general enough to be a significant contribution.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear and easy to follow if one knows Lean well. The formatting of tables can be made more professional, though. I believe the results are reproducible as the prompts are well-documented.\n\nFor novelty, see comments above.",
            "summary_of_the_review": "I am glad to see the nice qualitative study in this paper, but the overall contribution of this paper is rather insufficient. I suggest the authors try to add more content before the paper can be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4858/Reviewer_eYGZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4858/Reviewer_eYGZ"
        ]
    },
    {
        "id": "wkWu9wKmwL",
        "original": null,
        "number": 4,
        "cdate": 1666762132841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666762132841,
        "tmdate": 1666762132841,
        "tddate": null,
        "forum": "pKu077C57fH",
        "replyto": "pKu077C57fH",
        "invitation": "ICLR.cc/2023/Conference/Paper4858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the automatic generation of formal math statements from (less formal) mathematical statements in natural language. The statements are translated using Codex in a few-shot setting. The contributions are a postprocessing technique for formalized statements, prompt engineering to improve results, and some first attempts at formalizing proofs (not only theorem statements).\n\nThe paper also introduces two simple evaluation sets that address the problem that Codex might have seen the formalizations in their regular evaluation set during pretraining.",
            "strength_and_weaknesses": "I am not sure about the novelty and impact of the contributions:\n- The postprocessing technique presented in this work addresses the problem that most of the available training data is for Lean3, but the approach here used Lean4 syntax. So the problem addressed here is not of general interest. The technique itself seems to be rather adhoc as well.\n- The prompt engineering is similar to Jain et al. (2022), which the authors attribute correctly. It uses similar examples in the few-shot prompt to improve the results.\n- The formalization of proofs seems novel to me (it goes significantly beyond Wu et al. (2022)), but is restricted to a very small dataset and only 2 of the 18 attempts were successful.\n- Other possible contributions, like the evaluation sets, may be nice, but I'm not sure if they carry the paper.\n\nMinor questions:\n- \"For proofs quantitative analysis is infeasible\" Why?\n- \"We focused on theorem statements at the undergrad and more advanced level from various areas of mathematics. These statements tend to be more challenging for autoformalisation compared to mathematics competition problems studied in prior work (Wu et al., 2022) as they often assume more in terms of implicit context and draw from a much larger background (Wu et al., 2022).\" I do not understand the justification \"as they often assume more in terms of implicit context\". ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the write-up is good, but could be improved. Some formulations and claims are a bit confusing (see summary).",
            "summary_of_the_review": "Interesting paper, but I am not sure if the contributions make the bar for ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4858/Reviewer_orra"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4858/Reviewer_orra"
        ]
    }
]