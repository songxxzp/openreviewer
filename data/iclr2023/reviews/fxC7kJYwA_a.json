[
    {
        "id": "wx8oteDCw9J",
        "original": null,
        "number": 1,
        "cdate": 1666634020436,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634020436,
        "tmdate": 1666634020436,
        "tddate": null,
        "forum": "fxC7kJYwA_a",
        "replyto": "fxC7kJYwA_a",
        "invitation": "ICLR.cc/2023/Conference/Paper2509/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a method MuFAN that strikes a better stability-plasticity tradeoff compared to existing methods in an online (single-pass) continual learning setting. The main idea of the method is to use a pre-trained model as an encoder, aggregate the feature maps from different layers, and then pass the aggregated/ fused feature map to a classifier (typically a ResNet model). During training the pre-trained encoder is kept fixed and the classifier is updated. The authors propose a structured knowledge-distillation loss that maintains the angular distance between the successive tasks. Additionally, the authors propose a new normalization technique that splits the channels at a given layer and apply a combination of BatchNorm, InstaceNorm and GroupNorm on different splits of the feature map. The experiments are reported on several supervised continual learning benchmarks. \n",
            "strength_and_weaknesses": "**Strengths**\n\n1. The paper studies an important problem of online continual learning. \n2. The use of a pre-trained model as an encoder and fusing features of the encoder from different scales is a nice idea which is not explored in the context of continual learning extensively. \n\n**Weaknesses**\n\n1. **Overall the method seems very ad-hoc**: While the use of a pre-trained model as an encoder that generates a fused feature map for the classifier is a nice idea, the other two contributions of the paper seem very ad-hoc. The structured-distillation loss seems like a cumbersome way of enforcing knowledge-distillation, and it is not clear if the so-called point-wise knowledge-distillation on more points would achieve similar or better results than structured distillation. The overall training objective (Eq. 4) is the combination of everything, experience-replay, point-wise knowledge distillation, and structured distillation and it is not clear if everything is needed in that objective. Second, the SP-normalization seems even more ad-hoc where half of the features are passed through BatchNorm layers, and the other half are sent to a combination of GroupNorm and InstanceNorm, without any criterion. If BatchNorm is bad for continual learning then there would be excessive forgetting through the half that was sent to the batchnorm. As for Group or InstanceNorm, if the features drift from one task to the next, how well these normalization techniques would be able to cope with forgetting. Overall, it seems that in designing SP-normalization the whole kitchen sync is thrown at the features. \n\n2. **Writing is not clear**: The writing of the paper is not clear and makes the paper a difficult read. The tuple notation for denoting data samples is incredibly cumbersome and non-intuitive. Sec 3.2 is not clear at all, terms are used without any definitions. For example, the functional form of the so-called structure-wise potential $\\psi$ is not provided until the end of the section making it difficult to understand what is happening in Eq. 2. Similarly, throughout Sec 3, intuitions are lacking and it seems that everything is combined in an ad-hoc way. What the authors refer to as knowledge-distillation based on ER, is simple experience replay, there is no knowledge-distillation there. \n\n\n3. **Experiments**:\n\n* Do the baselines, GEM, ER, MER etc also use the pre-trained model? This is not clear from the experiments. \n\n* Do all the baselines use the same codebase?\n\n* Could the authors compare the compute and memory cost of their method vs the baselines?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing needs improvement. Please refer to the strengths and weaknesses section above for details.",
            "summary_of_the_review": "Please refer to the strengths and weaknesses section above for details.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2509/Reviewer_zSUJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2509/Reviewer_zSUJ"
        ]
    },
    {
        "id": "xciffOPdyT0",
        "original": null,
        "number": 2,
        "cdate": 1666641616507,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641616507,
        "tmdate": 1666641616507,
        "tddate": null,
        "forum": "fxC7kJYwA_a",
        "replyto": "fxC7kJYwA_a",
        "invitation": "ICLR.cc/2023/Conference/Paper2509/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors address the problem of stability and plasticity in continual learning in a fully online manner. They do that by introducing the extraction of multi-scale feature maps from shallow and deep layers of a pre-trained model, a structure-wise distillation loss across multiple tasks, and a parallel normalization module for stability and plasticity. The proposed approach outperforms most of the current SOTA methods in regard to accuracy (ACC), stability (FM), and plasticity (LA). In addition, the authors have performed extensive experiments in their ablation study foolproofing the significance of their approach.",
            "strength_and_weaknesses": "Strengths\n1. The paper essentially describes a novel way to address the problem of catastrophic forgetting.\n   a. First, similar to the architectures used in the domain of semantic segmentation, extracting the features from shallow to deep layers becomes essential especially when extracting deep high-level features and shallow low-level features. In this task, the extraction of multi-scale feature maps helps the model learn its intrinsic features when implying the addition of the top-down module.  \n   b. Although the significance of sequential hierarchical training in the domain of continual learning has not been explored widely, the authors have used a similar concept to force the model to learn the relationship between two consecutive tasks by maintaining an angular distance. Based on their ablation study, this has shown some improvements in stability, but not so much in accuracy and plasticity. \n   c. The proposed parallel stability-plasticity normalization module tackles the variability issues of the incoming data and problems of normalization in the inference step. For example, the inconsistency in the data during training and inference can cause a significant drop in the performance of the previous task, leading to catastrophic forgetting. Here, this method finds a balance between stability and plasticity. \n2. The authors have performed comprehensive experiments against other SOTA methods. Their extensive ablation study further solidifies their claims. \n3. The authors have made use of all the most common benchmark datasets that are widely used in this area, furthering the compatibility of their approach in regard to its reproducibility. \n4. The paper is easy to follow. \n\nWeaknesses\n1. Why are the ablation studies in Tables 3 and 4 performed on ER-Ring? Also, it seems to be the case with table 6 as well with DER++. The authors\u2019 proposed approach is MuFAN. (It is either not clearly stated in the paper or there can be a  typo)\n2. In table 9, it can be observed that by using all three proposed components and data augmentation, only the best overall accuracy can be achieved. However, as stated by the authors, for tackling the stability-plasticity dilemma,  row-8 has shown significant improvement in handling stability whereas row-7 handles plasticity quite well. Therefore, what is affecting these components when used altogether?\n3. The authors have mentioned that the architecture is similar to that of U-Net and discussed the improvements over utilizing standard and bottom-up extraction methods; however, as mentioned in this work [1], it would be interesting to see the compatibility of the top-down (without multi-scale feature extraction) approach with auto-encoders in their ablation study.\n\n[1] A. Rannen, R. Aljundi, M. B. Blaschko, and T. Tuytelaars, \u201cEncoder Based Lifelong Learning,\u201d in 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Oct. 2017, pp. 1329\u20131337. doi: 10.1109/ICCV.2017.148. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The authors have shown significant improvement in online continual learning especially when there is a task-free setting in addition to outperforming most SOTA methods in a standard setting.\n\nNovelty: Although more recent approaches like [1, 2] have used knowledge distillation between models as a similarity function (some settings of these approaches differ), using a structural distillation function for hierarchical training is a somewhat mildly explored concept, the authors have used this effectively to maintain the similarity of the tasks embedding space with the model as it is learning incrementally in an online fashion. In addition to that, the encoder-based top-down approach in this paper is somewhat explored in this work [3]; however, the authors have shown that multi-scale feature extraction, similar to U-net, can improve the accuracy significantly.\n\nReproducibility: based on the datasets the authors have used, the detailed methodology, and the clarity of the work, I believe this work is reproducible. \n\n[1] M. H. Phan, T.-A. Ta, S. L. Phung, L. Tran-Thanh, and A. Bouzerdoum, \u201cClass Similarity Weighted Knowledge Distillation for Continual Semantic Segmentation,\u201d in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, Jun. 2022, pp. 16845\u201316854. doi: 10.1109/CVPR52688.2022.01636.\n\n[2] M. Kang, J. Park, and B. Han, \u201cClass-Incremental Learning by Knowledge Distillation with Adaptive Feature Consolidation,\u201d in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, Jun. 2022, pp. 16050\u201316059. doi: 10.1109/CVPR52688.2022.01560.\n\n[3] A. Rannen, R. Aljundi, M. B. Blaschko, and T. Tuytelaars, \u201cEncoder Based Lifelong Learning,\u201d in 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Oct. 2017, pp. 1329\u20131337. doi: 10.1109/ICCV.2017.148. \n",
            "summary_of_the_review": "The authors have shown improvements over SOTA methods in the online continual learning paradigm. Their claims are supported with empirical evidence. In addition, each of the components in their work are contributing to the approach. I believe this would be a good contribution. Therefore, I am leaning towards accepting this paper; however, the authors will need to clarify the issues mentioned in the weakness section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2509/Reviewer_L1XG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2509/Reviewer_L1XG"
        ]
    },
    {
        "id": "ASb_vf6Hkb",
        "original": null,
        "number": 3,
        "cdate": 1666642218340,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642218340,
        "tmdate": 1668870522267,
        "tddate": null,
        "forum": "fxC7kJYwA_a",
        "replyto": "fxC7kJYwA_a",
        "invitation": "ICLR.cc/2023/Conference/Paper2509/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper argues that a good continual learner should strike a different stability - plasticity tradeoff in online and offline CL, the former being more stable and the latter more plastic. The authors propose a new method MuFAN, which comprises 3 new components : the use of multi-scale features of pretrained models, an added cross-task distillation loss, and a new normalization technique. The authors evaluate their method on standard online CL benchmarks, and provide some ablation studies to highlight the effectiveness of the components. ",
            "strength_and_weaknesses": "Strengths : \nThe paper positions itself well in the current literature. \nThe paper is clear in detailing has many ablations to evaluate the utility of the proposed components\n\nWeaknesses : \n1. *On the stability / plasticity conclusions drawn from Figure 1*. \nThe authors argue because online methods have a lower accuracy after a training task (LA), then they are less plastic. I strongly believe that this is property is not intrinsic to online learning. Rather, this property is a direct result of **the number of training iterations spent on each task.** An easy way to increase the number of training iterations in the online setting is to simply do multiple gradient optimization steps (each with different rehearsal batches) as in ([1], [2]). It would be good for the authors to redo figure 1, where the number of iterations on a datapoint is equal to the number of epochs in the offline setting (so that online and offline methods use the same compute). \n2. *On the use of pretrained models in online CL* . \nThe authors propose a novel (to the best of my knowledge) yet quite convoluted way to leverage pretrained models. Yet, highly simple and straightforward baselines (such as a nearest-class-mean classifier in the feature space of the pretrained model, or downright finetuning of such model) are omitted. Given that there is ample evidence [3,4,5] that these methods are surprisingly effective, it is crucial that one compares to such naive approaches : if one cannot beat them, then why bother with the complexity of a new method ?\nHere is a small code snippet [6] that evaluates one such simple approach using the setting described on the Split CIFAR100 setting. **Without any training, one can boost the accuracy from 75% to 85% using solely the pretrained model!** \n3. *On the added complexity of the method*\nFrom the paper, it is unclear how many additional forward passes are required at each step. Looking at 1 and 2, it seems that the full buffer is forwarded through the model at each step ? If this is the case, it is important to note that this comes at a very high additional cost in compute, and should be properly discussed. Also, what is the cost of the additional forward passes through the old model for distillation ? Given that data augmentations are used, I am assuming hidden activations cannot be cached and have to be recomputed ? \nOn a similar note, it would be much more interesting to replace table 8 with a FLOP per iteration count for several methods. \nSimilarly, what is the additional compute cost of using a top-down module ? my understanding is that upsampling the representation before feeding it to the classifier will have a significant increase in compute cost. It would be nice if the authors could analyse this. \n4. *On the task-free version of MuFAN* \nThis version uses a hyperparameter $\\mathcal{S}$, which essentially determines the size of each task (in number of unique labels). In a \"real\" online setting, the learner does not know how long the learning stream is, nor does it know how many unique labels it will see. Therefore, how would one set $\\mathcal{S}$ a priori ? Moreover, in this section you are comparing to baselines which **do not** use a pretrained model; I don't see how this is a fair comparison\n\n\n\n[1] Drinking from a Firehose: Continual Learning with Web-scale Natural Language https://arxiv.org/abs/2007.09335\n\n[2] Online Continual Learning with Maximally Interfered Retrieval https://arxiv.org/abs/1908.04742\n\n[3] REMIND Your Neural Network to Prevent Catastrophic Forgetting https://arxiv.org/abs/1910.02509\n\n[4] A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning https://arxiv.org/abs/2210.04428\n\n[5] Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis https://arxiv.org/abs/1909.01520\n[6]\n```\nimport os\nimport PIL\nimport torch\nfrom torchvision import datasets, transforms\n\nif not os.path.exists('efficientnet_lite0.pth'):\n    os.system('wget https://github.com/RangiLyu/EfficientNet-Lite/releases/download/v1.0/efficientnet_lite0.pth')\n\nif not os.path.exists('eff_net_code'):\n    os.system('git clone git@github.com:RangiLyu/EfficientNet-Lite.git eff_net_code')\n\n# load torch model\nfrom  eff_net_code.efficientnet_lite import build_efficientnet_lite\nmodel = build_efficientnet_lite('efficientnet_lite0', 1000)\nmodel.load_state_dict(torch.load('efficientnet_lite0.pth'))\nmodel = model.cuda()\nprint(f'total of {sum(x.numel() for x in model.parameters()) / 1e6 :.2f} M params.')\n\n# remove linear head\nmodel.dropout = model.fc = torch.nn.Identity()\nmodel.avgpool = torch.nn.Identity()\nDIM = model(torch.cuda.FloatTensor(2, 3, 224, 224)).size(-1)\n\n# default transforms for the model in question\ntfs = transforms.Compose([\n    transforms.Resize(224 + 32, interpolation=PIL.Image.BICUBIC),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.498, 0.498, 0.498], [0.502, 0.502, 0.502])\n    ])\n\nds_train = datasets.CIFAR100('./', train=True, download=True, transform=tfs)\nds_test  = datasets.CIFAR100('./', train=False, download=True, transform=tfs)\nBS, n_cls, n_way = 256, 100, 5 # use n_way == 100 for the single-head task-free result\n\n\n@torch.no_grad()\ndef get_prototypes(model, dataset, n_classes):\n    protos = torch.zeros(n_classes, DIM).cuda() \n    counts = torch.zeros(n_classes, dtype=torch.int64).cuda()\n\n    loader = torch.utils.data.DataLoader(dataset, batch_size=BS, num_workers=8, shuffle=True)\n    offset = torch.arange(DIM).cuda()\n\n    print(f'processing : {(str(type(dataset)))}')\n    for i, (x,y) in enumerate(loader):\n        print(f'{i}/{len(loader)}', end='\\r')\n        x, y = x.cuda(), y.cuda()\n        feats = model(x)\n        \n        # accumulate\n        counts.scatter_add_(0, y, torch.ones_like(y))\n        idx = offset.view(1, -1) + y.view(-1, 1) * DIM\n        protos.view(-1).scatter_add_(0, idx.view(-1), feats.view(-1)).view_as(protos)\n\n    return protos / counts.unsqueeze(-1)\n\n\n@torch.no_grad()\ndef evaluate(model, dataset, protos, n_way):\n    loader = torch.utils.data.DataLoader(dataset, batch_size=BS, num_workers=8, shuffle=True)\n    n_cls = protos.size(0)\n\n    n_ok, n_tot = 0, 0\n\n    for i, (x,y) in enumerate(loader):\n        print(f'{i}/{len(loader)}', end='\\r')\n        x, y = x.cuda(), y.cuda()\n        feats = model(x)\n\n        dist = (feats.unsqueeze(1) - protos.unsqueeze(0)).pow(2).mean(-1)\n        pred = dist.argmin(1)\n        acc  = (pred == y).float().mean()\n\n        # since in CL we typically partition and eval on a subset of classes, let's mimick this\n        _, preds_in_order = dist.sort(1, descending=False)\n        pos_of_correct_answer = torch.where(preds_in_order == y.unsqueeze(-1))[1]\n\n        n_extra = n_way - 1\n\n        # if we pick any `n_extra` classes at random, what is p(correct) ?\n        p_correct = dist.new_ones(size=y.size())\n        for it in range(n_extra):\n            n_worse_left = ((n_cls - 1) - (pos_of_correct_answer) - it).clamp_(min=0)\n            n_better = pos_of_correct_answer # fixed\n            p_correct = p_correct * (n_worse_left / (n_worse_left + n_better))\n\n        n_ok  += p_correct.sum().item()\n        n_tot += x.size(0)\n\n    print(f'\\nTest Acc : {n_ok / n_tot  * 100 :.2f}')\n\nprotos = get_prototypes(model, ds_train, n_cls)\nevaluate(model, ds_test, protos, n_way)\n```",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very dense. The authors do a reasonable job explaining most of the contributions, however some parts are hard to read : for example, please consider using a different notation than $\\mathcal{X}^{1,2}_{i,j}$ : you could just list the subsets in task $i$ and task $j$ one after the other. \n\n",
            "summary_of_the_review": "Overall, given that the main contribution relies on proposing a new way to leverage pretrained models, but that key baselines are missing from this analysis, my opinion is that the paper should make these changes and be resubmitted. A proper evaluation of the computation complexity of the method is missing as well. While these two key issues are not properly addressed, my rating will stay the same. \n\nPost rebuttal : The authors have addressed some of my concerns, so I am raising my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2509/Reviewer_dVei"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2509/Reviewer_dVei"
        ]
    },
    {
        "id": "OETdDzLlfS",
        "original": null,
        "number": 4,
        "cdate": 1667312852769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667312852769,
        "tmdate": 1668850095077,
        "tddate": null,
        "forum": "fxC7kJYwA_a",
        "replyto": "fxC7kJYwA_a",
        "invitation": "ICLR.cc/2023/Conference/Paper2509/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to use a pretrained model for its multi-scale feature map for better performance in continual learning along with cross-task structure-wise distillation and a new normalization layer. The proposed method is compared to recent work including DualNet (NeurIPS 2021), CTN (ICLR 2020), MIR (NeurIPS 2019) and etc on SVHN, CIFAR100, miniImageNet and CORe50 dataset. The proposed method improves the performance over the compared methods.",
            "strength_and_weaknesses": "**Strength**\n- S1: Pretraining the model for continual learning brings a noticeable gain in performance.\n\n**Weakness**\n- W1: Pretraining using the online stream and complicated normalization scheme incurs a long pipeline for learning. One of the most important feature in continual learning is the short training time (i.e., online learning). This proposal sacrifice the efficiency for the continual learning.\n- W2: Most of the gain comes from using multi-scale feature extracted from the pretrained model, which is an existing idea (not applied to CL context yet). Gains by other proposals are marginal (at the expense of larger model than prior arts (Table. 8).\n- W3: The `pretraining' is not well suited to continual learning setup as it learns the streamed data by storing them in advance (this part is not very clear in description) for representation learning. As the continual learning setup encourages to learn the data with limited access over the tasks, the notion of pretraining may not be widely applicable in practice.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper does not clearly describe the pretraining procedure; how do they train the pretraining model - what data, how may epochs, what is the pretext task and etc.",
            "summary_of_the_review": "Given that the paper uses seemingly expensive pretraining schemes and the gain is marginal except the gain by the existing idea of using multi-scale feature of pretrained model, the method is not very interesting to be reported in the community. Moreover the description (esp., the pretraining procedure) is not clear enough to judge the value of the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2509/Reviewer_aHBY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2509/Reviewer_aHBY"
        ]
    }
]