[
    {
        "id": "UIj62BWhtr",
        "original": null,
        "number": 1,
        "cdate": 1665632769508,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665632769508,
        "tmdate": 1669601776474,
        "tddate": null,
        "forum": "iOc57X9KM54",
        "replyto": "iOc57X9KM54",
        "invitation": "ICLR.cc/2023/Conference/Paper822/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a procedure planning model which enforces the logical order. The authors claim that they can parse the goal to subgraphs and translate the knowledge into admissible knowledge. The proposed model also learns casual relationships via an SCM for procedural planning. Experimental results show the proposed model surpasses the original baselines by a large margin.",
            "strength_and_weaknesses": "## Strengths:\n1. The procedural planning task is interesting and fundamental. Through procedural planning, we can understand how the neural networks understand the task structure and generate the corresponding plans. The counteract cases are also interesting and touch the core of AI.\n2. The source code is provided, so this work might be reproducible.\n3. The idea of using causal models to prevent relational biases and keeping order in LLMs is very important and is introduced with good examples.\n4. The improvements in the experiments seem to be large. I like the illustration of the results introduced in Table 5. Sadly, some results are omitted in Table 5. On the contrary, I suggest the paper should have a large table showing the concrete procedural results to help understand each part of the model. Try always to be concrete and precise.\n\n## Weaknesses:\nI am confused by this paper's methodology a lot of times. In general, this paper could be challenged in the following positions:\n\n1. Probably, the proposed model is not technically sound. In fact, many of the components are not well-defined, and the system looks too complicated. The overall model is not ablated well and is full of tricks.\n2. The proposed model seems to borrow a lot of big concepts, such as neural-symbolic or casual models. However, the specific technical contributions are not very clear, and many problems remain unaddressed. In general, the system looks too complicated. This is not a good paper. A good paper should stick to one major point and show the advantage of this point over previous baselines.\n3. Human evaluations are not very objective and cannot be easily reproduced. I fail to see a clear motivation for using expensive human evaluations. Instead, showing a lot of generated procedures is far more beneficial than showing a lot of numbers. \n\nThe specific comments are:\n\n1. In the task definition, what are the actions, and what are the object sets? I think in real natural language, the set of actions (described in natural language) or objects can be extremely large. How can the formulation handle this? I did not see the specific usage of these variables. Probably the problem is over-formulated.\n2.  The authors use the same variable D for task D_T and confounding variable D. This is hard to decode.\n3.  I can hardly understand the implementation of the SCM. What are the input and output of the SCM? Is there any replacement (e.g. standard transformers) for this module? What is the sample output of the SCM? Is there any reference to the SCM? Take Figure 2 as an example; how does the SCM forward the reasoning path? How does the SCM encode the action and object? How does the SCM produce the output? These problems are still unclear to me.\n4. Why does procedural planning need so many steps (five-stage pipelines)? I see that motivation is to fuse different motivations (task definition, previous steps, and external knowledge). If so, why not encode this information in parallel so that the fused (e.g. concatenation) information can be used in downstream tasks?\n5. Algorithm 1 is a waste of space, and it is better to replace it with a figure (or make figure 3 better).\n6. Many concrete designs, such as the symbolic executors, are missing. It seems that the symbolic executors are trivial and can be easily learned. In other words, I do not think the \"neural-symbolic\" stuff in the title plays an important role in this system. In fact, what is neural symbolic in this line of work? Why is it important? How to connect the big concept with the experimental results with the LLMs? These questions are far more important to show than the ad-hoc model designs.",
            "clarity,_quality,_novelty_and_reproducibility": "As we have discussed in the prior section, reproducibility is good.\nIn overall, the novelty should be good because introducing casual models and neural symbolic methods should still be novel.\nThe major limitation is the clarity and the paper presentation. I tend to the acceptance part considering the good directions and good ideas. However, if I do not see a major revision in the rebuttal phase in the writing parts clarifying the related concerns, I might decide to reject this paper.",
            "summary_of_the_review": "Addressing an important task with promising novel technics. Writing needs to be improved, and more insights should be given. \n-------------------------------------\nAfter reading the responses, I think most of my concerns are addressed. Therefore I raised my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper822/Reviewer_UGe5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper822/Reviewer_UGe5"
        ]
    },
    {
        "id": "wr4E4tUUXs",
        "original": null,
        "number": 2,
        "cdate": 1666665888120,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665888120,
        "tmdate": 1669013141860,
        "tddate": null,
        "forum": "iOc57X9KM54",
        "replyto": "iOc57X9KM54",
        "invitation": "ICLR.cc/2023/Conference/Paper822/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work shows that by adding a dash symbolic reasoning to a neural model, it shows better performance w.r.t consistency and generalization. It is unclear to me how much engineering efforts are required to add these symbolic reasonings. It appears their approach is general, using a common External Knowledge Base, but the writing is confusing and I cannot tease out the details.",
            "strength_and_weaknesses": "strength : the proposed approach works. they conduct a user study where they ask crowd workers to rate which agent, one with symbolic reasoning and one without, performed better on a task, and the crowd workers preferred the agent with symbolic reasoning. this result is solid and shows evidence of the proposed approach.\n\nweakness : \n\nthe proposed method is may not be entirely novel. people have been adding symbolic reasoning to neural models for awhile, and the finding has always been : \"If we can successfully 'hack' the underlying DSL that represented the set of tasks, adding symbolic reasoning would perform well\". For instance, these works tend to follow the steps of: 1) identify a set of tasks that would be easily represented with symbolic execution, and 2) devote significant engineering efforts to construct the DSL and a symbolic interpreter to help the neural/llm model make better inferences/plans. \n\nthis work would be of significant contribution if it can show that steps 1) and 2) can be avoided by using a generic external knowledge base (as shown in figure 3). however the writing is too confusing I cannot be sure if that is the case or not.",
            "clarity,_quality,_novelty_and_reproducibility": "clarity : poor. And this is a huge problem because the writing prevented me from judging the work clearly.\n\nIn the introduction there's this block of text that reads\n\n\"adjustment (Hu & Li, 2021; Weber et al., 2020; Yue et al., 2020) are not applicable in our SCM. Instead, we build a mediator and implement it as a commonsense-infused prompt. Through the mediator, we can identify causal effects among goals and steps by investigating the indirect effect from the goals, which is essentially the frontdoor adjustment in causality (Pearl, 2009).\" \n\nWhat is it even saying? I have zero clue. What is a mediator? What is a commonsense-infused prompt? What are these \"indirect affect from the goals\" mean? What is \"essentially the frontdoor adjustment\" mean? \n\nThese are highly technical terms that mean very little unless explicitly defined. The reader tends to look for easy metaphors and intuitions on why your approach should work, and why intuitively it should work well. This passage sounds intuitive, yet it uses words that nobody know what they mean (yet), and ended up being just gibberish.\n\nThis confusion continued for the rest of the paper, making it hard for me to judge if it is worthwhile.\n\nA re-write of the intro section is warranted, with a concrete example explaining why the proposed approach should work well, without the jargons.\n\nI highly recommend the authors ask people outside of their immediate project -- walk down the hallway a few offices and knock on some doors -- ask these people to read the paper and give feedbacks, and adjust the paper based on what was confusing. \n\nquality : unclear / potentially good. The human-evaluation is clearly stated, and I can feel confident in saying \"the approach is performing better than the baseline\". However, I would also like to make an assessment on \"is this approach general? or is it domain-specific and hacky?\", this is hard to judge as the work seemed very complex with many moving parts (in figure 3 there's 5 stages), and the writing isn't clear.\n\nnovelty : unclear / potentially good. same reason as above. If this work is generalizable to different domains with very little tweaking, then it definitely has merits most prior works that brings symbolic reasonings into neural models heavily rely on a DSL, i.e. a domain SPECIFIC language, and isn't really generalizable. ",
            "summary_of_the_review": "Overall, the paper is difficult to read, and as a result I cannot judge it properly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper822/Reviewer_i21n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper822/Reviewer_i21n"
        ]
    },
    {
        "id": "aDMUKHJVdX",
        "original": null,
        "number": 3,
        "cdate": 1667101291841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667101291841,
        "tmdate": 1668993769183,
        "tddate": null,
        "forum": "iOc57X9KM54",
        "replyto": "iOc57X9KM54",
        "invitation": "ICLR.cc/2023/Conference/Paper822/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Summary:\n- Existing large language models (LLMs)  require manual exemplars to acquire procedural planning knowledge in the zero-shot setting. \n- The paper proposed a neuro-symbolic procedural PLANner (PLAN) with commonsense-infused prompts elicited from an external knowledge base (i.e., ConceptNet) to solve the pure-language-based procedural planning problem in a zero-shot manner. \n- Human and automatic evaluations on WikiHow and RobotHow show the superiority of the proposed PLAN over the prior methods on procedural planning. \n",
            "strength_and_weaknesses": "Strength:\n1. Solid and reasonable idea: Based on the observation that due to potentially biased pre-training data, pre-trained knowledge in LLMs may confound the model to make wrong decisions when asking the model to generate a procedural plan of a task, the authors proposed to apply the frontdoor adjustment in causality to build a mediator and implemented it as a commonsense-infused prompt. The prompt obtained from their neuro-symbolic-based method allows the LLM to attend to the causal entities instead of the highly correlated ones for the next step generation. \n2. Strong performance: the proposed method PLAN outperformed two recent SOTA methods LLMaP (\u201cLanguage models as zero- shot planners\u201d) and Chain (\u201cChain of thought prompting\u201d) statistically significantly. \n3. Quite thorough experimental study and analysis: for the experiments, the authors utilized several evaluation methods including human evaluations, two datasets, and several pre-trained LLMs. \n4. The paper is well written and organized.\n\nWeaknesses:\n1. Discussion on the failure cases is currently missing. In addition, the generated procedural plan of the proposed method was shown, but it would be interesting and useful for readers to see the exact intermediate outputs of the proposed framework given an actual task from the evaluation dataset, e.g., $G_s$, $P_G$, $\\hat{P}_G$, etc. In this way, readers may have a better understanding of the current capabilities of each module of the proposed framework.\n\n2. Some minor issues, e.g.:\n(1) \u201cNote that in the path T \u2192D\u2192Si \u2190Pi, Si is a collider\u2026\u201d (second line of the \u201cStage1\u201d paragraph), should it be \u201cT\u2190D\u201d?\n(2) It would be good to refer readers to the appendix for definitions of backdoor path, frontdoor adjustment, etc.\n(3) How is the Symbolic Rule Set $R$ obtained? \n(4) One more ablation experiment: what if removing the first Translation $LM_T$ and replacing $\\hat{P}_G$ with $P_G$?\n(5) Suggesting one relevant work: https://arxiv.org/abs/2205.11916 (manual exemplars not required).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: some descriptions in the paper are currently not clear enough.\n- Quality: the quality of the paper is good.\n- Novelty: the contributions of the paper are novel.\n- Reproducibility: Some additional details are required for one to reproduce the results in the paper. Partial code was provided but not the executable full code.\n",
            "summary_of_the_review": "The paper proposed to generate commonsense-infused prompts elicited from an external knowledge base (i.e., ConceptNet) to allow LLMs to solve the pure-language-based procedural planning problem in a zero-shot manner. The key idea of this paper is novel and solid. The proposed method has a strong performance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper822/Reviewer_ai1E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper822/Reviewer_ai1E"
        ]
    },
    {
        "id": "MUX63gdA7Rl",
        "original": null,
        "number": 4,
        "cdate": 1667316657229,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667316657229,
        "tmdate": 1667316657229,
        "tddate": null,
        "forum": "iOc57X9KM54",
        "replyto": "iOc57X9KM54",
        "invitation": "ICLR.cc/2023/Conference/Paper822/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes an approach for procedural planning with LLMs and casual models. The approach first builds a commonsense casual model from an external knowledge base with adjustments. Then, given a query, it builds a task-relevant subgraph, which is provided as a procedural prompt. Finally, this is translated to admissible actions for the LLM to plan. This is demonstrated on two large planning tasks, RobotHow and WIkiHow, and shown to outperform baseline planners. ",
            "strength_and_weaknesses": "\nThe paper is interesting and the topic is very relevant. Planning from LLMs is promising and grounding them in common sense as well as admissible scenes is a core challenge in this area.\nThe approach of building a large knowledge base and leveraging within an LLM is well founded.\nThe two large planning datasets are large and good environments to test in. The results show improvements across the board compared to baselines.\n \nThe paper however has a few areas for improvement. \n\n(1) The clarity could be improved. Some sections like Section 3 are quite dense and difficult to parse, particularly Section 3.1. Perhaps an earlier overview could help clarify or explicit running examples. Section 3.1 could use it\u2019s own algorithm block and potentially a zoomed in figure of the computation. Figure 3, though a nice overview, is very dense.\n\n(2) Though the performance of PLAN is stronger than baselines, it is a smaller improvement than I would think given the additional complexity and also somewhat difficult to judge how large of an improvement it is. In many of the metrics it seems PLAN outperforms by a few percentage points or in voting it wins 50% of the time. While I acknowledge that this shows that PLAN is performing better, it isn\u2019t clear from these results that it is worth the vast additional complexity compared to baselines. The authors should add an additional metric similar to executability in Huang et al., showing the actual success rate of these plans, as this is the ultimate metric we care about. A few related questions:\n* I would be interested to learn more about what the main failure modes are.\n* Some results, such as Table 1, seem to outperform baselines with larger models. I\u2019m surprised by this as I would think your approach would be particularly important to add structure when LLM\u2019s are more inaccurate. Do you have any intuition on why this might be?\n\nMinor notes:\n* \u201cThe Termination Condition is either reaching the max step t or the matching score is below threshold \u03b8. \u201c Instead of thresholding, one can compare to an end of statement token\u2019s probability.\n* Mention in the intro where the common sense external knowledge comes from (though I know it is in Figure 1).\n* How do you extract entity names from the task name?\n* \u201cshow that pre-trained knowledge (D) in LLMs confounds\u201d What is D here?\n* \u201cWe describe the implementation of such frontdoor adjustment in Section 3.1.\u201d but section 3.1 is 2 pages long, be more specific.\n* Table 3 bolds 0.433 in the button right, though GPT3 + Chain outperforms it with 0.471\n* \u201cPLAN surpasses powerful baselines (Chain of Thoughts (Wei et al., 2022) and Zero-shot Planner (Huang et al.)) by large margins on both the original and the counterfactual samples.\u201d Are these baselines powerful? They don\u2019t have external information except their prompts.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See strengths and weaknesses.",
            "summary_of_the_review": "The paper presents and interesting and novel approach to an impactful area of research. PLAN also shows performance on two large and varied environments. However, the paper is quite dense and could be improved in clarity. The results show PLAN outperforms baselines, but by a small margin given the additional complexity, and the results do not include success rate metrics that may be most crucial for understanding the gains in performance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper822/Reviewer_UWNS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper822/Reviewer_UWNS"
        ]
    },
    {
        "id": "g4aAedSQ036",
        "original": null,
        "number": 5,
        "cdate": 1667457578150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667457578150,
        "tmdate": 1667457578150,
        "tddate": null,
        "forum": "iOc57X9KM54",
        "replyto": "iOc57X9KM54",
        "invitation": "ICLR.cc/2023/Conference/Paper822/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a principled algorithm for incorporating symbolic information related to text generation using an LLM. The idea is justified using a causal analysis, so the algorithm is motivated by the front door criteria. The external information is extracted by entity extraction and extracting related information from ConceptNet. The results are concatenated to the prompt and interpreted as conditioning. The human evaluation and metrics indicate the method leads to improvement in three different LLMs.\n",
            "strength_and_weaknesses": "Strengths\n- Principled motivation using causality\n- Concrete technical solutions challenges for integrating with the selected source of information\n- Ablation study shows the impact is not trivial.\n- Idea is novel as far as I know.\n\t- For instance, this survey appeared after the submission. It does not mention work as specific as this submission: Feder, Amir, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, et al. \u201cCausal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond.\u201d Transactions of the Association for Computational Linguistics 10 (October 2022): 1138\u201358. https://doi.org/10.1162/tacl_a_00511.\n\nWeaknesses\n- It's not clear which is the current causal model used at each point. (See question below)\n- The paper might be assuming P_i is the only path, but the existence of other paths to be blocked it's not discussed. \n- The argument loses a bit in the adaptation plus other decisions on what to retrieve.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "My main concern is that the causal analysis is both very thorough but it's also confusing. After trying the causal models in the tool https://causalfusion.net/app, I'm obtaining different estimands. The reason seems to be that the model in Fig 2 is updated after each iteration, fixing the values of the previous variables. That makes it hard to understand some of the statements. For instance, eq (1 and 10) says\nP(Pi=p | do(S_{i-1})) = P(Pi=p | do(S_{i-1}))\nHowever, using that tool I obtain something like\nP(P_3 \u2223 do(S_2))=\u2211_{P_2\u200b,T} P(P_3\u200b\u2223S_2,P_2,T)P(P_2,T)\nP(P_2 \u2223 do(S_1))=\u2211_{P_2\u200b,T} P(P_3\u200b\u2223S_2,P_2,T)P(P_2,T)\nbut if I fix T and P_{i-1}, that is what would happen in greedy decoding with an LLM, then I should obtain P(Pi=p | do(S_{i-1})).\n\nSomething similar happens with Eq (9)\n\nHowever, this makes the analysis hard to follow as the new causal models are not referred in the equations.\nThere is also a comment on P_{i-1} being copied into P_i that makes things more complicated.\n\nQuestion:\n- Am I right about these concerns?\n\n**Let's keep in mind that the algorithm is just an implementation.**\n**In principle, what we want is the prediction of all the do() compounded.**\n\nPerhaps the appendix is the place to clarify this point, explaining:\n- what's exactly the new causal graph after each iteration.\n- why does it make sense to simplify the graph.\n- Clarify which causal graph is related to each equation.\n\nMore questions:\n- does the causal analysis holds given de \"adaption\" that was necessary? I'd like to see a more clear causal criticism of that situation.\n- Does the use of the front criterion holds given that there could exist other words related to the task?\n- Where would this break? The results are just positive but there is not a detailed discussion on limitations.\n\nI think the work is interesting and solid.\nMy only concern is the one I just mentioned on the clarity of the causal analysis.\n\n",
            "summary_of_the_review": "The paper provides a causality-justified algorithm for retrieving information about a task. Although some points seem to break the causal argument, I suspect this is indeed the explanation for better performance. Some aspects of the causal analysis are hard to follow.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper822/Reviewer_W99u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper822/Reviewer_W99u"
        ]
    }
]