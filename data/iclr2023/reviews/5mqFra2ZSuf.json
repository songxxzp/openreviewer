[
    {
        "id": "WhCvbJQPlWT",
        "original": null,
        "number": 1,
        "cdate": 1665689956816,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665689956816,
        "tmdate": 1668202597704,
        "tddate": null,
        "forum": "5mqFra2ZSuf",
        "replyto": "5mqFra2ZSuf",
        "invitation": "ICLR.cc/2023/Conference/Paper4750/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper considers the problem of solving finitely many non-linear\nequations, under the assumption that a solution exists. The key idea\nis projecting the current iterate onto the hyper-surface defined by\nthe quadratic approximation of the current equation of choice. The\nfirst proposed method (SP2-GLM) considers the case where the non-linear equation is the\ncomposition of some loss with a linear function. The second method\n(SP2+) further linearizes the quadratic approximation. Some\nconvergence theory is derived for the two methods (Section 4). Extensions to the\ncase where there is an \"approximate\" solution are considered (Section\n5). And experiments are demonstrated on several problems (Section 6).",
            "strength_and_weaknesses": "\n# Strength\nThe paper enjoys the following strength:\n\n- The paper considers an interesting problem of solving non-linear\nequations, which is potentially important and relevant to some\nreaders of ICLR.\n\n- The paper is written clearly, and the proposed method consists of a\n  simple idea.\n\n- The paper covers several aspects of the proposed methods, including\n  convergence theory, experiments, and extensions to the case of\n  *quadratic with slack*.\n\n- Full proofs are in the appendix in a single file, which makes the\n  the paper easy to navigate.\n\n# Weakness\nThe weakness of the paper manifests itself from several aspects:\n\n- The assumption of Proposition 1 is too strong, which makes it not very\n  interesting. First, it is a bit weird that the global minimizer\n  $w^*$ has already shown up in the definition of $f_i$ (21), which\n  is quite artificial. Second, $f_i$ is already quadratic,\n  and local quadratic approximation does not make too much sense.\n  Third, $H_i$ is positive semidefinite, which means that the\n  quadratic constraint $f_i(w)=0$ is equivalent to the linear\n  constraint that $w$ should lie in the null space of $H_i$ (if $H_i$\n  is rank-deficient). This makes the convergence analysis trivial, as\n  it is reduced to the situation that we already know (e.g., from the\n  analysis of the Kaczmarz method). Finally, the discussion on the case\n  where $H_i$ is invertible is misleading: In this case $H_i$ is\n  positive-definite, the equation $f_i(w)=0$ has a unique\n  solution, and one does not need to do a projection as solving\n  $f_i(w)=0$ directly would suffice.\n- Proposition 2 is a bit mysterious. Since it follows directly from\n  Gower et al. (2021), that should be mentioned explicitly right after\n  Proposition 2. Besides this, SP2+ itself is also mysterious and not\n  very well motivated: In (18), intuitively, what is the benefit of\n  considering a linearization of a quadratic approximation, instead of\n  a linearization of the original function? That question also applies\n  to Section 5.\n\n- The two propositions stand in contrast to the name SP2 and very\n  much also the purpose of the paper: Under the given assumptions the\n  paper is essentially analyzing first-order methods (in a trivial\n  way, based on prior works). The paper would be significantly\n  strengthen if there is some solid analysis that reveals superlinear\n  or quadratic rates of the proposed idea, as is typical for\n  second-order methods.\n\n- Section 5 would be a powerful technical contribution of the paper\n  if it were more well-motivated. For example, what is the benefit of\n  having two different slack formulations (L1 and L2)? What is the\n  reason of resorting (30) for solving the L1 slack problem? After\n  all, they seem to perform very similarly in the experiments.\n\n\n- There are several sub-optimal aspects in experimental evaluation.\n  First of all, the comparison to Newton's method is unfair, as it is well known\n  that standard Newton's method is not always globally convergent. A slightly\n  more considerate option is to compare with the damped Newton's\n  method (as described in Boyd's book), which is guaranteed to\n  converge, at least in the (strongly) convex case. Second, the\n  plots should not be *function values versus epochs*, but it should\n  be *function values versus running time*; the reason is that the\n  methods under consideration have different time complexities at each\n  iteration. Third, by just reading the paper, the reader has no idea\n  how large the dataset in each experiment is. Since Sections 1\n  and 2 make it an important point to design incremental methods that\n  can work with mini-batches, it is important to run experiments on\n  large datasets for which traditional offline methods run out of\n  memory (e.g., Newton's method), otherwise, there is no reason to believe that online methods\n  could perform better than offline methods. E.g., online methods\n  would have very large variances and could easily fail in the case\n  where some equation is far from being satisfied at the global minimizer.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and novelty are good. \n\nThere are some drawbacks in terms of the quality, as mentioned in \"Weakness\".\n",
            "summary_of_the_review": "I found the proposed method simple (a strength in my point of view), and some technical contributions are made (say in Section 3.1 and Section 5). But I also found Section 4 regarding convergence analysis and the experimental section unsatisfactory, as mentioned above in \"Weakness\". \n\nAt this moment, I made a borderline score, \"marginally below the acceptance threshold\".\n\nNote that I am not an expert in this line of research. I will re-evaluate the paper based on the comments from other reviewers and the rebuttal.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4750/Reviewer_fkL2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4750/Reviewer_fkL2"
        ]
    },
    {
        "id": "LimqCYqp4D",
        "original": null,
        "number": 2,
        "cdate": 1666574925294,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574925294,
        "tmdate": 1666574925294,
        "tddate": null,
        "forum": "5mqFra2ZSuf",
        "replyto": "5mqFra2ZSuf",
        "invitation": "ICLR.cc/2023/Conference/Paper4750/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper consider the interpolation settings. It proposes to use a similar approach as the stochastic Polyak step size (solving the interpolation equations). However, instead of using the first-order approximation, this paper uses the local second-order information of the model. The authors then listed several approaches for this, namely SP2 (closed form solution applying for generalized linear models), SP2+ with approximating the solution to subproblem, and other versions with slack formulation.  ",
            "strength_and_weaknesses": "The strengths of this paper come from the nice framework to approximate the second-order function. It also proposes various different ways to solve the quadratic problem, including discussions using a generalization of the interpolation property. \n\nThere are potentially several limitations of this method. Firstly, it is not clear whether the quadratic subproblem can be solved efficiently in a big-data machine learning setting. In addition, most proposed methods only can approximate the solution of those problems, which poses some difficulty in analyzing these method theoretically and practically. Secondly, since the convergence theory is for quadratic functions, it is unclear if this method would have theoretical guarantee in most of the ML setting with interpolation (which is the motivation for the problem). Finally, the experiment and theoretical setting does not seem to match each other. \n\nOther comment: The detailed algorithms should be stated separately in a box to avoid any confusion for the readers. ",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of using stochastic Polyak step size and second-order approximation is not novel, but the authors did combine them relatively well. ",
            "summary_of_the_review": "This paper has some good contributions, though the practicality of this method is in question. I would like to hear from the authors regarding the limitations of this papers. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4750/Reviewer_DF7u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4750/Reviewer_DF7u"
        ]
    },
    {
        "id": "8dn66LZPpH-",
        "original": null,
        "number": 3,
        "cdate": 1666765889609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666765889609,
        "tmdate": 1666765889609,
        "tddate": null,
        "forum": "5mqFra2ZSuf",
        "replyto": "5mqFra2ZSuf",
        "invitation": "ICLR.cc/2023/Conference/Paper4750/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "SP (Stochastic Polyak step size) can be interpreted as a method specialized to interpolated models since it solves the interpolation equations. SP can be interpreted as a projection into a stochastic-constrained linearization of the objective function. The main idea of this paper is that it extends this constrained linearization to a quadratic approximation of the objective (SP2). They show a closed-form solution for this minimization of some objectives common in machine learning such as quadratic loss. In general, there may not be a solution for SP2 minimization, so they introduce SP2+ that contains two steps: 1- minimizing on a linearized objective and 2- minimizing on a quadratic approximation of the objective. They provide a convergence guarantee for SP2(+) for a quadratic loss in an interpolation setting. \n",
            "strength_and_weaknesses": "Empirically their proposed methods shows outperform other existing optimization methods in the literature. \nWeakness: their theoretical analysis holds just for too limiting settings. ",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: The main novelty of this paper is extending the linearized constrained into quadratic constraints and proposing closed-form solutions for various losses. ",
            "summary_of_the_review": "Comments: \n1- The empirical results are based on the number of epochs used through training. However, the iteration costs for SP2 and SP2+ are greater than SP or SGD. Therefore the right criteria would be wall clock time. So I suggest to change all x-axis changes to the wall-clock time so a fair comparison is doable. \n\n2- In your theoretical result, specifically Prop 1 and 2, if we assume d > n, and H_i is non-zero only in the (i,i) element. This setting satisfies your assumptions for Prop1 and 2. For this setting, \\rho would be 1 and your analysis won\u2019t show a convergence. \n\n3- Since your SP2 method needs a Matrix-vector product, it would be useful to show that for a simple loss how some can implement it efficiently. \n\n4- It will be nice to first give the original formula of SP method before eq 4.   \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4750/Reviewer_etGz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4750/Reviewer_etGz"
        ]
    }
]