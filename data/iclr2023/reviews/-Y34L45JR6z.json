[
    {
        "id": "MPGyyDubEGu",
        "original": null,
        "number": 1,
        "cdate": 1666023316749,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666023316749,
        "tmdate": 1669750638389,
        "tddate": null,
        "forum": "-Y34L45JR6z",
        "replyto": "-Y34L45JR6z",
        "invitation": "ICLR.cc/2023/Conference/Paper2806/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses how online RL can benefit from an existing dataset and/or a policy created by offline RL. The suggested method proposes that both the offline policy (fixed) and the online policy (to be trained) provide action candidates, which are then evaluated using a Q-function (to be learned online). The evaluations are converted into selection probabilities and the action to be executed is drawn accordingly. The method is being tested on several D4RL benchmark tasks.",
            "strength_and_weaknesses": "**Strengths**\n- The paper deals with a problem that is relevant for practical applications\n- The method is very interesting. I find it surprising that it works.\n\n**Weaknesses**\n- The, in my opinion, central aspect of how actions are selected is presented somewhat scarcely.  I am not sure that the method can be reliably re-implemented using the explanations provided.\n\n**Further suggestions for improvement**\n- I think the calculus of how the selection probabilities are calculated from the Q-values of the candidate actions should be explicitly presented.\n- The temperature parameter $\\alpha$ probably has central influence on the functioning of the method. It would be interesting to see this influence through experiments.\n- Also the entropy of the two policies may have a significant influence. It would be interesting to see this influence through experiments.\n- Some sentences could be worded more concisely. For example, sentence \u201eAfter learning the offline policy, we use it as one candidate policy in a policy set, and further learn another policy that will be responsible for further learning as an expansion to the policy set\" seems a bit awkward and thus difficult to understand.\\\nI stumbled over \u201elimiting its applicability to many practical scenarios\u201c, wouldn't \u201elimiting its applicability in many practical scenarios\u201c or \u201ewhich limits its applicability in many practical scenarios\u201c be better?\n- Why do you write \"e.g.\" in italics while \"et al.\" is not italic? This seems inconsistent to me.\n\n\u201eto a a number\u201c\\\n\u201eonehot\u201c -> \u201eone-hot\u201c\\\n\u201ethe policy set set as\u201c\\\n\u201eq-learning\u201c -> \u201eQ-learning\u201c\\\n\u201eShould i run\u201c -> \u201eShould I run\u201c\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\nshould be improved as stated above\n\n**Quality**\nGood\n\n**Novelty**\nGood\n\n**Reproducibility**\nshould be improved as stated above\n",
            "summary_of_the_review": "A surprisingly simple approach to solve an important problem. The description of the approach should be more explicit in parts and thus more reproducible. The language should be improved in parts, errors should be fixed. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2806/Reviewer_NB3z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2806/Reviewer_NB3z"
        ]
    },
    {
        "id": "AdIbsqPAt_",
        "original": null,
        "number": 2,
        "cdate": 1666295325492,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666295325492,
        "tmdate": 1670902589512,
        "tddate": null,
        "forum": "-Y34L45JR6z",
        "replyto": "-Y34L45JR6z",
        "invitation": "ICLR.cc/2023/Conference/Paper2806/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the offline-to-online RL setting and proposes policy expansion (PEX) by constructing the stochastic online policy: a mixture of the offline policy (trained using the offline data) and the online policy (trained with the online rollouts). In the experiments, the paper shows that PEX has some performance gain over some very recent methods that consider a similar setting. ",
            "strength_and_weaknesses": "# Strength \n* The idea of policy expansion, i.e., instead of using the offline policy as a warm start, one can use the offline policy as a checkpoint and combine it with the online policy (which could warm start from the offline training) and still query the checkpoint online, seems a general framework. Also as the paper already points out, the PEX framework could work agnostically to the particular online and offline RL methods. \n* The experiments study is on the general D4RL benchmark, and the paper shows that PEX is competitive in most environments and outperforms some recent baselines such as JSRL and BT.\n* The ablation study gives us more insights into some specific design choices such as the benefit of transferring the offline Q-function and why we should freeze the offline policy. \n\n# Weakness\n* The work seems to overlook a few more previous works in this online RL with offline dataset setup, such as [1,2,3,4]. \n* [5] is also very relevant to this paper's setting, and seems even more relevant than the current baselines. I wonder why [5] is not considered as in the experiment section. Also, I believe the paper should contain more explanation on why the current baselines are significant enough (I might be wrong but the baselines seem both from unpublished papers?)\n* From the technical side, there are a few points that confuse me:\n 1. when does $P_W$ make sense? First there is minor ambiguity in the algorithm: when it says that sampling $a \\sim \\tilde \\pi$ according to (6), in the definition of $P_w$, $\\textrm{Q}$ seems arbitrary although we can assume in the context of the algorithm it may refer to $Q_{\\phi}$. However, $P_W$ is only a valid weight if $Q_{\\phi}$ is accurate, otherwise, it seems just some random heuristic to select which policy to execute. But if $Q_{\\phi}$ is accurate, then we should be done because an accurate $Q_{\\phi}$ should implies a near optimal policy and thus we don't need to rollout and collect data anymore. Adding on this, there is no explanation of Fig.9, which records the usage of the two policies: it seems like the coefficient stays almost the same during the entire training, does this mean the $Q_{\\phi}$ is already accurate at the beginning so minor updates of $Q_\\phi \\implies$ no changes in the amount of policy switching?\n\n2. Another issue is why we should ever rollout the offline policy $\\pi_\\beta$. The ablation shows that it is beneficial to include the offline dataset into the online replay buffer, which makes perfect sense. However, the action take by $\\pi_\\beta$ is just a reflection of the offline data distribution, because the pessimism/penalty applied in the offline RL algorithms. Thus why should we rollout with $\\pi_\\beta$ because it does not seem to help with exploration/increasing data diversity?\n\n* Finally, the stochastic policy constructed with both the offline policy/expert policy and the online policy seems relevant to the DAgger algorithm [6], with the different being that DAgger is using imitation learning loss while PEX is using RL loss. Nevertheless, it seems fair to include a discussion about such stochastic policy approaches. \n\n[1] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, John Agapiou, Joel Z. Leibo, and Audrunas Gruslys. Deep Q-learning from demonstrations.\n\n[2] Stephane Ross and J Andrew Bagnell. Agnostic system identification for model-based reinforcement learning.\n\n[3] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations.\n\n[4] Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rotho \u0308rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards.\n\n[5] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement learning with offline datasets.\n\n[6] Ross, St\u00e9phane, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall clarity of the paper seems good, except some notation could be more clear in some specific context (such as the definition of $\\textrm{Q}$). \n\nRegarding reproducibility, the link to the code in the paper does not seem to work for me.",
            "summary_of_the_review": "Overall, there are several flaws in the current version of the paper: 1. insufficient literature review; 2. baselines not strong enough (at least no argument to show that the baselines are strong enough); 3. some technical part includes heuristic that may seem rather random. Thus I think the current version of the paper does not deliver enough contribution to the community and thus would recommend a reject at the current stage. \n\n===========================\nraised my score after the rebuttal",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2806/Reviewer_EsUG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2806/Reviewer_EsUG"
        ]
    },
    {
        "id": "3d191jkVZTz",
        "original": null,
        "number": 3,
        "cdate": 1666565821015,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666565821015,
        "tmdate": 1666565821015,
        "tddate": null,
        "forum": "-Y34L45JR6z",
        "replyto": "-Y34L45JR6z",
        "invitation": "ICLR.cc/2023/Conference/Paper2806/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new offline-to-online finetuning approach that proposes to maintain both the policy learned offline and the new online policy during online-finetuning. The authors first train the offline policy and Q-function, select the policy from a categorical distribution with the learned Q-values during online data collection, and optimize the online policy using data sampled from both the offline dataset and the newly collected online samples. The offline policy is kept frozen during the finetuning step. The authors show that doing so can alleviate the distributional shift and over-conservatism issues when switching from offline training to online training with a single policy. Empirical results suggest that the method can outperform prior method IQL and its variants on the D4RL benchmark.",
            "strength_and_weaknesses": "Strength:\n\n1. The idea of using a policy set to reduce the distributional shift issue and conservatism when switching from offline to online training is neat.\n2. The empirical results are pretty impressive. the ablation studies are also quite thorough, clearly suggesting the benefits of different components of the algorithm.\n\nWeaknesses:\n\n1. I think the authors should compare the method to more offline-to-online works such as Lee et al., 2021, Yang et al,. 2021, Lu et al., 2022, Zheng et al., 2022 and etc. I think it is important to see if the method can do better than approaches that are not like IQL.\n2. The story of using a policy set seems a bit disconnected to the main point of the paper. The main idea of the paper is essentially to train a different online policy and maintain the offline policy for exploration whereas the policy set idea is more than this. I think the authors should make the writing more clear and to the point rather than making too many less relevant statements.\n3. The authors don't provide much theoretical insight of the method.\n4. The error bars of the experiments are quite high. I wonder if the authors should run more random seeds / do hypothesis testing to make sure the results are statistically significant.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall easy to follow but makes many less relevant claims such as expanding policy set. The originality of the work is good. The reproducibility is unclear as the code is not provided.",
            "summary_of_the_review": "Based on my comments above, I vote for a weak accept of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2806/Reviewer_2rLA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2806/Reviewer_2rLA"
        ]
    },
    {
        "id": "47YrRAFDAPg",
        "original": null,
        "number": 4,
        "cdate": 1666689984576,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689984576,
        "tmdate": 1670454274983,
        "tddate": null,
        "forum": "-Y34L45JR6z",
        "replyto": "-Y34L45JR6z",
        "invitation": "ICLR.cc/2023/Conference/Paper2806/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a scheme of policy expansion (PEX) to bridge offline and online RL. Instead of directly fine-tuning the policy learned by offline dataset during online interaction, PEX freezes the learned policy and expands the policy set with a newly added policy. This new policy is optimized during the online learning phase. Then, a single policy is derived from the policy set, where action candidates are sampled by each policy and then the final action is selected by a resampling distribution. In the D4RL benchmark, PEX overall outperforms all of the baseline algorithms.\n",
            "strength_and_weaknesses": "[Strengths]\n1. This work provides a simple yet effective scheme for offline-to-online RL. The proposed method is easy to implement.\n2. In the experiments, PEX significantly outperforms baseline algorithms.\n\n[Weaknesses]\n1. In the experiments, comparisons with existing methods for offline-to-online RL are missing, e.g. [1,2].\n2. To claim that PEX is a general framework, rather than an IQL-specific algorithm, it would be great to demonstrate the effectiveness of diverse combinations of PEX and existing offline RL algorithm backbones. For example, it would be great to see the performance of CQL+PEX or FisherBRC+PEX and whether PEX can consistently outperforms the baselines even for those different backbones.\n3. The proposed method is designed heuristically, without theoretical justification.\n\n[Questions]\n1. In the PEX framework, the offline training phase is to learn a Q function and a policy, where the policy should be frozen while the value should be fine-tuned (i.e. transfer Q). I am wondering why value-finetuning is beneficial but policy-finetuning is detrimental to offline-to-online performance. It would be great to provide a more detailed explanation about this.\n2. In Figure 2, why is the performance of PEX in halfcheetah-random better than in halfcheetah-medium-replay even though medium-replay dataset would contain more high-reward experiences?\n3. It would be great to see the result using Q-transfer only without the policy expansion, i.e. initialize the Q-function with the pretrained one, and run an online RL algorithm using the Q initialization.\n4. How sensitive is the algorithm with respect to the hyperparameter alpha?\n\n\n[1] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020\n\n[2] Lee et al., Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble, 2021\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written and easy to follow. The idea of policy expansion and its combination with adaptive policy composition seems novel, but it is a rather heuristically designed method, rather than a principled one. ",
            "summary_of_the_review": "The idea of policy expansion is interesting, but it lacks theoretical justification. For the experiments, it would be great to make a comparison with more baselines (see Weaknesses part) and to see the generality of the proposed PEX framework by conducting experiments with different offline RL backbones.\n\n== post-rebutal\nThank you very much for your detailed responses. The added experiments are also greatly appreciated. I raised my score accordingly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2806/Reviewer_U35N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2806/Reviewer_U35N"
        ]
    }
]