[
    {
        "id": "voHaKrxze_K",
        "original": null,
        "number": 1,
        "cdate": 1666401880252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666401880252,
        "tmdate": 1666401880252,
        "tddate": null,
        "forum": "yLv6eSBmA-",
        "replyto": "yLv6eSBmA-",
        "invitation": "ICLR.cc/2023/Conference/Paper1904/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to detect adversarial samples in tree ensembles without affecting either the model's structure or its original performance. Since the existing adversarial defense method may affect the model's natural performance, this paper's method enables the users to decided whether to apply defense or not. The proposed method extracts a new representation of a dataset based on the structure of the given tree ensemble to understand its behavior on normal examples and to detect adversarial examples. The authors conducted experiments to verify the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1. The authors conducted many experiments on different datasets to show the effectiveness of the proposed method. \nWeakness:\n1. In general, the proposed method is kind of over complicated. The authors split the original dataset into 4 parts and the tree model is only trained with S_T. Then the tree model itself's performance may be worse than before due to lack of data.\n2. I think the rationale of model design in this paper is kind of unclear. More explanation and ablation studies are needed to show why each part is necessary. For example, in data representation, why we use the task in Algorithm 1 as representation? If the output is the whether s_i and s_j agree on the condition in n_k, are we simply predicting whether a specific feature's values of s_i and s_j are at the same side of n_k's threshold or not? \n3. As the detector is a XGboost ensemble, it is also susceptible to adversarial attacks. If the attacker has access to this detector model, they may also be able to fool it and consequently fool the whole design. \n4. I am concerned about the black box attacks used in the experiments and they may not be strong enough. I would suggest the authors to use the MILP method in Kantchelian et al 2016 as it is the strongest method for attacking tree ensembles (theoretically it is the optimal attack). An implementation of that method can be found in Chen et al 2019's Github Repo, which is compatible with XGBoost's data structure.",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation and high level structure of the method is well presented. But the proposed method is kind of complicated and the rationale is not well explained and it seems that there are many hyperparameters, which may lead to some difficulties for reproduction. ",
            "summary_of_the_review": "In general, I think the authors made a good amount of effort in design and experiments. But the proposed method is kind of too complicated and the rationale of this complex design is not well-explained. So I recommend a borderline reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1904/Reviewer_WNyf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1904/Reviewer_WNyf"
        ]
    },
    {
        "id": "dwdJVQ6Zz8N",
        "original": null,
        "number": 2,
        "cdate": 1666434873016,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666434873016,
        "tmdate": 1666434952375,
        "tddate": null,
        "forum": "yLv6eSBmA-",
        "replyto": "yLv6eSBmA-",
        "invitation": "ICLR.cc/2023/Conference/Paper1904/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors designed a method to detect adversarial examples against decision tree ensembles. The basic idea is to extract features from decision treed and use its representations on train a classifier to detect normal and adversarial example.",
            "strength_and_weaknesses": "Strength\n1. The authors evaluate their algorithm on 18 databases with diverse sources and the databases have different properties, e.g., binary values and image etc,\n\n2. The authors claim that their method is state-of-the-art.\n\nweakness:\nQ1)  The authors claim that \u201cMost of the research is focused on adversarial attacks targeting neural network models because of the nature of their continuous learning space\u2026\u201d It is inaccurate statement. We study adversarial example against NN because 1) NN has been outperformed all other methods on many datasets, 2) Its performance is very high, in some cases, even outperforming human experts, 3) DNN has a lot of application and commercial values. However, decision trees are not comparable DNN in all these aspects. \n\nQ2) The authors claim that \u201cTree-based models continue to be very popular (Nielsen 2016)\u201d. 1) the reference is 6 years ago, and it is a master thesis. Please give better justification on this statement.\n\nQ3) The authors mention that \u201cOf the eleven datasets tested, seven shown a decrease in accuracy\u201d. Decrease in accuracy is not necessary a problem. The question is how much. \n\nQ4) Section 3,2 N1 is not defined although it is understandable.\n\nQ5) Similar idea, extracting information from classifier to train an adversarial example detector has been proposed for DNN. They also have been broken. There is no theory to support that the proposed method is secure. \n\nQ6) Since a lot of methods and tools have been developed for NN for classification, explanation and secure them, the necessity for using decision tree is weakened. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Q1: Presentation quality is poor. The figure quality is very low, not up to the standard. The description of the algorithm, in particular, section 4.1 and 4.3 and Fig. 1 and 2, are very low. I cannot follow some parts of the algorithm description. \n\nQ2: In terms of novelty, similar idea has been tested on DNN and has been broken. \n\nQ3: I cannot follow the algorithmic description due to the quality of presentation. Thus, to me, it is not reproducible.\n\nQ4: I recommend the authors asking a senior researcher to read and check their paper carefully.\n",
            "summary_of_the_review": "The presentation quality is not up to the standard, Many part is unclear.\n\nThere is not theory to support that the proposed method is secure.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1904/Reviewer_yzr9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1904/Reviewer_yzr9"
        ]
    },
    {
        "id": "cZuhrGfEcI",
        "original": null,
        "number": 3,
        "cdate": 1666584027043,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584027043,
        "tmdate": 1666584027043,
        "tddate": null,
        "forum": "yLv6eSBmA-",
        "replyto": "yLv6eSBmA-",
        "invitation": "ICLR.cc/2023/Conference/Paper1904/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies adversarial robustness of decision trees, under the typical L_p norm attacks. The idea is to generate a representation data set, and then train a neural network to detect anomalies. Experiments seem to suggest that this approach works well.",
            "strength_and_weaknesses": "The topic seems novel - indeed that most of the current research focuses on vision research and thus deep nets.\n\nOn the other hand, from my understanding of this work, it does not seem to consider adaptive attacks -- namely adversarial attacks that takes into consideration of the proposed defense and then attack. For example, what happens that the attacker can try to generate attacks that bypass the proposed embedding network/detector?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing is not good and reads like some very rough draft. In fact, I feel that it does not read like a ``normal\" ICLR paper I usually came across these days (no offense, just stating my feeling)",
            "summary_of_the_review": "The lack of considering adaptive attacks seems a major flaw, and I am not sure I am convinced by the experiments.\n\nAlso -- just that decision tree is used a lot does not necessarily justify that we should study the norm-based adversarial attacks for it (in vision, this motivation is very naturally justified), some real world examples why this is meaningful would help",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1904/Reviewer_9gBn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1904/Reviewer_9gBn"
        ]
    },
    {
        "id": "Gn4hM1HDzh",
        "original": null,
        "number": 4,
        "cdate": 1666703393432,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703393432,
        "tmdate": 1666703393432,
        "tddate": null,
        "forum": "yLv6eSBmA-",
        "replyto": "yLv6eSBmA-",
        "invitation": "ICLR.cc/2023/Conference/Paper1904/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper describes a method to detect adversarial samples without affecting either the target model structure or its original performance. By using representation learning based on the structure of the tree ensembles, the claim is of better detection rates than SOTA. They claim the approach is better than using the original representation of the dataset to train an adversarial detector.",
            "strength_and_weaknesses": "Strengths:\n1. Table1 - the variety of datasets for experiments is appreciated\n2. The appendix part gives more details on individual sub-section which is helpful\n3. A different look away from crowd of NN works - traditional interpretable ML + tweaks put to good use\n4. Good to see usage of UMAP instead of tSNE\n5. Hard work has been put wrt the experiments\n\nWeakness:\n1. Logic behind feature threshold in n_k\n2. Any logic for the 2-node setting in contrast to a m-node setting (seems like a mix of RANSAC and KNN)\n3. Fig . 2 a - suddenly sigmoid came in pic - need figure explanation\n4. Logic behind -  We extract four new sub-datasets\n5. Embedding are not clear\n6. Results need proper analysis - very little efoort put here\n7. The abstract, conclusion should be to the point - this is the problem, what others have done and their gaps, this is what we have done with numbers to support.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThis paper gives details of the algorithm step very lucidly.\n\nQuality:\nThe quality of experiments is good. The technical depth however is a bit lacking.\n\nReproducibility:\nThe authors have given enough references and description to reproduce results.\n\nNovelty:\nIt is unclear in the way the paper is presented, what non-obvious methods can be claimed as novelty. Stitching of existing known concepts or algorithms in a pipeline cannot be claimed as a main novelty - I expect a subsection listing of key contributions and hypothesis.",
            "summary_of_the_review": "The paper in general needs some rework for ICLR  - please follow below.\n\nSuggestions:\n1.Rewrite the paper to bring in the main contributions to light and clearly explain the reseach gaps in SOTA where this paper is adding value.\n2. Can use help of graph representations to explain the equation variables.\n3. If the algorithm sub-modules can be mapped to graph theoritic standard problems, it will be good. Not sure about the time complexity of the task for large datasets and practical aspects.\n4. Random node (sample) selection can be done smartly for better results.\n5. Can some notes be written for explainability.\n6. List down limitations of approach clearly\n\nMiscellaneous:\n1. The usage of XGBoost 2016 is a bit old - LightGBM (faster) and CatCBoost (more control) could have been tried.\n2. Why numbering of lines is not done - difficult to review and refer the text for correction.\n3. In this work, we present a technique to detect adversarial evasion attacks against tree-based classi\u0002fiers and mainly boosting ensembles which heavily used. - reword - grammatically incorrect.\n4. Unclear line - Our primary motivation for this work is to create a decision tree ensemble defense against adversarial\nattacks that do not affect the model itself, allowing the model owner to decide if they want defense\napplied to their existing model and fine-tune it.\n5. For p \u2208 N1 t , For p = \u221e --- format with commas for ease of reading\n6. Fig 1, 2 font size is very small - need to zoom a lot - let it take the \\linewidth (redraw if possible)\n7. We define a supervised task like so: - grammar\n8. Where are equation numbers? Hard to refer 0 use latex equation.\n8. as our metrics; for multi-class cases - break the sentence please\n9. Friedman test on the ROC-AUC, Nemneyi post-hoc test - explain",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable. The docoloc plagiarism check is 6% which is excellent.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1904/Reviewer_KhPV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1904/Reviewer_KhPV"
        ]
    }
]