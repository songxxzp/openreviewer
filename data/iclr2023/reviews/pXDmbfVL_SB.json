[
    {
        "id": "C9D5Cf-T72u",
        "original": null,
        "number": 1,
        "cdate": 1665758770152,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665758770152,
        "tmdate": 1666624621733,
        "tddate": null,
        "forum": "pXDmbfVL_SB",
        "replyto": "pXDmbfVL_SB",
        "invitation": "ICLR.cc/2023/Conference/Paper5515/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an encoding method that replaces the standard positional encoding in transformers. The label-based order encoding method achieves strong generalization to sequences longer than training ones.\n\nThis paper also analyzes two-layer causal transformers to learn multiple algorithmic operations. It finds that multi-level task learning improves when more attention heads at deeper layers.\n\nThe paper also finds that these models have signs of systematic decomposition within tasks and exploitation of shared structure across tasks.",
            "strength_and_weaknesses": "***Strength***\n\n- This paper proposes a simple yet effective new method that has improved over a state-of-the-art and widely used method.\n- It analyzes how transformers address the generalization problem.\n- It finds how to improve the performance (e.g., putting attention heads at deeper layers) and signs of systematic phenomenons.\n\n***Weakness***\n\n**The weakness mainly comes from the concern of whether the results and findings in the particular setting still hold in more complicated cases.**\n\n**1. The advantage over conventional positional encoding**\n\nThe conventional positional encoding was designed for natural language inputs.\nFor example, it contains information on relative position or distance in input, as the context window is helpful for natural language.\nThe experiment in this paper does not have natural language input, and the tasks are not much related to relative position or distance in input.\nSo it is not very convincing that the advantage still exists in other tasks, e.g., natural language processing.\n\n**2. Disentangled item representation**\n\nThe items contain shape, color, and texture.\nHowever, the item units indicate the value in each feature dimension.\nSo the item representations are already disentangled, e.g., color and shape values are in different input nodes.\nAddressing entangled data (e.g., the images of the items) is a critical expectation for systematic generalization.\nThe model behavior on disentangled data may not be the same as on entangled data.\n\n**3. Model design**\n\nIt is not very convincing that findings in two-layer transformers will naturally extend to deeper transformers.\nThe limit of depth may prevent the model from having complicated behaviors.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThis paper is clear to read.\nHowever, the words in the figures are too small.\n\nQuality:\n\nThere are concerns about the task, data representation, and model designs (mentioned above).\n\nNovelty:\n\nThere are novelties in proposing a new encoding method, analyzing with two-layer transformers and multiple findings.\nHowever, whether they are still valid in more complicated cases is unclear.\n\nReproducibility:\n\nThe paper does not mention (anonymized) source codes.",
            "summary_of_the_review": "This paper proposes a new encoding method that outperforms a widely used one and has informative analyses and findings.\n\nHowever, the reviewer does not recommend acceptance because of the abovementioned weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5515/Reviewer_ygnx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5515/Reviewer_ygnx"
        ]
    },
    {
        "id": "megJ_DHKBWK",
        "original": null,
        "number": 2,
        "cdate": 1666601877320,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601877320,
        "tmdate": 1666601877320,
        "tddate": null,
        "forum": "pXDmbfVL_SB",
        "replyto": "pXDmbfVL_SB",
        "invitation": "ICLR.cc/2023/Conference/Paper5515/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors explore Transformer's systematic generalization in algorithmic tasks, including copy, reverse, and hierarchical group or sort operations on an input sequence. The authors create a set of tasks and show that a two-layer Transformer successfully learns these tasks and generalizes to sequences longer. Particularly, a random label-based order encoding method, in place of the positional encoding, improves the systematic generalization of Transformer on the studied tasks.",
            "strength_and_weaknesses": "# Strengths\nThe paper provides a thorough analysis of a small Transformer on a set of algorithmic tasks.\n\n# Weaknesses\n1. The paper is of limited novelty. The major technical novelty is the proposed random label-based order encoding method. However, there lacks an explanation and insights why it works.\n\n2. The authors do not conduct any experiment on the well established datasets, like SCAN. Besides, there is no baseline in this work. The authors should have comapred with Transformers using relative positional encoding (Csordas et al, 2021).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has limited novelty.",
            "summary_of_the_review": "The paper is of limited novelty and lacks comparison with previous methods. Therefore, I recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5515/Reviewer_yZ2V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5515/Reviewer_yZ2V"
        ]
    },
    {
        "id": "JvhMstv8w0",
        "original": null,
        "number": 3,
        "cdate": 1666659625103,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659625103,
        "tmdate": 1666659625103,
        "tddate": null,
        "forum": "pXDmbfVL_SB",
        "replyto": "pXDmbfVL_SB",
        "invitation": "ICLR.cc/2023/Conference/Paper5515/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper introduces a novel positional encoding that improves transformers' systematic (length) generalization capabilities on a set of algorithmic tasks, including copying, reversing, sorting, and grouping. Concretely, the paper replaces the learnable position embedding method with random labels from longer sequences, thus allowing the model to encode the longer sequences with familiar labels. The paper empirically demonstrates that the proposed method achieves better length generalization performance than learnable and sinusoidal position embeddings. Finally, the paper analyzes the attention matrices of the trained models and shows that they implement the algorithmic operations required to solve the corresponding tasks.",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper proposes a novel position embedding method that increases length generalization on a series of algorithmic tasks compared to learnable and sinusoid position embeddings. To that end, the paper introduces a new suite of algorithmic tasks and considers the single-task and multi-task settings, which are interesting. Moreover, the paper conducts an in-depth analysis of the model behavior on the different tasks and shows that, e.g., a two-layered transformer trained on the sorting task learns to first sort shapes and then the other features.\n\n**Weaknesses**\n\nThe paper fails to discuss and compare to relevant related work:\n* The paper does not compare its position embedding method to ALiBi encodings (Press et al., 2021), which were proposed to increase the length generalization capabilities of transformers on natural language processing tasks.\n* The paper does not compare its position embedding method to the Neural Data Router (Csord\u00e1s et al., 2022), which is an extension of the transformer architecture (consisting of shared layers, gating, geometric attention, and directional encodings) that significantly improves the systematic generalization capabilities of transformers on a wide range of algorithmic tasks.\n* The paper does not discuss the length generalization benchmark introduced by Del\u00e9tang et al. (2022), which evaluates transformers on a highly related set of tasks, including reversing, copying, and sorting. In particular, Del\u00e9tang et al. (2022) assess the length generalization capabilities on significantly longer sequences (training on lengths up to 40 and evaluating on lengths 41 to 500). Given the accuracy decrease over sequence lengths (see Figures 2 D) and 6 C)), I am not convinced that the proposed encodings meaningfully increase length generalization for substantially longer sequence lengths.\n* It would be interesting if the paper would discuss recent work showing that transformers can learn positional information without position embeddings (Haviv et al., 2022). \n\nMoreover, while the paper shows that its proposed position embedding increases the transformer's length generalization on a set of algorithmic tasks, it does not evaluate its effectiveness on natural language processing tasks. In particular, it would be interesting to see if the proposed embedding can also be used to \"train short and test long\", as demonstrated by the ALiBi encoding (Press et al., 2021).\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is generally well-written and easy to follow. However, several aspects of the paper are unclear:\n* Is there a quantitative difference between multi-hot and one-hot encodings of the input sequence? In principle, the multi-hot encodings could be reformulated as one-hot encodings. To the best of my knowledge, using multi-hot encoding is quite nonstandard.\n* How do the grouping and sorting tasks differ? Is the order of the grouped items irrelevant? If yes, how is that evaluated?\n* Are the labels randomized at every training step or just once per sequence?\n* Does the label-based encoding correspond to the original learned encoding for sequences of length 50, given that the item labels range only from 0 to 49?\n* Why does the paper consider item *and* label prediction, which is quite nonstandard, as the paper states itself? Also, why does the model have to predict the task token in the multi-task setting if the task is provided as input?\n\nMoreover, there are a few minor mistakes:\n* Abstract: analysis -> analyses\n* Figure 5: cross -> across\n\n**Quality**\n\nAs mentioned in the weaknesses section above, the paper fails to discuss and compare to a series of highly related works, which calls into question the practicality and validity of the results.\n\n**Novelty**\n\nTo the best of my knowledge, label-based encodings have not been proposed by prior work.\n\n**Reproducibility**\n\nThe paper does not provide sufficient details of the experimental setup to reproduce the results. In particular, the sampling process used for label assignment is not specified. Code is not provided.\n",
            "summary_of_the_review": "Given the insufficient comparison to related work, which calls into question the practicality and validity of the results, I do not recommend accepting the paper in its current form.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5515/Reviewer_1ohG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5515/Reviewer_1ohG"
        ]
    }
]