[
    {
        "id": "T7B_i9hto8",
        "original": null,
        "number": 1,
        "cdate": 1666489599451,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666489599451,
        "tmdate": 1666489733490,
        "tddate": null,
        "forum": "NBES8BZ5wnZ",
        "replyto": "NBES8BZ5wnZ",
        "invitation": "ICLR.cc/2023/Conference/Paper6005/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an efficient transformer architecture, a skeleton transformer (SKTformer), for modeling long sequence data. It contains two main components: a smoothing block to mix information over long sequences through Fourier convolution, and a matrix sketch method that simultaneously selects columns and rows from the input matrix for efficient self-attention. Theoretical and experimental analyses of the proposed method are provided and its performance is validated compared to other Transformer variants on several tasks.",
            "strength_and_weaknesses": "[+] This paper proposes a novel idea for modeling long sequences and its performance is well-validated through extensive experiments. \n\n[+]  The idea of using a subset of rows or columns for self-attention and combining both is interesting. It reduces computational complexity and also improves overall performance. It seems meaningful that the proposed model is not only computationally efficient but also robust against noise. \n\n\n[-] The choice of models for comparison is not consistent across Tables. For example, Nystroformer and Performer show good train speeds in Table 4 but are not included in Table 2. There is a tradeoff between performance and computational complexity, so a clearer explanation of why the chosen methods are used in each experiment will make the comparison fairer. \n\n[-] Because the advantage of this model is that it has high computational efficiency as well as good performance, I think it is good to add models with higher performance than the models proposed in the training speed experiment as a baseline model.\n\n[-] The description of the theoretical analysis could be improved. For example, Lemma 1 is included in the last part of section 3.1: skeleton attention, but it\u2019s not well explained in the context of 3.1. In addition, since the concept of mu-incoherence might not be familiar to some readers, it\u2019d be better to introduce it near Lemma 1 in the same section, before using it in a later part. It's defined in the supplementary but not mentioned in the main manuscript. \n\n[-] Table 6 is not referred to elsewhere in the manuscript. \n\n[Q]. The choice for the value of hyperparameters such as s1 and s2 in combination with other hyperparameters seems to affect the performance much. Is there a guideline or suggestion about an efficient search strategy? \n\n[Minor] \n- In abstract: villain -> vanilla\n- Section 4.4, 2nd line: SKTformer achieve -> SKTformer achieves\n- Section 5, 1st line: n robust and efficient transformer architecture -> a robust and efficient transformer architecture\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written overall. It might be better to re-organize the theoretical analysis in a more coherent way, apart from the model architecture, for better readability. Regarding reproducibility, the code is publically available, and implementation details are also provided in the supplementary materials. ",
            "summary_of_the_review": "The novelty of the proposed method for efficient self-attention attention and other mechanisms introduced could inspire the readers. Also, the authors have conducted many experiments for validation. My concern is mostly on the selection of comparison methods shown in some of the experiments.  If these concerns can be resolved, I think it's worth being introduced at this conference.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6005/Reviewer_6UUx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6005/Reviewer_6UUx"
        ]
    },
    {
        "id": "m8JBCTxqEy5",
        "original": null,
        "number": 2,
        "cdate": 1666548927791,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548927791,
        "tmdate": 1669673739618,
        "tddate": null,
        "forum": "NBES8BZ5wnZ",
        "replyto": "NBES8BZ5wnZ",
        "invitation": "ICLR.cc/2023/Conference/Paper6005/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Through multiple iteration of rebuttal, the authors have provided empirical evidence to address my concern. I am willing to raise my score to 6, conditioned on polishing the method section. The following are my original review:\n\nThis paper proposes to sample rows and columns of key and query matrices to approximate attention matrix in transformer layers. The authors argue that if these matrices are smooth enough, then the downsampled matrix should retain most information. Therefore, the authors propose to smooth input by convolving it with some learnable kernel.",
            "strength_and_weaknesses": "Clarity is a major concern. The theoretical arguments are decorative but of little relevance and significance. In fact, the lemmas harm clarity. They do not help answer the key question: \"why should we smooth the key matrices?\" We all know that smoothed matrices are easier to approximate via sampling. However, after all, approximating the matrix is not the purpose of learning representations.\n\nEven if we assume smoothing is really helpful, there is still major unclearness in method development. Section 3.2.1 aims to design an efficient smoothing method. However, it is very hard to read. In specific, I don't see why Eq (5) can approximate Eq (4). The $S$ matrix in Eq. (5) essentially applies an averaging pooling mask along the feature dimension. Its effect is unexplained. Note that lemma 2 and 3 do not provide an answer.\n\nAlso, the smoothing is applied to the input, whereas eventually we need smoothed keys and values. There are linear transforms that map input to keys and values. does smoothed input necessarily lead to smoothed keys and values?\n\nMinor: Lemmas are for proving major theorems. But here the authors mainly want to argue for correctness. I'd suggest change \"lemma\" to \"proposition\".",
            "clarity,_quality,_novelty_and_reproducibility": "See clarity issues above. With unclearness, it is hard to judge correctness.",
            "summary_of_the_review": "This paper has major clarity issues and cannot justify its correctness.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6005/Reviewer_9G8D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6005/Reviewer_9G8D"
        ]
    },
    {
        "id": "o_1SyhoRZk",
        "original": null,
        "number": 3,
        "cdate": 1666663557420,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663557420,
        "tmdate": 1666663557420,
        "tddate": null,
        "forum": "NBES8BZ5wnZ",
        "replyto": "NBES8BZ5wnZ",
        "invitation": "ICLR.cc/2023/Conference/Paper6005/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a novel SKTformer for long sequence modeling, which reduces the complexity of the vanilla transformer based on the Skeleton approximation. There are two major novelties: Skeleton attention and Fast Fourier Transformation-based smooth convolution. Theoretically, analysis is provided to show the proposed model could reduce the complexity. The experimental results show that SKTformer could outperform existing methods to a certain degree.",
            "strength_and_weaknesses": "Strengths:\n1. The proposed Skeleton attention and Fast Fourier Transformation based smooth convolution is novel.\n2. Theoretical analysis is provided to demonstrate SKTformer could reduce complexity and retain sufficient information.\n3. The experimental results show that SKTformer could outperform SOTA methods to a certain degree.\n\nWeaknesses:\n1. It is necessary to perform t-tests for SKTformer since it does not consistently outperform SOTA methods.",
            "clarity,_quality,_novelty_and_reproducibility": "This writing is clear in general. \n\nThe quality is good.\n\nThe skeleton attention and smooth convolution are novel.\n\nThe reproducibility is good since implementation details are well-documented.",
            "summary_of_the_review": "In general, this paper introduces a novel SKTformer to reduce the complexity of the Transformer. Theoretical analysis is provided and the empirical results demonstrate the effectiveness of SKTformer. However, SKTformer does not consistently outperform SOTA baselines. Thus, it is necessary to conduct t-tests. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6005/Reviewer_ELnS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6005/Reviewer_ELnS"
        ]
    },
    {
        "id": "V4PuY6RQjD7",
        "original": null,
        "number": 4,
        "cdate": 1666817115544,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666817115544,
        "tmdate": 1669234686370,
        "tddate": null,
        "forum": "NBES8BZ5wnZ",
        "replyto": "NBES8BZ5wnZ",
        "invitation": "ICLR.cc/2023/Conference/Paper6005/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new approach to apply transformers to Long Sequence Data, called SKTFormer. The approach combines a CUR matrix approximation technique, a Fourier-convolution based smoother, and a convolution stem to avoid over-smoothing. The authors apply their method to a number of problems, showing promising performance; they also report ablation study results, analyzing the importance of different components of their method.",
            "strength_and_weaknesses": "Strengths:\n- The paper addresses a highly important problem\n- The proposed method shows good results on a sufficiently wide range of problems\n- Experiments are thorough, and ablation results are reported\n- The paper is generally clearly organized\n\nWeaknesses:\n- The most impressive results are achieved when hyper parameters of the proposed method are freely varied for each subtask. It may be creating an overly optimistic impression of the model performance. I'd prefer a higher emphasis on the performance of the fixed hyperparameter version (i.e. SKTformer (r, s1, s2 = 8)). For the benchmark used (Long Range Arena), it goes contrary to the established approach.\n- Performance variability from run to run is not reported fully (its reported in the appendix for LRA substasks, but not for the overall performance, and is not referenced in the main text).",
            "clarity,_quality,_novelty_and_reproducibility": "** Clarity **\n\nGenerally, the paper is well organized and is well-written.\n\nIt does have a surprising amount of typos, but those can be easily resolved.\n\nAdditionally, sometimes the phrasing is a little suboptimal. For example, in the abstract, the authors say that their method \"addresses\" the tradeoff between computational cost, information retention, and noise. It's not clear what is meant by that. The paper does not propose a method to avoid this tradeoff completely; it might improve upon previous results, but, in my understanding, it does not address the tradeoff. Perhaps something along the lines of \"improves upon the previous results/attempts to negotiate this tradeoff\" would work better in this context.\n\nAdditionally, table numeration in appendix is very confusing. Specifically, table 10 is listed in text noticeably later than table 11. I suggest that tables are re-numbered.\n\nOverall, as long as these issues (especially typos) are resolved, the clarity matches the standards of the ICLR conference. I trust the authors to fix simple typos, so they did not affect my overall assessment.\n\n** Quality **\n\nThe experiments are well - designed and well-executed. I find it preferable to highlight the fixed hyperparameter SKTformer (r, s1, s2 = 8) performance more, since this is more consistent with how results were previously reported in the literature (e.g. in the Long Range Arena,. and in Luna: Linear Unified Nested Attention, the authors do not fine-tune major hyperparameters to each subtask).\n\nAdditionally, run-to-run variability is not included. Since even for the fixed hyperparameter model, some hyperparameter search was still performed (number of epochs, batch size, dropout, etc.), it's highly important to report results averaged over different runs & their variability. Methodologically, this is my main concern.\n\nTo be precise, it is reported, but hidden. It's only in the \"Appendix G\" that the authors mention the run-to-run variability and the fact that the results they report are averaged over 5 runs. This information (and the variability of the overall result, not only of each separate task) should be included in Table 1.\n\n** Novelty **\n\nThe novelty/originality of this paper is not its main selling point, as the method is largely a combination of pre-existing techniques, but it is sufficient & up to the standards of the ICLR conference.\n\n** Reproducibility **\n\nAs far as I can judge, the detailed model description & supplementary materials make this paper comply with highest reproducibility standards.\n\n** Typos and other suggestions **\n\nGenerally, the amount of typos in this paper is a little concerning (I provide a sample below). I would suggest a thorough proofread of the paper before the final version is submitted.\n\nExtensive studies over both Long Range Arena (LRA) datasets, six time-series forecasting show that SKTformer significantly outperforms -> \n..., and six time-series\n\nwithout having to suffice the linear complexity w.r.t. sequence length ->  without having to sacrifice the linear complexity w.r.t. sequence length\n\nWe propose SKTformer, n robust -> We propose SKTformer, a robust\n\nbuild the column self-attention as follow -> build the column self-attention as follows",
            "summary_of_the_review": "Generally, the paper addresses an important problem, and runs a number of reasonably planned experiments, supplementing them with theoretical results. \n\nIts main issue is the way experimental results are reported. Contrary to how most previous papers approached the topic (see Long Range Arena original paper, section 3.2 \"Philosophy Behind the Benchmark\"), authors fine-tuned the major hyperparameters for each subtask in the Long Range Arena. Given that these hyperparameters do have a major impact on performance (see table 7), such reporting may create a confusing picture of how good the method actually is, and make it harder to compare it with both previous and future results. The authors report a fixed hyperparameter version, but only as a secondary metric, and I suggest that it is reported more clearly either along or instead of the fine-tuned one, e.g. in the abstract and other parts throughout the paper. \n\nAdditionally, even though Appendix G gives the performance variability for each subtask in LRA, it is not mentioned in the main text and does not give the variability of the overall result, which is the most important part (I strongly suggest that it's included for reader's convenience, especially since it can be calculated assuming independence at no additional computational cost).\n\nOverall, however, the pros outweigh the cons, and I believe that the paper is above the acceptance threshold, although, unless the issues above are fixed, I believe that it's borderline. I will stay open to adjust my assessment based on the rebuttal and other reviewer's comments.\n\n** UPD **\n\nI have read other reviews and the authors' responses. They alleviate many of the concerns voiced by me and other reviews. In my opinion, I can not quite improve the score to an \"8\", but I would have given the paper a \"7\" if I could.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6005/Reviewer_FKSC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6005/Reviewer_FKSC"
        ]
    }
]