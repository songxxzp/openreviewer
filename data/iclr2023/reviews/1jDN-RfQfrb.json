[
    {
        "id": "aGlnIp-IUy",
        "original": null,
        "number": 1,
        "cdate": 1666445954346,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666445954346,
        "tmdate": 1669470978418,
        "tddate": null,
        "forum": "1jDN-RfQfrb",
        "replyto": "1jDN-RfQfrb",
        "invitation": "ICLR.cc/2023/Conference/Paper5434/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides empirical investigation of transformer by constructing a synthetic reasoning task -- LEGO (Learning Equality and Group Operations). The LEGO tasks consist of group operations and a chain of reasoning. The major purpose of the synthetic experiment is to provide a controlled setting and unveil what transformer learns. To do so, the paper conduct a series of experiments on the both classic setting (training and testing distribution are the same) and the OOD setting (out of distribution) and they found\n\n(1) Classical generalization happens all the architecture.\n\n(2) OOD generalization depends on the architecture and data preparation.\n\n(3) The non-extrapolating models (architecture fails for OOD) learns short-cut solution.\n\nThe author also propose hardcode attention layer that incorporates knowledge over the dataset that seems to work well on LEGO. ",
            "strength_and_weaknesses": "Strength. The paper provides solid contribution towards understanding the Transformer architecture. By constructing a synthetic benchmark, it provides a series experiment that provide new insight and confirm existing observations. \n\nWeakness. There is no major weakness.\n\nMinor comment. The related work section can be improved, in particular, there is a previous line of work \"unveils\" transformer over regular language ( in particular dyck language) that should be cited and discussed. Examples include [1] (and see the reference therein), which studies the dyck language and provide some theory and empirical investigation on what transformer learns.\n\n[1] Self-Attention Networks Can Process Bounded Hierarchical Languages. Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan. ACL'2021\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Yes",
            "summary_of_the_review": "The paper provides solid contribution to the literature and I vote for acceptance.\n\n\nSome minor questions over \"shortcut\":\nThe paper finds transformer sometime learns short-cut solution to the reasoning tasks. However, it is unclear to me why this is a bad thing. I feel it is not equivalent to spurious feature because some reasoning tasks involves short-cut solution or parallelizable solution, e.g., it is possible that a linear size circuit only has depth $O(\\log n)$ (i.e., do not follow the chain of reasoning). Some further discussions should be a good addition on the finding!\n\n\n\n------------\n\nPost rebuttal\n\nI have read the rebuttal and my evaluation remains.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5434/Reviewer_7SQ8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5434/Reviewer_7SQ8"
        ]
    },
    {
        "id": "FKAskJ-vD9O",
        "original": null,
        "number": 2,
        "cdate": 1666628341500,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628341500,
        "tmdate": 1666628341500,
        "tddate": null,
        "forum": "1jDN-RfQfrb",
        "replyto": "1jDN-RfQfrb",
        "invitation": "ICLR.cc/2023/Conference/Paper5434/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a synthetic task, LEGO, and uses it to understand how Transformers operate. The task is in following a chain of simple reasoning, in particular for simple mathematical operations with variables valued either +1 or -1 and expressions of the kind a=+b or a=-b. With this task, the authors consider BERT\u2019s and ALBERT\u2019s abilities to generalize to longer inputs. They find specific attention heads (e.g., local attention patterns) and propose modification with some of the heads fixed to these patterns.",
            "strength_and_weaknesses": "While the paper introduces the new task to help understanding Transformers, the task does not lead to new observations not widely known before. Specifically, it\u2019s well-known that the most important Transformer attention heads play interpretable roles, including positional ([1], [2], [3], etc). This was used extensively to improve/simplify models: [4] fixes attention window size for some of the heads and gets improvement for low-resource settings (similar to this work), [5] develops hard-coded attention without learned parameters, to name a few. From this perspective, (1) neither finding these attention patterns nor proposing fixed attention is novel; (2) doing it on a synthetic task when this has already been done before for different real-world tasks does not make much sense.\n\n\n[1] ACL 2019: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.\n\n[2] EMNLP 2019: ADaptively Sparse Transformers.\n\n[3] BlackBoxNLP 2019: What does BERT Look at? An Analysis of BERT\u2019s Attention\n\n[4] EMNLP findings 2020: Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation\n\n[5] ACL 2020: Hard-Coded Gaussian Attention for Neural Machine Translation\n",
            "clarity,_quality,_novelty_and_reproducibility": "See comments above.",
            "summary_of_the_review": "The paper introduces a synthetic task to analyse Transformers. In the analysis, finds specific attention patterns and proposes model modification with some of the heads being fixed. Most of the observations done in the paper and model modification have already been done before without resorting to synthetic tasks.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5434/Reviewer_3VMG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5434/Reviewer_3VMG"
        ]
    },
    {
        "id": "q0bBMB__t1d",
        "original": null,
        "number": 3,
        "cdate": 1666691060590,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691060590,
        "tmdate": 1666691060590,
        "tddate": null,
        "forum": "1jDN-RfQfrb",
        "replyto": "1jDN-RfQfrb",
        "invitation": "ICLR.cc/2023/Conference/Paper5434/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "LEGO is a synthetic reasoning task that targets chain reasoning. Think Markov chain, where the current state only depends on the previous state. They specifically evaluate BERT and ALBERT. ALBERT is particularly interesting since weights are shared across all layers in ALBERT and is hypothesized that it may be particularly suited for this type of reasoning. Indeed, it outperforms BERT. Various ablations are carried out to understand why.",
            "strength_and_weaknesses": "The task and setup are framed well. Ablations such as stochastic depth length extrapolation, pretraining, and probing classifier are creative ways at understanding the inductive biases of the networks.\n\nThe task seems not that specific to transformers. I do wonder how other RNNs would fare.",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper clear. I appreciated the language of abstract algebra, but I am not sure if all readers will. I thought the figures were good. The experiments are well done and should be simple to reproduce. There is not much originality in the modeling, but the task and analysis are novel",
            "summary_of_the_review": "I think the paper adds to the understanding of the various inductive biases that different transformer architectures have. There is much interest int the mathematical ability of transformers. I would like to see more models benchmarked on this task.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5434/Reviewer_s6py"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5434/Reviewer_s6py"
        ]
    },
    {
        "id": "OxFq6AWS-C",
        "original": null,
        "number": 4,
        "cdate": 1666697228544,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697228544,
        "tmdate": 1666697228544,
        "tddate": null,
        "forum": "1jDN-RfQfrb",
        "replyto": "1jDN-RfQfrb",
        "invitation": "ICLR.cc/2023/Conference/Paper5434/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a synthetic task that aims to capture aspects of multi-hop reasoning, and uses it to characterize the behavior of Transformer models. It finds that pre-trained transformers learn the task more rapidly, and attributes this to specific patterns of attention in the network. Introducing mechanisms that emulate these specific attention patterns helps close this gap in performance without the need for expensive pre-training. Models struggle with extrapolating to longer chains of reasoning than required during training. This deterioration can be mitigated by repeating layers with the same parameters as well as training with stochastic depth. ",
            "strength_and_weaknesses": "Strengths:\n- The proposed task is useful as a tool for understanding some ways in which a Transformer can perform multi-hop reasoning\n- The proposed operationalization of requiring length extrapolation is effective at disentangling issues with positional encoding from the question of whether the model can extrapolate to longer chains of reasoning \n- Experiments help provide an understanding for how pre-training, as well as stochastic depth, help improve certain aspects of the models' capabilities\n- Hardcoded attention patterns are shown to be just as effective for solving this specific task\n- The writing of the paper is clear and well-structured\n\nWeaknesses:\n- One of the strengths of the attention mechanism is that it can learn any attention pattern a task may require, and the multi-head version can learn when (at what layer) and how much to use each pattern. The hardcoded attention patterns seem sufficient for the proposed synthetic task -- almost by construction of the LEGO task. There are some preliminary findings that they may also be effective for masked language modeling and at least SQuAD, but this direction is underexplored and perhaps not all of the necessary components for real NLP tasks are captured.\n- The proposed task is perhaps not the best test-bed of *pre-training* specifically, because the approach used is to pre-train on natural language but then test on synthetic expressions. It would not be surprising for language-specific reasoning to not undergo transfer in this case, potentially leaving only the most general of structural patterns, like the attention patterns revealed in this work.\n- Overall, it's not clear how much of the Transformers' capabilities are stressed by the proposed synthetic task. It could be that understanding multi-hop computation in the style of the LEGO task is the key for explaining current behaviors and proposing improvements. Alternatively, multi-hop reasoning may just not be a crucial component of many NLP tasks as currently formulated, or LEGO might not be capturing the actual character of chain-based reasoning as required for real-world task.",
            "clarity,_quality,_novelty_and_reproducibility": "See above\n\nMinor:\n- Regarding Transformers that repeat layers: in addition to ALBERT, it would be helpful to cite other instances such as Universal Transformers (Dehghani et al)",
            "summary_of_the_review": "My overall recommendation is based on weighing the thorough and well-written exposition/investigation of the LEGO task with the potential downsides of any synthetic benchmark, namely that it may or may not capture the key elements of actual real-world tasks, and can therefore be limited in its impact.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5434/Reviewer_drii"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5434/Reviewer_drii"
        ]
    }
]