[
    {
        "id": "aTUH2x-e9s",
        "original": null,
        "number": 1,
        "cdate": 1666448681918,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666448681918,
        "tmdate": 1666448681918,
        "tddate": null,
        "forum": "W-VfwHzA2yg",
        "replyto": "W-VfwHzA2yg",
        "invitation": "ICLR.cc/2023/Conference/Paper1045/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The research is on developing an adaptive optimization algorithm called AdaDQH, which proposes a new technique to approximate the Hessian matrix in the second-order gradient descent scheme in a computationally efficient manner. The authors demonstrate the optimizer\u2019s performance on datasets from Natural Language Processing, Computer Vision, and Recommendation Systems to show faster convergence and better generalization. The paper also provides theoretical proofs on bounds on regret and time complexity to explain the trade-off between generalization, time and computational resources. Weights in a deep-network at iteration are updated as:\n   w_(t+1) = w_t \u2212 \u03b1_t B_t^(\u22121t) m_t\nwhere,\u03b1_t is the step-size, represents a linear combination of the gradients \u2207_wL(w_t) , and B_t, called as the precondition matrix is an approximation of the Hessian H_t . Several previous second-order optimization algorithms like AdamW, AdaBelief, AdaHessian have developed methods to approximate . The current algorithm, AdaDQH, selects B_t as: B_t^2 = diag(EMA(s_1s_1^T, s_2s_2^T, s_ts_t^T ))/(1 \u2212 \u03b2_2^t))\nwhere, EMA represents an exponential moving average, and is given by a recurrent relation in terms of s_t.\nThe structure of is adopted for bias-correction is similar to Adam. In addition to approximating the Hessian, the research also proposes a way to switch between an adaptive version and EMA version for the gradients.\n\n",
            "strength_and_weaknesses": "1. The paper provides mathematical details to support time-complexity and computational analysis, and bounds for establishing convergence.\n2. Results for the paper cover problems from different areas in Machine Learning and Optimization, adding to credibility and usefulness in the future.\n3. An essential point covered in the paper is: Trade-off of computations, time and generalization.\n4. It would be interesting to note the performance of AdaDQH on highly non-convex optimization problems  (i.e. functions with several local minima) like Ackley and Schwefel. This would help test the idea of whether AdaDQH is able to converge to global minima or not. Challenge problems from CEC 2019 would also help.\n5. An important question arises: Is AdaDQH able to converge to global minima of few functions wherein the current state-of-the-art algorithms are only able to make it to local minima? This would make it a significant improvement over the current state-of-the-art algorithms.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The research is a novelty since it showcases a different mechanism to estimate the Hessian matrix.\n2. The paper is well-presented to include the algorithm, mathematical details and proofs for convergence, and\nlastly, empirical results on different datasets.\n3. It is clear in its approach and covers most of the essential details for completion o\n4. The paper quality is good since it has come up with a optimizer, with potential applications in all sub-fields of Machine Learning.\n5. Seems reproducible\n",
            "summary_of_the_review": "Research paper is well-presented and produces a novel technique to approximate the Hessian for optimization. Overall, it\u2019s an important paper to be considered in Machine Learning.\n\nAdditional Comments:\n\nWhat is s_t, b_t and beta_2?\nCould different notations be used for vector and scalar?\nThe paper updates B_t with max(B_t , delta*I). Why is this so?\nHow was equation 3 obtained from equation 2?\nIs it possible to give mathematical guarantees on autoswitch? Say when it switches and in which conditions does it occur.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1045/Reviewer_cyVQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1045/Reviewer_cyVQ"
        ]
    },
    {
        "id": "G8uLcKx2ck",
        "original": null,
        "number": 2,
        "cdate": 1666577316310,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577316310,
        "tmdate": 1666577316310,
        "tddate": null,
        "forum": "W-VfwHzA2yg",
        "replyto": "W-VfwHzA2yg",
        "invitation": "ICLR.cc/2023/Conference/Paper1045/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies an Adam type optimizer, adaDQH, by designing a new rule to update $v$. The idea is i) using gradient difference to approximate (diagnalized) Hessian, and ii) using a max in the denominator of to switch AdaDQH to SGD when necessary. Theoretical results and numerical tests are carried out to support the convergence of proposed AdaDQH.",
            "strength_and_weaknesses": "**Strength:**\n- S1. The idea to approximate Hessian is interesting.\n- S2. On tested tasks, AdaDQH exhibits improved performance. \n\n**Weakness:**\n\n- W1. (Novelty.) Two of key ideas, Hessian approximation, and auto switch have been used in the literatures but not well cited. For example, the idea of using gradient difference to estimate Hessian appeared in http://proceedings.mlr.press/v70/zheng17b.html, and the auto switch shares similar idea to https://arxiv.org/abs/1902.09843. In addition, although the idea of approximating Hessian is interesting, the motivating equation in two lines after Alg. 1 is less convincing. First, the value of $\\theta$ is taken as 1, however, the reason is unclear. If it is taken from mean value theorem, the choice of $\\theta$ can be different in different cases. Second, the reason to set $\\Delta w$ to $e_i$ is also unclear, and it looks like this is not the case when updating the model weights. It can be more reasonable if the authors can plot the difference between the proposed Hessian estimation and real Hessian. Another question is that how well does EMA approximates the true gradient. The lack of justification makes the claim \"better estimated Hessian\" less convincing. \n\n- W2. (Numerical experiments.) It looks like the models for evaluating the proposed algorithm are not well chosen. For example, it is standard to use ResNet50 rather than ResNet18 on ImageNet. And ResNet18 can easily achieve 93 - 94\\% test accuracy on CIFAR10, while the tested ResNet20 and ResNet32 struggle on the same task. The choice of testing models makes the numerical results less competitive.\nAnd why are some results in Table 4 are training on P100 and some on V100? \n\n\n(Minor 1) Does the experiment in Figure 1 take stochastic gradient into account?\n\n(Minor 2) The theory uses diminishing $\\beta_{1,t}$ while the main algorithm adopts fixed $\\beta_1$.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is easy to follow and well presented. \n\nNovelty can be found in W1. \n\nCode is provided for reproducibility.",
            "summary_of_the_review": "The proposed AdaDQH is interesting in how the Hessian is estimated. However, this paper does not provide convincing explanation to their Hessian estimate, and does not compare with existing works with similar ideas. Some of experiments are not carried out on standard models making their numerical improvement less impressive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1045/Reviewer_EHjo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1045/Reviewer_EHjo"
        ]
    },
    {
        "id": "2mTZzZlSAMx",
        "original": null,
        "number": 3,
        "cdate": 1666612568139,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612568139,
        "tmdate": 1666899533045,
        "tddate": null,
        "forum": "W-VfwHzA2yg",
        "replyto": "W-VfwHzA2yg",
        "invitation": "ICLR.cc/2023/Conference/Paper1045/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper extends AdaBelief by making two modifications: (1) the 2nd momentum s_t is computed by taking the EMA of (m_t/(1-beta_1^t)-m_{t-1}/(1-beta_1^{t-1}) )^2 in comparison to the EMA of (m_t-g_t)^2 in AdaBelief; (2) the parameter delta (or epsilon  in AdaBelief) is introduced in a slightly different manner to avoid division by zero.  The new optimiser is referred to as ADADQH. Experiments on different DNN tasks indicates that ADADQH produce either better and competitive performance. ",
            "strength_and_weaknesses": "Strength: \n1. The paper propose new types of modifications to AdaBelief, aiming to improve its performance, which I haven't seen from literature before. \n2. Theoretical analysis is provided for the new method. \n\nWeekness: \n(1) As I mentioned earlier, ADADQH made two modifications to AdaBelief. It is not clear which modification plays a crucial role for the performance gain. I am guessing the first modification for computing s_t makes a difference in performance improvement. The reason is that m_t/(1-beta_1^t)-m_{t-1}/(1-beta_1^{t-1}) = (1-beta_1)/(1-beta_1^t) * [g_t - m_{t-1}/(1-beta_1^{t-1})]. The scalar  (1-beta_1)/(1-beta_1^t) essentially reduce the variance of the adaptive stepsizes in comparison to AdaBelief which computes the EMA of (g_t-m_t)^2.  In a recent paper \"On Exploiting Layerwise Gradient Statistics for Effective Training of Deep Neural Networks\", another method named Aida was proposed to reduce the variance of the adaptive stepsizes of AdaBelief by performing vector projections between m_t and g_t before being used for the computing the 2nd momentum. I suggest the authors to conduct performance also in comparison to Aida. In the abstract, the authors state that ADADQH is compared to SOTA optimizers. Both Aida and ADADQH are closely related to AdaBelief and should be compared.  \n\n(2) In AdaBelief paper, the validation accuracy of AdaBelief for training ResNet34 over CIFAR10 is above 95%. In the current paper, the validation accuracy of AdaBelief for training ResNet32 over CIFAR10 is below 93%. Even though ResNet 34 is slightly complicated than ResNet32, I think the performance gap is too large for AdaBelief.  I suggest the authors take the opensource from AdaBelief to make a direct comparison. \n\n(3) My personal experience is the selection of parameter epsilon in Adam and AdaBelief plays a very important role in the performance. In order to make a fair comparison, the parameter epsilon needs to searched for each optimizer. In Figure 1, I don't see that the parameter epsilon  is being searched for Adam and AdaBelief. Inproper epsilon value in Adam and AdaBelief would slow down their convergence speed.  ",
            "clarity,_quality,_novelty_and_reproducibility": "1. The proposed method ADADQH is in fact an extension of AdaBelief. As I mentioned earlier, The EMA of (m_t-g_t)^2 in AdaBelief can also be viewed to approximate the diagonal Hessian of a function, which is very similar to the EMA of ( m_t/(1-beta_1^t)-m_{t-1}/(1-beta_1^{t-1})  )^2= ( (1-beta_1)/(1-beta_1^t) * [g_t - m_{t-1}/(1-beta_1^{t-1})] )^2 in ADADQH.   In the paper, I don't see the study or discussion on the connection between AdaBelief and ADADQH. The authors only mention AdaBelief without detailed information.   \n2. I don't get how (3) is obtained by solving (2). Note that m_t does not appear in (2).  There is a gap between (2) and (3). \n",
            "summary_of_the_review": "The paper proposes two new modifications to AdaBelief, and demonstrates performance gain on both synthetic and real-data experiments. 1. The connection between the proposed method  ADADQH and the two existing methods AdaBelief and Aida should be studied, or properly described in the introduction in the paper so that the readers better understand the reasoning behind it. 2. The parameter epsilon needs to be searched for each optimizer in order to make a fair comparison in all experiments. 3. The paper needs to conduct ablation study to find out which modification in ADADQH is more important to improve performance in AdaBelief. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1045/Reviewer_FWdz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1045/Reviewer_FWdz"
        ]
    }
]