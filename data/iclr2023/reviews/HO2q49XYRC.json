[
    {
        "id": "t4Gr-DYMbP",
        "original": null,
        "number": 1,
        "cdate": 1666683380856,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683380856,
        "tmdate": 1666683380856,
        "tddate": null,
        "forum": "HO2q49XYRC",
        "replyto": "HO2q49XYRC",
        "invitation": "ICLR.cc/2023/Conference/Paper3095/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": " This paper proposes a new MoE model Architecture to improve the parameter efficiency of MoE by learning a soft combination of a global set of expert layers. \n",
            "strength_and_weaknesses": "Weaknesses: \n\n(1). This paper carried out analysis first and listed three challenges from analysis. However, I did not know which MoE model does this paper study. In Figure 1, it shows \u201cMoE\u201d but I don\u2019t know which MoE model is used to carry out experiments. There are plenty of MoE models such as Gshard (Lepikhin et al., 2020), Switch Transformer (Fedus et al., 2021), Base Layers (Lewis et al., 2021), HASH Layers (Roller et al., 2021), and etc. Different MoE models may lead to different conclusions. The author needs to announce which model they used for analysis and add citations. \n\n(2). This article used only one MoE model to draw analysis conclusions, which I cannot agree with. Because different MoE models may have different performance, analysis conclusions need to conduct experiments with at least two representative MoE models when talking about common challenges with MoE models.\n\n(3). I am very suspicious about the expert pool method proposed in this article. How to choose the size of the expert pool. I speculate that the amount of experts required by a MoE model may be related to the diversity of the dataset. Table 1 in BASE layers paper [1] shows similar words usually gathered to the same expert unit. However, this article only uses one dataset for pretraining, and does not use multiple datasets to test the required expert pool size.\n\n(4). Followed by the third problem, this paper selected the Pile dataset as the pre-training dataset. However, the Pile dataset is full of duplicate documents (see [2] page 2), and this paper does not perform additional de-duplication processing. Because the dataset selected in the article has a lot of repetition and the tokens are not diverse, the size of the expert pool does not need to be large. The conclusion is likely to change when changing to a different (diverse) pre-training dataset.\n\n(5). As a MoE model, it is basically necessary to control the number of flos and compare it with the dense models and sparse models with the same number of flops, but this paper does not report total training flops number and total train computer (PF-days). In addition, this paper doesn\u2019t compare it with a dense model with the same amount of flops in table 1. \n\n(6). I also have some questions about the experimental results of table 2. When we compared SaMoE (350M-128E) with dense model (350M), SaMoE should have more flops since it needs additional all2all communication cost. However, I notice usually a dense model (350M) could get a score of 70.2 on piqa. This SaMoE with more flops achieves a score 68.9. \n\n(7). Minor suggestion: usually we reported pretraining perplexity instead of validation loss in figure 3.\n\n\n*References:*\n\n[1]. Lewis, Mike, et al. \"Base layers: Simplifying training of large, sparse models.\" International Conference on Machine Learning. PMLR, 2021.\n\n[2]. Zhang, Susan, et al. \"Opt: Open pre-trained transformer language models.\" arXiv preprint arXiv:2205.01068 (2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality:\n\nDue to the above weaknesses, this paper does not have a high quality.\n",
            "summary_of_the_review": "This paper proposes a new MoE model. However, in the analysis part, it only carried out analysis experiments with one MoE model, which is hard to tell if findings applied to all MoE models. In addition, this paper proposes to have a fixed number of global MoE layers, which is probably not suitable when a pre-training dataset has very diverse tokens. It happens that this paper selects the Pile as the pretraining dataset, and Pile is widely considered to contain many repeated sentences. (see [2] page 2). \n\n*References:*\n\n[1]. Lewis, Mike, et al. \"Base layers: Simplifying training of large, sparse models.\" International Conference on Machine Learning. PMLR, 2021.\n\n[2]. Zhang, Susan, et al. \"Opt: Open pre-trained transformer language models.\" arXiv preprint arXiv:2205.01068 (2022).\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3095/Reviewer_9vyj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3095/Reviewer_9vyj"
        ]
    },
    {
        "id": "8cRjGv5xOX",
        "original": null,
        "number": 2,
        "cdate": 1666722417085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666722417085,
        "tmdate": 1666722417085,
        "tddate": null,
        "forum": "HO2q49XYRC",
        "replyto": "HO2q49XYRC",
        "invitation": "ICLR.cc/2023/Conference/Paper3095/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "MoEs have been reported to be parameter inefficient such that larger models do not always lead to better performance. This work proposes a parameter-efficient MoE models, by learning a soft combination of a global set of expert layers for each MoE layer. Experimental results show that SaMoE improves parameter efficiency by reducing up to 5.2x parameters while obtaining strong pretraining and zeroshot generalization results. ",
            "strength_and_weaknesses": "Strengths:\n- The proposed method is simple in design and implementation but achieves reasonably good results. \n\nWeaknesses:\n- The paper does not provide a fair comparison by fixing total parameters but ignore the computational cost (FLOPs) or activated parameters. In traditional MoE research, the general goal is to achieve better quality with a fixed computational cost (FLOPs), not with a fixed total parameters. The reviewer understand that this method provides a efficient way saving total parameters, but the reviewer suspects to achieve better quality, this method would also significantly increase the FLOPs compared to traditional top1 or top2 based routing used in switch transformer and GLaM. Table 1 does not provide any details on computational cost. According to multiple prior works including T5 and the Chinchilla [3], there is always a tradeoff between model capacity and training tokens, the larger the model is, the lower training data/steps can be achieved within the same computational cost budget.  According to Table 1, activated parameters are made fixed around 1B, however, it might not be clear about computational cost in this work's setting. \n\n- The paper misses fair comparisons with many significant related work including autoregressive sparse MoE, GLaM [1]. GLaM adopts a top-2 based routing, that can yield much better results than top-1 based routing. Various efficient routing functions should be compared with in this work, as intelligent routing functions achieve similar effects of improving parameter efficiency. For example, Expert Choice [2] routing achieves heterogeneous experts such that different tokens can utilize a variable number of parameters. \n\n[1] https://arxiv.org/pdf/2112.06905.pdf\n[2] https://arxiv.org/abs/2202.09368\n[3] https://arxiv.org/abs/2203.15556\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. \nThe proposed method is a minor improvement over traditional parameter sharing scheme like suggested in Universal Transformer. \nThe paper can be reproduced but the results are not valid. ",
            "summary_of_the_review": "\"No free lunch\" in deep learning: reducing parameters and reducing training time will not come for free without sacrificing model quality. \nThe paper's results are based on fixing total parameters but not on fixing computational cost (activated parameters and FLOPs), which can be unfair to many related works including GLaM. The reviewer would not believe in any results that purely relying on parameter sharing, we could improve quality without introducing additional computational cost. For example, whether this method increases activated parameters (experts per token) is unclear and should be explained. For example, the paper can be increasing the number of layers or expert width or number of experts per token compared to GLaM. All these increase activated parameters, thus inference time. The paper should be also more proactive in explaining why quality gains can be achieved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3095/Reviewer_WUCK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3095/Reviewer_WUCK"
        ]
    },
    {
        "id": "249_3oxx5F",
        "original": null,
        "number": 3,
        "cdate": 1667196041258,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667196041258,
        "tmdate": 1667196041258,
        "tddate": null,
        "forum": "HO2q49XYRC",
        "replyto": "HO2q49XYRC",
        "invitation": "ICLR.cc/2023/Conference/Paper3095/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "SaMoE is a novel routing algorithm for mixture of experts (MoE) that allows different MoE layers select experts from a global shared pool. Well designed experiments and scaling law studies are reported in the evaluation section.\n",
            "strength_and_weaknesses": "Strengths:\n1. I think the authors found the right critical bottlenecks for the MoE models, which are the trainability (model quality aspect) and the total number of parameter count (system performance aspect). The proposed solution with the empirical results shows it's on the promising direction,  although it's still far from completely addressing those foundational limitations of MoE, \n\n2. The ablation and scaling laws section are very helpful to the research community to understand how to set the hyperparameters. \n\nWeaknesses:\n1. How the speedup in table 1 is evaluated?\n\n2. The gains in the downstream tasks are marginal. It's better to report the variation of zero-shot results at nearby checkpoints as well. \n\n3.  I feel it's important to report the inference step time during autoregressive decoding to best demonstrate the gains from a smaller number of parameters. Because during decoding on accelerators, it's more often memory bound instead of compute bound. When decoding a single token given the prefix, the flops to compute each token is relatively small. However, the whole model parameters (in billions or even trillions) needs to send more HBM to the actual compute units during each decoded step. This HBM-cache communication is usually the dominant factor in the inference cost.\n\n4. Double check the multi-rc results? It's a big jump from 1+ to 70+.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The flow of this paper is crystal clear. The authors first identified the bottlenecks of an existing algorithm, found the root cause, proposed a solution, and finally demonstrated the effectiveness of the proposed solution with well prepared experimental evaluations. The text, tables, and figures are all of high quality.\n",
            "summary_of_the_review": "This paper worked on an important research topic: how to reduce the training and serving costs for large language models. The proposed algorithm is only marginally novel but empirically significant. The 5x reduction in the number of total parameters would improve the serving speed for MoE models by a lot. \n\nHowever, the gains in the downstream tasks are only marginal. So it would be better if the authors clearly demonstrated why the large reduction in the parameter count matters using the metrics people care about: the serving latency, the step time, etc. Alternatively, the authors can show the quality difference when matching the inference cost. \n\n ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3095/Reviewer_VcJV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3095/Reviewer_VcJV"
        ]
    }
]