[
    {
        "id": "iNwFhZJ9Yt",
        "original": null,
        "number": 1,
        "cdate": 1666112397630,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666112397630,
        "tmdate": 1666112397630,
        "tddate": null,
        "forum": "7tJyBmu9iCj",
        "replyto": "7tJyBmu9iCj",
        "invitation": "ICLR.cc/2023/Conference/Paper3992/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method based on a binary neural network to learn classification rules from sequential data. The model contains a stackedOR layer for categorical variables, a logical AND layer and an OR layer. The training strategy includes dynamically-induced sparsity.  \n",
            "strength_and_weaknesses": "Strengths. The paper introduces a method that can be useful for real applications. The paper is well-written. \n\nWeaknesses. Comparison to state-of-the-art methods are missing. \nThe local and global patters are not so easy to understand. In general, in my opinion, Section 2.2 is not easy to follow. \n\nTwo window values (for sequences), namely, 3 and 6 were tested. It is difficult to make strong conclusions from results based on these two values only. It is also mentioned that it is a good idea to take a window size which equals to the maximum length. First, I wonder whether if such a big window is taken, the model would suffer from overfitting. Second, it is indeed interesting to take quite a big window but also include into the model some shorter windows (similarly to n-gram models). Third, windows of various lengths (performing data segmentation) would be an optimal choice.  \n\nSpeaking about interpretability: what kind of rules were learnt? \nI did not really see that the interpretable aspect of the method was validated, i.e., that some rules were extracted and considered/validated by human experts. \n\nOther remarks. In Section 3, \"the model is trained via automatic differentiation\", and it is also mentioned that the losses are differentiable. Why do you prefer automatic differentiation if the first derivative can be computed?   \n\nIn Section 3, the paragraph \"Loss function\" is not very easy to understand: why there is product of three \\Pi matrices? Is the multiplication coordinate-wise, if yes, do they have the same dimensions? What is \\Pi_{stack} and why the sum is taken over \\Pi_{stack} matrices?\n\nIt is mentioned that both balanced and unbalanced data sets are considered. How the data sets are balanced? I did not find any information on whether some undersampling or weighted loss was used. \n\nIn Table 2: what the column \"Penalty\" contains?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and some ideas are novel. ",
            "summary_of_the_review": "The method is sound but some points should be clarified. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3992/Reviewer_r7kZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3992/Reviewer_r7kZ"
        ]
    },
    {
        "id": "nTysN4tASj",
        "original": null,
        "number": 2,
        "cdate": 1666687141835,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687141835,
        "tmdate": 1668523123773,
        "tddate": null,
        "forum": "7tJyBmu9iCj",
        "replyto": "7tJyBmu9iCj",
        "invitation": "ICLR.cc/2023/Conference/Paper3992/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Learning rules is difficult and often lead to low performances. From the computational point of view, it poses a problem of differentiability. This article propose to learn both the discriminant pattern and explicit decision rules.\nThe authors first insist on the cabablities associated with a 3 layer rule network. Then, they describe the convolution layer which applies rules on the discrete signal. Section 3 explains how the architecture is learnt. Then, the authors conduct experiments on synthetic and real discrete signals.",
            "strength_and_weaknesses": "**positive**\n\nArchitecture with potential for xAI\n\nA very interesting way to learn rules with gradient descent\n\n**negative**\n\nn section 3, the authors should define the notation and matrix dimensions: I don't understand (6) and (7).\nI can't understand the regularization procedure.\n\nI am not familiar with logical networks, but I don't get how the author make the architecture differentiable\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The model is not described with enough details to enable reproducibility",
            "summary_of_the_review": "The idea of the article is really interesting but the whole model description (section 3), in particular the learning process, is impossible to understand (at least for a reviewer that is not familiar with rule neural networks). For this reason, I can't accept this article.\n\n\n**AFTER REBUTAL**\n\nThe paper has been significantly improved by the authors. The procedure is much more detailed than in the original version and an anonymous repository has been made available. This encourage me to change my ratings.\n\nI am not familiar enough with the previous contribution in the field to assess reliably the novelty of the proposal.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3992/Reviewer_JYCZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3992/Reviewer_JYCZ"
        ]
    },
    {
        "id": "Kf8YGSoFSc",
        "original": null,
        "number": 3,
        "cdate": 1666689338975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689338975,
        "tmdate": 1666689338975,
        "tddate": null,
        "forum": "7tJyBmu9iCj",
        "replyto": "7tJyBmu9iCj",
        "invitation": "ICLR.cc/2023/Conference/Paper3992/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a convolutional binary neural network to learn binary classification rule for sequence data. The learned rule is able to learn global/local pattern for better explainability. Furthermore, the authors propose a training strategy with latent weights and regularization term penalizing the complexity of the rule, and dynamically enforces the sparsity using sparsity-during-training method. Experiment results on 4 synthetic datasets and one real dataset are shown to exhibit the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength\n1. The paper improves on the base rule model by operating on sequence data and could learn global/local pattern\n2. The training strategy on penalizing the complexity of the rule and sparsify-during-training is useful and could benefit other rule learning algorithm  \n\nWeaknesses\n1. The proposed method does not work for unbalanced data for learning an empty rule. I am wondering if up/down-sample the positive/negative class helps\n2. The paper could be polished in terms of clarity. For example, the authors should explain the notations for grammar in equations 3-5 for better presentation. Equation 9 is also not explained with the motivation on using such form for the rate.\n3. There is no experiment results on explainability on real dataset. The paper will be stronger and better motivated with such a demo.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\nIn general the paper is easy to follow. However, the paper could be polished in terms of clarity. For example, the authors should explain the notations for grammar in equations 3-5 for better presentation. Equation 9 is also not explained with the motivation on using such form for the rate.\n\nQuality, Novelty And Reproducibility\nThe paper is novel for proposing rule learning model for sequence data. There is no code provided and detailed experiment setting so I doubt with reproducibility, which downgrades the quality of the paper. The paper could also be improved by more explanations/motivations on the techniques used and providing experiment result on explainability.",
            "summary_of_the_review": "Overall I think the paper contributes a novel method for learning rule with sequence data, and recommend for acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3992/Reviewer_UfDD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3992/Reviewer_UfDD"
        ]
    }
]