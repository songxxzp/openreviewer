[
    {
        "id": "pdpuNb6yAR",
        "original": null,
        "number": 1,
        "cdate": 1666534203472,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666534203472,
        "tmdate": 1666591290881,
        "tddate": null,
        "forum": "4daKS8wEze5",
        "replyto": "4daKS8wEze5",
        "invitation": "ICLR.cc/2023/Conference/Paper4655/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes ResGrad, a diffusion-based model that models the residual between the model output and the GT mel. For efficient sampling, ResGrad refines the output of the pre-trained TTS model (FastSpeech 2) by modeling the residual. ResGrad speeds up the inference for DDPM by using the pre-trained TTS model. The experimental results show that ResGrad outperforms other baselines except for GradTTS and is comparable to GradTTS with the same sampling steps. For the single-speaker setup, the performance of ResGrad doesn\u2019t deteriorate much with 4 sampling steps. ",
            "strength_and_weaknesses": "Strengths\n* Reduces the number of sampling steps of DDPM by modeling the residual.\n* ResGrad shows comparable performance to the Grad-TTS and performs well with the 4 sampling steps. \n* The demo page shows that ResGrad refines the sample quality of FastSpeech 2 well.\n\nWeaknesses\n* ***The proposed method is just the refinement model of the pre-trained FastSpeech 2.*** ResGrad can be applied to the specific deterministic TTS model (FastSpeech 2), but it seems difficult to apply to other TTS models. ResGrad requires the GT pitch and GT duration for training. It is challenging to model the residual between the output of other stochastic TTS models and GT mel.\n* ***The proposed method is quite similar to Grad-TTS.*** The residual between the model output and GT mel can be modeled with DDPM in two ways. (1) Directly modeling the residual (GT mel - the model output). (2) Use the mean of the prior distribution with the model output and model the GT mel w/ DDPM. The latter method can be seen as the simple modification of Grad-TTS, which replaces the output of the text encoder with that of FastSpeech 2. There is no guarantee that the former method is more effective in reducing the sampling step than the latter method, which makes ResGrad similar to Grad-TTS.\n* ***The baselines of the paper are the single-speaker TTS models.*** FastSpeech 2, DiffGAN-TTS, DiffSpeech, and Grad-TTS do not have experimental results for the multi-speaker TTS dataset (LibriTTS and VCTK) in their original papers. Therefore, these models may not perform well in multi-speaker settings by simply introducing the speaker embedding. For example, Grad-TTS shows very poor quality in the multi-speaker setting as can be seen from the demo page of this paper. To show that it works well on the multi-speaker dataset, it should compare with the multi-speaker TTS models.\n* ***The generated samples from ResGrad sound a lot similar to the samples from FastSpeech 2 except for the sample quality.*** It is necessary to show whether the sample diversity of the diffusion-based generative model is lost by performing the residual modeling. It would be better to provide several samples of ResGrad-4 and ResGrad-50 for the same FastSpeech 2 output to the demo page.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is clear but can be seen as the diffusion-based refinement model for FastSpeech 2. The method is similar to the existing model (Grad-TTS). Detailed explanations are shown in the weaknesses part. ",
            "summary_of_the_review": "The method is limited to improving the output of FastSpeech 2, and it does not look much different from the existing model (Grad-TTS). The paper should compare ResGrad with the appropriate baselines for the multi-speaker datasets. The samples of ResGrad on the demo page have similar prosody (pitch & duration) to the samples of FastSpeech 2. It is required to show whether ResGrad does not lose sample diversity by modeling residuals. (which can be a possible disadvantage of the proposed methods)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4655/Reviewer_DApv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4655/Reviewer_DApv"
        ]
    },
    {
        "id": "K_IIuoK6aW",
        "original": null,
        "number": 2,
        "cdate": 1666683174798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683174798,
        "tmdate": 1666683174798,
        "tddate": null,
        "forum": "4daKS8wEze5",
        "replyto": "4daKS8wEze5",
        "invitation": "ICLR.cc/2023/Conference/Paper4655/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Predicts the residual between the output of a FastSpeech spectrogram predictor and the ground truth. Reduces the complexity of the problem the diffusion model is performing.\n",
            "strength_and_weaknesses": "# Strengths\n\n* Straightforward application of diffusion to improve a FastSpeech-based TTS system.\n\n# Weaknesses\n* Increases complexity in an unprincipled manner: The overall combination of FastSpeech and ResGrad is a mashup of a deterministic / non-probabilistic model and a deep generative model. Add in a vocoder to invert that spectrogram \u2013 say a HifiGAN or WaveNet and now you have yet another generative model in the stack. All of which were trained with different objectives and might interpret the spectrogram in different ways.\n* Plug-and-play is not a strength, if the overall system is now a chain of 3 models (FastSpeech, ResGrad, and a vocoder). It would be more efficient to simply improve the vocoder to be able to work with oversmoothed spectrograms. \n* The outputs of FastSpeech are oversmoothed and are likely a boring \u201caverage\u201d prosody for the provided text due to the L1 or L2 loss used to predict prosodic details. Once it generates a spectrogram with prosodic decisions already made, you\u2019ve lost access to the linguistic details that would be most informative for fixing the bad spectrogram it produced. ResGrad is not the best place to fix these decisions \u2013 the best place is in the FastSpeech model itself. If ResGrad changes the trajectory towards something that is more realistic or similar to the training data it may in doing so corrupt the prosodic meaning expressed in the original spectrogram (for example, if there were 5 valid ways to say something and the textual context strongly hinted at one of the 5 but the FastSpeech model was unable to realize it in a realistic way).  The ResGrad model has no way of knowing which of the valid ways of saying it it should go with, since it has no access to text or context that would resolve that question.\n* Would like to see a CMOS comparison with GradTTS, which had equivalent MOS. MOS Is not a good tool for comparing models that have substantially similar performance.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of editing the outputs of a spectrogram predictor is not novel -- Tacotron 2 does this with its \"post edit network\", which is the first instance I have seen of it. Since Tacotron, GANs have been used to convert spectrograms from low-quality / over-smoothed to higher fidelity / crisper. This is the first application I have encountered of using diffusion models in this setting.\n\nThe manuscript is clear and easy to understand.",
            "summary_of_the_review": "I appreciate the work that went into this and found it an interesting read. \n\nThe motivation of this work is to improve an existing deterministic spectrogram prediction system (FastSpeech 2) by improving the quality of its spectrogram outputs to appear more realistic. The flaw in this method is that the improvements have no way of resolving between multiple valid prosodic trajectories, since it has no access to the text. \n\nThe experimental results are not compelling \u2013 the method seems on-par with GradTTS-50, with a slight improvement in RTF. I suspect that if you pair FastSpeech 2 with a vocoder (e.g. a diffusion-based vocoder or WaveRNN, or HifiGAN) trained on its outputs, you could outperform or match the performance of ResGrad. It\u2019s not clear why we would want to employ another, very complicated, deep generative model, when we already going to follow FastSpeech with a deep generative model like a HifiGAN \u2013 that could easily be trained to handle the over-smoothed spectra produced by FastSpeech 2.\n\nAt a high level, I don't understand why we would want to solve the problems with FastSpeech in this particular way, and the stated goals of being faster than other diffusion methods do not seem to necessarily have been achieved given the empirical results.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4655/Reviewer_U3kA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4655/Reviewer_U3kA"
        ]
    },
    {
        "id": "EPCSytJapH",
        "original": null,
        "number": 3,
        "cdate": 1666745397079,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745397079,
        "tmdate": 1666746689786,
        "tddate": null,
        "forum": "4daKS8wEze5",
        "replyto": "4daKS8wEze5",
        "invitation": "ICLR.cc/2023/Conference/Paper4655/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes ResGrad to improve the existing TTS models. Specifically, it uses a DFM to learn to generate the residual between an existing TTS model\u2019s output and the groundtruth spectrogram. The residual is then added back to the TTS model\u2019s output to get the refined output. Experiments on three datasets (LJSpeech, LibriTTS, and VCTK) show high sample quality and inference speed.\n",
            "strength_and_weaknesses": "Strengths\n1. The method generates the residual between the output of an existing TTS model and the groundtruth, which makes it efficient compared to those generating speech from scratch.\n\n2. The method is flexible to apply to other existing TTS models without re-training them.\n\n3. Experiments show better MOS scores with the similar RFT and better RFT at similar MOS scores.\n\nWeakness\n1. Section 3.1 says that  \u201cThe generated residual contains high-frequency details and avoid over-smoothness in the estimated mel-spectrogram, and thus can improve the sample quality.\u201d. When first reading this, I feel It\u2019s unclear what design in ResGrad makes it produce high-frequency details. There is a lack of context before making this claim. I think the reason is because of the way to calculate the residual in training, which is introduced in the later Section 4.1.\n\n2. The paragraph \u201cCalculation of Residual\u201d in Section 4.1 explains why using  ground-truth or predicted pitch/duration to calculate the residual. I think the explanations can be improved. The reason for using groundtruth pitch in calculating residuals is to make ResGrad not learn to correct pitch and reduce its learning burden. However, in inference, the ResGrad still conditions on the mel-spectrogram generated by the predicted pitch. The mel-spectrogram generated with the predicted pitch may be different from the one generated with groundtruth pitch.  If ResGrad is trained to improve the later one, how can we make sure it performs well on the former one in inference?\n\n3. The first paragraph of Section 5.2 mentions \u201cAs it can be seen, when we use pitchPred, the residual related to pitch information is not generated as it is in the ground-truth residual samples.\u201d I feel this contradicts the statement in Section 4.1 that \u201cHowever, if we calculate the residual with predicted pitch as shown in Figure 2(b), ResGrad will simultaneously correct the pitch in estimated samples towards the ground-truth samples\u201d. My understanding is that if using the pitchPred, ResGrad will learn to generate pitch related information to correct the pitch. Table 2 also seems to contradict the descript \u201cThus, we use ground-truth pitch to calculate the residual.\u201d in Section 4.1. Table 2 shows using pitchGT gives worse results.\n\n4. Regarding Figure 3, are the two residual examples (row 2) generated by the same ResGrad trained with the pitchGT? Or are they trained with pitchGT and pitchPred, respectively?\n\n5. What\u2019s the relation of rows 1 and 2 in Figure 4? Are row 1 one example and row 2 another example? What do the red rectangles mean?\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. This paper studies to improve the diffusion models' efficiency by making them model the residual, which is interesting and novel.\n\n2. This paper is easy to understand, but some details regarding whether using pitchGT or pitchPred and experimental settings of some figures are not clear.\n\n3. I guess the experiments are reproducible based on the training details in Section 4. It would be better to provide code.",
            "summary_of_the_review": "It provides a new view in improving the efficiency of DFMs in speech synthesis. The method is simple and results look promising. The paper can be improved if it can refine some technical descriptions and experiment details.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4655/Reviewer_Zr8k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4655/Reviewer_Zr8k"
        ]
    },
    {
        "id": "_o9ykw-6PWq",
        "original": null,
        "number": 4,
        "cdate": 1666824212555,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666824212555,
        "tmdate": 1666824212555,
        "tddate": null,
        "forum": "4daKS8wEze5",
        "replyto": "4daKS8wEze5",
        "invitation": "ICLR.cc/2023/Conference/Paper4655/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors propose diffusion-based refinement of an existing TTS system. Two competing approaches are non-iterative TTS (i.e. approaches which don\u2019t use diffusion process) and diffusion based models.\n\nAuthors show improvements in speech quality compared to DDPM based models at similar inference speed.\n\nIt is worth noting that the suggested approach is not a new method to speed up diffusion process, rather a TTS system with fast sampling and high quality synthesis.",
            "strength_and_weaknesses": "Strength:\n* Results shown on a bunch of datasets\n* Good practical application\n\nWeakness:\n* Limited Novelty\n* Poor baseline results",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and presents the key contributions clearly.",
            "summary_of_the_review": "The paper presents a simple idea of starting the diffusion process to refine the predictions of an existing TTS network. While the authors show that the idea indeed leads to improvement in the inference speed, there are very limited things to learn from the paper. Further study related to use of different residuals e.g. more than one base TTS model -- would strengthen the paper.\n\nI am curious about the baseline MOS reported by the authors -- the variance in the reported MOS is considerably higher compared to other research papers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4655/Reviewer_TPsB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4655/Reviewer_TPsB"
        ]
    }
]