[
    {
        "id": "SHY1et_NFc",
        "original": null,
        "number": 1,
        "cdate": 1666652797666,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652797666,
        "tmdate": 1666652797666,
        "tddate": null,
        "forum": "nsz2ZxnD9D",
        "replyto": "nsz2ZxnD9D",
        "invitation": "ICLR.cc/2023/Conference/Paper1874/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work propose a federated learning algorithm, named FedMIM, which adopts the multi-step inertial momentum on the edge devices and enhances the local consistency for free during the training to improve the robustness of the heterogeneity. Specifically, the authors incorporate the weighted global gradient estimations as the inertial correction terms to guide both the local iterates and stochastic gradient estimation, which can reckon the global objective optimization on the edges\u2019 heterogeneous dataset naturally and maintain the demanding consistent iteration locally.  ",
            "strength_and_weaknesses": "strength: multi-step inertial momentum on the edge devices to enhances the local consistency\n\nweakness: lack of novelty compared to the server side momentum (Karimireddy et al., 2020b) or client side momentum Xu et al. (2021) ",
            "clarity,_quality,_novelty_and_reproducibility": "lack on novelty\n\nlack of code to reproduce experiment results ",
            "summary_of_the_review": "federated learning is a great area to study. this work adopts the multi-step inertial momentum on the edge devices to improve the robustness of the heterogeneity.  However, the novelty of the paper is not well-established. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1874/Reviewer_WgvB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1874/Reviewer_WgvB"
        ]
    },
    {
        "id": "c8bzh2kIJh",
        "original": null,
        "number": 2,
        "cdate": 1666679005463,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679005463,
        "tmdate": 1666845223370,
        "tddate": null,
        "forum": "nsz2ZxnD9D",
        "replyto": "nsz2ZxnD9D",
        "invitation": "ICLR.cc/2023/Conference/Paper1874/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new federated learning algorithm (i.e., FedMIM),  which adopts the multi-step inertial momentum on the edge devices and enhances the local consistency for free during the training  to improve the robustness of the heterogeneity. Theoretically, it proved that \nthe FedMIM algorithm achieves the best known convergence rate with a linear speedup property with respect to the number of selected clients and proper local interval in each communication round under the nonconvex setting. Empirically, it conducted comprehensive experiments on various real-world datasets and demonstrate the efficacy of the proposed FedMIM against several state-of-the-art baselines.\n",
            "strength_and_weaknesses": "Strength:\n\nThis paper proposed a new federated learning algorithm (i.e., FedMIM),  which adopts the multi-step inertial momentum on the edge devices and  enhances the local consistency for free during the training  to improve the robustness of the heterogeneity. Theoretically, it proved that \nthe FedMIM algorithm achieves the best known convergence rate with a linear speedup property with respect to the number of selected clients and proper local interval in each communication round under the nonconvex setting. \n\nWeakness:\n\nThe novelty of this paper is limited. This paper basically follows the existing Nesterov' momentum techniques.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing can significantly be improved. \nThe novelty of this paper is limited. This paper basically follows the existing Nesterov' momentum techniques.\nThe motivation of this paper is not clear. \n",
            "summary_of_the_review": "This paper proposed a new federated learning algorithm (i.e., FedMIM),  which adopts the multi-step inertial momentum on the edge devices and  enhances the local consistency for free during the training  to improve the robustness of the heterogeneity. Theoretically, it proved that  the FedMIM algorithm achieves the best known convergence rate with a linear speedup property with respect to the number of selected clients and proper local interval in each communication round under the nonconvex setting. Empirically, it conducted\ncomprehensive experiments on various real-world datasets and demonstrate the efficacy of the proposed FedMIM against several state-of-the-art baselines.\n\nSome Comments:\n\n1. The motivation of this paper is not clear. Why could multi-step inertial momentum on the edge devices \n enhance local consistency ?\n\n2. What is the parameter $I$ in the FedMIM algorithm \uff1f\n\n3. How to choose the tuning parameters $\\alpha_j$ and $\\beta_j$ in the FedMIM algorithm ?\n\n4. Reproducibility is a crucial aspect of any work based on experiments. \nYour code should be available to the reviewers throughout all stages of the submission and publicly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1874/Reviewer_YuXz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1874/Reviewer_YuXz"
        ]
    },
    {
        "id": "iWhV-ZkEXrj",
        "original": null,
        "number": 3,
        "cdate": 1666940486372,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666940486372,
        "tmdate": 1666940486372,
        "tddate": null,
        "forum": "nsz2ZxnD9D",
        "replyto": "nsz2ZxnD9D",
        "invitation": "ICLR.cc/2023/Conference/Paper1874/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new federated learning algorithm, named FedMIM, which adopts the multi-step inertial momentum\non the edge devices and enhances the local consistency for free during the training to improve the robustness of the heterogeneity.\n\nIn terms of theory the paper, the paper proves that FedMIM achieves the $O(1/\\sqrt{SKT})$ convergence rate under the nonconvex setting, where $S$ is the number of selected clients and $K$ is the proper local interval in each communication round. For non-convex function under PL condition, a convergence rate $O(1/T)$ was also proved for FedMIM. \n\nThe paper conduct comprehensive experiments on various real-world datasets and demonstrate the efficacy of the proposed FedMIM against several state-of-the-art baselines.",
            "strength_and_weaknesses": "The paper is well written and the main contributions of the work are well presented. I went through the main proofs and the important steps looks also correct.\n\nThe paper is easy to follow and the main contributions are clear. \n\nSuggestion: It would be nice at the point where FedMIMN proposed a pseudocode to explain how one can obtain the classical Local SGD as a special case (if this is possible). \n\nMissing references: Closely related papers are [1] and [2] below. \n\nQuestions: \n1. Also to the best of my knowledge one of the state-of-the-art methods for solving non-convex problems is the $\\tau$-overlap SGP from paper [2]. How your approach is related to this method? \n\n2. SlowMo from the paper Wang et al. (2019), which is already cited in the paper, is also a fast method. Why is not used in the experiments? \n\nMissing References:\n\n[1] Koloskova, Anastasia, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. \"A unified theory of decentralized sgd with changing topology and local updates.\" In International Conference on Machine Learning, pp. 5381-5393. PMLR, 2020.\n\n[2] Assran, Mahmoud, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat. \"Stochastic gradient push for distributed deep learning.\" In International Conference on Machine Learning, pp. 344-353. PMLR, 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the above review for further details.",
            "summary_of_the_review": "As I mentioned above, the paper is well written and the main contributions of the work are well presented.\n\nI give a score of \"6: marginally above the acceptance threshold\" to this submission. \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1874/Reviewer_qspk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1874/Reviewer_qspk"
        ]
    }
]