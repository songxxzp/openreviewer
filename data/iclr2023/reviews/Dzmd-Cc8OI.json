[
    {
        "id": "R2Vo4ZBCUw5",
        "original": null,
        "number": 1,
        "cdate": 1666657215879,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657215879,
        "tmdate": 1668928783887,
        "tddate": null,
        "forum": "Dzmd-Cc8OI",
        "replyto": "Dzmd-Cc8OI",
        "invitation": "ICLR.cc/2023/Conference/Paper5365/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This is a theoretical paper that aims to prove the benefits of pretext-based self-supervised pretraining before supervised probing. To do so they consider a toy data generative process where the inputs are a concatenation of label-dependent features  (a linear function of the label) and some random features (Gaussian noise such that noise strength is larger signal). They then prove that an overparametrized 2-layer CNN that was pretrained will achieve near-zero downstream test error if the pretraining set is large, while a fully supervised model cannot if the number of labeled data because it will memorize the noise.\n",
            "strength_and_weaknesses": "**UPDATE**: I\u2019m updating my score 5->6 given that the authors addressed some of my initial concerns. The main remaining issues with this theoretical paper is that there are no actionable insights and that the considered setting is far from practice.\n- - -\n\nStrength:\n- **theory is non-trivial** \n- **proof sketch is clear and useful**\n- **good review of related work**\n\nWeakness\n- (addressed in rebuttal) **Some lack of clarity in SSL nomenclature** multiple times when reading the paper the authors used terms that indicate that one setting was considered when actually it was not. For example:\n    - linear probing vs fine-tuning. You often say that the model is \"fine-tuned on small amount of labeled data\", which suggests that you are finetuning all the weights of your model (eg abstract or figure 2). In other places, you seem to suggest that you only train a linear probe but freeze the pretrained model (eg end of section 3.1). From section 3.2 it seems that you indeed only train a linear probe with weights $a_k$\n    - you use the term \"pretext task\" which in SSL typically means some task that is not directly related to the downstream task (eg predicting frames in a video, predicting the rotation of an image, predicting the order after a jigsaw transformation ...). Yet the paper is really about pseudo labeling in the sense that the pseudo labels are noisy labels of the downstream tasks\n   - The model seems to be a 2 layer neural network instead of a CNN? Or is that a CNN with stride $d$?\n- (addressed in rebuttal) **Not about self-supervised learning** as I just mentioned the paper is about pretraining to fit pseudo labels that are noisy versions of the downstream labels. This is not about self-supervised learning but semi-supervised learning (the pseudolabeler has to first be trained with some labels). To be clear there is nothing wrong with studying pseudo-labeling in semi-supervised learning but I think that the paper (especially the title) is misleading because of that.\n- **toy assumptions** the setting is very toy (2-layer CNN, toy data generating process, uncommon SSL setting ...). That being said I do think that the high-level idea might generalize to more realistic settings.\n- (addressed in rebuttal) **title and abstract are too grandiose** the title and abstract make it seem that the authors do much more than they actually prove. Authors need to be clear how constraint and limited the considered setting is.\n\n**Question**\n- **Is it really about representation learning?** the title and abstract seem to put much emphasis on \"a representation learning perspective\" but wouldn't a supervised learning model that is jointly trained on the pseudo labels and the real labels have the same performance gains? (with proper scheduling of the ratio of pseudo and actual labels\"",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity** the paper has some clarity issues due to some choices of names (fine-tuning / self-supervised learning / CNN ...) as discussed above. The rest is clear.\n\n**Novelty**: the main theorem seems novel\n\n**Quality**: I have not checked the proofs in details, but the sketch proof and end results seems reasonable.",
            "summary_of_the_review": "Assuming that the authors modify (or explain why not) their use of the terms \"fine-tuning\" and \"self-supervised learning\" I think that it could be of interest to some theoretical researchers. I am nevertheless not sure whether ICLR is the right venue to reach such researchers. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5365/Reviewer_r8ze"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5365/Reviewer_r8ze"
        ]
    },
    {
        "id": "YSOXGtrwtf",
        "original": null,
        "number": 2,
        "cdate": 1666671050678,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671050678,
        "tmdate": 1668665678179,
        "tddate": null,
        "forum": "Dzmd-Cc8OI",
        "replyto": "Dzmd-Cc8OI",
        "invitation": "ICLR.cc/2023/Conference/Paper5365/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper theoretically studies optimization and generalization of self-supervised learning with pseudo labelers setting. The pseudo label and true label are correlated. It considers linear data setting (signal/noise ratio = $O(d^{0.01})$) and two-layer neural networks with ReLU^3 activation function. Comparing two settings (a) train networks on constant labeled data and $O(d)$ pseudo-labeled data (b) train on constant labeled data only, the setting (a) will have $o(1)$ test loss while setting (b) will have $\\Theta(1)$ test loss. The proof idea is to manipulate the gradient and try to give a bound on the signal growth rate and noise growth rate. ",
            "strength_and_weaknesses": "Strength:\n1. In my understanding, the key idea is that pseudo labels with a positive correlation with true labels can provide more gradient projection on the effective directions. When the pseudo-label data number is much larger than the true-label data, there is a generalization gap between settings (a) and (b). The main results in Section 4 are non-trivial. \n2. The paper has clear motivation and good writing. The proof sketch section is comprehensive and insightful. \n\nWeakness:\n\nMy major concern is how to generate pseudo-labels in Condition 4.1. Based on my rough calculation, it needs $\\Theta(d)$ true labeled data to train a non-trivial teacher such that test accuracy can be larger than \u00bd with a constant margin with high probability (correct me if I am wrong). Then, here is an unfair comparison for setting (b) which only use constant true labels. If both setting (a) and (b) has $\\Theta(d)$ true labeled data, I think the guaranteed generalization gap may be small or even may vanish. Let me know how to fix it or I may misunderstand here. \n\nHere are other concerns:\n1. The paper considers linear data and it is acceptable. However, it is worth considering non-linear data settings such as XOR or low-degree polynomial in [1,2,3]. \n2. It seems the paper is considering neural networks (NN) rather than convolutional neural networks (CNN). \n3. I understand the authors need ReLU^3 in the proof so that the gradient has a better formulation and property. However, it is a good try to consider the ReLU activation function by using some feature purification tricks (Allen-Zhu & Li (2020a)). \n4. How large is $K$ in Section 3.4? Is it necessary to have a $K$ here rather than using $K=1$?\n\n[1] Shi, Zhenmei, Junyi Wei, and Yingyu Liang. \"A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features.\" International Conference on Learning Representations. 2021.\n\n[2] Frei, Spencer, Niladri S. Chatterji, and Peter L. Bartlett. \"Random feature amplification: Feature learning and generalization in neural networks.\" arXiv preprint arXiv:2202.07626 (2022).\n\n[3] Damian, Alexandru, Jason Lee, and Mahdi Soltanolkotabi. \"Neural networks can learn representations with gradient descent.\" Conference on Learning Theory. PMLR, 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper has good clarification and motivation. \n\nQuality: I checked the most proof in the appendix. They look good to me. \n\nNovelty: The paper has novelty in studying representation learning in self-supervised learning. \n\nReproducibility: I believe the experiments part can be reproduced. \n",
            "summary_of_the_review": "The paper is well-writing and has a non-trivial conclusion. However, I have a major concern about unfair comparison in settings (a) and (b). I may accept the paper if the author can fix my concern, otherwise, I tend to reject it. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5365/Reviewer_jwNu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5365/Reviewer_jwNu"
        ]
    },
    {
        "id": "tiU616Z1qty",
        "original": null,
        "number": 3,
        "cdate": 1666775946045,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666775946045,
        "tmdate": 1668680831410,
        "tddate": null,
        "forum": "Dzmd-Cc8OI",
        "replyto": "Dzmd-Cc8OI",
        "invitation": "ICLR.cc/2023/Conference/Paper5365/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper theoretically analyse how self-supervised pretraining can achieve better performance than full supervised training in a synthetic setting, with fixed data, a fixed architecture, and a fixed self-supervised training scheme. More precisely, it looks at how, when a precise data generating process is assumed and a precise architecture is used, combining a form of self-supervised learning with supervised learning achieves zero test loss, while supervised learning achieves a non zero test loss.\n",
            "strength_and_weaknesses": "**Strengths:**\n- The paper is reasonably well-written.\n- Most of the hypothesis are clearly presented and the derivations look reasonable (even though I did not go through the full derivations in appendix).\n- While the data and architecture setups are synthetic, the paper analyses the full gradient descent dynamic.\n\n**Weaknesses:**\n- Starting from the title, the claims made by the paper are very strong compared to the restrictive setting presented. This paper does not answer the question asked in the title: 'How does self-supervised learning work?'. It only shows that, in a restrictive (and somehow convoluted, more on that below) setting, one can design a self-supervised learning method, which has only few similarities with existing competitive self-supervised methods, that succeeds in learning more efficiently than a supervised counterpart. I think the title, abstract, and conclusion should be significantly reworked to reflect the restrictions imposed on the setup.\n- The setting presented is very restrictive, and some of the restrictions/design choices diverge quite significantly with usual practical use cases. Without some proper motivations and explanations, it's hard to understand how generalizable the theoretical results provided by the paper are. Some examples of such restrictions/departure from practice:\n  - The paper uses a ReLU^3 activation. Most common activation function are linear when pre-activations grow big, and either saturate or decrease linearily when preactivations get low. How crucial is the use of this specific activation function?\n  - The generation process of the data is extremely simple, far from any practical setup.\n  - SSL methods often rely on 1 pretext task to build a representation, then learn a linear predictor on top of the representation. Here, the paper relies on K pretext tasks to build K features, then aggregate the features linearly. Can we relate this setting to the usual SSL setting?\n  - The architecture that is used is very ad-hoc, with features operating directly on the two separated parts of the data, and with only the input layer being trainable for the pretraining phase. The results obtained might be extremely dependent on this ad-hoc architecture. Would adding trainable features on the second layer break the analysis? Would removing the hard split in both data and architecture break the analysis?\n  While I fully understand that studying setups closer to practically reasonable setups is much harder and might render the analysis intractable, I still believe that the current analysis does not support the claims made.\n- While the paper aims to tackle self-supervised learning, the goal that is used to train the self-supervised model is a mix of pretext goal and supervised goal. This kind of mixing is usually considered semi-supervised more than self-supervised. Framing the paper in the context of semi-supervised learning might be a better fit.\n- There are a couple of hard coded numerical values in condition 4.1. that could probably be replaced by constants bounded by some less ad-hoc thresholds. My personal view would be that this would make the analysis more readable and general, but that is mostly a minor point.\n\n----\nPost rebuttal comments:\nI thank the authors for the efforts they put in the rebuttal. Many of my comments have been addressed and I am therefore raising my rating from 3 to 5. I think all the changes made are net positives. The reason I am not raising my rating further is that I still have my doubts on the practical insights that can be drawn from this analysis, which I already mentioned in my initial review.\n\nA couple of additional remarks/questions:\n- You mentioned in the rebuttal that working with K = 1 would recover a more standard SSL setup. If I am understanding things properly, the case K = 1 means that you are learning a single feature (the logits of a single pseudo-label predictor), then probing on this single feature (and thus learning a single weight a_0). I would still not consider this to be a standard SSL setup, where you would rather remove the final layer of your self-supervised network (here the untrained second layer), and replace it with a linear probe, working on the actual 'self-supervised' features. Am I misunderstanding something?\n- I thank the authors for mentioning the fact that the analysis also worked with n_l = 0 which is something I missed in my first read. I would argue in favor of adding an additional comment mentionning that the case n_l = 0 corresponds to usual self-supervised training, whille n_l > 0 corresponds to a semi-supervised training regime (which only adds some generality to the paper).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably clear and well-written. The theoretical result is most probably novel, and the experiments most probably reproducible. While the paper is clear, I also believe that it generalizes too broadly its contributions, making claims about self-supervised learning in general, when the results presented are specialized to one specific data generation process, one specific architecture and one specific pretraining method (which I would consider a semi-supervised learning method rather than a self-supervised learning method).",
            "summary_of_the_review": "The paper at hand provides a clear and probably correct theoretical analysis of a synthetic self(/semi)-supervised learning setup. Based on this analysis, the paper makes broad claims about self-supervised learning. I believe those claims are too broad, and should be properly tamed down to account for the narrowness of the setting considered.\nAside from this claim problem, I am not sure the theoretical results presented here can translate into practical insights, given how far the setup considered is from practical setups of interests.\n\nGiven those points, I don't recommend accepting the paper for now, but I am willing to discuss my current rating with both authors and other reviewers.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5365/Reviewer_eLuC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5365/Reviewer_eLuC"
        ]
    }
]