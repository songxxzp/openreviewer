[
    {
        "id": "kCLBBUpsCY",
        "original": null,
        "number": 1,
        "cdate": 1666094597351,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666094597351,
        "tmdate": 1669086902928,
        "tddate": null,
        "forum": "lEkl0jdSb7B",
        "replyto": "lEkl0jdSb7B",
        "invitation": "ICLR.cc/2023/Conference/Paper3701/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an any-scale balanced proposal for discrete distribution. Motivated by recent findings that non-local proposals improve sampling efficient, the proposed method closes the gaps of the choice of weight function and the accuracy of first order gradient approximation. Comparison to several baseline method demonstrate the efficiency of the proposed method.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is clear and well written.\n2. non-local proposals are helpful for efficient sampling of discrete distributons and the paper introduces several techniques to deal with the associate problems.\n\n\nWeaknesses:\n\n1. The preconditioning (using $D$ instead of $\\sigma^2 I$) seems to be a useful trick. The same trick can be applied to other baseline method (for example, DLP can also use anisotropic diagonal as well), additional comparison to these variants would better illustrate the merits of the second order quadratic approximation.\n\n2. Lack of a comparison to the precondition version PAVG of Rhodes and Gutmann, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly writtern. However, there is a lack of originality as the main idea seems to be explored in a previous paper by Rhodes and Gutmann, 2022. ",
            "summary_of_the_review": "The paper provides useful tricks for hyperparameter choice but lacks originality. Also, a very important baseline method that is highly related to the proposed method is missing (PVAG in Rhodes and Gutmann, 2022).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3701/Reviewer_XxHt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3701/Reviewer_XxHt"
        ]
    },
    {
        "id": "P3BK1YxSifL",
        "original": null,
        "number": 2,
        "cdate": 1666472046876,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666472046876,
        "tmdate": 1669559361757,
        "tddate": null,
        "forum": "lEkl0jdSb7B",
        "replyto": "lEkl0jdSb7B",
        "invitation": "ICLR.cc/2023/Conference/Paper3701/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides a non-local extension of the locally balanced samplers for improved sampling efficiency. The authors start by providing an analysis of existing approaches to improve the sampling efficiency of locally balanced samplers. They then introduce the any-scale balanced sampler in a step by step manner. Numerical results showing the effectiveness of the proposed algorithm conclude the paper.",
            "strength_and_weaknesses": "The proposed algorithm is clearly explained, I personally appreciated the step by step manner in which the algorithm is presented. In terms of performances, the algorithm offers notable improvements over its locally balanced counterparts. Results are interpreted and the limitations of the algorithm are clearly stated.\n\nThe authors highlighted rather well themselves the limitations of the proposed algorithm. I would actually count this as a strength, not a weakness. Personally, I don't see any other weaknesses.",
            "clarity,_quality,_novelty_and_reproducibility": "The authors clearly state their goals, show that their proposal fulfils their claims and are not shy of highlighting the limitations of their proposal. The paper is in my opinion clear and of very good quality. The results are new, thus the paper ticks the novelty criterion. I believe that enough details are given in the paper and the supplementary material to also tick the reproducibility criterion.\n\nThere are some minor typos and errors:\n- page 2, paragraph above section 2, typo neumerical\n- page 4, paragraph above section 3.2, typo hyperparamter\n- page 5, paragraph beneath eq. 15, typo should gives\n- page 5, paragraph \"A common approach ...\", shouldn't j=2,...,d instead of j=1,...,d ?\n- page 7, paragraph \"In this experiment ...\", last sentence, it is said that more results are given in Table 2 and Table 3, however, it should be mentioned that they are to be found in the appendix since on the next page we have Table 2 and Table 3 which don't contain results for the Ising model",
            "summary_of_the_review": "The paper proposes an any-scale balanced sampler, a non-local extension of locally balanced samplers. The proposed algorithm is clearly presented, numerical results that show the advantages of the proposed sampler are presented and interpreted. Limitations are clearly discussed. Good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3701/Reviewer_XywG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3701/Reviewer_XywG"
        ]
    },
    {
        "id": "FkZ8ZV4Sw_V",
        "original": null,
        "number": 3,
        "cdate": 1666609424623,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609424623,
        "tmdate": 1666609424623,
        "tddate": null,
        "forum": "lEkl0jdSb7B",
        "replyto": "lEkl0jdSb7B",
        "invitation": "ICLR.cc/2023/Conference/Paper3701/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The locally-balanced approach of Zanella has recently proven to be useful for MCMC sampling in discrete state spaces. The article proposes two main innovations when compared to recent articles building on the approach of Zanella:\n1. tuning of the \"balancing function\" $g(t) = t^{\\alpha}$ and length-scale $\\sigma$ of the kernel $K_\\sigma$. The tuning of the parameters $(\\alpha, \\sigma)$ is done quite naturally by attempting the maximise the \"Expected Squared Jumping Distance\" during an adaptation phase.\n2. take second order effect by choosing a global \"preconditioning\" matrix such that $f(y) \\approx f(x) + \\nabla f(x)^\\top (y-x) + \\frac12 \\langle (y-x), W (y-x) \\rangle$. The choice of the global matrix $W$ is done during an adaptation phase by minimising $\\|W(y-x) - (\\nabla f(y) - \\nabla f(x))\\|^2$, which is quite cheap computationally to implement.\n\nUsing a second order approximation of the log target leads to sampling from a Ising-like distribution. As has already been discussed in other article [1], this can be approached by using the standard Gaussian integral trick (i.e. Hubbard-Stratonovich transformation). In order to implement this approach, a diagonal matrix $D$ such that $W+D$ is positive definite needs to be chosen. The authors propose to choose $D$ so that $W+D$ as \"isotropic\" as possible, which is a sensible and standard strategy that appears to work well in this case.\n\n\n### References\n1. Rhodes B, Gutmann M. Enhanced gradient-based MCMC in discrete spaces. arXiv preprint arXiv:2208.00040. 2022 Jul 29.",
            "strength_and_weaknesses": "## Strengths\nThe paper reads well and the proposed approached is very natural and convincing. Furthermore, the simulations are quite well executed.\n\n## Weakness\nI do not see any particular weakness.\n1. Indeed, the proposed methods is extremely natural after one has read the recent paper [1] but the article seems (?) to have been written at the same time.\n2. I am not really sure we need the discussion at the bottom of page 3 (eg. Equations 5/6/7) since this does not seem to add much to the text. But it certainly does not hurt.\n\n### Very Minor comments\n1. The authors seem to parametrize Gaussian distributions with mean and standard-deviation $N(\\mu, \\sigma)$ instead of mean and variance $N(\\mu, \\sigma^2)$, which is a bit awkward (is it common in some communities?).\n2. Should it be $K_\\sigma$ instead of $H_\\sigma$ in Equation 5. Also, $k_\\sigma$ is used of $K_\\sigma$ on page 2.\n\n### References\n1. Rhodes B, Gutmann M. Enhanced gradient-based MCMC in discrete spaces. arXiv preprint arXiv:2208.00040. 2022 Jul 29.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity:\nthe article is clearly written and very easy to follow.\n\n## Quality:\nThe proposed method is convincing and natural. It appears to empirically work extremely well.\n\n## Novelty:\nRelatively weak: the tuning part of $\\alpha,\\sigma$ is entirely straightforward. The tuning of $W$ is interesting. The use of the Gaussian trick is also very standard.\n\n## Reproducibility:\nEnough detail to reproduce everything.",
            "summary_of_the_review": "The article proposes an extension of the work of Zanella to take second order effect into account (i.e. preconditioning) when sampling from discrete distributions, which is very important in practical scenarios (i.e. preconditioning can lead to very large improvements in sampling efficiency). Even if the approach is relatively straightforward, it works well and the numerical simulations are convincing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no concern.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3701/Reviewer_Ce4Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3701/Reviewer_Ce4Q"
        ]
    }
]