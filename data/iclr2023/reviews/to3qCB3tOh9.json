[
    {
        "id": "6saYRuB9i3",
        "original": null,
        "number": 1,
        "cdate": 1666467255605,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666467255605,
        "tmdate": 1669047613357,
        "tddate": null,
        "forum": "to3qCB3tOh9",
        "replyto": "to3qCB3tOh9",
        "invitation": "ICLR.cc/2023/Conference/Paper1473/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a simple SE(3) invariant model and pretrains it on large dataset of alphafold predicted structures. This is then finetuned on a handful of downstream tasks, where it achieves better results than some prior methods.",
            "strength_and_weaknesses": "Strengths:\n* The paper is technically sound, and the method presented is relatively simple and scalable, so could be extended to training on larger datasets relatively easily.\n* The paper is well written and easy to follow + understand\n* Baselines chosen are reasonable and convincing that this method performs well on the chosen downstream tasks.\n\nWeaknesses:\n\n1) The downstream evaluations chosen are just not that convincing to me. This set of tasks has become somewhat standard among a subset of the geometric modeling community, but these tasks are fairly contrived, and the results are not so impressive.\n    - 3D structure-based models are *provided the structure as input*. This is a huge amount of information relative to sequence-based models (as the authors state in the paper). So I am simply confused as to why they perform only slightly better than sequence based models such as ESM-1b. I would expect that, given the structure, it would be very simple to outperform models such as ESM-1b for many function-prediction based tasks, especially since ESM-1b likely contains less information than a multiple sequence alignment.\n    - Before breaking out a neural network predictor, the approach I would take to classify functions based on structure would be to use a structure search tool, such as foldseek. A simple baseline therefore would be a 1-NN algorithm: take the test set structure, use foldseek to search the training set, and return the label of the structure with best similarity. Is the approach proposed here better than that baseline?\n    - The one task where structure-based methods show a clear advantage is the fold classification task. However, the construction of this task deliberately designed to advantage structure-based models over sequence-based models. The dataset splits were constructed such that the goal was to match inputs with very different sequences, but similar structures (i.e. remote homology detection). When the structure is provided, however, it is not clear why this task is interesting. I would expect the baseline I suggested above to perform very well.\n\n2) There are two application areas to my knowledge where structure-based modeling has shown a great deal of promise. The first is inverse folding (e.g. Dauparas et al. 2022, https://www.science.org/doi/10.1126/science.add2187), which has enabled breakthroughs in de novo protein design. The second is structure-based search (van Kempen et al. 2022, https://www.biorxiv.org/content/10.1101/2022.02.07.479398v4) which uses a small neural network to speed up structural searches, an important task given the development of large scale protein structure databases. \n\nThe second task (structure-based search) seems like a very promising application for the model, and would be much more convincing than those shown in the paper. Given the contrastive nature of the training method, structural similarity search seems like a natural application (although care would have to be taken to ensure generalization to novel structures outside the method's training set).\n\n3) The inability to model side chains seems like a potential drawback for structure-based methods. Especially in regards to function / binding affinity prediction, this may hinder the ability to model the output better than sequence-based models can.\n\nMinor Comment: How much does the use predicted structures affect the model's capabilities? What is the relationship e.g. between AlphaFold pLDDT and model accuracy if predicted structures are used?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written and easy to follow.\n\nQuality: The paper is technically sound. Evaluations are correctly implemented with proper train/test splits. The equivariant model is equivariant.\n\nNovelty: The paper is not that novel. On the modeling side, equivariance is achieved by ensuring the model only operates on equivariant features, which is a common approach. No major new capabilities (modeling side chains, handling uncertain inputs) is discussed. On the pretraining side, Hsu et al. 2022 did perform large scale structure-based pretraining in the context of an inverse-folding model and showed significant benefit.\n\nReproducibility: The results seem reproducible. Previously published datasets and splits are used. The authors promise to release code + model weights upon publication.",
            "summary_of_the_review": "The paper is technically sound and is likely an improvement on the prior methods they benchmark against (e.g. Hermosilla et al). However, the paper does not add major novel insights, and does not have strong results on actually interesting downstream tasks.\n\nI recommend the authors look into areas where structure-based modeling is actually used in practice and seeing how their model may be applied.\n\n*********** After Author Response ***********\n\nThe authors have performed significant additional work, including adding the baseline I requested (kNN-1 search via Foldseek) and a benchmark on structure search as a task. The results on this are very promising since fast structure-based search is actually a current pressing issue in the field. Combining accurate cosine-similarity based search with tools like FAISS is potentially a highly valuable tool for the community, and something I would encourage the authors to pursue.\n\nOverall, I now believe this paper is a good candidate for acceptance at ICLR and have revised my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1473/Reviewer_roE6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1473/Reviewer_roE6"
        ]
    },
    {
        "id": "EUI0_BDjFt7",
        "original": null,
        "number": 2,
        "cdate": 1666532861005,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666532861005,
        "tmdate": 1666533369424,
        "tddate": null,
        "forum": "to3qCB3tOh9",
        "replyto": "to3qCB3tOh9",
        "invitation": "ICLR.cc/2023/Conference/Paper1473/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "While a large part of the literature focuses on learning protein representation from their amino acid sequences (allowing pre-training from huge existing database of known protein sequences), the authors suggest a novel approach to learn representation from their 3D structure.\nThe first contribution lies in the definition of a novel encoder called GearNet, based on a relational Graph Neural Network.\nThe protein at hand is initially represented as a graph, where each node (i.e. the alpha carbon of a residue) is characterized by its 3D position in its observed or predicted structure.\nThe edges are added based on closeness in the sequence or in an Euclidean sense (nodes within a certain radius and K-nearest neighbor nodes), if they are not close in a sequence sense.\nTo obtain the protein representation, the aforementioned graph is passed through a relational GNN.\nThe authors suggest a second contribution to explicitly model interactions between edges of the graph: they design an edge message passing layer, coined GearNet-Edge.\nIt is based on a relational graph where the nodes are the edges of the aforementioned graph and the edges in this new graph represent the (discretized) angles between the adjacent edges in the previous graph.\nA straightforward message passing layer is defined over that new graph and the aggregation function in GearNet-Edge combines the aggregation functions defined over both graphs.\nAs another contribution, the authors suggest to pre-train the resulting encoders on large collections of unlabeled protein structures.\nTo do so, they suggest a procedure inspired from SimCLR where different, random, views of a protein graph are created by cropping their full graph and randomly masking some of the edges.\nPairs of views are generated and labeled as positive if they originate form the same initial protein.\nThey also suggest more straightforward baselines to pre-train their encoders based on self-prediction where the encoders are used to perform masked predictions (residue type, distances etc.) on single, pair, triplets or quadruplets of residues.\nFinally, the authors validate the approach by comparing their (pre-trained or not) method with different baselines and sequence and structure-based approaches from the literature, on 4 different downstream prediction tasks: enzyme commission number, gene ontology term, fold classification, reaction classification.\nThey also perform an ablation study to validate the different contributions more individually.",
            "strength_and_weaknesses": "Strength:\n- Protein modeling / representation is indeed a very important topic that has a high impact both inside and outside biomedical applications. By tackling this problem in a relatively generic way (the representations are not tied to a specific downstream task), this paper definitely has a very high potential impact.\n- The suggested approach seems to be well \"rooted\" in the current trends in the field. Recent breakthroughs offered by AlphaFold2 to (mostly) accurately predict protein structure definitely offer new opportunities for designing methods that leverage 3D structure information.\n- To my knowledge, the related work section is well documented and it is relatively clear where the authors' contributions are.\n- The experimental section of the paper is quite elaborate with GearNet* being evaluated on various tasks, datasets and against a relatively large number of competing methods and baselines.\n- The amount of work seems substantial, to the point where the authors provide 10 pages of supplementary material with a lot of interesting comments and results.\n\nWeaknesses:\n- While the idea of having a better general protein representation (as opposed to one tailored for a specific downstream task) is of course seducing, it makes for a much stronger claim that is hard to completely substantiate. The results seem good on various downstream tasks but it is of course not clear how that would carry over other relevant tasks (for instance anything pertaining to predicting protein-protein interactions).\n- While the paper reads well overall, I find that some parts lack clarity. In general, many choices of modeling could be better justified and/or offer more insights.\n- Notably, from reading the \"Protein graph construction\" subsection, I was confused about which type of features where used for the nodes. Only the spatial coordinates are mentioned, while not actually used and I had to go deep into the supplements to really understand that the type of residue, e.g. the actual sequence information was directly used.\n- I find Section 3.2 to also lack clarity. Can the authors offer a bit more insight on what kind of information the angles, that determine the edge types, actually give? Can you describe more clearly how it's different (and why a better choice) from the representation used in AlphaFold2?\n- Why are those angles (in section 3.2, but also in 4.2) discretized? Would it not be a more straightforward option to keep the angles as float values (and perform regression in 4.2)?\n- Despite some efforts, I do not really understand the assumption underlying the contrastive learning approach. Especially for proteins with longer and less redundant sequences, why should different views, describing potentially radically different regions of the protein, be more related to each other than views taken from different proteins?\n- The construction of the views, mixing subspace and subsequence sampling, along with the addition of masking or not, seems a bit convoluted to me. From reading the manuscript, I don't get a strong intuition why this mixture is meaningful and the results reported in Table 3 seem to indicate that adding the subsequence sampling and identity transformation does not yield better performance.\n- I am not sure I really like the argument that the proposed method pre-trains with much less data than sequence-based approaches. While the number of proteins used for pre-training is indeed much lower, much more data per protein is being used when incorporating 3D structure.\n- Related to the previous point, it would be nice to see some runtime and memory usage analysis of the different methods used.\n- Although it is qualitative, and not entirely compelling because the analysis has been performed only for the author's model, and it could be that they are hand picked, I find the results presented in Appendix H to be very interesting. It would be nice to at least put a reference to it in the main text.\n\nMore open question:\nThe current model includes mostly structural information about the backbone of the protein, which seems to be sufficient for the chosen downstream tasks. For tasks like complex interaction prediction, that probably will be a bottleneck.\nDo the authors have suggestion as to how to extend the current model to include side chain information?\n\nMinor:\n- Enzyme Commission acronym EC is used in section 4.1 but only defined later in 5.1.\n- There's a typo just before 4.2 \"subsequecne\".",
            "clarity,_quality,_novelty_and_reproducibility": "I think the quality of the work is quite high. There are always holes (e.g. no side chain information being used), especially as there are unlimited valid ways to evaluate the quality of a \"representation\", but given the amount of different contributions and results, it would be unreasonable to request more for an ICLR paper.\nMaybe this work would be a better fit for a journal offering a longer format?\nIn my opinion none of the contributions presented is ground-breaking in itself but that does not need to be a criticism. The value of this work lies in putting together the right pieces together (hence the need for justifying the choices) and performing a relatively deep experimental validation.\nThe clarity is, the part that I think could be more easily improved. Please refer to the points I raised in the previous section of the review for more detailed suggestions.\nThe reproducibility of the work is currently hindered by the code not being accessible. This being said, the authors have committed to making it available upon acceptance and provide enough details about the architecture and (numerous!) hyperparameters for the motivated reader to try and reproduce the result.",
            "summary_of_the_review": "Overall, the authors suggest a novel method to learn efficient representations of proteins, leveraging their 3D structure.\nFollowing the aftermath of the recent release of AlphaFold2 and their prediction of the structure of a huge amount of new data, this contribution seems well-timed and seem to result in good performance compared to other very recent methods.\nI hope that this review process will offer the authors the opportunity to especially improve the clarity of the current manuscript.\nThat being said, I believe that time is of the essence as I suspect that a handful of related methods are currently being developed and published.\nGiven the general quality, I recommend that this paper be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1473/Reviewer_86an"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1473/Reviewer_86an"
        ]
    },
    {
        "id": "jOA2uyQJ74",
        "original": null,
        "number": 3,
        "cdate": 1666576799286,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576799286,
        "tmdate": 1666576885106,
        "tddate": null,
        "forum": "to3qCB3tOh9",
        "replyto": "to3qCB3tOh9",
        "invitation": "ICLR.cc/2023/Conference/Paper1473/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, authors propose a 3D structure-aware pre-training method for protein representation learning. Two novel structure-based protein encoders, GearNet and GearNet-Edge, are proposed to derive per-residue and whole-protein representations. The multi-view contrastive loss is adopted to maximize the similarity between different sub-structures of the same protein, while minimizing those from different proteins. Empirical evaluation is conducted on four down-stream tasks to verify the effectiveness of the proposed protein encoder and pre-training learning mechanism.",
            "strength_and_weaknesses": "Pros:\n1. The construction of protein graph with multiple edge types, each associated with a GCN kernel matrix is novel. Additionally, as shown in Table 3, there is a significant performance boost after introducing edge-type-specific convolution (GearNet-Edge vs. w/o relational convolution).\n2. Different from commonly used self-supervised learning objectives that randomly masking out certain residues and/or coordinates, the proposed multi-view contrastive learning looks interesting and is reasonable to some extent.\n3. The empirical evaluation is extensive, which includes four down-stream tasks and various network architectures and pre-trained models. The ablation study also reflects the importance of some key components of the proposed method.\n\nCons:\n1. The proposed protein encoder only considers C-alpha atoms, which may be insufficient for certain down-stream tasks that require more fine-grained formulation of protein structures. For instance, to predict the binding affinity between protein and ligands, protein side-chain conformation should be considered.\n2. The ablation study on the necessity of relation convolution in Table 3 is not entirely convincing. By replacing edge-type-specific convolutional kernels with a single shared one, the number of learnable model parameters is greatly reduced, which may limit the model capacity. How about a deeper network with shared convolutional kernel?\n3. The proposed protein encoder is invariant to arbitrary translations, rotations, and reflections. I am not sure whether reflection-invariant is indeed necessary for protein representations since native L-amino-acid and unnatural D-amino-acid (only differ in the handedness) may have different physicochemical and biological properties.\n4. For self-prediction baselines, the constructed protein graphs may have permanently lost detailed 3D coordinate information in the input node features, which may limit the self-prediction learning performance. How about those GNN models that explicitly consider node-wise 3D coordinates, e.g., EGNN?\n5. In Table 3, it seems that \u201csubspace + random edge masking\u201d works better, even outperforming \u201cGearNet-Edge (Multiview Contrast)\u201d. So, why not directly using this for sampling sub-structures, rather than alternating between different sampling schemes?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Well-written, easy to follow.\n\nQuality: Extensive experiment results.\n\nNovelty: The proposed multi-view contrastive learning objective looks novel to me.\n\nReproducibility: Authors have claimed that all code and models will be released upon acceptance.",
            "summary_of_the_review": "The overall quality is satisfying, but there are still a few unresolved concerns (as listed in the \u201cCons\u201d section) that may need further discussion and clarification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1473/Reviewer_swLM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1473/Reviewer_swLM"
        ]
    },
    {
        "id": "1jeFfY0DmA",
        "original": null,
        "number": 4,
        "cdate": 1667451130451,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667451130451,
        "tmdate": 1667451130451,
        "tddate": null,
        "forum": "to3qCB3tOh9",
        "replyto": "to3qCB3tOh9",
        "invitation": "ICLR.cc/2023/Conference/Paper1473/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a method to leverage unlabeled protein structural data to improve the prediction of various protein properties.  The authors present two main contributions: (1) a geometric encoder for protein structures, and (2) a multiview contrastive learning pretraining strategy, which together lead to state-of-the-art performance on several protein property prediction tasks.  The geometric encoder makes use of edge messaging passing, which the authors claim had not been used for macromolecular representation learning.  The use of different kinds of edges (sequential, radius, KNN) is also claimed to be novel for this task.",
            "strength_and_weaknesses": "The empirical results are compelling, demonstrating that the new wealth of structures can be successfully used to improve sequence-based models.  The proposed architecture is conceptually straightforward and does not include any particularly novel algorithmic contributions, though several components have not been used in the macromolecular representation learning area.  The ablation studies are a very nice addition as well.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clear, and of good quality.  The methods used are not particularly novel.  The code and models are not currently provided, though the methods seem to be sufficiently described to be reproducible.\n\nMinor typo: In section 4.1 the heading \"contastive learning\" is missing an 'r'.",
            "summary_of_the_review": "I recommend weak acceptance.  In my opinion, the empirical results and overall clarity of the paper outweigh the lack of algorithmic novelty, though the lack of code prevents better assessing reproducibility.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1473/Reviewer_pcxa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1473/Reviewer_pcxa"
        ]
    }
]