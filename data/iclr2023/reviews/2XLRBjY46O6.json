[
    {
        "id": "xteU9GCARjj",
        "original": null,
        "number": 1,
        "cdate": 1666702126299,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666702126299,
        "tmdate": 1666702126299,
        "tddate": null,
        "forum": "2XLRBjY46O6",
        "replyto": "2XLRBjY46O6",
        "invitation": "ICLR.cc/2023/Conference/Paper2694/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to improve text-supervised segmentation model i.e., GroupVIT by enforcing multi-view semantic consistency. Specifically, a text-to-views consistency module is included to encourage different views to be aligned with the same text input. In addition, there is a cross-view consistency module which enforces features from the different views of the same input image to be closer. The authors conduct extensive experiments on the semantic segmentation benchmarks and show SOTA results.",
            "strength_and_weaknesses": "Strength\n\n-The idea of multi-view consistency is new in visual-language learning. This paper proposes a new method to implement the idea. \n\n-This paper outperforms GroupVIT consistently on multiple benchmarks.  \n\nWeakness\n\n-A potential related work is missing. The DenseCL [A] paper studies multi-view consistency for self-supervised contrastive learning. The author should discuss the difference.\n\n[A] Dense Contrastive Learning for Self-Supervised Visual Pre-Training. Want et al., CVPR'21",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The paper has a good quality and is evaluated extensively.\n\nClarity: The paper is written well and easy to understand.\n\nOriginality: The proposed method is new.\n",
            "summary_of_the_review": "Overall, I think this is a good paper with sufficient novelty and good results. I would recommend to accept it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2694/Reviewer_kMrx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2694/Reviewer_kMrx"
        ]
    },
    {
        "id": "IA1VYKg434",
        "original": null,
        "number": 2,
        "cdate": 1666772825549,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666772825549,
        "tmdate": 1666772905656,
        "tddate": null,
        "forum": "2XLRBjY46O6",
        "replyto": "2XLRBjY46O6",
        "invitation": "ICLR.cc/2023/Conference/Paper2694/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies text-supervised semantic segmentation. The authors propose multi-View Consistent learning (ViewCo) to introduce the correspondence among multiple augmented views of the same image. The experimental results on serval datasets demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength\n+ Compared to the baselines and other methods, the proposed methods could bring some improvements in accuracy.\n+ Some ablation experiments are introduced to prove the effectiveness of the proposed method.\n\nWeaknesses\n-  The definition of positive pairs and negative pairs in the cross-view consistency module is missing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "There are many hyperparameters and implemental details in the new system, so it could be difficult to reproduce the experimental results without the author's source code. ",
            "summary_of_the_review": "Overall, it is a solid paper and gives a reasonable idea to improve text-supervised semantic segmentation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2694/Reviewer_wv76"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2694/Reviewer_wv76"
        ]
    },
    {
        "id": "0kwRZ6hZ41",
        "original": null,
        "number": 3,
        "cdate": 1667419348804,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667419348804,
        "tmdate": 1667419348804,
        "tddate": null,
        "forum": "2XLRBjY46O6",
        "replyto": "2XLRBjY46O6",
        "invitation": "ICLR.cc/2023/Conference/Paper2694/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper improves Group VIT, a text-supervised segmentation model, by introducing region-wise cross-view consistent regularization. The author benchmarked for zero-shot segmentation capability across multiple datasets and showed improvements over several baselines. \n\nMy primary concern with this work is selecting the positive segmentation across views, where the author assumes the segments with the same id form positive pairs. I think this assumption is wrong, and I'll expand the discussion to the Weaknesses section. \n\nThis work's contribution is also limited since most of its success is built upon the Group VIT model. The quantitative improvements from the newly introduced regularization are not substantial, around 1-2% over Group VIT.   \n\n\n\n\n\n",
            "strength_and_weaknesses": "### Strength:\n\n(1) This work improves Group VIT by enforcing view consistency\n\n(2) Author conducts substantial experiments across multiple datasets and shows consistent quantitative improvement over the baseline. \n\n### Weaknesses:\n\n(1)  I think the assumption of cross-view segment consistency by segment id is wrong. As observed in other VIT frameworks, which use learnable tokens for instance segmentation or objective detection, the tokens usually have spatial rather than semantic biases. Based on that, the segment id primarily indicates the segmentation's relative spatial location to the image, and matching group ids does not make sense (we can simply horizontally and vertically flip the images to break the assignment).  \n\nInstead of directly matching segment ids, I think reasonable alternatives are: (a) Compute bipartite matching based on group features similarity and (b) Spatially warp one view to another and compute the segmentation overlapping to match the segments. Then ignore the segment with zero overlapped pixels.   ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the motivation is clear. However, the novelty is limited, and the major assumption doesn't hold. ",
            "summary_of_the_review": "Given my concerns over the assumption of the objectives and the limited improvement over the baseline, I tend to reject this submission. \n\nI'll reconsider my rate if the author can justify why using segment id for cross-view consistency makes sense or provide experimental results to show that segment ids indicate semantic categories rather than spatial bases. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2694/Reviewer_cqK1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2694/Reviewer_cqK1"
        ]
    },
    {
        "id": "8Ov6dQDBIp",
        "original": null,
        "number": 4,
        "cdate": 1667983330983,
        "mdate": 1667983330983,
        "ddate": null,
        "tcdate": 1667983330983,
        "tmdate": 1667983330983,
        "tddate": null,
        "forum": "2XLRBjY46O6",
        "replyto": "2XLRBjY46O6",
        "invitation": "ICLR.cc/2023/Conference/Paper2694/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on text-supervised semantic segmentation by studying a multi-view consistency learning framework. Specifically, a text-to-views consistency and a cross-view segmentation consistency training strategy are proposed and added upon existing work Group VIT. The proposed method tries to construct more training constrains based on correspondence among multiple augmented views to obtain more improvements. Experiments are conducted on multiple datasets for zero-shot segmentation, e.g. PASCAL and COCO., showing improvements upon some existing baselines. \n",
            "strength_and_weaknesses": "\nStrength:\n\n+ The motivation of introducing multi-view consistency constrain to text-supervised semantic segmentation is reasonable.\n\n+ Experimental reuslts on multiple datasets show some benifits compared with existing methods. \n\n\nConcerns: \n\n- The general idea of introducing multi-view consistency constrains to vision-language pretraining models is not new. Some existing works, e.g. DeCLIP [a] have explored adding such multi-view self-supervision constrains and validated its effectiveness when learning vision-language models. The differences here are mainly about applying multi-view constrains on segment level rather than whole image when building upon the framework of Group VIT.\n\n[a] Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm, ICLR 2022\n\n-  Cross-view constrains are applied on segments of two augmented views. Since there is no guarantee for location (segments) correspondence between these two augmented views, the proposed strategy in Figure 5 of constructing positive and negative pairs for the corss-view segments seems not reasonable. \n\n- The performance of the most important baseline GroupViT seems not consistent with the original paper. For example, in Table 1 Comparison with zero-shot baselines on PASCAL VOC 2012, it reports 51.2 mIoU for GroupViT and 52.4 mIoU for the proposed method ViewCo, while in the original paper [b] GroupViT obtains 52.3 mIoU, which is much higher than 51.2 reported in this submission and is only 0.1 lower than the proposed method. Besdies, on PASCAL Context, GroupViT reports 22.4 mIoU, the improvements from ViewCo seems also marginal (23.0 mIoU v.s. 22.4 mIoU). \n\n[b] GroupViT: Semantic Segmentation Emerges from Text Supervision, CVPR 2022",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow. The novelty is limited due to the concerns raised in the Strength And Weaknesses section. The code is not provided for reproducibility.",
            "summary_of_the_review": "My current rating is based on concerns for limited technical contributions, unreasonable positive/negative pair selection for cross views, quite limited improvments upon baseline GroupViT. Please find more details in the Strength And Weaknesses section.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2694/Reviewer_Dedj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2694/Reviewer_Dedj"
        ]
    }
]