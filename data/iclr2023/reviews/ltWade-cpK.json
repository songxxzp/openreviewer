[
    {
        "id": "DUSXsBFQSLg",
        "original": null,
        "number": 1,
        "cdate": 1666571036981,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666571036981,
        "tmdate": 1666571036981,
        "tddate": null,
        "forum": "ltWade-cpK",
        "replyto": "ltWade-cpK",
        "invitation": "ICLR.cc/2023/Conference/Paper2279/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper designs optimal activation functions for the Random Feature Regression model: a special case of a two layer neural network. Here, consider a two-layer neural network with inputs of dimension $d$, a second layer of dimension $N$, and a single output node. The mapping from the input to the hidden layer is generated as a random matrix, passed through a nonlinearity function we can pick, and Ridge regression is used to learn a linear combination of the hidden layer parameters and predict the output value.\n\nThis paper studies proves what activation functions are optimal in various limits as the problem grows larger. \"Optimal\" is characterized in a variety of ways, like balancing mean squared error of the solution against the flatness of the optimization landscape, or by picking regularization criteria on the activation functions (so when there's many possible optimal activation functions, we still typically have a unique solution). The paper always studies the limit as $d \\rightarrow \\infty$, with analysis specialized to three cases:\n1. _Unregularized Case_: The limit as $d \\rightarrow \\infty$ and $\\lambda \\rightarrow 0$ (the regularization parameter), but $\\frac{d}{n}$ and $\\frac{d}{N}$ remain constant\n1. _Way Overparameterized Case_: The limit as $d \\rightarrow \\infty$ and $\\frac{d}{N} \\rightarrow 0$, but $\\frac{d}{n}$ and $\\lambda$ remain constant\n1. _Large Sample Complexity Case_: The limit as $d \\rightarrow \\infty$ and $\\frac{d}{n} \\rightarrow 0$, but $\\frac{d}{N}$ and $\\lambda$ remain constant\n\nIn all cases generated by all combinations of these parameters, exact characterizations of the optimal activation function are given. The details and notation get a bit laborious, but the theory provides a nice model for understanding the value of different activation functions. Like in the limit when we have a huge number of samples, we should just use a linear activation function, making the overall neural net linear. Or in showing that choosing the right activation function can completely avoid double descent, whereas the same data with a ReLU activation function would suffer from the spike in double descent.\n",
            "strength_and_weaknesses": "The paper is long, and the detailed math is....pedantic, but the key takeaways are really neat and the bulk of the math is buried in the appendix (as it should be).\n\n---\n\nLet me first note that I'm not an expert on anything in this paper, especially the deep learning theory literature. I can't really speak to novelty or significance, so I'll leave that to the other reviewers. I'll just give my impressions, as a relative outsider who can read a nice proof sketch and enjoy a good paper.\n\nThis paper was pretty fun to read. It's got a precise statistical model that's been used to study neural nets in the past. The paper's got really precise results on the shape of optimal activation function. The proofs are long, but the proof sketches in the body of the paper are short and sweet and clear. The paper's got really nifty implications of these optimal functions. These are the key strengths of the paper. I recommend publishing the paper.\n\nThis paper kinda feels like a shoe-in to get accepted. I only really see two potential weaknesses in this paper:\n1. The mathematical choice of norms and objective function and regularization and generative model are all very arbitrary. These are arbitrary, and the authors notes these are arbitrary but have some foundation in recent research (e.g. bottom of page 3, top of page 4; bottom of page 9). It's a theoretical exploration and these choice are arbitrary so they have to analyze _something_. It's a weakness, but not a serious one.\n1. There's no way for someone to review all of the math in the appendix in time. This is an issue with the broader peer review process for ML though. Not the fault of the paper in the slightest.\n\nThe paper is well written, intuition is regularly given, and for a paper with such mathematically precise results in so many different cases of analysis, the huge list of optimal activation functions is broken down in an impressively not-unapproachable way.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and clear.\nI can't really comment much on novelty given that I'm not particularly familiar with the prior work in the area.\nIt certainly seems to be an original result though.\n\n\n## Some typos or minor edit recommendations\n1. [Page 1, second paragraph of section 1, the line that starts with \"optimize AFs have\"] missing the word \"to\" before \"optimize\"\n1. [Across the entire paper] Please use a symbol other than O for the overall loss. How about $L$?\n1. [Page 3, just before the bullet points] Why mention the last paper in the list was published at ICLR last year. This is odd to point out, no? The other papers in the list were published at NeurIPS, ICML, and The Annals of Statistics; all prestigious in ML.\n1. [Page 3, just before the bullet points] The very last sentence in this paragraph (\"Although theoretical...\") reads really weirdly. I don't know what you're trying to say, but I don't think it's working. Consider just cutting the line, or rewrite it from scratch?\n1. [Page 4, paragraph after equation 12] Can you clarify why $F_{\\star}$ is like the norm of the noise? The expectations in the definition of $\\Sigma_d$ already looks pretty odd, so I don't understand the intuition you give about what $F_{\\star}$ is doing.\n1. [Page 5, theorems 2-6] consider pushing all of these polynomials to the appendix, like you do for later theorems. You could even write something like\n>$$E_{R_2}^{\\infty} = \\lim \\lim E = F_1^2 p_1 + (\\tau^2 + F_\\star^2) p_2 + F_{\\star}^2$$ where $p_1$ is a rational function in $\\zeta, \\psi,$ and $\\bar\\lambda$, and where $p_2$ is a rational function in $\\zeta,\\psi_2$ and $\\bar\\lambda$. Both rational functions are shown in Appendix XYZ.\n1. [Page 7, equations 27 and 29] The formatting on where the equation number are seems to be broken. Also, you can probably just get rid of these equation numbers? And probably most of the equation markers in this paper could be removed? Y'all's choice though.\n1. [Page 7, remark 3] When you say $\\psi \\in \\{A,B\\}$, what are $A$ and $B$?\n1. [Page 7, last paragraph] Isn't this trivial that the choice of scaling in the AF won't impact the overall error in the unregularized case.... because it's unregularized so the ridge-less regression model will just compensate?\n1. [Page 8, proof sketch] \"which turns out to be\" should be \"which *turn* out to be\"\n1. [Page 9, Figure 1] Isn't $\\rho$ the symbol for SNR, and not $\\tau$? Why not just actually use the symbol $\\rho$ here?\n",
            "summary_of_the_review": "Nicely intuitive and very well written paper.\nI don't know how significant it is, since I don't especially work in this area.\nStill seems like it should be published imo.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2279/Reviewer_7nWY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2279/Reviewer_7nWY"
        ]
    },
    {
        "id": "nGn_RePw40M",
        "original": null,
        "number": 2,
        "cdate": 1666603808722,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603808722,
        "tmdate": 1668761377762,
        "tddate": null,
        "forum": "ltWade-cpK",
        "replyto": "ltWade-cpK",
        "invitation": "ICLR.cc/2023/Conference/Paper2279/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies random features ridge regression where the data $\\{(x_i, y_i)\\}_{i=1}^n$ is generated from a noisy nonlinear model $y_i=f_d(x_i)+\\epsilon_i$ with the covariates $x_i$ i.i.d. uniformly sampled from $\\mathbb{S}^{d-1}(\\sqrt{d})$. \n\nThe estimator is given by $f_{a,\\Theta}(x)=\\sum_{i=1}^{N}a_i\\sigma(\\langle\\theta_i, x\\rangle/\\sqrt{d})$ with $\\sigma$ being the activation function and $\\Theta\\in\\mathbb{R}^{N\\times d}$ with $i$th row $\\theta_i\\in\\mathbb{R}^d$ satisfying $||\\theta_i||=\\sqrt{d}$, where $\\theta_i$ is i.i.d. uniform on $\\mathbb{S}^{d-1}(\\sqrt{d})$. The authors aim at finding the optimal activation function $\\sigma$ in the asymptotic regime by minimizing a linear combination of the test error and sensitivity of the random features regression models.",
            "strength_and_weaknesses": "$\\textbf{Strength}$\n\n1. This is a novel contribution that provides insights into the understanding of finding optimal activation functions for random features regression models in the asymptotic regime.\n\n2. This paper provides some interesting observations on the benefit of finding optimal activation functions (and figure out whether they are linear or nonlinear) from the theoretical perspective.\n\n3. Codes are provided.\n\n\n$\\textbf{Concerns}$\n\n1) How to use these theoretical findings in practice? e.g., how to choose $\\alpha$;\n\n2) How to combine the optimization problems (2) and (8)?\n\n3) The theoretical results only apply to shallow architecture, which may restrict their significance.\n\n4) All theorems in Section 3.1 are the special cases of Mei&Montanari (2022), which are essential for the main results in Sections 4.1 and 4.2. \n\nMinor typos:\n1) Below (23), Guass error should be Gauss error",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-organized and clearly written. The results are technically sound and novel.",
            "summary_of_the_review": "This work provides some interesting insights on the understanding of finding optimal activation functions in the random features regression models, which should be of interest to the ICLR community. However, I am not quite sure whether these theoretical findings can benefit the design of learning algorithms in practice, esspecially for the deep architecture case.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2279/Reviewer_X96N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2279/Reviewer_X96N"
        ]
    },
    {
        "id": "KMcSIYS-Lr",
        "original": null,
        "number": 3,
        "cdate": 1666625789730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625789730,
        "tmdate": 1669593219586,
        "tddate": null,
        "forum": "ltWade-cpK",
        "replyto": "ltWade-cpK",
        "invitation": "ICLR.cc/2023/Conference/Paper2279/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work considers the problem of finding optimal activation functions for a random features regression setting under a performance vs. sensitivity trade-off. Under the same setup as in [Mei, Montanari '22] (spherical input data, linear target function + noise, Gaussian random features projection), the starting point of the analysis is the exact formula derived in [Mei, Montanari '22] for the asymptotic generalisation error of the model in the proportional regime.\n\nThe main new technical contribution is an exact asymptotic formula for the sensitivity (squared norm of the model gradient). Combining this with the asymptotic formula for the error allow the authors to study the optimal RF activation for the different regimes of interest by minimising a convex combination of the error and the sensitivity. In particular, the authors discuss when it is better to have a linear vs. non-linear activation, and how the optimal choice of activation compares with optimal cross-validation of the $\\ell_2$ penalty.",
            "strength_and_weaknesses": "**Strengths**:\n\n- Given the cumbersome asymptotic formulas, getting sharp results for the optimal activation is a quite impressive *tour de force*.\n- The authors make an effort to connect their discussion to more realistic settings, despite leaving this to the Appendix F.\n\n**Weaknesses**:\n\n- The work strongly builds on previous theoretical results, with only small novel technical contributions.\n- The analysis is limited to ridge regression with Gaussian random features, despite the progress in the literature deriving similar asymptotic results for more general feature matrices, losses and regularisation.\n- The discussion is sometimes hard to follow, specially because of the heavy notation which requires the unfamiliar reader to go back and forth for the definitions.\n\n**Comments**:\n\n- **[C1]**: A drawback of this work is that it lacks a discussion of some important intuition underlying the key conclusions. For instance, I believe some of the results in this paper can be directly understood from Gaussian equivalence of random features in the proportional regime. For some context, Gaussian equivalence builds on early spectral universality results in the kernel regression literature [El Karoui '10; Pennington, Warah '17] and is an important ingredient for the derivation of the exact asymptotics in the regression case [Mei, Montanari '22], but it also allow one to extend the exact asymptotic analysis of the RFs model to other loss functions and projection matrices, as shown in [Gerace et al. '20; Goldt et al. '22] and later proven in [Hu, Lu '20].\n\nGaussian equivalence states that the asymptotic statistics of the random features estimator is equivalent to the asymptotic statistics of an equivalent Gaussian problem. Employing the notation in the paper, one way of writing is:\n$$\n\\sigma(\\Theta x_{i}) \\asymp \\mu_{0} 1 + \\mu_{1}\\Theta x_{i} + \\mu_{\\star} z_{i}\n$$\nwhere $z_{i} \\sim\\mathcal{N}(0,I_{N})$ is an uncorrelated noise. Therefore, the problem considered here is asymptotically equivalent to:\n$$\ny_{i} = f_{d}(x_{i})+\\rm{noise} = \\beta_{0} + \\langle\\beta_{1}, x_{i}\\rangle + \\rm{noise}\n$$\n$$\n\\hat{y} = f(x) = \\mu_{0}\\langle a, 1\\rangle + \\mu_{1} \\langle a, \\Theta x\\rangle + \\mu_{\\star}\\langle a, z\\rangle\n$$\nwhere in this regime the non-linear part of the target is also equivalent to additive mismatch noise. From the equivalent model perspective, it is clear that the random features is just introducing a model mismatch that hinders performance. Indeed, since the target is just a linear function in $\\mathbb{R}^{d}$, the optimal predictor would just do linear regression in this space, while the projection $\\Theta x\\in\\mathbb{R}^{N}$ is forcing the model to fit a linear function in $\\mathbb{R}^{N}$. From this characterisation, it is also clear how $\\mu_{\\star}$ plays the role of an effective regularisation $\\lambda$. However, since it also appears in the effective model mismatch noise this leads to the richer behaviour observed by the authors in Fig 1 (C) & (D).    \n\nI am not sure if the authors exploited this intuition, but I believe it would be very important to explicitly discuss it in the manuscript.\n\n- **[C2]**: One important aspect which is missing in the discussion is how much the conclusions depend on the target function. For instance, if the target function itself was given by a random features model with features $\\sigma_{\\star}(\\Theta x)\\in\\mathbb{R}^{N}$ for some activation $\\sigma_{\\star}$, it seems an optimal linear predictor would be obtained by matching the features $\\sigma=\\sigma_{\\star}$. Would this substantially change the conclusion?\n\n- **[C3]**: The heavy use of notation in the discussion can be confusing for the unfamiliar reader, who has to go back and forth to definitions in order to follow. Reminding the reader of what the different quantities mean over the discussion and the plots would be very helpful. For instance, here are some suggestions:\n  - In Fig. 1, instead of having in the x-axis $\\psi_{1}/\\psi_{2}$, one could write directly $N/n$ .\n  - Using $n/d$ and $N/d$ instead of $\\psi_1, \\psi_2$ could be helpful also during the discussion. Instead of $\\psi_{1}<\\psi_{2}$ one can say $N<n$. Of course, this is always understood asymptotically.\n  - Instead of saying \"In regime $R_2$\", one could be redundant and say \"In the highly overparametrised regime $\\psi_{2}\\to\\infty$\". Same for $\\alpha = 0$: \"minimising the error\" or $\\tau=0$: \"noiseless\".\n  - The whole notation in Table 1 is very hard to parse. Unfortunately I don't have a suggestion on how to improve it, but from the reader perspective one just skips it.\n\n- **[C4]**: A few comments concerning the related literature:\n\n  - The sensitivity has been studied in the closely related setting of kernel ridge regression in [Simon et al. '21], where a non-asymptotic formula for this quantity as a function of the kernel spectrum was derived. This should coincide with the expression here in the overparametrised regime $R_{2}$ which corresponds to the kernel limit of random features.\n\n  - In Sec. 3, point 4 the authors mention that:\n    > Existing proof techniques make it very hard yet to extend our type of analysis to more than two layers or complex architectures.\n\n    Although the authors are right that there are no rigorous multi-layer generalisations (except for not-so-interesting cases such as $\\mu_1 = 0$, c.f. [Pennington, Warah '17]), in [Goldt et al. '22, Loureiro et al. '21] it was empirically shown by comparing the learning curves that Gaussian equivalence holds in multi-layer cases, including in cases when the layers $\\Theta$ are pre-trained.\n\n\n**References**:\n\n[[El Karoui '10]](https://projecteuclid.org/journals/annals-of-statistics/volume-38/issue-1/The-spectrum-of-kernel-random-matrices/10.1214/08-AOS648.full) N El Karoui. *The spectrum of kernel random matrices*.  Ann. Statist. 38(1): 1-50 (February 2010). DOI: 10.1214/08-AOS648.\n\n[[Pennington, Warah '17]](https://papers.nips.cc/paper/2017/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html). J Pennington, P Worah. *Nonlinear random matrix theory for deep learning*. Part of Advances in Neural Information Processing Systems 30 (NIPS 2017).\n\n[[Gerace et al. '20]](https://proceedings.mlr.press/v119/gerace20a.html) F Gerace, B Loureiro, F Krzakala, M M\u00e9zard, L Zdeborov\u00e1, \"Generalisation error in learning with random features and the hidden manifold model\", Proceedings of the 37th International Conference on Machine Learning, PMLR 119:3452-3462, 2020.\n\n[[Goldt et al. '22]](https://proceedings.mlr.press/v145/goldt22a.html) S Goldt, B Loureiro, G Reeves, F Krzakala, M Mezard, L Zdeborova. *The Gaussian equivalence of generative models for learning with shallow neural networks*. Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference, PMLR 145:426-471, 2022.\n\n[[Hu, Lu '20]](https://arxiv.org/abs/2009.07669): H Hu, YM Lu. *Universality Laws for High-Dimensional Learning with Random Features*. arXiv: 2009.07669 [cs.IT]\n\n[[Simon et al. '21]](https://arxiv.org/abs/2110.03922) JB Simon, M Dickens, D Karkada, MR DeWeese. *The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks*, arXiv: 2110.03922 [cs.LG]\n\n[[Loureiro et al. '21]](https://proceedings.neurips.cc/paper/2021/hash/9704a4fc48ae88598dcbdcdf57f3fdef-Abstract.html) B Loureiro, C Gerbelot, H Cui, S Goldt, F Krzakala, M Mezard, L Zdeborov\u00e1. *Learning curves of generic features maps for realistic datasets with a teacher-student model*. Part of Advances in Neural Information Processing Systems 34 (NeurIPS 2021).\n\n**Small typos**:\n\n- Below eq. (23): *\"Guass\"* -> Gauss.",
            "clarity,_quality,_novelty_and_reproducibility": "- **Clarity**: As discussed above, I found that the heavy use of notation hinders the clarity of the discussion. An intuitive discussion of the results would also add to the paper.\n\n- **Quality**: Clarity aside, the work is sound and technically correct. The algebra is cumbersome, so it is impressive the authors get sharp results in arbitrary asymptotic regime.\n\n- **Novelty**: I am not aware of other works discussing optimal choices of activation for random features regression. Therefore, to my best knowledge the discussion and results are original. However, as noted above from a technical standpoint the results strongly rely on previous literature.\n\n- **Reproducibility**: Code for reproducing some of the derivations is released with the paper.",
            "summary_of_the_review": "Overall, although the discussion is novel and the sharp results derived from cumbersome algebra is impressive, I think this manuscript could be considerably improved. See the points raised above for some constructive suggestions.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2279/Reviewer_riDW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2279/Reviewer_riDW"
        ]
    },
    {
        "id": "t2b-3MjMwR",
        "original": null,
        "number": 4,
        "cdate": 1667029851407,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667029851407,
        "tmdate": 1668720914943,
        "tddate": null,
        "forum": "ltWade-cpK",
        "replyto": "ltWade-cpK",
        "invitation": "ICLR.cc/2023/Conference/Paper2279/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This submission studies the optimal activation function in a two-layer random features model that minimizes a weighted sum of the test error and the model sensitivity measured by a Sobolev norm. The analysis assumes the proportional limit and spherical data; in this setting both the test error and the sensitivity have been analytically derived in prior works. Using these analytic formulae, the authors presented a few cases where the optimal activation can be either linear or nonlinear. ",
            "strength_and_weaknesses": "## Strength\n\nThis paper contributes to the growing literature on the asymptotics of random features regression. Unlike most prior works that focused on the shape of the risk curve (double descent), the current submission considers the design of optimal activation function, which is a new application of the precise error formulae. \n\n## Weaknesses\n\nMy main concern is on the motivation and implications of the analysis. \n\n1. Under the setting of [Mei and Montanari 2019], random features model in the proportional regime can only learn linear functions on the input. Specifically, the test error of the RF model only depends on the activation function through the first two Hermite coefficients, and is equivalent to that of a Gaussian linear model -- this is often refereed to as the Gaussian equivalence property [Hu and Lu 2020] [Loureiro et al. 2021] [Montanari and Saeed 2022]. \nConsequently, designing the activation function is equivalent to tuning the magnitude of Gaussian noise added to the features (this can be interpreted as implicit ridge regularization), which I do not find very meaningful because the resulting model cannot outperform linear regression on the input (see [Ba et al. 2022] Section 4). \n\n2. Given the Gaussian equivalence property, it is not surprising that the optimal activation function can be linear, which reduces the RF model to linear regression on the input in certain regimes. Similarly, due to the self-induced ridge regularization of the nonlinear activation function, it is intuitive that tuning the Hermite coefficients provides similar benefit as tuning $\\lambda$.  \n\n3. I also do not see the motivation of minimizing the sum of test error and sensitivity; this no doubt makes the computation more involved, but ultimately we are still designing noisy Gaussian linear models. It would be nice if the authors can outline an example where the deigned RF model achieves smaller loss + sensitivity than directly doing ridge regression on the input. Such result can provide some justification of the use of nonlinear random features. \n\n4. It is unclear how the analysis can benefit practitioners in choosing the activation function, since the optimal Hermite coefficients require knowledge of the target function and SNR, which is not known in practice. \n\nHu and Lu 2020. Universality laws for high-dimensional learning with random features.  \nLoureiro et al. 2021. Learning curves of generic features maps for realistic datasets with a teacher-student model.  \nMontanari and Saeed 2022. Universality of empirical risk minimization.   \nBa et al. 2022. High-dimensional asymptotics of feature learning: how one gradient step improves the representation.  ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity & Quality:** \nThe main text is very compact and not easy to follow. The authors presented many theorems on different regimes and objectives, most of which are not immediately interpretable. Personally I am not too interested in how the roots of the polynomial equations are characterized, so I would suggest the authors to remove some technical contents from the main text and highlight a few important messages. \n\n**Novelty:** The asymptotic formulae for both the test loss and the sensitivity have been computed in prior works ([Mei and Montanari 2019] [D\u2019Amour et al. 2020]). The main technical novelty is to solve for the optimal coefficients in the activation function, which is a rather tedious computation. \n\n**Reproducibility:** N/A. ",
            "summary_of_the_review": "In my opinion this submission requires some major revision to be relevant to the ICLR community. Hence I cannot recommend acceptance. \nI am willing to update my evaluation if the authors can address my concerns and elaborate on the motivation / importance of the studied problem. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2279/Reviewer_1hhM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2279/Reviewer_1hhM"
        ]
    }
]