[
    {
        "id": "Vhf1tOq-Es",
        "original": null,
        "number": 1,
        "cdate": 1666563289285,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666563289285,
        "tmdate": 1666563289285,
        "tddate": null,
        "forum": "hFUlfiyf1oQ",
        "replyto": "hFUlfiyf1oQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4682/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the dimensionality collapse in self-supervised learning (SSL). Though the uniformity of the representation is regarded as an important feature for downstream tasks, the existing loss function in SimCLR is insensitive to dimensional collapse. With a theoretical guarantee, the authors propose a Wasserstein-based uniformity measure to avoid dimensionality collapse. Numerical experiments show that the proposed method efficiently works to extract useful features for downstream tasks. ",
            "strength_and_weaknesses": "Strength\n- The readability of the Introduction and related works is excellent. The idea of the proposed method is easy to follow. \n\nWeaknesses\n- In section 3, some theorems are presented. Since these theorems are elemental results, theorems should be moved to the appendix. Instead, the authors could discuss more important issues, such as the theoretical properties of the proposed Wasserstein-based uniformity measure.\n\n- The Wasserstein distance from the empirical distribution to the Gaussian distribution with mean zero and variance I/m is employed to measure the uniformity on the unit sphere. To derive the uniformity measure, some assumptions and approximations are introduced. Therefore, the theoretical properties of the proposed measure are not very clear. Even when the data is not uniformly distributed, the Wasserstein-based measure, i.e., the negative Wassersteindeitance, can take a large value.\n\n- The authors should concentrate on thorough theoretical analysis to guarantee the effectiveness of the proposed method. \n  - The measure -W_2 takes the first and second-order moment into account. How the higher order moment affects the W_2-based measure (4)? \n  - Is it possible to analyze the generalization error of downstream tasks when the proposed measure is used for feature learning? \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. To avoid dimensionality collapse, the Wasserstein-based measure is used. This is a practical approach. However, The proposed method is based on assumptions and approximations, and the theoretical analysis is insufficient. ",
            "summary_of_the_review": "The proposed method is based on some assumptions and approximations, and the theoretical analysis is insufficient. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4682/Reviewer_ZorS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4682/Reviewer_ZorS"
        ]
    },
    {
        "id": "t1VjdObcCL",
        "original": null,
        "number": 2,
        "cdate": 1666572283503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666572283503,
        "tmdate": 1668776992520,
        "tddate": null,
        "forum": "hFUlfiyf1oQ",
        "replyto": "hFUlfiyf1oQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4682/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the Wasserstein distance between an isotropic zero mean Gaussian distribution and the distribution of the learned representations as a robust uniformity metric for self-supervised learning. Five desirable properties of uniformity are discussed and the proposed metric is compared against another recently introduced uniformity metric (Wang & Isola, 2020) in terms of how well the two metrics satisfy these properties. The proposed metric is also added to the loss function and several recent self-supervised algorithms have been retrained using this modified loss function. Results suggest that adding this new uniformity metric-based loss increases accuracy and improves convergence of the training. ",
            "strength_and_weaknesses": "Strengths:\n\t- The emphasis on dimensional collapse and how it was overlooked by earlier work was well thought\n\t- The metric is very simple to implement because it only involves evaluating the Wasserstein distance of two Gaussian distributions\n\t- Extensive experiments have been performed reimplementing several of the recently introduced techniques with the modified loss function. \n\nWeaknesses:\n\t- The paper assumes that l2-normalized zero-mean isotropic Gaussian distribution follows a Gaussian-like distribution. The illustration in 1D is helpful, but it would have been more interesting to see the deviation trend as the dimensionality increases. \n\t- Five properties were introduced. It was not discussed whether these five properties were sufficient to prove ideal uniformity. \n\n---------------\nAuthors' responses somewhat address my critiques but not at a level to increase my rating. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper was organized and written well with some minor problems. \nSuggested Corrections:\n\nRemove the extra \"on the\"\n\u2026 on the on the surface of the unit \u2026\n\nWe instantiate Equation 11 with the distribution \u2026 There is no equation 11.\n\nThe last inequality in Property 5 is supposed to be an equality\n\nSome of the lemma (Lemma 2) and theorems were textbook level concepts. Proofs may not be needed even in the supplementary. \n\nQuality:\n\nThe quality of the paper was OK. Experimental results are somewhat strong. However, the experimental results do not confirm whether the improvement is due to network learning better representations or due to improved convergence and shift of the learning curve. \n\nOther aspects of uniformity that would not be satisfied by the proposed metric were not discussed. It was not clear whether these five properties were sufficient to confirm uniformity. \n\n\nNovelty: \n\nDINO, one of the key self-supervised learning work of recent years was not discussed in the paper. This could have been categorized under \"Asymmetric Model Architecture\" in related work. \n\nCaron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9650-9660).\n\nReproducibility:\n\nCode is not provided. Given the scope of experiments spanning various techniques it is hard to figure if the results would be reproducible. ",
            "summary_of_the_review": "The proposed uniformity metric can be considered as an important contribution of the work for the self-supervised learning literature. However, the fact that limitations and potential pitfalls were not discussed  raises important questions about whether the reported improvements are indeed due to the proposed metric or some other aspect of learning is being affected by the modified loss functions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4682/Reviewer_iVeQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4682/Reviewer_iVeQ"
        ]
    },
    {
        "id": "7Zv2DE04Qm",
        "original": null,
        "number": 3,
        "cdate": 1666583818508,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583818508,
        "tmdate": 1666583818508,
        "tddate": null,
        "forum": "hFUlfiyf1oQ",
        "replyto": "hFUlfiyf1oQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4682/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work presents a systematical analysis of the collapse degree of representations in self-supervised learning. To achieve a quantifiable analysis of the dimensional collapse, the work proposed to use the Wasserstein distance between the distribution of learned representations and the ideal distribution as the metric of uniformity. Based upon this, it introduces five desirable constraints for ideal uniformity metrics to make theoretical comparisons between the proposed metric and the existing one. Finally, the work shows that by imposing the proposed uniformity metric as an auxiliary loss term for various existing self-supervised methods, it consistently improves the downstream performance.\n\n",
            "strength_and_weaknesses": "# strength: the work studies an interesting problem in self-supervised learning. \nThe work proposed a new metric that better evaluates the quality of representations learned via self-supervised learning.\n\n# weakness: the reviewer finds the overall result a bit incremental, compared to the work Wang & Isola 2020\n1. The theorems in Section 3.1 & Section 3.2 are some known facts, \n2. more validation is needed on the assumption that a uniform distribution over the sphere is approximately the same as Gaussian distribution in high dimension\n3. the improvement of accuracy on downstream tasks in Table 2 is quite marginal.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is well-written and well presented.\nThere are many existing work studying the representation quality of self-supervised learning, the result and the improvement seem to be a bit incremental.\nThe reproducibility can be improved if code can be released.\n\n",
            "summary_of_the_review": "Overall, this is a well-presented paper. However, the reviewer finds the result a bit incremental overall. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4682/Reviewer_UX2Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4682/Reviewer_UX2Z"
        ]
    },
    {
        "id": "o6sgUAvDoJo",
        "original": null,
        "number": 4,
        "cdate": 1666598058214,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598058214,
        "tmdate": 1669239203671,
        "tddate": null,
        "forum": "hFUlfiyf1oQ",
        "replyto": "hFUlfiyf1oQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4682/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors are motivated by a recent observation in self-supervised learning---in order to prevent a model from dimensional collapse and subsequently poor data representation, we should encourage the representation to be uniform; see for example [1]. The authors argue that zero-mean isotropic Gaussian distribution has the ideal uniformity property, and based on this, they propose to use Wasserstein distance as a uniformity metric as a part of the loss function. It is shown that the Wasserstein distance is useful for avoiding dimensional collapse and improving representation.\n\n[1] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, 2020.\n",
            "strength_and_weaknesses": "Here is a summary of strengths and weaknesses.\n\nPros:\n- The perspective of alignment and uniformity is very helpful for understanding data representation in self-supervised learning, and yet this perspective is not fully explored and some issues such that dimensional collapse remains. The authors suggest that one reason may be that common uniformity metrics do not satisfy Cloning Constraint and feature Baby Constraint properties. This argument seems plausible and provides a new way of thinking uniformity.\n\nCons:\n- Section 3 seems to be very obvious to many people. So it is not clear what the theoretical contribution of this paper is.\n- The empirical contribution is also fairly weak: the results in Table 2 are not overwhelmingly convincing, there is no visualization of data representation, there is a lack of in-depth analysis and comparison of different uniformity metrics, etc.\n- Estimation of covariance matrices from data, as shown in Eq. 3, is not well grounded. It is well-known in statistics that estimating covariance matrices in high dimensions is not reliable. Also, the Gaussian assumption may not be valid in the representation space.\n\n**Update:** After the revision, I feel that the empirical part is strengthened, so I slightly increased the score.",
            "clarity,_quality,_novelty_and_reproducibility": "I think that this paper is clearly written. Analyzing uniformity metrics from the listed five measures is nice. However, I doubt there is any genuine theoretical contribution from this paper, despite its lengthy argument. The empirical justification is also quite weak and incomplete in my opinions. ",
            "summary_of_the_review": "The authors study uniformity in self-supervised learning and propose Wasserstein distance as a part of the loss function. I find the overall argument plausible. However, the major weaknesses---namely claiming obvious results as theoretical contributions, and weak empirical justications---severely lower the quality of this paper. Thus, I would not recommend acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4682/Reviewer_9DpF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4682/Reviewer_9DpF"
        ]
    }
]