[
    {
        "id": "YiliOlMvcVX",
        "original": null,
        "number": 1,
        "cdate": 1666292131969,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666292131969,
        "tmdate": 1666292131969,
        "tddate": null,
        "forum": "7lvuPvDNhI4",
        "replyto": "7lvuPvDNhI4",
        "invitation": "ICLR.cc/2023/Conference/Paper1329/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extended DEC from regret minimization to PAC guarantee and reward-free case. Similar algorithms, lower bounds and upper bounds are established. ",
            "strength_and_weaknesses": "I appreciate the authors' great effort to extend DEC to PAC setting and reward-free setting with a lot of tedious calculation. However, I feel this work is a bit incremental. Instead of covering material as much as possible, I suggest the author focuses on one problem and thinks deeper. There are some important problems left for DEC and this paper inherits them to other settings. \n \n1. It's better to explain clearly what do you mean \"sufficient and necessary\"? Clearly the upper bound (Thm 5) and lower bound (Proposition 6) differ by a log(|M|) term.\n \n2. The algorithm is far from computational efficient. There are several minimax operators involved such that there is little useful guidance for practitioners. The algorithm is not implementable and of course no experiments. \n \n3. The guarantee in Example 13 is far from optimal. The dependency for S is very bad. It is unclear this is due to analysis or due to the complexity measure. Again, then why you can claim \"sufficient and necessary\"? \"Unified algorithm\" should not be an excuse. That means this unified algorithm cannot pass the sanity check of tabular MDPs.\n \n4. It's very unusual for ICLR to have a 58 page submission. That means it is impossible for reviewers to check the correctness of the proof. And a lot of proofs are repeated from Foster et al. (2021) (for example, Section B.3). Such a paper is more suitable for journal submission in my mind.   \n",
            "clarity,_quality,_novelty_and_reproducibility": "Proof is nice and structured. Novelty is a bit limited. ",
            "summary_of_the_review": "A solid theoretical paper but a bit incremental. Computational efficiency is a big limitation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1329/Reviewer_tzo7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1329/Reviewer_tzo7"
        ]
    },
    {
        "id": "lU9gWrqdVu",
        "original": null,
        "number": 2,
        "cdate": 1666796883866,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666796883866,
        "tmdate": 1666796883866,
        "tddate": null,
        "forum": "7lvuPvDNhI4",
        "replyto": "7lvuPvDNhI4",
        "invitation": "ICLR.cc/2023/Conference/Paper1329/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper follows the framework of decision-estimation coefficient (DEC) proposed by (Foster et al. 2021). The major contribution of this work is to combine DEC with tempered aggregation to design algorithms for several problems (e.g., regret minimization, PAC-learning and reward-free learning) in DMSO (decision making under structured observation).",
            "strength_and_weaknesses": "Strength: The authors proposed E2D-TA by combining E2D with tempered aggregation to help avoid estimating the transition model. \nIn technique, the authors propose a new definition for $dec$ and manage to derive a stronger bound for error term $Est_{RL}$. \n\n\nWeaknesses: The motivation is not strong enough. The authors compare this work with Foster et al., 2021 in the third paragraph in Section 1.  As stated above, the main merit of the proposed algorithm is to avoid estimating the transition model. However, the proposed algorithm is still computational intractable given this improvement.  Even assuming the tabular state-action space (where we can estimate the model efficiently), E2D-TA seems still computational inefficient. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation should be improved. In particular, the comprison with Foster et al. 2021 is not sufficient. Since E2D-TA is very similar to E2D, the authors may also present E2D to show the difference. There is no numerical experiments.",
            "summary_of_the_review": "Given the consideration above, I think the motivation is still not clear and tend to reject this paper. It would be helpful if the authors could give some meaningful examples where E2D-TA is computational efficient while E2D is not. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1329/Reviewer_Tm8H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1329/Reviewer_Tm8H"
        ]
    },
    {
        "id": "jcntbKgTm9T",
        "original": null,
        "number": 3,
        "cdate": 1667441295056,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667441295056,
        "tmdate": 1667441295056,
        "tddate": null,
        "forum": "7lvuPvDNhI4",
        "replyto": "7lvuPvDNhI4",
        "invitation": "ICLR.cc/2023/Conference/Paper1329/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce a new model-based meta-algorithm called E2D-TA under the framework of E2D(Forster et al.,2021), which updates the model estimator by Tempered Aggregation. Under this main algorithm, the authors further introduced two complexity parameters, EDEC and RFDEC, and correspondingly designed two algorithms for PAC and reward-free learning tasks. The authors also provide the rate of growth for E2D, EDEC and RFDEC, and prove that with delicate choice of the parameters, these algorithms enjoy a sublinear regret bound, which also meets the lower bound. Moreover, the authors provide sublinear regret bounds for RL with Bellman representability under E2DTA framework.",
            "strength_and_weaknesses": "The strengths of the paper according to me are the following:\n1. The authors prove the upper bound of DEC, EDEC and RFDEC under a uniform framework, which is not covered by previous works.\n2. The authors design an algorithm for reward-free and PAC learning case, with finite class and low-bellman dimension models as examples. This is not covered in previous works under E2D framework.\n\nThe weaknesses of the paper (according to me) are the following:\n1. In my humble optinion, it is hard to justify the paper's novelty. For me, the main algorithm E2D-TA seems to be a modification of Algorithm 1 in (Foster et al.,2021) only by changing the method of learning the underlying model.  Foster et al. first estimate the underlying model using methods such as Vovk aggregation which only incurs an extra estimation error of \\gamma\\log(|M|/\\delta). This paper estimates the underlying model by computing posterior distribution on the model set and claim to be a tighter bound justified by Jensen's inequality, but its contribution to the overall regret is still of scale \\gamma\\log(|M|/\\delta) and therefore this technique, as the main novelty, doesn't seem to optimize the regret bound. Meanwhile, this paper adopts different definition of estimation error i.e. EST. Therefore the authors need to further justify the importance of using this method.\n\n2. Forster et al. provide an analysis for infinite model class case in Section 3.2.3 with a proof base on covering number and further show its application in bilinear RL settings and covers low BE dimension results in Appendix F.   It is hard to see the main difference between the analyses in those two paper. The results in reward-free cases and PAC learning under those settings also seem to be a straightforward generalization. \n\n3. The necessity of using TA as the model estimator is not obvious. In my humble opinion, it would be better if the authors can justify why simply using other estimators (e.g. Vovk's aggregation) can not be generalized to reward-free/PAC cases.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in good language and the notations are easy to follow. Since this is a theoretical machine learning paper there is no experiment and reproducibility is not applicable. ",
            "summary_of_the_review": "This paper generalizes the E2D framework in Foster et al. to PAC learning and reward-free learning by substituting the model estimation method with Tempered Aggregation and new complexity metrics. It further provides minimax optimal regret bound, which is also proved to be sublinear under many cases, including finite model class and low BE dimension cases. However, there are still difficulties to grasp its true novelty. In my opinion, it would be better if the authors can further highlight the contribution of their techniques in more concrete ways, especially:\n1. The generalization from simple E2D to their reward-free and PAC learning edition, and\n2. The connection between this generalization and using TA as the model estimator.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1329/Reviewer_oWcL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1329/Reviewer_oWcL"
        ]
    }
]