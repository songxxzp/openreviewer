[
    {
        "id": "G3K2GwFtnSd",
        "original": null,
        "number": 1,
        "cdate": 1666406914528,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666406914528,
        "tmdate": 1666406914528,
        "tddate": null,
        "forum": "pgU3k7QXuz0",
        "replyto": "pgU3k7QXuz0",
        "invitation": "ICLR.cc/2023/Conference/Paper2167/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work presents a \u201cconversion +fine-tuning\u201d two-step method for training SNNs for text classification. Meanwhile, a new encoding method for converting pre-trained word embeddings to the spiking version is proposed. As results, the converted SNNs achieved comparable results compared with DNNs on text classification benchmarks. Also, the proposed model shows better robustness against adversarial attack than DNNs.",
            "strength_and_weaknesses": "Strength:\n1. There are few works that demonstrate the efficacy of SNNs in language tasks. This paper makes a breakthrough.\n2. The results are excellent. The proposed models achieve comparable results to the original TextCNNs.\n3. The idea is clear and the paper is well organized.\n\nWeakness:\n1. The idea is not very novel. The whole pipeline is quite similar to the converting process of original SCNNs in the image recognition domain.\n2. The spiking encoding may lead to the increasing of time steps during inference and training for each word embeddings that are converted to spike trains periodically. The authors did not mention that.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is clear and the paper is well written and organized. The idea is not very novel but the authors applied it in a new application. ",
            "summary_of_the_review": "This work fills the gap in the field of SNNs for text classification tasks. The experimental results are excellent and the conclusions are well supported. But the authors need to justify the impact of time steps during inference and training of the converted SNNs. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2167/Reviewer_xeVA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2167/Reviewer_xeVA"
        ]
    },
    {
        "id": "2QnlOut9qvw",
        "original": null,
        "number": 2,
        "cdate": 1666723290260,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723290260,
        "tmdate": 1666723290260,
        "tddate": null,
        "forum": "pgU3k7QXuz0",
        "replyto": "pgU3k7QXuz0",
        "invitation": "ICLR.cc/2023/Conference/Paper2167/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper applies SNNs to text dataset tasks. It proposes a method of converting a pre-trained standard ANN into an SNN. This improves on current methods by allowing the model to handle large vocabularies.",
            "strength_and_weaknesses": "\n(strengths)\n\nWell-written and organized.\n\nExtends viability of SNNs to large vocabularies.\n\nA wide array of example tasks, in 2 languages.\n\n(weaknesses)\n\nThe opening motivation, reducing energy consumption, is less impactful than it appears because the proposed method requires pre-training and fine-tuning a standard ANN before conversion to an SNN. So the only savings are over inference time. For a very few successful and widely deployed algorithms this will be significant, but for most algorithms during research, there is little difference in energy cost. This distinction should perhaps be clarified.\n\n(Miscellaneous)\n\nTable 1: is it possible to report mean +/- std dev over k-fold splits? This gives a much more powerful and informative report.\n\nTable 2: Since the SNNs are still clearly harmed by the adversarial attacks, perhaps you could discuss whether there are ways to improve on this (or conversely comment that robustness, while much better than CNNs, is unlikely to improve).\n\nPage 1: is the \"question classification task\" one of the 17 toy tasks, or is it substantially harder?\n\nPage 1: \"which is less biologically plausible\": Does this matter at all? perhaps this clause could be removed.\n\nBottom page 2: Maybe replace \"former\" and \"latter\" with \"conversion\" and \"spike-based\" for clarity.\n\nSection 3.1: \"is trained by the same way as one would with \" -> \"is trained in the same way as\"\n\nSection 3.1: \" some operations that cannot be realized by \": this raises an instant question in the reader \"what operations?\" which is not answered until 3.1.1. Perhaps add a pointer to that future list, eg \"(listed in 3.1.1)\"\n\n3.1.1: \"method is applied\" -> method can be applied\n\nTop of page 4: \"embeddings learn\" -> \"embeddings to learn\"\n\n3.1.1 third bullet: It seems that removing biases would affect whether units activated or not. Why does removing biases not matter?\n\n\"clamp\": maybe \"clip\" is more standard?\n\n\"dividing by sigma\": divide by 6*sigma?\n\nFigure 2: This should be moved to after eqn 2, so that the reader knows what the variables mean. In its current location, the figure is not comprehensible when encountered.\n\n\"illustrated in Figure 2\" -> \"2a\"\n\n3.2.3: It looks like the target vector increases as the vocabulary size. How big is it, and are there limits to vocabulary size due to this part of the architecture?\n\n\"3.2.3: \"we list the efficient\" -> \"an\"\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well-written and organized, and is a compelling read.\n\nThe research program and experiments are well-scoped and well-thought out.\n\nThe findings extend the current art meaningfully.\n\nThe Introduction lays out relevant prior work. I do not have sufficient background in the literature to confirm completeness of citations or novelty (so the scores I give for Novelty might be wrong).",
            "summary_of_the_review": "The paper documents a well-designed research program and experiments that extend current art in the field of SNNs.\n\nThe paper itself is well-written and a pleasure to read.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2167/Reviewer_SuuR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2167/Reviewer_SuuR"
        ]
    },
    {
        "id": "f3VGDXFGRN",
        "original": null,
        "number": 3,
        "cdate": 1666843677278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666843677278,
        "tmdate": 1666843677278,
        "tddate": null,
        "forum": "pgU3k7QXuz0",
        "replyto": "pgU3k7QXuz0",
        "invitation": "ICLR.cc/2023/Conference/Paper2167/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on applying spiking neural networks (SNNs) to text classification tasks. \n\nSpiking neural networks more closely mimic biological neural networks, with the main difference from typical artificial neural network being a temporal quality: each neuron accumulates a membrane charge over time (across inferences) and only transmits information when its threshold is hit. When this event happens, the neuron is said to fire (or spike), it resets itself, and its signal travels to the connected neurons which in response will increase or decrease their charge.\n\nThe main contribution is to convert and fine-tune a regular TextCNN architecture into its SNN counterpart. In particular,\n(i) the given TextCNN is tailored to replace or remove unsupported operations such as word embeddings, negative values, biases, max-pooling.\n(ii) This tailored NN is then trained with Gradient Descent\n(iii) The corresponding SNN is initialized with the weights learned in the previous step, and then is fine-tuned using surrogate gradients.\n\nExperiments are conducted on sentiment classification tasks.\n",
            "strength_and_weaknesses": "Strengths:\n- Simple approach, easy to follow.\n\nWeaknesses:\n- Contributions are incremental and only marginally novel.\n- This approach still requires training a regular TextCNN, partially defeating the motivation behind SNNs.\n- Experimental section is quite limited and inconclusive. In particular, the lift from using fine-tuning vs simply using a converted SNN is rather marginal.\n- Experimental robustness section: why are the \"Clean\" metrics in Table 2 different from the ones in Table 1. If that's because of the variance of results over different run, then the observed variance would invalidate the results and ranking presented in Table 1.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite easy to follow. However, being spiking neural networks quite a niche field, I would have hoped the authors had introduced them in more details to help the reader understand the SNN setup and differences from regular neural networks (if not in the main body, at least in the appendix).",
            "summary_of_the_review": "Given the limited novelty and improvements, my recommendation is to reject this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2167/Reviewer_PFzn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2167/Reviewer_PFzn"
        ]
    },
    {
        "id": "80zIaQmMWH",
        "original": null,
        "number": 4,
        "cdate": 1666958137713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666958137713,
        "tmdate": 1666958137713,
        "tddate": null,
        "forum": "pgU3k7QXuz0",
        "replyto": "pgU3k7QXuz0",
        "invitation": "ICLR.cc/2023/Conference/Paper2167/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new scheme for training the SNN model to achieve text classification in the NLP community. Due to the fact that it is hard to directly train SNN for language tasks using the error backpropagation through time. The authors start with an SNN converted from a normally-trained tailored network and perform backpropagation on the converted SNN. The pre-trained word embeddings are also transformed for SNN based text classification. Pre-trained word embeddings are projected into vectors with positive values after proper normalization and shifting, which can be used to initialize tailored networks and converted to spike trains as input of SNNs. \n\nIf not considering the contributions of SNN for NLP tasks, the transformation from ANN to SNN is not a new idea actually. This paper is well-written, and the structure is clear for readers to follow.",
            "strength_and_weaknesses": "Strength \n1. design SNN for text classification further extends the applications of SNN model; \n\n\nWeaknesses\n1. after a series of operations, the ANN is transformed into SNN version, however, the accuracy and running efficiency are all dropped? A natural question is do we have to conduct such a transformation? \n2. the Transformer networks achieve remarkable performance in the NLP tasks, would people select the SNN for their application? I doubt this selection. \n3. The experimental results are not impressive. ",
            "clarity,_quality,_novelty_and_reproducibility": "1. the writing of this paper is good. It is clear for readers to follow and read. \n2. the novelty is OK, but I doubt the significance of the transformation from ANN to SNN model. \n3. Not sure about the reproducibility of this work. ",
            "summary_of_the_review": "This paper proposes a new scheme for training the SNN model to achieve text classification in the NLP community. Due to the fact that it is hard to directly train SNN for language tasks using the error backpropagation through time. The authors start with an SNN converted from a normally-trained tailored network and perform backpropagation on the converted SNN. The pre-trained word embeddings are also transformed for SNN based text classification. Pre-trained word embeddings are projected into vectors with positive values after proper normalization and shifting, which can be used to initialize tailored networks and converted to spike trains as input of SNNs. The writing of this paper is good. It is clear for readers to follow and read. but I doubt the significance of the transformation from ANN to SNN model.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2167/Reviewer_fFGu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2167/Reviewer_fFGu"
        ]
    }
]