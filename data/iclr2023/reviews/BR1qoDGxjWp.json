[
    {
        "id": "EGNedLL9lrG",
        "original": null,
        "number": 1,
        "cdate": 1666215387674,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666215387674,
        "tmdate": 1670796928453,
        "tddate": null,
        "forum": "BR1qoDGxjWp",
        "replyto": "BR1qoDGxjWp",
        "invitation": "ICLR.cc/2023/Conference/Paper4182/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new problem setting: They assume a model is pretrained on source data, then the model is used to predict data in a target domain. They also assume that the resources are low and the method cannot update the parameters\u2014i.e., cannot use backprop.\n\nThe proposed model is based on meta-learning. It is pretrained by taking batches as tasks and predicting target labels. The core computation part is to find the target image, in the given target set, that is most similar to the test image using a mechanism similar to self-attention in transformers. Then summing up the embedded vector of this image by the embedded vector of the test image to be used for prediction.\n\nThe model is evaluated in four datasets and shows some improvements.",
            "strength_and_weaknesses": "Reasons to accept:\n\nThe paper proposes a new research subject, and the results are good.\n\nReasons to reject:\n\nThe paper is extremely verbose, it is full of unnecessary information, it has some missing would-be useful information, and it lacks some visualizations to facilitate the convey of information. All together these have made the article extremely boring to read. As a person who has read many domain adaptation papers, in my opinion this paper is not worth reading, given the amount of information that it offers. Just a few examples (not a full list and not ordered): \n- You use too many mini-passages separated by a bold-faced phrase. These confuse the reader, particularly when each one focuses on a specific subject, and occasionally drift from the subject of the section.\n- You used these mini-passages in the incorrect sections. For example I don\u2019t expect to see mini-passages called \u201cLatent domain adaptation\u201d or \u201cFeed-forward domain adaptation\u201d in the introduction section, when you already have a full section called \u201cBACKGROUND AND RELATED WORK\u201d. This continues all over your paper. For example, you have a mini-passage called \u201cPreliminaries\u201d in the section called methods, but weren\u2019t you supposed to cover these in the section called background? Even worse, when you read this mini-passage, you would find that it is not actually preliminaries, it is a mix of background and notations.\n- You have a mini-passage called \u201cDeployment phase\u201d, aren\u2019t you supposed to cover the model itself, then explain its deployment?\n- You have a section called \u201cOBJECTIVE\u201d, weren\u2019t you supposed to explain the model first, and then explain its objective?\n\n   You see, this list goes on and on.\n\nI am not sure about the applicability of the proposed problem setting. This is important because failing to fully justify and evaluate this part, makes me think that the authors have just tried to come up with an unrealistic problem setting to publish a paper for. The authors assume the model cannot use backprop, but cannot we replace the encoder with a smaller one to use a smaller compute? Also based on the examples that you are using in you paper, I am not convinced that why we cannot use source data. You are already using target data, so memory usage shouldn\u2019t be an issue.\n\nIn the mini-passage called \u201cTraining phase\u201d, you have assumed that the domain of the query image appears in the target set. Why?",
            "clarity,_quality,_novelty_and_reproducibility": "See the above section",
            "summary_of_the_review": "The paper proposes a new problem setting. But the presentation is horrible, and the problem setting needs further experiments to be justified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_J7SR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_J7SR"
        ]
    },
    {
        "id": "FDkpJxgpHT",
        "original": null,
        "number": 2,
        "cdate": 1666508912889,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666508912889,
        "tmdate": 1666508912889,
        "tddate": null,
        "forum": "BR1qoDGxjWp",
        "replyto": "BR1qoDGxjWp",
        "invitation": "ICLR.cc/2023/Conference/Paper4182/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed the Latent Domain Adaptation setting where the a source model should be adapted to a target dataset that contains a mixture of unlabelled domain-relevant and domain-irrelevant examples. The \u201clatent'' means that we are not given the domain label in the training phase. To handle this problem, the authors proposed a feed-forward method which incorporates the cross-attention module with an episodic meta-learning framework. The authors have carried out extensive experiments to verify the performance of the proposed method. Furthermore, the authors analysed the training and inference time to show the strength of the proposed feed-forward method.",
            "strength_and_weaknesses": "Strength: \n1. The authors proposed a new setting for source-free domain adaptation. This setting seems more practical in real-world scenarios compared with the conventional SFDA.\n\n2. The writing of this manuscript is clear and easy to follow. The overall presentation is nice, including smooth sentences, nice figures, and organised paragraphs.\n\n3. The pipeline of the method is technically sound and provides detailed implementation of the proposed method.\n\nWeakness:\n\n1. In Section 3.1, the authors mentioned that \u201cWhile the conventional domain adaptation setting assumes that xq and xs are all drawn from a common distribution\u2026\u201d. In the conventional domain adaptation, however, they usually belong to different domains.\n\n2. Compared with ARM, the proposed method seems a bit incremental, it is just the simple combination of ARM and cross-attention module. The authors didn\u2019t provide strong motivation for combining these techniques. In addition, neither ARM nor cross-attention module was proposed by the authors. \n\n3. Typo in Section 3.1: \u201cTo train a model than can be used as described above\u201d. \u201cthan\u201d should be \u201cthat\u201d.\n\n4. In Sect. 3.3, the author mentioned that \u201cimage-to-image attention is more suitable for domain adaptation than patch-based option because the overall representation should better capture the nature of the domain rather than a patch.\u201d Is there any theory that supports this conclusion?\n\n5. In my opinion, it is necessary to compare the proposed method with TENT and SHOT even though they are based on back-propagation. Although the proposed method does not require back-prop, this is a detail of the specific implementation and is not relevant to the task.\n\n6. Even with the feed-forward, the proposed method still requires several iterations for training. So, what are the advantages of using feed-forward?\n\n7. As the training time is heavily affected by the number of iterations of the method, the numbers in Table 3 are not convincing enough. Which method performs best with the same training time? Which method requires less training time to achieve the same performance? Furthermore, comparisons with TENT and SHOT are still missing from Table 3.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "There are some typos in the manuscript. The authors didn\u2019t clearly demonstrate the strength of the feed-forward training. The technical novelty is very limited.",
            "summary_of_the_review": "Considering the limited technical novelty and insufficient comparison, e.g., with TENT and SHOT, the reviewer would not be able to recommend an acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_zikv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_zikv"
        ]
    },
    {
        "id": "xS53pRqTvZ",
        "original": null,
        "number": 3,
        "cdate": 1666578116266,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578116266,
        "tmdate": 1670371488522,
        "tddate": null,
        "forum": "BR1qoDGxjWp",
        "replyto": "BR1qoDGxjWp",
        "invitation": "ICLR.cc/2023/Conference/Paper4182/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to meta-learn a network capable of embedding the mixed-relevance target dataset and dynamically adapting inference for target examples using cross-attention. It further analysis the method performance in the domain-supervised adaption. ",
            "strength_and_weaknesses": "Strength: \nthe writing of this papers seems easy to follow and it demonstrates its method from several perspectives. \n\nWeakness:\n1.\tThere are several implementation details associated with the proposed method such as the augmentation, cross attention, meta learning. It would be better to have  the ablation studies on decomposing the each components. \n2.\tI am concerned the performance boost with the relatively large computational time. Although in table 1, the authors shows the improvements but coming with larger computations. \n3.\tThe author claims privacy in the paper. However, there are not many privacy baselines are used as the baselines. I would recommend the authors to compare with some privacy related domain adaption such as black box related domain adaption. \n4.\tI also have the questions regarding the scalability of the proposed method. With the meta-training, how does the proposed method formulate on the larger dataset or settings? And what are there performance? It would be better to extend the proposed method on the larger dataset and with the detailed running time. I suspect that the running time will be much larger compared to the baselines. \n",
            "clarity,_quality,_novelty_and_reproducibility": "See the weakness. ",
            "summary_of_the_review": "----------------------------Updates---------------------------------\nAfter reading the response and other reviewers' comments, I would like to keep my score.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_tuYo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_tuYo"
        ]
    },
    {
        "id": "QoCDGFZjlYu",
        "original": null,
        "number": 4,
        "cdate": 1666597710938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597710938,
        "tmdate": 1666597710938,
        "tddate": null,
        "forum": "BR1qoDGxjWp",
        "replyto": "BR1qoDGxjWp",
        "invitation": "ICLR.cc/2023/Conference/Paper4182/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, authors focus on source free latent domain adaptation. Here the content or label of  target data overlaps with the one of the source data. And authors doest not get access to the source data when using target data. Furthermore, authors also strict setting where the network is not updated which is backward-free. To achieve this goal, authors explore the probability of using meta-learning, and propose to use cross-attention to select relevant examples for adaptation. The experiment results demonstrate the effectiveness of the proposed method.   \n\n\n\n",
            "strength_and_weaknesses": "1) This paper explore new setting about latent domain adaptation: source-free and forward-pass, which is a new highly practical setting.\n\n2) Authors try to give clear explanation about this setting, and show why it is interesting in practical application. \n\n3) The paper is well-written and easy to follow.\n\n4) The experiment is sufficient to support the proposed method. \n",
            "clarity,_quality,_novelty_and_reproducibility": "1) The paper explores a new setting which is interesting.  The proposed method is based on meta-learn mechanism, which seems to be less interesting. \n\n2) Authors present the difference about the latent domain adaptation and standard domain adaptation, which is clear. However, the used dataset is suitable for this setting, since it is standard dataset. \n\n3) The result (e.g., table 1) shows the proposed method has less advantage, which seems to be hard to support the effectiveness of the proposed method. \n\n",
            "summary_of_the_review": "This paper use the meta-learning method to explore source-free latent domain adaptation, which sounds interesting. However, for forward-pass, it is less convincing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_wyNm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_wyNm"
        ]
    },
    {
        "id": "K8o81v_tZS",
        "original": null,
        "number": 5,
        "cdate": 1666598640765,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598640765,
        "tmdate": 1666598640765,
        "tddate": null,
        "forum": "BR1qoDGxjWp",
        "replyto": "BR1qoDGxjWp",
        "invitation": "ICLR.cc/2023/Conference/Paper4182/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new source-free domain adaptation method that can work without back-propagation and is capable of handling a target dataset containing both domain-relevant and domain-irrelevant samples (called LDA setting). The proposed method firstly embeds the target dataset into features and conduct adaptive inference for target samples using cross-attention mechanism with the embedded features. The modules for embedding, cross-attention, and classification are jointly trained via a scheme of meta-learning. Experimental results with several popular datasets show that the proposed method works well especially in the LDA setting.",
            "strength_and_weaknesses": "<Strength>\n\n- Since the proposed method does not require any training process with back-propagation operations, it should be easy to use and can be applied to a wide range of applications. \n- The proposed method is also useful in terms of robustness against domain-irrelevant samples in the target dataset.\n- The proposed method consistently achieves good performance in several benchmark datasets.\n- This paper is well-written and easy to follow. I enjoyed reading it.\n\n<Weakness>\n\n- I do not have any major concern.\n- Shown below are minor things (almost just out of curiosity).\n\t- Do we need to fix the size of the support set? As a mechanism, the cross-attention module can process a variable size of the support set. Allowing this might be useful, because it makes it possible to handle a various size of a target dataset as is.\n\t- Can we use the proposed method in an online manner like test-time adaptation? Since LN(f(x_s)) can be stored for the subsequent inference process once we compute it, we can conduct online adaptation by using the proposed method, if we have a strategy to properly update the stored feature with LN(f(x_q)). It would be great if we can use the proposed method as an unified method to solve both source-free domain adaptation and test-time adaptation. (I understand that the above makes the proposed method much similar to ARM, though)\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "<Clarity>\n\nThis paper is well-written and easy to follow. I enjoyed reading it. \nMinor concerns are as follows:\n- it might be better to change the notation of the classifier in Eq. (1) from f to any other letter. \n- z represents a domain in Eq. (1) but extracted features at the end of Section 3.3. It is also encouraged to change either notation.\n\n<Quality>\n\nAs far as I understand, this submission is technically sound.\n\n<Novelty>\n\nAlthough the proposed method follows a similar flow to ARM, there are two critical differences as steted in Section 3.1. Especially, the second one, which is robustness against domain-irrelevant samples in the target dataset, is practically important and is effectively tackled by utilizing a cross-attention mechanism, which provides sufficient originality to this study.\n\n<Reproducibility>\n\nThe paper provides sufficient implementation details in Section 4.3 and the appendix.\n",
            "summary_of_the_review": "The proposed source-free domain adaptation is sufficiently novel and empirically works well. The manuscript is well-written. I vote for \"accept.\"\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_tHMU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4182/Reviewer_tHMU"
        ]
    }
]