[
    {
        "id": "nMANhYPovPh",
        "original": null,
        "number": 1,
        "cdate": 1666139955351,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666139955351,
        "tmdate": 1668128093036,
        "tddate": null,
        "forum": "7t3ggLCjl7G",
        "replyto": "7t3ggLCjl7G",
        "invitation": "ICLR.cc/2023/Conference/Paper1481/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a compatibility condition between the data distribution and the algorithm used for regression. This corresponds to a weaker requirement than the benign overfitting studies that simply study the generalization property *at convergence*. ",
            "strength_and_weaknesses": "### **Strength** \n\nThe authors introduce a framework that can be rich theoretically. Instead of studying the *implicit bias* of an algorithm at convergence, it studies whether the path of training dynamics has reached a good generalization point in a certain time zone. They also prove a separation with benign overfitting under certain condition on the spectrum of the covariance matrix.\n\n### **Weaknesses** \n\nDespite the fact that the set-up is clear, the (only) reel results presented are the one of gradient descent for a linear model. The link between early stopping and good generalization properties has already be well studied and I am surprised the authors cite Yao et al, Ali et al as they are doing exactly the same thing! As far as I understood, the only difference is the way to present the results: here, as now commonly stated in the benign overfitting, the generalization bounds are presented under the form of certain cut-off eigenvalue summations. Whereas in Yao et al, eigenvalue decay as polynomials are stated to make the bounds more readable. Morevover, on the contrary to what the authors say, the Assumption 1 *is not mild* (mainly the independence of the entries of $\\tilde{x}$). As the rates of Yao et al are known to be minimax in the setup of the paper, I do not think that the present analysis could be tighter than theirs.\n\n\n**Minor issues**\n\n- Prefer *at convergence* than *the last iterate*\n- Need a deeper comparison with early stopping.\n- Under this form, Assumption 1 is hard to understand: the notation $O, o, w$ are not defined: are they with respect to $n$? If this is so, how to understand $o(n/\\lambda)$, does $\\lambda$ depends on $n$, if yes, $\\lambda = O(1/Tr(\\Sigma))$ is misleading ?",
            "clarity,_quality,_novelty_and_reproducibility": "My overall comments are done above.",
            "summary_of_the_review": "Without a proper comparison with the papers from early stopping, I cannot consider the restatement of it under *compatibility condition* as a proper contribution. I am happy to change my evaluation if I am wrong.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1481/Reviewer_rKAH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1481/Reviewer_rKAH"
        ]
    },
    {
        "id": "OxfTYsqeJAH",
        "original": null,
        "number": 2,
        "cdate": 1666215216619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666215216619,
        "tmdate": 1666215216619,
        "tddate": null,
        "forum": "7t3ggLCjl7G",
        "replyto": "7t3ggLCjl7G",
        "invitation": "ICLR.cc/2023/Conference/Paper1481/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the generalization performance of a simple model \u2014 linear regression. Given that a prior work (Bartlett et. al. 2019) has shown a benign over-fitting of the final iterate of SGD for over-parameterized linear models, this work with milder conditions theoretically shows that good generalization can also be achieved during the training, not necessarily by the last iterate. ",
            "strength_and_weaknesses": "Strength:\n\nCompared to the benign over-fitting study (Bartlett et. al. 2019), this work requires weaker conditions on the covariance matrix. Particularly, it allows the scenarios where the eigenvalues decay polynomially, $\\lambda_k \\sim 1/k^\\alpha$. Instead,  (Bartlett et. al. 2019) requires a heavy tail of the spectrum of the covariance. \n\nFor the scenarios that are included by the conditions of this paper but are excluded by (Bartlett et. al. 2019), this paper obtains a theoretical guarantee of the existence of good generalization solutions during the GD training. However, the tradeoff for these weaker conditions is that the good generalization is not necessarily at the final iteration. \n\nWeaknesses:\n\nThe analysis is still restricted to the simple linear regression model, while most of the interests of the machine learning community are on the non-linear models, especially neural networks. It is widely believed that many properties of, or finding/observations on, linear models do not extend to non-linear models. \n\n\nFigure 1 is not consistent with the theoretical results and could be misleading. \n\n>First, Figure 1b is trained using SGD, while the theory (Theorem 4.1) is on full-batch GD. These two algorithms do not have the same trajectory, hence, the U-shape curve for SGD in Figure 1b does not necessarily tell the same story as Theorem 4.1. A curve with full-batch GD training is needed in Figure 1. \n\n>Second, Figure 1a seems still at the early stage of training, as the training loss is still large. It is hard to see whether the test loss will have a second descent. For overparameterized models, the training loss can touch zero, and we should train the model after the zero training loss is obtained. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "see comments in the \"strength and weaknesses\" section",
            "summary_of_the_review": "The analysis in the paper requires weaker conditions and includes more scenairos, e.g., polynomically decaying eigenvalues of covariance, than prior works. For these scenarios, it proves existence of good generalization solutions during full-batch GD training.\n\nHowever, the analysis is only performed on the simple linear regression model, and it is not clear whether it applies to non-linear models. \n\nSome of the numerical verifications are not consistent with the theory.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1481/Reviewer_kDAx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1481/Reviewer_kDAx"
        ]
    },
    {
        "id": "uf6A-cHy5S",
        "original": null,
        "number": 3,
        "cdate": 1666720809158,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666720809158,
        "tmdate": 1670617022128,
        "tddate": null,
        "forum": "7t3ggLCjl7G",
        "replyto": "7t3ggLCjl7G",
        "invitation": "ICLR.cc/2023/Conference/Paper1481/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "- This paper provides a follow-up of the \"benign overfitting\" result from  Bartlett et al., 2019. \n- While the benign overfitting setting studies the min-norm interpolator of the data (which corresponds to the _final_ solution learned by linear regression), this paper considers excess risk guarantees for intermediate early-stopped solutions of linear regression. In this sense, the paper analyzes the trajectory of GD solutions.\n- This allows them to prove convergence of excess risk for a much more general class of data distributions than was covered in Bartlett et al., 2019.\n",
            "strength_and_weaknesses": "Strengths\n===\n1. The paper makes a valuable extension over the existing studies of overparameterized linear regression. While earlier studies look at the min-norm interpolator, this paper goes a step further to analyze intermediate solutions, which is highly non-trivial and interesting. \n2. Early-stopping is also a practically relevant concept, thus justifying the significance of the setting considered in the paper.\n3. It is also interesting that they are able to formally identify scenarios where the final iterate has overfitting that is _not_ benign, while the early-stopped solution does have excess risk converging to zero (which they term as \"compatibility\"). The formal notion of compatibility itself is novel.\n4. The proof seems to be a non-trivial extension of Bartlett et al., 2019.\n4.  The paper is well-written. The proof and its implications were clearly outlined in the main paper. There are also many example distributions which are helpful in drawing comparisons with benign overfitting.\n\nWeaknesses\n===\n1. My main complaint is with the way the result is framed. I am afraid I do not agree with the motivation/narrative that this paper analyzes the interplay between data and algorithm while prior work. While classical bounds (such as VC dimensions), do ignore the algorithm altogether, more recent bounds (such as those based on norms, flatness etc.,), _do_ care about how the algorithm induces various biases in the model. Even in the context of studies in high-dimensional linear regression, the very fact that they study the min-norm interpolator says that they are looking at an algorithm-dependent solution and not any ERM solution. What might be correct to say here is that while previous studies of overparameterized linear regression perform a trajectory-independent analysis, this paper takes a trajectory-dependent analysis. \n\nOverall, I would frame this paper as \"a statistical study of early-stopping\"  (which _is_ valuable in itself) rather than the current claim of  a \"capturing data-algorithm interplay which has never been done before\" which seems misguided and disproportionate.  \n\n2. I'd like to map the result intuitively to the motivation behind early-stopping, such as the one outlined in the introduction. Early-stopping helps the classifier ignore noise in the data, which in this case should correspond to the variance term. Is this essentially the reason why the early-stopping iterate's bound is better than the final iterate? e.g., in Thm 4.1, does $V(\\theta) \\neq o(1)$ when  $\\theta$ is the final iterate?\nAdding a discussion of how the result maps to the standard intuition of early-stopping would be helpful.\n\n### Minor points:\n3. Since the order of quantifiers is very important (the $\\forall t$ must follow _after_ the \"w.h.p\"), I'd be explicit about this order in all the lemmas and intermediate theorems.\n\n4. Sec B.4 argues that uniform convergence upper bounds do not enjoy as strong a guarantee. I wonder if it's possible to show a lower bound on uniform convergence such as in the sense of Negrea et al., 2020 i.e., consider the set of early-stopped iterates for various draws of the data, and show that the uniform convergence bound considered on that set has poor sample-complexity dependence.\n\n5. I'm not sure I followed the first step in Eq 7. Minor: might be worth labeling every equation. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is novel. The results are stated clearly. \nI've not gone through the proofs in detail; I did go through the key lemma statements in the appendix and how they connect to each other.",
            "summary_of_the_review": "The paper presents an interesting, significant and non-trivial extension of the statistical analsyses in overparameterized linear regression to early-stopped settings.  The proof and results are explained well. There are issues with how the motivation is framed, which I hope can be addressed during the rebuttal.  \n\n### Update after response\n\nThanks to the authors for their response. I appreciate the new added comparisons to related work. \n\nI had concerns that the claim suggesting that this is the first technique that is data- and algorithm- dependent is too strong. \nThe authors admit that there are indeed existing techniques that depend on data and algorithm, but that this dependence is too superficial.\nWhile I agree, I still believe that centering the paper around the notion of \"compatibility\" as a way of encoding algorithm and data-dependence is not as valuable and significant in itself as the paper claims. The paper does make a solid contribution in terms of statistically analyzing early-stopping for the benign overfitting type of setting; however, it also claims that there's novel insight and value in the definition of compatibility, which I do not see. Therefore, I wish to keep my score as it is. I strongly encourage the authors to rethink the story of the paper and focus on highlighting the key technical contributions rather than the abstraction of compatibility --- unless, there are multiple applications of compatibility beyond linear regression that can somehow drive home the point that this abstraction is insightful.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1481/Reviewer_9FRm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1481/Reviewer_9FRm"
        ]
    },
    {
        "id": "ZCB5b5GQ5VH",
        "original": null,
        "number": 4,
        "cdate": 1666917831774,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666917831774,
        "tmdate": 1666917831774,
        "tddate": null,
        "forum": "7t3ggLCjl7G",
        "replyto": "7t3ggLCjl7G",
        "invitation": "ICLR.cc/2023/Conference/Paper1481/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work has a key focus on generalization in overparameterized learning problems. The authors propose a new trajectory analysis for generalization that depends both on the data-generating distribution and the algorithm of the underlying setting, unlike most of the existing work which typically concentrates on one of either. A condition, named *compatibility* is being introduced, which asserts that a data-generating distribution and an algorithm are compatible roughly when the excess risk converges (both data and algorithm are treated random, as a key component of this work)  to zero as the training sample size grows. A *compatibility region* is the collection of \"allowed\" values that the data generating distribution and the algorithm can be parametrized with where the resulting pair is compatible. The authors' main focus after this definition is to derive the compatibility region of a linear regression model that is trained with gradient descent (number of iterations as a parameter that belongs in the compatibility region) with the underlying feature-label pairs. Some of the claims are exhibited via synthetic numerical experiments, and then comparisons with some of the existing bounds are analyzed. ",
            "strength_and_weaknesses": "I would like to note that, I am **not** an expert in generalization. I am aware of the general idea, but I would not consider myself as a researcher who is up to date with all the advancements. However, I would have been definitely interested in reading this work if I were to see this paper in the proceedings, hence I will ask questions that I am genuinely wondering, and I will specify strengths and weaknesses from my perspective.\n\n**Strength:** The assumptions are mostly standard and I cannot foresee an \"assumption-related problem\". I really like Theorem 1 and its proof looks correct as much as I am concerned. Corollary 5.1 might have a good impact. Appendix B is extremely well, but \"too well\" that some of the results can even move back to the main paper (e.g., without seeing Example B.1 it is hard to imagine the main idea of the relevant Thm; similarly, Theorem B.2. looks very interesting).  \n\n**Weaknesses:** \n*I list the major and minor concerns that made me give a \"marginal accept\" instead of \"accept\". However, I still think the paper has some novelty hence I am also going to stay active during the discussion period in case the authors have updates or questions regarding my review.*\n\n***Major Weaknesses/Questions***\n- The paper after Definition 3.1 is more or less all about linear regression with gradient descent. The \"strong\" proofs exploit a lot about the structure of linear regression, which already has lots of cool generalization bounds in the literature. I am therefore wondering, to what extent can we use it for other models. Because if we already know we want to use linear regression, then the compatibility definition itself does not have much advantage; I would consider this more useful if we had results over several algorithms and we understand which is good in what setting. Also, due to the simplicity of linear regression with gradient descent, it looks like the compatibility region tells us more about when to stop for how kind of data generating distribution, which is an overstudied topic.\n- The literature summary is not very thorough in my view, especially compared to the rest of the paper. PAC-Bayes generalization theory is almost nowhere discussed, however, there are so many new studies that may be doing what this paper is doing implicitly. Except for an analysis in the Appendix, I did not see much on Rademacher complexity-based generalizations, and I would like to know more about how these approaches differ. \n- The numerical experiments are weak. There is not much these experiments tell the reader, rather than displaying mostly what was shown. The selection of parameters is also done in an ad-hoc manner. \n- I am not sure if considering the data-algorithm pair was *completely* not studied. This is a very strong and risky statement. I agree that in generalization theory this is studied much less, though.\n\n***Minor Weaknesses/Questions***\n- Can we get have any finite sample guarantees? This is very popular these days. I guess it should be easier to analyze in the overparameterized setting.\n- Abstract: The Nagarajan citation is on Deep Learning, but the sentence sounds general. \n- Abstract: \"more suitable notion\" -> more than what?\n- Abstract, and anywhere else: \"last iterate analysis\" is either not cited or not explained. This looks vague.\n- Introduction has a sentence that says \"both the training algorithm and the data distribution play essential roles in generalization analysis\". This is well-known for any setting, not just overparameterized regime.\n- Introduction and overall: The question \"How to incorporate both data factor and algorithm into generalization analysis?\" is not motivated. Why should we be interested in this? For example, later on, you show that this would require fewer assumptions. This might be one reason. But overall, please answer \"why\" in addition to \"how\".\n- The introduction has \"the final interpolator ...\" this term and sentence stay unclear.\n- Figure 1 (a): the right axis is different than the left axis. Why is that?\n- Page 2: The sentence that starts with \"Informally speaking,\" is not clear to me.\n- Section 3.1, **Algorithm**: The parameter $t$ has almost no explanation. This is not clear at all. In general, we keep referring to $t$ in Definition 3.1, too, but this only makes sense in the gradient descent setting. The notation or motivation is not clear.\n- Definition 3.1: As stated above, $\\boldsymbol{\\theta}_{n}^{(t)}$ is not clear. Could you also somewhere state why such an asymptotic convergence in probability would imply generalization? $T_n$ notation is not clear until someone comes to the next sections. \n- Could you please relate **Assumption 3** to the fact that $(\\boldsymbol{X}\\boldsymbol{X}^\\top)$ is invertable? \n- \"iff\" -> if and only if\n- Page 6 refers to Theorem 5.1, this should be a typo.\n- Overall too many times Vartless et al. (2019) are cited. Could you please mention it less? Maybe we can mention benign overfitting first, and then keep referring to \"benign overfitting\" instead?\n- In the Conclusion and throughout the paper the authors say \"this method eases the assumptions\". However, I am not sure about one thing. Here, since we consider algorithm and data together, we are somehow splitting the assumptions equally over them. If we had not parametrized one of either, then we would have needed more assumptions on the latter. In other words, having control over both already gives us the power to decrease the severity of assumptions.\n- The **Setup** part in Section 6 is very simplistic. In general, it is not \"fun\" to read these experiments. They are very standard. I would appreciate seeing more original experiments. The selection of parameters is very ad-hoc and not clear.",
            "clarity,_quality,_novelty_and_reproducibility": "For the details of these points, please see my review of \"Strength and Weakness\". However, to give a high-level answer:\n\n**Clarity:** The paper is not very clear. Sometimes it is hard to follow.\n\n**Quality:** The proofs and statements are strong and have high quality.\n\n**Novelty:** The paper might have some novelty, but I am not experienced in generalization theory enough to judge.\n\n**Reproducibility:** This work would be considered as a \"theory\" work where the findings are clear, hence I believed can be implemented by researchers if they find this work relevant.",
            "summary_of_the_review": "The literature review, writing of the paper, numerical experiments, and motivation are poor. However, the paper has a thorough analysis, strong results, and looks mathematically correct. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1481/Reviewer_a4RZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1481/Reviewer_a4RZ"
        ]
    }
]