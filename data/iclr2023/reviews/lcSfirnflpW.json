[
    {
        "id": "oEgHX6twT9",
        "original": null,
        "number": 1,
        "cdate": 1666680476337,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680476337,
        "tmdate": 1666680476337,
        "tddate": null,
        "forum": "lcSfirnflpW",
        "replyto": "lcSfirnflpW",
        "invitation": "ICLR.cc/2023/Conference/Paper1905/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Paper introduces a many-domain generalization approach, a siamese-type architecture that takes in paired samples from the same domain (e.g. patient) which learns sample as well as domain embedding and in the end predicts the label. The architecture is trained through optimization of a 4-part objective which aims to 1) mutually reconstruct the samples, 2) make domain representation similar, 3) try to align the sample and domain embeddings and 4) get accurate predictions. Extensive evaluation was performed on 4 datasets, plus two studies of effects of small datasets and continuous learning. Framework shows favorable results compared against several relevant competitors.",
            "strength_and_weaknesses": "Strengths\nAs mentioned, compelling empirical evaluation, with favorable results against relevant contemporary approaches.\nThe architecture appears fairly novel, and scaling up the number of domains seems like a very relevant direction. To add just one comment, \u201cmany-domain\u201d prefix does not do justice to the real scale of the tackled problem, as previous approaches also tackled many domains (eg six in Li et al. (2020)). To borrow a term from \u201cmulti-label classification\u201d area, thousands of labels there would make the problem \u201cextreme multi-label classification\u201d, so maybe \u201cextreme multi-domain\u201d would fit here too. \n\nWeakness\nAblation studied impact on loss function pattern only, but have not provided insight on how that impacts supervised task performance downstream. \nIn the \u201cRemark for mutual reconstruction\u201d, stating that z would not be incentivised to preserve label information requires a bit more support. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is written in an understandable way, the approach seems novel and fairly well evaluated. Datasets are publicly available and details seems sufficient for reproducibility.",
            "summary_of_the_review": "Based on the perceived novelty of the framework and thorough empirical evaluation with compelling results, I am on a positive side wrt this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_TUM8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_TUM8"
        ]
    },
    {
        "id": "AQuQrTCT5uV",
        "original": null,
        "number": 2,
        "cdate": 1666759064914,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666759064914,
        "tmdate": 1666759064914,
        "tddate": null,
        "forum": "lcSfirnflpW",
        "replyto": "lcSfirnflpW",
        "invitation": "ICLR.cc/2023/Conference/Paper1905/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the many-domain generalization problem by treating each patient as a domain in healthcare applications. The paper proposes a model MANYDG to capture the domain and domain-invariant label representation by combining mutual reconstruction and orthogonal projection to explicitly remove the patient covariates. Experiments on four real-world datasets demonstrate the effectiveness of MANYDG.\n",
            "strength_and_weaknesses": "S1: The methodology part is written in a clear way and well addressed the main claims in the introduction part. \n\nS2: It is an interesting idea to treat each patient as a domain and address the more general many-domain generalization propblem.\n\nS3: The extensive experiments on four healthcare datasets and two realistic and common\nclinical settings demonstrates the effectiveness of the proposed model.\n\nW1: It's concerned to involve the label information in steps 1 and 2, which causes higher prediction performance due to label information leakage.\n\nW2: It is better to provide theoretical analysis, followed by Data Generative Model and Factorized Prediction Model. \n\nW3: It is better to clarify mutual Reconstructions in section 3.3 about the features v\u2032 hat and v hat.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Most parts of the paper are written in a clear way. \n\nQuality: The quality of the paper can be further improved. \n\nNovelty: The novelty is fair. \n\nOriginality: Good. \n",
            "summary_of_the_review": "Below the acceptance threshold. \nPlease see Strength And Weaknesses for detail.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_Snq3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_Snq3"
        ]
    },
    {
        "id": "L4OaWlCic2",
        "original": null,
        "number": 3,
        "cdate": 1667207668915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667207668915,
        "tmdate": 1667207668915,
        "tddate": null,
        "forum": "lcSfirnflpW",
        "replyto": "lcSfirnflpW",
        "invitation": "ICLR.cc/2023/Conference/Paper1905/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a way to encode patient's data such that domain and personalized (or label) information are separated. This is achieved through domain generalization that assumes each patient as a domain. The method has multiple components, the first component to embed patients data into a different representation v. The second component extracts the domain information from v; this is learned through two objective functions: one for mutual reconstruction and the other for domain similarity. Then, by projecting v on z and removing the domain information from v, it can be used for predicting the label. \n\n  ",
            "strength_and_weaknesses": "Strength: \n- The paper is easy to read (except in some paragraphs)\n- The method is simple \n\nComments: \nThe paper is nice to read, however, I have the following comments: \n\nSec 3.1: the method is applicable only if there is a label for each sample and this is not necessarily the case for healthcare application. In other words, there is one y^d for all samples x_i^d.\n\nSec 3.2: Motivation 2 was not lear at the beginning but after finishing step 2 it became clear. Maybe rewriting that part would be beneficial.  \n\nSec 3.3: \n  - what does this sentence mean \"sg(\u00b7) means to stop gradient back-propagation for this quantity\"? does it mean that you do not update v_mu and only update z? \n  -  equation 11, should the denominator be norm(z)^2? \n\nExperiments:\n  - In table 3, is there any explanation why the second row has low similarity?\n  - Results in Figure 3 and the message \"our ManyDG consistently outperforms the baselines and shows increasing improvements as the number of labels decreases.\" are not convincing as the \"rate\" of deterioration is similar across all models.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is nice to read and the method is simple which is a strength. The results seems promising with incremental improvements. ",
            "summary_of_the_review": "Read the strength and weakness section. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_e35g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_e35g"
        ]
    },
    {
        "id": "FdyhMPjFQe",
        "original": null,
        "number": 4,
        "cdate": 1667303799910,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667303799910,
        "tmdate": 1668698168984,
        "tddate": null,
        "forum": "lcSfirnflpW",
        "replyto": "lcSfirnflpW",
        "invitation": "ICLR.cc/2023/Conference/Paper1905/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to treat each patient as a domain and multiple observation of covariates as samples from the same domain. Consequently, one attempts to isolate the domain and sample specific features in the process of representation learning. Experiments on two datasets demonstrate competitive results compared with SOTA. This idea seems also generalizable beyond healthcare to other applications where such hierarchical assumption on the covariates may apply. \n\n-----------------------------------------------------------------\nUpdate: I very much appreciate the informative and detailed feedback from the authors. ",
            "strength_and_weaknesses": "This paper is firstly very well structured, and the idea has been explained very clearly. \n\nThe introduced idea seems novel to me and I\u2019m convinced of its application in such non i.i.d. data situations. I also appreciate the quantitative verification and the ablation study. \n\nWhat I found less intuitive is the procedure of first learning v from x, z from v and then trying to decompose v into two directions parallel and orthogonal to z, respectively, since v is supposed to contain all information about z already. I\u2019m wondering if one could simply decompose\n\nv=z+u, v\u2019=z+u\u2019 \n\nwhere z encodes the domain information and u encodes the sample information - similar to the factor analysis. \nTechnically, we could use two objectives: \n\nThe similarity between encoder_z(x), encoder_z(x\u2019) is to be minimized, and\n\nThe orthogonality between encoder_u(x) and encoder_u(x\u2019) is to be maximized. \n",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above, clarity is one of the strengths of this paper.  \nThe proposed method seems novel enough and interesting. \nThe authors mentioned that the source codes are included in supplementary materials to which I don\u2019t seem to have access. There\u2019s detailed information on the experiment setup though. \n",
            "summary_of_the_review": "In general this is a fairly good paper implementing assumptions on the data situation into the representation learning, which is highly generalizable beyond patient data.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "Privacy and potential misuse of patient data are always risk factors in the healthcare domain. However, I would argue that the proposed method is not limited to this domain and does not pose more risk than similar use cases. ",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_FdB9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_FdB9"
        ]
    },
    {
        "id": "49OQwX0Zmw",
        "original": null,
        "number": 5,
        "cdate": 1667452804056,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667452804056,
        "tmdate": 1667452804056,
        "tddate": null,
        "forum": "lcSfirnflpW",
        "replyto": "lcSfirnflpW",
        "invitation": "ICLR.cc/2023/Conference/Paper1905/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper assumes that each patient belongs to different domains given by some unobserved covariates. Mutual reconstruction is used to learn the domain covariates, which are then removed by orthogonal projection.  The proposed methods improve prediction performance on healthcare monitoring and EHR datasets.",
            "strength_and_weaknesses": "\nStrength:\n\n- There's an ablation study shows the contribution of each loss to the model performance.\n- The paper experiments on several interesting datsets and applications.\n\nWeakness:\n\n- No definition of any of the variables. What dimensions they are? \n- The mutual reconstruction seems unclear to me. \"we take a new sample and its label (x',y') from the same domain as x\".  Isn't the model \"treating each patient as an individual domain\"? Is the assumption of one patient having multiple samples mandetory?\n- The motivation is unclear. The proposed predictor does not marginalize over z. Where does equation 2 come from? Also, why consider p(x|z,y) in motivation 2? It's irrelvant to motivation 1 and equation 2. \n- It's unclear what the magnitudes of decomposed two orthogonal vectors are for each sample. It would be interesting to see a scatter plot for that.\n- Suppose there is a strong spurious correlation between domain and labels. E.g. all the samples from some patients are all negative/positive, which actually happens a lot. In this case, the domain representation z should be able to predict the label very well. Will that cause a problem when you use the vector orthogonal to z?\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some parts of the paper are not justified or not well-motivated. ",
            "summary_of_the_review": "The paper proposes an interesting method that treats each patient as a domain. However, different components of the model are not well explained.  E.g. what does z looks like? what does the model want to disentangle from the direction of z? Also, the writing is not rigours in maths.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_xtPR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1905/Reviewer_xtPR"
        ]
    }
]