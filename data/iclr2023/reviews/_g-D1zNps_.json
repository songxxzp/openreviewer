[
    {
        "id": "sjc31JLCAIy",
        "original": null,
        "number": 1,
        "cdate": 1665883146735,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665883146735,
        "tmdate": 1668899093750,
        "tddate": null,
        "forum": "_g-D1zNps_",
        "replyto": "_g-D1zNps_",
        "invitation": "ICLR.cc/2023/Conference/Paper1501/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a new mechanism, called *gradient-guided parity alignment* to ensure fairness in neural models. This is done by studying the fairness problem from the perspective of decision rationale, which has been one of the mainstream areas of focus in interpretability literature. The authors implement their proposed mechanism along with the standard fairness regularizer which is the mainstream strategy in the in-processing group fairness literature. More concretely, first the loss change before and after removing a specific parameter $w_k$ of the neural model if computed in eq 4. This is then used to define the *neuron parity score* in eq 5. Intuitively, this characterizes that the change in the loss based on a given $w_k$ should be the same across the subgroups. The parity score is computed for all the parameters in the model. Furthermore, since directly computing this for all samples and neurons is expensive, the authors utilize a sampling strategy. Eq 7 is relaxed to obtain the gradient weighted estimation of the change in loss based on the first order Taylor expansion. Lastly, these parity scores are normalized layerwise and used in the final loss objective, with the underlying parity computed based on cosine similarity. Extensive experiments have been performed against popular fairness methods along with interesting analyses.\n",
            "strength_and_weaknesses": "1. The proposed method can be easily integrated in the training procedure for an arbitrary network, making it highly generalizable.\n2. Experiments have been performed across multiple values of $\\lambda$ and clearly details the pareto frontier. \n3. The subsection on \u201cLayer-wise Decision Rationale Alignment\u201d provides interesting characterization against the depth of the model. \n4. An important citation to  (Zheng et al 2021; NeuronFair: Interpretable White-Box Fairness Testing through Biased Neuron Identification) is missing. This work has some similarities to that of Zheng et al, in that both focus on proposing a function over the network neurons.\n5. The computation of the second order derivatives is a significant overhead. It would be interesting to see some results comparing the runtime of the proposed method against the baselines. \n6. While computing eq 7, do the authors turn off the dropout layer in the model? Not turning it off could have detrimental results on the performance.\n7. There is no study for the variation of results against $\\beta$, which is the hyperparameter of the proposed misalignment objective. \n8. The complete hyperparameter details such as the optimizer used, batch size, weight decay etc are missing. \n9. Using cosine similarity in eq 9 is a reasonable strategy. Is it chosen for simplicity? Do the authors have some intuition on how the cosine will compare against other metrics, a simple one being the metric induced by the L2-norm?\n10. The final objective in eq 9 utilizes a fairness regularizer as well. It will be more useful to observe the results solely based on the cross-entropy loss along with the proposed alignment regularizer to ensure that the claims hold.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writeup of the paper is very clear. There are some missing hyperparameter details, the completion of which can aid in assessing the reproducibility of the work. ",
            "summary_of_the_review": "Based on the questions, comments and concerns raised in the weakness section, I lean towards weak rejection of the work. I am willing to reconsider my score if the authors can answer the questions raised above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1501/Reviewer_X7LC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1501/Reviewer_X7LC"
        ]
    },
    {
        "id": "-QMgMctFFxH",
        "original": null,
        "number": 2,
        "cdate": 1666451866657,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666451866657,
        "tmdate": 1666451866657,
        "tddate": null,
        "forum": "_g-D1zNps_",
        "replyto": "_g-D1zNps_",
        "invitation": "ICLR.cc/2023/Conference/Paper1501/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposed a novel method of training a fair classifier. In this work, the authors are concerned about the parity of accuracy across protected groups and propose a new parity score that is computationally manageable at a level of individual neurons. This work illustrates the advantages of this method in several empirical settings. ",
            "strength_and_weaknesses": "Strength: \n* This work is easy to follow and clear.\n* AI Fairness is an important, emerging topic. \n* The insight from network size and fairness is valuable. \n* Imposing fairness regularization at the neuron level that is computationally scalable is interesting.\n\nWeakness: \n* Prediction is confused with decision-making (see below).\n* This work does not provide an argument why parity in the accuracy of prediction, specifically in their experiments, is desired at a cost of lowering the accuracy (see below).  \n* The error bars are not provided, so it is hard to evaluate the statistical significance of the results (e.g., Figure 3). The results seem marginal and statistically insignificant. \n* This work does not discuss the limitations of this work. \n* Computational costs are not discussed nor how it is compared with the other methods.  \n\n\n**More details.**\n\n1. Fairness, prediction, and decision-making. \n\nThis work is concerned with the parity of accuracy across protected groups in a prediction task. As an example, they ask how well a model can predict the income level (binary prediction >50k or below) of male and female participants. There is no decision-making downstream component here though. So I do not understand why parity is the accuracy of prediction is considered a fairness criterion. Potentially there is more intrinsic stochasticity in the income level of the female labor force participants and less stochasticity in male participants. This inherent feature reduces the accuracy of a classifier for female participants. By imposing an accuracy parity constraint we essentially degrade the quality of our prediction for male participants while keeping the quality of prediction for the female participant the same, hence degrading total accuracy. Why is this a desired feature? If there is no decision-making component why the authors are after a degraded classifier?\n\nIn the section *Connection with human society*, prediction task that is the focus of this work is confused with the decision-making task. While a classifier can be used as an ingredient in a decision-making task the connection to decision-making and what type of problem is solved here is vague. Suppose I want to predict which galaxy is star-forming and which galaxy is non-star-forming and I have a prediction disparity between spiral galaxies and elliptical galaxies. Is this fair or unfair? But now I can ask how to allocate telescope resources for a set of science applications; then we can actually discuss the fairness in the allocation of resources even though we are talking about galaxies. Now, let's go back to the income prediction example. If a classifier has a poor performance for the female participant, is that classifier unfair? a classifier cannot be intrinsically fair or unfair without specifying the downstream decision-making task, especially if we achieved the optimal Bayes classifier. \n\n2. Lack of confidence intervals. \n\nThe results are marginal and improve the previous methods only by a small margin with a significant additional computational cost (even though the proposed algorithm is scalable). This work should provide error bars for the results so the reader can understand whether the results are significant and whether the additional computational cost is worth the improvement. Without confidence intervals, it is impossible to discuss the significance of the results.  \n\n3. Connection with over-parameterization.\n\nThis section actually provides an interesting insight. However, the comparison is incomplete. Still, we do not know how the accuracy degrades across different models. Does the accuracy also converge across over parameterized models or does the accuracy differs? Hence, a designer does not know whether they should shoot for overparameterization and larger regularization or lower parameterization and smaller regularization.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Novelty and quality. **\n\nThe work is novel, in my opinion. Imposing the Neuron level to achieve parity is interesting and this work provides some additional insight and an algorithmic solution for learning tasks. Overparametrization results, while incomplete, are novel and interesting. This work is well-written and easy to follow (see the following for the typos and suggestions). \n\n** Reproducibility. **\n\nWhat is the value $\\beta$ in Eq. (9) in the experiments? Are the results sensitive to the choice of $\\beta$?\n$\\lambda$ is discussed in detail but not $\\beta$, am I missing something? \n\n**Clarity.**\n\nPage 1. \n\nSec 1. What does social fairness in line one of the introduction mean?\n\nPage 3. \n\nSec 3.1. \"A fair DNN (i.e., F(\u00b7)) is desired to obtain a similar accuracy in the two subgroups.\" This is only a definition of fairness, that is suboptimal and leads to model underperformance. \n\nPage 4.\n\nSec 3.3.\nDoes the background color gradient in Figure 2 mean anything? \n\n\"(i.e., smaller DP)\" do the authors mean larger DP or smaller absolute value of DP? \n\n\u2777 --> \u2778 \n\n\"The data augmentation-based method (i.e., FairReg(\u2206DP, Aug)) can alleviate such an issue to some extent and achieves higher fairness than FairReg(\u2206DP, noAug) under similar accuracy.\" This is not consistent with Figure 2. \n\nPage 5.\n\nSection 4.1\n\n$$\\mathcal{J}$$ is not defined where it first appears. \n\nPage 6. \n\nSection 5.2\n\n\"Then, we can update Eq. (5) by minimizing the\" --> do you mean Eq. (6)? \n\n\n\n",
            "summary_of_the_review": "While this work has potential and is interesting, there are areas that could be improved. Specifically, \n(1) what kind of decision problems is this work concerned about and why the choice of fairness metrics are desired? (2) what is the statistical significance of the results (confidence intervals)? (3) what are the computational costs and how it is compared with the prior methods? how the accuracy changed with $\\lambda$ and the parameter size.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "Fairness is not put into context. It is not argued why degrading accuracy is desired when there is no decision-making component. ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1501/Reviewer_rJaT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1501/Reviewer_rJaT"
        ]
    },
    {
        "id": "KqxVH59YyEz",
        "original": null,
        "number": 3,
        "cdate": 1666533876000,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666533876000,
        "tmdate": 1666533876000,
        "tddate": null,
        "forum": "_g-D1zNps_",
        "replyto": "_g-D1zNps_",
        "invitation": "ICLR.cc/2023/Conference/Paper1501/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new fairness regularization technique based on an interpretation method called neuron parity score. In a nutshell, the proposed method penalizes the gap among the _subgroup loss increments_ from zero-ing out a specific parameter, in addition to the model-level fairness loss. Empirically, the proposed method achieves a better fairness-performance frontier than FairMixup and LAFTR.",
            "strength_and_weaknesses": "__Strength__\n\n- The proposed method is versatile, in the sense that it can be provided to several different notions of fairness.\n- The method shows an impressive performance over the baselines in most empirical setups considered in the paper.\n- Empirical validation has been performed in terms of the full tradeoff curve, instead of showing only on data point.\n\n__Weakness__\n\n- The paper argues that it will give an analysis of the existing fairness-via-regularization technique via decision rationale analysis, but I do not see it. I can only see the experimental results showing that the existing method performs worse. Am I missing anything?\n\n- The \"limitations\" pointed out in Section 3.3 is not very clear to me. The paper says that existing methods are limited, as they could \"hardly generate enough fair networks under similar accuracy.\" But the \"enough\"-ness is in the eye of the beholder; why is 0.058 DP enough, and 0.078 not enough? The logic sounds like, 'existing methods are limited, as it does not perform as well as the method that we are going to propose, which motivates us to develop a new method.' This does not really point out any fundamental, structural limitations of the existing methods.\n\n- The intuition behind using the neuron parity score for fairness is unclear. Authors seem to motivate the technique from the fact that \"how the fairness regularization term affect the final network parameter is not well understood,\" but I am not sure how the existing method provides a strictly better interpretation, or why such better parameter-wise interpretability should help improving the ultimate performance.\n\n- The range of baselines considered is quite limited. There is a wealth of methods for fair classification, including the ones based on adversarial debiasing, optimal transport, etc. Is there a reason why authors confined the baselines to only a few?\n\n- Eq (5) is quite atypical, in the sense that it is squaring the already-squared quantities. Although this is not really used in the main algorithm, I am curious about the reason why such nested squaring is a desirable thing to do. Could authors give a little more explanation?",
            "clarity,_quality,_novelty_and_reproducibility": "I am generally satisfied with the clarity and quality of writing. The proposed method seems to be novel and reproducible (the code has been provided).",
            "summary_of_the_review": "Although the method seems to provide decent performance gain over the baselines, I think there is a significant room for the manuscript to improve in terms of (1) the diversity of the baseline methods, and (2) clarity of the motivation behind the algorithm.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1501/Reviewer_HMGh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1501/Reviewer_HMGh"
        ]
    },
    {
        "id": "o3M5XwRS98-",
        "original": null,
        "number": 4,
        "cdate": 1666656038564,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656038564,
        "tmdate": 1669975591611,
        "tddate": null,
        "forum": "_g-D1zNps_",
        "replyto": "_g-D1zNps_",
        "invitation": "ICLR.cc/2023/Conference/Paper1501/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The motivation of this paper is to achieve fairness by modifying the learning of the neuron of deep networks (addressing decision rationale misalignment). The proposed algorithm is feasible for large weights in deep networks, and experiments results show that the algorithm makes the result more fair, combined with a conventional regularizer for fairness (measured by DP, EO, etc.) Analysis shows the data augmentation can help achieve fairness in some datasets.\n",
            "strength_and_weaknesses": "Pros: \nThis paper proposed a new way to achieve fairness by adjusting the inner learning neurons. This approach is interesting and sheds a way to achieve fairness thoroughly. Experiments were conducted, combined with DP and EO. Intensive   \n\nCons:\nMore analytics analyses such as the relationship between alignment and various fairness metrics, are not thoroughly studied. In the experiments of Credit, FairReg (Aug) shows better performance in DP. It raises the issue of how augmentation can help the DRAlign algorithm. However, it was not addressed.   \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: \nIt is valuable to study the relationship between alignment and fairness penalty function. It is clear that the neuron alignment combined with the fairness penalty function help achieving fairness in a thorough way. I have a question that when if there is no penalty function for fairness, the alignment can affect improving fairness. At first glance, the alignment itself cannot affect fairness. The more interesting part is that alignment cannot be biased for the DP or EO. The basic concept of alignment seems related to the DP. Therefore, it can be an interesting point to clarify the property of alignment related to other fairness metrics, such as predictive parity, counterfactual fairness, etc. \n\nQuality:\nWell-written. The editing of the paper can be improved. In figures, the fonts of legends are somewhat small, and the use of $\\Delta DP, no AuG$ and $\\Delta DP, Aug$ are inconsistent. Also, the \"combined with human society\" Section does not provide much insight or philosophy.      \n\nNovelty and reproducibility:\nThe approach and algorithm are somewhat novel, which is valuable. The reproducibility does not have a severe problem because the authors used a well-known algorithm with regularizes.\n\n",
            "summary_of_the_review": "This paper considers an interesting approach with alignment, which can make the fairness algorithm plenty. Experiment results seem promising. The deficit is not thoroughly studied with various fairness metrics.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "This paper studies the fair algorithm, which can have a social impact. ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1501/Reviewer_3JWw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1501/Reviewer_3JWw"
        ]
    }
]