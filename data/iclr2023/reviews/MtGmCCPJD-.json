[
    {
        "id": "hhWb5DvQ_i_",
        "original": null,
        "number": 1,
        "cdate": 1666581802322,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581802322,
        "tmdate": 1666617090554,
        "tddate": null,
        "forum": "MtGmCCPJD-",
        "replyto": "MtGmCCPJD-",
        "invitation": "ICLR.cc/2023/Conference/Paper4604/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors propose prompt generator based on the whole code repository for the code completion tasks.\nAuthors show a significant improvement over baseline Codex results by using their prompt generator.",
            "strength_and_weaknesses": "Strengths:\n- Addresses an important and novel area of good prompt generation for LLM tasks\n- Approach does not require access to the weights of LLM\n- Suggests and implements prompt generator that uses information from the whole repository. The prompt generator also uses repository level prompt \"proposals\" (rules/suggestions).\n- Using generated prompts significantly improves Codex results\n\nWeaknesses:\n\n- It is not clear whether prompt proposals are really per-repository. It seems that they will be pretty universal and should not vary much. So the value of repository-specific prompt proposals is not really established or proven. This is a minor issue though.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. \nI think that the paper presents a novel approach for improving results of tasks such as code completion by generating prompts from whole repository using a trained framework.\nThe results of the paper should be reproducible assuming that the dataset used by authors to train the PPC is available.\n\n",
            "summary_of_the_review": "I think that the paper presents a novel approach for improving results of tasks such as code completion by generating prompts from whole repository using a trained framework. This is interesting work and should be accepted to ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4604/Reviewer_9is8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4604/Reviewer_9is8"
        ]
    },
    {
        "id": "ddhYN3Isbv",
        "original": null,
        "number": 2,
        "cdate": 1666601925028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601925028,
        "tmdate": 1666601925028,
        "tddate": null,
        "forum": "MtGmCCPJD-",
        "replyto": "MtGmCCPJD-",
        "invitation": "ICLR.cc/2023/Conference/Paper4604/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a repository-level prompt generation (RLPG) approach for language models of source code. Without having access to the weight of LLMs, RLPG can improve the performance of LM in code completion task. Through training the prompt proposal classifier, the prompt composer can generate high quality prompts for LM, increasing the accuracy of code completion.",
            "strength_and_weaknesses": "Strengths:\n\n- A novel way of generating prompts.\n\n- Extensive experiments.\n\n- RLPG is efficient for both training and inference.\n\nWeaknesses:\n\n- The paper is better for a software engineering/programming language conference.\n\n- Only the structure of the repository and the context from other relevant files are considered in the prompt proposals.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written. The implementation of the model is not available for replication study. The proposed work appears to be novel.",
            "summary_of_the_review": "I appreciate that the proposed technique can generate prompts in a black box manner. Different from previous work about line-level code completion, this paper considers the completion from the cursor to the end of the line. It involves a wide range of prompt sources and prompt context types. The proposed approach is well ablated and extensive experiments are conducted. RLPG is also efficient for both training and inference.\n\nThe proposed work learns to generate example-specific prompts using prompt proposals, which are taken from the structure of the repository and the context from other relevant files (e.g., imports, parent class files). The authors may also consider other code information such as API. See, for example:\n\nLyu et al., Embedding API dependency graph for neural code generation. Empirical Software Engineering, 26(4):61, 2021. \n\nFor RLPG-H, the used code context is two lines before and after the hole. Are two lines (or four) enough? Maybe the target hole is related to some statements far away from the target (e.g. some global variable definition).\n\nThe Identifier Usage (Random) also achieves promising results, why?\n\nFigure 1 is not easy to understand.\n\nRecently, the following work also applies prompt learning to pre-trained code models: Wang et al., No More Fine-Tuning? An Experimental Evaluation of Prompt Tuning in Code Intelligence, https://arxiv.org/abs/2207.11680.\n\nThe proposed technique is more about software engineering/programming language, the novel technical contribution to neural networks/language models is a bit limited. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4604/Reviewer_VAEL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4604/Reviewer_VAEL"
        ]
    },
    {
        "id": "-ksgVN5wz7R",
        "original": null,
        "number": 3,
        "cdate": 1666772319204,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666772319204,
        "tmdate": 1666892329311,
        "tddate": null,
        "forum": "MtGmCCPJD-",
        "replyto": "MtGmCCPJD-",
        "invitation": "ICLR.cc/2023/Conference/Paper4604/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper is part of the stream of papers that try and generate code given a prompt. This paper tries to assess that if additional context is given as part of the prompt, whether the performance can improve. In particular, they predefine different types of contexts and then train a classifier to predict which context should be prepended to the prompt. To get the training data, they given different types of contexts to Codex and if a particular context is able to make the completion match the ground truth completion, they label that context as 1. They then train a multi-label classifier and use its predictions at inference time to decide what context to add on to the prompt.\n\nThey compare against several baselines - no additional context, oracle context (using the ground truth correct contexts) which is the upper bound, random context, nearest neighbors in representation space from the random contexts, lines using the closest identifier throughout the repo, and context to the right of the line to be completed.\n\nThe strongest baseline is the right context baseline against which they show a 1-4% relative improvement and a 14-16% relative improvement over no additional context (Table 1)",
            "strength_and_weaknesses": "Strengths:\n\n(1) The idea of generating training labels for the correct contexts and training a classifier to predict what the correct contexts should be is novel and interesting\n\n(2) They investigate multiple possible baselines \n\nWeaknesses:\n\n(1) Overlap with pretraining data -- Github is not the only source of training data for Codex (https://openai.com/blog/openai-codex/ -- \"OpenAI Codex is a descendant of GPT-3; its training data contains both natural language and billions of lines of source code from publicly available sources, including code in public GitHub repositories\"). Thus it is very possible that Google Code -- which they use for their test data -- is part of the training data for Codex. Furthermore, deduping based on just repo name match is highly imperfect. A much better way would be file level deduping or suffix array based deduping. Unless there's a better understanding of what the pretraining data consists of, the results might be invalid. The authors could consider using a model like CodeGen for which the pretraining dataset is known. Alternatively they could use code published after June 2021 (the training data cutoff for the Codex davinci-002 model), dedupe it against the code data available upto June 2021 and use that as their test data.\n\n(2) The improvements over both right context as well as identifier usage baselines are modest.\n\n(3) They don't explore how different prompt proposals could be combined with each other",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nIt's not clear how they are selecting the prompt proposal at inference time. Are they taking the argmax over the probabilities for the different prompt proposals?\n\nQuality:\nError bars are not given. It's unclear how much overlap the test set has with the pretraining data.\n",
            "summary_of_the_review": "While I believe the ideas in the paper are interesting and novel, the fact that we do not know the pretraining data which could easily overlap with the test data + the deduping with respect to the the known part of pretraining data being imperfect (based on repo name match instead of file content match or suffix array deduping), the results are not reliable.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4604/Reviewer_xrrm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4604/Reviewer_xrrm"
        ]
    },
    {
        "id": "bmtUB-Rhu_",
        "original": null,
        "number": 4,
        "cdate": 1666851224074,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666851224074,
        "tmdate": 1666851224074,
        "tddate": null,
        "forum": "MtGmCCPJD-",
        "replyto": "MtGmCCPJD-",
        "invitation": "ICLR.cc/2023/Conference/Paper4604/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to make prompt proposals from repository-level context and add concatenate the prompt proposals with normal context to LLM in order to feed more related information to achieve better generations. The paper explores different sources of repository level prompts, as well as prompt context types. Experiment results show that additional repository level prompt help improve the performance significantly.\n",
            "strength_and_weaknesses": "Strength:\n- The paper is well-written. The method is well explained with examples. Experiment results are extensive with analysis.\n- The paper explores several variants on the prompt sources and prompt types, and made a nice figure to demonstrate which prompt source works the best, though only one is used in experiment at a time.\n\n\nWeakness:\n- Taking the whole file from the whole import file seems unnecessarily large context. Rather, if the import statement only imports one class or one function from the module, it makes sense to only consider the function implementation, rather than the whole file where the function resides. This especially applies to 7) Method Names and Bodies prompt type.\n- The paper only considers one prompt proposal, which is a large limitation of the paper. Why don't the authors take top-k prompt proposals and put into context instead of just one? Besides, I'm curious whether there is a better choice of prompt proposal, for example, intuitively the function names, docstrings, and variable names of the imported function could help inform the model the functionality of the imported function.\n- It seems that, RLPG-H is more like classification, while RLPG-R is more like retrieval. Would a lexical search method like BM25 could retrieve relevant prompts already compared? That way no training for the PPC is needed. And I would expect it works reasonably well.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-writtern and is of good quality. The proposed method is somewhat novel. And I expect that the results should be easy to reproduce.\n",
            "summary_of_the_review": "Overall, I think this is a decent paper with novel method proposed and well-performed experiments. I have some concerns on the paper, and would want to see more experiments and results to make the paper more complete.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4604/Reviewer_oQyE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4604/Reviewer_oQyE"
        ]
    }
]