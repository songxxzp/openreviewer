[
    {
        "id": "WknvuyTZSVK",
        "original": null,
        "number": 1,
        "cdate": 1666623006707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623006707,
        "tmdate": 1670519174374,
        "tddate": null,
        "forum": "5spDgWmpY6x",
        "replyto": "5spDgWmpY6x",
        "invitation": "ICLR.cc/2023/Conference/Paper4026/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the properties of sharpness aware minimization through a second order Taylor approximation, in the setting where the perturbation radius tends to zero. Under many strong assumptions, some sort of equivalence (in the limit) between different notions of sharpness-aware-minimization and the spectra of the Hessian of the loss function is derived. The stochastic setting is also studied where two common notions of SAM are shown to be equivalent. The main ideas of the proofs follow the arguments in the particular case of quadratic functions, which are then extended to the general case by means of a second order Taylor approximation. Some of the results are in fact presented only for the quadratic case.\n\n**after rebuttal**\nAuthors have made an effort to address some of my concerns. Although I do not feel all of them have been addressed in full, I am not opposed to it being accepted in current form. I can increase my score to \"marginally above acceptance threshold\". The main drawback of this paper is that its hard to read and the appendix is so long and full of equations that **it's unrealistic that anyone will review it in full for a conference cycle**. Hence, my score should be seen as reflecting the fact that the main 9 pages present a coherent story with (potentially) significant results, but **trusting their validity is left to the authors and potential readers**. As a result, I will also decrease my confidence.",
            "strength_and_weaknesses": "The main weakness is that the notation and results are presented with really complicated math, hurting the readability. The results are basically a study on sharpness aware minimization on quadratic functions (in this setting the results are more clear and readable), and the added complexity comes from extrapolating such results to the general case by using a second-order Taylor expansion. I will list some further issues:\n\n1. Definition of $U$ in page 3 should be made clear as it appears in other statements. This is problematic as theorem 4.3 which is supposed to be important, is no longer self-contained which hurts readability and clarity\n2. The definition of the \"Limiting regularizer\", as it contains a limit when $\\rho \\to 0$, simply has the goal of making the second order Taylor approximation to have a vanishing error. This facilitates the analysis and makes the function behave like a quadratic function $L(x)=x^\\top A x$. Indeed all the arguments become simple and clear if one just studies this family of functions. Most of the results presented like Theorems 4.5, 4.6 and 4.7 follow easily in this case. The arguments dealing with the approximation error become highly technical and confusing. This hurts readability. I would say the authors could expose the results more clearly if they just explained the arguments for the case of quadratic functions, and leave all the technicalities of the approximation to the appendix.\n3. Another trick the authors use for simplifiyng the analysis is assuming that the solution of the regularized objective is also a solution of the unregularized objective. However this is never clearly stated, and is probably not true except in the limit when $\\rho \\to 0$ (in which case there is no sharpness-aware minimization). This leads to confusing claims like the ones in the paragraph after equation (5): \"The first order term vanishes when we try to minimize the *regularized objective* because every minimizer of the *original loss* has zero gradient\". Again, this is potentially false as there is no reason to believe that a solution of the regularized objective is also a solution of the unregularized objective.\n4. Also in the paragraph following (5) the authors claim \"both the first order terms are the same since $\\sup_{\\|v\\| \\leq 1} v^\\top \\nabla L(x) = \\| \\nabla L(x) \\|_2$\", however this is not true as the supremum in equation (4) contains a second order term of the form $v^\\top \\nabla L(x) v$, hence the supremum is not necessarily the supremum of the first term only. This claim is misleading.\n5. Issues with notation: In page 4 a regularizer is defined as a positive function as implied by the notation $R_\\rho: \\mathbb{R}^D \\to \\mathbb{R}^+ \\cup \\{\\infty\\}$, however immediately after this they discuss regularizers that might be negative? the notation should be fixed.\n6. In definition 4.2 it should be noted that $S$ can take the value $\\infty$ in some cases. Furthermore there is a typo $\\epsilon > 0, \\epsilon > 0$.\n7. The appendix is rather careless with a lot of notation that is not introduced with care. For example already at the first page of the appendix (page 13) we have in Lemma B.5 some letter $C$ that then does not appear at all in the body of the lemma. After a lot of thinking I assume the authors meant $K$ instead of $C$. But the problem does not stop there. Item 3 in lemma B.5 states that $L$ is $\\mu$-PL in some set $K^r$ where $K$ is some compact set. This is clearly false in general. I had to do my best effort to track the issue and indeed $K$ is not an arbitrary compact set $K \\subseteq \\Gamma$ but has a particular structure that is defined in the reference Arora et al. 2022, but which is not reproduced here. Indeed I had to go to the appendix in that reference to decipher that $K$ here corresponds to $Y$ in the reference, which is defined as \"the trajectory of limiting flow Equation (4)\", none of which is clarified in the appendix of this work. I believe the authors simply copied and pasted without care. Hence, the beginning of the appendix is already unreadable and hard to review.\n8. Lemma B.6 has the same issue where there is some letter $C$ that actually refers to $K$ (I believe).\n9. To me, it is not clear how the second order Taylor approximation can be used for $L^\\text{asc}$ as in equation (5): Note that this is only possible in a neighborhood of points where $\\nabla L (x) \\neq 0$, as this term appears as a denominator. However the authors then start arguing that they study points where $\\nabla L(x)$ vanishes? I don't think this is really rigorous as equiation (5) assumes that the taylor expansion has an error that is uniformly bounded by the term $\\mathcal{O}(\\rho^3)$ but in fact this term can depend on $x$.\n10. Assumption 4.1 holds trivially as soon as the regularizer is continous, as any continuous function attains a minimum over a compact set. $R^\\max$ is continuous. Hence it is trivial that assumption 4.1 holds and it is not necessary to go over the complicated argument in Step 1 in the proof of theorem 4.5 (appendix page 17). The same goes for theorem 4.7. \n11. There are some missing citations and some of the results like theorem 4.5 and 4.7 might be well-known, see the clarity/quality/novelty/reproducibility section for details.\n\nAnother weakness is that there is no clear impact from the results presented. Even though they are interesting, what practical conclusions can we draw from this? Having a better understanding of SAM could be achieved by studying the case of quadratic functions, but the heavy math to extrapolate the results to the general case through Taylor approximation, at least in my case, does not improve my understanding of what SAM does further. In any case the results hold in the regime where $\\rho \\to 0$ which means when the regularization is almost negligible. I think this will be of interest only in a small niche in the community. I would say the authors could achieve more impact and more interest if they would rewrite the results with simpler notation and just exposing the core ideas in the context of quadratic functions, while relegating the notation-heavy stuff to the appendix in case someone wants to delve further.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: notation could be improved a lot and there could be an effort from the authors to make the results more accesible and readable to a more general public. There are many typos and things that are not properly defined in the appendix that makes it difficult to review the results. The second paragraph in the introduction is hard to read because at that point the reader doesn't know anything about the content and definitions in the paper that the authors refer to.\n\nQuality: The final results need a lot of technical proofs. The main text feels rushed, there should be more care in the appendix regarding notation.\n\nOriginality: the relation between sharpness and the eigenvalues of the Hessian appear to be somewhat known. From the original work of Keskar et al. 2017 (end of page 6):\n> Note that Metric 2.1 is closely related to the spectrum of $ \\nabla^2 f(x)$. Assuming $\\epsilon$\u000f to be small enough, when $A = I$, the value (4) relates to the largest eigenvalue of $ \\nabla^2 f(x)$ and when $A$ is randomly\nsampled it approximates the Ritz value of $ \\nabla^2 f(x)$ projected onto the column-space of $A$.\n\nAlso see https://arxiv.org/pdf/2206.10654.pdf (June 2022) page 3:\n> While the final SAM procedure (their Algorithm 1) does not directly penalize $\\lambda_{\\text{max}}$, the idealized regularizer in their Equation 1 is, at a local or global minimum, equivalent to the maximum Hessian eigenvalue, if one assumes that\nthe training objective is well-approximated by its second-order Taylor approximation.\n\nHence, this leads me to believe that some (not all) of the results here are already understood. The following is *concurrent work* so it should not be taken into account for the decision regarding this submission, but it illustrates that the convex quadratic case leads to more understandable results while still requiring careful work: https://arxiv.org/pdf/2210.01513.pdf \n\nLemma 3.3 in https://arxiv.org/pdf/2203.08065.pdf is equivalent to Theorem 4.5\n\nReproducibility: the work only contains theoretical contributions.",
            "summary_of_the_review": "The paper requires a good effort to improve readability and be more accessible to a wider audience. The appendix was not carefully written which makes it hard to review. Some of the results might be already understood, but there is no mention of the works that already establish some relation between sharpness and the spectra of the Hessian.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4026/Reviewer_G4Gz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4026/Reviewer_G4Gz"
        ]
    },
    {
        "id": "duqBzDgliry",
        "original": null,
        "number": 2,
        "cdate": 1666769509162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666769509162,
        "tmdate": 1666769509162,
        "tddate": null,
        "forum": "5spDgWmpY6x",
        "replyto": "5spDgWmpY6x",
        "invitation": "ICLR.cc/2023/Conference/Paper4026/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Despite the success of SAM in many applications, the existing theory for SAM is still insufficient to explain its impressive empirical performance. To bridge the gap, this paper rigorously proposes three types of sharpness: worst-direction, ascent-direction, and average-direction, which correspond to the notions of sharpness used in the objective, the actual SAM implementation, and the term used in the generalization analysis of SAM, respectively. They demonstrate that these three types of sharpness lead to different explicit biases in the deterministic case. Moreover, they show that the full-batch SAM minimizes the worst-direction sharpness though it adopts the inexact computationally efficient variants in realization, owing to the implicit alignment between the gradient and the maximal eigenvalue of Hessian. Besides, stochastic SAM with batch size 1 minimizes the average-direction sharpness, which directly benefits the generalization performance. ",
            "strength_and_weaknesses": "**Strengths:**\n\n* The paper is very well-written. I enjoy reading this paper. Instead of stating the theoretical stuff alone, they elaborate various notions and theorems with the high-level idea throughout the paper. \n\n* Three notions of sharpness, their explicit biases, and correlations to SAM discovered in this paper are interesting and could stimulate future research in this area.  \n\n* The toolboxes used in the analysis are interesting, which borrow ideas from the recent literature on edge of stability in SGD. \n\n**Weakness:**\n\n* It is not very clear why assuming the minimizers of loss form a manifold following (Fehrman et al. 2020; Li et al. 2021; Arora et al. 2022) is important. What is the limitation and advantage of this assumption? Please clarify. \n* The proof techniques largely follow (Arora et al. 2022). So it would be helpful to clarify the new challenges and how they are being solved in this paper.  \n* In Section 5.1, it is not clear why the Hessian of each individual loss is rank-1 although I am aware of the Hessian of the total loss is rank-M. Does the individual loss need to be independent?\n* This paper is mainly theoretical sound, but it can benefit from more empirical validations beyond the toy example. For example, adding a real simulation to verify different convergence properties of deterministic and stochastic SAM will be useful. If not, adding references that can support the theoretical results is also helpful. \n\nMinor issues:\n* Sharpness in stochastic setting seems not well defined. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Good clarity, high quality, and sufficient novelty.",
            "summary_of_the_review": "This paper is novel and solid, which solves an open question in the theoretical area and provides a new aspect in sharpness and generalization analysis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4026/Reviewer_ReRk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4026/Reviewer_ReRk"
        ]
    },
    {
        "id": "0pIhc5oa7c",
        "original": null,
        "number": 3,
        "cdate": 1666843024560,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666843024560,
        "tmdate": 1666843024560,
        "tddate": null,
        "forum": "5spDgWmpY6x",
        "replyto": "5spDgWmpY6x",
        "invitation": "ICLR.cc/2023/Conference/Paper4026/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Sharpness-aware training has been widely adopted for its generalization performance.\nThis paper provides the theoretical analysis for sharpness-aware minimization (SAM).\nThis paper categorizes the SAM in three ways, worst-direction, ascent-direction, and average direction.\nBesides, they provide explicit bias for each way.\nThe paper is clear and theoretically sound.",
            "strength_and_weaknesses": "Strength\n1. This paper categorizes the SAM in three ways, and they explain each method in detail.\n2. The paper claims the novel findings and their explicit bias.\n3. There are no experimental results for the real-world dataset, but they provide an intuitive toy experiment to support the claim.\n\nWeakness and Questions\n1. There is no experiment for the real-world datset.\n2. This paper raises a new perspective to analyze the SAM. It would be more interesting if the paper provided a new approach from a new perspective.\n3. This paper claims that batch size=1 SAM corresponds to the average-direction sharpness. In the SAM paper, they introduce the m-sharpness, and m=1 shows a strong performance improvement compared to the full-batch SAM.\nI wonder about the author's opinion about the average-direction sharpness that performs better than worst-direction sharpness in real-world experiments. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to follow.",
            "summary_of_the_review": "I carefully read the paper and the main statement of the theorem.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4026/Reviewer_pjDp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4026/Reviewer_pjDp"
        ]
    }
]