[
    {
        "id": "K5yjbcBlp7I",
        "original": null,
        "number": 1,
        "cdate": 1666321250633,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666321250633,
        "tmdate": 1671038857198,
        "tddate": null,
        "forum": "5ohslQBnxUw",
        "replyto": "5ohslQBnxUw",
        "invitation": "ICLR.cc/2023/Conference/Paper2667/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to analyze the convergence of gradient flow of multi-layer linear models. The paper deposit that in general, the convergence rate depends on two trajectory-specific quantities: the imbalance matrices (which measure the difference between the weights of adjacent layers) and on the least singular values of the weight product. The framework of the paper is designed to cover several existing initialization schemes and applies to both regression and classification.",
            "strength_and_weaknesses": "### Strengths\n\n- The paper is well-written with a sufficient literature review. The main ideas and techniques of the work were developed based on well-founded rationales.\n- The mathematical analyses of the work are rigorous and seem correct. The framework of the paper is general and broadly applicable (covers several existing initialization schemes; general loss functions)\n- The paper obtains several new technical results/improvements\n    - The derived bounds (Theorems 2,3) characterize the general effect of weight imbalance on convergence. Previous works on this aspect focused on two-layer models or when all imbalance matrices are positive semi-definite.\n    - The analysis applies to general loss functions and thus can be used to study classification tasks (Theorem 4). Existing works mostly focused on l2 loss for regression tasks.\n    - Three-layer model: Theorem 2 is stronger than previous work (Yun et. al.(2020)) in that it doesn\u2019t force the partial ordering of the weights for convergence. The paper also proves that the bound in this case is optimal.\n    - Deep linear models:\n        - The bound of Theorem 3 dominates and unifies two existing bounds on deep networks (Arora et. al.(2018a), Yun et. al.(2020)) and characterizes the general effect of weight imbalance on convergence\n        - The unimodality condition seems to be novel and contains that of Yun et. al.(2020) as a special case\n    - Some technical constructions of the proof of Theorem 2 (interlacing of spectra of two matrices whose difference is positive definite and low-rank; explicit construct of the optimal solution to the lower bound) might be of general interest.\n\n### Weaknesses\n\n- None noted",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written with a sufficient literature review. The main ideas and techniques of the work were developed based on well-founded rationales. \n- The mathematical analyses of the work are rigorous and seem correct. \n- The paper obtains several new technical results/improvements. ",
            "summary_of_the_review": "The paper addresses a meaningful question. The work obtains several new technical results, and provides some useful insights about the effect of weight imbalance on convergence for deep linear models.\n\nUpdate: The reviewers' discussions come to a general assessments that while the contributions of the manuscripts are significant and somewhat new, most of the technical improvements are marginal with some concerns about the realisticness of some of the assumptions. My final score was adjusted from 8 to 6 to reflect this summary of the discussions. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2667/Reviewer_QQB2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2667/Reviewer_QQB2"
        ]
    },
    {
        "id": "LDTYgSKX-cT",
        "original": null,
        "number": 2,
        "cdate": 1666673955406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673955406,
        "tmdate": 1666673955406,
        "tddate": null,
        "forum": "5ohslQBnxUw",
        "replyto": "5ohslQBnxUw",
        "invitation": "ICLR.cc/2023/Conference/Paper2667/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proves the exponential convergence of gradient flow on multi-layer linear models in which the loss function f satisfies the gradient dominance property. It also provides a lower bound on the convergence rate that depends on the imbalance matrices and the least singular value of the weight product. ",
            "strength_and_weaknesses": "Strength:\n1. The paper is well written and clearly motivated.\n2. The obtained convergence result is quite general and can be applied to a wide range of imbalanced initializations.\n\nWeakness:\n1. Limited to linear networks",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: High\nQuality: High\nNovelty: Fair. It seems that the work is an extension of Min et al. 2022 to multi-layer setting.",
            "summary_of_the_review": "The paper provides the exponential convergence result for the gradient flow of multi-layer linear networks under a gradient dominance assumption. It applies to a wide range of imbalanced initializations. It is technically correct and the obtained result is new. The novelty of this paper is fair and it builds upon several existing works on overparametrized linear networks. The impact of this work is not clear as it only applies to linear networks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2667/Reviewer_5qgv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2667/Reviewer_5qgv"
        ]
    },
    {
        "id": "_709ZsgyIR",
        "original": null,
        "number": 3,
        "cdate": 1667946454609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667946454609,
        "tmdate": 1667946454609,
        "tddate": null,
        "forum": "5ohslQBnxUw",
        "replyto": "5ohslQBnxUw",
        "invitation": "ICLR.cc/2023/Conference/Paper2667/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends the interesting results of [1] on the convergence\nof gradient flow for two-layer linear networks to the case of deep\nnetworks, and to a wider variety of loss functions, obtaining\nbounds on the rate of convergence in terms of notions of the\n``imbalance'' of the initialization of the network.  They also extend\nresults from [2] on deep networks to a wider variety of\ninitializations, again exposing a wider variety of effects.\nTheir bounds for three-layer networks are quite general, and they\nprovide bounds for deeper networks whose initializations satisfy\na condition like one used in [2], but more general.\n\n[1] Hancheng Min, Salma Tarmoun, Ren\u00e9 Vidal, and Enrique\nMallada. Convergence and implicit bias of gradient flow on\noverparametrized linear networks. arXiv preprint arXiv:2105.06351,\n2022.\n\n[2] Yun, Chulhee, Shankar Krishnan, and Hossein Mobahi. \"A unifying\nview on implicit bias in training linear neural networks.\"\nInternational Conference on Learning Representations. 2020.",
            "strength_and_weaknesses": "The way that the PL condition is used to formulate the results is\nunfamiliar to me, and seems nice.\n\nWhere the authors write ``cannot explain well the training efficiency\nin practice'', I don't find this completely convincing, because people\ndon't use extremely small initializations in practice.  Of course it\nis interesting to determine how the convergence time depends on the\nsize of the initialization.\n\nIt is not clear to me how good the lower bound in Theorem 2 is.  It is\nconsistent with my current understanding, such as it is, that the\nbound of Theorem 2 is often zero, which, when true, makes Corollary 1\nvacuous.  It would be helpful to provide some examples of where\nTheorem 2 provides a good bound.\n\nI don't think that some of the claims made in the text after Theorem 2\nare justified by the result.  For example, since Theorem 2 only has a\nlower bound, I don't see that the claim that they ``fully characterize\nthe effect of imbalance on the convergence of three-layer networks''\nis justified.  Also, to show that ``such a partial ordering is not\nnecessary'', you would need to provide an example of where the RHS of\n(17) was positive without such a partial ordering.  Similarly, I don't\nsee that claims made in the intro about characterization are\njustified.\n\nThe condition defined in Definition 3 seems like a \nstrong assumption to me.  For example, it seems like it\nis very unlikely to be satisfied, or even nearly satisfied,\nby a random initialization.  I think that more justification is\nneeded that this is an interesting condition to study.\nAlso, while it is more general than the condition used\nin [2], it seems to make available a similar set of tools.\nIt also reads to me as being crafted to be able to apply\nWeyl's inequality, rather than to capture a useful product\nof natural, and random, initializations.  Roughly speaking,\nit looks to me that unimodality is ``looking under the\nlamppost''.\n\nSome more comparison with prior work would be helpful.  For example, I\nbelieve that the results of this paper are incomparable in strength\nwith the results in [3]-[5], which touch on an overlapping set of\nissues.  I think that the claim that `` the convergence analysis for\nimbalanced networks not in the kernel regime has only been studied for\nspecific initializations'' is not correct, though I do feel that,\ndespite this, the authors' comparison with [2] seems fair overall.\n\nHere are a couple of smaller points.\n\nThe unimodality index is not defined until Definition 3, but is\nused earlier in the proof of Theorem 2 -- the authors should move\nit earlier.\n\nBefore (13), the authors indicate that they are going to define\n$D_1$ and $D_2$, but then they define $D_{21}$ and $D_{23}$.\n\nI will carefully read and consider the authors' reply.\n\n[1] Hancheng Min, Salma Tarmoun, Ren\u00e9 Vidal, and Enrique\nMallada. Convergence and implicit bias of gradient flow on\noverparametrized linear networks. arXiv preprint arXiv:2105.06351,\n2022.\n\n[2] Yun, Chulhee, Shankar Krishnan, and Hossein Mobahi. \"A unifying\nview on implicit bias in training linear neural networks.\"\nInternational Conference on Learning Representations. 2020.\n\n[3] Jin, Chi, et al. \"How to escape saddle points efficiently.\"\nInternational Conference on Machine Learning. PMLR, 2017.\n\n[4] Hu, Wei, Lechao Xiao, and Jeffrey Pennington. \"Provable Benefit of\nOrthogonal Initialization in Optimizing Deep Linear Networks.\"\nInternational Conference on Learning Representations. 2019.\n\n[5] Zou, Difan, Philip M. Long, and Quanquan Gu. \"On the Global\nConvergence of Training Deep Linear ResNets.\" International Conference\non Learning Representations. 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "The mathematical writing is clear.  There seem to be a number of substantial new ideas in the paper.  ",
            "summary_of_the_review": "The strength of the main results was unclear to me, and I felt that some claims made in the paper were not justified by the results.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2667/Reviewer_h9zp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2667/Reviewer_h9zp"
        ]
    }
]