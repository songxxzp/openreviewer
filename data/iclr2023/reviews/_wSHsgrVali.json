[
    {
        "id": "oKojUYCxZoy",
        "original": null,
        "number": 1,
        "cdate": 1666553986418,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666553986418,
        "tmdate": 1669764413511,
        "tddate": null,
        "forum": "_wSHsgrVali",
        "replyto": "_wSHsgrVali",
        "invitation": "ICLR.cc/2023/Conference/Paper3230/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a backdoor poisoning attack that aims to defeat latent separability-based defenses that purify the training set. The attack has two main components: regularization samples that contain the trigger but assigned to the correct class and asymmetric triggers that are weaker versions of the actual trigger planted into the poison samples.  As a result, the attack avoids recent defenses more successfully than baseline evasive attacks.",
            "strength_and_weaknesses": "Strengths:\n\n+ Interesting formulation, the idea of weak triggers combining into a single trigger is novel and seems to be effective to make the attack stealthy.\n\n+ Most work on making backdoor attacks stealthy focuses on supply-chain attacks (not poisoning). This formulation can enable further research in a more realistic threat model.\n\n+ Well written and evaluated on recent attacks and defenses from multiple perspectives to show the benefits of the proposed attack.\n\nWeaknesses:\n\n- The comparisons between baseline attacks and the proposed attack might be unfair and difficult to interpret.\n\n- Ablation study needs a little more work. ",
            "clarity,_quality,_novelty_and_reproducibility": "There has been decent amount of work on making backdoor poisoning stealthy, but the specific formulation this paper offers is original. The evaluation is done thoroughly but I left some feedback down below. ",
            "summary_of_the_review": "1) My biggest concern is that the baseline attacks and the proposed attack have some configuration differences that make comparisons difficult/unfair. \n\nFor example, TaCT uses the trigger in Fig 6m at test-time, an NxN trigger. On the other hand, the proposed attack uses two different triggers overlaid on the image at the test time (the corner pattern and a square). Further, some attacks inject 150 samples and others inject 300 samples (TaCT and the proposed attack). This again makes the threat models inconsistent.\n\nI recommend the authors to first clearly specify the treat model and the attacker's capabilities across the board, e.g., P poison samples [patch] test-time: NxN trigger and training-time: NxN or smaller a trigger or [blend] maximum opacity levels at the training and testing times. Then, run all the attacks within these constraints to make sure that the same attacker can perform any of these attacks within their constraints. Right now, it's not clear if that's the case for all attacks as the threat models in each have minor differences.\n\n2) When evaluating the defenses, the default hyper-parameters have different impact on each attack. For example, SCAn sacrifice rate for TaCT is 4.9% whereas it is 1.2% for Adapt-Blend. This means, SCAn is less conservative on TaCT, which might explain the high elimination rate. I would recommend tuning the defenses across the board to achieve the same sacrifice rate (e.g., 1% - 2% - 5%) and then reporting the elimination rate, which would make the comparisons more fair. Alternatively, you can put a hard threshold over the clean accuracy drop caused by the defense (e.g., 0.1% - 0.5% etc.) then report the corresponding elimination/sacrifice rates.\n\n3) Ablation study doesn't show that regularization is absolutely necessary. Would it be possible to achieve the same impact by reducing the number of poisons (150 - 100 - 50 - 25 etc.) while not adding any regularization sample? In Table 2, you experimented with 150 poison- 0 regularization but what about 100 poison - 0 reg. for example?\n\n4) I know the point of the paper is to evade latent clustering-based defenses but what about other defenses? For example, is the proposed attack more effective against trigger reverse-engineering [1], distillation [2] or data augmentation [3,4]? It would be great to have a small section on these different defensive methods as well. My worry is that the weak triggers would make the attack easier to defuse especially using [2,3,4], would be great to show otherwise.\n\n[1] https://arxiv.org/abs/2102.05123\n[2] https://openreview.net/forum?id=9l0K4OM-oXE\n[3] https://arxiv.org/abs/2011.09527\n[4] https://arxiv.org/pdf/2103.02079.pdf",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3230/Reviewer_YTpa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3230/Reviewer_YTpa"
        ]
    },
    {
        "id": "vd33BuaV1R",
        "original": null,
        "number": 2,
        "cdate": 1666561172613,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561172613,
        "tmdate": 1666561172613,
        "tddate": null,
        "forum": "_wSHsgrVali",
        "replyto": "_wSHsgrVali",
        "invitation": "ICLR.cc/2023/Conference/Paper3230/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the latent separability assumption in backdoor defenses. The paper presents adaptive backdoor poisoning attacks against this assumption and circumvents the recent defenses.",
            "strength_and_weaknesses": "Strengths:\n1. The proposed adaptive attack is well-motivated by the two insights and aims to avoid the latent separability assumption to circumvent the existing defenses.\n2. The evaluation shows the superior attack performance of the proposed adaptive attacks, compared with the existing attacks against four state-of-the-art latent separation defenses.\n3. The paper is well-written and easy to follow.\n\nWeaknesses:\n1. The scope of this work is somewhat limited. The paper only investigates the defenses built upon the latent separation assumption. Other effective defenses, e.g., trigger synthesis, are not examined in the paper. \n2. The proposed attacks become ineffective when the poison ratio is high. It could be one limitation of the attacks when a relatively high poison ratio is required in the attack.\n3. Since the attacks apply strong triggers in the test time and weakened triggers in the training data, will the asymmetric triggers become easier to detect in the test time? \n4. It would be great to increase the font size in the figures (Figure 4 and 5). \n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed attack is novel. The code is provided in the submission for reproducibility.",
            "summary_of_the_review": "The paper investigates a pervasive assumption in many backdoor defenses -  the defenses aim to identify the separation of latent features in backdoor attacks. The proposed adaptive backdoor poisoning attacks are well-motivated and effective in countering the defenses. However, the scope of the work is limited. The paper does not consider other defenses that are not based on the latent separation assumption.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3230/Reviewer_sY26"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3230/Reviewer_sY26"
        ]
    },
    {
        "id": "qqbKk_5O_q",
        "original": null,
        "number": 3,
        "cdate": 1666670882075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670882075,
        "tmdate": 1666670882075,
        "tddate": null,
        "forum": "_wSHsgrVali",
        "replyto": "_wSHsgrVali",
        "invitation": "ICLR.cc/2023/Conference/Paper3230/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Motivated by the observation that backdoor attacks tend to cluster in the latent space of neural networks, this paper proposes a set of adaptive attacks whose poison samples are indistinguishable (in terms of representation) from clean samples.",
            "strength_and_weaknesses": "Strengths:\n\n- The approach is well-motivated and the results are convincing and thorough. The paper evaluates against state-of-the-art latent separability attacks and succeeds against all of them.\n- The writing and motivation is clear.\n- The authors conduct ablation studies to pinpoint the sources of indistinguishability.\n\nWeaknesses:\n\n- I am a bit unclear on the difference between the Adaptive-Blend attack and the m-way attack from Xie et al (https://openreview.net/forum?id=rkgyS0VFvr) \n- Figure 4 suggests that the backdoor attack may not be truly \"indistinguishable\" in the sense that knowing which samples are poisoned in advance allows one to train a somewhat reliable classifier for finding the backdoor samples. Is there any formal sense in which the samples are indistinguishable (beyond just circumventing latent separability-based attacks?)\n- A relevant baseline might be to use a method like Tan et al (I think cited in this paper as Shokri et al) https://arxiv.org/abs/1905.13409 but using a separate training algorithm to the one used by the defender (this would be something like a \"substitute model attack\" in adversarial robustness).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clear and of high quality. I am not an expert on backdoor attacks and thus I cannot judge the novelty and originality (the only similar attack I have seen is the m-way attack cited above, but it's possible there are more attacks aimed at circumventing latent separability that I'm not aware of).",
            "summary_of_the_review": "This paper proposes a set of backdoor attacks that circumvent the latent separability assumption. The corresponding backdoor attack circumvent a variety of latent separability-based assumptions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3230/Reviewer_4n1q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3230/Reviewer_4n1q"
        ]
    },
    {
        "id": "wYICw6-F_V",
        "original": null,
        "number": 4,
        "cdate": 1667662497035,
        "mdate": 1667662497035,
        "ddate": null,
        "tcdate": 1667662497035,
        "tmdate": 1667662497035,
        "tddate": null,
        "forum": "_wSHsgrVali",
        "replyto": "_wSHsgrVali",
        "invitation": "ICLR.cc/2023/Conference/Paper3230/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper investigates the latent separability of the existing backdoor attacks, which makes them easily caught by existing defenses that look at the latent space. The paper then crafts an attack using data poisoning without latent separability. The proposed adaptive backdoor attack method uses regularized samples (samples with triggers but correct labels) and asymmetric triggers that have different intensities in training and inference. The proposed attack is shown to escape existing latent-space defenses on CIFAR10, GTSRB, and 10-class subset of Imagenet.",
            "strength_and_weaknesses": "The motivation of the paper is well-described. The paper extends the work in some backdoor attack methods to data poisoning, which is a different threat model. This could be a valuable contribution to the backdoor research community. However, I think that there are some major issues in the paper, especially in its similarity to an existing work in the backdoor domain:\n\n- One of the 2 main parts (regularization samples) of the proposed method exhibits a close similarity to Peng et al's work. This idea has been already explored and studied extensively before. Peng et al. also reduce the strength of the association between the trigger and the target using the exact same method, but with a more extensive theoretical analysis. \n- While the proposed method is effective, I think that the experiments also show that other existing backdoor attacks are also pretty effective against most latent-space defenses. However, the paper does not show the results of the existing methods on other studied datasets, besides only CIFAR10.\n- The paper should include evaluations against other defense methods. This is a new type of attack, thus it should be validated not only against latent-space defenses but also other types of defenses, or at least include any insight on whether the method can bypass other defenses. Making it pass latent-space defenses may make it more detectable in other defenses.\n\nsome minor grammatical errors: \"while keep\" (threat model)\n\nPeng et al. Label-Smoothed Backdoor Attack. https://arxiv.org/abs/2202.11203",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The the work can be reproduced given the description and the code.",
            "summary_of_the_review": "The paper has made some important contributions but its similarity to some existing work makes it less novel. Therefore, it is not strong enough for an ICLR acceptance recommendation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3230/Reviewer_XjVu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3230/Reviewer_XjVu"
        ]
    }
]