[
    {
        "id": "-WK1WGfRGqL",
        "original": null,
        "number": 1,
        "cdate": 1665992412480,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665992412480,
        "tmdate": 1670357599324,
        "tddate": null,
        "forum": "9Q7wZ0Uq4Z6",
        "replyto": "9Q7wZ0Uq4Z6",
        "invitation": "ICLR.cc/2023/Conference/Paper1957/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors compare finetuning and freezing pretrained representations on various RL tasks.\nThey seek to understand under what conditions each of these can be effective, and when one should be preferred to the other.\nOn a simple task where the optimal policy is known, they show that the pretrained representation does contain enough information to regress the optimal actions, but finetuning still fails to recover the optimal policy in spite of this.\nThey use insights from these experiments to propose a new self-supervised objective for RL, and evaluate this objective on a variety of tasks.\n",
            "strength_and_weaknesses": "Strengths:\n1. The proposed objective, PiSCO, seems elegant and promising. I am curious if the authors could say more about the projector $h(\\cdot)$ that is used in the PiSCO loss. The intuition for PiSCO makes sense, but I am not sure why why should compare $\\pi(\\cdot|h_1)$ and $\\pi(\\cdot|z_2)$ instead of simply $\\pi(\\cdot|z_1)$ and $\\pi(\\cdot|z_2)$?\n2. The paper is well-written, and the authors attempt to be insightful in their discussion.\n\nWeaknesses:\n1. The narrative of the paper feels a bit incoherent. It feels to me that the freezing vs finetuning investigation did not well-motivate PiSCO, and that PiSCO does not address the original problem statement that motivated said investigation. From what I can tell, PiSCO is an auxiliary loss to be used during RL, not a pretraining objective. I think this paper's contributions would be more clear if either: (a) the authors proposed a pretraining objective, or (b) the authors more directly motivated PiSCO, e.g. by discussing robustness or data augmentation/efficiency.\n2. Related to the previous point, if the authors were to consider option (b), then I feel that PiSCO could benefit from a more thorough experimental evaluation (a greater variety of tasks/domains, with more emphasis paid to sample efficiency).\n3. The authors make broad statements about finetuning vs freezing solely based on results from a toy environment, MSR jump. I understand the attractiveness of that domain (in that the optimal policy is known), but I'm wondering if it would be possible to conduct a similar study in other domains as well?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the writing is easy to follow, but the high-level clarity is a bit lacking (see the \"Weaknesses\" section).\n\nNovelty: to the best of my knowledge, the contributions are novel.\n\nReproducibility: the authors provide both algorithm pseudocode and their actual code in the supplementary material. The paper contains many experimental details.\n",
            "summary_of_the_review": "I think that the proposed approach shows promise, but the motivation is unclear (mainly per Weaknesses #1). I would feel comfortable accepting this paper if this were addressed (and the experiments/evaluation may need to change accordingly).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1957/Reviewer_b3ER"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1957/Reviewer_b3ER"
        ]
    },
    {
        "id": "h_HpILOU1E-",
        "original": null,
        "number": 2,
        "cdate": 1666564279927,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666564279927,
        "tmdate": 1670335193549,
        "tddate": null,
        "forum": "9Q7wZ0Uq4Z6",
        "replyto": "9Q7wZ0Uq4Z6",
        "invitation": "ICLR.cc/2023/Conference/Paper1957/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies transfer learning in the context of Reinforcement Learning. Using 3 diverse domains as a testbed (a toy platform game, some tasks from the DeepMind Control Suite, and some navigation tasks from habitat), it makes the following contributions:\n- Contribution #1: Shows that when transferring representations, the best strategy involves freezing the deepest layers and fine-tuning the others\n- Contribution #2: Provides a strategy to select which layers to freeze and which to finetune\n- Contribution #3: Introduces an auxiliary self-supervised objective to further enhance the transfer performance.\n\n\n",
            "strength_and_weaknesses": "All 3 contributions outlined above are more or less independent from each other, as such this section will be divided into three parts discussing the strength and weaknesses of each of them.\n\n## Contribution 1\n\nThe paper starts by analyzing the performance of two main transfer strategies, namely freezing the representations or finetuning them. This is compared to the baseline of training the representations from scratch on the target task (strategy nicknamed \"de novo\"). In a nutshell, the results can be summarized as follows:\n- For the toy jump task as well as the Deepmind control suite, finetuning outperforms \"de novo\" while freezing representations substantially decreases performance\n- For the Habitat tasks, both strategies outperform \"de novo\" but none seems strictly better than the other.\n\nThe paper provides a thorough and solid analysis of the failure of the freezing of the representations for the jump task, linking it to robustness issues.\nHowever, in my opinion, this falls short of making a broader conclusion: the results above suggest a meaningful qualitative difference between the Habitat task and the other ones (in Habitat, freezing works while it doesn't in the others). The reason for this difference is not discussed in the paper, yet it seems to be somewhat at the crux of the matter. Does this difference comes from intrinsic characteristics of the environments or from other factors?\nAs for the other factors, we can list 2 main ones: \n- Network capacity/architecture: The jump task uses the original DQN network, which is rather primitive (no skip connections for example) and has a tiny capacity (around 100k trainable parameters). By contrast, the Habitat task uses a state-of-the art CNN, with 28M parameters, ie two orders of magnitude larger.\n- Pre-training tasks: The convnext for Habitat is pre-trained on ImageNet, while in the other experiments the backbones are pretrained on related control tasks.\n\nIn a nutshell, the networks used for Habitat have received a much more varied pre-training and have a much higher capacity. It is well known in the vision community (see [1] for example) that both these properties improve robustness and transferability of the features, making the results presented in the paper somewhat unsurprising.\n\nOverall, it is not clear to me if the \"frozen\" transfer strategy would still fail in the two tasks where it currently does, if the same convnext, with the same pretraining, was used as the feature extractor. Depending on the answer to that question, a slightly different conclusion could be made: similarly to trends in other domains (Vision, NLP, speech, ...) does RL benefit from larger models pre-trained on large, general datasources?\n\nFinally, I believe the paper could discuss a bit the alternate method to full freezing and finetuning that is being privileged more and more both for vision and NLP: freezing the pre-trained weights but adding a small amount of trainable parameters to allow \"adpatation\" of the learned representation to the new task. See [2], [3] and references therein.\nAnother thread of work of interest is the idea of using a different learning rate per layer (typically lower for the deepest layers and increasing from there). See [4] for an example.\n\n\n## Contribution 2\n\nThe paper proposes to resort to linear probing of the feature of each intermediate layer to determine where the cut-off between frozen and fine-tuned should be made.\nWhile the method seems sound, several design choices are not really explained:\n- What is the justification behind reducing the dimensionality with PCA first, as opposed to simply using a linear layer directly on the features?\n- The paper states that the probing is using the output of a action-value function network. Would it be possible to alleviate the need to train such a model by simply predicting an empirical estimate (eg the mean) of the expected discounted return in each state?\n\n\nFigure 4.b (and similar figures in the appendix) would benefit from error bars since it's not clear what the variance is. Why does Fig. 11 not contain the estimates for the fine-tuned features?\n\nFinally, the main weakness of the approach is its computational cost. Section 5.1 states \"For each layer, we measure the action value estimation error of pretrained and finetuned representations, and only freeze the first pretrained layers that closely match the finetuned one\". This implies access to the said fine-tuned representations, which in-turn implies that a full RL training is required as a first step. As such, this doubles the cost of the method (both in wall-clock time as well as sample complexity), significantly reducing its practicality. For proper comparison, it would be necessary to see whether the \"Finetuned\" baseline from figure 5 is still lower than the \"Frozen+Finetuned\" if the former is trained for twice the number of steps.\nPerhaps, a \"softer\" scheme akin to [4] would do the job equally well while side-stepping the computational cost entirely.\n\n## Contribution 3\n\nThe last contribution is a self-supervised auxiliary objective aiming at improving the quality of the representation. Though it is inspired from findings from contribution #1, it is a contribution that is not specific to the transfer setting and could potentially be applied elsewhere.\n\nHere are some questions and comments related to this contribution:\n- What is the motivation for the projection layer in the SSL loss? The choice is not ablated in the paper. I understand the need for such a layer in methods that do direct embedding supervision (either contrastive loss or simple L2 loss for example), but in this case the loss directly operates on the output distribution. Similar vision method like DINO [5] don't use such projector\n- It would be valuable to provide \"De Novo + Pisco\" as a baseline to disentangle the gains from the transfer scheme from those due to the additional objective\n- Though comparison to CURL is provided, SPR [6] has been shown to outperform it significantly, so a comparison seems warranted.\n\n\n\n\n[1] \"Big Transfer (BiT): General Visual Representation Learning\", Kolesnikov et al, ECCV 2020\n\n[2] \"AdapterHub: A Framework for Adapting Transformers\", Pfeiffer et al, EMNLP 2020\n\n[3] \"Learning multiple visual domains with residual adapters\", Rebuffi et al, Neurips 2017\n\n[4] \"Universal Language Model Fine-tuning for Text Classification\" Howard et al, ACL 2018\n\n[5] \"Emerging Properties in Self-Supervised Vision Transformers\", Caron et al, ICCV 2021\n\n[6] \"Data-Efficient Reinforcement Learning with Self-Predictive Representations\", Schwarzer et al, ICLR 2021\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written and easy to read.\nI believe most technical details required to allow reproduction are provided, with one exception: I could not find specifications on the data augmentation used.\n\n",
            "summary_of_the_review": "My main critiques for each of the contributions are as follows:\n\n- The analysis of finetuning vs freezing representations misses the discussion on the capacity and pre-training diversity of the network involved, which in my opinion makes the picture incomplete\n- The proposed strategy to choose which layers to freeze seems too costly, in its current form, to be broadly applicable\n- The auxiliary objective is not studied in depth, be it with respect to its own design choices (eg the projection layer) or with respect to current state of the art auxiliary objectives (eg SPR)\n\nAs such, I don't recommend the paper in its current form for publication. That being said, all the aforementioned limitations seem fixable, and I am willing to increase my score if they are.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1957/Reviewer_nmYq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1957/Reviewer_nmYq"
        ]
    },
    {
        "id": "nWpsK8vSBOb",
        "original": null,
        "number": 3,
        "cdate": 1666823798453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666823798453,
        "tmdate": 1666823798453,
        "tddate": null,
        "forum": "9Q7wZ0Uq4Z6",
        "replyto": "9Q7wZ0Uq4Z6",
        "invitation": "ICLR.cc/2023/Conference/Paper1957/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies fine-tuning vs freezing in the context of reinforcement learning. The authors argue that the layers that represent general-purpose features(mainly the early layers) could be frozen while those that represent task-specific features(mainly the last layers) should be fine-tuned to achieve higher performance in the downstream task.",
            "strength_and_weaknesses": "Strength:\n\n- The paper is well-written and easy to follow.\n\nWeaknesses:\n\n- I am not an expert in reinforcement learning. However, the argument in the paper in terms of representation learning does not seem convincing and sufficient as an ICLR paper to me.  First, the author's argument about freezing vs finetuning in the introduction(namely the second and third paragraphs) holds for any neural network and downstream task. Posing it specifically as an RL problem does not seem rigorous to me.  Second, as it is mentioned by the authors themselves, the fact that the early layers of a convent capture general-purpose features while the later layers represent more task-specific features, is known and widely accepted. In fact, the pioneering work on fine-tuning such as Fast-RCNN achieves higher performance when the early layers are frozen, and the last layers are fine-tuned.\n\nI can not evaluate the performance, but  I am not entirely convinced about the sufficiency of the contributions in. this paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written. However, suffers from a lack of novelty as I discuss above.",
            "summary_of_the_review": "In my opinion, this is a well-written paper that suffers from a lack of novelty and convincing arguments. Please see the above for more detailed feedback.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1957/Reviewer_ZmQR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1957/Reviewer_ZmQR"
        ]
    }
]