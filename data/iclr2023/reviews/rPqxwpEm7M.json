[
    {
        "id": "SI5qyS1E2d7",
        "original": null,
        "number": 1,
        "cdate": 1666715228987,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666715228987,
        "tmdate": 1670877977221,
        "tddate": null,
        "forum": "rPqxwpEm7M",
        "replyto": "rPqxwpEm7M",
        "invitation": "ICLR.cc/2023/Conference/Paper2430/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on recognizing actions based on the motion information in the videos. The proposed idea (Dense Correlation Fields, DCF) which can be applied to different CNN backbones, computes frame to frame similarity to find spatial-temporal correlation fields. ",
            "strength_and_weaknesses": "- DCF is a simple and novel idea that outperforms SOTA by focusing on motion in videos.\n- Using spatial pyramid correlation feature \n- DCF is stand-alone and can be applied to different backbones. ",
            "clarity,_quality,_novelty_and_reproducibility": "The implementation of the method is explained in detail and is clear to reader. \nThe paper is in good quality shape and also the idea seems novel. I am sure it is also easy to implement and reproduce this approach. ",
            "summary_of_the_review": "The paper is pretty easy to follow and approaches the video action recognition from motion modeling point of view (which is not new, a lot of papers have leveraged differently from motion, either using Optical Flow or a different representation of the motion). I like this paper because it proposes a module that helps the network learn from motion at different scales (compared to SOTA which uses most likely a fixed representation of motion). It is a simple idea that works well.\nMy only concern about this method would be its computational time complexity and I would like to see a section on that to see how it performs comparing other approaches (specially since it captures motion at different stages in the network, my guess is it might make it computationally expensive). \nAlso, it would be nice to cite more related papers that took into account the motion modeling and representation, there are a lot, but here are some examples: \n- V. Choutas, P. Weinzaepfel, J. Revaud, and C. Schmid. Potion: Pose motion representation for action recognition. In CVPR 2018, 2018.\n- S. Sun, Z. Kuang, L. Sheng, W. Ouyang, and W. Zhang. Optical flow guided feature: a fast and robust motion represen- tation for video action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n- J. Jiang, Y. Cao, L. Song, S. Z. Y. Li, Z. Xu, Q. Wu, C. Gan,\nC. Zhang, and G. Yu. Human centric spatio-temporal action\nlocalization.\n- L.Fan,W.Huang,S.E.ChuangGan,B.Gong,andJ.Huang.\nEnd-to-end learning of motion representation for video understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6016\u2013 6025, 2018.\n- Asghari-Esfeden, S., Sznaier, M. and Camps, O., 2020. Dynamic motion representation for human action recognition. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 557-566).\n- C.-Y. Wu, M. Zaheer, H. Hu, R. Manmatha, A. J. Smola, andP.Kra \u0308henbu \u0308hl. Compressedvideoactionrecognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6026\u20136035, 2018",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2430/Reviewer_bPjE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2430/Reviewer_bPjE"
        ]
    },
    {
        "id": "6Bh0go2hRCu",
        "original": null,
        "number": 2,
        "cdate": 1666754199570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666754199570,
        "tmdate": 1666754199570,
        "tddate": null,
        "forum": "rPqxwpEm7M",
        "replyto": "rPqxwpEm7M",
        "invitation": "ICLR.cc/2023/Conference/Paper2430/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new module, termed Dense Correlation Fields (DCF), to model motion information at feature level within neural networks. DCF involves a short-term module (i.e., correlation between adjacent frames), a long-term module (i.e., correlation between bidirectional and consecutive frames) and a spatial pyramid design (i.e. spatially downsampling correlation features for the next network stage). DCF can be applied to different backbone architectures and improves the baseline model on temporal modeling and action recognition. The paper conducts experiments on three common video action recognition datasets and provide ablation and visualization to analyze the impact of the DCF module.",
            "strength_and_weaknesses": "Weakness\n1. My major concern is the limited technical novelty and contribution of the paper. Most designs in DCF have considerable overlap with existing works: For example, the idea of using correlation at the feature level has been extensively studied in [1,2,3] and the idea of exploiting long-term motion and motion hierarchy has been discussed in [3]. The paper also doesn't discuss and compare with some important recent work on motion modeling, such as [2, 3, 4, 5]. \n\n2. The experiments in the paper is not convincing and the overall performance of DCF is not strong enough. First of all, many related works on motion model [2,3,4,5] are not compared in the paper, and many recent works on action recognition are not included. The visualization of learned motion patterns in Fig. 4 seem less appealing as those in [2, 3] as well.\n\n\n[1] Wang, Heng, et al. \"Video modeling with correlation networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n[2] Kwon, Heeseung, et al. \"Motionsqueeze: Neural motion feature learning for video understanding.\" European conference on computer vision. Springer, Cham, 2020.\n[3] Yang, Xitong, et al. \"Hierarchical contrastive motion learning for video action recognition.\" BMVC 2021.\n[4] Piergiovanni, A. J., and Michael S. Ryoo. \"Representation flow for action recognition.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n[5] Diba, Ali, et al. \"Dynamonet: Dynamic action and motion network.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "The introduction of the proposed method can be further improved. I'd suggest to add one or two equations in Sec. 3.2 to make the description of long-term correlation and hierarchy aggregation more precise and clear.\n\nAs discussed in Weakness, the originality of the work is limited, and the paper doesn't sufficiently discuss how it is distinguished from the existing works on motion modeling.",
            "summary_of_the_review": "The paper proposes a new module, DCF, for modeling motion information at the feature level. However, the technical novelty and contribution of the proposed method is not sufficient, and the experiments in the paper is not convincing enough. Therefore, I'd suggest \"weak reject\" to the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2430/Reviewer_NeMJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2430/Reviewer_NeMJ"
        ]
    },
    {
        "id": "m922Yc891r",
        "original": null,
        "number": 3,
        "cdate": 1666938621176,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666938621176,
        "tmdate": 1666999353929,
        "tddate": null,
        "forum": "rPqxwpEm7M",
        "replyto": "rPqxwpEm7M",
        "invitation": "ICLR.cc/2023/Conference/Paper2430/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The paper proposed dense correlation fields for action recognition. Based on CorrNet, the paper proposed to model temporal long-term correlation by computing correlation for frames that are far apart. The paper also designs a spatial hierarchical architecture to aggregate the information at different granularities. The paper presents results on the something-something V1 & V2, and Kinetics-400 dataset. The proposed method shows superior results than the original CorrNet.\n",
            "strength_and_weaknesses": "The following are more detailed comments and questions regarding the paper:\n1, In Section 3.2, the paper suggests computing bi-directional correlation, L forward frames and L backward frames. This will lead to the correlation being computed multiple times for a given pair of frames. This may lead to a waste of computational cost. How does the paper handle this problem?\n \n2, How to do padding for frames at the boundary of the sampled clip? Since now we are considering L frames, the padding may cause issues for the computation of correlation.\n \n3, Figure 2 and Figure 3 (c) are not very illustrative.  I suggest explaining what exactly each operator is, and what are the sizes of input tensor and output tensor.\n \n4, In section 4.1, how to use R(2+1)D with ResNet pretrained with ImageNet? Since R(2+1)D has separate spatial and temporal filters, how to initialize the filters with ImageNet pretrained ResNet?\n \n5, In section 3.3, the paper suggests that L is set to L=2 and L=1 for the last stage. Will L=2 allow the model to long-term temporal information? It seems pretty short.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2430/Reviewer_hBxH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2430/Reviewer_hBxH"
        ]
    },
    {
        "id": "AFQLT4K5lSU",
        "original": null,
        "number": 4,
        "cdate": 1667529478490,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667529478490,
        "tmdate": 1670794814070,
        "tddate": null,
        "forum": "rPqxwpEm7M",
        "replyto": "rPqxwpEm7M",
        "invitation": "ICLR.cc/2023/Conference/Paper2430/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes to use Dense Correlation Fields which is a pyramid of correlation volume of visual features. The correlation takes into account long-term correlation, and high-level semantically correlation feature. The method shows improvement over 2D CNN and 3D CNN baselines by a healthy margin.",
            "strength_and_weaknesses": "### Pros\n1. The idea is simple and the paper describes it in an organized way.\n\n2. Nice visualization on correlation pyramid. It is an interesting finding that higher-level feature (low-resolution) has more semantically stong correlation.\n\n\n### Cons\n1. The idea of using cross-frame correlation is not new. The authors are suggested to discuss more papers which either construct a cost volume [a] or apply multiplicative interactions [b].\n\n    [a] Zhao, et al. Recognize Actions by Disentangling Components of Dynamics. CVPR 2018\n\n    [b] Wang, et al. Appearance-and-Relation Networks for Video Classification. CVPR 2018\n\n2. The spatial correlation hierarchy is realized by \"simply downsample\"-ing. Have you tried other ways such as (1) conducting correlation operation on higher-level (lower spatial resolution) appearance feature map, or (2) a downsample version of the same-level feature map (i.e. downsample -> correlation instead of correlation -> downsample), or (3) conducting correlation on feature map with larger stride?\n\n3. The baselines to compare with are weak and outdated. \n\n    (1) Worse tradeoff than SlowFast. DCF w/ ResNet-50 is higher than SlowFast (R-50) by 0.4% but with 30% more GFLOPs. If the authors show an accuracy-GFLOPs tradeoff curve, I don't think DCF would have a comparable trade-off on par with with SlowFast compared to Slow-only.\n\n    (2) There lacks a comparison to recent Transformer-based video models which remarkably boost the performance in the late few years. Also, I think the correlation operator is conceptually similar to the dot product operation in the self-attention. It makes me wonder if self-attention is a better way to capture such multiplicative relations and the gain of DCF compared to plain 2D/3D ConvNets might be simply diminishing.",
            "clarity,_quality,_novelty_and_reproducibility": "+ Clarity: The paper is well written.\n\n+ Quality: good performance compared to baseline but the baselines are somewhat weak (see weakness 3).\n\n+ Novelty: Not novel enough. See Weakness 1. \n\n+ Reproducibility: According to the appendix, I think the results should be reproducible.",
            "summary_of_the_review": "Considering the limited novelty and the fact that improvements are based on a somewhat weak baseline, I don't think the paper meets the bar of ICLR.\n\n\n===\n\nSee the latest comment for the final rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2430/Reviewer_SoQA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2430/Reviewer_SoQA"
        ]
    }
]