[
    {
        "id": "JS7BY2WxNB",
        "original": null,
        "number": 1,
        "cdate": 1666632420280,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632420280,
        "tmdate": 1666632420280,
        "tddate": null,
        "forum": "uHrJ1AY1xR1",
        "replyto": "uHrJ1AY1xR1",
        "invitation": "ICLR.cc/2023/Conference/Paper802/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new method for solving distributional RL problems. The main idea behind the paper is to use the Sinkhorn distance (wasserstein with entropic regularization) as the basis for the distributional updates. Several theoretical results are proven concerning the contractive property of the Sinkhorn update as well as equivalences with other updates in certain settings. Extensive experimental results are presented with comparison to existing distributional RL algorithms",
            "strength_and_weaknesses": "Strengths:\n- The update rule appears to be novel for distributional RL; however, it is already known that you can use any distance. This is just one particular example that has also seen applications and success in other areas.\n- The experiments are fairly extensive, evaluating the algorithm against existing DRL methods across 50+ Atari tasks\n- The theory seems reasonable, but I didn\u2019t check carefully.\n\nWeaknesses:\n- Although probably new in application, the algorithmic idea is not particularly novel, so the results should probably be judged more on significance of improvement and analytical insights.\n\n- The clarity could be greatly improved. There are numerous inconsistencies, typos, undefined notation, etc. See specifics below.\n\n- The novel part of Theorem 1 (c) is not actually a contraction in the usual sense as it depends on the iterates. It\u2019s not clear that this is truly significant and actually yields the desired convergence.\n\n- The rates in Table 1 are not proven (there is no theorem) or discussed or interpreted. This is especially important since the first one is convoluted.\n\n- The empirical results do not appear to yield that significant of an improvement over existing algorithms. But there is improvement in some domains.\n\n- There are claims made in the experiments section that don\u2019t appear to be supported by the results. It says \u201cSinkhornDRL achieves better performance across more than half\u201d but it looks like it is the opposite. It\u2019s really only half the games for (a) and less than half for (b). Am I just reading the plot backwards? Blue (negative) is worse for Sinkhorn, no? \n\n\u201cthe performance of SinkhornDRL tends to QR-DQN or MMDDRL on Seaquest when we decrease or increase epsilon.\u201d This doesn\u2019t seem to be supported by the plots. The performance decreases, but it\u2019s not at all clear that it\u2019s leveling out at the performance of the other algorithms.\n\n\n\nQuestions:\n- Why was DQN not included in the ratio plots?\n\n- In Fig 3 (b), since the performance doesn\u2019t seem to be effected much by N, why not choose N to be very small such as N = 1, 2? Theoretically it doesn\u2019t make much sense, but it seems it would be useful to see when performance finally drops off.\n- \u201cSample complexity\u201d in Table 1 is never defined. Sample complexity to achieve what? What is the objective? Sub-optimality in distribution, in expectation, etc? \n",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above, the clarity could be improved.\n- There are typos throughout such as \u201cSinkhron,\u201d \u201csuit,\u201d etc.\n- The distr bellman operator should really not be an =. It should be distributionally equal (this should be apparent from the notation, not just the text).\n- theta and theta_* are not defined in Sec 3.1 or Algorithm 1.\n- It\u2019s only described very informally how the Z_{\\theta} are represented.\n-  (5) and (6) should not have k in Q and Z.\n- Generally there are just arbitrary mixtures of treating the $Z$ (and related quantities) as either random variables or as distributions.\n- The actual algorithm is never explicitly stated. Just the update rule is given. It\u2019s unclear how the data is collected and used in the update rule. It\u2019s never defined what the policy should do.\n- Algorithm 1 returns a distance. It\u2019s unclear what one is supposed to do with this.\n- The constraints in (13) seem to be inaccurate. I think they should sum to the input distributions.\n- Proposition 1 \u201ccan be equivalent to.\u201d Is it only sometimes equivalent or always? If sometimes, under what conditions? I think this should be made more precise. \n",
            "summary_of_the_review": "In summary, this paper presents an interesting idea and does an interesting theoretical analysis and extensive experimental evaluation. However, it seems to fall short in a few categories as described in the above sections, with several unanswered questions. The lack of clarity is also a drawback.\n\nOther notes:\n- More exact convergence results exist for the general sinkhorn algorithm: \u201cNear-linear time approximation algorithms for optimal transport via Sinkhorn iteration\u201d\n- I encourage the authors to also consider more in depth analysis of experimental results as prescribed by the following paper considering only 3 seeds are used: \"Deep Reinforcement Learning at the Edge of the Statistical Precipice\"\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper802/Reviewer_tLLY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper802/Reviewer_tLLY"
        ]
    },
    {
        "id": "-3fjfF-I-D",
        "original": null,
        "number": 2,
        "cdate": 1666675302289,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675302289,
        "tmdate": 1666675302289,
        "tddate": null,
        "forum": "uHrJ1AY1xR1",
        "replyto": "uHrJ1AY1xR1",
        "invitation": "ICLR.cc/2023/Conference/Paper802/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the sinkhorn algorithm for optimal transport to learn the distributional Q-estimate via a bootstrapped wasserstein distance between distributions. It builds on top of existing distributional off-policy model-free RL methods which estimate a distribution over the discounted sum of rewards ahead and leverage it for policy learning. Additionally, the paper proposes a theoretical framework wherein their method interpolates between the wasserstein distance and MMD between distributions. ",
            "strength_and_weaknesses": "Strengths:\n- The paper provides a nice theoretical framework for distributional RL via sinkhorn iterations and how the optimal transport problem it solves in distributional RL interpolates between Wasserstein and MMD distances, both of which are appealing for their contractive divergences in Bellman equations. Furthermore, the free choice of cost function is an interesting one which allows more flexibility than the contractive requirement for divergences needed for conventional distributional RL. It is shown that sinkhorn algorithm using Gaussian kernels is equivalent to regularized MMD. \n- There is an implementation to utilize the sinkhorn algorithm for distributional value-distribution learning in the RL setting, which requires Sinkhorn to be differentiable and efficiently computed.\n- For several tasks, the proposed algorithm significantly outperforms a state-of-the-art distributional RL algorithm in MMDRL, which is impressive.\n\nWeaknesses:\n- The advantages of OT vs MMD is known to be mostly taking advantage of the data geometry and the geometry of the flat distance measures when following the gradient of the divergences. However, it is unclear to me whether this advantage is applicable to the setting of estimating a 1-dimensional value-distribution. Furthermore, OT suffers from high computational cost, with the same number of design choices required to tune (cost function replacing kernel choice) as MMD. \n- Along the lines of the above, it is unclear whether the benefits of this algorithm outweigh its implementation difficulty along with overall complexity. The results for which MMDRL is superior seems to indicate that for a majority of Atari games, this does not do much better\n- It would be have interesting to see if these results generalize to harder tasks either in discrete control or even continuous control. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and the proposed algorithm is novel and high quality. I am unsure about the reproducibility, in particular because the OT requires a high implementation complexity, but from a theoretical perspective, it should be reproducible given how the proposed algorithm only replaces the divergence used in distributional RL.",
            "summary_of_the_review": "Overall, while this is a nice applied paper, the Sinkhorn algorithm used in this context requires further study, in particular why it would work well for certain tasks and not others. I would recommend rejecting the paper in its current state but would increase my score if more justification could be provided for how beneficial this approach is vs existing ones for other tasks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper802/Reviewer_LQMW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper802/Reviewer_LQMW"
        ]
    },
    {
        "id": "010OJ8erPm8",
        "original": null,
        "number": 3,
        "cdate": 1667482301782,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667482301782,
        "tmdate": 1667482301782,
        "tddate": null,
        "forum": "uHrJ1AY1xR1",
        "replyto": "uHrJ1AY1xR1",
        "invitation": "ICLR.cc/2023/Conference/Paper802/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new distributional RL algorithm named SinkhornDRL, which proposes to solve the distribution matching problem in distributional RL with Sinkhorn divergence. \n\nSinkhorn divergence in formulation is equivalent to an entropy-regularized version of MMD, and it could be featured as an interpolation between Wassersteon distance  and MMD, so that it simultaneously leverages the geometry of Wasserstein distance and unbiased gradient estimate property of MMD in moment matching.  SinkhornDRL is highly related to MMDDRL, as they are both moment matching approaches that use samples to approximate the distribution associated with kernel tricks. \n\nThe main new theoretical insight introduced in this paper is to prove the contraction property of SinkhornDRL in a short section from Appendix (Equation 32 in Appendix 3.4).  The other proof on the statistical properties of Sinkhorn, such as its equivalence form to regularized MMD, is not novel. \n\nFor empirical comparison, the authors compare SinkhornDRL with DQN, C51, QRDRN and MMDDQN on their modified 55 Atari games benchmark, where they evaluate the agents under 10M frame regime. The results show that SinkhornDQN could outperform MMDDQN occasionally and overall results in inferior median and median HNS than MMDDQN. \n\n",
            "strength_and_weaknesses": "**Strength**\n* Sinkhorn divergence has not been formally considered for distributional learning in the existing distributional RL literature.\n* The paper is clearly written and easy to understand.\n\n**Weaknesses**\n\n[Motivation] \n* Overall I feel this work is not well motivated from distributional RL perspective. The motivations  introduced for the work are mainly related to: (1) introducing a new distributional RL algorithm  (2) SinkhornDRL bridges between Wasserstein distance and MMD. It remains unclear what challenge in distributional RL could be  tackled by Sinkhorn, and why bridging the two major types of distributional attempts could be more beneficial than employing each single one. (I think motivating the RL perspective from the entropy-regularization might also be valid but there is little statement of this type.)\n* The related works discussed/compared in this paper are not inclusive enough. In the sections before methodology, I only see two major distributional RL methods, QR-DRL and MDDRL,  introduced in details, while it would be unfair to describe the literature through only two works. In some important part in methodology part. the authors also compare SinkhornDRL with a limited subset of distributional RL baselines, e.g., the complexity and convergence rate shown in Table 1. I refer the authors to the related works presented in the MDDRL paper, where there is a more inclusive and informative categorization/description on related works.\n\n\n[Formulation] \n\n* The overall SinkhornDRL solution comes with limited novelty because the algorithmic approach of solving moment matching with unrestricted statistics of deterministic samples highly depends on the existing work MMDDRL. The main change of SinkhornDRL from  MMDDRL is very small, i.e., to minimize a regrarized squared MMD, while the major idea like applying unrestricted statistics and approximating the distance with samples of measures, are the same.  \n*  The statistical relationship between Sinkhorn and MMD is not newly developed, since it's well adopted from existing statistical literature. \n* The authors make rather strong claim on the theoretical contribution of this paper, but I feel it is not very convincing as the only new proof is the proof of contraction theorem only, which itself is not very complicated. Furthermore, considering the limited algorithmic change has not been well justified by the empirical evaluation, I feel the significance of the proposed method is not enough for being published in ICLR conference.\n\n[Empirical Evaluation] \n\n* The authors only adopt a single experimental domain Atari 2600, but they could not align the well adopted Atari 2600 setting with the other baselines which all adopt the same domain, e.g., C51, QRDQN and even MMD. The authors only adopt a much smaller frame number, which they claim is **10M**, 1/20 from the most widely adopted standard **200M**.  I understand the computational resource required for Atari 2600 is a lot, but if not accessible to such resources, should turn to alternative domains which have lots of candidates (e.g., Mujoco and ViZDoon), as well as proof-of-the-concept toy domains. To me, lack of the computational budget is not a valid excuse for using such small frame budget and make the setting biased from its most related baselines.\n\n* There is one important issue the experiment, which is the **10M** frame budget the authors claim they use. $\\textcolor{red}{\\mbox{I suspect the authors actually use a much larger number than 10M}}$. The authors never mention *frame repeat* or  *frame stack* in their paper, so based on the well-adopted standard, **10M** should correspond to the total number of frames, not that number before multiplying frame repeat. So it's 1/20 to C51, QRDQN, MMDDQN and many other classic DRL methods like IMPALA, RAINBOW and MUZERO. It's important to make that much progress within 10M only. For example, with Breakout, SinkhornDRL uses less than 5M frames to score > 350, but that progress in C51 paper's Fig 3, QRDQN paper's Fig 6 and even Rainbow's Fig 5, it takes way more than 10M frames for Breakout agent to progress to that standard. [Highly recommend reviewers and AC to take a look into aforementioned papers]. \n\n* I think the reported median/mean HNS scores for Sinkhorn as well as its baselines (in Table 2) are not responsible. I have computed the median HNS for C51 and QR-DQN from a very reliable  open-source experiment logs released in a public repository dqn-zoo. Within 10M frame, the median HNS for both methods are bellow 50%, and within 10M * 4 repeat (assume they forget to multiply frame repeat),  the scores for both methods > 100%. For neither case, the standards of median HNS aligns with the reported numbers. Therefore, I'm not convinced if SinkhornDRL could be identified as an effective distributional RL method. $\\textcolor{red}{\\mbox{Even their reported numbers for Sinkhorn underperform MMDQN in both median and mean HNS metrics. }}$\nIt seems that the empirical results for their sole benchmark domain reveals the performance of MMDQN is better than Sinkhorn. \n\n* In Fig 2, the ratio improvement (%) curve seems to show that both QRDQN and MMDDRL could dominate Sinkhorn in the number of games they outperform Sinkhorn, i.e., width for orange bars are shorter than blue bars.\n\n* The baselines are not inclusive. Note that both C51 and QRDQN are not methods with the unrestricted distributions like SinkhornDRL. Instead, their follow-up yet well-adopted works IQN and FQF are methods with unrestricted distributions. I feel the authors should include those unrestricted distribution baselines (apart from MMDQN) for compression. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**:  moderate\n\nThe paper is well organized, clearly written and easy to follow.\n\n**Quality**:  moderate\n\nThough the writing is okay, the quality is limited because SinkhornDRL comes with limited technical novelty compared with MMDDRL,  there is not inclusive discussion/comparison with related distributional RL methods,  and the empirical evaluation setting/result is not convincing, while sine if the result even highlights SinkhornDRL is less effective than baselines. \n\n**Novelty**:  weak\n\nThe novelty is weak because as the authors claim, SinghonDQN is only different from MMDDRL with their distribution divergences.  And the only new theoretical novelty is the proof of contraction theorem. \n\n**Reproducibility**: weak\n\nThe authors present very limited details on the implementation details of the method. Though the authors claim the empirical setting is inherited from QRDQN, it is possible settings as both sides are not identical so the reproducibility information for this work needs to be self-contained. ",
            "summary_of_the_review": "This paper introduces a novel distributional RL method where the distributional divergence is estimated from deterministic samples with Sinkhorn divergence. The authors evaluate the method on a Atari 2600 domain under a customized frame number 10M, where I feel the results reveal MMDDRL outperforms SInkhornDRL and the settings are falsely claimed. \n\nI'll re-evaluate the recommendation score to this paper if  my concerns on (1) technical novelty, (2) presentation on empirical settings and results,  (3)  sufficient details, atari wrappers, code or pseudocode  to implement the experiment could be provided to convince me the result is reproducible. It's even better if the authors could replace the results under 10M with those under 200M. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper802/Reviewer_VKzt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper802/Reviewer_VKzt"
        ]
    },
    {
        "id": "7hwNf0ytIAZ",
        "original": null,
        "number": 4,
        "cdate": 1667595611040,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667595611040,
        "tmdate": 1667595611040,
        "tddate": null,
        "forum": "uHrJ1AY1xR1",
        "replyto": "uHrJ1AY1xR1",
        "invitation": "ICLR.cc/2023/Conference/Paper802/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new DRL method leveraging Sinkhorn divergence. Analogous to DQN, they consider learning the action value distribution by minimizing the divergence between the current distribution and the target distribution. The main difference with previous DRL methods is they replace the divergence to be Sinkhorn divergence. Theoretically, they study the convergence of the distributional\nBellman operator, the moment matching meaning of Sinkhorn divergence, and the connection with MMD DRL. Although the paper is generally well-written, the experimental results does not show significant improvements over the current state-of-the art methods. ",
            "strength_and_weaknesses": "Strengths: 1. The new contributions of this paper is to replace the divergence in DRL to be Sinkhorn divergence, which can be viewed\nas an interpolation between Wasserstein distance and MMD.\n\nWeakness: 1. \"The main limitation of our proposal is that the superiority over existing state-of-the-art algorithms\nmay not be sufficiently significant.\" The authors themselves point this out. So what is the advantage of using Sinkhorn iterations?\n2. Figure 3 (b) Why does the performance degrade with increase in sample size? Increasing the samples should better approximate the return distribution.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. \n\nI consider it as a \"new\" work. The new contributions of this paper is to replace the divergence in DRL to be Sinkhorn divergence, which can be viewed as an interpolation between Wasserstein distance and MMD.\n\nI am not convinced with average over only 3 seeds. At least a few experiments should be presented with more number of seeds.",
            "summary_of_the_review": "This paper proposes a new DRL method leveraging Sinkhorn divergence. Analogous to DQN, they consider learning the action value distribution by minimizing the divergence between the current distribution and the target distribution. The main difference with previous DRL methods is they replace the divergence to be Sinkhorn divergence. Theoretically, they study the convergence of the distributional\nBellman operator, the moment matching meaning of Sinkhorn divergence, and the connection with MMD DRL. Although the paper is generally well-written, the experimental results does not show significant improvements to the current state-of-the art methods. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper802/Reviewer_vqCE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper802/Reviewer_vqCE"
        ]
    }
]