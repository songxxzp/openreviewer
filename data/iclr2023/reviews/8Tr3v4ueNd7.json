[
    {
        "id": "35PZKL5E1M",
        "original": null,
        "number": 1,
        "cdate": 1666456169750,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666456169750,
        "tmdate": 1666528012535,
        "tddate": null,
        "forum": "8Tr3v4ueNd7",
        "replyto": "8Tr3v4ueNd7",
        "invitation": "ICLR.cc/2023/Conference/Paper6127/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a new attention mechanism for graph transformers aiming at handling graphs with large number of nodes.\n\nMore precisely, the attention mechanism is three-fold:\n- Local attention, applied to 1-hop neighbors of a given node.\n- Global attention via virtual nodes.\n- Sparse attention based on an expander graph, which is the main contribution of the paper.\nThe main model is used within GraphGPS framework, which combines an attention scheme with message passing and positional/structural encodings.\n\nThe authors motivate the use of an expander graph based attention by the theoretical properties of expander graphs: \n- The expander graph is a good approximation of the complete graph.\n- Random walks on the expander graph mix well, meaning that provided there are log(nb_nodes) expander graph attention layers, each node is able to communicate with any other node.\n\nThe author then proceed to experimentally demonstrate the benefits of the proposed attention mechanism over various GNNs and Graph Transformers, both in terms of accuracy and number of parameters/scalability. Finally, some ablations are conducted i) by combining other attention schemes than Exphormer with MPNN and ii) by comparing Exphormer without message passing to models that do not necessarily use message passing.",
            "strength_and_weaknesses": "Strengths:\n- Scaling competitive graph transformers to large graph is an important problem for which this work offers a potential solution.\n- This work proposes interesting motivations for the use of expander graphs.\n- The proposed mechanism scales to larger graphs (order of magnitude of 100k nodes).\n- Some ablations demonstrate the usefulness of Exphormer compared to other graph transformers. More generally, the experimental protocol is thorough (error bars, various baselines and tasks).\n\nWeaknesses:\n- Although the use of expander graph is motivated, it is not clear whether these motivations are responsible for the performance of Exphormer: there is to my understanding no ablation on the role of each of the three attention mechanisms (local, global, expander), and the attention pattern could be studied to check whether expander attention indeed uses communication between distant neighbors.\n- Experiments on ogbn may be more convincing: experiments on arxiv are behind GAT, and behind GraphGPS for molpcba and code2. Moreover, it would be useful to display the results of Exphormer only on molpcba and code2.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is globally well-written with a few caveats:\n- It is not clear how the three attention mechanisms are combined.\n- Is the model sensitive to the choice of number of virtual nodes and expander degree?\n\nQuality:\n- The paper offers interesting mathematical motivations for the use of the expander attention scheme but I don't think the experiments illustrate the benefits of this scheme, which makes the paper somehow imbalanced. It would be more convincing to either study the expander attention pattern and show it leverages the properties detailed in Section 4, or reduce Section 4 to further experimentally study the proposed attention mechanism (sensitivity to hyper-parameters or ablation of local/global/expander attention).\n\nNovelty:\n- The use of expander graphs has been recently proposed [1] in the context of GNNs: it would be worth discussing it in the related work to better delineate this contribution. Apart from this, the proposed scheme (combination of local-global-expander attention) seems new.\n\n[1] SPARSIFYING THE UPDATE STEP IN GRAPH NEURAL NETWORKS, Johannes F. Lutzeyer, Changmin Wu, Michalis Vazirgiannis (2021)\n\nReproducibility: no code is not provided but the appendix seems to contain enough experimental details.",
            "summary_of_the_review": "This work offers a sensible solution, a sparse attention scheme relying on expander graphs, to an important problem, scaling graph transformers to large graphs. Expander attention has theoretically suitable properties and the experiments demonstrate that its combination to other attention mechanisms within GraphGPS framework scales to large graphs with reasonable performance in terms of accuracy.\n\nHowever, it seems to me that the paper stresses on the mathematical motivation of expander attention without studying whether these properties are indeed useful in practice, and the results on ogbn seem to be behind other baselines.\n\nTherefore, I do not recommend acceptance for now. I am willing to increase my score if my concerns (clarification of usefulness of expander attention, discrepancy with GNNs on ogbn-arxiv and with GraphGPS on ogbn-molpcba and ogg-code2) are answered.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6127/Reviewer_4gAE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6127/Reviewer_4gAE"
        ]
    },
    {
        "id": "IPD5pNPfEi",
        "original": null,
        "number": 2,
        "cdate": 1666521477254,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666521477254,
        "tmdate": 1666521477254,
        "tddate": null,
        "forum": "8Tr3v4ueNd7",
        "replyto": "8Tr3v4ueNd7",
        "invitation": "ICLR.cc/2023/Conference/Paper6127/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes Exphormer, a framework to improve the performance and scalability of graph transformers. Exphormer consists of three components, including expander graph attention, local neighborhood attention and global attention. The key innovation of this work is that Exphormer introduces expander graphs on graph transformers as a graph data augmentation method, which has rich theoretical properties. Meanwhile, Exphormer uses a sparse attention technology similar to BigBird, which reduces the quadratic time complexity of self-attention, making it possible to use it on larger-scale data. The authors demonstrate the abality and efficiency of Exphormer from two aspects of theoretical and experimental analysis, and achieves sota results on three datasets.",
            "strength_and_weaknesses": "Strengths:\n1. Nice English writing and clear sentences.\n2. The figure provides a good overview of the method.\n3. The theoretical analysis of expander graphs is solid and complete.\n4. Compared to other graph transformers, Exphormer has significant advantages in computational time complexity.\n\nWeaknesses:\n1. Insufficient motivation of using graph transformers as encoders. In the formula of the generalized (dot-product) attention mechanism at the bottom of page 3, each node only calculates attention scoers with its own 1-hop neighbor nodes. Such an attention process is not much different from the graph attention network (GAT), and destroys the properties of the global receptive field of graph transformers, which is a key advantage of graph transformers towards GNNs. There seems no problem if the graph transformer here is replaced with GAT, so the motivation of using graph transformers as encoders is not sufficient.\n2. The authors discuss in section 4.2.2 that if a certain number of transformer layers are stacked, the global receptive field of the Exphormer can be recovered. However, this method is common in GNN. Besides, the authors mention in the introduction that the stacking can lead to problems such as oversmoothing and oversquashing. It is mentioned above that the graph transformer in Exphormer is not very different from GAT, so will Exphormer also face the problems of oversmoothing and over squashing if stacking multiple layers?\n3. The authors mention in the introduction that most of the experiments done by GraphGPS focused on small graphs, but this work only conducts experiments on one large dataset, ogbn-arxiv. A key advantage of Exphormer is scalability, so it needs to be supplemented with more node classification experiments on other large-scale graph datasets.\n4. Theorem 4.1 analyzes the  approximation properties of expander graphs to complete graphs, but complete graphs are only a kind of special structures, which cannot represent the approximation propertities for general graph structures, and it seems there is no proof for this theorem. Besides, $S$ in equation 1 has no annotation, $\\pi^{(t+1)}=D_{G}^{-1}A_{G}\\pi^{(i)}$ in section 4.2.2 should be replaced with $\\pi^{(t+1)}=D_{G}^{-1}A_{G}\\pi^{(t)}$.\n5. In Appendix D, the authors analyze the universal approximation properties of Exphormer. However, the authors only uses the global attention module. It seems that expander graph attention and local neighborhood attention modules do not contribute to the theoretical performance of the model.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This work is well-organized, well-written, and the figures and the diagrams are helpful to understand the model architecture.\n\nQuality: The theoretical analysis of expander graphs is solid and complete. However, the motivation of using graph transformers and expander graphs is insufficient, and the lack of large-scale dataset experiments is not convincing.\n\nNovelty: This work combines graph transformers and expander graphs, which is innovative.\n\nReproducibility\uff1aThe authors did not provide supplementary material, so it cannot be reproduced.\n\n",
            "summary_of_the_review": "This work combines graph transformers and expander graphs together, and uses sparse attention mechanism to reduce quadratic time complexity. However, the motivation of using graph transformers and expander graphs is insufficient, and the lack of large-scale dataset experiments is not convincing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6127/Reviewer_X95y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6127/Reviewer_X95y"
        ]
    },
    {
        "id": "MoMce54y_0",
        "original": null,
        "number": 3,
        "cdate": 1666599001633,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599001633,
        "tmdate": 1666599001633,
        "tddate": null,
        "forum": "8Tr3v4ueNd7",
        "replyto": "8Tr3v4ueNd7",
        "invitation": "ICLR.cc/2023/Conference/Paper6127/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new and scalable graph Transformer, named Exphormer. The motivation of this work is that existing graph Transformers can not efficiently scale to large graphs. Exphormer designs a sparse attention mechanism that consists of three main components: expander graph attention, local neighborhood attention and global attention. As the main building block, expander graph attention utilizes the edges of a regular expander graph with a constant degree in the interaction graph, thus allowing it have a linear computational cost. The authors further provide theoretical analysis of expander graphs and the implication with Exphormer. Extensive experimental results demonstrate the effectiveness of the proposed Exphormer.  ",
            "strength_and_weaknesses": "Strengths: \n-\tComparing with full-attention graph Transformers, Exphomer can perform sparse attention mechanism on larger graphs and efficiently reduce the computational cost.\n-\t The paper provides plenty of theoretical analysis to support their claims, although I have some concerns.\nWeaknesses:\n-\tFor two graphs with the same number of nodes, using the same degree constant may generate the same regular graphs, thus preserving the same edges.  How does this help in learning the features of the two graphs? In fact, the paper doesn\u2019t distinctly illustrate the function of expander graph attention for learning node representations.\n-\tThe degree of regular graphs, d, is an important hyperparameter and graphs of different sizes may need different d, especially on large graphs. However, the parameter studies of d on different graphs are missing.\n-\tLacking of ablation studies to verify the effectiveness of the three main components of Exphormer. I\u2019m curious about how much performance gain can expander graph attention bring.\n-\tMore experiments need to be conducted on large graph benchmarks (Citeseer, Pubmed, Computer, ogbn-proteins, etc.) to validate the effectiveness and scalability (especially the comparison of time and memory cost) of the proposed method on large graphs. In addition, existing sparse attention methods (GT[1]) and sampling-based methods (NAGphormer[2], Gophormer[3]) shall be chosen as baselines on large graphs.\n-\tThe paper is not very easy to follow and some notations are confusing. And the formulas are not numbered. Specifically, the format of Table 1 is totally a chaos and need to be reorganized.\n\n[1] Dwivedi V P, Bresson X. A generalization of transformer networks to graphs[J]. arXiv preprint arXiv:2012.09699, 2020.\n[2] Chen J, Gao K, Li G, et al. NAGphormer: Neighborhood Aggregation Graph Transformer for Node Classification in Large Graphs[J]. arXiv preprint arXiv:2206.04910, 2022.\n[3] Zhao J, Li C, Wen Q, et al. Gophormer: Ego-Graph Transformer for Node Classification[J]. arXiv preprint arXiv:2110.13094, 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing is not very clear and not friendly for readers.\nQuality: The related theoretical analysis maybe reasonable, but the experimental results cannot validate the efficacy of the proposed method.\nNovelty: A transferable work from BigBird, and the effectiveness remains uncertain.\nReproducibility: Not Applicable. \n",
            "summary_of_the_review": "The method and theoretical analysis look good, but the experimental analysis, comparisons and writings need to be improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6127/Reviewer_U7gQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6127/Reviewer_U7gQ"
        ]
    },
    {
        "id": "-jzCfRD1j9W",
        "original": null,
        "number": 4,
        "cdate": 1666651634376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651634376,
        "tmdate": 1666652139324,
        "tddate": null,
        "forum": "8Tr3v4ueNd7",
        "replyto": "8Tr3v4ueNd7",
        "invitation": "ICLR.cc/2023/Conference/Paper6127/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new sparse attention mechanism for graph-structured data to address the quadratic complexity issue of graph transformers. Inspired by the idea of the sparse attention module BigBird for sequences, the proposed method, named Exphormer, also constructs an interaction graph based on three types of edges for attention. These edges include a random expander graph (random attention), the original graph (local neighborhood attention), and edges between any node and a virtual node (global attention). Thanks to the fruitful mathematical properties of expander graphs, the authors prove mixing properties and a universal approximation property of Exphormer. The authors show the first time that BigBird-like models, namely Exphormer, achieve very good performance on five graph benchmark datasets.",
            "strength_and_weaknesses": "##### Strengths\n1. The idea of using expander graphs (specifically d-regular expanders for fixed d) as a sparse approximation of complete graphs is elegant and original to my best knowledge.\n2. The proposed method seems to be simple to implement and to incorporate into existing framework combining transformers and GNNs, which only requires a sampler for generating random regular expander.\n3. The empirical comparison is fair and solid.\n4. The paper is well-written and easy to follow.\n\n##### Weaknesses\n1. (Positioning) This work is motivated by the fact that most existing graph transformers are limited to small graphs due to the quadratic complexity of full dot-product attention. However, the conducted experiments are somewhat misleading as most of the results are only shown on datasets of small-scale graphs. Given marginal improvements over the previous SOTA transformer GraphGPS that used either full or linear attention (e.g. Performer), the impact of this method is not very clear. On the other hand, for the more interesting tasks with large-scale graphs like ogbn-arxiv, the paper only shows very preliminary results, far behind the SOTA results. I expect a thorough evaluation of Exphormer on more datasets with large-scale graphs, such as other node classification datasets of OGB.\n2. (Limited task types) Exphormer is only evaluated on classification tasks. It would be more convincing to demonstrate the effectiveness of Exphormer on graph regression tasks.\n3. (Missing experiments) Some important studies of hyperparameters are missing, including the effect of expander graph degrees, the effect of the number of virtual nodes. It would also be useful to show the memory usage of the vanilla transformer, BigBird, Performer, and Exphormer.\n",
            "clarity,_quality,_novelty_and_reproducibility": "__Clarity__:\n1. What is the difference between the BigBird model used in GraphGPS and Exphormer? Do they only differ in the choice of the random expander graphs?\n2. Exphormer seems to work well mostly on synthetic and image datasets. On molecular graph datasets (ogbg-molpcba), there is a clear performance gap between Exphormer and GraphGPS, could the authors elaborate more on why Exphormer does not work well for molecular graphs? Also, why there is no standard deviation available in Table 5 for Exphormer?\n3. What are the search grids for different hyperparameters?\n4. Is Exphormer permutation-invariant? At inference time, does Exphormer still use a random expander graph? If so, how to ensure generating the same output for the same graph with different permutations of nodes.\n\n__Quality__:\n1. The work addresses an important and challenging problem for graph transformers, showing promising empirical results.\n2. The implementation descriptions are detailed, and the experimental setup is fair and sound.\n\n__Novelty__:\n- The use of d-regular expanders is original. However, it would be important to clarify the difference between the BigBird and Exphormer in experiments.\n\n__Reproducibility__:\n- All hyperparameters are provided, but the code was not provided.",
            "summary_of_the_review": "This work successfully extends BigBird to graph-structured data and demonstrates the first time that BigBird-like sparse attention achieves SOTA performance on several graph classification datasets. I have some concerns about its insufficient experimental results on large-scale graphs, limited task types considered, and some missing experiments relative to hyperparameters and memory usage.\n\nOverall, I think the strong points overweight the weak points and I recommend a weak accept. I would further increase my score if all my concerns and questions are answered.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6127/Reviewer_eDs8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6127/Reviewer_eDs8"
        ]
    }
]