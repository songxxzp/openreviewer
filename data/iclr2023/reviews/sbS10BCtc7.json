[
    {
        "id": "__y092qXE9p",
        "original": null,
        "number": 1,
        "cdate": 1666190847461,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666190847461,
        "tmdate": 1666190847461,
        "tddate": null,
        "forum": "sbS10BCtc7",
        "replyto": "sbS10BCtc7",
        "invitation": "ICLR.cc/2023/Conference/Paper2284/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper is an extension of variational auto-encoders that uses a different type of metric for performing training. The suggested metric directly matches the latent and data distributions using the variational autoencoding scheme; this is based on the Gromov-Wasserstein (GW) metric between the trainable prior and given data distributions.\n\n",
            "strength_and_weaknesses": "Pros: \n1. The authors perform an extensive ablation study of the method which convinces me that the shown results are not statistical artifacts.\n2. The method is based on a solid theory which is motivated well in the manuscript. \n3. The derivations, although straightforward, are correct. \n\n\nCons:\n1. The idea is a delta upon the large corpus of VAEs; importantly, it heavily builds upon WAE.\n2. Even though the method shows promise in the considered datasets, I think that much more datasets are needed for the empirical evidence to be convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "The method is sound, the assumptions are clear, and the empirical results clearly reproducible. The method is only marginally novel. ",
            "summary_of_the_review": "The method is interesting and sound, though marginally novel. The experiments show promise and depth, but we need more datasets for these to be convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2284/Reviewer_y4yE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2284/Reviewer_y4yE"
        ]
    },
    {
        "id": "r8ntBQ3HLnk",
        "original": null,
        "number": 2,
        "cdate": 1666630623582,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630623582,
        "tmdate": 1668705966637,
        "tddate": null,
        "forum": "sbS10BCtc7",
        "replyto": "sbS10BCtc7",
        "invitation": "ICLR.cc/2023/Conference/Paper2284/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A representation learning scheme is proposed using the Gromov-Wasserstein (GW) distance, a variant on optimal transport distances which allows comparison between distributions in different metric spaces. An auto-encoder based on the GW distance is proposed and evaluated on common image datasets.  Empirical tests on disentangling and clustering are made.\n",
            "strength_and_weaknesses": "[+] Convincing empirical evidence that GW metric is indeed estimated (Figure 1a).\n\n[+] Good evidence that proposed method quantitatively improves disentanglement scores (Table 1).\n\n[+] Comprehensive review of literature.\n\n[-] Unclear empirical advantage of GWAE for sample generation task (Table 1). Prior work 2-stage VAE appears better.\n\n[-] No discussion of computational cost or scalability of GW estimation. Isn't L_GW (Eq. 8) quadratically expensive?\n\n[-] Only (relatively) small scale datasets are considered (CelebA, 3D Shapes, Omniglot). Is this related to the cost of estimating GW? More discussion is desired.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work is clearly written and appears to me to novel. The results appear reproducible after a quick look at the submitted code.\n",
            "summary_of_the_review": "This work is a natural successor to many OT based generative modeling works existing in the literature. The empirical advantage of the proposed method appears limited with regards to generative quality measures, but does some improvements in disentanglement. I would like to see more discussion of cost of estimating GW. Nevertheless this work appears to me to be a worthwhile contribution and therefore I support acceptance.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2284/Reviewer_JFYx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2284/Reviewer_JFYx"
        ]
    },
    {
        "id": "kPrh0kWD82",
        "original": null,
        "number": 3,
        "cdate": 1666635819081,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635819081,
        "tmdate": 1666635819081,
        "tddate": null,
        "forum": "sbS10BCtc7",
        "replyto": "sbS10BCtc7",
        "invitation": "ICLR.cc/2023/Conference/Paper2284/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an integrated VAE-based approach for representation learning by minimizing Gromov-Wasserstein distance between the metric-measure space of the data and latent vectors. Isometric encoding is obtained from the trained prior and their performances in meta-priors (e.g. disentanglement and clustering) are empirically demonstrated without additional penalty terms. In addition, the model has good generation performance although its goal is matching latent and data distributions.\n",
            "strength_and_weaknesses": "## Strengths \n\nThis paper proposes a method that can perform disentanglement and clustering tasks within a single model. The objective for the proposed model is intuitively derived. The performance of the proposed method is extensively demonstrated with many supporting experiments. The effect of each regularization term is experimentally confirmed.\n\n## Weaknesses\n\n1. Merging Eqs. (8) and (10) into the constraint $p_{\\theta}(x,z) = q_{\\phi}(x,z)$ needs a justification. The latter implies Eqs. (8) and (10), but the vice versa. I wonder if you can explain further about the advantages of using $\\mathcal{L}_D$ in (11) compared to the maximum-mean discrepancy (MMD) between the aggregated posterior and the prior.\n\n2. That the generated CelebA images preserves the details of hair is encouraging, but this may be a consequence of adding high-frequency noise to the generated smooth image. In fact, other parts of the faces exhibit spotty noise.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well constructed and easy to understand. Experiments and their results are well described, but the proposed method seems to lack details. An autoencoder model that approximately minimizes the Gromov-Wasserstein distance is novel. Its ability of reconstructing the high-frequency components of hair seems novel, although need scrutization. It appears that the structure of the neural sampler of $Z$ is not included.",
            "summary_of_the_review": "An autoencoder model that approximately minimizes the Gromov-Wasserstein distance seems novel, although its derivation needs further justification. The proposed model is extensively compared with other models. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2284/Reviewer_ZoWi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2284/Reviewer_ZoWi"
        ]
    },
    {
        "id": "fbkGsRCO8iP",
        "original": null,
        "number": 4,
        "cdate": 1666694962028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694962028,
        "tmdate": 1669293927714,
        "tddate": null,
        "forum": "sbS10BCtc7",
        "replyto": "sbS10BCtc7",
        "invitation": "ICLR.cc/2023/Conference/Paper2284/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose Gromov-Wasserstein Autoencoders (GWAE). The application of the GW metric allows to match data distributions with a given learnable prior, even when the distributions lie in spaces of different dimensions, thus allowing matching between latent and data space (in equation 4). Further, the GW objective allows rich meta-priors for flexible representation learning. Experiments on disentanglement and clustering using benchmark datasets (CelebA, MNIST and 3D Shapes) demonstrate the capabilities of the proposed approach.",
            "strength_and_weaknesses": "The authors present a new perspective of using GW for the construction of flexible variational autoencoders. For the most part the proposed approach is well motivated and properly justified, though requiring three auxiliary and complementary (based on the ablation studies) loss functions. Though mostly presented in the supplementary material, the authors present a thorough overview of the variational autoencoding landscape.\n\nThe work has some key weaknesses:\n\nThe justification for \\rho=1 citing alleviating outlier effects is not necessarily convincing, more so when in (9) they adopt a \\zeta=2 reconstruction loss, so why are outliers a concern in (7) but not in (9)?\n\nThough the ablation study is a welcome addition to the experiments as they demonstrate the contribution of L_W, L_D and R_H to (13), it will be important to include results without L_GW.\n\nThe results in Figure 2 though impressive at first raise some questions: i) it seems that both VAE and GWAE separate (disentangle) object hues in 2 out of the 16 dimensions considered, and ii) even though it seems that the x-axis in Figure 2c captures the object hue, either it does not preserve the order (see color map) or there is no ordering in the hue, in which case one dimension captures the hue, but one wonders how consistently across runs and model hyperparameters.\n\nIn Table 2, it is not clear how biased are the results considering that hyperparameters were selected to optimize DCI-C that is used as performance metric and happens to show the largest difference with existing methods.\n\nFrom Table 2 it is not clear why the authors could not obtain results for WAE considering the closeness with the proposed approach. Further, the table seems to indicate that FID was taken from the original papers but not PSNR? \n\nThe results for clustering structure are interesting, however, are probably not the best way to showcase the clustering capabilities of the prior used in the proposed approach. In fact, as the authors show, a relatively simple VAE lacking a clustering prior achieves near perfect outlier detection.\n\nThe authors do not discuss why for Section 2 they use NP exclusively. It will be interesting to see results for GW estimation and minimization using FNP.\n\nMinor points:\n\nThe marginal p_\\theta(x) in (8) is not defined.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively easy to follow, though somewhat inconvenient due to the large amount of content in the supplementary material. The approach seems novel and properly justified in light of the existing literature, though innovation is somewhat difficult to assess considering the complexity of the proposed objective (for instance ablation without L_GW is not considered). The authors provide details of the data, model parameter, optimization details and source code that should in principle facilitate reproducibility.",
            "summary_of_the_review": "The authors offer an interesting and seemingly flexible approach to optimize variational autoencoders by leveraging the GW metric, which allows direct matching between observed and latent spaces, however, not without the help of complementary objective functions, specifically, marginal matching given the approximate posterior (WAE), critic-based joint distribution matching (KR), and entropy regularization to alleviate collapsing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2284/Reviewer_PxfM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2284/Reviewer_PxfM"
        ]
    }
]