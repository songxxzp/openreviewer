[
    {
        "id": "OCaotjw6HB",
        "original": null,
        "number": 1,
        "cdate": 1666210315768,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666210315768,
        "tmdate": 1666210315768,
        "tddate": null,
        "forum": "v-3dUexkNn",
        "replyto": "v-3dUexkNn",
        "invitation": "ICLR.cc/2023/Conference/Paper1231/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new dataset of probabilistic dynamic stability of synthetic power grids which is much larger than the existing datasets. Moreover, it also includes a Texan power grid model alongside with the dataset. Second, this paper also proposes a new method to identify the nodes which could be the one leading to the unstability. Finally, the papers evaluate several GNN baselines on the proposed benchmark and made a complete analysis. In the experimental results, the paper shows the potential to use the GNN model trained with the offline data to conduct the prediction on the real-world power grid.",
            "strength_and_weaknesses": "## Strength:\n1. This paper is generally well written and the background is sufficient to let the people who is not an expert in power systems understand the task.\n2. The details of generating data are provided.\n3. The figures are concise to show the ideas.\n4. The anlysis of experimental results are sufficient and solid.\n\n## Weaknesses:\n1. No significant novelty appeared in this paper.\n2. The method to identify the trouble-maker nodes is not clearly described at least from the main part of paper. I suggest the authors can merge the related contents in the appendix and rewrite it using compact words.\n3. The authors claim that the trouble-maker identification is a classification task. However, in my view it is just a criterion rather than a classification task that need the ML model to conduct.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The originality of this paper is good, since it provides a new dataset about a significant real-world problem with detailed description of the generation of data so that it is trustful.\n\nThe novelty of this paper is lacking, but it is just because of the output of this paper. It provides a larger dataset than the previous ones, rather than propose a new research problem.\n\nThe clarity of this paper is good enough, except the description of identification of trouble-maker nodes. \n\nThe reproducibility of this paper is good, since it provides the experimental details to reproduce the results and the authors promise that they will open-source the dataset and codes afterwards.\n\nThe general quality of this paper is above the average in my view.\n\n",
            "summary_of_the_review": "This paper provides a new benchmark dataset about a significant real-world problem and it has shown the evaluation with several baselines which could provide some lessons to the following people. The background of the problem is sufficient. The description of the identification of trouble-maker nodes needs to be clarified, which may change my decision.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1231/Reviewer_iZuM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1231/Reviewer_iZuM"
        ]
    },
    {
        "id": "6eyPXZVaMg",
        "original": null,
        "number": 2,
        "cdate": 1666642102334,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642102334,
        "tmdate": 1670380418281,
        "tddate": null,
        "forum": "v-3dUexkNn",
        "replyto": "v-3dUexkNn",
        "invitation": "ICLR.cc/2023/Conference/Paper1231/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce new datasets of dynamic stability of synthetic power grids. \nThey show that large Graph Neural Networks GNNs outperform GNNs from previous work at predicting single-node basin stability.They demonstrate that GNNs can be used to identify trouble maker-nodes in the power grids and show that GNNs trained on small grids can perform accurately on a large synthetic Texan power grid model.",
            "strength_and_weaknesses": "I am unfortunately not super knowledgeable about power-grids but enjoyed reading this interesting paper. \n\nI believe that the strength of this paper lies in the introduction of a new dataset of higher complexity than previous existing datasets. I think this is very important for evaluating and training models.\nThe other strength of the proposed model is the computational efficiency and scalability (1,800 times faster for grids of size 100 and 4,6 \u00d7107 times faster for the synthetic Texan power grid). \n\nThe fact that GNNs can generalize from a small dataset to a bigger one is very compelling, but the datasets are very similar, and generated using the same modeling strategy. I am not surprised that a GNN can generalize well in this setting, since what it learns from dataset20 will hold in dataset100. I think that this result is overstated by the authors. I might not understand the differences between dataset20 and dataset100 well enough to understand the depth of this statement. I'd appreciate if the authors could explain this to me in more details. \n\nI'm also not sure that it makes sense to flag a node as trouble maker if small disturbances are amplified by a factor of at least 6. This seems a bit arbitrary. Is there any reason coming from analysis of real datasets for this? \nThis is then used for a classification task whereas I think a regression task would be more appropriate. For example, predicting the amplification factor rather than predicting if the factor is above or below 6. Also, reporting the uncertainty should be interesting. Is the model more confident on nodes with higher amplification factor? To use in a real setting, we would want the model to have a good uncertainty representation. \n\nOne thing I didn't understand is why, in figure 3, the texan grid has 97% of stable nodes although the distribution of SNBS seems to give much less probability for high SNBS (around 1). I'd appreciate some clarification here.\n\nIn figure 5, the model seems to work very well for low SNBS but have skewed predictions for high SNBS. Would it be possible to fix this using some non-linearity? \n\nOverall, there are lots of typos that make the paper a bit hard to read. For example in the abstract: \nWe show that large GNNs outperform GNNs from previous work as well as as handcrafted graph features and semi-analytic approximations.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work is novel as it introduces a new synthetic dataset and extends previous models. \nI think the clarity and quality of the writing could be improved by carefully checking the typos (a few entire sentences do not make sense, even in the abstract)",
            "summary_of_the_review": "I am not very confident, and overall enjoyed the paper. \nThe new dataset is a big contribution, but I would like some clarification on the tasks that are used to evaluate the model (trouble node classification, SNBS prediction) as I'm not sure they fully support the claims made in the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1231/Reviewer_SqnB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1231/Reviewer_SqnB"
        ]
    },
    {
        "id": "Vvro8eEPoL-",
        "original": null,
        "number": 3,
        "cdate": 1667172337378,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667172337378,
        "tmdate": 1669601368253,
        "tddate": null,
        "forum": "v-3dUexkNn",
        "replyto": "v-3dUexkNn",
        "invitation": "ICLR.cc/2023/Conference/Paper1231/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a dataset containing synthetic models of power grids combined with the statistical results of dynamic simulations (quantifying single-node basin stability and survivability - abbreviated SNBS and SURV). The authors train a baseline GNN to show initial efficacy of the task of predicting SNBS and SURV, as well as of transferring the learned model to larger power grids.",
            "strength_and_weaknesses": "Strengths:\n* The proposed dataset characterizes an important problem. Specifically, the problem of dynamic stability is indeed increasingly important to address as power grids integrate larger proportions of time-varying renewable energy.\n\nWeaknesses:\n* The dataset structure is not clearly described, and the benchmark task (what are the inputs, what are the outputs, and what is the metric of success) is not cleanly defined. This information is implicit within the text and citations, but needs to be laid out much more clearly and explicitly to enable understanding of the specific benchmark presented - e.g., I would have expected to see some of the relevant equations laid out in the paper.\n* The setup of the baseline GNNs is also not clearly enough described. For instance, the text describes that the GNNs are given \"an adjacency matrix representing the topology of power grids and a binary feature vector representing sources and sinks,\" without additional detail - however, for a dataset paper, those inputs should be clearly spelled out.\n* Given that this is a machine learning audience as opposed to a power systems audience, the introduction needs to be made much more accessible. For instance, terms/phrases such as \"have less inertia,\" \"contingencies,\" and \"self-organized synchronization mechanism\" may not be immediately accessible to an ML audience. In addition, the word \"transformers\" should likely be clarified as \"power transformers,\" due to the different default meaning of this term in the deep learning literature.\n* Since all the data is synthetically generated, it is not clear to me why this is presented as only a static dataset, rather than the data generation code also being shared. See, e.g., [1] for an example of a dataset paper that does it this way.\n\n[1] Joswig-Jones, Trager, Kyri Baker, and Ahmed S. Zamzam. \"OPF-Learn: An open-source framework for creating representative AC optimal power flow datasets.\" 2022 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT). IEEE, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "While there is great potential in this paper, I think there is unfortunately great room for improvement in terms of clarity and reproducibility. The novelty is in that dynamic stability datasets of the presented size do not exist in the literature. The quality of the dataset is likely good, but it is difficult to tell from the writeup.",
            "summary_of_the_review": "The proposed dataset characterizes an important problem. However, the writeup does not clearly define the dataset/benchmark task or the baseline method. In addition, it would likely make more sense to present this as a software package for generating data, _alongside_ a fixed dataset, rather than just as a fixed dataset. \n\nIf the authors are able to make significant revisions to the writeup during the revision period, I would potentially be willing to significantly increase my score, given that my issues are (likely) with the presentation rather than the content of the work (at least, based on what I'm able to assess from the current writeup). In general, I think dataset papers are extremely valuable, particularly those dealing with climate/energy problems, but unfortunately the current writeup is not of high enough quality for this reviewer to recommend acceptance without such significant revisions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1231/Reviewer_wgck"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1231/Reviewer_wgck"
        ]
    }
]