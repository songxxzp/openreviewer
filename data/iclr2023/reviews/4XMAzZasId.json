[
    {
        "id": "Zle-vaYwbnZ",
        "original": null,
        "number": 1,
        "cdate": 1666186120579,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666186120579,
        "tmdate": 1666186623286,
        "tddate": null,
        "forum": "4XMAzZasId",
        "replyto": "4XMAzZasId",
        "invitation": "ICLR.cc/2023/Conference/Paper6/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduces a notion of task difficulty, which they coin \"inductive bias complexity\". Intuitively, it is the fraction of interpolating hypotheses (in the space of all possible functions) that also generalize well. After introducing the formal definition, the authors use a series of approximations to come up with a computationally tractable approximation. Then, they apply it on various tasks and argue that this measure of complexity matches with expectations; e.g. MNIST < CIFAR10 < ImageNet. ",
            "strength_and_weaknesses": "*Strengths*:\n- The authors pursue a novel direction, which is to provide a model-agnostic measure of task difficulty. However, the final outcome (Equation 11) is quite similar to several recent works that study scaling laws, which use much simpler arguments.\n\n*Weaknesses*:\n- Under the hood, \"inductive bias complexity\" corresponds to the fraction of interpolating hypotheses that also generalize well; i.e. the probability that an interpolating hypothesis would also generalize. When this is taken as a definition, we obviously have that interpolating functions would generalize well on simple tasks and perform poorly on more difficult tasks; but this is really somewhat tautological. I don't think the definition offers any insight that can be useful in practice. \n- The authors argue that the definition is model-agnostic. However, the definition does assume that one can (somehow) parameterize the space of all possible hypotheses.\n- If we consider simpler arguments used in the scaling laws literature, we deduce that difficulty of a task is roughly $O(m^{-1/d})$, where $m$ is the size of the training set and $d$ is the intrinsic dimension of the data manifold. This follows from a partitioning argument; see for example [1, 2, 3]. It's not clear if the proposed \"inductive bias complexity\" in Theorem 1 would offer any additional insight beyond such simpler measures of complexity. In fact, the series of approximations that the authors propose are similar in spirit to previous arguments and also lead to similar conclusions (see for example Equation 11).\n- The definition that the authors use involves a loss $\\epsilon$. In Appendix C, the authors use different values of  $\\epsilon$ for different tasks (e.g. they use a very small value in MNIST and a much larger value in ImageNet) even though they also show that \"inductive bias\" is sensitive to the choice of this parameter (Figure 3b). I'm not sure if this makes the comparison between datasets meaningful. \n\n[1] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. arXiv preprint arXiv:2102.06701, 2021.\n\n[2] Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021.\n\n[3] Utkarsh Sharma and Jared Kaplan. Scaling laws from the data manifold dimension. JMLR, 23(9):1\u201334, 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear. The proof of Theorem 1 is easy to follow. However, the paper does not use notations consistently, which makes it a bit difficult in a few places. For example, the authors use small letters for functions $f$ but capital letters for the loss function $L$, while using small letters for dimensionality of data and size but not dimensionality of output, etc.\n\nIn a few places, the authors introduce complicated definition to describe simple concepts that are already well-established in the statistical learning theory literature. For example, the discussion in Section 3.1 corresponds basically to the \"general setting of learning\", which was pioneered by Vapnik [4]. \n\nIn addition, the authors refer to \"inductive bias complexity\" repeatedly in the paper before it was formally defined in Page 5. It would be helpful to define it first, at least informally, so that the discussions in Sections 1, 2, and 3 can be easier to follow.\n\n[4] V. Vapnik, \u201cAn overview of statistical learning theory,\u201d IEEE TNN, 1999.",
            "summary_of_the_review": "I think the direction is novel but the approach the authors use is very similar to recent works that study scaling law estimators. In that line of work, the goal is to understand power laws in overparaemterized models. In this work, the goal is to quantify the difficulty of a task but the approach is questionable for several reasons as I mention above. The paper would benefit from providing a stronger motivation that highlights clearly how this measure of complexity can be useful to the community (either by providing new insights or practice applications).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6/Reviewer_Du5K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6/Reviewer_Du5K"
        ]
    },
    {
        "id": "DF5GUK3en6P",
        "original": null,
        "number": 2,
        "cdate": 1666561312369,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561312369,
        "tmdate": 1666561312369,
        "tddate": null,
        "forum": "4XMAzZasId",
        "replyto": "4XMAzZasId",
        "invitation": "ICLR.cc/2023/Conference/Paper6/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the amount of inductive bias needed to fit a dataset and derives a computationally feasible proxy for such a measure. In contrast to sample complexity, this metric aims to measure the difficulty of learning a given dataset, i.e. how much inductive bias needs to be present in order to achieve a certain level of generalization at a given sample size. The derived metric is largely agnostic to the model space and works for a very broad set of tasks, enabling the comparison of seemingly very remote tasks such as standard supervised learning on ImageNet and reinforcement learning.\n",
            "strength_and_weaknesses": "**Strengths**\n\n1. The idea to reverse roles and instead assess the difficulty of a task in a way agnostic to the model class used is very interesting. The term \"inductive bias\u201d has been used very vaguely in the deep learning literature and a theoretically grounded measure for it is a very important contribution.\n\n2. The studied setting is very general and not many assumptions are made to arrive at a tractable metric for inductive bias. This makes this work applicable to a wide variety of problems.\n\n**Weaknesses**\n\n1. One of the major weaknesses of this work is its readibility. Large parts of the paper are very informally written (especially the entire section 4.2), which makes it very difficult to assess the validity of the results. I have listed my specific questions in the clarity section. I would greatly appreciate if the authors could clarify my concerns.\n\n2. While the great level of generality is certainly a strength of this work, it is also one of its bigger weaknesses. The data model which encapsulates a wide variety of tasks is very unintuitive and not easy to follow. While the authors explain in more detail how this model captures more specific settings in the appendix, I think more time could be spent in the main text to familiarize the reader with the framework. Walking through a simpler setting (i.e. supervised learning and Euclidean space) would be very helpful here.\n\n3. The setting of this work is extremely broad, the task considered can range from supervised learning to reinforcement learning, no assumptions on the training nor the test distribution are made, and the constructed hypothesis space also seems very general. While this is certainly a great feature of this work, it also remains unclear to me how much inductive bias can really be captured if so little assumptions are made on the data distribution. What happens for instance if instead of using clean targets on ImageNet, we study randomized targets, or varied the number of classes? What happens if we add Gaussian noise to input images and thus inflate the intrinsic dimensionality of the manifold? Do we really need to pay such a high price in terms of inductive bias? I think the results of this work would be easier to put into perspective if such settings were considered. It is not clear to me why Omniglot would require way more inductive bias than ImageNet as it is not so clear how to compare these two tasks intuitively. The authors give some insights by varying the sample size on the same task but somewhat un-intuitively, the amount of needed inductive bias does not decrease too much.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is at times very informal and arguments are often only vaguely outlined and prior work is used without giving much context. This made it very difficult for me to follow the arguments and verify their validity. Especially Section 4.2 is very informal and assumptions are listed on-the-go or specified after the calculations are done, making it very difficult to follow. The fact that the hypothesis space is all of a sudden restricted to only consist of linear combinations of the eigenfunctions of the Laplace-Beltrami operator is only stated after all the calculations. Some more background on technical tools such as the Laplace-Beltrami operator and its eigendecomposition would help unfamiliar readers to better understand this work. I personally found it very difficult to follow this section and still cannot really tell if it is entirely correct. I am listing my more precise questions in the following:\n\n1. I find the data model used (i.e. x) somewhat confusing. While the data also consists of continuous features, later in the work it seems that only the discrete components of x are considered (i.e. Section 4.2). How would the results change if instead of a general manifold, the features would come from standard Euclidean space? In general, it might be helpful for the reader to maybe walk through a simpler example, say supervised learning on \\mathbb{R}^{d} instead of only this very abstract setting.\n\n2. How is the implicit bias of a model defined? This term is already abused to mean so many things in deep learning that a proper definition is needed in my opinion. What is \\mathcal{K} in equation (3)? You define it as the event that the model fulfills some implicit bias, but what does that mean? In equation (3) you consider minimizing over \\mathcal{K} but how can we minimize over a given, fixed event? In equation (6), \\mathcal{K} is then all of a sudden chosen as the event that the hypothesis generalizes with error upper-bounded by \\epsilon. How does fulfilling some implicit bias translate to generalizing with error \\epsilon? In my opinion, it would anyways be simpler to define task difficulty directly by equation (6).\n\n3. In equation (9), you divide by the volume of hypotheses that interpolate the training set.  It is not clear to me why this volume is positive under uniform weighting of hypotheses. The probability that a continuous random variable (here the training loss) takes a particular value is always zero. Without specific assumptions on the loss and the model class it is not obvious to me how equation (9) makes sense.\n\n4. The assumptions in Theorem 1 on the function space and loss function are somewhat non-standard. For which function classes does the Lipschitz-like property apply? Which loss function satisfy the listed assumptions? What loss functions do you consider for the tasks in the numerical evaluation section, i.e. for reinforcement learning etc? Do these loss functions meet the assumptions needed?\n\n5. The Wasserstein distance approximation seems very crude and needs further motivation in my opinion. How close must q be to uniform in order for the bound to be reasonable? It seems quite surprising to me that one can arrive at a reasonable estimate of the Wasserstein distance without making any assumptions on the two distributions, other than the fact that one is an \u201catomised\u201d version of the other. \n\n6. Why is it valid to approximate the set of interpolating solutions as a ball? If the loss is continuous (e.g. mse), then even slightly perturbing the parameters breaks interpolation to my knowledge. \n\n7. If I understand correctly, in section 4.2 you restrict the hypothesis space to linear combinations of the Laplace-Beltrami operator over the manifold. Could you give some intuition on the size of this space? How does it look for instance if we were to consider standard Euclidean space instead of a general manifold?\n\n8. This might be minor but how do you define a uniform distribution over an uncountable, non-compact set? Even for the model class of linear regression we are dealing with an uncountable, non-compact set of parameters (i.e. \\mathbb{R}^{d}) so it is not clear to me how one would reparametrize in this case.\n\n9. Some of the figures are not sharp on my monitor, reducing readibility.",
            "summary_of_the_review": "The idea of this work is very interesting, having a notion to estimate the amount of implicit bias needed to learn a dataset is very important and could lead to some new insights. In its current version however, I find this work very difficult to read and follow, large parts of the arguments are presented rather informally and the great generality of the result with only minimal assumptions on the data distribution seems somewhat surprising. More empirical evaluations for intuitively comparable setting would help the understanding as well. In summary, I have to recommend rejection of this work as in its current form, I find myself unable to confirm its validity, both theoretically and empirically. It is however very much possible that I have misunderstood parts of this work and I am happy to change my score if the authors can answer my questions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6/Reviewer_NEm3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6/Reviewer_NEm3"
        ]
    },
    {
        "id": "jdf5OBEexr",
        "original": null,
        "number": 3,
        "cdate": 1666685702090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685702090,
        "tmdate": 1666686293052,
        "tddate": null,
        "forum": "4XMAzZasId",
        "replyto": "4XMAzZasId",
        "invitation": "ICLR.cc/2023/Conference/Paper6/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This submission tackles the problem of measuring inherent difficulty of a learning task, independent of a learning algorithm and model class. Formally, a learning task is defined as a training set of instances and corresponding actions sampled from a training distribution. The goal of a learner is to generalize well to instances sampled from a possibly related test distribution. It is assumed that there is a ground-truth mapping from instances to actions. It is also assumed that one has access to a large hypothesis set (different from the model class) with a uniform prior on it, that includes the ground truth mapping. The authors show that common learning settings, such as supervised classification, reinforcement learning, meta-learning for few-shot classification, and meta-reinforcement learning can be formulated in this framework.\n\nWithin this framework, the authors introduce a measure of task difficulty that quantifies the minimum \u201camount of inductive biases\" needed to generalize well. This boils down to computing the fraction of interpolating hypotheses that have a desired level of performance on the test distribution. As it is hard to compute this quantity exactly, the rest of the paper makes a series of simpliciations and assumptions in order to compute an upper bound for this quantity. The final section of the paper computes this upper bound for a few standard learning tasks and examines the trends of it with different aspects of learning tasks.\n",
            "strength_and_weaknesses": "#### **Strengths**\n\n* This submission focuses on an important problem that was not addressed much to my best knowledge.\n* The proposed notion of inductive bias complexity is interesting.\n\n#### **Weakness 1: limitations**\n\n1. The proposed task difficulty depends on a large hypothesis set with a uniform prior on it. It is unclear how to select this hypothesis set and how it affects the findings.\n2. A uniform distribution might not exist for some of such large hypothesis sets.\n3. Even when a uniform prior can exist, it is still an arbitrary choice, as it disregards the geometry of the space. Maybe a more appropriate choice would be the Jeffreys prior.\n4. In the meta-learning setting this hypothesis set needs to be a set of mappings from datasets to functions. It is unclear how even distributions are defined on such spaces. Could you please elaborate on this?\n5. The proposed task depends only on the set of interpolating hypotheses. As mentioned in the paper, there might be well-generalizing but non-interpolating hypotheses. \n\n#### **Weakness 2: unrealistic assumptions and simplifications**\n1. In Thm 1, \u201cSuppose that the loss function is invariant to shifts $L(y_1 + y, y_2 + y) = L(y_1, y_2)$ for all $y_1, y_2, y$.\u201d:  This assumes that there is an addition operation defined on the action space. How does this hold in the settings of supervised classification or meta-learning for few-shot classification?\n2. The assumption of Eq. (8) seems too strong. Can you give examples of large hypothesis sets for which this assumption holds?\n3. In Sec 4.2, the data distribution is approximated as a mixture of uniform distributions on disjoint spheres. In particular, in a supervised classification setting, it is assumed that examples of the same class have a uniform distribution on a sphere.\n\n#### **Weakness 3: It is hard to validate the experiment findings.**\n1. Since there are no baselines presented and the proposed approach does many simplifications, it is hard to judge the validity of the experimental findings.\n2. I recommend designing some sanity checks for validating the results.  For example, how does the difficulty of union of two related (or independent) tasks compare to those of the individual tasks?\n3. In Figure 3(a), ImageNet training data is hypothetically increased to the size more than $10^{17}$, but we see that the approximated task difficulty remains roughly the same. It seems that with so many training examples, the set of interpolating solutions should contain only the ground-truth function (or extremely similar ones), therefore making the task difficulty close to zero. This entails that we should at least see a large drop in the approximated task difficulty when the number of examples is so large.\n\n\n**One related paper:** Achille et al. [1] also define information-theoretic model-agnostic task complexities.\n\n#### **References**\n\n[1] Achille, A., Paolini, G., Mbeng, G. and Soatto, S., 2021. The information complexity of learning tasks, their structure and their distance. Information and Inference: A Journal of the IMA, 10(1), pp.51-72.",
            "clarity,_quality,_novelty_and_reproducibility": "In general the paper is hard to follow and unclear in some places, especially in Section 4.2. Some of the problems come from trying to accommodate many learning settings under one umbrella. As this is the first step towards defining inductive bias complexity, maybe one just needs to consider only the standard supervised classification setting first.",
            "summary_of_the_review": "In summary, while I like the problem and the perspective of this submission, I find the paper not ready for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6/Reviewer_FUck"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6/Reviewer_FUck"
        ]
    },
    {
        "id": "UL8pHOHt72",
        "original": null,
        "number": 4,
        "cdate": 1666828312897,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666828312897,
        "tmdate": 1666828312897,
        "tddate": null,
        "forum": "4XMAzZasId",
        "replyto": "4XMAzZasId",
        "invitation": "ICLR.cc/2023/Conference/Paper6/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a novel notion called \"inductive bias complexity\". Just as sample complexity captures the number of samples required for learning a problem (upto a desired error) given a fixed inductive bias, the \"inductive bias complexity\" refers to the minimum amount of \"inductive bias\" required for learning a problem (upto a desired error) given a fixed number of training data. This notion is interesting because it, to some extent, explains the amount of prior knowledge (or handcrafted arts) one needs to set in order to solve a problem with the given amount of data. \n\nAfter introducing the notion of \"inductive bias complexity\", this work further provides a bound on it based on a set of conditions, which are somewhat strong but are also quite standard in learning theory. Finally, some empirical results are provided to verify the theoretical predictions. ",
            "strength_and_weaknesses": "# Strength\n1. I find the paper written in a very clear manner. I have enjoyed a lot reading the paper!\n2. The notion of \"inductive bias complexity\" is very novel to my knowledge, and is also very interesting. \n3. Both theory and experiments are provided for understanding the proposed inductive bias complexity, which form the work in a rather mature shape. \n\n# Questions\nI think the work is very interesting and is worthy getting more attention from the community. I have the following questions/suggestions:\n\n1. I think Appendix C is crucial for understand the empirical results. Perhaps it is worthy to summarizing some of the important quantities (e.g., $m, \\\\delta, \\\\epsilon$), as well as the methodology for estimating them, and adding them into main text?\n\n2. I feel the \"inductive bias\" could have been formally defined. Based on my understanding, it refers to a restricted set of models (as a subset of the full hypothesis class) that can be outputted by some fixed procedures/code (or algorithms, with some fixed or tunable hyperparameters?). \n\n3. It might also be worthy to discussing the sources of randomness. To my understanding the randomness is from the randomly drawing of the training samples (correct me if not). How does the framework account the randomness in the learning algorithm? For example, people believe that the random nature of SGD encodes certain inductive bias. \n\n4. While I understand that the current theory might be in their ultimate sharp version, I would like to see some discussion on why the inductive bias complexity has an exponential dependence on the data dimension. Would you say this exponential dependence is fundamental and un-improvable or just technical? Could you discuss some intuition on the exponential dependence?\n\n5. Some discussion in Section 5.2 is not very clear. In particular, how do you use test data in computing the inductive bias complexity? Do you treat the test error as the $\\\\epsilon$ in your formula?\n\n6. I am curious how useful is the inductive bias complexity in practice, even though I think it is already very interesting in terms of theory. Let us say we are given with a training set and a task, what else do we need to compute the inductive bias complexity? Or how do we compare the inductive bias complexity to that of other dataset+task? What implication can we get, if we obtain some bounds/estimations on the inductive bias complexity?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is very good\n\nQuality is good\n\nNovelty is very good\n\nReproducibility could be made more clear. Some of the experimental details could be discussed in the main text as well. ",
            "summary_of_the_review": "At this point I am very positive to this work. The authors reply on my questions can help me better understand the work. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6/Reviewer_eajX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6/Reviewer_eajX"
        ]
    }
]