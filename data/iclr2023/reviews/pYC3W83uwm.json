[
    {
        "id": "atwFNvSmKp",
        "original": null,
        "number": 1,
        "cdate": 1666378697572,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666378697572,
        "tmdate": 1666378697572,
        "tddate": null,
        "forum": "pYC3W83uwm",
        "replyto": "pYC3W83uwm",
        "invitation": "ICLR.cc/2023/Conference/Paper2292/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This proposes to solve online combinatorial optimization problems using reinforcement learning composed of latent MDPs and curriculum learning. The paper presents formal formulations and theoretical results regarding the performance bound based on the relative condition number. In addition, the paper also presents theoretical results showing the power of curriculum learning in reducing the relative condition number of the selection (secretary) problem. Experimental studies are also provided to support the theoretical findings.\n",
            "strength_and_weaknesses": "Strength:\n\n- S1: A combination of latent Markov Decision process, natural policy gradient technique, and curriculum learning can be an effective solution to online combinatorial optimization problems, and therefore, its theoretical foundations are important.\n- S2: The presented analysis seems sound, but the proofs are not completely checked.\n- S3: This paper is well-written, with formal settings being provided. Many details are carefully organized in the supplementary materials.\n\n\nWeaknesses:\n\n- W1: The so-called latent MDP appears to be a linear combination of classic MDPs. In such a sense, the existing analysis of MDP might be applicable to latent MDP without many technical challenges. The paper could better explain the technical challenge in obtaining the theoretical results. \n- W2: Some minor comments:\n     - While the main contribution of this paper is on the theoretical side, the experiments could be improved by having more baselines and problems. In particular, given the simple nature of the two considered problems, the paper may include simpler baselines based on, for example, multi-armed bandits. In addition to the interest of  Theorem 7, is there any other justification for the choice of the two selected problems?\n  - Is the entropy regularization equivalent to enforcing a certain prior distribution?\n  - The burden of notations might be reduced by avoiding introducing new notations, e.g., V^{\\pi}=V^{\\pi,0}, F(\\theta)=\\sum_{d^{\\theta}}^{\\theta}, and \u201chere /club can be any symbol\u201d.\n  - What is the logic behind sampling from a fixed policy rather than the current policy?\n  - The concept of environment E could be treated in a formal manner.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and reproducibility are good. \n\nThe novelty seems good, but the theoretical contributions were thoroughly examined. In addition, the paper could better explain the technical challenges in applying the existing frameworks.\n",
            "summary_of_the_review": "This paper proposes to solve online combinatorial optimization problems using reinforcement learning composed of latent MDPs and curriculum learning, and presents theoretical studies as well as experimental results. This paper is overall good and sound, but the significance of the theoretical contribution needs better justifications.\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2292/Reviewer_GcdD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2292/Reviewer_GcdD"
        ]
    },
    {
        "id": "gJFcr3FHnuB",
        "original": null,
        "number": 2,
        "cdate": 1666630266589,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630266589,
        "tmdate": 1666630266589,
        "tddate": null,
        "forum": "pYC3W83uwm",
        "replyto": "pYC3W83uwm",
        "invitation": "ICLR.cc/2023/Conference/Paper2292/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The current study formulates online CO problems as latent Markov Decision Processes and propose a policy optimization method using natural policy gradient. In addition, the current study characterizes performance and investigates the effect of curriculum learning in deriving optimum policy.",
            "strength_and_weaknesses": "\nThis study formulates online combinatorial optimization and proposes a way to derive optimum sequential decision-making policy. In addition, the current study provides a rigorous analysis of the performance of the policy. \n\nI think the level of analysis and mathematical rigor is very high. Since this field is not my area of expertise, I could not accurately evaluate this process.\n\nI think the current study fits better with an optimization or statistical analysis paper rather than a machine learning conference. I believe the current study should at least provide intuition or inspiration on how to effectively employ a learning-based solver for such a problem based on the discussed mathematical properties and structures.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n<Methodology>\n\n1. Does the proposed methodology suggest a particular representation or learning method for dealing with combinatorial structures?\n\n2. What is the reason for using a natural gradient instead of a typical gradient-based optimization method? Can you describe the advantages of using a natural gradient in terms of computational complexity and convergence speed?\n\n3. In curriculum learning, how to quantify the difficulty of the task?\n\n4. How does the convergence rate change when the distribution for multi-task changes?\n\n\n\n<Experiments>\n\nThe proposed algorithm does not compare its performance with other baseline algorithms. The provided experiment results are nothing more than an ablation study that proves the effectiveness of curriculum learning.\n",
            "summary_of_the_review": "I think the level of analysis and the mathematical rigor are very high. Since this is not my area of \u200b\u200bexpertise, I could not accurately evaluate this course. However, I think this paper is closer to optimization or optimal control paper than a machine learning paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2292/Reviewer_qByH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2292/Reviewer_qByH"
        ]
    },
    {
        "id": "rsRWQoO11_w",
        "original": null,
        "number": 3,
        "cdate": 1666636837496,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636837496,
        "tmdate": 1666636837496,
        "tddate": null,
        "forum": "pYC3W83uwm",
        "replyto": "pYC3W83uwm",
        "invitation": "ICLR.cc/2023/Conference/Paper2292/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the impact of the curriculum learning used in applying reinforcement learning to solve the combinatorial optimization problems. To demonstrate the impact of the curriculum learning, the paper first formulates the combinatorial optimization problem as latent markov decision process (LMDP) and applying the technics developed in LMDP to show the convergence rate. Besides that, the paper shows that the curriculum learning is helpful in finding a strong sampling policy to reduce the distribution shift. It also formally proves that the relative condition number is reduced exponentially with the curriculum learning comparing to the naive random sampling policy.",
            "strength_and_weaknesses": "Strength:\n  1. The paper reformulates the combinatorial optimization as a latent markov decision process (LMDP), which is interesting and useful in applying the technics of the RL in LMDP case. \n  2. The paper's motivation to find when and why the curriculum learning is helpful in applying RL in LMDP is meaningful. It also shows that the curriculum learning is helpful in finding a strong sampling policy to reduce the distribution shift by either formally proving or empirically showing that the relative condition number is reduced.\n\nWeakness:\n  1. The technics behind the theoretical results on the convergence rate of the proposed algorithm are well-known, which makes the theoretical result like Theorem 6 very incremental\n  2. The theoretical result to show that the curriculum learning could find a good sampling policy and reduce the relative condition number exponentially is only w.r.t. the naive random sampling policy, which is not usually used in practice. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has some misleading claim in the abstract about reducing the relative condition number exponentially by applying curriculum learning. It's only for the SP problem and w.r.t. the naive random sampling policy.\nI also suggest the author/s considering moving the related work section to the end, because it has some notations not defined before.\nFor the novelty part, please check my comments above.",
            "summary_of_the_review": "The paper focuses on finding the role of curriculum learning in helping the RL algorithms to solve the CO problems, which is well motivated and interesting. The paper has a few misleading notations but overall the paper is okay to read and understand. The insights of treating CO problems as LMDP is interesting but somehow straightforward. The convergence rate theoretical result is very incremental comparing with previous RL works in LMDP domain. For the curriculum learning role in the RL algorithms for solving the CO, it's interesting to see that it help reduce the relative condition number when using it as sampling policy, but the baseline is only the naive sampler which is not ideal when considering the practical usage. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2292/Reviewer_eGuH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2292/Reviewer_eGuH"
        ]
    },
    {
        "id": "qQhXuYi91n",
        "original": null,
        "number": 4,
        "cdate": 1667156980133,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667156980133,
        "tmdate": 1667156980133,
        "tddate": null,
        "forum": "pYC3W83uwm",
        "replyto": "pYC3W83uwm",
        "invitation": "ICLR.cc/2023/Conference/Paper2292/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an exploration of the combination of curriculum learning for on-line reinforcement learning problems by application to two related combinatoric optimization problems.  Central to the approach is to solve for a mixture $w_m$ of MDP models called a \"Latent Markov Decision Process (LMDP) (Kwon et al., 2021a)\" by the relative condition number \u03ba is reduced. \n\nCurriculum learning, as popularized in Bengio (2009), presents an approach to non-convex optimization in learning problems, that takes advantage of training on progressively more challenging subsets of cases. Insights from the domain inform what makes cases  more challenging. This has a regularization effect, in addition to guiding - or \"shaping\" the optimization toward desired solutions. \n\nOnline combinatoric problems, that have been termed \"generalized assignment problems\" (GAP) (Albers 2021), attempt to rank items where the policy is restricted to making a selection \"without recall\" -- once an item is passed over, it may not be chosen.  These belong to a class of optimal stopping sequential decision problems. ",
            "strength_and_weaknesses": "The memorable take-away from the paper is the use of \"Latent Markov Decision Process (LMDP)\" to study sequential decision problems where the state cannot be fully observed, as opposed to trying to express these as partially  observable MDPS (POMDPs.) Solutions to  no-recall GAP problems is a vital area and deserves study. \n\nCurriculum implies exploiting a heuristic about problem complexity - to relate an evolution of simplicity versus complexity during training.  The only description of the curriculum I find is in the description of experimental results, that \"There is no explicit relation between the curriculum and the target environment, so the curriculum can be viewed as random and independent. \"  So as stated, \"even if the curriculum is randomly generated\" shouldn't this be viewed as a kind of random-restarts approach? \n\nThere is also the question of reframing GAP problems as RL-style problems that either implicitly or explicitly consider the probability distribution of samples. In related work \"Currently, Online Matching and Online Knapsack have only approximation algorithms (Huang et al., 2019; Albers et al., 2021).\"    This is true, but \"online matching -- eg. the SP -- is underspecified as a probability model. These \"generalized assignment problems\" (Albers 2021) do not consider the probability distribution of samples.  In short, one needs to make an assumption about the sample, as the authors do,  that the sample distributions are relevant; value and size distributions were assumed uniform (Section 3.2).   As Granger (\"Optimal Statistical Decisions, 1970, p. 331) states \" In the context of this fantasy (that only information received is the rank of the sample) it is assumed at the beginning of the process (one) knows nothing about the quality\" (e.g the distribution they are drawn from) of the n items one will see.   One may argue this is unrealistic, more importantly it obscures much of what's going on in a sequential process. \n\nAs a minor point, for the statement  \"As surveyed in Bengio et al. (2009), Curriculum Learning has been applied to training deep neural networks and non-convex optimizations and improves the convergence in several cases.\" I think Bengio (2020) is the citation of the survey you mean. DNNs were not prevalent in 2009, and the 2009 paper works merely with a 3-level NN. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper meanders through algorithmic approaches with little insight into the performance they offer in the results.  To be true, I cannot follow how the algorithm described in Section 4 fits into a conventional RL approach. \n\nAside from the question of how curriculums are defined, What exactly does \"history-independent\" mean? Is this about knowledge of the state of previous stages?  In MDP's an optimal policy only need depend on the current state, unlike when the state is unobservable.  What then is the condition for LMDPs? \n\nA few words that describe the relevance of using Natural Policy Gradients would also help clarify the  paper's approach. ",
            "summary_of_the_review": "It's a credit to the authors to have made realistic claims, and to have provided valid evidence to support them.  Granted that the work is correct, the results are not substantial.  Again one recognises as stated that this is exploratory work on a novel formulation.  I would question how the problem is approached, and wonder if as stated it does not admit of a clean solution.   ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "Just a small point -- common usage is and has been to refer to the \"secretary problem\" by other names that use less culturally loaded terms.  ",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2292/Reviewer_1B7j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2292/Reviewer_1B7j"
        ]
    }
]