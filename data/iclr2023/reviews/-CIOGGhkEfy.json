[
    {
        "id": "IYy0RsTdmQP",
        "original": null,
        "number": 1,
        "cdate": 1666286532248,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666286532248,
        "tmdate": 1667003208776,
        "tddate": null,
        "forum": "-CIOGGhkEfy",
        "replyto": "-CIOGGhkEfy",
        "invitation": "ICLR.cc/2023/Conference/Paper1445/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Traditional training-set attacks manipulate model predictions by inserting adversarial instances directly into the training set.  This paper proposes a novel threat model for training-set attacks whereby the adversarial training instances are created during the training process itself via a malicious augmentation scheme.  \n\nThe authors propose three novel augmentation-based attacks.  The simplest attack is non-clean label and has a substantial adversarial trigger. The second attack is clean-label, with an adversarial trigger that remains perceptible. The third attack is clean-label and close to the true data manifold.",
            "strength_and_weaknesses": "#### *Strengths*\n* The attack threat vector is -- to the extent of my knowledge -- novel.\n* The first-order motivation of the paper (mostly) holds and is probably an unconsidered threat model in practice.  Many practitioners use off-the-shelf augmentation routines without verifying their exact procedure.\n#### *Weaknesses*\n* The first two attacks proposed by the authors make clearly perceptible perturbations. This restricts their stealthiness.\n  * The *Simple Transform Attack* assumes access to the training instances' labels.  Most augmentation schemes only have access to the feature vector not the label so this attack is especially unrealistic.\n* While the paper's basic motivation is largely true, in the broad scheme, open-source augmentation schemes are not complete black boxes. They often undergo code review and other verification where an augmentation backdoor may be discoverable. This mitigates some of the vulnerabilities since the attack could be mitigated by human inspection of the augmentation implementation even if the inserted augmentations are imperceptible.\n* I would have preferred evaluation on ImageNet rather than simpler CIFAR/MNIST.  Given the simplicity of the idea and its relative obviousness, this is not a critical limitation of the paper but is still a noteworthy omission.\n* The claim that the GAN-based augmentation is only detectable through the weights is somewhat overstated.  Often the code to train the GAN would be open-source not just the model parameters so someone who is security conscious could inspect the training code for vulnerabilities and then train the GAN themselves.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality is adequate.  Specific aspects that should/could be improved include:\n* The paper emphasizes demonstrating the three attacks visually (Fig. 3-5).  The paper would be better served with longer, more detailed exposition describing and providing intuitions about the attack.   \n* Figure 2 does not significantly improve understanding or insight into the topic.  It appears mostly like filler.  \n* Figure 6 is not intuitive and took longer to understand than it should.  Reformulating and redrawing the image may significantly improve its readability. (Note the inconsistent capitalization of *Augmix* in Figure 6).\n* Sec. 4: \"*Trigger accuracy*\" is not terminology I recall seeing in other papers.  I more commonly see this referred to as \"attack success rate\"\n* Sec. 4: I did not see where the notation $\\Delta$ is defined.  I assume this is the change in clean accuracy when training with the alternate backdoors.\n\nThe contributions of this work, while novel, are not particularly insightful.  The evaluation results are what one would expect.  They do not offer especially meaningful insights nor do they advance the state of the art.  This paper's lack of overall novelty is by far my largest concern.\n\nA non-exhaustive list of typos:\n* Pg. 10: \"...*at at*...\"\n* Pg. 12: \"*arcitecture*\"",
            "summary_of_the_review": "While the attack vector used in this paper is novel, the results are foreseeable and expected.  In short, the attack repackages existing attacks into augmentations rather than creating discrete attack instances.  This reduces the paper's overall contributions and novelty -- in my view substantially.  Therefore, I do not see the (marginal) contributions as sufficient for inclusion in ICLR.  \n\nThis paper has merit and is worth publication, albeit at a different venue.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "With any new attack, there is a non-zero risk that a malicious attacker could use the proposed method for nefarious ends. This paper is not an exception to this general rule.  This paper should go through the standard ethics review like any other attack paper.  To the extent of my understanding, that equates to no additional ethics-related scrutiny. ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1445/Reviewer_jPb2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1445/Reviewer_jPb2"
        ]
    },
    {
        "id": "PpXYoZ4u9RA",
        "original": null,
        "number": 2,
        "cdate": 1666589046214,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589046214,
        "tmdate": 1666589046214,
        "tddate": null,
        "forum": "-CIOGGhkEfy",
        "replyto": "-CIOGGhkEfy",
        "invitation": "ICLR.cc/2023/Conference/Paper1445/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces backdoor attacks on neural networks via data augmentation procedures. The authors consider backdoors related to simple image transforms, GAN-based augmentation, and composition-based augmentation. The method is evaluated in these three scenarios, and shows interesting performance.\n",
            "strength_and_weaknesses": "The main strength of the paper is in its motivation for inserting backdoors via augmentation, and the interesting approaches to exploit various augmentations to that end.\n\nOne of the weaknesses of the paper lies in the related work discussing backdoor attacks. Unfortunately, the discussion there is somewhat vague (``However, some of these attacks...''), and makes it difficult the properly position this manuscript in the context of prior work. For instance, the papers by Quiring et al., Gao et al.. and Wu et al. seem extremely related, but their discussion is limited to 2-3 sentences (and evaluation is not comparing to these approaches). Why? What are the novel differences between the proposed method and prior work?\n\nAnother issue is with the correctness of the approach in the setting of simple image transforms. It seems like some of the augmentations (e.g., rotation) may be related to data existing in the train set, no? What happens then in terms of the backdoor? Do we get inconsistent behavior for these examples?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear, and it is written in an easy-to-follow manner. I believe it is reasonably reproducible, but providing the code will definitely help. The novelty of the paper depends on the proper positioning of the paper with respect to prior work, which is somewhat unclear given the current text.",
            "summary_of_the_review": "Overall, the authors propose three different straightforward methods to exploit augmentations in order to introduce backdoor attacks to the network. The method is evaluated on relatively simple benchmarks, and compared to a minimal set of approaches. Given that backdoor attacks are discussed in the literature since 2017, I would have hoped for a more thorough treatment regarding prior work, and a more extensive evaluation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1445/Reviewer_hnQ2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1445/Reviewer_hnQ2"
        ]
    },
    {
        "id": "hxzwqe4EOs",
        "original": null,
        "number": 3,
        "cdate": 1666838163593,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666838163593,
        "tmdate": 1668525048884,
        "tddate": null,
        "forum": "-CIOGGhkEfy",
        "replyto": "-CIOGGhkEfy",
        "invitation": "ICLR.cc/2023/Conference/Paper1445/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to add backdoor by designing some specific \"malicious\" data augmentation methods. Some important parts of the paper is not clear to me. Please see my comments below. ",
            "strength_and_weaknesses": "I have the following questions:\n\n1. In traditional backdoor attacks, an identical transformation T is applied on both poisoned training and poisoned test samples. This is practical, for example in BadNet, where we can put the square backdoor sticker onto a stop sign. However, in your methods, is the backdoor transformation T also applied on test images? If yes, why is this practical for your cases (where you use rotation/GAN/AugMix/etc)? The attacker can add those augmentation operations during training under your problem setting. But why they are also used in testing? Is it possible for the attacker to rotate/color invert a physical stop sign? \n\n2. I don't understand the visualization in Figure 5. Does the right side shows augmented images after \"backdoored AugMix\" images? If yes, I think this backdoor attack is too obvious. Usually backdoor attacks need to be concealed to bypass human inspection. Also, will these seemingly random noise images after backdoored AugMix lead to an acceptable clean accuracy like traditional backdoor attacks? \nOne related work [1] regarding learning the adversarial AugMix parameters is missing. \n\n[1] AugMax: Adversarial Composition of Random Augmentations for Robust Training. NeurIPS, 2021. \n\n3. Can the proposed new attacks bypass state-of-the-art backdoor defense methods [2,3,4]?\n\n[2] Anti-Backdoor Learning: Training Clean Models on Poisoned Data. NeurIPS, 2021.\n[3] Adversarial Unlearning of Backdoors via Implicit Hypergradient. ICLR, 2022.\n[4] Trap and replace: Defending backdoor attacks by trapping them into an easy-to-replace subnetwork. NeurIPS, 2022. \n\n4. Do the proposed new attacks outperform state-of-the-art attacks (such as [5]) in terms of attack success rate and clean accuracy?\n[5] Rethinking the Backdoor Attacks\u2019 Triggers: A Frequency Perspective. ICCV, 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "I feel a bit confused after reading the first draft. Please answer my above questions before my final evaluation. ",
            "summary_of_the_review": "I feel a bit confused after reading the first draft. Please answer my above questions before my final evaluation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1445/Reviewer_Z2bo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1445/Reviewer_Z2bo"
        ]
    }
]