[
    {
        "id": "oUFL7ESut2",
        "original": null,
        "number": 1,
        "cdate": 1666578718477,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578718477,
        "tmdate": 1666578718477,
        "tddate": null,
        "forum": "ncQCD9M8SwT",
        "replyto": "ncQCD9M8SwT",
        "invitation": "ICLR.cc/2023/Conference/Paper5556/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Prior work in continual learning has focused on addressing the catastrophic forgetting (CF) problem. However, this paper notes that \"knowledge transfer\" (KT) between tasks is also important, and proposes a new method which \"addresses both issues.\" \n\nTo address CF, each task is associated with a learned mask identifying a task-specific sub-network. The mechanism by which this is accomplished is \u201cborrowed from (Wortsman et al., 2020)\u201d and consists of boolean gates indicating which parameters in the network should be employed for which tasks.    \n\nTo address KT, the idea is to measure the importance of the mask of the previous task relative to the current task. This uses an idea from network pruning (Michel et al., 2019) and uses gradients as measure of parameter importance. A contribution of this work is to extend this idea to entire task-specific masks.\n\nThe experiments consist of five benchmarks: (1) 19 tasks for aspect sentiment classification, (2) 5 tasks for continual classification dataset, (3) 6 tasks from ConvoSum, (4) 5 tasks for dialogue response generation and (5) 5 named-entity recognition tasks. On these benchmarks, 11 baselines are compared, including continual learning and non continual learning methods.\n",
            "strength_and_weaknesses": "The experiments are fairly systematic in terms of considering a large number of baselines and datasets. However, it should be noted that the improvements over certain CL baselines like SupSup (Wortsman et al., 2020) are very similar in performance (in one case, SupSup achieves 11.63, while the proposed method gets 11.36 and is incorrectly labelled in bold as the best performance).\n\nI would have liked to see more motivation for the proposed masking approach. Specifically, looking at the gradients does not strike me as the most obvious or computationally expedient mechanism to compute task similarity. For example, task similarity could be measured instead on the basis of simple statistics of the feature representations (e.g., multivariate Gaussian fit to each task\u2019s activations).  In general, additional discussion of the limitations of the proposed approach and discussion of alternatives would benefit the paper.\n\nI would have liked to see a multi-task \u201coracle\u201d that has access to all tasks at once during training. This would help characterise the potential performance benefit of knowledge transfer separate from the impact of catastrophic forgetting.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The technical presentation was hard to follow at times. For example, exactly how masks are applied to parameter sets is not clear from this paper alone; I had to go read the cited papers to understand the ideas being applied here (Wortsman et al, 2020; Houlsby et al., 2019). In several places, the gradient is used to score the importance of different masks, but I had trouble figuring out what the gradient is being evaluated on, and then how it is aggregated across examples to yield a single task importance score. To this end, Figure 1 seeks to illustrate how the whole framework operates, but I didn\u2019t find it particularly helpful (it could also benefit from proper typesetting).",
            "summary_of_the_review": "Overall, this paper provides a sensible framework for continual learning, providing mechanisms to address both CF and KT. My main concern is the lack of novelty: the main technical contribution is how task similarity is estimated but even this is largely based on prior work. Furthermore, while experiments are fairly comprehensive, in some cases the improvement over baselines is inconsistent or relatively small (e.g., SupSup).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5556/Reviewer_Umdv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5556/Reviewer_Umdv"
        ]
    },
    {
        "id": "MuUkRTY47MA",
        "original": null,
        "number": 2,
        "cdate": 1667038152570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667038152570,
        "tmdate": 1667038242783,
        "tddate": null,
        "forum": "ncQCD9M8SwT",
        "replyto": "ncQCD9M8SwT",
        "invitation": "ICLR.cc/2023/Conference/Paper5556/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "- This works address two majors challenges in continual learning: catastrophic forgetting  and knowledge transfer (KT) across tasks applied to NLP tasks.\n- In doing so, the approach is based on sub-networks, and computes task similaity via gradients and mask importance where catastrophic forgetting is controlled using a different mask while knowledge transfer across tasks is encourage using the same mask across tasks.\n- The proposed approach is applied to several NLP tasks using 5 datasets, and evalued against exhaustive baselines. The paper show improved performance in terms of controlling catastrophic forgetting  and performing knowledge transfer (KT) across tasks\n- The paper is clearly written and well motivated \n",
            "strength_and_weaknesses": "Strengths:\n- paper address key questions in continual learning and applied to NLP tasks\n- the approach is incremental yet effective\n- novel ideas in computing task similarity using mask importance applied to presenting CF and encouraging knowledge transfer, however inspired from existing works\n- paper is well motivated and clearly written \n- evaluates the proposed approach on 5 different NLP data sets \n- paper employs a good set of related baselines to empirical comparisons \n- sound quantitate analysis and ablation study \n\nWeaknesses:\n- incremental novel steps (limited novelty) yet effective and simple \n- unclear / missing experimental setup: how is the data split over time across tasks in Cl setup? Does a different sequence of tasks leads to different performances?\n- unclear how the approach address the question: how new entities are learned over time, not seen in the historical tasks? How does the SoftMax adjust and the parameters shared over a sequence of NRE tasks as new entities evolve?\n\nQuestion:\n- See Weaknesses section \n- How does the approach scale to unsupervised tasks - such as topic modeling in continual learning paradigm [1]? How does the task similarity be applied on document/text latent representations: topics?\n- Include additional existing works such as [2] in continual topic modeling: Unsupervised Continual learning for NLP that overcomes CF via both regularization approach and data replay and computes task similarity using topics - that represents data documents [1] and perform KT.\nIt is interesting to see how the tasks similarity approaches scales to unsupervised continual learning (close to real world). \n\n\nReferences:\n[1] Topic modeling using topics from many domains, lifelong learning and big data. ICML 2014.\n[2] Neural Topic Modeling with Continual Lifelong Learning. ICML 2020.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: good\nNovelty: incremental\nReproducibility: yes",
            "summary_of_the_review": "Please see above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5556/Reviewer_k1vn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5556/Reviewer_k1vn"
        ]
    },
    {
        "id": "YWeV4LQwYZx",
        "original": null,
        "number": 3,
        "cdate": 1667140467293,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667140467293,
        "tmdate": 1670310828892,
        "tddate": null,
        "forum": "ncQCD9M8SwT",
        "replyto": "ncQCD9M8SwT",
        "invitation": "ICLR.cc/2023/Conference/Paper5556/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work introduces a new continual learning method TST.\nTST considers both preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT).\nTo prevent CF, TST directly use one existing CL method called SupSup.\nTo encourage KT, TST borrows the idea of gradient-based importance score and applies this score to assign sub-network masks for sequential tasks.\nExperimental results show that TST meets its motivation.\nConcretely, it improves the CL performance on similar tasks and keeps low forgetting rates.\n",
            "strength_and_weaknesses": "**Strength**\n\n* The motivation behind the design of TST (i.e., assigning sub-network masks based on task similarity) is promising.\n* The design of importance score (Equ. 2-5) and the choosing strategy (Equ. 7) is interesting. Moreover, the analysis results in Table 3 and 4 show the effectiveness of these designs.\n\n**Weakness**\n\n* The impt mask defined in Equ. 2 and used in Equ. 3 is a little weird. It seems that the elements in impt mask are not 0/1 and could be extremely larger than 1. Therefore, multiplying this mask to the original weight matrix may strongly affect the functionality of the original modular (i.e., the adapter), and the gradients (which will be used to calculate the importance score) can also be influenced.\n* Lack of further analysis of the mechanism behind TST.  I suppose the most important and interesting part in this work is the designs in Equ. 2-7. I agree that these designs sounds good, but I feel it is too amazing for a neural model to perform in a so interpretable way.\nThis is my major concerns to this work. I list several concrete questions as follows and will change my score according to your response.\n\n**Questions**\n\n* Q1: **Did you cherry pick the visualization in Figure 3?** Your analysis in 'Effectiveness of task similarity detection' tried to demonstrate that the actions (or called the choices of masks) in TST are in line with the intuition that the similar tasks share the mask while dissimilar tasks choose different masks.\nTherefore, I'm wondering if these analysis are from the cherry picked model or general ones.\nFor instance, for the two tasks 'icsi' and 'ami' in SUM, when the random seed and task sequence order are changed, can TST still find them similar?\n* Q2: **Does Equ. 7 work correctly at the beginning of the task sequences?** Since masks are randomly initialized, the importance scores calculated in the first several tasks could be hugely affected by this randomness, and the comparison in Equ. 7 might also be influenced. Moreover, I notice that for (a) (b) (e) in Figure 3, TST found most similar tasks at the beginning of the task sequences. Therefore, I'm wondering whether these 'similar tasks' are superficial correlations according to their order in task sequences.\n\n**Minor Questions**\n\n* How are the adapter used in TST trained? Since the paper claims that only the mask is trained, I wonder if the adapter is just randomly initialized without further training?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality**: The analysis are not clearly demonstrated.\n\n**Novelty**: The novelty of the proposed method is enough.\n\n**Reproducibility**: The author upload code for reproduction.",
            "summary_of_the_review": "**Post-revision update:**\nThe response make me further worried about the relation between the 'performance improvement' and the story-telling of 'task similarity detection'.\nThe insight about 'task similarity detection' is not well-supported.\nThus, I decrease the overall score.\n\n\nI have some concerns for the effectiveness of the proposed method and therefore give a borderline score.\nI list several questions and will change my score according to the response.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5556/Reviewer_iK2Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5556/Reviewer_iK2Y"
        ]
    },
    {
        "id": "yyuLNUfCvV",
        "original": null,
        "number": 4,
        "cdate": 1667590828927,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667590828927,
        "tmdate": 1667590828927,
        "tddate": null,
        "forum": "ncQCD9M8SwT",
        "replyto": "ncQCD9M8SwT",
        "invitation": "ICLR.cc/2023/Conference/Paper5556/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies the problem of task continual learning wherein new tasks are continuously learned in an incremental fashion. Specifically, the paper aims to achieve two key objectives for a successful continual learning system: preventing catastrophic forgetting of the previous tasks and enabling knowledge transfer to new tasks. To achieve this, the paper proposes the TST approach, Task-continual learning based on Sub-networks and Task similarity. The main idea is to start with a mask pool, then select the relevant mask for the given task with gradient-based importance measure, and fine-tune it with the current task data. With the proper selection of the relevant mask, one can alleviate the interference from unrelated tasks and simultaneously enable backward/ forward transfer to related/ current tasks. The paper conducts an extensive set of experiments on a wide range of NLP tasks - sentence-level classification, token-level classification, and generation and compares the proposed TST approach with relevant baselines. In summary, the paper shows that TST improves the overall accuracy of the system after sequentially training on different tasks, reduces forgetting, and in some cases (ASC and NER) even enables positive backward transfer.  ",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper attempts to solve an important problem of task continual learning with a specific focus on reducing forgetting and enabling forward transfer objectives. These objectives constitute important desiderata for realistic continual learning systems.\n\n2. The proposed TST approach employs parameter-efficient adapter modules to learn sub-networks for different tasks, enabling their approach to scale to a large number of tasks. Also, the proposed task similarity detection method based on gradients is easy to compute and seems effective.\n\n3. The paper conducts a rigorous set of experiments spanning different NLP tasks and compares their performance with several relevant baselines.\n\nWeakness:\n\n1. Although the paper conducts several experiments, there are some open questions regarding the statistical significance of the results provided in Table 1. Specifically, comparing the results with SupSup, it is unclear whether TST is the clear winner (see SUM, CCD, DRG). As the paper reports, the average results over 5 random task orderings, please consider reporting significant test results. \n\n2. Apart from alleviating forgetting of the previous task, the paper claims that the TST approach also encourages knowledge transfer. Results presented in Table 1 do not provide any convincing evidence for the same. The paper should consider reporting the learning accuracy as mentioned in [1]. For example, to understand the knowledge transfer from Tasks 1 and 2  while learning Task 3, one should consider reporting average performance for tasks after learning on their datasets. Also, if the paper argues that there is positive backward transfer in that case to compute the forgetting, the drop in the performance should be from the max performance for previous tasks and not just right after Task i. Please see the definitions of forgetting and learning accuracy in Eq(1) in [1] for reference.\n\nReferences: \n\n[1] Mehta, Sanket Vaibhav, Darshan Patil, Sarath Chandar, and Emma Strubell. \"An empirical investigation of the role of pre-training in lifelong learning.\" arXiv preprint arXiv:2112.09153 (2021).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. Moreover, the proposed solution is grounded in recent works on parameter-efficient transfer learning-based approaches. Also, the proposed mask importance measure is built on existing pruning literature and is successfully applied to the continual learning problem. The paper also provides its code thereby convincing the reader of the reproducibility of their experiments. \n\nTypos in Eq(4) and Eq(5): should it be $\\nabla_{g_{(k)}}^n$?\n",
            "summary_of_the_review": "Overall, I rate this paper marginally below the acceptance threshold. It is interesting to see the efficacy of the proposed approach for task continual learning. However, with the currently reported numbers, it is unclear how significant the improvements are over considered baselines. The addition of the statistical significance test results would be more convincing and I would be happy to increase my score during the discussion phase.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5556/Reviewer_ktyy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5556/Reviewer_ktyy"
        ]
    }
]