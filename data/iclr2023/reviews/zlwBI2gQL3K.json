[
    {
        "id": "TkMmnUv68jI",
        "original": null,
        "number": 1,
        "cdate": 1666491546028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666491546028,
        "tmdate": 1671497247225,
        "tddate": null,
        "forum": "zlwBI2gQL3K",
        "replyto": "zlwBI2gQL3K",
        "invitation": "ICLR.cc/2023/Conference/Paper315/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies the few-shot KG completion problem and proposes a hierarchical relation learning method for this task. The authors propose three levels of relational information to learn and refine the meta-representation of few-shot relations. Specifically, a contrastive learning-based context-level relation learning is proposed to update the entity embeddings, and a transformer-based triplet-level relation learning is proposed to learn the meta-relation representation, and an MTransD is proposed as the score function. The authors conduct experiments on widely used datasets.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is well-organized and the proposed method is easy to follow.\n2. The studied problem is an important research problem for KG completion.\n\nWeaknesses:\n1. The proposed method is based on previous works and uses different components to replace the existing ones. The incremental improvement shows the lack of novelty in this work.\n2. The motivation for the proposed method is not clear and hard to understand why the proposed design is necessary.\n3. The lack of an ablation study to demonstrate the superiority of the MTransD.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n1. The claim of \u201cthe potential of leveraging context-level relation information\u201d is inaccurate because the existing work aggregates information from the local neighbors.\n2. The authors claim the existing works lose critical information. First, I wonder how many triples will have shared information. Second, why the previous methods cannot capture the shared entity? I think the previous methods implicitly encode the shared entity in their embeddings. The claim needs more explanations.\n3. The motivation for using the transformer-based meta-relation learner is not clear. Why is the utilization of LSTM based on an unrealistic assumption and what is the impact caused by the insensitive of the size of the reference set and the permutation-invariant? And Why these two properties are essential?\n4. The advantages of the proposed MTransD are not clarified. Why not use the existing translation-based methods? And what is the disadvantage of these methods?\n5. The ablation study is necessary to demonstrate the advantage of MTransD compared with TransE, RotateE\u2026 the related algorithms.\n\nQuality and Novelty:\nThis work replaces the key components in existing works and proposes an incremental solution to improve the performance of few-shot KG completion. The proposed method is reasonable but lacks novelty.\n\nReproducibility:\nThe authors provide the data and code for reproducing their results.",
            "summary_of_the_review": "The paper proposes an incremental solution for the few-shot KG completion and lacks novelty to some extent. And some claims and motivations are not clear and need further explanations. Thus I think the quality of this paper is below the standard bar of ICLR and would not like to accept it.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper315/Reviewer_8oAh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper315/Reviewer_8oAh"
        ]
    },
    {
        "id": "Ue83y3aTZ0v",
        "original": null,
        "number": 2,
        "cdate": 1666642842598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642842598,
        "tmdate": 1666642842598,
        "tddate": null,
        "forum": "zlwBI2gQL3K",
        "replyto": "zlwBI2gQL3K",
        "invitation": "ICLR.cc/2023/Conference/Paper315/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a hierarchical relational learning framework for few-shot KG completion. The authors proposes jointly capturing three levels of relational information for enriching entity and relation embeddings, i.e., entity-level, triplet-level and context-level. Experiments on two benchmark datasets demonstrate the effectiveness of the proposed model.",
            "strength_and_weaknesses": "Strength:\n\n1. The idea of using multiple different levels of information for relational learning on KGs is interesting;\n\n2. The paper is well-written and the technical details are basically presented clearly;\n\n3. The experimental results demonstrate significant improvements over baseline methods.\n\nWeaknesses:\n\n1. The novelty of this work is limited;\n\n2. The term \"hierarchical\" in the title is confusing;\n\n3. Some of the technical details are not clearly presented;\n\nSee below for details of weaknesses.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The novelty of this work is limited. The authors propose using three levels of information: entity-level, triplet-level, and context level, which has been extensively explored in the literature (For entity-level and triplet-level, all KG models use those information, of course; for context-level, there has been a lot of work studying the context information of entities and triplets in KGs. All GNN-based methods can be seen as context-aware). The components of multi-head self-attention, contrastive learning, and translation-based KG representation learning (MTransD) are also extensively explored in the literature.\n\n2. The term \"hierarchical\" in the title is confusing. When I first read the title I thought that the authors somehow construct a hierarchical structure among relations. However, it actually refers to the three-levels of information. It may not be appropriate to call such information \"hierarchical\", since there is no hierarchy here.\n\n3. When using contrastive learning to construct a false context for a given triplet, are all (r, t) pairs in context corrupted, or only one (r, t) pair is corrupted? If all (r, t) pairs are corrupted, this false sample may be too weak and can be easily identified, but if only one (r, t) pair is corrupted, this context may be too similar to the true one and is hard for the model to distinguish.\n\n4. For the two parts of the loss, i.e., Q_r and L_c, are entity and relation embeddings shared across the two modules, or they have two sets of separate embeddings?\n\n5. For the MTransD part, did the authors try other translational-based methods, such as TransE, TransR, TransH?\n\n6. For the context, the authors only consider one-hop neighbors as the context information, but prior work has shown that more neighbors are helpful to model entities and triplets. It is recommended that the authors try to increase the hop of neighbors to see if the model performance can be further improved.\n",
            "summary_of_the_review": "This paper studied an interesting problem in KG representation learning. The paper is basically well-written and the experimental results are promising. However, the technical novelty is limited, and some of the technical details are not clearly presented. Overall, I think this paper is marginally below the acceptance threshold of ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper315/Reviewer_HQvj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper315/Reviewer_HQvj"
        ]
    },
    {
        "id": "N61sE5n3ZKg",
        "original": null,
        "number": 3,
        "cdate": 1666848256538,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666848256538,
        "tmdate": 1666848256538,
        "tddate": null,
        "forum": "zlwBI2gQL3K",
        "replyto": "zlwBI2gQL3K",
        "invitation": "ICLR.cc/2023/Conference/Paper315/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "- Real-world KGs are incomplete and suffer from the long-tail distribution over the relations.\n- Thus the performance (on completion tasks) on the low-frequency relations is poor.\n- Prediction of the tail entity given the (head, relation, ?), is considered a few-shot completion problem, in which one could try to learn meta representation for these relations from the limited amount of reference (head, relation, tail) observed.\n- In order to learn the meta-relation representation, the authors try to jointly capture three levels of relational information:\n  a. Context level b. Triplet level c. Entity level.\n- Finally, they propose a meta-learning-based optimization approach to obtain an effective embedding for the tail relation that achieves a few-shot generalization",
            "strength_and_weaknesses": "Strength:\n1. The authors very succinctly point out the gap in the existing few-shot paradigm, viz, inconsistent sequential assumption, and lack of context awareness. \n2. Their extensive ablation study indicates the necessity of each component of the model, e.g., the set-based transformer used in triplet-level relation capture was justified by introducing simple sum and LSTMs.\n\nWeakness:\n A few places where complicated model choices were not motivated properly. For example, why authors picked the TransE or distance-based framework for entity-level generalization? The pairwise interaction of the triples could have been done using any sort of permutation invariant operation, why SAB was picked?",
            "clarity,_quality,_novelty_and_reproducibility": "N/A, see weakness.",
            "summary_of_the_review": "The proper correctly identifies the problem with the existing approaches and addresses them with noble techniques. For that reason I feel the paper is a suitable candidate for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper315/Reviewer_b64W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper315/Reviewer_b64W"
        ]
    },
    {
        "id": "njBG88-OAz",
        "original": null,
        "number": 4,
        "cdate": 1667503030419,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667503030419,
        "tmdate": 1667545822780,
        "tddate": null,
        "forum": "zlwBI2gQL3K",
        "replyto": "zlwBI2gQL3K",
        "invitation": "ICLR.cc/2023/Conference/Paper315/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes novel methods for few-shot KG completion. They identify two issues with existing methods for this task -- (a) They learn entity-level information from local nbr aggregators. (the paper jointly takes into account the nbr of head and tail entity for triple context) (b) They learn relation-level information using a sequential model (while the sequentiality assumption is invalid -- the paper use transformers instead). The authors proposed HiRe, which takes into account triplet-level interactions, context-level interactions, and entity-level information for query (h,r,?). MAML based training strategy used to train the model. The model shows improved performance on 2 benchmark datasets - Nell-One and Wiki-One. The ablation study demonstrates the importance of the key components of the model, where transformer-based MRL outshined.",
            "strength_and_weaknesses": "The paper address an important problem in KBC. It identifies valid issues in existing methods. However:\n1. Paper writing can be improved. Certain sections are hard to understand (especially sections 4.2/4.3/4.4). Since these sections form the core of the paper it is imperative to write them very clearly. \nAdding details of MAML training startegy, MSA and SAB in appendix should be helpful.\n2. \\bigoplus used before definition\n3. Some comments on the scalability of the model will be insightful.\n4. Why did the authors not look at (?, r, t_j) queries along with (h_j, r, ?) and report the mean (the standard way of evaluation in KBC).\n5. Definition 2: Consider adding an \\exist r.\n6. Some understanding of how the models perform on conventional KG completion datasets (where relations are associated with many more triples) is also important.\n7. Authors should use a more competitive version of ComplEx (ComplEx-N3) for comparison. (See Lacroix, Timoth\u00e9e, Nicolas Usunier, and Guillaume Obozinski. \"Canonical tensor decomposition for knowledge base completion.\" International Conference on Machine Learning. PMLR, 2018.)\nAlso why did the authors choose a translation model (sec 4.3) in place of more competitive KBC models like ComplEx?",
            "clarity,_quality,_novelty_and_reproducibility": "\n1. It is imperative to share the code of the model, to be able to reproduce the results.\n2. The paper identifies valid gaps/issues in previous techniques and provides sensical first steps to address them.\n3. Further see answer to Q2",
            "summary_of_the_review": "I feel the paper has interesting contribution but I believe the quality of paper writing needs to be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper315/Reviewer_Pexb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper315/Reviewer_Pexb"
        ]
    }
]