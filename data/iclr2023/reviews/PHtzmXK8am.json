[
    {
        "id": "O6bNfUSGkMc",
        "original": null,
        "number": 1,
        "cdate": 1666573091504,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573091504,
        "tmdate": 1671351027164,
        "tddate": null,
        "forum": "PHtzmXK8am",
        "replyto": "PHtzmXK8am",
        "invitation": "ICLR.cc/2023/Conference/Paper3020/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Recent work shows that large batch size can give better privacy-utility trade-off in differentia private ML training. However, training with large batch sizes is computationally expensive. The paper observes (empirically) that the privacy budget of differential private ML training depends mostly on the total amount of noise added during training when the noise multiplier \\sigma>=2. At the same time, the accuracy of the final results can be predicted by the results on a corresponding hyper-parameter setting in which the batch size is small. Following these observations, the paper proposes to do hyper-parameter tuning for small batch sizes (which is computationally efficient) and then transform the hyper-parameters to the target large batch size. Experiments demonstrate that this approach works well in practice.",
            "strength_and_weaknesses": "\nStrength:\n\n* The observations (batch scaling law, privacy budget computation) in the paper are new and interesting.\n\n* The proposed hyper-parameter tuning process can be very useful in practice, as it saves the computation requirement and therefore speeds up the development process.\n\nWeaknesses:\n\n* Overall, the writing is clear, but there are still some issues that need to be addressed:\n    - Equation 2: many of the notations here are not defined, e.g., clip_C, l, theta, C.\n    - Figure 4: B_ref, S_ref, \\sigma_ref are not defined. I can guess their meaning from the context, but all notations should be defined explicitly in the text.\n    - Page 5: it says \"Simultaneously doubling the batch size and \\sigma has a negligible or small impact on accuracy (Figure 4)\". However, in Figure 4(b), the impact on the accuracy is not small. Indeed, this is claimed in the next sentence \"Reciprocally, if \\sigma > 4, dividing it by 2 simultaneously with halving q is likely to improve performance.\" These two sentences contradict each other.\n    - Table 1: I guess \"8,8.\" should be \"8,\"?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity: The paper is overall well-written and clear.\n\nQuality: The observation and the proposed approach are interesting and useful, and the claims are supported by experiments. Overall the quality is good.\n\nReproducibility: the code of the experiments is not provided.\n\nNovelty: The observation is new and the proposed approach is novel.\n",
            "summary_of_the_review": "\nOverall, the paper is well-written and the observations and the proposed approach can potentially be very useful for the community. It would be even better if the authors can demonstrate/verify the observation on more network architectures, datasets, and models (e.g., generative models).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3020/Reviewer_MS1Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3020/Reviewer_MS1Z"
        ]
    },
    {
        "id": "DFJvK7iLLI",
        "original": null,
        "number": 2,
        "cdate": 1666625958770,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625958770,
        "tmdate": 1671351225891,
        "tddate": null,
        "forum": "PHtzmXK8am",
        "replyto": "PHtzmXK8am",
        "invitation": "ICLR.cc/2023/Conference/Paper3020/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\n\nThe paper studies how to transfer the hyperparameters of differentially private training based on low-cost search. It shows the privacy budget only depends on the total amount of noise (TAN) injected throughout training. They derives a scaling law for training models with DP-SGD with more than a 100 times reduction in computational budget.",
            "strength_and_weaknesses": "Strength: It studies an important problem of how to scale DP-SGD with hyper-parameter search. The paper provides a neat and simple relationship between the privacy budget and the other parameters, i.e., the noise multiplier, steps and the sampling rate. This relationship motivates a way of scaling the batchsize and the noise multiplier without affecting the privacy accountant.\n\nWeakness: \n1. There is no application of using TAN relation in language task, either pretraining or fine-tuning.\n\n2. The reference needs to cover more. Especially, Yu et al. 2021 starts the fine-tuning large language models with differential privacy and low-rank reparametrization. \nYu et al. 2021 Large Scale Private Learning via Low-rank Reparametrization.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the finding is novel and useful for tuning DP-SGD.",
            "summary_of_the_review": "How to tune DP-SGD is both theoretically and practically important for private learning. The paper makes a clear contribution towards tuning DP-SGD in principle. The concluding solution is solid and well-supported. I recommend its acceptance.\n\n\\#### After rebuttal and discussion with other reviewers \\#####\nAs discussed with other reviewers, the relation of TAN with privacy accounting has been derived in literature e.g., Lemma 2 and Proposition 3 of Bun et al. 2020. The paper is not fully aware of the literature, which undermines the novelty of the contribution. I change the score to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3020/Reviewer_AFbM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3020/Reviewer_AFbM"
        ]
    },
    {
        "id": "znFh_byyNT6",
        "original": null,
        "number": 3,
        "cdate": 1666694311835,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694311835,
        "tmdate": 1666695027613,
        "tddate": null,
        "forum": "PHtzmXK8am",
        "replyto": "PHtzmXK8am",
        "invitation": "ICLR.cc/2023/Conference/Paper3020/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed to use the total amount of noise (TAN) to determine the privacy budget in Renyi DP. They observe scaling laws with TAN for DP-SGD which can be used to reduce the computational cost for hyper-parameter tuning. By the hyper-parameter tuning, the authors greatly improve the state-of-the-art on neural network training under privacy guarantees.",
            "strength_and_weaknesses": "Major strengths:\n1. The observation of the scaling law is new. \n2. The results of hyperparameter tuning improve the state-of-the-art greatly.\n\nMinor strengths:\n1. This paper is easy to follow.\n2. The visualization of the experimental results is good.\n3. Figure 2 seems to deliver the same message as the linear part of Figure 3 (left) in (Li et al, 2021a), but the authors provide more explanations in Section 3.2.\n\nMajor weaknesses:\n1. The privacy guarantee parameter $\\varepsilon_{TAN}$ is an approximation of $\\varepsilon_{RDP}$ and it is only a lower bound. For strict privacy guarantees, we need an upper bound.\n2. There is no explanation for the reason for the scaling law. \n\nMinor weaknesses:\nSome of the statements in this paper are inconsistent with each other. \n1. In the left figure of Figure 4, the legend shows that $\\sigma_{ref}$ is constant for each line, but the caption says the $\\eta_{step}$ is constant. I also wonder why the authors choose to fix different values in Figure 4 for CIFAR-10 and ImageNet. Is it because the results of CIFAR-10 do not look as linear as of ImageNet?\n2. In Page 5, Choice of $\\sigma$, the authors claimed that if $\\sigma>4$, dividing it by 2 with halving q is likely to improve performance, but I cannot find experimental results supporting this.\n3. In Page 7, the authors say that they decide not to use the gain from 'testing with augmentations' in Table 1 and Table 4. In Table 2, I guess the Total column is the sum of the other columns for B=128, ..., 1024. Therefore, the last row, B=16384, also has the AugTest (+0.8) in the Total result (6.7%). I guess in Table 4, the test ACC for B=16384, 36.9%, is derived by adding the total improvement 6.7% to the baseline 30.2%. Therefore, I am not sure if the authors forget to remove the improvement from AugTest.\n4. In Page 9, in the last paragraph, the authors believe that the exponential increase in the privacy budget $\\varepsilon$ as the noise level $\\sigma$ decreases. I guess this is from Figure 2, but the x-axis for $\\sigma$ is log-scale. Therefore, it is not easy to identify whether there is a linear or exponential relationship between $\\varepsilon$ and $\\sigma$.\n\nBy the way, there are duplicate references (Li et al, 2021a and 2021b).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written. The statements in the paper are clear and concise. The novelty of this paper is good (see the strengths). The code is not provided, and the computation cost of the comparison experiments in this paper seems to be high; therefore the reproducibility is low.",
            "summary_of_the_review": "This paper demonstrates the scaling law between the batch size and the test accuracy in private deep learning. This law and the new concept, TAN, are very useful in improving private learning results by DP-SGD when the computation resources are limited. This law is found in the results of ImageNet and CIFAR-10 experiments, but the reason behind it is still unknown and it may be difficult to reproduce the results since the code is not provided. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3020/Reviewer_FyxV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3020/Reviewer_FyxV"
        ]
    },
    {
        "id": "7NC5gPCt6b",
        "original": null,
        "number": 4,
        "cdate": 1666796721144,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666796721144,
        "tmdate": 1666796721144,
        "tddate": null,
        "forum": "PHtzmXK8am",
        "replyto": "PHtzmXK8am",
        "invitation": "ICLR.cc/2023/Conference/Paper3020/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors argue that our ability to optimize a model trained with DP-SGD is governed by a quantity they call TAN (the total amount of noise), specifically $\\Sigma^2 = 2 \\sigma^2 N^2/(B^2 S)$, where $\\Sigma$ is the TAN, $\\sigma$ is the scale of the added Gaussian noise, $N$ is the dataset size, $B$ is the batch size and $S$ is the number of updates.\n\nAdditionally, they show empirically that when $\\sigma \\gtrsim 2$, the privacy parameter $\\epsilon$ becomes a simple function of $\\Sigma$ and $\\delta$. Since it is standard to set $\\delta \\approx 1/N$, in practice this means that the privacy is determined solely by $\\Sigma$, the TAN. Meanwhile, the privacy loss $\\epsilon$ is very large when $\\sigma \\ll 2$.\n\nThe authors propose to exploit this observation by tuning hyper-parameters with small $\\sigma$, small batch size $B$ and large privacy loss $\\epsilon$, before extrapolating those hyper-parameters to $\\sigma \\gtrsim 2$ while increasing the batch size to ensure that the TAN $\\Sigma$ is constant. This significantly reduces the privacy loss $\\epsilon$ however it also significantly increases the compute cost of training. Extrapolating hyper-parameters from cheap non-private training runs to expensive runs with tight privacy guarantees reduces the total computation required to achieve strong performance with DP.",
            "strength_and_weaknesses": "Strengths:\n1) Integrating the constraints of DP-SGD with optimization theory, in order to reliably predict good hyper-parameter choices, is one of the most important problems that needs to be solved in order to make differentially private deep learning practical. I'm pleased to see the authors tackle this difficult and important problem.\n2) The metric proposed by the authors is intuitive and it appears to provide some helpful intuition.\n3) The authors achieve strong performance when training from scratch on ImageNet with DP-SGD.\n\nWeaknesses:\n1) If TAN were an accurate measure of optimizability, then we should expect the train accuracy to be roughly constant at fixed TAN. However in practice, the authors find in figure 1 that the test accuracy decays log-linearly at fixed TAN as the batch size rises. I feel that the current paper is quite confusing because this discrepancy is not explicitly stated anywhere. I'd encourage the authors to make this explicit, and to explore in more detail why this discrepancy arises (eg comparing test vs train performance). \n\n2) As discussed above, Figure 1 shows that TAN over-estimates the performance of large batch sizes. It would be nice to also see a figure at fixed batch size, sweeping the number of steps S, to see whether TAN under/over-estimates the performance of small/large step budgets.\n\n3) As a minor point, note that on some truly sensitive data we cannot train non-private models (even internally). It would be good to acknowledge that the hyper-parameter transfer process described here cannot be used in these cases.\n\n4) For many of the experiments, key hyper-parameters are lifted from prior work (eg step budget, target batch size). Is it not possible to directly infer good settings for these hyper-parameters using the TAN framework?\n\n5) The paper essentially only considers ImageNet (with some very limited experiments on CIFAR-10). Since very few groups have run experiments at this scale it is not clear how challenging the baseline from De et al. is in practice.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nI found the paper difficult to follow. I think this is because the authors do not clearly acknowledge that, if TAN were an accurate measure of trainability, then test accuracy should not fall as batch size rises (at constant TAN) in figure 1. It is also not always clear which hyper-parameters are extrapolated using the TAN framework, and which are lifted from prior work or tuned in the large batch/low privacy loss regime.\n\nQuality:\nI think the underlying quality of the work is quite high (if the clarity can be improved).\n\nNovelty:\nThis is the first paper I am aware of to provide an explicit toy model predicting the trainability of DP-SGD. Although TAN has some flaws, I think this is a novel and worthwhile research direction.\n\nReproducibility:\nI'm not sure how easy it would be to reproduce the results",
            "summary_of_the_review": "On the positive side, the authors identify an important and difficult problem, and provide some useful insights. However on the downside TAN is not a reliable measure of performance in practice, and the authors only achieve strong results on one dataset so it is not clear how reliable the methodology is.\n\nI have scored the paper weak accept, however I would encourage the authors to improve the clarity of the writing and to empirically explore in more detail how test accuracy depends on (batch size, step budget, added noise) under the constraint of constant TAN.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3020/Reviewer_6jSv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3020/Reviewer_6jSv"
        ]
    }
]