[
    {
        "id": "OmuUC31DAjz",
        "original": null,
        "number": 1,
        "cdate": 1666253060865,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666253060865,
        "tmdate": 1666253060865,
        "tddate": null,
        "forum": "ESR6hysKDsW",
        "replyto": "ESR6hysKDsW",
        "invitation": "ICLR.cc/2023/Conference/Paper2894/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "For the possible repetition in Continual Learning, the paper makes contributes in the following three areas: (1) The author proposes two stochastic scenario generators that produce a wide range of CIR scenarios starting from a single dataset and a few control parameters. (2) The author conducts a comprehensive evaluation of repetition in CL by studying the behavior of existing CL strategies under different CIR scenarios. (3) The author presents a novel replay strategy that exploits repetition and counteracts the natural imbalance present in the stream.",
            "strength_and_weaknesses": "Strengths: The paper proposes two practical CIR generators and performs a comprehensive series of comparative experiments of existing CL methods, which provides the CL area with a referenceable basis for further research. \n\nWeaknesses: The analysis of the FA storage policy is not sufficient. In the paper, the FA method is only proposed based on ER. Can the FA method be applied to other CL methods? How does the ER-FA method perform when the dataset is not unbalanced? Does FA cause a decrease in class accuracy for frequent classes? The author may want to respond to the weakness the reviewer raised.\n",
            "clarity,_quality,_novelty_and_reproducibility": "In terms of algorithms, the paper describes the steps of the algorithms in detail so that the algorithms can be clearly understood.\n\nIn terms of experimental results, the paper provides multiple experiments in various settings and uses high-quality graphs to present the experimental results.\n\nIn terms of novelty, the paper proposes two novel CLR generators and an improved FA method on ER method.\n\nIn terms of reproducibility, the author promises to provide an open-source implementation of our generators and algorithms with the scripts needed to reproduce the results reported in the paper. \n\nIn summary, the paper is well above average in terms of clarity, quality, novelty, and reproducibility. \n",
            "summary_of_the_review": "Class-Incremental with Repetition (CIR) scenarios are important in realistic scenarios but have not been sufficiently studied so far. This paper proposes two practical CIR generators and performs a comprehensive series of comparative experiments of existing CL methods, which provides the CL area with a referenceable basis for further research. Therefore, it is recommended to accept this paper if the questions about the weaknesses can be answered.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2894/Reviewer_U14g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2894/Reviewer_U14g"
        ]
    },
    {
        "id": "LPxpGV8u8CC",
        "original": null,
        "number": 2,
        "cdate": 1666369043838,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666369043838,
        "tmdate": 1666369043838,
        "tddate": null,
        "forum": "ESR6hysKDsW",
        "replyto": "ESR6hysKDsW",
        "invitation": "ICLR.cc/2023/Conference/Paper2894/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on class-incremental with repetition (CIR), which ranges from class- and domain-incremental learning. For building streams and managing repetition over time, the authors propose two CIR generators (i.e., slot-based and sampling-based), which are from two aspects to generate data streams. Moreover, the authors propose a frequency-aware (FA) storage policy tailored to CIR scenarios. Further, the authors perform the first comprehensive evaluation on CIFAR 100 and TinyImageNet to compare other replay approaches in the CIR scenario, and then show the effectiveness of FA.\n",
            "strength_and_weaknesses": "Strengths\n1. The idea of CIR is absorbing and reasonable. Because for realistic data streams, it is natural to occurrences previously seen classes in new coming samples. Further, they designed a paradigm to build a stream with repetition via the proposed generators. \nWeaknesses\n\n1. The novelty of this paper is insufficient. In my view, this paper conducted a sampling procedure via two generators. As for frequency-aware replay, its improvement is dealing with unbalanced data. What is neglected in this paper, however, is that there have been numerous works on adaptative store policy in continual learning.\n\n2. The writing of this paper is not clear enough to follow. Firstly, the explanation of the discrepancy between CIR and traditional incremental learning (i.e., class- and domain-incremental) is not clear. Figure 1 replaced with actual objects may give a more intuitive understanding of the authors' motivation. Moreover, it is necessary to use a coherent algorithm to introduce the overall procedure, however, absent in the main text. Finally, the conclusion part is tedious and should be refined.\n\n3. The lack of formulas in this paper makes comprehending have more barriers. For example, the generators build streams part lacks the complementary mathematical explanation. \n\n4. The authors should provide comprehensive theoretical analyses of their method, in my view, which is necessary for solid work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this paper is not high-quality work.\nClarity: This paper is not easy to follow. The figures in this paper don't help others understand the authors' motivation. For example, Figure 1 does not make sense in intuitively showing the difference between CIR and CI (or DI). Besides, the authors don't provide a visual explanation of the slot-based generator. The tedious conclusion part makes it difficult to grip the main contributions of this paper.\n\nNovelty: This paper is not rich in novelty, and lacks relevant theoretical analyses. The main contribution of this paper, in my view, is to build a CIR data stream. Recently, analogous works with frequency-aware replay are not rare.\n\nReproducibility: The authors promise to provide open-source code and the algorithm.\n",
            "summary_of_the_review": "This is not a high-quality paper without a clear expression and lack of solid theoretical analyses, which could not stimulate other research works. So, I recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2894/Reviewer_MpJD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2894/Reviewer_MpJD"
        ]
    },
    {
        "id": "EmqkcFDinBy",
        "original": null,
        "number": 3,
        "cdate": 1667161463758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667161463758,
        "tmdate": 1667161463758,
        "tddate": null,
        "forum": "ESR6hysKDsW",
        "replyto": "ESR6hysKDsW",
        "invitation": "ICLR.cc/2023/Conference/Paper2894/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a study on a different flavor of continual learning scenarios than previously considered in the literature. Precisely, one in which data separation is not strict among concepts, and later tasks can revisit one of the previously introduced classes. The authors name this \"class incremental with repetitions\" (CIL).\n\nThe authors show that in these scenarios storing and replaying data is less important (which is not surprising), unless it corrects for imbalanced exposures to the different classes. Therefore the authors propose a sampling method which balances the class representation during training.\n\nExperiments on the standard computer vision tasks show comparisons between domain incremental, class incremental and class incremental with repetitions.",
            "strength_and_weaknesses": "Strengths:\n - novelty of the analysis\n - relevant CL algorithms analysed under the different scenarios\n - well written paper\n\nWeaknesses:\n - the results are not necessarily surprising; e.g. it is known that balancing classes in training batches benefits performance / optimization",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and clear about the details, therefore I consider it's easily reproducible.",
            "summary_of_the_review": "The CL community would benefit from an analysis of a more natural stream of tasks than the extreme cases considered so far. Due to the clear exposure, interesting analysis and experiments I suggest the paper should be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2894/Reviewer_rWm4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2894/Reviewer_rWm4"
        ]
    }
]