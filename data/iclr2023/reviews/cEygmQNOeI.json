[
    {
        "id": "3IDVS75cHH",
        "original": null,
        "number": 1,
        "cdate": 1665978896212,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665978896212,
        "tmdate": 1668628869943,
        "tddate": null,
        "forum": "cEygmQNOeI",
        "replyto": "cEygmQNOeI",
        "invitation": "ICLR.cc/2023/Conference/Paper3921/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes GReaT (Generation of Realistic Tabular Data), a large language model fine-tuned to generate samples for tabular datasets. First, GReaT encodes examples in a tabular dataset into sequences of text, then GReaT fine tunes a large pretrained language model using the textual representation of the tabular data. Once tuned, GReaT can generate entirely new examples, or sample missing values using arbitrary conditioning.\n\nExperiments on 4 tabular datasets (3 classification and 1 regression) demonstrate GReaT generally outperforms existing tabular data generators on a wide and comprehensive range of evaluation metrics.",
            "strength_and_weaknesses": "Strengths\n---\nGReaT operates directly on the tabular dataset, avoiding lossy data preprocessing steps such as 1) encoding of categorical data (e.g., one-hot encodings); 2) data scaling or normalization; 3) missing value imputation; and 4) outlier removal. These data preprocessing methods can often result in loss of information and ultimately reduce the quality of generated samples.\n\nGReaT is a simple and novel approach for generating samples for tabular datasets, the most common type of data used in industry. This is a potentially significant tool that can help combat noisy or missing values, or can be used to balance imbalanced datasets.\n\nGReaT fine-tunes a large language model using random feature order permutation, allowing sampling with arbitrary conditioning. This also allows GReaT to leverage the vast amount of contextual knowledge learned during pretraining of the large language model to better generate appropriate values for certain features given the values of other features.\n\nIn comparison with existing methods, the experimental results suggest GReaT generates samples that 1) better substitute as training data, 2) are less discernible with the actual training and test data, and 3) qualitatively produce more appropriate values than existing methods.\n\nWeaknesses\n---\nExperiments are run on just 4 rather small tabular datasets, limiting claims of GReaT's generalizability.\n\nThe datasets used contain a relatively small number of features; thus, I am unsure of how GReaT would perform for much higher-dimensional tabular datasets resulting in significantly longer input sequences for fine-tuning.\n\nOne of the motivations for GReaT is the fact that tabular datasets may contain sensitive or personal information, but wouldn't a high-quality data generator potentially reproduce some of this personal data since it is trained on that information?\n\nMinor Weaknesses\n---\nWhat is the train-test split percentage mentioned in the \"Data sets.\" paragraph in Section 4?\n\nIn Figure 3, the occupation \"Adm-clerical\" in the first row of the bottom-left table does not match the first input sequence of the textual representation where the occupation is \"Transport-moving\".\n\nIn the paragraph \"Baselines.\" in Section 4, \"...on a only...\" -> \"...on only a...\".\n\nTable 1 caption, \"...in case of the regression...\" -> \"...in the case of regression...\".\n\nIn the paragraph \"Baselines.\" in Section 4, \"...an variational autoencoder...\" -> \"...a variational autoencoder...\".",
            "clarity,_quality,_novelty_and_reproducibility": "As far as I can tell, the work is certainly novel and is an interesting and clever application of large language models. Since the approach is very straightforward, and the code is provided as an easy-to-use Python package, the reproducibility of the results seems very likely.",
            "summary_of_the_review": "GReaT leverages large language models to generate samples for tabular data in a way that avoids many of the lossy data preprocessing steps required by existing approaches, resulting in high-quality contextually-relevant tabular examples. Overall, the number of datasets evaluated is rather low, however, the positives currently slightly outweigh the negatives.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3921/Reviewer_EaHN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3921/Reviewer_EaHN"
        ]
    },
    {
        "id": "sMaU8WOibW",
        "original": null,
        "number": 2,
        "cdate": 1666344326313,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666344326313,
        "tmdate": 1666344351111,
        "tddate": null,
        "forum": "cEygmQNOeI",
        "replyto": "cEygmQNOeI",
        "invitation": "ICLR.cc/2023/Conference/Paper3921/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes the use of large-scale autoregressive generative language models (LLMs) for generating tabular data. Several recent works have similarly attempted the task of generating tabular data using popular computer vision methods such as Variational Autoencoders and Generative Adversarial Networks, but require an initial preprocessing step of encoding the categorical columns and (optionally) transforming the numerical columns, which often have a large impact on the performance of the generative model. In this paper, a different approach is taken by using a transformer-decoder network, a popular architecture in the NLP field. In this approach, the columns are first converted to text using a textual encoding scheme, the feature order is permuted to allow arbitrary conditioning and sampling, and fine-tuning is performed using a pretrained generative LLM.",
            "strength_and_weaknesses": "Strengths:\n- The proposed approach is easier to handle because it removes the bottleneck of the first preprocessing step for the numeric and categorical columns \n- The textual subject-predicate-subject encoding scheme, together with the random permutation of the feature order, provides the end user with full probabilistic control over the sampling procedure of the generative model\n- The paper shows significant improvements over the state of the art evaluated with the proposed methods\n\nWeakness:\n- Lack of related works. The authors claim that there are no previous works using pre-trained LLM to generate tabular data. However, a quick Google search revealed that there are some recent works (2021) that have considered a similar architecture for generating tabular data, including time series data. Granted, the pre-processing steps are different and this work is new in that regard. Regardless, the authors should highlight such related work and differentiate their work or justify the omission \n- Bad samples. The authors state in Section 3.2 that some synthesised samples that violate the resulting patterns for tabular data have been discarded, but fail to provide an explanation of why such a violation occurs or provide examples of such samples to guide the reader's imagination\n- Practicality: the proposed methods require significantly longer fine-tuning (~9hrs) compared to other baseline methods (< 2mins). The authors have not commented on this or justified why such a computationally intensive approach is preferable\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The writing is clear, detailed and not ambiguous\n- The textual encoding scheme proposed by the authors in quite novel in this setting\n- The work seems easily reproducible, although the authors are strongly encouraged to share their code\n\nMinor corrections: \n- Confused Indices. In section 3.1, definition 1. the authors seem to have confused the indices for rows and columns. m was originally defined for rows, but later used for columns/features. In general, the authors are advised to clarify the notations in the various definitions\n- Typos. Section 4 (baseline, line 3 and DCR, line 5)\n\nFurther clarification needed:\n- The authors claim that previously proposed methods such as mode-specific normalisation introduce artificial ordering. The authors should clarify what is meant by this statement",
            "summary_of_the_review": "The proposed method is a breath of fresh air compared to previous work on GANs and VAEs for generating tabular data. Transformer architectures are increasingly dominating various fields, and this approach seems promising for future deep-learning-based synthesis of tabular data, but how to protect the privacy of the data synthesised from such models still needs to be studied in depth.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3921/Reviewer_gUA4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3921/Reviewer_gUA4"
        ]
    },
    {
        "id": "n1tsm_ybZX",
        "original": null,
        "number": 3,
        "cdate": 1666413513051,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666413513051,
        "tmdate": 1666413513051,
        "tddate": null,
        "forum": "cEygmQNOeI",
        "replyto": "cEygmQNOeI",
        "invitation": "ICLR.cc/2023/Conference/Paper3921/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Hi, I'm currently on a medical leave and won't be able to perform ICRL review duties. sorry for the late notice.",
            "strength_and_weaknesses": "Hi, I'm currently on a medical leave and won't be able to perform ICRL review duties. sorry for the late notice.",
            "clarity,_quality,_novelty_and_reproducibility": "Hi, I'm currently on a medical leave and won't be able to perform ICRL review duties. sorry for the late notice.",
            "summary_of_the_review": "Hi, I'm currently on a medical leave and won't be able to perform ICRL review duties. sorry for the late notice.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3921/Reviewer_HxcB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3921/Reviewer_HxcB"
        ]
    },
    {
        "id": "kdBelh5BfCZ",
        "original": null,
        "number": 4,
        "cdate": 1667260954655,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667260954655,
        "tmdate": 1667260954655,
        "tddate": null,
        "forum": "cEygmQNOeI",
        "replyto": "cEygmQNOeI",
        "invitation": "ICLR.cc/2023/Conference/Paper3921/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a novel approach to generate synthetic tabular data using large language models. ",
            "strength_and_weaknesses": "Strengths:\n\nThe authors proposed an interesting way to use large language models. It's a novel use case.\n\nWeaknesses:\n\n1. The computation cost for the proposed approach seems profitably expensive. In table 6, both GReaT and Distill-GReaT required two order of magnitudes bigger compute budget comparing to other baselines. This leads to serious scaling concerns. \n\n2. Have the authors consider using few-shot evaluation from the large language models as the alternative way to generate synthetic tabular data? It would be cheaper than fine tuning such a big model using hundreds of epoches.  With proper prompting, large language models should be able to generate desired outputs.\n\n3. I am not sure the significance of the proposed method. It's not clearly to me the significance of the deltas in those measures reported in the experiment section. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written. The text, related work, figures and tables are well prepared. ",
            "summary_of_the_review": "The authors did find a novel application of large language models. However, it's unclear if the gains of the proposed method can justify the 100x increase in the computation cost.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3921/Reviewer_7WMZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3921/Reviewer_7WMZ"
        ]
    }
]