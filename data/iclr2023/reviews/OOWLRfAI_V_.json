[
    {
        "id": "TJehayqeBS",
        "original": null,
        "number": 1,
        "cdate": 1666339368508,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666339368508,
        "tmdate": 1669622442120,
        "tddate": null,
        "forum": "OOWLRfAI_V_",
        "replyto": "OOWLRfAI_V_",
        "invitation": "ICLR.cc/2023/Conference/Paper3904/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a novel method for reconstruction from quantized compressive sensing measurements. The proposed method is a modification of score-based models to incorporate information about the forward measurement process and quantization noise. Overall, the method is interesting and generally well-explained. However, the experimental assessment seems somewhat incomplete. As a minor point, the quality of figures is poor and they are often hard to read.",
            "strength_and_weaknesses": "Strengths:\n- Novel and interesting method to adapt score-based generative models to compressive sensing reconstruction in presence of quantization\n- Good performance in the specific settings that are presented and on \"toy\" datasets like MNIST and CIFAR\n\nWeaknesses:\n- Experimental evaluation seems rather limited. Some of the alternative methods shown in the comparisons are not quite mainstream, while important and widely-used baselines for deep-learning CS image reconstruction are missing (for example, ISTA-net Zhang 2017, and Neumann networks Gilton 2019) \n- Authors only report comparisons with other techniques for the 1-bit case but it is unclear how the method is positioned with respect to the state-of-the-art for a wider range of quantization rates\n- For the purpose of quality comparisons of image reconstructions, I feel cosine similarity is not very useful. The most used metrics are just PSNR and SSIM. The space saved removing cosine similarity could be better used expanding the set of comparisons. \n- Training SGM models is known to be very computationally expensive. The need for a pretrained one limits the applicability of the proposed method to a wider range of data for which no pretrained SGM is available, or it would be too expensive to train or not enough data are available. The authors need to discuss this limitation of the approach with respect to existing techniques, especially the ones not relying on generative models.\n- Related to the previous point, results are presented on toy datasets for which pretrained SGMs are easily available, but it is unclear how the method generalizes to higher resolution images. ",
            "clarity,_quality,_novelty_and_reproducibility": "The work is generally clear and the idea is novel. Reproducibility seems good from the textual description, and the authors pledged to release the code.",
            "summary_of_the_review": "The authors present a generally novel and interesting idea to use score-based models for CS reconstruction in presence of quantization. However, I feel the idea is not thoroughly validated from the experimental standpoint, making it unclear how it is positioned in the wider literature on CS reconstruction.\n\n-----\n\nPost-rebuttal: the authors significantly improved the original submission. I raised my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3904/Reviewer_CEkX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3904/Reviewer_CEkX"
        ]
    },
    {
        "id": "D6FjJA6Ljci",
        "original": null,
        "number": 2,
        "cdate": 1666555062189,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555062189,
        "tmdate": 1666555062189,
        "tddate": null,
        "forum": "OOWLRfAI_V_",
        "replyto": "OOWLRfAI_V_",
        "invitation": "ICLR.cc/2023/Conference/Paper3904/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an approach for signal recovery in quantized compressed sensing (CS) that leverages score-based generative models. This addresses a specific setting for the first time, as previous approaches to SGM-based CS do not assume quantization, and quantized CS approaches with generative models have used other options such as GANs and variational autoencoders. A new score is proposed that leverages knowledge of the measurements when assessing the likelihood of a particular candidate signal.\n\nSample candidates for the recovered signal are generated using a Angevin dynamics formulation where the probability term becomes conditioned on the measurements observed. A Bayes rule is leveraged to allow for the score training process to be independent of the measurement matrix.\n\nThe paper sets up a recovery method that leverages a score-based generative model and derives a closed form solution under an orthogonal-row measurement matrix assumption. This result is specialized to the one-bit and non-quantized cases. \n",
            "strength_and_weaknesses": "The numerical results use real-world datasets of varying complexity and show significant performance improvements vs. other generative model-based approaches to quantized CS recovery. However, I would argue that the use of Daubechies-1 (i.e., Haar) wavelets for images does not provide a fair comparison, as other wavelet or patch DCT bases are widely known to be better suited for images. Other results show good scaling to multi-bit quantization and out-of-distribution samples. \n\nThe method opens some questions that the authors may want to briefly address, or consider in the future:\n\nIt would be interesting to comment on the robustness of these results when the rows of the measurement matrix are only approximately orthogonal, or perhaps validate it numerically.\n\nIt would be interesting to consider the effect of noise in the recovery, and how the different methods would perform on a \"fixed budget basis\" setting - e.g, M one-bit measurements vs. M/B B-bit measurements.",
            "clarity,_quality,_novelty_and_reproducibility": "There are a handful of points that can be clarified/corrected:\n\nTheorem 1 is somewhat confusing as phrased. Can one instead start with the assumption of orthogonality for the rows of A, and then make the \"if gaussian then closed form solution\" statement after?\n\nIn Corollary 1.2, it is not clear what a_m corresponds to in (17) - it appears at first look that this equation is meant to be for entry m of the score?\n\nPage 5, typo \"expliciitly\"\nAlgorithm 1, suggest changing the reference to eq (25) to be (11-13) so that the reader does not have to delve into the appendix.",
            "summary_of_the_review": "An interesting approach to quantized CS recovery with good analytical treatment and compelling experimental results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3904/Reviewer_CmCq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3904/Reviewer_CmCq"
        ]
    },
    {
        "id": "aUP6duwwZGz",
        "original": null,
        "number": 3,
        "cdate": 1666595269271,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595269271,
        "tmdate": 1666595564227,
        "tddate": null,
        "forum": "OOWLRfAI_V_",
        "replyto": "OOWLRfAI_V_",
        "invitation": "ICLR.cc/2023/Conference/Paper3904/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Motivated by the power of score-based generative models (SGM), this work studies quantized compressed sensing with score-based generative models and proposed an unsupervised data-driven approach that is denoted by QCS-SGM. One key to QCS-SGM is proposing an annealed likelihood score (noise-perturbed likelihood score), which incorporates information from the measurements while performing the posterior sampling. In particular, if $\\mathbf{A}\\mathbf{A}^T$ is a diagonal matrix, QCS-SGM reduces to a form similar to that of Jalal et al. (2021a), and corresponding guarantees are provided in Theorem 1 and Corollary 1.1. Numerical results are provided to show the superiority of QCS-SGM. ",
            "strength_and_weaknesses": "Strength:\n\nAs the authors mentioned, their work is the first one that utilizes SGM quantized CS. Such an idea is interesting, and the experimental results are promising. \n\nWeaknesses:\n\n(a). SGM has been used for linear CS in some recent works, including Jalal et al., 2021a;b and \n\nDaras, G., Dagan, Y., Dimakis, A. and Daskalakis, C., 2022, June. Score-Guided Intermediate Level Optimization: Fast Langevin Mixing for Inverse Problems. In International Conference on Machine Learning (pp. 4722-4753). PMLR.\n\nI hope the authors can provide more discussions on these closely relevant works and provide intuition on why the QCS-SGM can outperform the methods proposed in these works. Ideally, similar to Jalal et al., 2021a, also perform the experiments for some MRI datasets.\n\n(b). In Theorem 1, the assumption that $\\mathbf{A}\\mathbf{A}^T$ is diagonal is restrictive. Although this assumption holds approximately for some popular measurement matrices such as i.i.d. Gaussian and discrete cosine transform matrices, it is more desired to provide guarantees for these measurement matrices directly (i.e., for the case that $\\mathbf{A}\\mathbf{A}^T$ is approximately diagonal ).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written, and the main idea of this work is natural and clearly presented. This work may seem to be incremental compared to Jalal et al., 2021a and Liu & Liu, 2022. \n\nAs for reproducibility, I hope that the code has been submitted along with the main paper, instead of saying that \"Our code will be open-sourced after acceptance\". ",
            "summary_of_the_review": "Although this work seems to be incremental compared to some closely relevant prior works, the main idea is interesting and the experimental results are promising. Therefore, I am inclined to acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3904/Reviewer_DMn5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3904/Reviewer_DMn5"
        ]
    }
]