[
    {
        "id": "7okpoYTk32",
        "original": null,
        "number": 1,
        "cdate": 1665617371015,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665617371015,
        "tmdate": 1665617371015,
        "tddate": null,
        "forum": "jdEXFqGjdh",
        "replyto": "jdEXFqGjdh",
        "invitation": "ICLR.cc/2023/Conference/Paper163/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors introduce a variational autoencoder with associated loss function that enables to supervision of the latent embedding by class information. In particular, the authors consider linear separability preventing overlapping representations for different classes.",
            "strength_and_weaknesses": "_Strengths_:\n\n- The idea is interesting, well motivated and well presented. \n- It makes sense to use label information when it is available. Guiding the latent variable using a weak classifier is a sensible regularization strategy.\n- Using a weak classifier helps impose structure on the latent space.\n\n_Weaknesses_:\n\n- The paper is lacking experiments in several directions. There is only one experiment in the paper and that experiment is with the common MNIST dataset. The approach doesn't compare to any of the previous semi-supervised approaches. \n- While I appreciate the utility of a weak classifier, I am not convinced that linear separability is completely necessary or even desirable. I think a more detailed argument would be necessary. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well written and very easy to follow. \n- Many aspects of the idea have not been as deeply investigates as potentially possible. For example, the experiments are limited to the very common MNIST data set, which is inadequate to fully investigate the utility and performance of the method.\n- Also the manuscript does not contain any experiments on semi-supervision. \n- The literature review is lacking much if not all relevant previous works. For example, the original semi-supervised VAE [1] isn't even mentioned. In addition, a more recent paper [2] discusses the same idea in great detail. \n\n- There are some weaknesses in the factual correctness of the writing, e.g. the paper by Higgins et. al. is cited as semi-supervised, which seems inadequate. Similarly, the paper states that 28*28 = 526.\n\n\n[1] - Kingma, D. P., Mohamed, S., Jimenez Rezende, D., & Welling, M. (2014). Semi-supervised learning with deep generative models. Advances in neural information processing systems, 27.\n[2] - Joy, T., Schmon, S., Torr, P., Siddharth, N., & Rainforth, T. (2021). Capturing Label Characteristics in VAEs. In International Conference on Learning Representations.",
            "summary_of_the_review": "- Overall the ideas presented in the paper are useful. The presentation is good, but relatively weak in terms of theory and experiments. The same ideas have been discussed in the literature before and in more detail. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper163/Reviewer_ftSd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper163/Reviewer_ftSd"
        ]
    },
    {
        "id": "CZ-Be1mQiX1",
        "original": null,
        "number": 2,
        "cdate": 1666521357680,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666521357680,
        "tmdate": 1666521357680,
        "tddate": null,
        "forum": "jdEXFqGjdh",
        "replyto": "jdEXFqGjdh",
        "invitation": "ICLR.cc/2023/Conference/Paper163/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors proposed Class-Informed Variational Autoencoder (CI-VAE), a generative neural network model comprised of an encoder and decoder similar to the standard architecture of variational autoencoder, with an addition of a linear discriminator network which predicts classes of data from its latent representation. The model was tested with MNIST against vanila VAE.\n",
            "strength_and_weaknesses": "Strength\n\n- The strategy seems straightforward.\n\nWeaknesses\n\n- In the main text, the model was tested only using MNIST against vanila VAE, and lacking quantitative comparison.\n- In the appenix, the authors applied teh CI-VAE against a particular colon cancer single cell RNA-seq dataset, but the reason why this particular dataset was chosen is unclear, and there is a huge jump from MINST.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Code availability is not clearly stated.\n",
            "summary_of_the_review": "The authors proposed Class-Informed Variational Autoencoder (CI-VAE), a generative neural network model comprised of an encoder and decoder similar to the standard architecture of variational autoencoder, with an addition of a linear discriminator network which predicts classes of data from its latent representation. The proposed model was examined only using MNIST and comparison is mostly qualitative.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper163/Reviewer_W4UK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper163/Reviewer_W4UK"
        ]
    },
    {
        "id": "9Zf_kGH08xf",
        "original": null,
        "number": 3,
        "cdate": 1666637100187,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637100187,
        "tmdate": 1666637100187,
        "tddate": null,
        "forum": "jdEXFqGjdh",
        "replyto": "jdEXFqGjdh",
        "invitation": "ICLR.cc/2023/Conference/Paper163/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes adding a linear discriminator in the training loss of VAE in order to ensure separability of the classes. The goal of such an approach seems to either increase the VAE performance or allow linear interpolation between the data class, which hence provide a quite versatile approach to perform interpolation between object of the same class.\n\n",
            "strength_and_weaknesses": "The paper is very written, clear, and sound. The main weakness is that there is a lack of comparison with other ML approaches for interpolation and a discussion about how the approach differs from the disentangled VAE approaches.\n\nIn particular, one important approach to performing class-specific interpolation can be found in the Optimal Transport ML literature. How does OT interpolation compare to CI-VAE interpolation for MNIST? What would be the benefit/drawback of such an approach versus OT? (The versatility of the CI-VAE is one benefit, but a thorough discussion would be very welcome).\n\nFor biology, there is recent literature about interpolation with OT that the authors should cite and look into. For instance (but not limited to) see and references therein:\n- Fast and Smooth Interpolation on Wasserstein Space\n- TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics\n",
            "clarity,_quality,_novelty_and_reproducibility": "Very clear and reproductible.\n",
            "summary_of_the_review": "The main drawback of the paper is the lack of discussion and comparison with concurrent approaches, which make the importance of the paper not clear enough.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper163/Reviewer_29ie"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper163/Reviewer_29ie"
        ]
    },
    {
        "id": "gWrD4pVUFjZ",
        "original": null,
        "number": 4,
        "cdate": 1666800144512,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666800144512,
        "tmdate": 1666800144512,
        "tddate": null,
        "forum": "jdEXFqGjdh",
        "replyto": "jdEXFqGjdh",
        "invitation": "ICLR.cc/2023/Conference/Paper163/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to apply a logistic regression to the latent space samples of a VAE to enforce linear separability in the latent code. The authors argue that this approach is useful for interpolating between two samples of the sample class without crossing class boundaries. The experiments on MNIST indicate that the regularised latent space fulfils the set-out requirements better than a vanilla VAE.",
            "strength_and_weaknesses": "Strengths:\n- The paper is clearly written and easy to follow.\n- The proposed method seems to achieve the goal of linear class separability in the latent code.\n\nWeaknesses:\n- The paper is missing important baselines and comparisons to e.g. class-conditional general or GMM-VAEs. Furthermore, it is lacking any discussion of previous work on latent space shaping such as [1]. [1] seems very related and also enforces class separation and is only a single example for missing literature overview.\n- It is not clear whether, linear interpolation is the best or correct interpolation method, instead spherical interpolation could be used. [2] has an overview of different interpolation methods. This choice should be better justified.\n- The paper seems unfinished and unpolished in places - it mentions results on cancer genomics data in the abstract but it seems to only show results on MNIST, which as a dataset is not enough to warrant publication.\n\nMisc:\n- Please correct the usage of `\\citep` and `\\citet` for the ease of reading.\n- Most Figure references seem to be incorrect.\n- Around eq. 1 it is mentioned that AEs optimise the distance between original data and reconstructed data. It would be good to be specific and add example distance metrics.\n- The introduction of VAEs is a bit imprecise when it comes to the likelihood of the output space.\n- Fig. 3 shows a schematic example latent space for the CI-VAE. However, the logistic regression is applied to the samples of z ~ p(z|x) which means that there is no guarantee that z is not sampled outside of the class boundary.\n- Sec 2.2 describes some usage of the model for data augmentation. However, it is very imprecise and should compare to class-conditional VAEs.\n- Fig. 4 shows the losses but it is unclear which loss is which.\n- p 5: 28*28 = 784 instead of 28*28=526\n- Table 1: It would be good to add error bars.\n- Why did you use dropout in this model?\n- Figure 5: How do you interpret these latent space plots?\n- When you perform linear separability testing, how is the new classifier trained? What is the reported value tested on?\n- I do not understand the value of figure 8.\n- What values are chosen for $\\alpha, \\beta$?\n\n[1] Connor, Marissa, Gregory Canal, and Christopher Rozell. \"Variational autoencoder with learned latent structure.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.\n[2] Mi, Lu, et al. \"Revisiting Latent-Space Interpolation via a Quantitative Evaluation Framework.\" arXiv preprint arXiv:2110.06421 (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, however, some details are missing to ensure full reproducibility of the paper.",
            "summary_of_the_review": "The paper seems generally unpolished with references to missing experiments and incorrect figure references. A lot of relevant background literature is missing which also leads to insufficient experiments and baselines for them.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper163/Reviewer_RrTY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper163/Reviewer_RrTY"
        ]
    }
]