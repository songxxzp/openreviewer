[
    {
        "id": "M6m51BpRra",
        "original": null,
        "number": 1,
        "cdate": 1666324152874,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666324152874,
        "tmdate": 1666324152874,
        "tddate": null,
        "forum": "YjKqWExiy6s",
        "replyto": "YjKqWExiy6s",
        "invitation": "ICLR.cc/2023/Conference/Paper656/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Abnormal Adversarial Examples Regularization (AAER), a regularizer to address the known catastrophic overfitting issue observed in fast adversarial training (AT). AAER is based on suppressing the output difference of abnormal adversarial examples (irregular adversarial samples such that their loss is smaller after adding perturbation) and their number, as well as maximizing the difference of model output between unperturbed and perturbed samples. ",
            "strength_and_weaknesses": "Strength:\n1. The proposed method is simple to implement and has low overhead\n\nWeakness:\n1. The uniqueness of Fig. 3 to fast AT needs more justification\n2. The role of the second term in eq. (7) is unclear\n3. What is \"unclipped perturbation\"? Is it a rational regularizer? Why it gives better performance?",
            "clarity,_quality,_novelty_and_reproducibility": "Although the proposed method is novel and addresses an important challenge in fast AT, the current presentation of this submission has several issues that need to be addressed. I list my main concerns and comments as follows.",
            "summary_of_the_review": "1. Figure 3 is a very nice motivating example of the challenge and the proposed solution. However, it is unclear whether the emergence of abnormal examples is unique to the fast AT setting or not. To complete the analysis, I suggest running the same analysis on standard AT. If standard AT does not suffer from this issue but fast AT does, then this is a more convincing analysis.\n\n2. How critical is the second term in eq. (7)? In Line 15 of Algorithm 1, I noticed the authors use a zero threshold $\\max${$\\cdot$,0}. Does this suggest the second term in eq. (7), which contributes to negative numbers fo AAER, may not be important? Can the authors run an ablation study of the experiments with and without the second term?\n\n3. In most cases, the AAER_RUC method is the best regularizer. However, it's unclear to me what \"unclipped perturbation\" means in this context. Do the authors mean allowing the perturbed samples to go beyond the pixel values, or the authors did not clip the norm of the perturbation? Both cases make little sense to me, and the result needs more justification.\n\n4. Can AAER show complementary performance gain together with existing methods (e.g., gradient alignment) to mitigate catastrophic forgetting? For instance, any benefit of combining AAER with any of the existing methods in Table 1?\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper656/Reviewer_5TBo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper656/Reviewer_5TBo"
        ]
    },
    {
        "id": "zWGwZY2t6_",
        "original": null,
        "number": 2,
        "cdate": 1666597771584,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597771584,
        "tmdate": 1666597771584,
        "tddate": null,
        "forum": "YjKqWExiy6s",
        "replyto": "YjKqWExiy6s",
        "invitation": "ICLR.cc/2023/Conference/Paper656/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Recently, FGSM-based adversarial training faced an emergency problem of catastrophic overfitting. To mitigate this, the authors first observed that the catastrophic overfitting is related to some abnormal examples, whose loss decreases under the FGSM attack. Based on this observation, in this paper they proposed a new abnormal adversarial example regularization (AAER), which considers two key components: one is about the number of the abnormal examples, while the other is about the logits variation. The experiments on CIFAR-10/100 show that the proposed AAER is effectiveness.\n  ",
            "strength_and_weaknesses": "**Strength**:\n1. The observation of the abnormal adversarial examples is interesting.\u00a0\n2. The experiments show their effectiveness on two widely-used CIFAR datasets.\n\n**Weaknesses**:\n1. In section 3, the authors analyzed the number and logit variation of normal and abnormal examples, respectively. Section 3.1 shows that the abnormal adversarial examples dominate the catastrophic overfitting since the number has a significant increase when catastrophic overfitting exists. In section 3.2, the authors found that the logit variations during the inner process will increase after the catastrophic overfitting.\u00a0Both of these two observations are new and interesting. And the authors claimed that their proposed AAER is inspired by them. However, it's very difficult for me to understand how Eq.7 is derived from these two observations? \n1.1 From the second observation of logit variations, whether it is a normal sample or an abnormal sample, the change of logical distance shows an increasing trend, and the trend of distance change is also the same. Therefore, why does Eq.7 reduce the gap between these two type of adversarial examples? \n1.2 At the same time, Eq.7 is not very rigorous, because the $x_k$ in the second term may contain abnormal examples. \n1.3 The authors wanted to reduce the number of the abnormal examples. So they introduced a term $\\frac{n}{m}$ in Eq.8. But the authors did not analyze the mechanism of this design.\n\n2. The authors conducted experiments on CIFAR-10 and CIFAR-100. I don't think the experiments are enough. As we know, FGSM-based adversarial training is one of the most effective training schemes that can also train the model on a large-scale dataset. So it's better to evaluate the proposed method and analyze the properties of abnormal adversarial examples on a large dataset, like Tiny-ImageNet or ImageNet.\u00a0\n\n3. Since the proposed AAER is to reduce the impact of abnormal adversarial examples. However, there were no corresponding experiments on this part in the experiment. For example, show the number curve after using AAER (as shown in Figure 3 (a)). Also, as training progresses, it is better to show the loss curve.\n\n4. On page 7, the authors said that they used the unclipped perturbation to achieve better robustness. However, this is not a commonly used technique. This may cause the pixel value of the adversarial sample to exceed the actual value. The authors need to clarify the reasons for using this technique.\n\n5. On page 8, what's the parameter $\\beta$? In previous section, I didn't find this hyper-parameter. ",
            "clarity,_quality,_novelty_and_reproducibility": "The observations are new and I think the proposed method is easy to reproduce. But the experiments are not very sufficient to verify the proposed regularization technique.",
            "summary_of_the_review": "This paper finds some interesting observations, but the proposed technique does not appear to be strongly correlated with these observations. Moreover, the experiments are not very sufficient. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper656/Reviewer_gJHh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper656/Reviewer_gJHh"
        ]
    },
    {
        "id": "A0oR-A5VZ_",
        "original": null,
        "number": 3,
        "cdate": 1666635377487,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635377487,
        "tmdate": 1666635377487,
        "tddate": null,
        "forum": "YjKqWExiy6s",
        "replyto": "YjKqWExiy6s",
        "invitation": "ICLR.cc/2023/Conference/Paper656/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis paper studies the robust overfitting of one-step adversarial training and finds that robust overfitting is related to the increase of abnormal adversarial examples generated for training. Abnormal adversarial examples have even lower losses after adding the adversarial perturbations, which can be a reason for causing robust overfitting in one-step adversarial training. Based on this observation, this paper proposes a novel regularization term to reduce the number of abnormal adversarial examples. By comparing the proposed methods with other baselines on CIFAR10 and CIFAR100 datasets, the evaluation results show that the proposed method achieves better robustness against the PGD attack.\n",
            "strength_and_weaknesses": "Strength: \n+ This paper points out the existence of abnormal adversarial examples when robust overfitting happens and speculates that robust overfitting in single-step adversarial training is caused by abnormal adversarial examples. \n\n+ This paper proposed a novel regularization term to reduce the number of abnormal adversarial examples in training.\n\n+ The evaluation results show that the proposed method outperforms other baseline methods.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Weakness:\n\n+ There are several places that I am not quite clear about in writing and may need further clarification:\n1. In equation 5, is $\\eta$ the random noise in the allowed perturbation ball? Why should we need an $\\eta$ here? \n2. In Line 7 to line 11 of algorithm 1, $\\eta$ is calculated for each iteration. Why does Algorithm 1 need this? What is the role of this component in the algorithm? What is $\\eta_t$ and $\\delta_t$ here.\n3. In Line 15, there is a multiplier $t/T$, what is this for?\n\n+ I don\u2019t find some important experiment details: What is the number of training epochs and learning rate scheduler for the proposed training method? Which checkpoint are you using for evaluation? What\u2019s the robustness difference between the best checkpoint and the last checkpoint? It will be nice to draw a figure to show how robustness changes for the proposed algorithm (a figure like Figure.1). \n\n+ The robustness reported in Table.1 is evaluated with the PGD attack, which is always found an overestimated robustness issue. Compared to the PGD attack, AutoAttack is usually more reliable to evaluate the robustness. Although the appendix compares the AA robustness with the vanilla PGD-based adversarial training method, it will be more convincible to compare the robustness of the proposed method with other SOTA baselines against AutoAttack.\n",
            "summary_of_the_review": "There are several points/places lacking clarity in the paper  (as discussed in the weakness). Further clarification and details may be necessary.\n\nReproducibility: The experiment settings section does not include all experiment details and no code is provided.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper656/Reviewer_qRtw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper656/Reviewer_qRtw"
        ]
    },
    {
        "id": "f-9ZUSYt24",
        "original": null,
        "number": 4,
        "cdate": 1666668882397,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668882397,
        "tmdate": 1668780069650,
        "tddate": null,
        "forum": "YjKqWExiy6s",
        "replyto": "YjKqWExiy6s",
        "invitation": "ICLR.cc/2023/Conference/Paper656/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new training objective for single-step adversarial training to reduce catastrophic overfitting (CO). In particular, the paper establishes a correlation between CO and the presence of \"abnormal adversarial examples,\" i.e., instances where taking a single step of FGSM actually decreases the loss rather than increasing it. To this end, the authors propose a regularizer called AAER meant to penalize abnormal adversarial examples, and show that it often improves the accuracy of SSAT.",
            "strength_and_weaknesses": "Strengths:\n\n- The observation that CO co-occurs with abnormal adversarial examples is both novel and interesting.\n- The attack seems efficient (e.g., not much slower than N-FGSM) while providing better adversarial accuracy.\n- The authors conduct an extensive evaluation of other methods and baselines.\n\nWeaknesses:\n\n- Given the link between CO and abnormal adversarial examples, a natural baseline would be to simply ignore any abnormal examples in the loss and train only on examples where the loss actually increased. Does this not work? If so, why?\n- The method does seem to improve on N-FGSM in terms of robust accuracy, but this often comes at the expense of clean accuracy, so it's unclear if the proposed method truly dominates N-FGSM. There is also some misleading bolding in the Table (i.e., for CIFAR-100 16/255 it doesn't look like the confidence intervals of the two bolded entries actually overlap) that should be fixed.\n- I am confused about the motivation behind Eq. 7---it seems like it is encouraging the network to classify the original and \"normal adversarial\" examples very differently (judging by the negative term)? Please correct me if I am wrong here, but it seems very counterintuitive.\n- Finally, there are many non-standard additions to the training routine (I'm thinking mainly of the linearly increasing $\\alpha$, as the other terms were ablated quite well). Does normal FGSM with linearly increasing alpha still suffer from CO?",
            "clarity,_quality,_novelty_and_reproducibility": "The work is certainly novel and original. The quality is reasonable, bar the weaknesses above. The clarity could be improved---there are some grammatical errors throughout, and there are some unclear sentences, e.g.:\n\n- Furthermore, the strength of AAER depends on the interaction of two parts learning objective, which can reflect the degree of classifier distortion more comprehensively and flexibly.\n- Based on the above analysis, we design a novel regularization term, AAER, which aims to suppress the anomalous training samples by the (i) number and the (ii) logits variation, ultimately achieving the purpose of preventing CO, which is shown in the following formula\n...\n\n",
            "summary_of_the_review": "In summary, the work proposes a new training objective for regularizing single-step adversarial training. Some of the motivation is a bit unclear and there is a natural baseline that the work does not explore. Overall, I think the paper as is is slightly below the bar for publication, but I am willing to raise my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper656/Reviewer_DBF4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper656/Reviewer_DBF4"
        ]
    }
]