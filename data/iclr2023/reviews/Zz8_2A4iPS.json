[
    {
        "id": "C5RXOWhl_A",
        "original": null,
        "number": 1,
        "cdate": 1666534158346,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666534158346,
        "tmdate": 1668794213369,
        "tddate": null,
        "forum": "Zz8_2A4iPS",
        "replyto": "Zz8_2A4iPS",
        "invitation": "ICLR.cc/2023/Conference/Paper5063/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper introduces a novel task-incremental approach that diminishes the gradients based on their importance (SPG). \nSpecifically, the importance is calculated as the normalized gradients on the current and past tasks.\nFor the current task, the gradients are taken w.r.t. the log-likelihood of the data.\nFor past tasks, the logits' gradients are used instead.\n\nAn extensive empirical study shows that SPG is on average superior than task-incremental baselines.",
            "strength_and_weaknesses": "**Strengths**\n\nThe method is simple and sensible.\nThe empirical study is extensive and appropriate.\nMy intuition is that the reported results would generalize to most task-incremental scenarios.\n\n**Weaknesses**\n\nMy main concern is about the studied scenario.\nMy opinion is that task-incremental learning isn't aligned with practical use-cases and over-studied.\nI understand that it's a useful setting to study catastrophic forgetting.\nHowever, we are seeing increasing evidence that catastrophic forgetting isn't as catastrophic as previously thought [1], especially in task-incremental scenarios where one does not have to deal with the problem of calibrating a single-head on all tasks.\n\nFurthermore, most literature in this space often get away with omitting the replay baseline, which is challenging to outperform, at least when computational resources are correctly fixed throughout the methods.\n\nNow, I do think that in the realm of task-incremental papers, SPG is a good one.\nI'm willing to increase my score if the SPG survives the introduction of the following baselines:\n- replay where all past data is allowed (memory is cheap, so there's no need for artificially constraining to small buffers).\n    - try different current vs replay task balances\n- this one is really important: copy the network at the end of each task, and only finetune the copy\n    - this method is simple, doesn't incur forgetting, should allow for some forward transfer, and has the same computational requirements as other baselines.\n\n\n[1] Pretrained Language Model in Continual Learning: A Comparative Study\n",
            "clarity,_quality,_novelty_and_reproducibility": "good on all accounts.\nThe English can be improved however.",
            "summary_of_the_review": "Simple and sensible approach.\n\nBecause of the weaknesses of the studied scenarios, the authors need some particular new baselines (see **weaknesses**)\n\n---- POST REBUTTAL ------\n\nI updated my score from a 5 to 6 in light of the added baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5063/Reviewer_7vyQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5063/Reviewer_7vyQ"
        ]
    },
    {
        "id": "pg_wnTcMYv",
        "original": null,
        "number": 2,
        "cdate": 1666689969800,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689969800,
        "tmdate": 1666689969800,
        "tddate": null,
        "forum": "Zz8_2A4iPS",
        "replyto": "Zz8_2A4iPS",
        "invitation": "ICLR.cc/2023/Conference/Paper5063/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a new method (SPG) for task-incremental learning. SPG uses soft-masks to condition the full model for each task. Experiment results on benchmark datasets demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "The main strength of the paper is that it is relatively easy to follow and understand the high-level idea.\n\nWeaknesses:\n- The topic of task-incremental learning is not particular interesting, given its proven simplicity in prior work.\n- The core idea of the work is incremental: using importance mask to condition the model is common in architecture-based methods.\n- The empirical evaluation is questionable. For example, in Table 2, all methods in the table has very small standard deviation. I do not think it corresponds with reported results in prior work.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the work is acceptable.\nThe overall quality of the work does not meet the standard of ICLR.\nThe novelty is incremental.\nThe reproducibility is questionable.",
            "summary_of_the_review": "The overall quality of the work does not meet the standard of ICLR, given the limited novelty and simplicity of the topic. Substantial amount of work is required to improve the paper. Therefore, I would recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5063/Reviewer_s4uK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5063/Reviewer_s4uK"
        ]
    },
    {
        "id": "vYu6cmOz2X",
        "original": null,
        "number": 3,
        "cdate": 1666800803813,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666800803813,
        "tmdate": 1666800803813,
        "tddate": null,
        "forum": "Zz8_2A4iPS",
        "replyto": "Zz8_2A4iPS",
        "invitation": "ICLR.cc/2023/Conference/Paper5063/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The goal of proposed method is to overcome the catastrophic forgetting in continual setting, while promoting the forward knowledge transfer, and it proposes a simple soft-masking approach applied to the gradients during back-propagation.",
            "strength_and_weaknesses": "(+) it simply uses gradient masks that can be easily computed by using gradient values.\n\n(+) the importance values can also be used in EWC framework, and this is computationally cheaper than using the Fisher information.\n\n(+) masking is only applied during the learning phase, and it is not required during the inference time.\n\n(-) the proposed method shows competitive performance compered to other methods, but it is not well-studied why it works better than others.\n\n(-) it is unclear why the loss $\\text{Sum}(\\mathcal{M}_\\tau(X_t))$ for the previous task $\\tau < t$ is useful to compute the importance value. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The overall performance seems meaningful, but it is not well explained why the proposed method works better than others.\n- Are there some results comparing 'soft'-masking with 'hard'-masking? \n- Recalculating the results by measuring the 'improvements from NCL performance', it will give better understanding of overcoming CF.",
            "summary_of_the_review": "- The method is simpler than other methods and the overall performance seems meaningful, but it is not well explained why the proposed method works better than others.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5063/Reviewer_5rH3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5063/Reviewer_5rH3"
        ]
    }
]