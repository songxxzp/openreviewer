[
    {
        "id": "yukz030i7gB",
        "original": null,
        "number": 1,
        "cdate": 1666260236499,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666260236499,
        "tmdate": 1666260236499,
        "tddate": null,
        "forum": "AsSdrNJ-DZG",
        "replyto": "AsSdrNJ-DZG",
        "invitation": "ICLR.cc/2023/Conference/Paper1513/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "While existing zero-shot NAS algorithms can usually achieve competitive performance in practice, their zero-shot estimators are unfortunately observed to perform inconsistently in different tasks (e.g., search spaces and datasets), which is even not comparable to the most straightforward one, i.e., the number of parameters, abbreviated to #params. This is known to be a critical problem in the zero-shot/training-free NAS field. So, this paper aims to develop a new zero-shot estimator that can perform consistently well on different benchmark datasets and search spaces. Before realizing this, this paper firstly observes that the number of Sweet Gradient (i.e., gradient within a special interval) consistently has better rank correlations than other zero-shot estimators in practice, which indicates that the number of Sweet Gradient may be a better choice for zero-shot NAS. Then, inspired by the close connection between the increasing proportion of Sweet Gradient and the increasing neural network depth, this paper proposes to leverage network depth to compute the Sweetimator and hence can find Sweet Gradient intervals without training. Finally, this paper uses extensive experiments to show that its zero-shot NAS based on Sweetimator can outperform existing zero-shot estimators in both ranking correlations and search results on various NAS benchmarks and datasets.",
            "strength_and_weaknesses": "## Strength\n1. This paper studies an essential problem in the zero-shot/training-free NAS field, i.e., improving the practical consistency of zero-shot estimators, and develops an interesting zero-shot estimator Sweetimator to tackle it, which empirically outperforms other zero-shot estimators in extensive experiments.\n2. Based on the interesting Sweetimator, this paper proposes a zero-shot NAS algorithm that performs better than other zero-shot baselines on various NAS benchmarks and datasets.\n3. This paper provides many ablation studies to help better understand their proposed Sweet Gradient interval.\n\n## Weakness\n1. The theoretical motivation or justification for the study of eq.1 is missing in this paper. Such a motivation or justification is necessary because it helps to understand why studying this eq.1 (rather than other forms of gradients) may help develop a consistent zero-shot estimator for NAS and how it helps improve the rank consistency over different NAS benchmarks and datasets. The whole paper only relies on empirical observations to support its conclusions, which is not so convincing to me.\n2. This paper doesn't provide a formal, clear, and effective definition of Sweet Gradient intervals, which leads to some problems. Specifically, in this paper, it's informally defined by comparing with the rank consistency of #params in Sec. 3.2. This means that to obtain these Sweet Gradient intervals, people need to measure the rank correlations of #params on various NAS benchmarks, which seems to contradict with eq.3 that only leverages the target NAS benchmark to obtain Sweet Gradient intervals (Problem 1). In addition, based on such an informal definition, Sweet Gradient intervals adapt when the rank correlation of #params changes. So, the eq.1 based on Sweet Gradient intervals may not be able to outperform other zero-shot estimators when the rank correlation of #params performs very poorly in certain tasks (since Sweet Gradient intervals only need to improve over #params even slightly) and thus can not guarantee to achieve better consistency than other zero-shot estimators (Problem 2).\n3. The effectiveness of using eq.3 to determine the best Sweet Gradient interval remains questionable. This is because (a) the discussion below eq.2 shows that the Sweet Gradient interval shifts with the increasing network depth and therefore the Sweet Gradient intervals obtained using the increasing network depth may differ from the desired one for the search space with a different fixed network depth (this is known to be common in NAS by searching with a smaller network depth and applying with a larger network depth), and (b) the parameter proportion in the Non-Sweet Gradient intervals may also increase according to Appendix B.2 and therefore eq.3 may not really output the best Sweet Gradient interval (it can also output Non-Sweet Gradient interval). This begs the question: Is the Sweet Gradient interval really the one that leads to the good search results in the experiments of this paper?\n4. The details of Fig. 3 need to be clearer, e.g., the rank correlations of these Sweet Gradient intervals and which layer is used to plot (since eq.2 is defined on a specific layer l). These are important to understand the difference among Sweet Gradient intervals and the possible impacts of different layers on the observations in the paper.\n5. As mentioned in the limitation section of this paper, this paper only focuses on classification tasks with different search spaces to compare the consistency of different zero-shot estimators. I think it may also be important to examine the improved consistency of the proposed method in different tasks to further support its superiority.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the clarity of this paper may be further improved (see the weakness above). Meanwhile, this paper should provide the reproducibility statement and source codes to support the reproducibility of its zero-shot estimator and NAS algorithm.",
            "summary_of_the_review": "In general, my major concerns lie in the soundness and clarity of this paper as the empirical results are already quite good in this paper. I hope the authors can address my concern in the rebuttal period.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1513/Reviewer_FxrW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1513/Reviewer_FxrW"
        ]
    },
    {
        "id": "97a8BuF5EF",
        "original": null,
        "number": 2,
        "cdate": 1666605793861,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605793861,
        "tmdate": 1669881338001,
        "tddate": null,
        "forum": "AsSdrNJ-DZG",
        "replyto": "AsSdrNJ-DZG",
        "invitation": "ICLR.cc/2023/Conference/Paper1513/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a zero-cost evaluation metric to improve zero-shot neural architecture search efficiency in this work. This work uses gradient distribution and network information at initialization for scoring candidate models and achieves a strong rank consistency with the performance of candidate models. Extensive experiment results are provided to validate the efficiency of the zero-cost metric.",
            "strength_and_weaknesses": "Strengths\n1. The paper is very well-written and easy to follow.\n2. The proposed idea to search \u201csweet gradients\u201d in candidate networks seems novel and interesting.\n3. The paper has provided extensive evaluations over different metrics and datasets. The results are presented in clear manner and are fairly convincing.\t\n\nWeakness\n1. The theoretical novelty is limited, although I do acknowledge the contribution of the proposed metric to zero-shot NAS.\n2. The proposed method in this paper looks like a universal evaluation metric, not limited to the computer vision field. So the diversity of experiments is not enough. The authors only use image datasets to validate the effectiveness of the proposed metric. It is better to validate the proposed metric on NAS-Bench-NLP and NAS-Beach-ASR should\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has strong motivations and a clear structure.",
            "summary_of_the_review": "This paper is well written and technically sound, but the theoretical novelty is limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1513/Reviewer_tGTD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1513/Reviewer_tGTD"
        ]
    },
    {
        "id": "dTzoziT8aMu",
        "original": null,
        "number": 3,
        "cdate": 1666651238787,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651238787,
        "tmdate": 1669057372762,
        "tddate": null,
        "forum": "AsSdrNJ-DZG",
        "replyto": "AsSdrNJ-DZG",
        "invitation": "ICLR.cc/2023/Conference/Paper1513/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Verifying Sweet Gradient: The paper first discovered through experiments that the Sweet Gradient of parameters, i.e., the absolute gradient values within a certain interval, brings higher consistency in network performance than the overall number of parameters. Then this paper also uncovered a positive correlation between the network depth and the proportion of parameters with sweet gradients in each layer.\n\nNAS: Based on the observation, this paper also proposes a Zero shot NAS method based on Sweet Gradient.",
            "strength_and_weaknesses": "Strength:\n1. The writing of the paper is clear and understandable.\n2. The comparison between Sweet Gradient and other indicators is completely sufficient.\n\nWeaknesses:\n1. There is a lack of theoretical analysis on why Sweet Gradient is better than other measurements.\n2. Lack of innovation in improving the Zero-shot NAS search method.\n\nQuestion:\n1. Figure 1 compares the results on three datasets. Has the author analyzed why there are gaps between the three datasets? If the differences between the three datasets are all very large. How does the author guarantee the versatility of Sweet Gradient?\n\n2. Does the author theoretically analyze why Sweet Gradient is better than other indicators?\n\n3. Is the improvement to the existing Zero shot NAS just updated with new metrics? I have a question about how the paper implements zero-shot NAS.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity:\nclear\n2. Quality:\nThe paper is clear but lacks more analysis.\n3. Novelty:\nA bit innovative.\n4. Reproducibility:\nSome details are unclear.",
            "summary_of_the_review": "The motivation for the research is clear. But the paper lacks the necessary theoretical analysis. I need the author to provide more details in the rebuttal to improve my score.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There are no Ethics Concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1513/Reviewer_3NxU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1513/Reviewer_3NxU"
        ]
    },
    {
        "id": "Ufc2iLiTQ_",
        "original": null,
        "number": 4,
        "cdate": 1666700290601,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700290601,
        "tmdate": 1666795339301,
        "tddate": null,
        "forum": "AsSdrNJ-DZG",
        "replyto": "AsSdrNJ-DZG",
        "invitation": "ICLR.cc/2023/Conference/Paper1513/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper finds that the absolute gradient values within a certain interval, called Sweet Gradient of parameters can be good zero-cost proxy evaluating candidate models in zero shot NAS. The authors find a positive correlation between the depth and the proportion of sweet gradient parameters, and further propose a way to automatically determine the Sweet Gradient interval. The effectiveness of the proposed method is validated on four benchmarks with eight search spaces.",
            "strength_and_weaknesses": "Strength:\n\n1. Utilizing the absolute gradient values within a certain interval as zero cost proxy evaluating candidate models in NAS is novel and interesting. Building a consistent zero-cost proxy is a very good contribution. \n2. The method is simple and easy to implement. \n3. The paper is clearly written. Experiments and ablation studies are well performed.\n\nWeakness:\n\n1. The proposed method, in its current form, is mostly heuristic. It is still not clear how the authors find that Sweet Gradient of parameters can be good proxies and why it works consistently well. More insightful discussion or theoretical analysis may be required to fully justify the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the proposed method is somewhat novel.",
            "summary_of_the_review": "The paper is clearly written. The main concern is about insightful discussion and theoretical analysis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1513/Reviewer_g7DU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1513/Reviewer_g7DU"
        ]
    }
]