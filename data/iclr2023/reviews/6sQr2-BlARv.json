[
    {
        "id": "llZrLAginl",
        "original": null,
        "number": 1,
        "cdate": 1666317487057,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666317487057,
        "tmdate": 1666317487057,
        "tddate": null,
        "forum": "6sQr2-BlARv",
        "replyto": "6sQr2-BlARv",
        "invitation": "ICLR.cc/2023/Conference/Paper2949/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approach that performs top-k classification with label ranking. The proposed approach has three stages. The first stage trains the base classifier. The second stage relabels training data using one ground truth label and k-1 most likely labels. The third stage trains the top-k classification model by introducing a new top-k loss function. Using text and image datasets, the paper evaluated the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "Strength:\n- Simple model to be easily implemented.\n- General approach to accommodate various machine learning approaches.\n- Extensive experiments are performed in the paper. \n\nWeakness:\n- Technical depth of the proposed approach is not so deep.\n- More machine learning approaches should be tested to evaluate the effectiveness of the proposed approach used in the proposed approach.\n- The impact of hyperparameters should be theoretically analyzed.",
            "clarity,_quality,_novelty_and_reproducibility": "Basically, I like the paper's motivation; it is quite a fundamental research problem to learn top-k classification with label ranking. Besides, this paper is well-structured, and easy to follow. I appreciate that the paper conducted experiments based on many real datasets. The experimental result shows that the proposed three-stage approach can more effectively perform top-k classification than the previous approaches. \n\nDespite its practical usefulness, the proposed approach seems to be straightforward. I think that more technical depth is required. As for two hyperparameters of m and alpha (the number of base classifiers and proportion), the paper concludes that m should be set to 8 for the text dataset and 5 for the image dataset, and alpha should be set to 0.8 from the experimental results. I think it is good to theoretically analyze the impact of hyperparameters for the proposed approach to improve the technical depth of the paper. \n\nI appreciate the generality of the proposed approach; it can use various machine learning approaches in performing top-k classification. However, the paper tested only BERT, LSAN, Swin, and C-Tran in the experiment. I think, to show the generality of the proposed approach, more machine learning approaches should be used in the experiments, such as SVM, which is a popular approach for top-k classification, as described in Section 2.",
            "summary_of_the_review": "The proposed approach is technically somewhat shallow.\nTheoretical analysis for hyperparameters should be added to the paper.\nTo show the generality of the proposed approach, more machine-learning approaches should be tested in the experiment.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2949/Reviewer_ALCH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2949/Reviewer_ALCH"
        ]
    },
    {
        "id": "BoVn9P_p3Bi",
        "original": null,
        "number": 2,
        "cdate": 1666549998060,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666549998060,
        "tmdate": 1666549998060,
        "tddate": null,
        "forum": "6sQr2-BlARv",
        "replyto": "6sQr2-BlARv",
        "invitation": "ICLR.cc/2023/Conference/Paper2949/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies top-k classification problem. Observing existing top-k classification methods neglect the ranking of the ground truth label among the predicted k labels, the authors proposed to take label ranking into consideration. Specifically, a new 3-stage approach was proposed to tackle this problem. In the first stage, an ensemble method is to train several base classifiers. In the second stage, these base classifiers are used to predict class probabilities, which are then averaged to create top-k most likely labels (including the ground truth label) for each example. In the third stage, a novel loss function that considers the ranking of the ground truth label is used to train a top-k classifier. The experimental results show that the performances of the proposed method are significantly better than those of existing methods on several text and image datasets.",
            "strength_and_weaknesses": "**Strong points.**\n\nThe consideration of the ranking of the ground truth label is well-motivated and is of significance for top-k classification problems. The proposed 3-stage method is relatively intuitive, and was clearly described. The experimental results show the method is very effective.\n\n**Weak points.**\n\nThe proposed 3-stage method is more complex and requires more computations. It is not clear from the experiments whether the better performances were due to the more complex model and more computations. Comparing with other ensemble methods might help make this point clearer. Nevertheless, it seems that the new loss function in the third stage did help improve performances. More theoretical understanding of the proposed loss function is needed.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is clear and well-written. It is novel at large. The descriptions of the experiments are generally detailed, so the reproducibility should not be a concern. It would be better to include the code.",
            "summary_of_the_review": "I vote for a weak accept since I think the merits outweigh the weak points.\n\n**Questions.**\n\n1. How many trials were conducted in the experiments?\n\n**Comments.**\n\n1. In Eq. 7, \"pso\" -> \"pos\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2949/Reviewer_iYfY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2949/Reviewer_iYfY"
        ]
    },
    {
        "id": "_KPzv2BIS6",
        "original": null,
        "number": 3,
        "cdate": 1666671592390,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671592390,
        "tmdate": 1666671592390,
        "tddate": null,
        "forum": "6sQr2-BlARv",
        "replyto": "6sQr2-BlARv",
        "invitation": "ICLR.cc/2023/Conference/Paper2949/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the top-k classification problem. Different from the existing top-k classification setting, in this paper the labels (including the ground truth label) are ranked. It proposes a three-stage approach to this problem: \n1. base classifier construction, \n2. training data construction with relabeling, and \n3. training top-k classifier with a revised order-sensitive loss function. The proposed approach is tested on text and image datasets. ",
            "strength_and_weaknesses": "Strength:\n- The writing of this paper is clear. The proposed idea is presented in a way that even for readers not in the field could follow easily. \n- The empirical study has considered both text and image datasets, with different characteristics. And the proposed approach has produced reasonable results. \n\nWeaknesses:\n- I think the proposed setting, order-aware top-k classification, should be better motivated. \n- One of the key technical contributions is the loss function used to train the top-k classifier. More discussion about this loss function is desired.\n- I think the related work can also be strengthened. ",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned before, the clarity and the writing quality are good. I am not sure about the paper's novelty though. The key technical contribution, IMHO, is the introduction of the new loss function for training the top-k classifier. It is not a sophisticated change, and more discussion on training with this new loss could make the paper better. ",
            "summary_of_the_review": "- The motivating example in the introduction doesn't properly highlight the setting of the paper. Indeed that in the human-in-the-loop scenario,  we would like to present the annotators the most relevant data for annotation; But in this scenario, isn't the setting the same as retrieval or, even more so, label ranking, where only top-k ranked items are shown? I think coming up with an appealing motivating example is important, especially as the experiments are based on syntactic datasets. (By which I mean the datasets are augmented to study the setting.)\n\n- The setting considered in this paper is in fact exactly the same as the one in *label ranking*, when the evaluation focuses on top-ranked label(s). The authors should consider them in the related work. \n\n- Between Eq. (4) and (5): \"should **be** as large as possible\".\n\n- I was expecting more discussion about the modified loss function in Eq. (5). For example, when you put it on top of BERT, what does the training process look like? Does it converge fast?\n\n- Experiments. The results of LSAN and C-Tran are so underwhelming, that one would ask why they were included and why the authors called them state of the art for the respective tasks. No better baseline could be found?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2949/Reviewer_Pewp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2949/Reviewer_Pewp"
        ]
    },
    {
        "id": "i8XNRrjcqsx",
        "original": null,
        "number": 4,
        "cdate": 1666677357300,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677357300,
        "tmdate": 1670903967853,
        "tddate": null,
        "forum": "6sQr2-BlARv",
        "replyto": "6sQr2-BlARv",
        "invitation": "ICLR.cc/2023/Conference/Paper2949/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a three-stage approach to learning top-k classification with label ranking concerned during modeling. ",
            "strength_and_weaknesses": "Pros:\n1. The proposed method is simple and easy to follow.\n2. The experiments show that the proposed method outperforms other baselines.\n\nCons:\n1. My first concern is that training base classifiers to obtain the prediction uncertainty is time-consuming which is however not discussed in the paper. \n2. The three-stage approach which relabels the training data with multiple labels and enhances the ground-truth one is reasonable but not novel. The formulated objective, i.e., Eq. (5), is correct but is not very inspiring to me.\n3. Following 1&2, one can alternatively do dropout at stage 1 to have the uncertainty measure.\n4. The noisy label was mentioned in the introduction as a motivation but was not discussed later.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well-presented, but the novelty is not enough. Making the top-k classification a form of multi-label learning with the groundtruth class being encouraged is interesting but incremental.",
            "summary_of_the_review": "This work has not touched the core problem of top-k classification (e.g., non-differentiable) and some related (e.g., Sparsemax+top-K truncation) works are absent.\n\nSolving top-k classification from the view of multi-label perspective is interesting to some audiences but perhaps needs more technical supports.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2949/Reviewer_6iPJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2949/Reviewer_6iPJ"
        ]
    }
]