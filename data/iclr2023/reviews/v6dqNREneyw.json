[
    {
        "id": "vj8uJAWU23",
        "original": null,
        "number": 1,
        "cdate": 1666423812712,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666423812712,
        "tmdate": 1666423812712,
        "tddate": null,
        "forum": "v6dqNREneyw",
        "replyto": "v6dqNREneyw",
        "invitation": "ICLR.cc/2023/Conference/Paper89/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to address the multi-domain long-tailed recognition problem.         \n(1) An augmentation strategy is proposed for this goal:     \nI. For each time, uniformly sample an image across all classes and uniformly sample an image across all domains.     \nII. Each image representation is disentangled by class-relevant features and domain-relevant features.     \nIII. Combining the semantic representation of one with the domain-related nuisance factors of the other to form the augmented representation.    \n\n(2) Experiments are conducted on several long-tailed datasets with subpopulation shift and domain shift. Clear improvements are observed.\n",
            "strength_and_weaknesses": "Strength:    \n(1) The paper writes clear and is easy to follow.    \n(2) The method is simple and elegant.    \n(3) Experiments on different benchmarks show good improvements over baselines.    \n(4) Sound ablations are conducted.  \n\nWeakness:     \n(1) Some long-tailed methods are selected as baselines, like CRT, and MiSLAS. However, state-of-the-art long-tailed methods can be transferred to this multi-domain long-tailed recognition task, like PaCo [1], and RIDE [2].    \n\n(2) Would the representation augmentation module be removed at inference?    \n\n(3)  Regular augmentations like RandAug can also improve performance. Can the proposed TALLY work with them?\n\n(4) Can the proposed TALLY  work with long-tailed methods like PaCo [1], and RIDE[2]?\n\n\n[1] Parametric Contrastive Learning. ICCV 2021.\n[2] Long-tailed Recognition by Routing Diverse Distribution-Aware Experts. ICLR 2021.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Code is not provided.",
            "summary_of_the_review": "This paper proposes an augmentation strategy to address multi-domain long-tailed recognition. \nIf more advanced long-tailed methods, like PaCo and RIDE, are explored (as comparison baselines or whether the proposed method is complementary to them), the experimental results will be more convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper89/Reviewer_WENZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper89/Reviewer_WENZ"
        ]
    },
    {
        "id": "8tE6OlvvzP_",
        "original": null,
        "number": 2,
        "cdate": 1666473923865,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666473923865,
        "tmdate": 1668973696351,
        "tddate": null,
        "forum": "v6dqNREneyw",
        "replyto": "v6dqNREneyw",
        "invitation": "ICLR.cc/2023/Conference/Paper89/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a balanced sampling data augmentation technique, for addressing the multi-domain and long-tailed learning problems in which the number of samples in different classes is imbalanced and potential domain shift. First, the class-specific and domain-specific representations are decoupled by instance-normalization (Ulyanov et al., 2016),  thus new domain-class pairs can be formulated by a swap operation similar to style transfer (Huang and Belongie, 2017). Moreover, prototype vectors are computed and mixup techniques are used to make the class and domain representations more stable and diverse. The proposed approach is compared against single-domain long-tailed learning methods and domain adaptation or generalization methods and evaluated under two settings. (1) when all classes in test data are balanced (2) when test data is from an unseen domain. Performance improvements are demonstrated especially on synthetic datasets.  The ablation study demonstrated that the proposed approach works better on small-size classes and can learn more domain-invariant features.  ",
            "strength_and_weaknesses": "The paper studies the long-tailed learning problem in the multi-domain setting. Unlike traditional methods that can only upsample or downsample using data from the same domain, the proposed approach is able to borrow representations cross-domain, so the model can see data of more variety with data augmentation. To achieve this, this paper combines a few effective techniques, including instance normalization, moment matching, prototype, and mixup. Experiment results show improvements against many baseline methods on both synthetic and real-world datasets. \n\nThe difference between the selective balanced sampling strategy and balanced sampling is unclear. I read Section 3.2 and \u201cAnalysis of sampling strategies\u201d but still confusing. Figure 3 does not do a good job on explain the difference either.  What is the difference between y, d ~ Uniform(C, D) vs y~ Uniform(C) and d ~Uniform(D)? Can you explain more about the transition probabilities in Figure 3?\n\nFor the evaluation protocol, it is unclear what exactly is held-out? It seems in the long-tailed learning setting, all the classes and domains (except for one) are seen. It is very important to make sure there is no information leakage from training to test sets, and that the comparisons are fair. \n\nThe paper considered a wide range of baselines, including both domain adaptation and domain generalization methods. It would be helpful to explain the underlying assumptions of different methods, such as whether labels or data from the target domain is needed. Is equation (4) possible for a new unseen domain?",
            "clarity,_quality,_novelty_and_reproducibility": "The concept of subpopulation shift is confusing. Conventionally, it refers to the setting that the supclass remains the same but the subclasses comprising each supclass are different across domains. For example, \n\nSanturkar, S., Tsipras, D., & Madry, A. (2020). Breeds: Benchmarks for subpopulation shift.\u00a0*arXiv preprint arXiv:2008.04859*.\n\nIt seems here it means training domains are imbalanced but the classes in the test domain are balanced. This seems to be a convention in long-tailed learning, although may not be realistic. It is more related to the long-tailed setting, rather than the subpopulation shift in domain adaptation. Would it be better to report performance vs class size such as in Figure 4?  \n\nHow does the presented work compare to a simpler approach: train on imbalanced and fine-tune on balanced datasets? For example, domain-specific transfer learning? \n\nCui, Y., Song, Y., Sun, C., Howard, A., & Belongie, S. (2018). Large scale fine-grained categorization and domain-specific transfer learning. In\u00a0*Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 4109-4118).\n\nWhat does TALLY stand for?",
            "summary_of_the_review": "This paper presents an interesting combination of ideas for the multi-domain long-tailed setting and achieves improved results over many baselines. However, the sampling scheme is unclear, and I am not sure whether the evaluation protocol is fair. I\u2019ll vote for borderline reject. I will re-evaluate based on the authors' responses.\n\n######\nAfter rebuttal: Thank you for the response and additional experiments during the rebuttal. Most of my questions have been clarified. I have updated my score to reflect this. Two additional comments: \n\n- Selective balanced sampling: it seems the main difference to balanced sampling is whether the sampling of class and domain representations are done separately and in proportion to the sample size. The proposed method can avoid oversampling from the minority class, but the current text description and the transition probabilities make things overcomplicated.  \n\n- Domain adaptation baselines: Fig 5, CORAL should be an unsupervised domain adaptation method since it uses unlabeled data from the target domain. This should be clarified - as the proposed method does not even need to use any data or labels from the target domain. \n ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper89/Reviewer_ijLa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper89/Reviewer_ijLa"
        ]
    },
    {
        "id": "hVIM7-wXEc",
        "original": null,
        "number": 3,
        "cdate": 1666616902694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616902694,
        "tmdate": 1666621947059,
        "tddate": null,
        "forum": "v6dqNREneyw",
        "replyto": "v6dqNREneyw",
        "invitation": "ICLR.cc/2023/Conference/Paper89/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a multi-domain long-tailed recognition method to simultaneously address the two types of distribution shift: subpopulation shift and domain shift. The basic idea is sample augmentation where new samples are generated by representation disentanglement for tailed class. The experimental results on synthesis and real-world multi-domain long-tailed data show its effectiveness.",
            "strength_and_weaknesses": "Strengths: \n--the paper focus on a general problem: the multi-domain long-tailed learning.\n\n--The motivation is clear. The solution for multi-domain long-tailed learning is data augmentation by representation disentanglement, where any sample is synthesized with class-relevant semantic information and domain-associated nuisances. \n\n--This paper is well written.\n\nWeaknesses:\n\n--The novelty is limited. Representation disentangle and date augmentation are the common approaches for long-tailed recognition, and no novel approach is proposed for the specific issue of multi-domain long-tailed recognition.\n\n--Some details are questioned. For example, why the representation of each examples can be decoupled into semantic information and nuisances, and the domain information is contained in nuisances? \n\n--The construction of prototype is too simple to ineffective. I think the updating of prototypes is needed.",
            "clarity,_quality,_novelty_and_reproducibility": "the originality is there, but novelty are limited.",
            "summary_of_the_review": "the proposed method is effective and interesting, but some details are missed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper89/Reviewer_nEAQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper89/Reviewer_nEAQ"
        ]
    },
    {
        "id": "8sNB2lfDas",
        "original": null,
        "number": 4,
        "cdate": 1666731837171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731837171,
        "tmdate": 1669172780386,
        "tddate": null,
        "forum": "v6dqNREneyw",
        "replyto": "v6dqNREneyw",
        "invitation": "ICLR.cc/2023/Conference/Paper89/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on multi-domain long-tail classification, where each domain has its own long-tailed distribution. To solve this problem, the authors propose a novel balanced augment method TALLY, which separates and reassembles class-relevant semantic information and domain-associated nuisance information. Specifically, first, the representation of each instance is divided into semantic representation and nuisances. Then the semantic information and domain-associated factors are separately enhanced using argument techniques to improve stability and reduce noise. Finally, the semantic information and nuisances are reassembled to generate new instances for training. Several experiments on four synthetic multi-domain long-tailed datasets and two real-world datasets indicate that the proposed methods improve the overall performance for subpopulation shift and domain shift compared to two types of baselines.\n",
            "strength_and_weaknesses": "This paper focuses on a less-studied problem and defines the problem formally. With an example on the real-world dataset and some discussion, the authors illustrate why it is not trivial to explore the problem. Unlike single-domain long-tail classification, the authors figure out that multi-domains exist with their own long-tail distributions, and previous approaches may exacerbate class imbalance in this scenario.\nFor the implementation details of the proposed method, the authors give good motivation. For example, in proposing selective balanced sampling, the authors illustrate by figure the drawbacks of balanced sampling from all domain-class groups and how selective balanced sampling solves this problem.\nThe proposed approach is relatively novel. Although some of the techniques are derived from existing work, such as the representation disentanglement and reassembly section, the authors provide some new technical insights.\nThe experimental results broadly validate the authors' claim that TALLY is more effective than the existing single-domain imbalanced classification and domain-invariant learning methods under subpopulation shift and domain shift tasks. In addition, sufficient experiments are carried out to demonstrate the importance of each component in TALLY.\nHowever, my main concerns are some details in the model and experimental part of this paper. Here are the detailed comments:\n[Presentation] The paper's writing and statements are easy to follow. However, I have some minor suggestions. (1) Since one of the main contributions of the paper is to formalize the multi-domain long-tailed learning problem, clearer notation should be given to help future works better understand the problem and better use these notations. For example, the meanings of some symbols are explained but remain complex, considering the different superscripts and subscripts. (2) Also, some definitions should be better given in more obvious places, such as class-agnostic statistics u_d^{(0)} and v_d^{(0)}.\nExperimental results demonstrate the effectiveness of the proposed method, but some theoretical analysis can be given to shed light on the algorithm designs of TALLY and inspire future work.\n[Experiments] On domain generalization benchmarks, overall performance is used to measure the performance of subpopulation shift and domain shift; on naturally imbalanced multi-domain data, macro F1 score and average accuracy are used. However, I am not sure of the reason for using these measures. Considering that we are exploring the multi-domain long-tailed problem, should some evaluation metrics that measure tail performance be considered?\n[Reproducibility] Although the critical parts of the method are carefully clarified with the pseudo algorithm, the code of this paper is not given, which limits the reproducibility of this paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing and statements of this paper are easy to follow. The authors focus on an essential problem of multi-domain long-tail learning with good motivation and further propose a novel TALLY algorithm with balanced representation reassembly to solve this problem. Nevertheless, the code of the proposed method is not given, which limits reproducibility. \n",
            "summary_of_the_review": "This paper identifies a critical problem, multi-domain long-tail learning, and proposes a novel TALLY method to address this problem with reasonable motivation and some novelty. The experimental results on synthetic and real-world datasets illustrate the effectiveness of the proposed method.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper89/Reviewer_yuqM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper89/Reviewer_yuqM"
        ]
    }
]