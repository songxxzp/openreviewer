[
    {
        "id": "winlJZ9Wdud",
        "original": null,
        "number": 1,
        "cdate": 1666560011792,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560011792,
        "tmdate": 1666560011792,
        "tddate": null,
        "forum": "EFTpmFg9cb",
        "replyto": "EFTpmFg9cb",
        "invitation": "ICLR.cc/2023/Conference/Paper5360/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Seq2seq based is a popular approach for generative sequence labelling. However, the popoular log likelihood training doesn't always model well the loss each practical problem speicfies. \n\nIn this paper, the authors first pose such a setting, and then proposes a framework (GROOT) that allows to incoporate those information in generative sequence modelling frameworks. Authors propose two novel techniques to solve this issue: 1) correction function which incrementally improves over the current predicion model 2) Margin loss that can be applied to leverage current predicitons, gold annotations as well as the candidates generated by correction function. Through empirical studies, the authors show that their method improves over baseline as well as recently proposed methods.",
            "strength_and_weaknesses": "Strength:\n- The paper exposes a clear motivation and proposes novel correction function to better explore the space.\n- The paper shows strong empirical results\n- The paper has done quite thorough ablation studies showing the effectivenss of their methods as well being stable and not very senstible to various settings\n\nWeakness:\n- The paper exposes results in one particular reward function setting (section 4.1) which shows convincing results. However, to know that the proposed algorithm is effective, one should at least know the performance of other settings; it is mentioned in the paper that other settings yielded similar empirical findings, however I can not find those findings including appendix. Without this, I can't know if the paper has proposed an effective algorithm or not.\n- (Question) I don't' understand why the paper proposes to incorporate these methods only once the model is fined tuned. The method is general and can be applied as a fine tuning method. Not sure why there is no experiment as well as no mentioning on this.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has clear motivation and well exposes their method. The empirical results as well as the ablation studies show convincing results for their approach. The way to explore more candidates seem normal to me, although the proposed loss has no particular novelty. The margion loss needs to be adjusted in the approach so potentially hinders reproducibility; nevertheless, the approach is interesting and does seem to solve some practical issues.",
            "summary_of_the_review": "For generative sequence labelling, authors propose to model reward to better guide seq2seq learning. Authors propose two novel techniques to solve this issue: 1) correction function which incrementally improves over the current predicion model 2) Margin loss that can be applied to leverage current predicitons, gold annotations as well as the candidates generated by correction function. Through empirical studies, the authors show that their method improves over baseline as well as recently proposed methods.\n\nMy main concern is that I can't find support that the algorithms authors propose generalize to more scenarios than the ones posed in 4.1. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5360/Reviewer_oMZW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5360/Reviewer_oMZW"
        ]
    },
    {
        "id": "0V63doFIQi9",
        "original": null,
        "number": 2,
        "cdate": 1666685002054,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685002054,
        "tmdate": 1666685002054,
        "tddate": null,
        "forum": "EFTpmFg9cb",
        "replyto": "EFTpmFg9cb",
        "invitation": "ICLR.cc/2023/Conference/Paper5360/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes GROOT,  a novel and general reward optimization framework for training sequence labeling models toward reward metrics, via construct corrective candidates as contrastive examples to guide the model learning.",
            "strength_and_weaknesses": "Strengths:\n* The paper is well-written and easy to understand.\n* The proposed approach is novel and general.\n\nWeaknesses:\n* Although the authors use 4 datasets and include languages more than English, they don't provide details of their specific domains and tasks, and looks all of the 4 datasets are for semantic parsing task in the spoken language domain. As the paper mentions, the sequence labeling tasks range from NER to QA. Therefore, the experiments are too narrow to support their claims generally on top of sequence labeling tasks.\n* As a paper on sequence labeling tasks, the authors don't include the basic solution, token classification, as one of the baselines. Although this paper focuses on the generative manner, the performance of token classification should be still considered as important reference.\n* Moreover, the token classification can also be performed using the same sequence-to-sequence backbone model, just like how BART (https://arxiv.org/pdf/1910.13461.pdf) does in the QA task (SQuAD), which should be also considered as an important baseline.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: 3/5\nClarity: 5/5\nOriginality: 5/5\n",
            "summary_of_the_review": "Overall, the proposed method is novel and general, and paper is well-written, but the experiments cannot support the general claim because they lack task and domain diversity, and important baselines are missing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5360/Reviewer_PagF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5360/Reviewer_PagF"
        ]
    },
    {
        "id": "USxrerK1eX1",
        "original": null,
        "number": 3,
        "cdate": 1667658422794,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667658422794,
        "tmdate": 1667658422794,
        "tddate": null,
        "forum": "EFTpmFg9cb",
        "replyto": "EFTpmFg9cb",
        "invitation": "ICLR.cc/2023/Conference/Paper5360/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a new framework for training sequence labeling models to optimize reward metrics, and a CML loss to help the model better understand the reward space. The experimental results show superiority compared with recent baselines. ",
            "strength_and_weaknesses": "### Pros\n1. The proposed GROOT is simple, effective, and easy to implement.\n2. The whole framework is clear.\n3. The experimental results show great improvement in reward function and precision.\n\n### Cons\n1. The relationship and difference with *Shu++* should be discussed more clearly.\n2. Only one machine translation baseline *Shu++* was compared. It will be more convincing if it is compared with more baselines.\n3. Sufficient experiments and figures are provided, but there are no consistent performance gains across different datasets. \n4. There should be more descriptions of the datasets.\n4. Some statements are difficult to read and may contain some grammatical errors, e.g, *One may assume that just the above may be suf\ufb01cient for models to be learn the output space suf\ufb01ciently as done in prior work (Shu et al., 2021)* on page 4, and *Correct erroneous candidate* paragraph on page 4, and *Works by replace incorrect tags in y with the correct ground-truth tags* on page 6. The authors should pay more attention to the writing.",
            "clarity,_quality,_novelty_and_reproducibility": "- The clarification can be clearer.\n- The proposed method could help to improve the performance of the sequence labeling task.\n- This work should be original.\n- The paper presents experimental details for reproducibility.",
            "summary_of_the_review": "I'd like to assign 5 (marginally below the acceptance threshold). I hope the authors could make the clarification clearer and do more analysis on the influences of different modules/hyperparameters.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5360/Reviewer_cdc3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5360/Reviewer_cdc3"
        ]
    }
]