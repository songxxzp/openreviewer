[
    {
        "id": "pty3Rg1MTH",
        "original": null,
        "number": 1,
        "cdate": 1666661876815,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661876815,
        "tmdate": 1666661876815,
        "tddate": null,
        "forum": "7pl0FRiS0Td",
        "replyto": "7pl0FRiS0Td",
        "invitation": "ICLR.cc/2023/Conference/Paper4012/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a method based on utilizing\u00a0the \"offline trajectories\" as a latent\u00a0variable (prompt), and then conditioning the behavior on the online agent on the latent\u00a0variable. The paper also explores using a model based planner to achieve higher reward by adapting the latent variable which the paper mentions as \"prompt\u00a0tuning\". The paper evaluates the proposed method across different settings: offline single-agent RL on the D4RL dataset, offline MetaRL on the MuJoCo benchmark.\u00a0",
            "strength_and_weaknesses": "Strengths\n\n- The paper is very well written, and motivated. \n- The underlying idea is not \"new\" per se, but the way the idea is instantiated is interesting and very relevant. The paper also explores the idea in various different settings such as offline RL and offline MetaRL.\n- The reviewer appreciate the ablations provided in the paper to dissect where the improvement is coming from.\n- The reviewer appreciates the comparison with various different baselines for offline meta-RL tasks such as CBCQ, FOCAL, Batch-Pearl.\n\nWeaknesses\n\n- One way to improve the paper, is perhaps building on the offline MetaRL results. It's interesting to see that the performance of the proposed method is zero-shot as compared to other baselines which requires some context. Further evaluating the method on more complex tasks which requires adaptation as well as exploring how the adaptation varies by varying the nature of the data in the offline dataset would be very interesting. \n- There are aspects of the work, which are already explored in the literature like learning a policy conditioned on the \"past\" experiences as done in Retrieval Augmented RL [1, 2]. These methods learn an encoding function using self-supervised learning, and then based on the context of the agent, figures out what information to retrieve to condition the current policy. The interesting difference is A_{g} in the current work is parameterized as  auto-regressive transformer which is trained via state, action and reward prediction, whereas in [1] it's parameterized as a neural network (non-LSTM) and trained with RL/behavior cloning (A_{e} is still parameterized as a bidirectional transformer. \n\nMinor point:\n\n- The reviewer is not sure if it's really important to mention the point \"\"CMT is the first algorithm to solve offline meta-RL from a sequence modeling perspective\". Not sure what \"solving\" means here especially after only evaluating on Ant, Half-Cheetah, Walker. \n\n\n[1] Retrieval-Augmented Reinforcement Learning, https://arxiv.org/abs/2202.08417 (ICML'22) ,\\\n[2] Large-Scale Retrieval for Reinforcement Learning, https://arxiv.org/abs/2206.05314 (NeurIPS'22) \n  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is very easy to read.\n\nNovelty: The individual \"components\" are known, and the way the paper tries to combine individual different components is very interesting. \n\nReproducibility: As \"individual\" components are well known or well \"used\" it should be easy to reproduce the proposed idea.",
            "summary_of_the_review": "The paper explores an interesting problem i.e., how to improve the policy given some offline data. The way the paper instantiates the proposed idea is interesting. It would be useful to evaluate the method on more complex tasks to further study the behavior of the proposed model. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4012/Reviewer_SQ2X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4012/Reviewer_SQ2X"
        ]
    },
    {
        "id": "r4JCxGc30L",
        "original": null,
        "number": 2,
        "cdate": 1666670950656,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670950656,
        "tmdate": 1666670950656,
        "tddate": null,
        "forum": "7pl0FRiS0Td",
        "replyto": "7pl0FRiS0Td",
        "invitation": "ICLR.cc/2023/Conference/Paper4012/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates how to improve decision transformers with prompt and meta-learning. Evaluation on D4RL and MuJoCo benchmark tasks shows that the proposed method outperforms other baselines in some cases. ",
            "strength_and_weaknesses": "# Strength\n\n- Improving decision transformers with prompting is natural.\n- The proposed prompt mechanism is well-designed.\n\n# Weakness\n\n- This paper failed to provide a complete literature survey. Especially, \"Prompting decision transformer for few-shot policy generalization, ICML 2022\u201d leverages the sequential modeling ability of the Transformer architecture and the prompt framework to achieve few-shot adaptation in offline RL, which is very similar to this paper\u2019s approach. The authors of this submission should clearly explain what are the differences and properly cite this ICML paper.\n- The performance of CMT doesn't seem to be impressive and robust. It would be better if the authors could conduct experiments to probe how performance varies according to different task prompts.\n- Figure 5 is not clear enough for me. Fig. 5 should be segmented into 3 or 4 different tables or figures to separately investigate different hyper-parameters' influence. Only providing C-L=10^{-6}, C-L=1, B-C = 0, and B-C = 1 settings cannot completely show how these hyper-parameters influence the model.\n- There is a performance gap between the proposed model and IQL. The performance should be further improved. Or the author should explain why the performance gap is acceptable at least.",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\n\nThe paper is well-written and easy to follow\n\n# Quality and Novelty\n\nAs far as I know, both the prompt design and meta-learning algorithms used in this paper are not new. The only novelty seems to be the combination of prompting and meta-learning for offline RL settings. \n\n# Reproducibility\n\nAlthough the authors provide the details like hyper-parameters in the Appendix, the authors did not provide their source code. They claim they will open source it in rebuttal. ",
            "summary_of_the_review": "This paper investigates how to improve decision transformers with prompt and meta-learning. But the proposed method does not show significant and consistent improvements over other baselines. More importantly, prompting decision transformers has been introduced in an ICML paper before. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4012/Reviewer_7JkM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4012/Reviewer_7JkM"
        ]
    },
    {
        "id": "kc2GiiYCcZ",
        "original": null,
        "number": 3,
        "cdate": 1666856131779,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666856131779,
        "tmdate": 1666856131779,
        "tddate": null,
        "forum": "7pl0FRiS0Td",
        "replyto": "7pl0FRiS0Td",
        "invitation": "ICLR.cc/2023/Conference/Paper4012/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents Contextual Meta Transformer (CMT) which is an offline RL algorithm based on prompt tuning. It targets on training a large-scale model that can be utilized on various downstream tasks from the sequence modeling perspective. The training of CMT consists of two stage, i.e. representation stage and improvement stage. The prompt tuning is designed for offline RL and guide the autoregressive model to generate trajectories with high rewards. Experiments are conducted in three different RL settings, offline single-agent RL (D4RL), offline Meta-RL(MuJoCo). and offilien MARL (SMAC).",
            "strength_and_weaknesses": "Pros:\nThis work is well motivated which is exploring an interesting problem: how prompts can help sequence modeling for offline RL algorithms. To achieve this, a Transformer encoder-decoder architecture is presented, technically it is a combination of a bidirectional transformer and a causal transformer (DT). To pretrain such a model, two pretraining objectives are leveraged, i.e. autoregressive prediction and a contrastive loss as auxiliary loss during training. To evaluate the proposed approach, three different settings are been conducted, i.e. meta RL, offline RL and prompt tuning. \n\nCons:\n1.the writing is hard to follow. Some statements are conflicted overall, e.g. in abstract, it is claim that this approach is training with supervised loss ('as such, we can pretrain a model ..... with supervised loss and .....'). While in Introduction, it claims that CMT is trained in a self-supervised manner (... Firstly, a model is pretrained on the .... through a self-supervised learning method). Based on my understanding, the training of CMT leverages two losses, where the autoregressive prediction is a supervised loss while the contrastive loss is a self-supervised loss. I would recommend the authors to keep the descriptions consistent in the whole paper. Otherwise, it is very confusing to the readers.\n2. lack of implementation details and experiment settings. For example, as a pretraining work, I did not find the pretraining and finetuning datasets details, especially the coverage of tasks. Are you evaluating and/or finetuning on the same tasks or unseen tasks? Without such information, it is hard the validate the generalization ability of CMT.\n3. The technical novelty is limited. From model architecture aspect, CMT is a combination of existing approaches, DT and bidirectional transformer. From algorithm aspect, it is an autoregressive loss (DT style) and contrastive loss. I can understand that this work is trying to propose a new pretraining regime, however, the usage is also not natural to me. Since the prompt  strategy can be directly utilized with only a causal transformer, in this case the DT. Then why an extra bidirectional transformer is needed? I did not find a clear answer in this paper. The rational under the contrastive loss is also unclear to me. For a specific task in a certain environment, the policy should be the same. However, based on the definition of positive and negative samples, it assumes that only the segments from the same trajectory share a policy while the other trajectories have different policies. I am confused about this setting, unless this contrastive loss are capturing other information or factors lie in trajectories, e.g. spatio-temporal information.\n4. the gains of CMT are marginal when comparing with other approaches. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is hard to follow. Some statements are conflicted overall, e.g. in abstract, it is claim that this approach is training with supervised loss ('as such, we can pretrain a model ..... with supervised loss and .....'). While in Introduction, it claims that CMT is trained in a self-supervised manner (... Firstly, a model is pretrained on the .... through a self-supervised learning method). Based on my understanding, the training of CMT leverages two losses, where the autoregressive prediction is a supervised loss while the contrastive loss is a self-supervised loss. I would recommend the authors to keep the descriptions consistent in the whole paper. Otherwise, it is very confusing to the readers.  lack of implementation details and experiment settings. For example, as a pretraining work, I did not find the pretraining and finetuning datasets details, especially the coverage of tasks. Are you evaluating and/or finetuning on the same tasks or unseen tasks? Without such information, it is hard the validate the generalization ability of CMT.\n\nThe gains of CMT are marginal when comparing with other approaches. \n\n\nThe technical novelty is limited. From model architecture aspect, CMT is a combination of existing approaches, DT and bidirectional transformer. From algorithm aspect, it is an autoregressive loss (DT style) and contrastive loss. I can understand that this work is trying to propose a new pretraining regime, however, the usage is also not natural to me. Since the prompt  strategy can be directly utilized with only a causal transformer, in this case the DT. Then why an extra bidirectional transformer is needed? I did not find a clear answer in this paper. The rational under the contrastive loss is also unclear to me. For a specific task in a certain environment, the policy should be the same. However, based on the definition of positive and negative samples, it assumes that only the segments from the same trajectory share a policy while the other trajectories have different policies. I am confused about this setting, unless this contrastive loss are capturing other information or factors lie in trajectories, e.g. spatio-temporal information.\n\nMinor:\nThere are no source codes are submitted.\nTypo\uff1a Sec 4.2 paragraph 1, should be Fig. 2 not Fig (3).",
            "summary_of_the_review": "This work is well motivated which is exploring an interesting problem: how prompts can help sequence modeling for offline RL algorithms. To achieve this, a Transformer encoder-decoder architecture is presented, technically it is a combination of a bidirectional transformer and a causal transformer (DT). To pretrain such a model, two pretraining objectives are leveraged, i.e. autoregressive prediction and a contrastive loss as auxiliary loss during training. To evaluate the proposed approach, three different settings are been conducted, i.e. meta RL, offline RL and prompt tuning. \n\nHowever, there are some concerns:\n1.the writing is hard to follow. Some statements are conflicted overall, e.g. in abstract, it is claim that this approach is training with supervised loss ('as such, we can pretrain a model ..... with supervised loss and .....'). While in Introduction, it claims that CMT is trained in a self-supervised manner (... Firstly, a model is pretrained on the .... through a self-supervised learning method). Based on my understanding, the training of CMT leverages two losses, where the autoregressive prediction is a supervised loss while the contrastive loss is a self-supervised loss. I would recommend the authors to keep the descriptions consistent in the whole paper. Otherwise, it is very confusing to the readers.\n2. lack of implementation details and experiment settings. For example, as a pretraining work, I did not find the pretraining and finetuning datasets details, especially the coverage of tasks. Are you evaluating and/or finetuning on the same tasks or unseen tasks? Without such information, it is hard the validate the generalization ability of CMT.\n3. The technical novelty is limited. From model architecture aspect, CMT is a combination of existing approaches, DT and bidirectional transformer. From algorithm aspect, it is an autoregressive loss (DT style) and contrastive loss. I can understand that this work is trying to propose a new pretraining regime, however, the usage is also not natural to me. Since the prompt  strategy can be directly utilized with only a causal transformer, in this case the DT. Then why an extra bidirectional transformer is needed? I did not find a clear answer in this paper. The rational under the contrastive loss is also unclear to me. For a specific task in a certain environment, the policy should be the same. However, based on the definition of positive and negative samples, it assumes that only the segments from the same trajectory share a policy while the other trajectories have different policies. I am confused about this setting, unless this contrastive loss are capturing other information or factors lie in trajectories, e.g. spatio-temporal information.\n4. the gains of CMT are marginal when comparing with other approaches. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4012/Reviewer_9AEp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4012/Reviewer_9AEp"
        ]
    },
    {
        "id": "DSVRc_nFiON",
        "original": null,
        "number": 4,
        "cdate": 1667182175145,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667182175145,
        "tmdate": 1667182175145,
        "tddate": null,
        "forum": "7pl0FRiS0Td",
        "replyto": "7pl0FRiS0Td",
        "invitation": "ICLR.cc/2023/Conference/Paper4012/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a transformer based method to solve offline meta reinforcement learning and offline reinforcement learning tasks. An auto-regressive transformer is learned on the existing trajectory, and prompt tuning is used to guide the prediction towards policy improvement. There is an additional contrastive loss term used to ensure better clustering properties. ",
            "strength_and_weaknesses": "Strength:\n- The paper is well-written and easy to follow.\n- Experiments are thorough, for offline, meta-offline, and ablation studies. \n- The proposed method aligns well with existing literature\n\nWeaknesses:\n- Hyper-parameters have been tuned to improve performance, but the process has not been explained. It is unclear if there is over-fitting to test data. \n- In LLMs, the size of the transformer model makes a large difference in performance. This has not been discussed or investigated. \n- Prompt-tuning relies on beam search. This will only work well if the replay buffer has sufficient \"good\" data points. This is evident with poor performance on Medium-replay buffers. This should be analyzed and discussed. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Very good.\n\nQuality: Good. Would be stronger if the weaknesses above are addressed.\n\nNovelty: Application to meta-offline-RL tasks is new. Methods are adopted directly from LLM community. \n\nReproducibility: Good. ",
            "summary_of_the_review": "Paper is good overall. It would be good to address the weaknesses listed in a revised version of the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4012/Reviewer_yV6V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4012/Reviewer_yV6V"
        ]
    }
]