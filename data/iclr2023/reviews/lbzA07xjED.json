[
    {
        "id": "mgScNqWzHk9",
        "original": null,
        "number": 1,
        "cdate": 1666395768191,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666395768191,
        "tmdate": 1666395768191,
        "tddate": null,
        "forum": "lbzA07xjED",
        "replyto": "lbzA07xjED",
        "invitation": "ICLR.cc/2023/Conference/Paper2226/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper analyzes the FL setup with error feedback for non convex objective. It analyzes two variants of the FED-EF algorithm, one with SGD and the other one with AMS. Theoretical analysis is done in both setting, in particular it is shown that a linear speedup is possible, i.e., the error rate goes down with $1/\\sqrt{n}$ when $T\\geq K n$, where $K$ is the number of local steps and $n$ is the number of machines. The partial participation setting is also analyzed. Experiments are done on MNIST, CIFAR and FMNIST.",
            "strength_and_weaknesses": "Strength: The paper shows linear speedup (a $n$ dependent term in the statistical rate) with the number of participating clients.\n\nWeaknesses:\n\n**Missing Comparison and Prior work:** The paper does not compare with several existing related works. The problem of Error Feedback in FL is studied extensively contrary to the claim of the authors. For example, the following papers\n\n (a)  `EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback' by Richtarik et. al;\n (b) `EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback' by Richtarik et. al; \n(c) `Communication-Efficient and Byzantine-Robust Distributed Learning With Error Feedback  ' by Ghosh et.al;\n\naddress similar distributed/Federated aspects with error feedback. These papers should be discussed and the results should be compared with the ones in these papers. I believe  `EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback' is the closest one, and a thorough comparison is required.\n\n**Overclaim:** The paper seems to overclaim on certain sections. Ex: `partial-participation has never been considered'--this is not correct.  `EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback' considers partial client participation. Moreover, the paper claims ` One of the very few, if not only, FL algorithms that uses biased compression with EF is QSparse-local-SGD'--the paper   `Communication-Efficient and Byzantine-Robust Distributed Learning With Error Feedback  ' considers the biased compression setting with error feedback as well.\n\n**Novelty:** The technical analysis, setup has a lot of overlap with `Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning' by Yang et. al, including the choice of step sizes. The proofs use the same arguments as the above mentioned paper, with the additional error feedback term. Please comment on this aspect and address the technical novelty of the work.\n\n**Choice of Step Sizes:** The choice of step size is quite non-standard (they are the same as in  `Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning' by Yang et. al). The local step sizes are very very small (overall with $K$ local iterations, the local iterates do not seem to move/drift enough it seems. On the other hand the global step size is very very big ($\\sqrt{K n}$). Please provide some intuitive explanations, on this choice.\n\n**Typo:** I believe the first term in the right hand side of Theorem 1 has a $1/T$ missing; otherwise, the algorithm would diverge given the step size choices.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. For novelty and quality, please see the weakness section.",
            "summary_of_the_review": "Please see the Strengths/Weaknesses section. The paper misses comparison with several existing works and I also have some technical novelty concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2226/Reviewer_GNNs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2226/Reviewer_GNNs"
        ]
    },
    {
        "id": "u_hCck9ZRyF",
        "original": null,
        "number": 2,
        "cdate": 1666632952376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632952376,
        "tmdate": 1666632952376,
        "tddate": null,
        "forum": "lbzA07xjED",
        "replyto": "lbzA07xjED",
        "invitation": "ICLR.cc/2023/Conference/Paper2226/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the paper, the authors consider the federated learning problem. The authors provide an algorithm that compresses the gradient before sending it from client to server (to combat communication cost), accounting for error of compression from previous iteration. Two versions of the algorithm is presented: Fed-EF-SGD and a momentum based FED-EF-AMS.  Theoretical convergence analysis on the convergence is provided (under reasonable conditions) and performance is shown to be on par with uncompressed full Federated Learning algorithm. An analysis on convergence rate under partial participation from the clients is also provided. Some numerical experiments are also provided and validate their theory.",
            "strength_and_weaknesses": "- The paper provides the first convergence analysis of compressed adaptive. The paper itself is well-organized.\n    \n - The is limited to an analysis for convergence to a neighborhood around a stationary point and not a global minimizer (or local). \n\n- In page 2: \"we present the general compressed FL framework named Fed-EF\" seems to suggest the algorithm does not use Error Feedback. Please consider modifying the phrase appropriately. Also, it is not clear what MLP stand for in page 2?\n - How is the stochastic gradient term $g_{i,i}^{(k)}$ introduced in Assumption 2 used? It unclear from Assumptions 1-3 and Algorithm 1.\n - From Figure 2, its not clear if methods with Error Feedback performs better than methods without. In most case (the figure is not big enough to be able to make a good comparison), method with EF and without EF seems to perform on par and the claim in page 7 \"test accuracy of Stoc is slightly lower than Fed-EF with hv-Sign\" seems selective to that instance of the experiment. It is not clear from the figures shown if the behaviour shown holds on average or is from one instance of the experiment.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is pleasant to read. The ideas in the paper are not new but the application to the setting of Federated learning is novel. Sufficient details of the experiments conducted are included in the paper.",
            "summary_of_the_review": "I believe the paper address an open problem in the setting of Federated Learning and provides compelling theorical and numerical evidence to support their claims.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2226/Reviewer_FJCK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2226/Reviewer_FJCK"
        ]
    },
    {
        "id": "jJRfOoKcgH",
        "original": null,
        "number": 3,
        "cdate": 1666973469360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666973469360,
        "tmdate": 1670028687312,
        "tddate": null,
        "forum": "lbzA07xjED",
        "replyto": "lbzA07xjED",
        "invitation": "ICLR.cc/2023/Conference/Paper2226/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "## Update\nI have read the authors' feedback and I remain confident this paper is not ready for publication for these reasons:\n1. The authors argued that their work is different from that of Fatkhulin et al. in that it uses the standard EF. I do not see, however, why standard EF would be better than EF21, I don't think that this difference justifies novelty.\n2. The authors also argued that they consider Adam stepsizes on top of that. The theory for Adam stepsizes is, however, underwhelming due to dependence on the dimension, which is caused by the fact that even for Adam itself we do not have a good analysis.\n3. The authors argue that $\\eta\\eta_l$ has to approach 0 as we target higher accuracy, which I agree with. However, once we set the value of $\\eta\\eta_l$, it is always beneficial to decrease $\\eta_l$ and increase $\\eta$ while preserving the value of $\\eta\\eta_l$. This implies that the local steps are not shown to be useful, even if the authors show that local steps are helpful under other (suboptimal) choices of $\\eta$ and $\\eta_l$.\n4. Please note that I **was not** \"raising the concern regarding the general research direction of communication compression in FL.\". In contrast, I was arguing that the compression used in the experiments might not work well with the communication primitives such as all-reduce. In other words, my comments were toward the experiments and not the research direction.\n\n#######  \nThis paper studies the combination of error feedback for compressed optimization with local method for federated learning. On top of that, the authors consider two potential ways of updating the global model based on local updates: with standard averaging and with Adam-based model update. The main contribution of the paper is to propose Algorithm 1 that combines all these features and to study its convergence with full and partial participation. The theory is supported by numerical experiments that show how different variants of Algorithm 1 based on different compressions and stepsizes perform on training neural networks.",
            "strength_and_weaknesses": "## Strengths\n1. The work addresses practical issues arising in federated learning: communication efficiency and partial participation of clients, and the authors make an advance in the considered direction.\n2. The rates are good in terms of the asymptotic dependence on the number of iterations.\n3. The theory is supported by experiments.\n\n## Weaknesses\n1. The paper makes some misleading claims about the novelty of the method. In particular, the claim that \"Partial participation (PP) has never been considered for error feedback in distributed learning.\" is not correct, see (Fatkhulin et al., 2021).\n2. The authors list as one of their contributions proposing \"a unified compressed FL framework\". As far as I can see, the unified framework has just two options (SGD and AMS) and calling it \"unified\" seems to be an overstatement.\n3. As far as I can see, Theorem 1 suggests that there is no benefit from the local steps. In particular, if $\\eta\\eta_l=\\mathrm{const}$, then the bound improves as $\\eta_l\\to 0$, which means that the local steps help only due to sampling more gradients. The same can be said about Theorem 2.\n4. All experiments seem to use a single random seed and no confidence intervals are reported. \n5. The experiments only present the number of communicated bits as the efficiency metric, which ignores the computation overhead of compressing the updates as well as it does not take into account if it is actually easier to perform communication of the compressed vectors. For instance, TopK does not support synchronization primitives (over batches of clients) such as all-reduce. Moreover, it is not clear if compressing the updates make so much sense since we already ask the clients to perform a lot of local updates, which reduces the communication bottleneck.\n6. Clients are required to maintain a state. When partial participation is considered, it is reported by Kairouz et al. \"Advances and Open Problems in Federated Learning\" that one might not see the same client twice, so stateful methods make less sense with PP.\n\nIlyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, Peter Richt\u00e1rik, 2021, \"EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback\"",
            "clarity,_quality,_novelty_and_reproducibility": "Some parts were a bit confusing to me.  \nWhy is $\\Delta$ called \"the effective gradient\"? As far as I can see, it's not a gradient of any function\n\n### Minor issues\nOn page 2, \"which are biased of the true gradients\", do you mean \"biased compressors\"?  \nOn page 5, \"the above two terms stays close\" => \"stay close\"  \nI think it would be good to add the definition of the \"Stoc\" compressor in the paper\n",
            "summary_of_the_review": "The considered setting might not be very meaningful due to the need for clients to maintain a state, while partial participation is usually employed when the number of clients is large and it is uncommon for them to keep a state. The theory has some small issues (no benefit from local steps). The experiments show only one side of the method's performance\u2013number of communicated bits, which is not the only aspect of time efficiency\u2013and do not illustrate if there might be an actual saving of time.  \n\nAll in all, this paper will probably have some audience, but the results are not impressive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2226/Reviewer_Xm1Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2226/Reviewer_Xm1Y"
        ]
    },
    {
        "id": "349PV-cNRS",
        "original": null,
        "number": 4,
        "cdate": 1667525004293,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667525004293,
        "tmdate": 1667525004293,
        "tddate": null,
        "forum": "lbzA07xjED",
        "replyto": "lbzA07xjED",
        "invitation": "ICLR.cc/2023/Conference/Paper2226/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "### Background\n\nIn the distributed optimization paradigm, it is common for the client nodes to communicate only compressed version of the computed local gradients (or parameters) with the server node to reduce communication latency. However, since compression schemes are often lossy (in expectation) and biased (ex Top K) , working with the compressed gradients might result in slower overall convergence. To alleviate this, it is common practice to use error compensation where the error by propagating compressed gradient ||g - C(g)|| is accumulated and used as feedback in subsequent iterations. Error Feedback ensures that the SGD converges at the same rate as the non-compressed counterpart. \n\n\n### Contribution:\n\nIn this paper, EF is applied and analyzed in the federated setting as follows: The server broadcasts the global model; each client (each client maintains an error vector e initialized to 0 ) does k local SGD steps; computes the error compensated gradient g = (gradient + e); computes the C(g); computes the error from compression; updates e by accumulating this additional error.  For the analysis, the authors also considered potentially heterogenous data distribution across clients ; partial client participation, adaptive gradient method at the global server.  ",
            "strength_and_weaknesses": "### Strengths:\n1. The paper is well motivated and written. \n\n2. The main contribution of the paper is the analysis of SGD and AMSGrad in the Heterogenous Federated setting ( heterogenous client data distribution +  local SGD, partial client participation ) + ( communication compression + EF ). \n\n3. The proofs are clear and rigorous and easy to follow. \n\n\n### Weaknesses / Clarifications : \n1. Definition 1. (qC - deviate compressor) It simply says the compression operator yields a contraction mapping. This is a standard assumption used in most theoretical works in communication compression and error feedback literature: ex see [1-2].  Mentioning this connection with proper citation around Def 1 in the main paper (as opposed to making this connection in Appendix. A.1) will improve clarity, \n\n2. Algorithm 1. is a straightforward ( almost trivial ) extension of distributed SGD with EF to Federated Learning setting;  so overall I feel algorithmic novelty is quite limited. \n\n3. In Fig 18-Title missing bracket inside the expectation - i guess it should be $ E \\|\\| C(x) - x \\|\\|^2 \\leq q_c^2\\|\\|x\\|\\|^2 $ where $ x = \\frac{1}{n}\\sum_{i=1}^n(\\Delta_{t, i} +e_{t,i}) $. What do the subfigures correspond to - not clearly specified : I am *assuming* left one is for normal and right for laplacian data. \n\n4. Let $\\epsilon^2 = || C(x) - x ||^2$ Then I believe the points on Fig 18 denote $\\frac{\\epsilon^2}{||x||^2}$ - However, it is not clear from the fig or B.1 on how C was used : e.g. Top-k what is k values ? In theory the bound trivially holds for $q_c = (1 - \\frac{k}{d})$ but empirically how does it correlate on the simulated data. In other words, additional analysis wrt k vs q_c over different heterogeneity (scale) would be more insightful.\n\n5. What is the significant of the offset between Definition 1 and assumption 1 -- as seen in Fig 18. \n\n6. Definition 1 has clear motivation , I am not clear on what is the intuition / how to interpret Assumption 1.  I in fact think that Assumption 1 is unrealistic and Fig 18 shows how in non-iid scenarios it is completely off from bounds derived from Definition 1 ( i.e. from reality ) - Could you please justify this ?  \n\n7.  Empirical Evaluation: We know in non-fed EF-SGD error feedback improves the results tremendously - and can give nearly same rate of convergence as full-SGD while using only ~10 % communication. The results in the fed setting should have identical trends - as confirmed by the experiments (not surprising - right ? )\n\n8. How was the communicated bits calculated in Fig 2, 3 etc ? Let's say for Top k one would use different k values to compute log comm bits -- it is not clearly mentioned how these plots are generated. Also, are these plots using EF ? It is not clearly mentioned and hard to read the figures - please update the captions to be self contained. \n\n9. What is the exact mechanism to simulate non-iid clients ? How did you control amount of heterogeneity in experiments ? \n\n10.  Further, how does heterogeneity impact the results especially since we see in Fig 18 with high heterogeneity Assumption 1 has larger offset than Definition 1 ( i.e. from reality ) ? \n\n11. Exp are small scale : done on MNIST, FMNIST. One slightly more challenging task (at least CIFAR 10 / CIFAR 100 /mini-imageNet ) might help with validating the claims - ex.  \n\n### References:\n1. https://papers.nips.cc/paper/2018/file/b440509a0106086a67bc2ea9df0a1dab-Paper.pdf\n2. https://proceedings.mlr.press/v97/karimireddy19a/karimireddy19a.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "#### Clarity:\nOverall the paper is well written; However, I feel the figures should be self explanatory from the figure title itself. Often the titles are missing / inadequate weakening readability. \n\n#### Novelty:\nI think novelty is limited (see Weakness and Summary of review)\n\n#### Reproducibility:\nNeeds more clarification and experimental details. (see Weakness)",
            "summary_of_the_review": "Overall, The contribution is quite incremental I feel. Firstly, the proposed algorithm is straightforward extension of distributed SGD with communication compression and error feedback [1,2]. Secondly, The main contribution is the analysis - which also I think is somewhat incremental overall given the tools to analyze already exist and the proof technique is also similar.  Thirdly, empirical evaluation is small scale overall. Fourthly, Assumption 1 is not clear - and seems unrealistic. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2226/Reviewer_SCXf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2226/Reviewer_SCXf"
        ]
    }
]