[
    {
        "id": "oY8YaqE28v",
        "original": null,
        "number": 1,
        "cdate": 1666594164961,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594164961,
        "tmdate": 1666594164961,
        "tddate": null,
        "forum": "rjYUBo_uWEs",
        "replyto": "rjYUBo_uWEs",
        "invitation": "ICLR.cc/2023/Conference/Paper850/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates properties of trained manifolds with different learning methods on simple image data. For this, the authors propose data augmentation method and define metrics that lead to quantification of the quality of the overall manifold. They show that the proposed metric, i.e., RMQM, has high correlation with performances of several downstream prediction tasks. ",
            "strength_and_weaknesses": "Strength:\n- Novelty. This paper relates prediction performance of downstream tasks with the characteristic of learnt manifold, which I think is quite interesting. \n\nWeakness:\n- Presentation of the paper. Too much white spaces and small text in the figures. Captions do not sufficiently describe the figures. \n- \"trained with different methods\" in the 4th paragraph in the intro is a little confusing. I thought \"different methods\" would refer to different optimization scheme but later it turns out that they refer to different tasks. \n- Perhaps a figure of the overall pipeline of the method will help a reader understand the ideas and aims of this paper. \n- There is a big jump going from Fig 2 to Fig 3 as two nubs are turned at the same time, i.e., dataset and perturbation method. It is difficult to tell which factor is causing \n- The authors propose a metric RMQM to measure the quality (e.g., smoothness) of the learned manifold. There must be other metrics, I am sorry but I am not sure which ones though (but at least the authors have done literature survey on manifold learning and comparison methods), but there is no way to tell that RMQM is the right one. \n- Lack of theory. While the paper is proposing interesting theories, they are only empirically validated with very limited settings, and it is not certain if the same observation will extend to more complicated experiments especially that may contain exhaustive local minima. ",
            "clarity,_quality,_novelty_and_reproducibility": "The text is quite clear (but the overall presentation of the paper is poor) and I think it will be easy to reproduce the work done in this paper. ",
            "summary_of_the_review": "The authors provide an interesting aspect of training machine learning models for interpretability. However, the ideas are not sufficiently validated or proved, and they should be better presented. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper850/Reviewer_xaAq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper850/Reviewer_xaAq"
        ]
    },
    {
        "id": "w8Cywzl45U0",
        "original": null,
        "number": 2,
        "cdate": 1666633172096,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633172096,
        "tmdate": 1666633172096,
        "tddate": null,
        "forum": "rjYUBo_uWEs",
        "replyto": "rjYUBo_uWEs",
        "invitation": "ICLR.cc/2023/Conference/Paper850/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a framework and new methods to measure and compare different representation manifolds (RMs) to explore various pretraining methods. The analyses show that some self-supervised methods learn an RM where alterations lead to large but constant size changes, indicating a smoother RM than fully supervised methods. These measurements are integrated to develop a new  method called the Representation Manifold Quality Metric (RMQM), which is used to explore relationship between accuracy of models and RMs on downstream tasks.",
            "strength_and_weaknesses": "The paper addresses an important problem of deep learning.\n\nHowever, there are various major and minor problems with the paper while addressing this problem. Briefly, there are unclear definitions and proposed methods are not employed correctly following common definitions and structures of manifolds. In addition, experimental analyses should be improved.",
            "clarity,_quality,_novelty_and_reproducibility": "The term metric is not used appropriately. Indeed, distance, metric and displacement are different entities.\n\nA distance metric is defined precisely in mathematics for different spaces. According to the well-known definitions of metrics employed for Euclidean spaces and nonlinear manifolds, (3) and (4) are not metric, therefore, RMQM is not a metric.\n\nTo resolve this issue, please either define your distances and metrics more precisely clarifying the differences between them, prove metric properties of the proposed functions, or use some other terms for the proposed entities.\n\nAnother major problem is comparison of points on manifolds. According to the discussions given in the motivation sections (e.g. Fig. 1), I suppose that nonlinear manifolds instead of linear Euclidean spaces are considered as RMs in the paper. Then, it is not clear why points on nonlinear manifolds are compared using Euclidean distances. \n\nIn addition, experimental analyses should be improved according to the proposed claims. First, additional datasets, methods and models should be explored for different vision tasks in addition to the classification tasks, and also for other AI tasks such as NLP and speech recognition. Second, the paper claims some results for self-supervised learning methods. Then, this claim should be verified using state-of-the-art self-supervised learning methods as well. If the experimental analyses cannot be extended, then some of the claims should be mathematically explored.\n\nSome typo:\nincludeTenenbaum \u2192 include Tenenbaum\nZhou et al. (2021) also evaluates \u2192 Zhou et al. (2021) also evaluate",
            "summary_of_the_review": "The paper addresses an important problem of deep learning. However, there are various major and minor issues with the paper. Therefore, the paper is not ready for publication without fixing these issues.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper850/Reviewer_s4yj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper850/Reviewer_s4yj"
        ]
    },
    {
        "id": "W4E175X4Qu",
        "original": null,
        "number": 3,
        "cdate": 1666674426563,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674426563,
        "tmdate": 1666674426563,
        "tddate": null,
        "forum": "rjYUBo_uWEs",
        "replyto": "rjYUBo_uWEs",
        "invitation": "ICLR.cc/2023/Conference/Paper850/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a score for evaluating the representational space of a model to estimate the potential generalization performance. An experiment with this method across a few datasets provides evidence to support this claim.",
            "strength_and_weaknesses": "Strengths: The paper is well organized. The argument to predict generalization is clear. The proposed metric is explained clearly.\n\nWeaknesses: I would have preferred to see the evaluation performed on larger datasets and not just small toy datasets. The support for the claim in the paper is solely empirical so this should be expanded on more. I also believe there should be more comparisons to existing work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "In terms of novelty it seems strange to me that \"there has been no work done on using the structure of the RM as a predictor of generalisation\". Although, studying the perturbations may be novel. This work for one uses the compactness of the representation to predict generalization https://arxiv.org/abs/2012.02775 and I suggest the authors read the proceedings of this workshop to identify relevant methods to contrast theirs with https://sites.google.com/view/pgdl2020/resources Some of these methods can be used as baselines to help understand the significance of the perturbation approach.\n\nAlso, when the models were the models selected using early stopping? One experiment that could be interesting (not saying you need it for publication) is if you compute the RMQM at each epoch. One would expect to see a similar trend to the validation loss. An application of this would share the motivation of this work: https://arxiv.org/abs/1703.09580 which aims to not use a validation dataset.\n",
            "summary_of_the_review": "I really enjoyed reading this paper and I think the authors are doing some great work and have great ideas. However, I would prefer that the claims are supported with more complex datasets as well as contrasted with existing baselines. I believe the authors will find some from the publications in the PGDL2020 workshop that will demonstrate the potential improvement of their method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper850/Reviewer_jVym"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper850/Reviewer_jVym"
        ]
    }
]