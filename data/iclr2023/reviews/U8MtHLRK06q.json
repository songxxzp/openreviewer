[
    {
        "id": "L7HkVwHfBg3",
        "original": null,
        "number": 1,
        "cdate": 1666335850399,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666335850399,
        "tmdate": 1666335850399,
        "tddate": null,
        "forum": "U8MtHLRK06q",
        "replyto": "U8MtHLRK06q",
        "invitation": "ICLR.cc/2023/Conference/Paper1340/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces a 3D position embedding module to a state-of-the-art feature matcher, LoFTR.\n\nSpecifically, the authors develop a 3D position embedding generator that encodes 3D point clouds for each pixel instead of encoding 2D pixel coordinates.\n\nThe proposed 3D position embedding generator requires a known relative pose from GT or from a third-party relative pose estimator.\n\nExperiments on the Scannet and Kitti datasets show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "[Strength]\n\n1.  Using 3D position embedding to replace the original 2D pixel position embedding is interesting.\n2. Results on the KITTI Stereo 2015 validation results are good.\n3. Overall, this paper is easy to follow.\n\n[Weaknesses]\nThe main weaknesses of this paper are experiments.\n\n1. Since the proposed 3D position embedding module requires a known relative pose, the proposed method has limited applications. \n\na) Applying the proposed method to the KITTI Stereo 2015 dataset is good. However, only results on the validation set are given. Please report the results on the testing set;\nb) For the relative pose estimation task, though authors show that the proposed method can refine an estimated pose from Lofter, the performance gap between the proposed method and the original Lofter is big. To address this, I would expect:\n\ni) Exactly following the training configuration of the original Lofter, and see whether the proposed 3D position embedding module still works, i.e., can refine an estimated pose from Lofter;\n\nii) Add a baseline experiment using the original 2D pixel position embedding and see whether the proposed 3D position embedding module can obtain better performance under the authors' setting;\n\niii) Using more recent networks (e.g., QTA) to check the proposed 3D position embedding module still works;\n\niv) Using the YFCC100M and IMC2021 datasets to check whether the proposed method can work on outdoor datasets.\n\nc) For the ablation study, authors use the GT relative pose. It will make this section questionable;\n\nd) Please add an ablation study, showing the robustness of the proposed method with respect to prior relative poses at a different level of accuracy.\n\n2. [Minor] typos. For example, \n\na) Gnerator in Figure 1, \nb) Self Attention (Inter-Image) ----->>>> Self Attention (Intra-Image)\nc) Please bold matrix/vectors.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity : Good;\nQuality: Fair;\nNovelty: Fair;\nReproducibility: Fair;",
            "summary_of_the_review": "The main weakness of the proposed method is relying on a known relative pose, either from GT or from a third-party estimator.\n\nThe experiments fail to show that using a third-party estimator (Lofter) would make the proposed method outperforms the third-party estimator. \n\nPlease use the same training setting as the third-party estimator.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1340/Reviewer_kZ1E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1340/Reviewer_kZ1E"
        ]
    },
    {
        "id": "rhya8mfWb3",
        "original": null,
        "number": 2,
        "cdate": 1666575873543,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575873543,
        "tmdate": 1666776703595,
        "tddate": null,
        "forum": "U8MtHLRK06q",
        "replyto": "U8MtHLRK06q",
        "invitation": "ICLR.cc/2023/Conference/Paper1340/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This manuscript proposes a method, PA-LoFTE, for solving feature matching between images which generally follows a detection-description-matching three-stage pipeline. In order to achieve that, the authors utilize depth information from a depth predictor to generate 3D position embedding, then combine visual features, depth features, and 3D position embeddings with transformer.\nThe discussion on the necessity of 3D space information is limited, which is the major motivation of the proposed method. The main contribution is leveraging the 3D position embedding generator for feature generation. They perform an empirical study on indoor dataset and multiple tasks and conclude the superior performance of PA-LoFTR.\n\nOverall, this paper could be a borderline paper. Given the clarification in the rebuttal, I would increase the score.\n\nFor the motivation:\n1. It should be aware depth estimation the similar challenges to correspondence prediction, why depth estimation helps correspondence prediction?\n\nSome suggestions for the experiments:\n1. Compared with LoFTR, PA-LoFTR injects 3D position information into the original Coarse-Level Local Feature Transform. The proposed method (depth feature generator and 3D position embedding generator) is effective based on LoFTR, how about adopting other baselines?\n2. In Table 1, how about the performance of LoFTR?\n3. In Table 3, Without Depth Feature Generator performs better on the test dataset while having lower performance on the validation dataset, could you please discuss this problem? \n4. Could you please give some internal estimated depth images in the depth feature generator?\n\nMinor comments:\n1. Figure 1: Depth Feature Gnerator -> Depth Feature Generator.",
            "strength_and_weaknesses": "Please see summary",
            "clarity,_quality,_novelty_and_reproducibility": "Please see summary",
            "summary_of_the_review": "Overall, this paper could be a borderline paper. Given the clarification in the rebuttal, I would increase the score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1340/Reviewer_PA3N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1340/Reviewer_PA3N"
        ]
    },
    {
        "id": "-mJIXoWyS1t",
        "original": null,
        "number": 3,
        "cdate": 1666602200515,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602200515,
        "tmdate": 1666602200515,
        "tddate": null,
        "forum": "U8MtHLRK06q",
        "replyto": "U8MtHLRK06q",
        "invitation": "ICLR.cc/2023/Conference/Paper1340/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a descriptor-free feature matching method based on LoFTR. Given a pair of input images, it uses a transformer architecture to calculate pairwise similarity between two sets of coarse features, and then optimize the fine-grained positions with fine-level features. The main differentiating factor compared with previous work is a 3d positional embedding generator that makes the transformer encoder aware of 3d information. A depth prediction head is also jointly learnt to provide depth encoding to aid the feature encoding. The proposed method achieves a large margin over the previous SoTA methods.\n",
            "strength_and_weaknesses": "Strengths:\n- The paper is written well with many ablation studies and numerical findings. The results also show that the proposed 3d positional embedding helps.\n\n- In the ablation study, the author illustrates the performance with and without the depth predicted, and presents a few failed approaches to make it work. Though the result probably would work against the proposed method in the paper, I\u2019d appreciate this part which makes it clearer where it works better.\n\nWeakness:\n- The idea of 3d position embedding is not new. For example, this recent paper \u201cInput-level Inductive Biases for 3D Reconstruction\u201d described how to encode camera intrinsics and extrinsics into a transformer in positional encoding. As the paper mentions, PETR (Liu et al. 2022) also introduces a similar idea to the problem of object detection. Therefore, the application of a similar idea to the specific problem (using a transformer to do image matching) is not very novel.\n\n- It\u2019s a bit weird that the paper contributes a big chunk to depth predictors and in the end it doesn\u2019t generalize well to many scenarios (Sec 4.3). In addition, training the depth predictor as a joint head would also make the training harder, converge slower, and increase the overhead for the model to be deployed. It\u2019s realistic not to assume that the depth head trained on ScanNet would generalize well on KITTI. In my opinion, if you want to try to add depths, starting with a pretrained mono-depth model would be a better route. Since it doesn\u2019t add overhead in the training process, and possibly would generalize better since it\u2019s trained on a large corpus of data.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is written clearly with useful figures.\n- The ablation study is especially good and informational, which clearly shows the strengths and weaknesses of the proposed method.\n- It should be reproducible since it\u2019s based on an established approach with well tested data. The authors also mentioned the code will be published soon which should make reproducing the results easy.\n",
            "summary_of_the_review": "I recommend reject due to the following reasons:\n- The proposed method should intuitively improve the results as a better positional embedding, incorporating more inductive bias such as 3d information of the input features has been demonstrated to improve the results quite well. However, the novelty is a bit hurt because this point has been shown on other domain-specific tasks. In addition, this paper is heavily based on LoFTR, the idea of using a transformer to do descriptor-free image matching.\n\n- The emphasis on the depth predictor as a contribution is a bit problematic, as the results seem to show contradictory conclusions. I suggest using an off-the-shelf depth predictor to help with this argument.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1340/Reviewer_iea1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1340/Reviewer_iea1"
        ]
    }
]