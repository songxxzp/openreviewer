[
    {
        "id": "sskqg01zQDa",
        "original": null,
        "number": 1,
        "cdate": 1666764236228,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666764236228,
        "tmdate": 1666764236228,
        "tddate": null,
        "forum": "4xzk3zGtz1h",
        "replyto": "4xzk3zGtz1h",
        "invitation": "ICLR.cc/2023/Conference/Paper2085/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the hybrid federated learning setting, where both samples and features can be different over clients. This setting is a combination of the horizontal and vertical FL. The main contribution of the paper is the HyFEM algorithm, where clients learn a model locally based on the available samples and features, then a matching algorithm is used to combine the local models. In empirical studies, authors show the proposed algorithm outperforms the standalone training baseline, i.e. only train local models.",
            "strength_and_weaknesses": "Strength:\n\n1.\tThe paper is well written and easy to follow in general.\n\n2.\tHybrid federated learning is an important setting with some real world applications.\n\n3.\tThe proposed algorithm is accompanied by convergence analysis.\n\nWeakness:\n\n1.\tThe matching algorithm used in Algorithm 1 is a straightforward extension of the FedMA [1] algorithm. The analysis follows [1] in general, but a small difference of the paper with [1] is that clients could have different number of neurons, furthermore, only the fully connected network case is analyzed, can the classifier use other architectures, such as CNN, LSTM and Transformer?\n\n2.\tThe empirical evaluation is not sufficient. Firstly, only vision datasets are used to do evaluation, while other more realistic applications are missing such as the medical diagnosis, recommendation systems and social networks (These applications are also used as the motivation of the paper). Next, more baseline methods are desirable. In the experiment section, the major baseline is the stand-alone model, which is a very weak baseline. For example, another important baseline could be the vertical FL method, where the different sets of features over clients are explicitly aligned. One of my concerns to the proposed HyFEM algorithm is related to this: the HyFEM algorithm does not (directly) model the interactions between cross-clients features, there might be some indirect exploitation, but empirical evidence is desired.\n\n[1] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations (ICLR), 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is good, but the novelty is somewhat limited. I assume the experiments can be reproduced as the algorithm is introduced clearly.",
            "summary_of_the_review": "This paper studies the hybrid federated learning setting, where both samples and features can be different over clients. This setting is a combination of the horizontal and vertical FL. The main contribution of the paper is the HyFEM algorithm. In empirical studies, authors show the proposed algorithm outperforms the standalone model baseline. The paper is well written and easy to understand, but the novelty of the proposed algorithm is restricted, furthermore, the empirical evaluation is not sufficient.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2085/Reviewer_FpoP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2085/Reviewer_FpoP"
        ]
    },
    {
        "id": "HCluQ8YIiQC",
        "original": null,
        "number": 2,
        "cdate": 1667404546316,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667404546316,
        "tmdate": 1667404546316,
        "tddate": null,
        "forum": "4xzk3zGtz1h",
        "replyto": "4xzk3zGtz1h",
        "invitation": "ICLR.cc/2023/Conference/Paper2085/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel framework for learning with distributed data that can be heterogeneous on both features and samples. In this framework, each client\u2019s model is partitioned into a feature extractor part and a classifier part. The feature extractor is used to process the input data, and the classifier will make a prediction based on the extracted features. A server model is then built based on these client models to assimilate local classifiers and feature extractors through a carefully designed matching mechanism. A communication-efficient algorithm is then designed to jointly train these models. Experiments on multiple image classification data sets demonstrate that the proposed algorithm can improve upon baseline methods under hybrid FL setup. ",
            "strength_and_weaknesses": "Strength:\n- This paper considers a novel setting for federated learning and proposes a novel method to solve this new problem\n- Empirical results against some baseline methods demonstrate that the proposed method leads to substantial improvements\n\nWeakness:\n- Method part:\n  - It seems that the proposed method assumes different clients at least has some shared features, i.e., there is no set of features that are only presented in one client. That seems a bit restrictive from my perspective. The authors may explain how such assumption is satisfied in real-world data. \n  - I suppose the proposed method actually tries to make extracted feature similar with different raw features. Nevertheless, that may not always be the case, as different raw features may have different but complementary semantics that need to be used together to obtain accurate final predictions. This can be a limitation of the proposed method and I am afraid some explanations are needed for that. \n- Experiments part:\n  - Based on the concern above, the authors may also consider using some other data sets. An example could be text data, where different parts of text can naturally have very different meanings. \n  - I am a bit confused on the exact setup of \u201cstand-alone\u201d baseline in experiments. Since different clients have different set of features, standalone training will lead to completely different models. As such, which model is chosen to report its performance? Some clarifications may be needed for that. \n  - Also, there seems to be some unfair comparisons in experiments. I suppose models in \u201cstand-alone\u201d baseline can only utilize the feature set on a single client. That should surely lead to worse performance due to information lost from unseen features. As such, another baseline that seems overlooked is to use ensemble models trained from different clients, which can make use of all features hence avoid possible unfair comparisons. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: this paper is not very easy to read in my opinion. Several definitions and setups are a bit hard to understand. \n\nQuality: though I am not an expert in federated learning, I think this paper has a fair quality.\n\nNovelty: this paper considers a novel problem that has not been studied in previous works and is substantially different. The proposed method for this problem is also novel.\n\nReproducibility: some experimental details (e.g., hyper-parameters) seem missing in current version, and source code is also not provided. As such, the reproducibility can be of some concerns for this paper. ",
            "summary_of_the_review": "This paper considers a novel setup for federated learning and proposes a new method with better empirical results than simple baselines. From my perspective, the proposed method still needs more clarification and empirical results can be further improved. Thus I choose weak reject as my initial score. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2085/Reviewer_Ss8B"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2085/Reviewer_Ss8B"
        ]
    },
    {
        "id": "L9tAq77PkD",
        "original": null,
        "number": 3,
        "cdate": 1667482164904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667482164904,
        "tmdate": 1667482970243,
        "tddate": null,
        "forum": "4xzk3zGtz1h",
        "replyto": "4xzk3zGtz1h",
        "invitation": "ICLR.cc/2023/Conference/Paper2085/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers a hybrid federated learning scenario (both sample and feature levels). To handle heterogeneity in the feature level, the authors adopt an idea from VFL to split the model into blocks. Each model block corresponds to a feature block. To handle heterogeneity in the sample level, the authors use a model matching method (proposed in HFL methods). A novel algorithm called HyFEM is also proposed to solve the hybrid FL problem.",
            "strength_and_weaknesses": "Strength:\n- The authors consider a challenging hybrid scenario (heterogeneity in both sample and feature levels) and propose a novel algorithm to address this problem.\n- For the proposed algorithm, its convergence is established, and its performance is examined empirically.\n\nWeaknesses and Questions:\n- **(major)** Experiments: although the experimental results can verify the effectiveness of the proposed algorithm, the settings are not from real applications. If the experiments include some applications listed as motivation in Introduction, the results will be more convincing.\n- **(major)** Concerns on run-time complexity: \n    - In Line 7 of Algorithm 1, $P_m\\in \\mathbb{R}^{d_m\\times d_0}$, thus, $\\sum_m p_m P_m^\\top P_m \\in \\mathbb{R}^{d_0\\times d_0}$. (1) As $d_0 > d_m$, this matrix can be singular. (2) It is expensive to invert a $d_0\\times d_0$ matrix when $d_0$ is large.\n    - For each iteration, the server needs to solve the minimization problem (11). Is it possible to get the minimizer exactly?\n- Nine assumptions are required for establishing Theorem 1. Are these assumptions standard in establishing convergence for other FL algorithms?\n\nAdditional Questions:\n- \u201cVFL algorithms require all the clients to have the same set of samples; otherwise, they cannot compute the loss and its gradient.\u201d Why? Can you elaborate on it?\n- What\u2019s the total run-time complexity of Algorithm 1? \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The writing is clear.\n- the novelty is limited.  For the proposed approach, both model splitting and model matching are already proposed in recent FL algorithms. The former is for feature heterogeneity (FH), while the latter is for sample heterogeneity (SH). \n- The experimental setting is new. As the code and data are not provided, I cannot check the reproducibility.",
            "summary_of_the_review": "Overall,  the proposed algorithm is reasonable for the challenging \"SH + FH\" scenario.\n- However, the settings in the experiment are simple, and the experimental results are not sufficient to verify the effectiveness of the proposed algorithm in real applications (as listed in Introduction).  \n- As the algorithm is a bit complicated, another concern is its run-time complexity.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2085/Reviewer_AHQz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2085/Reviewer_AHQz"
        ]
    },
    {
        "id": "U9wCs2xvhR",
        "original": null,
        "number": 4,
        "cdate": 1667680106992,
        "mdate": 1667680106992,
        "ddate": null,
        "tcdate": 1667680106992,
        "tmdate": 1667680106992,
        "tddate": null,
        "forum": "4xzk3zGtz1h",
        "replyto": "4xzk3zGtz1h",
        "invitation": "ICLR.cc/2023/Conference/Paper2085/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "    This work designs a novel mathematical model that effectively allows the clients to aggregate distributed data with heterogeneous, and possibly overlapping features and samples.",
            "strength_and_weaknesses": "\nThe major concern of the reviewer is missing references and discussion with existing works on hybrid federated learning. \nhttps://openreview.net/forum?id=H0oaWl6THa\nhttps://dl.acm.org/doi/abs/10.1145/3467956?casa_token=fegJizVFAJsAAAAA:x52Xyyfu4dTfvf9SmIFyGB8wc8z868hUfrRj_o7pB9hu8ayIc2mrByOH5ib1Yf4uo8RewEc0KfyPAg\n\nAs mentioned by the author, the algorithm seems quite complex. But the complexity of the algorithm is briefly discussed but not evaluated in the experiments. It would be useful to provide experiments ",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is well-written and easy to follow.\n2. more clarification about the novelty and difference between the proposed framework and existing works is needed.\n3 performance of the algorithm is evaluated over real data experiments and the convergence of the algorithm is evaluated. The code is provided for reproducibility.",
            "summary_of_the_review": "Overall, the reviewer finds this work provides some interesting results, but the could be improved by a thorough discussion about existing work in the area, and more experimental results could be provided for the efficiency of the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2085/Reviewer_3J31"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2085/Reviewer_3J31"
        ]
    }
]