[
    {
        "id": "C_dufGoRzY",
        "original": null,
        "number": 1,
        "cdate": 1666454400257,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666454400257,
        "tmdate": 1671938582940,
        "tddate": null,
        "forum": "I_YZANaz5X",
        "replyto": "I_YZANaz5X",
        "invitation": "ICLR.cc/2023/Conference/Paper70/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "It is known that dual gripper is a challenging yet valuable robotic problem because there are daily tasks that are hard to complete with a single gripper. When robots are deployed for home-assistive applications, it becomes even more difficult because of the diverse 3D objects in the daily environment. The authors propose a collaborative visual affordance to facilitate dual-gripper manipulation to overcome the hurdle. Dual-gripper manipulation has the problem of a high degree of freedom. The authors propose a learning framework that disentangles the two gripper pose prediction subtasks and optimizes the two subtasks via a collaborative adaptation procedure. The authors conduct experiments on a proposed benchmark based on the PartNet-Mobility and ShareNet datasets. Four diverse dual-gripper manipulation tasks, i.e., pushing, rotating, toppling, and picking up, are studied. The proposed method achieves good performance compared with three baselines (random approach, heuristic approach, and a modification to Where2Act [Mo et al., 2021]). Additional experiments involve (1) showing the effectiveness of an RL-based data collection strategy and (2) real-world robot demonstrations.",
            "strength_and_weaknesses": "Strength:\n1. The authors tackle a challenging problem, i.e., dual-gripper manipulation. The work is timely toward home-assistive robots.\n2. . The reviewer believes the work can motivate the community of visual affordance learning and dual-gripper manipulation to stimulate new ideas toward home-assistive applications.\n3. The authors propose a collaborative visual affordance learning framework to address the challenges of dual-gripper manipulation due to high-degree freedoms. Specifically, they propose to disentangle the dual-gripper learning problem into two separate yet coupled subtasks, i.e., the second gripper module is conditioned on the output of the first module. With the proposed design, it can reduce the search space. In addition, they propose a collaborative adaptation procedure to enhance cooperation. The proposed collaborative adaptation procedure is shown to be effective in Table 1, where the success rate on both the training and testing category is higher than the one architecture without a collaborative adaptation procedure.\n4. The authors demonstrate the feasibility of the proposed method on real robots that demonstrate dual-gripper manipulation on the following tasks, i.e., picking up containers, rotating boxes, pushing displays, and toppling buckets.\n\nWeaknesses:\n1. An essential piece of this work is the concept of collaborative visual affordance. However, the authors did not propose any metrics and benchmarks for this part. The reviewer found it critical as this is the \"direction\" the authors aim to promote. Otherwise, the authors should compare other bimanual manipulation approaches (e.g., Weng et al., 2021 and Xie et al., 2020)\n2. If visual affordance learning \"direction\" is the aim of this work, what are the limitations of other visual actionable approaches for the tasks tackled in this work? For instance, Weng et al. propose a flow-based method for bimanual cloth manipulation (Weng et al., FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy, CoRL 2021). The work shows the feasibility of bimanual cloth manipulation. While the task domain of Weng et al., 2021 is different from this work, the idea of disentangling has been discussed in Figure 3. In addition, the pick point prediction is another strategy for manipulation. What is the advantage of the proposed approach? The authors should discuss the work and the difference.\n3. One of the contributions is the benchmark based on SAPIEN. However, the authors did not discuss the \"value\" of the proposed benchmark over others (e.g., Xie et al., 2020, Chitnis et al., 2020, Chen et al., 2022). Please comment.\n4. Lack of detailed ablative studies: This work has several components for collaborative visual affordance learning. Specifically,\n    1. Disentangled gripper module design with conditioning. What is the performance without conditioning? Does the model fail to converge?\n    2. Collaborative adaptation procedure\n    3. RL data sampling vs. Random sampling\n5. Currently, only (b) is evaluated and shown its effectiveness. The other two aspects do not prove their values. The question regards (c) can be found in the next point. Please comment.\n6. Table 2 results are confusing: Specifically, based on the reviewer's understanding, the results aim to show that the success rate of the RL-based sampling method is more effective than a random method. However, the notion in Table 2 is \"Ours w/o RL\" and \"Ours,\" which is confusing. Moreover, the results are for the picking-up task. How about other tasks? Please comment.\n7. Diversity of data collected using RL policy: while the authors show the success rate of RL-based sampling in Table 2, the reviewer is interested in the diversity of the RL-based sampling. What is the coverage of the sampled points based on the RL-based method? Could we visualize them?\n8. The reviewer suggests that the authors can conduct experiments in the real world where a policy is trained on training categories but test on the testing categories to further strengthen the proposed method's value.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The quality writing is great and easy to understand.\n2. The authors provide detailed implementation details in the supplementary material.\n3. The reviewer has several questions (mentioned in the Weaknesses section) regarding the novelty.  ",
            "summary_of_the_review": "The authors propose a collaborative visual affordance learning framework for dual-gripper manipulation. In addition, a benchmark is proposed to validate the effectiveness of different algorithms for dual-gripper manipulation. In addition, a real-world demonstration is presented to prove the effectiveness of the proposed approach. However, several concerns are raised in the Weakness section, including the novelty of the proposed framework and benchmark and insufficient ablative studies.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper70/Reviewer_VYuj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper70/Reviewer_VYuj"
        ]
    },
    {
        "id": "qbymLCrivIN",
        "original": null,
        "number": 2,
        "cdate": 1666465946436,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666465946436,
        "tmdate": 1666465946436,
        "tddate": null,
        "forum": "I_YZANaz5X",
        "replyto": "I_YZANaz5X",
        "invitation": "ICLR.cc/2023/Conference/Paper70/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approach to learn a bimanual perceptual affordance model, provides a simulated benchmark to evaluate the model and compares its performance against a number of sensible baselines.",
            "strength_and_weaknesses": "Strengths:\n- very important topic, extremely relevant to representation learning for robotics. While it's been relatively easy to learn single-arm grasping directly from manipulation data, without having to learn an explicit affordance representation, doing so in the context of leads to an explosion in complexity which makes learning an independent proposal model for affordances very attractive.\n- the model appears sound, well-motivated, and immediately useful to derive further research on the topic.\n- the approach to combating the combinatorial explosion of having to learn two joint affordance models together is well-motivated, clever, and of practical relevance to any setting where chained conditional models have to be learned efficiently.\n\nWeaknesses:\n- the main weakness of the approach, in that it doesn't take the robot kinematics into account, is well documented and not a showstopper for future work.\n- the benchmark developed as part of this work would be exceedingly valuable to the community, particularly since the success rate on this task seems to hover around ~50%, which indicates it's in the sweet spot of not being too difficult or too easy. I saw no discussion of providing it as an open-source benchmark, which would greatly enhance the value of this submission.",
            "clarity,_quality,_novelty_and_reproducibility": "- Very well organized and thorough paper: experiments are detailed, and include real-world evaluation, which is a very significant bar for robotics papers seldom met in practice, but essential to deciding on the value of any approach that pertains to real-world robotics.\n- Good limitations section.\n- Great video\n",
            "summary_of_the_review": "Very nice paper overall, useful to the community with a few ideas that could apply beyond the narrow setting of affordance modeling.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper70/Reviewer_yumu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper70/Reviewer_yumu"
        ]
    },
    {
        "id": "BrG7W_Do9w",
        "original": null,
        "number": 3,
        "cdate": 1667308589716,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667308589716,
        "tmdate": 1667308589716,
        "tddate": null,
        "forum": "I_YZANaz5X",
        "replyto": "I_YZANaz5X",
        "invitation": "ICLR.cc/2023/Conference/Paper70/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends the visual actionable affordance learning method of Where2Act (Mo et. al. 2021) for dual-gripper manipulation. To deal with the quadratic complexity of dual action space, the authors propose to sequence action predictions from two grippers and conditioning the second on the first. This reduces the action space to linear complexity and aids in ensuring collaborative policies of the two grippers. To train the network components of their architecture, the authors generate in offline fashion dense affordance and gripper orientation maps from a simulation environment by sampling random policies, as well as with a soft actor-critic RL method. The paper also proposes a collaborative adaptation procedure to enhance collaboration between the predicted actions by training the network components end-to-end. Experiment evaluation showcases that the proposed method outperforms task-specific heuristics and a quadratic complexity extension of Where2Act, while being able to generalize in unseen object geometries.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is well-written, motivates its purposes, and presents its ideas in a clear fashion.\n2. The proposed method addresses the complexity bottleneck of dual-arm manipulation by sequencing predictions from two grippers and conditioning the latter on the first.\n3. The paper also proposes (and experimentally evaluates) an RL-based method for data generation.\n4. Evaluation includes a study of generalization properties (novel shapes to be manipulated), which greatly improves its experimental\n5. The proposed method is integrated with a real robot and includes demonstrations in real environments.\n6. The authors generate evaluation testbeds in a simulation environment and claim that will release the dataset, which is a great contribution given the sparsity of resources for dual-arm manipulation.\nWeaknesses\n1. I would like more discussion on Sec. 4.4 (Collaborative Adaption Procedure), as it was also shown experimentally that it is essential for achieving collaboration between the grippers. However, the process is not 100% clear to me. It seems that the authors pair the learning networks with the robot to perform actions end-to-end in simulation, but how/why does that contribute more than training the components separately in an offline fashion, given that ground truth offline data are generated with annotations for the desired collaboration?\n2. Evaluation baselines include a random policy, heuristics, and a dual-gripper-enhanced version of Where2Act (Mo et. al. 2021). I think the paper would benefit by comparing their method also with non-visual affordance-based methods for dual-arm manipulation. For example, how does this method compare with the mentioned related work keeping one gripper fixed method (Gadre et. al. 2021)?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity. The paper is overall well written The messages are clear and follow a logical progression throughout the text.\n\nNovelty: The authors propose an original work on extending visual actionable affordance detection for dual-gripper collaborative manipulation. This work innovates in vision-based dual-arm collaborative manipulation which has immediate applications in service robotics.\n\nTechnical Quality: Technical details are presented and seem generally sound, there are a few parts that would require additional analysis.\nClarity: The paper is overall well written The messages are clear and follow a logical progression throughout the text.\n\nReproducibility The authors provide details about hyper-parameter and architecture configurations, as well as a detailed setup of evaluation metrics for the concerned tasks.",
            "summary_of_the_review": "The paper introduces a novel method for dual-gripper manipulation based on visual affordances and it also publishes an evaluation suite for four collaborative tasks. I am fairly confident that the contributions are novel, impactful, and well-supported by the experimental section. However, I believe that the paper would benefit from some extra analysis and discussions on some aspects (see issues below):\n\n1. I would suggest adding more discussion in Sec. 4.4 (see also weaknesses). The authors mention: \u201d [...] In this way, the two gripper modules can better understand whether their proposed actions are successful or not as they are aware of interaction results, and thus the two separately trained modules are integrated into one collaborative system\u201d. How is this \u201dbetter understanding\u201d achieved? How does training together add to a better understanding of collaboration, given that separate gripper-wise annotations are also collaboration-aware?\n\n2. Some type of experimental confirmation of utilizing a dual-arm system vs a single gripper is missing. The authors motivate this well through the nature of the concerned task, but I think a single gripper could also achieve a subset of such tasks with extra effort (more actions). The benefits of then using a dual-gripper system are obvious, however, it would be nice if a comparison is included in the experimental section.\n\n3. Comparison with other recent dual-gripper manipulation systems (e.g. Gadre et. al. 2021).\n\n4. I would like to see a comparative analysis of the proposed vs. quadratic complexity version of Where2Act by training data size. How fast does your method pick up on the collaborative aspect compared to a holistic version?\n\n5. There are a few typos, e.g. in the Conclusion section \u201d[...] its superiority of the three baselines\u201d should be \u201dover\u201d etc. I believe an extra iteration by the authors would resolve these issues.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper70/Reviewer_RF6q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper70/Reviewer_RF6q"
        ]
    }
]