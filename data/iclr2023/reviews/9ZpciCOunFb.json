[
    {
        "id": "hhV7ano6M2",
        "original": null,
        "number": 1,
        "cdate": 1666216312321,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666216312321,
        "tmdate": 1669153721526,
        "tddate": null,
        "forum": "9ZpciCOunFb",
        "replyto": "9ZpciCOunFb",
        "invitation": "ICLR.cc/2023/Conference/Paper4792/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides an analysis of symmetries inherent to the architecture of certain models (overall the study is centred in neural-networks-like predictors), as well as the loss and data. Link to the set of local minimizers is also made. Finally, it is also shown that these symmetries have a corresponding echo in conserved quantities during gradient flow training.",
            "strength_and_weaknesses": "## **Strength**\n\nThe first real strength of the paper is that, despite the technicality of the theory of invariance, the paper is pretty clear and easy to follow (despite some paragraphs and typos -- see *Weaknesses*). Overall, at least at the beginning of the paper, there is a real care about pedagogy, and, being personally not familiar with this theory, I felt that the paper is really understandable from this perspective.\n\nFurthermore, beyond this point of view, two results appeared particularly interesting, at least to me:\n- They manage to recover, with their general framework, standard invariant already derived in precedent works that were simply seen as \"tricks\"\n- The fact that the authors try to tackle data-dependant symmetries\n\n---------------------------------------------\n\n## **Weaknesses**\n\nHere is a list of potential weaknesses that I personally felt when reading the paper: \n\n- First, even if the authors try to tackle the case of potential data-dependant non-linear symmetries, they do not find any application of this perpective for neural networks. In fact, the only conclusive result they have on these is the known balance property, but this *only* comes from a function symmetry and is not data-related. Do the authors have in mind an example where some data invariance is of paramount importance ? (I have a more detailed question about this in the next box).\n- In fact, going further, the different paragraphs of Subsection 1.5 are really fuzzy and there is a lack of details in the calculations. Propositions and facts are inline without emphasis between them so that it is very diffiicult to follow. Furthermore, it appeared to me (maybe I am wrong), that some of the derivations are very fast and this prevents for a proper easy reading of this *very technical* section. Examples of these are e.g.: (i) second line of the **General equivariance** paragraph $c(g_1, g_2 Z) c(g_1, g_2 Z) = c(g_1g_2, Z)$, what is a typical $c$? What does it mean \"with physical interpretation\"? why is this property called \"equivariance\"? This paragraph lacks of lot of details (both in the interpretation and in calculation) to be followed. (ii) Second line of **Using nonlinear transformed [...]** paragraph: do really the authors want to write: $g \\cdot (U, V, X) = (\\tilde{U} (g, VX), gV, X)$, and in this case, what is $\\tilde{U}$? In this paragraph, it is concluded that at permutation + small deformation of the data set the function should be almost the same: why should we care about this perturbative results? I have in mind that the assumption on the data for generalisation is more that they come iid from the same distribution not that they are close to each other in distance? (iii) The last paragraph of this section on **adapting models** is very hard to follow, notations are not properly introduced, e.g. in $Z' = gZP$, what is $g$? Why should we look for $U'\\sigma(VX') = U\\sigma(VX)P$? I am sure there are some calculations that permit to go from one line to the others and things are clear in the minds of the authors, but I felt in was impossible to follow this paragraph. Maybe a real mathematical Proposition (proved in Appendix) can clarify everything here: e.g. *Assume X' is such that $\\exists g \\in G$ such that for all $V$, [...]*.\n- As far as I understood, the authors are not the first to consider symmetries to explain invariants of the dynamics. Comparison with results of previous works would be welcome to understand how it articulates with theses previous works. I am sure that all the propositions of Section 1 are well-known facts for example. \n- Minor issues: Eq (6), the gradient is seen as a linear form in this article it seems because operations with matrices are left operations. It can be troubling so it needs to be said. Typo page 6, paragraph 3: $\\sigma(a) - \\sigma(b)$. Paragraph **need for data-dependant symmetries**, isnt this $\\mathcal{L}(\\theta', X') = \\mathcal{L}(\\theta, X') $ (and not $\\mathcal{L}(\\theta, X)$). After proposition 1.6, $F(x)$ should be $U\\sigma (Vx)$. After formula (10), the infinitesimal form of $\\sigma(gz)$ ... Like previously the formula (11) is weird as $\\langle Mz, d\\sigma_z \\rangle$ is a vector and the $\\langle \\cdot, \\cdot \\rangle$ usually indicate a scalar product. As far as I understand, the authors may come from a theoretical physics community where such notations may be usual.",
            "clarity,_quality,_novelty_and_reproducibility": "I already tackled these questions earlier. Let me ask a question here.\n\nIn 2-layer diagonal linear networks: that is 2-layers linear neural networks with only diagonal connexion between matrices, it is known that the gradient flow has a nice non-trivial invariance property: it can be recasted as a mirror flow, where the mirror map $\\nabla \\psi: \\mathbb{R}^d \\to \\mathbb{R}^d$ is some explicit function. Say the coordinate-wise logarithm (it is almost true that it is the case). Then, it can be show that $ \\forall t>0, \\ \\nabla \\psi(\\theta_t) \\in \\textrm{span} (X^\\top), $ where $\\theta_t$ is as in the article the dynamics of the gradient flow at time $t$ and $X$ is the design matrix (See for a reference http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf). Hence,  $\\nabla \\psi(\\theta_t)$ is not a *conserved*  quantity strictly speaking but a nice *geometric invariant of the dynamics*. Is this possible to derive the point of view on group invariance to unveil this without resorting to what is now to me more like a \"miracle\" ? ",
            "summary_of_the_review": "Overall, even if the paper could be very nice I think that I feel disappointed when it comes to summarize the results. Beyond learning a nice theory of Lie invariant, I do not know what I have really learnt after reading this. The novelty and promising application to data-dependant symmetry, in particular, is a bit disappointing. \n\nI will be very glad to largely increase my score if the authors answer my concerns and questions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4792/Reviewer_M5tH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4792/Reviewer_M5tH"
        ]
    },
    {
        "id": "4LsKE44ILn",
        "original": null,
        "number": 2,
        "cdate": 1666588124649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588124649,
        "tmdate": 1666588124649,
        "tddate": null,
        "forum": "9ZpciCOunFb",
        "replyto": "9ZpciCOunFb",
        "invitation": "ICLR.cc/2023/Conference/Paper4792/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the parameter space symmetries induced by equivariances in the activation function as well as data dependent nonlinear symmetries.  Furthermore they formalize how these symmetries lead to conserved quantities during gradient flow.",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper introduces a general framework to relate symmetries of parameter space to conserved quantities of gradient flow, which is something I was hoping to see.  They make extensive effort in the appendix to catalogue applications of their theory to common activation functions and give an extended tutorial of their theory.\n\n**Weaknesses**\n\nI think the data dependent symmetries are the most interesting component, and the work could be enhanced by more experiments or discussion that give insight into what these data dependent symmetries look like in some settings.  ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe work is clear and well written, although there are some typos and broken links.\n\np.5 \u201cLeakyReLU This is a special case of homoegeneous activation\u201d, misspelling of homogeneous.\n\nAppendix: \np. 37 broken link in last paragraph.  Equation overfull into margin\n\np. 38  Broken link at the end of Proposition I.2 and in the statement of Proposition I.2\n\nP. 39 in Appendix, broken link in statement of Lemma I.5\n\n**Quality**\n\nI believe the work is of high quality as it offers a useful formalism and an extensive tutorial in the appendix.\n\n**Reproducibility**\n\nSince this work is primarily theoretical and provides proofs of its theorems I don't think reproducibility is applicable here.\n\n**Questions/Comments**\n\np.5 \u201cSuppose the activation $\\sigma: \\mathbb{R}^h \\rightarrow \\mathbb{R}^h$ is homogeneous, so that $\\sigma$ is applied pointwise in the standard basis. Assume also that there exists $\\alpha > 0$ such that $\\sigma(c z) = c^\\alpha \\sigma(z)$ for all $c \\in \\mathbb{R}_{>0}$ and $z \\in \\mathbb{R}^h$.\u201d  I\u2019m confused.  Isn\u2019t the second condition simply stating the activation is $\\alpha$ positive homogeneous.  So what additional condition are you trying to specify with the first sentence?\n\n\n\n",
            "summary_of_the_review": "**Summary**\n\nThis paper offers a useful formalization of how equivariances lead to parameter space symmetries which in turn lead to conserved quantities of gradient flow.  They offer an extensive tutorial in the appendix.  I believe the theoretical community will appreciate this work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4792/Reviewer_FC6G"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4792/Reviewer_FC6G"
        ]
    },
    {
        "id": "MqmfDYEuWt2",
        "original": null,
        "number": 3,
        "cdate": 1667535324909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667535324909,
        "tmdate": 1667571421401,
        "tddate": null,
        "forum": "9ZpciCOunFb",
        "replyto": "9ZpciCOunFb",
        "invitation": "ICLR.cc/2023/Conference/Paper4792/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper seeks to better understand theoretically symmetries in the loss landscape of neural networks. It first uses group theory to formulate a more general notion of parameter space symmetries, beyond permutations. The authors then motivate and present a class of nonlinear, data-dependent symmetries and a procedure for deriving conserved qualities associated with the symmetries. They show that these conserved qualities can be used to parametrize symmetric flat directions and show numerical results on two layer neural networks.",
            "strength_and_weaknesses": "### Strengths\n- The characterization of symmetries is a novel perspective and sheds some light on a deep learning phenomena of interest (e.g. ensembles formed along flat minima have been shown to generalize better). The paper describes detailed analysis wider class of data-dependent symmetries and how they relate to conserved qualities in gradient flow. \n- The empirical results are interesting and support the theory. The conserved quality Q affects the convergence rate and distribution of the eigenvalues of the Hessian. The group action allows for an ensemble without retraining, which has potential in improving robustness to adversarial attacks.\n- Related work is discussed well throughout the paper.\n\n### Weaknesses\n- It would support the theory to run experiments on a non-toy dataset. Currently, they are done with small randomly generated matrices. If the same conclusions can hold for MNIST/Cifar, these results would be more relevant for practitioners. I believe experiments relating symmetries to OOD data would also strengthen the discussion in section 1.5.\n- The paper is long (56 pages with appendix) and has many different examples/derivations. I'm not sure a conference format such as ICLR is the correct venue for such a submission.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As the authors describe, there are many works characterizing the loss landscape of neural networks but relatively little on the origin of low-loss valleys. The need for data-dependent symmetries is well-motivated and the authors build upon previous work relating conserved qualities to symmetries.\n\nThe writing is clear and quality of the analysis appear sound.\n\nMinor:\n- Figure 1 caption typo \"minum\"\n- Figure 2 does not appear to be referenced in the text.\n",
            "summary_of_the_review": "The paper offers an interesting perspective on understanding neural network loss landscapes through ideas from group theory and physics. It would be strengthened if the empirical results could be reproduced beyond a toy dataset.\n\ncontext for review: I'm familiar with/written empirical deep learning papers but less capable of assessing the significance of the theoretical contributions in this work. I skimmed the appendix/some related works in writing this review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4792/Reviewer_z99y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4792/Reviewer_z99y"
        ]
    },
    {
        "id": "kN6Q-PtxFx5",
        "original": null,
        "number": 4,
        "cdate": 1667576458860,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667576458860,
        "tmdate": 1669806979396,
        "tddate": null,
        "forum": "9ZpciCOunFb",
        "replyto": "9ZpciCOunFb",
        "invitation": "ICLR.cc/2023/Conference/Paper4792/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the role of symmetries in multi-layer perceptrons,\nputting forward the idea of data-dependent symmetries.\nConservation laws stemming from Noether's theorem are discussed.\n",
            "strength_and_weaknesses": "\nI had a very hard time reading the paper. I find the organization\nof the paper quite confusing. A lot of mathematical formalism\nis squeezed in every page while giving away very little in terms of \nexplanations and intuions. Much of the paper revists classical concepts from group theory and their physical implications through Noether's theorem,\nwith  novel contributions seemingly contained in Section 1.5\nand possibly Section 2. It is hard to asses the relevance of such contributions.\n\nThe equivariance property they propose in eq. 12 and 13 seems useful\nto produce a new set of parameters with approximately the same loss as the original ones\nwhen observing a new batch of data that is close (up to a permutation) to the original data. In practice though I'm not sure how useful this is, and how \"close\" the data should be, since it has not been experimentally tested in the manuscript.\n\nThe analysis in Section 2 of the conserved quantities along the dynamics is also interesting, but it's not clear how much of it is original and how much is relevant. The experiments are not properly explained in the main text.\n\nIt could be the case that I didn't understand the main points of the paper, \nwhich in any case calls for a much better rewrite since I can easily read through\nsimilar papers such as e.g. \"Noether\u2019s Learning Dynamics\" arxiv:2105.02716.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Due to a lack of clarity, it is hard to assess the quality and the novelty of the work.\n",
            "summary_of_the_review": "The paper should be mostly rewritten and provide a more clear focus on the results and their implications.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4792/Reviewer_J3kj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4792/Reviewer_J3kj"
        ]
    }
]