[
    {
        "id": "yrTP9QuTaQl",
        "original": null,
        "number": 1,
        "cdate": 1666357384356,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666357384356,
        "tmdate": 1670563009973,
        "tddate": null,
        "forum": "g9VAye0eIKO",
        "replyto": "g9VAye0eIKO",
        "invitation": "ICLR.cc/2023/Conference/Paper2299/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the regret minimization problem of finite horizon Latent MDP with context in hindsight, which assumes the true context of the MDP will be revealed at the end of each episode. It proposes a sample efficient algorithm and proves the polynomial regret bound for the algorithm. The algorithm utilizes the optimistic model-based and value-based planning principle with standard high probability confidence set construction. Based on the horizon-free optimistic planning techniques, the regret of this algorithm is $\\tilde{O}(\\sqrt{MS\\Gamma AK})$, which scales with only $\\log H$. It also provides a corresponding lower bound of regret $\\Omega(\\sqrt{MSAK})$ with a novel symmetrization technique.\n",
            "strength_and_weaknesses": "There are two key contributions of the paper: an exploration algorithm with provable improved regret bound for LMDP when the context is revealed at the end of each episode, and a novel hard instance construction and the corresponding regret lower bound. The exploration algorithm is mainly built on the model-based and model-free horizon-free techniques proposed by previous works. The construction of the hard instance and proof of the lower bound are new, whose main idea is to split the regret into $M$ parts and show that each part has to be at least $\\Omega(\\sqrt{SAK/M})$. \n\nOn the other hand, I have some concerns regarding to the paper:\n- When the true context is revealed at the end of each episode, one can construct the empirical estimate of the transition and reward using MLE simply, which is in sharp contrast to the partially observable setting. This perhaps leads to a very similar confidence set (both for the model and the optimal value function) as used in previous works studying horizon-free exploration. The extra $\\sqrt{\\Gamma}$ term in the regret seemingly comes from the union bound over any $\\alpha$-vector for the optimism to hold, which is similar to the idea used in UCRL2. It appears to me that this part of the paper is mostly a direct application of existing techniques.\n- I agree that the lower bound construction is novel and the proof of the lower bound requires a delicated manipulation over existing results. The question is the paper claims that it uses the symmetrization technique from TCS, but I cannot find the explanation of this technique in the proof. On the other hand, the hard instance construction seems to be somehow straight forward, by reducing to $M$ independent MDPs since the true MDP can be fully identified in the first phase.  \n- The most interesting story to me for the problem is the minimax regret bound, since I believe the extra $\\sqrt{\\Gamma}$ term comes from the union bound over $\\alpha$-vectors, but we do not need a union bound overall. Could you provide any intuitions or ideas for removing this term? Or you believe the lower bound can be improved.\n\n----------Post rebuttal-----------\n\nThanks for explantionations. However, I still see this work unqualified for a conference paper. The setting is very limited, and many techniques are adapted from previous works. I recommend the authors to study further on the horizon-free setting in LMDPs, for example, the hidden context setting where one has to construct an estimated belief to obtain valid samples.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is very good in general, with clear expressions for lemmas and theorems. I have listed several minor problems below. The novelty seems to be a problem, see the discussion for the weakness.\n 1. Page 2 (main contributions and technical novelties): is helps -- helps\n 2. Page 3 (related work): hindsign -- hindsight\n 3. What is $\\pi^k$ in Line 5 of Algorithm 4?\n",
            "summary_of_the_review": "Overall, I think this paper is not good enough. The major drawback is the lack of novelty, since the paper mainly builds its theory based on direct applications of existing results except for the hard instance construction. However, I really appreciate the minimax regret bound for this problem.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2299/Reviewer_6Gt3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2299/Reviewer_6Gt3"
        ]
    },
    {
        "id": "jIhJXwNCU2Z",
        "original": null,
        "number": 2,
        "cdate": 1666630102723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630102723,
        "tmdate": 1666630102723,
        "tddate": null,
        "forum": "g9VAye0eIKO",
        "replyto": "g9VAye0eIKO",
        "invitation": "ICLR.cc/2023/Conference/Paper2299/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies episodic learning algorithms for a variant of POMDP, called LMDP. In such a setting, there are M different MDPs and a new MDP (say m) is drawn at the beginning of each episode. The learner only observes m at the end of the episode and uses to learn. The authors derive both an upper bound and a lower bound for their problem, that are very close.\n",
            "strength_and_weaknesses": "This paper is very nicely done but has (too me) limited impact:\n- it is precise, well-done, and contains an upper bound that (almost) matches the lower bound.\n- it contains an algorithm that has no practical application, and that cannot be implemented. It is not very original and (to me) oversold.\n\nTo give more details: \n- even if I did not check the proofs, I do think that the results hold because the techniques are very similar to many classical in the theoretical reinforcement learning literature. They are so close that I cannot really see what is special about latent MDPs in this settign: the confidence bounds are the same as for MDPs (because the latent variable m can be observed at the end of the episode), only the lower bound is different, but is a mere adaptation of the model of Domingues et al (2021).\n- The proposed algorithm is impractical for several reasons, and in particular:\n  - the number of possible values for \"h\" is exponential in H.\n  - the authors claim that one could restrict oneself to a subclass of policies, for instance Markovian policies but:\n    - it is not clear if an optimal policy is easy to compute for LMDP.\n    - the authors sells \"extended value iteration\" but this would not work for many Markovian policies.\n- Some claims are too strong for me:\n  - The authors claim that this \"symmetric technique\" is very new -> having variant of the same problem seems very standards and not very close to the symmetric technique in TCS.\n  - the claim that \"the lower bound matches for \\Gamma=2\" is clearly true but not very informative. It does not mean anything for larger \\Gamma.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear, and well written. The results are new and I believe correct. The methodology is very classical.\n\nThere is no simulation to reproduce.",
            "summary_of_the_review": "This paper is a nice treatment of a purely theoretical problem. I do not find it very original nor applicable.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2299/Reviewer_vZUE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2299/Reviewer_vZUE"
        ]
    },
    {
        "id": "vPxwRSxkouW",
        "original": null,
        "number": 3,
        "cdate": 1666963046940,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666963046940,
        "tmdate": 1667080340782,
        "tddate": null,
        "forum": "g9VAye0eIKO",
        "replyto": "g9VAye0eIKO",
        "invitation": "ICLR.cc/2023/Conference/Paper2299/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies regret minimization for reinforcement learning (RL) in Latent Markov Decision Processes (LMDPs) with context in hindsight. Episodic undiscounted case is studied, and regret guarantees are found. The regret bound only scales logarithmically with the planning horizon. Further, the state term dependence is reduced. ",
            "strength_and_weaknesses": "The results seem to improve the state of the art, both for the improvement in terms of H and S. \n\nThe paper assumes stationary MDP which is independent of epoch in the episode. The key aspect of episodic setup is that the transitions and the reward function could depend on h. How do the results extend in this case? I believe that an extra H dependence will come in the result. Extending to this setup can have additional nice results for the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "The contributions seem well explained. ",
            "summary_of_the_review": "This paper studies regret minimization for reinforcement learning (RL) in Latent Markov Decision Processes (LMDPs) with context in hindsight. Episodic undiscounted case is studied, and regret guarantees are found. The regret bound only scales logarithmically with the planning horizon. Further, the state term dependence is reduced. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2299/Reviewer_jzYm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2299/Reviewer_jzYm"
        ]
    }
]