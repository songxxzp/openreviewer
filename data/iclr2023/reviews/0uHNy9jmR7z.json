[
    {
        "id": "KjPayidV98n",
        "original": null,
        "number": 1,
        "cdate": 1666539720210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666539720210,
        "tmdate": 1666539720210,
        "tddate": null,
        "forum": "0uHNy9jmR7z",
        "replyto": "0uHNy9jmR7z",
        "invitation": "ICLR.cc/2023/Conference/Paper2956/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to address the shortcomings in image captioning models by generating richer captions with additional object attribute information. They pretrain vision language based transformer models using an additional masked attribute loss and match the object information to the corresponding attributes using Hungarian matching algorithm. They further finetune the transformer to generate informative captions conditioned on previous words, object labels , attributes and region based visual features.",
            "strength_and_weaknesses": "Strengths - The motivation of the paper to generate more descriptive and well grounded captions for an image is well inspired from the limitations of existing methods. They  integrate an additional loss based on object attributes to the pre-training of Vision language transformers. \n\nWeaknesses - #\n\nIn the pretraining stage in Fig 4, a very useful ablation / simpler modeling strategy would be to combine the object tags + attributes and then mask that information randomly for token loss prediction.\n\nDue to the absence of masked region modeling, the model might fail to draw associations between attributes / tags and regions and hence fail to generate novel combinations of attributes + tags (black hat - > black dog) during caption generation.\n\nA study of attention maps from the transformers could help to see what muli-modal attention is being learned by the model. \n\nThe results are far below the baselines and are not justified well by using human evaluation, I recommend the authors to compare using some human evaluation. \n\nAnother metric - SPICE - U [1] could be more useful in measuring such descriptive captions. \n\nThe contributions are very limited compared to VIVO (baseline) and the results worse. \n\n[1]Wang, Zeyu, et al. \"Towards unique and informative captioning of images.\" European Conference on Computer Vision. Springer, Cham, 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is original and well written. The figures are small and not very clear and should be improved. ",
            "summary_of_the_review": "The contributions and experimental outcomes of the paper are marginal and not well supported by ablations, human studies or architectural changes. I propose major improvements both in the method and evaluation for a good submission. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2956/Reviewer_4nF4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2956/Reviewer_4nF4"
        ]
    },
    {
        "id": "ndrcQqQ7Vz_",
        "original": null,
        "number": 2,
        "cdate": 1666583194858,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583194858,
        "tmdate": 1666583194858,
        "tddate": null,
        "forum": "0uHNy9jmR7z",
        "replyto": "0uHNy9jmR7z",
        "invitation": "ICLR.cc/2023/Conference/Paper2956/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an image captioning model, termed CWATR, where object labels, attributes, and visual features are combined together using transformers and masked pre-training methods. Basically, the main contribution of this work is that it shows that introducing rich information like visual features, object labels and attributes into image captioning models can improve the quality of the generated captions, such as detailed descriptions.",
            "strength_and_weaknesses": "Strength:\n\nS1. The quality of the generated captions is relatively high.\n\nS2: Using Hungarian Assignment Algorithm in the pre-training stage is interesting to me.\n\nWeaknesses:\n\nW1: The model is not novel at all. In many existing works, people have employed transformers to combine visual features, tags and attributes to enhance vision-language representation, such as Oscar, VIVO, VinVL. And the experimental results are not convincing. The author only compares the proposed model with VIVO. Plus, in the section on visual analysis, the author only describes the differences among the generated captions, but deeper explanations are required. Though the captions generated by the proposed model are more detailed with more adjective words, I think the main reason for this is that the proposed approach employs attributes, while VIVO does not.\n\nW2: It seems that the generated captions are more distinctive, but the author only considers CIDEr and SPICE metrics which are not appropriate for distinctiveness. So I suggest using CIDErBtw [1,2].\n\n[1] J. Wang et al. Compare and reweight: Distinctive image captioning using similar image sets. ECCV, 2020.\n\n[2] J. Wang et al. On Distinctive Image Captioning via Comparing and Reweighting. TPAMI, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear enough and we can easily follow it. However, the novelty of the idea and the experimental results are not significant.",
            "summary_of_the_review": "This paper lacks novelty and convincing experimental results, so I give it a score of 3 in this phase.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2956/Reviewer_E9nV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2956/Reviewer_E9nV"
        ]
    },
    {
        "id": "XPkDAK5_fJ",
        "original": null,
        "number": 3,
        "cdate": 1666609565385,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609565385,
        "tmdate": 1666676972446,
        "tddate": null,
        "forum": "0uHNy9jmR7z",
        "replyto": "0uHNy9jmR7z",
        "invitation": "ICLR.cc/2023/Conference/Paper2956/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The author proposed an approach which concentrate on attributes generation to general image captioning and NOC tasks. It is an interesting attempt, however, there are some serious defects:\n1. It is mentioned that existing methods \u201coverlook some aspects of the scene\u201d, it is also the motivation of this work. However, the shortcomings of these methods remain unclear. Methods like [1] also trained on Visual Genome with corresponding attribute. 2. As for the experimental results, there are several examples which CWATR outperforms VIVO. However, it can not prove that it works well on the whole dataset, unless a global metric is designed, like CIDEr. In traditional metrics, VIVO is better than this method.\n[1] Xu Yang, Hanwang Zhang, and Jianfei Cai. Learning to Collocate Neural Modules for Image Captioning. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019. doi:10.1109/ICCV.2019.00435\n",
            "strength_and_weaknesses": "Strength\uff1ameaningful attempt in attribute generation.\nWeakness: few illustration of the interpretability of experimental methods\uff1bthe results and conclusion are inadequate and superficial\uff1bexperiments are insufficient relatively\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\uff1afairly good with some flaws\nQuality\uff1amedium\nNovelty\uff1afairly good with some flaws\nReproducibility\uff1amedium\n",
            "summary_of_the_review": "The direction of this paper is innovative, but needs more explanation and design of experimental methods and more filling of experimental results and conclusions. The author could add more experiments to prove the effectiveness of this method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2956/Reviewer_hWrE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2956/Reviewer_hWrE"
        ]
    }
]