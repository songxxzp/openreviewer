[
    {
        "id": "zkcGJ9TgoCI",
        "original": null,
        "number": 1,
        "cdate": 1666681344720,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681344720,
        "tmdate": 1666681344720,
        "tddate": null,
        "forum": "PoU_NgCStE5",
        "replyto": "PoU_NgCStE5",
        "invitation": "ICLR.cc/2023/Conference/Paper5906/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates applying differentially private stochastic gradient descent (DP-SGD) for training models towards achieving high robustness against distributional shift. Experiments are done on datasets with covariate, label, or subpopulation shift using various noise multipliers in DP-SGD. The resulting performance is compared with models trained with empirical risk minimization (ERM) in terms of test accuracy and generalization gap.",
            "strength_and_weaknesses": "Strengths:\n- The authors have conducted lots of experiments on various datasets and distribution shift types\n- The experimental details are well documented\n\nWeaknesses:\n- The paper is lacking a message and is unclear what the conclusion from these experiments should be. The authors makes some attempts to justify the use of SL methods for robustness based on their results, but (a) there is quite large variability in the findings (eg whether SL improves both robustness and accuracy, (b) the standard deviations are large in many occasions making the comparisons even more obscure.\n- Overall, I am concerned about the impact of this work given the insufficient justification and explanation of experimental results. What is the reason for different tradeoffs between robustness and accuracy observed for different types of shift? Is it because of the use of the chosen evaluation metric or something else?\n- There is too much variation in the generalization gap for different shift severity levels. For example, for the Imbalanced-CIFAR, it seems strange that at shift severity level 3, the generalization gap is one order of magnitude smaller than others. And for most of them, the standard deviation is large.\n- The experiment results are not conclusive and consistent across datasets. For example,  for label shift with Imbalanced-CIFAR, it seems that there is some improvement in both accuracy and generalization gap of DP-SGD over ERM. However, the improvement in accuracy is not observed when covariate and subpopulation shifts are considered. Again, for natural datasets, no concrete conclusions can be drawn as different shift severity gives different results. \n- The test accuracy on CIFAR appears far from the current state of art. Why not use a larger model to evaluate all metrics?\n- It is not made clear what the novelty is compared to Kulynych et al. 2020,2022?",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above",
            "summary_of_the_review": "I am familiar with DP-SGD and algorithmic stability, but I have not followed (and did not have time to do so during the short review period) the closely related works by Kulynych et al.. Hence, I might be misunderstanding the contributions. \n\nThe title of the paper is \"limits of algorithmic stability for distributional generalization.\" After reading the paper, I am not sure I got an answer to what those limits are. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5906/Reviewer_iJTD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5906/Reviewer_iJTD"
        ]
    },
    {
        "id": "gNupogA18a",
        "original": null,
        "number": 2,
        "cdate": 1666739624254,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666739624254,
        "tmdate": 1670516156882,
        "tddate": null,
        "forum": "PoU_NgCStE5",
        "replyto": "PoU_NgCStE5",
        "invitation": "ICLR.cc/2023/Conference/Paper5906/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper praises the training of learning algorithms by incorporating differential privacy techniques. In doing so, the idea is to be able to explicitly control the level of stability in order to be more efficient against perturbations of the data distribution. The results are essentially experimental, through a handful of real database and distribution shift settings.",
            "strength_and_weaknesses": "The topic of the paper is very interesting and highly relevant for the AI/ML community. It proposes well-chosen numerically experiments to illustrate the main claim. However, I believe that the questions being tackled cannot be answered robustly without a very large sets of experiments, varying hyperparameter, significantly larger sets of data etc. Or propose a solid theoretical analysis.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written and easy to follow. May be the authors can add a word on how DP-SGD works for completeness.\nSee section below for other comment.",
            "summary_of_the_review": "\u201cUnfortunately these works don\u2019t provide any guidance on practical tradeoffs with robustness, except for a small positive result in Suriyakumar et al. (2021).\u201d It seems a bit rude to qualify such a result as \u201csmall\u201d.\n\u201ccurrent theory provides no insight into the relationship between the level of stability and other model performance metrics, such as accuracy\u201d. This seems a bit strong, most of the work on algorithmic stability, if not all, explicitly discuss the relation between stability and generalization (and so accuracy).\n\nThe authors raise some key questions in section 4. For example, what level of stability is needed to observe a positive difference between ERM and learning with stability.\n\nHowever, I do not see any clear, unambiguous, quantitative answers to these questions. This is the main criticism of the previous contributions. This seems to depend, of course, on the model used, the current data, the settings of the hyperparameters etc... At most, three algorithms have been tested (CNN, LR, ResNet18).\nThe title seems to be too strong for the content of the article. No theoretical results on the limits of stability are presented, nor a vast and diverse numerical experimentation to support their claim. \n\nTypos: \n- \"was was\" in section 4.2\n- \"it is important\" in section 7\n- \"the the\" in section 4.1\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5906/Reviewer_JhgW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5906/Reviewer_JhgW"
        ]
    },
    {
        "id": "cD4rrrkznb",
        "original": null,
        "number": 3,
        "cdate": 1666899352336,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666899352336,
        "tmdate": 1666899352336,
        "tddate": null,
        "forum": "PoU_NgCStE5",
        "replyto": "PoU_NgCStE5",
        "invitation": "ICLR.cc/2023/Conference/Paper5906/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper empirically investigates the distributional generalization in the context of distribution shift. More specifically, the authors studies the performance of models trained with differentially private stochastic gradient descent (DP-SGD) under different types of distribution shifts---including covariate shift, label shift, and subpopulation shift. Through extensive experiments, the authors identify three interesting findings about stable training: (1). models trained with DP-SGD achieve smaller generalization gap across different types of distribution shifts; (2). Stable training (i.e., DP-SGD training) can serve as an effective way to boost model performance (test accuracy) under certain distribution shifts; (3). Under distribution shifts, there exists a trade-off between stability, model generalization gap, and model performance.",
            "strength_and_weaknesses": "Strength:\n\n1. The connection between algorithmic stability and model generalization is an interesting topic and has not been extensively studied in the context of distribution shifts empirically. \n\n2. This paper studies three types of distribution shifts, which extends previous work [KYY+2022] and cover a wide range of interesting and practical distribution shifts in practice.\n\n3. The finding on the effectiveness of stable training boost generalization performance under certain distribution shifts (label shifts and natural shifts) is very interesting, which is practically interesting.\n\nWeakness:\n\n1. [minor] The related work section can be further improved, for example, including more related work on the 'robustness to distribution shift' part.\n\nTypos:\n\n1. Typo in Eq. (4), should be $D^{\\prime} \\sim P$.\n\n\n[KYY+2022] Bogdan Kulynych, Yao-Yuan Yang, Yaodong Yu, Jaros\u0142aw B\u0142asiok, and Preetum Nakkiran. What you see is what you get: Distributional generalization for algorithm design in deep learning. arXiv preprint arXiv:2204.03230, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "(Clarity) This work is well presented.\n\n(Quality) High quality work, makes important contributions.\n\n(Novelty) Novel.\n\n(Reproducibility) Good.",
            "summary_of_the_review": "This paper studies an important problem in distributional generalization under distribution shifts and obtains interesting and strong empirical results, I would recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5906/Reviewer_weZZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5906/Reviewer_weZZ"
        ]
    },
    {
        "id": "AwkVDLIpuT",
        "original": null,
        "number": 4,
        "cdate": 1667175691150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667175691150,
        "tmdate": 1667175691150,
        "tddate": null,
        "forum": "PoU_NgCStE5",
        "replyto": "PoU_NgCStE5",
        "invitation": "ICLR.cc/2023/Conference/Paper5906/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper experimentally investigates the effect of differentially private stochastic gradient training on robustness against three types of distribution shifts. The experiments involve data with synthetically generated distribution shifts as well as natural ones.",
            "strength_and_weaknesses": "The main strength of the paper is that it investigates a potential mitigating algorithmic approach to an important problem; robustness under distribution shift. Although the paper does not have any novel theoretical contributions, I think papers that exhaustively and empirically investigate the implications of novel theoretical output in the field are important contributions, and as such should be encouraged.\n\nHowever I do not believe this paper clears the bar in terms of making a thorough exploration of the questions at hand. The paper examines the implications of research by Kulynych et al. 2022 in three specific forms of distribution shift. First main weakness of the paper is the description of the problem setting. The authors' exposition is confusing rather than clarifying, and I will go more into the details of this in the next section.\n\nMy other main concern is that the authors' investigation falls short of answering the questions posed by their empirical results. Given the paper makes no theoretical or methodological contributions, it would be fair to expect more investigation - at least speculation - regarding the implications of their findings. Some examples of unfollowed leads are:\n\n- That the most severe shifts favor ERM is very important finding in this context, that should have been investigated more - the authors' conclusion \"stability has a limit\" falls short of answering the questions that are provoked by this.\n- Why might the results for synthetic and real distribution shift datasets differ so much?\n- Why do we not observe a robustness - accuracy trade-off consistently?\n- Why should differentially private training confer more robustness to distribution shifts vs. distributionally robust optimization?\n\nWhen I started reading this paper, I thought these would be the exact types of questions investigated by the paper - in absence of this I find the paper's publication hard to justify.",
            "clarity,_quality,_novelty_and_reproducibility": "In this section I want to point out some aspects of the authors' exposition that makes it hard to follow and/or understand their findings.\n\n- For a paper that exhaustively uses DP-SGD as its training method and discusses the effects of its hyperparameter, I am surprised to see it never being introduced explicitly.\n- What is the difference between M and \\mathcal{M}?\n- How is the Hamming distance defined in this context? (please explicitly state)\n- Should not Definition 1 be expressed for each subset of \\Theta instead of \\Theta? If not, and if \\Theta = Im(M), then is not Pr[M(D) \\in \\Theta] = 1?\n- Neither total variation distance, nor what T, P, and Q are supposed to stand for are introduced.\n- Pg.4 What does a distribution being equal to Cartesian product of features and labels mean?\n- How does covariate shift in MRI images not change disease prevalence?\n- \"features themselves stay the same\" is confusing, something to the effect of \"class-conditional densities are constant\" should be used instead.\n- Subpopulation shift is not sufficiently described.\n- Do you compute Difference in Generalization Gap as stated or do you use an estimate of it in experiments?\n- Why are different metrics used in MIMIC-III and PovertyMap?\n\nTypos and organization:\n- Please alphabetically order the references.\n- Pg. 2 \"algoirhtm\"\n- Pg. 9 \"it important\"\n",
            "summary_of_the_review": "I believe that the current paper attempts a worthy exploration of an important subject, however it neither states the problem sufficiently clearly nor explore the implications of its results thoroughly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5906/Reviewer_6xKr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5906/Reviewer_6xKr"
        ]
    }
]