[
    {
        "id": "8yRySJq0PAH",
        "original": null,
        "number": 1,
        "cdate": 1666021133195,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666021133195,
        "tmdate": 1666021133195,
        "tddate": null,
        "forum": "7UrHaeZ5Ie7",
        "replyto": "7UrHaeZ5Ie7",
        "invitation": "ICLR.cc/2023/Conference/Paper1217/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approximate inference algorithm for Markov Logic networks (MLNs) with disjunctive clauses.  The exponential grounding of the first-order structure is mitigated by leveraging a neural network architecture called LogicMP, which implements an efficient mean field approximation of the true posterior.\n",
            "strength_and_weaknesses": "Pros:\n+ MLNs represent a very popular class of relational probabilistic models. \n+ Advances in MLN inference are valuable for a broad audience.\n\nCons:\n- In my opinion, the presentation of the proposed approach should be greatly improved before publication (details in the Clarity section).",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper very hard to read in multiple sections, as well as a number of unsubstantiated claims.\n\n---\n\n    \"[...] lifted methods are not only difficult to implement ...\"\n\nWhy?\n\n    \"... but also infeasible when the symmetric structure becomes invalid, e.g., one other model is integrated.\"\n\nWhat does 'one other model is integrated' means in this context? Also, there exists work on lifted inference in asymmetric MLNs [2].\n\n---\n\nThe role of latent variables in Eq.1 is not clear to me. This is also the first time that I encounter LVs in the context of MLNs. Somewhere in the text it is mentioned that latent variables are used to model unobserved facts, under an open world assumption (OWA). Yet, most works on MLNs use CWA. I feel like the OWA made in this work is crucial for understanding the whole approach, but it is only mentioned in the experiments (Sec. 5.2) and its discussion is deferred to Appendix L.\n\n---\n\nHow mean-field variational inference can be used in MLN is also unclear. Adding an example to Section 4 would be beneficial as Fig. 1 is not really informative in this regard.\nHow is the variational distribution constructed? What parameters are optimized to minimize the KL divergence? \nThe work supposedly uses neural networks, but it is not clear how these LogicMP layers are structured or trained. What are the parameters of LogicMP?\n\n---\n\nI also found the use of Einsum optimization quite convoluted.\n\n    \"Naturally, we can generate all the propositional groundings in Gf as a set of hyper-edges to perform the aggregation.\"\n\nHyper-edges over what? No hyper-graph is mentioned before this sentence.\n\nThe abstract claims that \"In most practical cases, it can reduce the complexity significantly to polynomial for the formulae in conjunctive normal form (CNF)\". \nThis is a very strong statement. What 'most cases' means exactly? I couldn't find a formal definition of this set of cases in the text.\n\n---\n\nThe experimental section gives some clues on the proposed approach,\nbut also raises more questions.\n\n    \"In general, LogicMP poses no constraints to its applications and can incorporate logic rules for various purposes.\"\n\nThis is a very vague sentence. I thought that LogicMP layers are used to approximate inference in MLNs.\n\n    \"Ideally, it can serve as a logic CRF for arbitrary neural networks and be trained end-to-end via maximum likelihood estimation (MLE) as in CRFasRNN (Zheng et al., 2015). However, as the labeled data is rare in our datasets, MLE is prone to overfitting and we turn to semi-supervised learning where rules act as priors to infer the latent variables.\"\n\nThis paragraph is hinting at different applications.\n\n    \"We fix the formula weights of 1 [...] for all the experiments.\"\n\nWhy? Shouldn't the weights of the MLN be learned from data? If these weights are fixed, what is optimized during the training?\n\n    \"Note that LogicMP can perform full graph computation efficiently, but the practical results indicate that training with sampling is more stable for the learning of logical knowledge for the backbone model.\"\n\nWhat does 'full graph computation' mean? What is the 'backbone model'? The notion was never mentioned before.\n\n---\n\nNotes:\n\nHinge-Loss Markov Random Fields (HL-MRFs) is not an inference technique for MLNs, but a different (albeit similar) probabilistic model. See Section 4.2 in [1].\n\nReferences:\n\n[1] Bach, Stephen H., et al. \"Hinge-loss markov random fields and\nprobabilistic soft logic.\" (2017).\n\n[2] Van den Broeck, Guy, and Mathias Niepert. \"Lifted probabilistic\ninference for asymmetric graphical models.\" AAAI, 2015.\n",
            "summary_of_the_review": "The problem considered in this work is relevant, but I had an hard time reading the paper. I fully read the paper multiple times, but there are still important aspects of the proposed approach that I don't understand. I cannot reccomend the current version for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1217/Reviewer_qmoi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1217/Reviewer_qmoi"
        ]
    },
    {
        "id": "OAduMIET49H",
        "original": null,
        "number": 2,
        "cdate": 1666605618085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605618085,
        "tmdate": 1666605825180,
        "tddate": null,
        "forum": "7UrHaeZ5Ie7",
        "replyto": "7UrHaeZ5Ie7",
        "invitation": "ICLR.cc/2023/Conference/Paper1217/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new method for inference in MLNs.\n",
            "strength_and_weaknesses": "+ Very interesting idea\n+ Well written paper\n- lack of experimental results\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and it reports a new method to do efficient inference in MLNs",
            "summary_of_the_review": "The paper is well written and the claims are supported by theoretical proofs.\n\nIt is not clear whether the structure of the MLNs are fixed or learned from scratch. In this case this should be explained in the paper.\nIn particular, while the part concerning the mean-field approach is very clear and correctly presented, there is a lack of description about the structure learning adopted in the paper. \n\nFurthermore, more datasets could be used in order to prove the validity of the approach.\n\nFinally, the paper is interesting but could be improved integrating some parts as reported above.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1217/Reviewer_gHdR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1217/Reviewer_gHdR"
        ]
    },
    {
        "id": "pCSCvGsTu6",
        "original": null,
        "number": 3,
        "cdate": 1666663932843,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663932843,
        "tmdate": 1666663932843,
        "tddate": null,
        "forum": "7UrHaeZ5Ie7",
        "replyto": "7UrHaeZ5Ie7",
        "invitation": "ICLR.cc/2023/Conference/Paper1217/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to improve inference in Markov Logic Networks through the use of deep models. In particular, Mean Field inference is implemented as layers which pass messages to compute summations more efficiently by exploiting logical structure in the MLN.\n\nThe time complexity analysis is performed to show that the exponential complexity of a Mean Field iteration is reduced for chain ruled formulas in the MLN. Further by parallelizing, we can achieve far salable computations of the messages in the MF iterations. Einstein summation is used to reduce complexity of enumerating all possible groundings of a first-order formula. It is shown that in some cases, even when the total number of groundings of the formula is large, the complexity of computing messages can be reduced by performing summations using Einsum. \n\nExperiments are performed with 3 datasets in the MLN literature. Comparisons are with existing MLN-based inference algorithms as well as ExpressGNN which is a GNN-based approach for MLN inference. Results indicate that the proposed approach is accurate and also scale well with increased number of groundings.\n",
            "strength_and_weaknesses": "Strengths\n-The use of Mean-Field inference for MLNs through deep model layers seems like a novel contribution to me.\n-The complexity analysis is performed rigorously and shows the scalability of the proposed method.\n-Experiments show that the proposed method scales well over large number of groundings while also being accurate\n\nWeakness\n-One weakness is that the problem of controlling the exponential grounding problem in MLNs has been explored in earlier work. For example, Venugopal et al. (AAAI 2015) model the same problem using a junction tree formulation and use approximate message passing to scale up the computations. In such cases, the complexity is dependent not on the number of groundings but the width of the tree-decomposition. For a chain formula, this width is small but for more densely connected formulas, this is large. Other similar approaches have also been explored. \nThus, I am not sure how the proposed method compares in terms of significance.\n\n-The experiment baselines particularly for CORA does not seem as strong. Another approach that implements MLNs through deep models (Graph Markov Neural Nets, Qu et al. ICML 2020) seems to show better performance. In general, I feel experiments on just two datasets (since smokers is more of a toy dataset) does not strongly demonstrate the significance of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well written and has rigorous complexity analysis. The significance of the work could have been demonstrated better wth relation to prior work as well as experiments with more state-of-the-art systems.",
            "summary_of_the_review": "The connection of Mean Field iterations and the deep model layers seems nice. Also, parallelizing the computations and demonstrating scalability is a nice contribution since inference in MLNs is known to be hard due to the large number of groundings. However, as I understand it, other approaches have tried to address the grounding problem in MLNs, so maybe the novelty is limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1217/Reviewer_cRow"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1217/Reviewer_cRow"
        ]
    },
    {
        "id": "a0_LPhNQoE",
        "original": null,
        "number": 4,
        "cdate": 1666848866342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666848866342,
        "tmdate": 1670912345986,
        "tddate": null,
        "forum": "7UrHaeZ5Ie7",
        "replyto": "7UrHaeZ5Ie7",
        "invitation": "ICLR.cc/2023/Conference/Paper1217/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed using tensor operator, i.e. einsum, to perform mean field inference in MLN. Author demonstrated that each iteration of the MF message passing can be efficiently computed using a sequence of einsum operation. This not only enables one to perform approximate inference in MLN, the inference procedure can easily implemented as neural layers to perform neural-symbolic learning. In particular, author demonstrated that it achieves good results on relational prediction tasks when combining a neural predictor and a MLN regularizer. ",
            "strength_and_weaknesses": "Strength:\n1. It is likely the first work that applies mean field on MLNs. The reduction of the mean field procedure using einsum makes the procedure easy to be implemented and accelerated. \n2. Author shows good result of using MLNs as a regularizer for learning a model in relational application, e.g. UW-CSE and Cora.\n\nCons:\n1. The notation introduced in the paper is a little bit dense, e.g. variables with many super scripts and lower scripts. It was challenging for the reviewer to go through the equations within few passes. One unsolicited suggestion is to remove some of the super script when the context is clear. E.g. In Theorem 4.3, the super script on the n might be known from the context and can be dropped. Alternatively, adding more examples on how each equation is computed might also help. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is good, although the notations are quite dense. \n\nThe idea of performing mean field inference on MLN using einsum is interesting and author further demonstrated its applications in various \nprediction tasks. \n\nOn the reproducibility, author explained well on the implementation of LogicMP layer, and they also explain how the LogicMP layer interplay with a neural network backbone for generating predictions in relational problems.\n\nI have two questions/comments:\n\n1. It is known that probabilistic database (PDB) can be converted to a MLN [1]. There is an inference algorithm in PDB that uses relational join and aggregation operator, which is similar to the einsum. The inference algorithm is exact for safe query, and can be an approximation when the query is not safe. Is there any possible connections between the proposed method and PDB inference literature.\n[1] https://simons.berkeley.edu/sites/default/files/docs/5662/talk-simons-2016.pdf\n\n2. On the  experiment result shown in Table 2, can author also present the quality of the NN backbone alone without the distillation from the LogicMP? I think this could be a better motivation for neural modelers of applying LogicMP when there are soft logical connections between the outputs.\n\n",
            "summary_of_the_review": "In general, the paper delivers an interesting approach to perform inference in MLNs, and authors further demonstrated that the inference procedure can be integrated with a neural backbone to perform prediction tasks while leveraging the logical connections between the output.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1217/Reviewer_sKHS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1217/Reviewer_sKHS"
        ]
    }
]