[
    {
        "id": "UMGl7XEx64",
        "original": null,
        "number": 1,
        "cdate": 1666440027093,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666440027093,
        "tmdate": 1666440027093,
        "tddate": null,
        "forum": "czL6NLxJsx",
        "replyto": "czL6NLxJsx",
        "invitation": "ICLR.cc/2023/Conference/Paper5672/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors address the problem of outlier-robust group inference via gradient space clustering. They extract features first and then calculate the gradient of the extracted features. Finally, they cluster in gradient space.",
            "strength_and_weaknesses": "Strength: The paper is well written at the beginning and validate features in gradient space is robust to outliers. It seems that numerical is good.\nWeakness: \n1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\n2. In comparison of gradient space and feature space, the normalization of the data in Figure 2 is not so clear. I think you do not need to do normalization of the data, since the shrinkage of the correctly classified points is profit to outlier identification.\n3. It is not clear what variable the derivative is based on. I thought the gradient is a very large dimensional vector (tensor). Since the network is with many layers (e.g. ResNet-50), the dimension of the derivative should be about 50 times. When moving to numerical result, I found the ResNet-50 is pretrained and the derivative is only related to the parameters of logistic regression.",
            "clarity,_quality,_novelty_and_reproducibility": "It may not be so novel.",
            "summary_of_the_review": "The authors address the problem of outlier-robust group inference via gradient space clustering. They extract features first and then calculate the gradient of the extracted features. Finally, they cluster in gradient space. But the method just stacks some existing methods together, it is not so novel. I think it is not enough for this top conference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5672/Reviewer_tuP3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5672/Reviewer_tuP3"
        ]
    },
    {
        "id": "V4diA_Dqze",
        "original": null,
        "number": 2,
        "cdate": 1666724580258,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724580258,
        "tmdate": 1666724580258,
        "tddate": null,
        "forum": "czL6NLxJsx",
        "replyto": "czL6NLxJsx",
        "invitation": "ICLR.cc/2023/Conference/Paper5672/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on a clustering problem. The authors proposed to cluster the data in the gradient space. The proposed method is tested on multiple real-world datasets.",
            "strength_and_weaknesses": "Strengths\nThe authors proposed to cluster in the space of gradients, which looks novel.\n\nWeaknesses\nThe problem setting might need to be better explained.\nThe hypothesis behind clustering in the space of gradients needs to be clarified.\nSome details of the experiments need to be elucidated. ",
            "clarity,_quality,_novelty_and_reproducibility": "The problem setting of the paper is clear to me. On page 3, the author claim that the goal is to learn the model h. Is identifying the memberships of the groups also the goal of the problem? I am not sure what it means by \"group loss\" If the authors do not want to provide a detailed formulation, they might want to describe the purpose for such loss.\n\nI am confused by Figure 1. the outlier has the same label as the group g=3. Why is it considered an outlier rather than a member of group g=3? It is also not obvious why the solution in Figure 1(d) better than that in Figure 1(c). Figure 1(c) looks like a more meaningful result than 1(d).\n\nThe hypothesis of conducting clusters in the gradient space is unclear to me. I understand that such a method can identify mislabeled data. But I do not think the mislabeled data is equivalent to outliers. Even if the proposed method can identify outliers, a straightforward strategy is to remove these identified outliers and then conduct clustering in the original feature space. I do not understand what the clustering results in the gradient space mean and what the hypothesis behind it is.\n\nOn page 8, the authors need to clarify what the worst-group performance means. How are the worst-group accuracy and average accuracy computed?\n",
            "summary_of_the_review": "Based on the experimental results, the method might be promising. However, I have difficulty in understanding some details of the paper. I believe a better clarification is necessary before the acceptance of this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5672/Reviewer_zxmE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5672/Reviewer_zxmE"
        ]
    },
    {
        "id": "o9dCjjHYVL",
        "original": null,
        "number": 3,
        "cdate": 1666827217741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666827217741,
        "tmdate": 1666827217741,
        "tddate": null,
        "forum": "czL6NLxJsx",
        "replyto": "czL6NLxJsx",
        "invitation": "ICLR.cc/2023/Conference/Paper5672/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is concerned with learning in the presence of relatively small `minority' groups in training data, through the use of clustering of loss gradients.  The authors argue that in addition to its previous uses in identifying outliers in training data, clustering in the gradient space can also be used to identify and annotate minority groups, in preparation for group-aware learning.  As a byproduct, outliers can also be filtered from the training set.  The approach is shown to be competitive with state-of-the-art methods for Distributionally Robust Optimization.\n",
            "strength_and_weaknesses": "Strengths:\n\n1) Well-motivated and well-organized. The authors take care to situate the gradient-based clustering strategy with respect to the issues in group annotation learning.\n\n2) A good explanation is given as to why gradient-based clustering can distinguish outlier points as well as minority groups. Such training examples tend to be misclassified, with higher-than-average contributions to the loss, and thereby higher than average gradient magnitudes. The authors analyze the group dispersion effect with respect to Euclidean and centered cosine distance.\n\n3) Loss gradient clustering is shown to be both conceptually simple and practical. The experimental results provide evidence of very significant improvements over clustering in the learned feature space, and is competitive with the state-of-the-art methods for DRO.\n\nWeaknesses:\n\n1) Gradient-based clustering, even for loss functions, is not a new idea. For example, see:\n\nArmacki et al., \"Gradient Based Clustering\", arXiv 2202.00720, 17 June 2002.\n\nMonath et al., \"Gradient-based hierarchical clustering\", Discrete Structures in Machine Learning Workshop, NIPS 2017.\n\n2) The proposed group annotation learning is a fairly straightforward application of the loss gradient clustering technique, one which would present little in the way of difficulty to most practitioners. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The organization and clarity of the paper is excellent. The authors build their case carefully, situating the gradient-based clustering strategy with respect to the issues in group annotation learning. The advantages of loss gradient clustering are well-supported with examples and analysis for two common distance measures. The supplementary information gives sufficient information for the experiments to be reproduced.\n\nHowever, loss gradient clustering cannot be considered as particularly novel. It is a relatively simple (but effective) technique that is likely to be rediscovered by practitioners in many contexts.\n",
            "summary_of_the_review": "Although the authors have clearly shown the value of loss gradient clustering for group annotation learning, the work may not meet the standard of novelty expected of an ICLR submission.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5672/Reviewer_5MtF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5672/Reviewer_5MtF"
        ]
    }
]