[
    {
        "id": "0tI84BuZqI",
        "original": null,
        "number": 1,
        "cdate": 1665754232825,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665754232825,
        "tmdate": 1668702032840,
        "tddate": null,
        "forum": "CjTHVo1dvR",
        "replyto": "CjTHVo1dvR",
        "invitation": "ICLR.cc/2023/Conference/Paper61/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the presented work, the authors present an approach to perform self supervised learning on tasks where molecular 3D geometry is relevant. They do so by taking advantage of equivariant neural networks in combination with a denoising score matching method on molecular distances. ",
            "strength_and_weaknesses": "Strengths:\n* Strong empirical results\n* Novel idea\n* Clear methodology\n* Evaluation on several relevant biophysical datasets\n\nWeaknesses:\n* Hard to read at times\n* Does not reference a very related work, while claiming to be the first study to perform pretraining on 3D molecular data.\n* Does not provide code to support analyses\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is novel, albeit it needs to be put in the context of other related studies (see summary of the review).\n\nAdditionally, the manuscript could use additional time for rewriting, as it reads very clunkily at times (see last paragraph of section 2, or the energy-based model paragraph on section 3 for examples). \n\nThere is no code provided to support the claims or help reproduce the study, as authors claim this will be released in the future.",
            "summary_of_the_review": "Overall, the work is original and in my belief deserves publication. However, I think the authors should probably reconsider some of the claims presented in the study, specifically those regarding them being the first to use pretraining strategies on 3D molecular data. Specifically, I believe that claim should be adapted based on the following works:\n\nhttps://arxiv.org/abs/2206.00133?context=q-bio\n\nThe authors should either compare to these approaches or alternatively discuss why their approach is theoretically different, or whether there are computational disadvantages in either of them.\n\nSome other comments/questions:\n\n\u2022\tIt is not clear how the perturbed geometry g2 is obtained from g1 in Algorithm 1. \n\n\u2022\tDo the authors have any suggestions on how to choose the different noise levels \\sigma_l in Algorithm 1?\n\n\u2022\tIt is not clear from the text whether the Molecule3D dataset is a subset of the PubChemQC database.\n\n\u2022\tIt is also not clear what the authors are referring to when they refer to the so-callsed \u201csupervised pretraining baseline\u201d. In particular I do not see where the pretraining is done here. From the text it appears that this is simply a supervised baseline?\n\n\u2022\tIt would be fantastic if the authors could provide standard deviations for the numbers reported in Tables 1, 2, as they are reported in 3.\n\n\u2022\tIn the downstream tasks on binding affinity prediction, is the pretraining done on both the pocket + ligand graph, or only on the ligand? Have the authors experimented with both of these approaches? In this section too, it is not clear to me what the second task (LEP) is. What is an active/inactive conformer of a protein? Are the authors referring to an active/inactive classification tasks for ligands (i.e. as in virtual screening)? Are the same molecules considered in both tasks?\n\n\u2022\tSome references do not seem to be present in the main manuscript (e.g. 20)\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper61/Reviewer_WmAR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper61/Reviewer_WmAR"
        ]
    },
    {
        "id": "V00zL-IazW2",
        "original": null,
        "number": 2,
        "cdate": 1666258325206,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666258325206,
        "tmdate": 1666258325206,
        "tddate": null,
        "forum": "CjTHVo1dvR",
        "replyto": "CjTHVo1dvR",
        "invitation": "ICLR.cc/2023/Conference/Paper61/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed to use denoising as pretraining for SE(3)-invariant neural network models. The main idea is to add some noise to the coordinates, and use a score model to predict the added noise. The pretrained model are used in several different tasks, including molecule property prediction, force field and binding affinity prediction. The proposed pretraining method was proved better than other pretraining methods in the tasks.",
            "strength_and_weaknesses": "Strength: It is a good idea to use denoising in pretraining neural networks in molecular tasks, and the results showed that the idea is valid. The paper is written clearly.\n\nWeakness: The advantage of GeoSSL compared to other pretraining methods seems minor, as seen in Table 1, 2 and 3. I think it is necessary to include two more experiments: (1) ablation study on hyperparameters (2) error bars over multiple random seeds. With these two experiments, it will be more convincing that the advantages are really from pretraining, not hyperparameter-tuning or randomness.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly with high quality. The idea is moderately novel. The reproducibility can be improved by providing ablation study on hyperparameters and error bars.",
            "summary_of_the_review": "This paper proposed to use denoising in training 3D roto-translation invariant neural networks, and proved its superiority in molecular property prediction, force field and binding affinity tasks. However, the experiments can be more convincing if ablation study and error bars are provided",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper61/Reviewer_wxWw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper61/Reviewer_wxWw"
        ]
    },
    {
        "id": "SQHwZoHkFvg",
        "original": null,
        "number": 3,
        "cdate": 1667493725160,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667493725160,
        "tmdate": 1667493725160,
        "tddate": null,
        "forum": "CjTHVo1dvR",
        "replyto": "CjTHVo1dvR",
        "invitation": "ICLR.cc/2023/Conference/Paper61/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a self-supervised method for 3d molecular data. The authors started with maximizing mutual information, and obtained an objective loss (denoising score matching). The idea is supported by several 3d geometric pretraining experiments.",
            "strength_and_weaknesses": "\nStrengths\n1. The derivation from mutual information maximization to denoising score matching is novel.\n2. The experiments are convincing.\n\nWeakness\n1. In Sec 4.3.1 Eq.(5), the definition of $d$ and $r$ are missing\n2. The calculation of $d$ is pariwise distances of all points (or atoms). If the number of points is very large, it could be very slow (quadratic complexity)?\n3. It seems that the method can also be used with general point cloud data (no edges between vertices), can the authors elaborate more about this? The complexity seems to be an important problem if we want to apply the method to large point clouds.\n4. The main contribution is the denoising distance matching. Is it appropriate to put \"SE(3)-invariant\" in the title?\n5. Why is this method superior to a vanilla denoising autoencoder (the simplest method we can come up with when dealing with SSL)? More discussions are needed\n6. How is the memory consumption given different sizes of input graphs?\n7. If I am understanding this correctly, the row marked with a dash (-) means not using any pretraining method (Table 1-3). It is just equivalent to training from scratch.\n8. In D.1, how many v100 cards do you use for a single experiment?\n9. Can the authors discuss a bit more about the limitations of the proposed method?",
            "clarity,_quality,_novelty_and_reproducibility": "The description of the method is clear and easy to follow. However, the code is missing. It would be good if the authors can include the computing resources for each experiment.",
            "summary_of_the_review": "The paper proposed an interesting SSL method. But I am concerned about the computing resources needed due to the quadratic complexity.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper61/Reviewer_a9bJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper61/Reviewer_a9bJ"
        ]
    }
]