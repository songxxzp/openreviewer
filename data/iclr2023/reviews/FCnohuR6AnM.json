[
    {
        "id": "cLmV-w__wrE",
        "original": null,
        "number": 1,
        "cdate": 1666668760534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668760534,
        "tmdate": 1672831789063,
        "tddate": null,
        "forum": "FCnohuR6AnM",
        "replyto": "FCnohuR6AnM",
        "invitation": "ICLR.cc/2023/Conference/Paper1288/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a simple and well-motivated technique for merging different models in a \"dataless\" way, and evaluates the method in comparison to simple averaging and Fisher-weighted averaging.",
            "strength_and_weaknesses": "Strengths\n\nThis paper presents an elegant technique and presents it clearly.  It includes a number of helpful experiments that show that the technique performs well, albeit not uniformly better than all baselines.\n\nWeaknesses\n\nThe paper fails to explain some important relationships between its approach and previous work, and may be missing important comparisons.  I also had a question about the ensemble baseline and I think comparing against other ensemble alternatives may be important.  I discuss these in more detail below.\n\nSection 3.2 says it will explain the relationship between the proposed method and Fisher-weighted averaging, but it doesn\u2019t really (it only discusses a distinction between an extreme special case of the proposed method and Fisher, and only very briefly and abstractly).  More discussion of how the proposed approach differs from Fisher and why we would expect it to perform better in certain situations is really critical.\n \nIn the experiments, the paper says the ensemble works \u201cby obtaining logits from all models and doing an argmax\u201d\u2014this is confusing.  Ensembles I\u2019m familiar with tend to average the probability outputs of each model, and then take the argmax of that average.  You could also average the logits (effectively, multiplying the probability outputs).  But taking the max over all the models, which is what this text seems to be saying, is not something I\u2019ve seen.  Maybe I\u2019m just unfamiliar with this approach and/or maybe it works better on these data sets than the other possibilities (especially in this setting with non-iid partitions), but the paper needs to explain this one way or another.\n\nRegarding potentially missing baselines, the last sentence of the model merging paragraph in the related work section mentions several \u201cimproved merging algorithms\u201d---how do these differ from the proposed method?  Why does the paper not include an experimental comparison against the best of these?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly quite clear, aside from the important omissions discussed above.  A couple of other minor questions:\n\nSec 5.1 \u2013 \u201cdifferent label distributions,\u201d how different are they?\n \nA sentence that says what the domains are from the different-domains experiment (e.g. a couple of examples of the domains) would be helpful.\n\nRegarding novelty, I feel that the approach is novel in detail although the high-level approach seems quite similar to Fisher-weighted averaging.",
            "summary_of_the_review": "While this paper presents an elegant and non-trivially effective technique on an important problem, I was not convinced that it explained or evaluated its relationship to previous work well enough for publication.  However, if other reviewers (especially ones more directly familiar with the recent previous work) are enthusiastic about the work, I would not try to stand in the way of acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1288/Reviewer_PCeq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1288/Reviewer_PCeq"
        ]
    },
    {
        "id": "BDCHCaluUu",
        "original": null,
        "number": 2,
        "cdate": 1666711629983,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666711629983,
        "tmdate": 1668658380961,
        "tddate": null,
        "forum": "FCnohuR6AnM",
        "replyto": "FCnohuR6AnM",
        "invitation": "ICLR.cc/2023/Conference/Paper1288/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on merging individual models on the parameter space without access to the respective training data. The individual models could come from different domains or even different tasks, their knowledge could be fused by merging without any training, and the resulting merging model could perform relatively well on all domains/tasks. Notably, the inference cost after merging is the same as the individual models. The authors propose the Regression Mean (RegMean) method to merge model parameters derived from merging linear models. RegMean uses the Gram matrices of the training data to reweight and linearly combine rows in weight matrices during merging. This paper consists of comprehensive experiments to examine merging multiple models trained on different domains / different tasks and test on in-domain and out-of-domain settings. Empirical results demonstrate the effectiveness of the method compared with recent baselines.",
            "strength_and_weaknesses": "### Strengths:\n\n1. I think the model merging problem studied in this paper is a very important direction to pursue \u2013 it is an efficient way of fusing knowledge from multiple large models without training or increasing the inference cost.\n2. The proposed method RegMean is derived clearly from linear regression, and is novel and simple.\n3. The experiments are comprehensive including multiple challenging settings and detailed analysis over pretrained models and hyperparameters. Particularly, the paper tests merging more than 2 models which the Fisher merging paper did not test. I think the insights revealed by these results could be useful for future researchers.\n4. The experimental results of RegMean are good, demonstrating the usefulness of model merging in various settings and outperforming recent baselines like Fisher merging.\n\n\n### Weaknesses:\n\n1. I think one limitation of RegMean is that it requires access to the Gram matrix stats of the training data of each individual model. Even though this is much more reasonable than accessing the original training data, it would still make RegMean not applicable in many cases. This problem exists for Fisher merging as well, and average merging is the most flexible\n2. I would like to see a more detailed discussion on other baselines mentioned in the related work by \"improved merging algorithms\" and why the authors do not compare with them.\n3. I feel some results from this paper are contradictory with the ones in Matena et al. (the Fisher merging paper) \u2013 this paper observed performance drop in all pairwise test settings (Figure 3, Figure 4), while Matena et al. observe gains over task-specific models when merging donor tasks with the target task. Particularly, Table A2 in Matena et al.  and Figure 4 in this paper are both on the GLUE tasks but with different conclusions, in Matena et al there is no clear performance drop. I wonder how the authors explain this difference, is it because the pretrained models are different (Bert v.s. DistillBert/Roberta)? The performance drop in pairwise merging seems pretty consistent in this paper though.  \n\n\n\n### Questions:\nIn Table 2, are the results a mixture of test examples from multiple domains? If so, how are domain-specific numbers calculated?\n\n\n```\nUpdate during rebuttal:\n\nI agree with the authors that the permutation lines of work are orthogonal and complementary to this paper, and the added discussion on this aspect is helpful. I also appreciate that the authors clarify my confusions in the response. Therefore, I increase my score to 8.\n```\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is very clear and novel with high quality from my perspective.\n",
            "summary_of_the_review": "I recommend acceptance of this paper since it studies an important and intriguing problem \u2013 model parameter merging, proposes new algorithms, and achieves good results in demonstrated comprehensive experiments. The weakness is that the method makes strong assumption on access to the Gram matrices of private training data.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1288/Reviewer_hdqV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1288/Reviewer_hdqV"
        ]
    },
    {
        "id": "6JVm8OXOcJx",
        "original": null,
        "number": 3,
        "cdate": 1666713853162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713853162,
        "tmdate": 1666756180285,
        "tddate": null,
        "forum": "FCnohuR6AnM",
        "replyto": "FCnohuR6AnM",
        "invitation": "ICLR.cc/2023/Conference/Paper1288/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new strategy for merging the parameters of models which have been fine-tuned on different datasets. The merging algorithm is intended for models which are all fine-tuned from the same base model. The primary contribution of the paper is the algorithm for merging models. The algorithm is formulated as a least squares problem where the goal is to minimize the squared distance between the merged model parameter and the individual model parameters applied to the individual datasets. The authors perform experiments on emotion classification and named entity recognition with a variety of models. They show that their approach improves over alternative prior algorithms though is generally not as good as full multi-task training.",
            "strength_and_weaknesses": "Strengths:\n\n- The algorithm is simple to understand, well justified, and as far as I can tell novel (though I am not that familiar with prior work for this specific task).\n- The RegMean algorithm appears to work better than existing alternatives (simple averaging and Fisher averaging) while being roughly as efficient in terms of computation time and memory.\n- The empirical results are well carried out with good baselines and comparisons for different models.\n\nWeaknesses:\nI have two concerns with this algorithm which I think should be addressed more prominently than they are. They are a bit buried late in the intro or the methodology sections.\n- First, the algorithm is really only intended to work in the case where every model starts from the same pertained baseline. It's pretty clear the algorithm would not work even for the same model architecture trained on the same dataset with a different random initialization as there is no attempt to solve the permutation problem. I would recommend being more upfront with this limitation.\n- Second, the primary justification for this approach is to maintain data privacy (since otherwise the models could all be trained on the data). However, the algorithm reveals the quite a bit of data for each task, roughly the size of the model which can be fairly large. I would be more upfront in discussing this weakness.\n- The algorithm is only used for \"linear\" layers. It would be interesting if it the authors also examined how well it worked for other types of layers (the self-attention matrices, convolutional layers, etc.).\n\n\nOther minor comments:\n\n- \"Margin Models Trained on Different Tasks\": Point of experiment unclear.\n- Results in Table 3 for Fisher under DeBERTa-large for Emotion with same/diff are very strange. Why does using a different head result in such a huge performance improvement compared to the same head?\n- In Figure 6a does it make much difference which two models you choose? Or is performance just worse when you go from any two models to three and so on.\n- Did you try experiments with using just a simple regularization (e.g. adding some small value to the diagonal of the Hessian prior to inversion)? I notice you scale the diagonal up in your algorithm but I'm curious if you tried the simpler/more traditional alternative and how it compared?\n- In the computation of the input features for each layer's regression problem you use 1000 batches (section 3.4?). How important is this parameter? Is the performance of the algorithm quite sensitive to it or not that much?\n\nNits:\n\n- The Gram matrix is usually the matrix of sample size x sample size - the matrices that come up in the normal equations e.g. X^T X aren't usually called Gram matrices. I would avoid using this name as it is confusing.\n- In section 3.4, 1k means 1,000? I would just write it out for clarity.\n- Unneeded period after \"Figure. 5a\" at the bottom of page 8.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- To the best of my knowledge the proposed algorithm is novel and interesting.\n- The paper is well written with clear description of the motivation, the algorithm, the experimental setup and the results.\n- The paper uses open datasets and model architectures and appears to be easily reproducible.\n",
            "summary_of_the_review": "The proposed algorithm is novel and an improved solution to the problem the authors describe. The algorithm is also fairly simple to understand and efficient to implement which are good features. The authors also show support that RegMean works well by demonstrating results on two NLP tasks with several models.\n\nMy main concern are the two primary limitations (as discussed above). These limitations might make the situations in which this algorithm is applicable somewhat niche.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1288/Reviewer_fiB5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1288/Reviewer_fiB5"
        ]
    },
    {
        "id": "dny6qB1y-VY",
        "original": null,
        "number": 4,
        "cdate": 1666755785938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666755785938,
        "tmdate": 1666755785938,
        "tddate": null,
        "forum": "FCnohuR6AnM",
        "replyto": "FCnohuR6AnM",
        "invitation": "ICLR.cc/2023/Conference/Paper1288/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors work on merge  individual models built on different training data sets to obtain a single model. More specifically, the authors propose a dataless knowledge fusion which is guided by weights that minimize prediction differences between the merged model and the individual models. ",
            "strength_and_weaknesses": "Strength\n1. The proposed setting is practical and has its merits in the real world.\n2. The proposed method has clear motivation and derivation. \n3. The authors evaluate the proposed method in several settings including in-domain and out-of-domain,  showing improvements compared to  various baseline. \n\n\nWeakness\n1. One of the motivation is that sharing training data may have data privacy issue. A detailed analysis regarding risks about sharing of  Gram matrices along with the models is needed. \n2. The proposed setting is more or less too specific. To protect the training data privacy, the federated learning serves the similar purpose. The authors did not compare their models to this setting. Another qualified baseline like ensemble is not included in the experimental Table 1. \n3. The improvements are not significant compared to Fisher weight merging on GLUE benchmark.  \n3. The problem setting is limited to that the training data is not allowed to share. It is also helpful to understand if this method is able to bring improvements when training data is allowed to share. Such a setting is more commonly used when studying weight merging.\n4.  The performance improvements brought by weight merging may decay as the training data size increases. The training data is limited to 1000 in  Table 1 without a clear reasons. To better understand the effectiveness of the proposed mechanism,  it is important to study if the weight merging could bring improvements when the training data is in a relatively large scale. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality: The paper is well written and easy to follow. \nNovelty: The paper has its merit but still some claims need to be further validated. The technical novelty is somewhat limited. The problem setting is limited to a newly proposed one and the proposed method is not validated in the existing settings.  \nReproducibility: The authors provide psuedo code and implementation description. The code release plan is unknown. ",
            "summary_of_the_review": "The authors propose a dataless knowledge fusion based on weight merging. The motivation of the proposed method is clear. However, the studies setting comes with many restrictions and generalization ability of this method is somewhat limited. More details can be found in Strength And Weaknesses section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1288/Reviewer_MwUi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1288/Reviewer_MwUi"
        ]
    }
]