[
    {
        "id": "k9__gyr4NeY",
        "original": null,
        "number": 1,
        "cdate": 1666541027648,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541027648,
        "tmdate": 1669056297405,
        "tddate": null,
        "forum": "ZqK0Hnlqo-",
        "replyto": "ZqK0Hnlqo-",
        "invitation": "ICLR.cc/2023/Conference/Paper3015/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors provide a new pooling strategy for GNN on learning physical systems on unstructured meshes along with a down sampling and up sampling method to reduce overhead between levels. Current GNN method to infer physical states suffers from two limitations. The computation complexity is high and over-smoothing problems will happen.  The author claim that such limitations is caused by pooling and building connection at coarser levels. So, the authors provide a new pooling strategy to solve this problem.  \n\nThe motivation is to create a pooling strategy that creates a 2-nd enhancement to preserve the connectivity on any input graph. So, they topologically sort the input graph and find that pooling on every depth will create a bipartition. To resemble the bipartition in mesh, they implement the BFS to compute the distance from a seed to other nodes. Then, they will do the pooling on the BFS frontiers. Also, building auxiliary edges can benefit from the bi-stride pooling.  \n\nAlso, the propose a method to reduce the overhead of transition modules between adjacent levels. There is a down sampling process to project the latent information to pivot nodes. After un-pooling, there will be an up-sampling process to return message back. \n\n## update after reading rebuttal.\nThe authors resolved my concerns and I would like to keep my score.",
            "strength_and_weaknesses": "The authors propose that the current limitation of GNN method for physics-based learning is pooling and building connections at coarser level. This is novel since not too many people is focused on this area. The idea is simple and easy to follow. The result shows a noticeable decreasing in training and inference time since the overhead is reduced. \n\nThere are still a few things that the author can do to improve data. First, it is better to involve some ablation study to show how each part contribute to the increase of training speed. Also, I am wondering if there are erroneous connections existed in graph produced by your method and what causes these wrong connections. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to follow. The idea is novel in this area. The authors provide the dataset and the evaluation method involved in the experiment. So, I think it can be reimplemented.  ",
            "summary_of_the_review": "This paper proposes a new pooling strategy for GNN in physics-based learning. They try to tackle the problem from a different aspect compared to previous research work. They borrow the idea of bi-partition, which is novel in this area. And the training speed and memory usage is decreasing, which means the computation is decreased.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3015/Reviewer_4tsk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3015/Reviewer_4tsk"
        ]
    },
    {
        "id": "kt9uY6ArKx",
        "original": null,
        "number": 2,
        "cdate": 1666611534196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611534196,
        "tmdate": 1666611534196,
        "tddate": null,
        "forum": "ZqK0Hnlqo-",
        "replyto": "ZqK0Hnlqo-",
        "invitation": "ICLR.cc/2023/Conference/Paper3015/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is concerned with how to pool irregularly spaced meshes for multi-scale estimation in physical simulations. The idea proposed herein is to use so-called bi-strides which \"pools nodes by striding every other BFS frontier\". The importance lies in the fact that many meshes of important problem exhibit complex topologies so naive pooling strategies may fail.",
            "strength_and_weaknesses": "* The problem considered in this paper seems to be very relevant.\n\n* The experimental evaluation seems compelling as the authors can demonstrate results superior to SOTA. The caveat here is that the algorithms are compared on simulated data only although that is common practice in that particular field.\n\n* \"works on any challenging mesh in the wild\": Considering that the authors evaluate on simulated data, this is a bold claim and should be removed.\n\n* My objections against this paper stem from the description of the algorithm. Generally, there are so many inaccuracies (each one of them minor by itself) that I have difficulties understanding the proposed solution.\nIn details:\n2 \n\"Motivations of Our Method\": What is $A$? The definition (adjacency matrix) only comes in 3.3. This makes reading unnecessary speculative.\nWhat are \"pivot nodes\"?\nWhat does connection conservatism mean? Why is this a good metric to optimize?\nFig 2: What is the update step? What is  an MP?\n3\nedge set: Are edges discrete or continuous?\nEq (1): Where does $v_0,i$ come from? \n3.2\nWhat is 2nd order enhancement?\nWhat is a BFS frontier?\nWhat does \"stride and pool all nodes at every other BFS frontiers\" mean? Why is that a bi-stride?\nSeeding heursitics: What is that and why is it relevant for this problem?\nWhat is \"interfacial information\"?\nHow can \"two adjacent matrices $A_l$ and $A_l^C$ be determined? What is $C$? Is it related to the edge weights in $C$ in \"Downsampling\" in 3.3?\n3.3\nThe authors claim to \"reduce the overhead of the learnable transition modules\" but do not provide insight as to why that is the case.\nWhat is a \"conserved variable\"?\nWhat is the nodal mass?\nWhat is a near uniform mesh?\nWhat are the volume/mass fields for irregular meshes? How can that be determined?\nUpsampling: Why do all nodes except the pivots have zero information after unpooling?\n\nIt could be that all those minor issues are clear to an expert. Nonetheless, the paper needs a thorough re-write to avoid guesswork and speculation.",
            "clarity,_quality,_novelty_and_reproducibility": "With the current description, I do not feel able to re-implement the algorithm. Due to the many small bugs in the explanation, I find the paper unclear and hard to follow.",
            "summary_of_the_review": "see above",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3015/Reviewer_YsGt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3015/Reviewer_YsGt"
        ]
    },
    {
        "id": "a1U3Gsr9jk",
        "original": null,
        "number": 3,
        "cdate": 1666660171342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660171342,
        "tmdate": 1666716225578,
        "tddate": null,
        "forum": "ZqK0Hnlqo-",
        "replyto": "ZqK0Hnlqo-",
        "invitation": "ICLR.cc/2023/Conference/Paper3015/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Long-range spatial information can be passed with a low computational cost through pooling and unpooling operations. On an irregular mesh, there is a challenge of how to select pivot nodes and assign non-pivot nodes to a pivot node for pooling so that all assignments are at most 2 hops away. Previous methods have used spatial information for assignment, but this can result in assignments of nodes greater than 2 hops away or even for which a path does not exist. \n\nThis paper makes two contributions:\n\n1. A pooling strategy for pivot node selection and non-pivot node assignment that guarantees all non-pivots are assigned to a pivot at most 2 hops away. \n\n2. A deterministic, parameter-free pooling/ unpooling algorithm\n\nBSMS-GNN is compared to 2 competitors: one is a flat GNN, and the other is a hierarchical GNN that uses spatial information for pooling. The comparison is made on 4 datasets for physical simulation. BSMS-GNN is shown to significantly improve number of training epochs to reach a target test error, time per epoch, inference time, and memory requirements on all datasets. It's also shown on a small toy dataset that the hierarchical competitor adds incorrect edges, while BSMS-GNN does not. \n",
            "strength_and_weaknesses": "------------------------------------------------------\n\nStrengths \n\nThe paper is well written and addresses a relevant problem. Previous methods that attempt to solve this problem are well-documented. The solution is elegant and is shown to be quite effective at significantly improving efficiency relative to the considered competitors. \n\n------------------------------------------------------\n\nWeaknesses \n\nThe main weakness are in the experiments:\n\n1. Not enough competitors are considered. Several previous methods for graph pooling are mentioned: \n\n- Gao & Ji (2019) \n- Lino et al. (2022a; 2021; 2022b) \n- Liu et al. (2021)\n- Fortunato et al. (2022) \n\nHowever, only the work of Lino, et al. is compared in the experiments. It is important that the test error and efficiency for all methods be compared.\n\n2. For the considered methods, there is no report of the test error from the converged models. Although BSMS-GNN is shown to achieve the same test error as GraphMeshNets as reported by Pfaff et al., Lino et al. (Simulating Continuum Mechanics with Multi-Scale Graph Neural Networks) report greater performance for MS-GNN-Grid than than GraphMeshNets. Thus, it is not clear which method produces the best test error. \n\n3. Related to points 1 and 2, it is not shown experimentally that connections that violate the boundary necessarily result in worse performance.\n\n4. It is claimed that BSMS-GNN will work well on the cloth benchmarks. This claim is too strong to be made without experimental evidence since the dynamics of the cloth appear to be very different from those considered.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing in the paper is of high quality, with a good flow and minimal typos. Logic is easy to follow, and methods are explained clearly. \n\nThe solution to the proposed problem is elegant and appears to be original.\n\n------------------------------------------------------\n\nTypos / Clarity issues\n\n\"BFS\" (acronym not yet defined),  abstract\n\n\"pivot nodes\" (not defined), p3\n\n\"none-parameterized transition\", p3\n\n \"$\\{p_i, q_i\\}$\" (should be \"$\\{p^i, q^i\\}$\" for consistent notation), p4\n\n\"$\\forall s\\in[1, S]$\" (implies $s$ can be any real number in $[1,S]$), p4 ",
            "summary_of_the_review": "This contribution is marginally above the acceptance threshold. It provides an elegant solution to an important problem with the bi-stride pooling strategy and proposes a parameter-free downsampling technique that allows for deeper hierarchies. This contribution could be strengthened with more thorough experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3015/Reviewer_iDNU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3015/Reviewer_iDNU"
        ]
    },
    {
        "id": "t5Rrn2McGy",
        "original": null,
        "number": 4,
        "cdate": 1666697445701,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697445701,
        "tmdate": 1666697445701,
        "tddate": null,
        "forum": "ZqK0Hnlqo-",
        "replyto": "ZqK0Hnlqo-",
        "invitation": "ICLR.cc/2023/Conference/Paper3015/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles the problem of pooling at different scales in the context of irregular geometries with a different level of granularity. The challenge is to learn / design the transition matrix between two successive scales while preserving the topological structure of the physics of the underlying systems. This strategy could be beneficial to study efficiently physical systems which are described by graph-meshes. The latter are highly irregular, especially at the boundary layer.\n",
            "strength_and_weaknesses": "The strength of the paper is in the novel design of the inter-scale pooling strategy that preserves the complex geometrical structure of a mesh.  It is not an easy problem in physics tasks because the downsampling / upsampling operations need to respect the mesh topology including the faces and cells. This is also related to the design of mesh and type of triangulations (tetraether, triangle, rectangle, etc) at the different parts of the physical domain. The developed strategy is extensively compared with state-of-the-art methods and shows an important gain in the training complexity, inference time, and memory footprint.\n\nHowever, a set of points are not discussed :\n\n1/ Absence of quantitative results (in a table) and comparison with state-of-the-art methods including some ablation studies on pooling strategy. We need to assess how the model performs. This point is of high importance. \n\n2/ A baseline comparison with deterministic pooling is missing. The idea is to see to what extent is important to have a learnable pooling rather than setting \u00e0 priori the different adjacency matrices at different scales and the transition matrices between two successive scales given the physical knowledge we have on the task and that the finest-grained mesh is known. In other terms,  One can predefine the structure of pooling in a deterministic manner given the physical domain boundary and geometry. Local connectivity is easily achieved and doesn't let the downsampling make connected-components (avoid isolated nodes).\n\n3/ Why do you think you need to keep the graph mesh structure? What about training a model on point clouds such as PointNet, PointNet++, and Geodesic convolution where the graphs at different scales are constructed using a radius graph sampler? These models have shown good results in neighboring tasks and can help to get rid of the graph/mesh. Especially, in physics, there is no unique graph for a downstream task. Graph/mesh construction is an ill-posed problem.\u00a0To confirm / infirm that a comparison between a cloud of points methods VS Graph methods is needed. This point is a follow-up of 1/ and 2/. \n\n4/ Is your model impacted by the over-smoothness, and-oversquashing problems encountered in GNNs? \n\n5/ In section 2 \"background and related works\", challenges in GNNs are not discussed. It could be helpful to add a paragraph to discuss the current challenges in GNNs including over-smoothness, and over-squashing w.r.t to spatial and spectral methods while making a link with the bi-stride neural network.\n\n6/ In figure one,  you said that it can lead to a wrong connection ? one simple way is to use PointNet and iterate with a neural network sufficiently deep to propagate the information from all the points to all the points in the coarse level. It ensures the global representation of information by local propagation in a hierarchical way which can help to keep the physical structure of data. It is linked to 3/\n\n7/ You mentioned in the motivation that a loss of connectivity is observed even with a powered adjacency matrix (higher-order). From an over-smoothness standpoint, higher-order adjacency matrices lead to a drop in performance. It is related to the fact that to ensure that the information has reached all the points, the depth of the neural network needs to be equivalent to the diameter of the graph (which is not tractable in practice). Moreover, higher-order adjacency matrices as the depth increase lead to A=1 at every entry. As a consequence the nodes become indistinguishable. \n\n8/ What is wrong if pivot and non-pivot nodes are not connected? Can you elaborate on that and add more information to the paper?\n\n9/ The claim of 3.2 Bi-stride pooling and adjacency enhancement: 2) not introducing wrong edges by spatial proximity. Why not?\nExtra edges beyond the local neighboring system could help to propagate information to further nodes which could not be accessible due to the over-smoothness of GNN architectures. It is related to 7/ \n\n10/ Can you support your claim? \"This enhancement can be geometrically interpreted as such: an auxiliary edge (i, j) should exist if j is reachable from i in 2 hops and one of which is an auxiliary edge at the finer level. \"\n\n11/ In A.4 scaling analysis, what is the graph-conv used in figure 7 ? and why not compared with GraphMeshNet and other related methods?\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow. The references are fair and sufficient. The method is original and the claims are well supported in the experimental part. However, l am not sure if the paper is reproductible as long as it is not mentioned if the code will be provided.",
            "summary_of_the_review": "The paper is good but several points need to be clarified and adrressed. I can increase my score depending on the responses and the updates. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3015/Reviewer_fbaN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3015/Reviewer_fbaN"
        ]
    }
]