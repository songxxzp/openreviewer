[
    {
        "id": "gJD-PFu4dCk",
        "original": null,
        "number": 1,
        "cdate": 1666164315969,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666164315969,
        "tmdate": 1666230905730,
        "tddate": null,
        "forum": "UpyXmNMdQEn",
        "replyto": "UpyXmNMdQEn",
        "invitation": "ICLR.cc/2023/Conference/Paper4535/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to apply knowledge distillation and model quantization techniques to Deep Reinforcement Learning (DRL), which contributes to less memory footprint and space complexity during training. They show that the dual-head architecture leads to superior performance than the common actor smoothing and policy distillation. The experimental results reported in Figure 4 seem to further support the effectiveness of dual distillation. ",
            "strength_and_weaknesses": "# Strength:\n- The results of \"these combined methods can effectively compress a policy network down to 0.5% of its original size, without any loss in performance\" seem encouraging. Though the authors mainly focus on the deployment, I personally think the proposed scheme may shed some light on the efficient training process of DRL, considering that the dual head architecture may serve as a self-distillation training pipeline to accelerate the convergence of both actor-network and critic-network. The idea of *AUXILIARY CRITIC LOSS* is interesting.\n- The proposed method is quite simple and easy to follow, which may help to reproduce the results reported in this paper.\n# Weaknesses:\n- ### Major Comments:\n  - As far as I can tell, the novelty of this paper can be limited given the current manuscript. The quantization and distillation methods have been proposed in previous model compression works [1], and are even widely discussed in the DRL community [2,3,4]. The only difference between QPD and pioneering approaches is the AUXILIARY CRITIC LOSS. However, the ablation study on the AUXILIARY CRITIC LOSS seems missing. It remains unclear whether the performance improvements come from the dual-head architecture or the CRITIC LOSS. \n  - The authors elaborate on the benefits of \"Distillation\" $\\rightarrow$ \"PTQ\" $\\rightarrow$ \"QAT\". However, it seems that the authors use the same quantization strategy for \u201cPTQ\" and \"QAT\", which makes the Algorithm degrade into \"Distillation\" $\\rightarrow$ \"QAT\". From my point of view, the overall training pipeline of Algorithm 1 is not new. It is common knowledge to equip \"Distillation\" with \"Quantization\" simultaneously or successively.\n- ### Minor Issues:\n  - According to Figure 7, it is obvious that network quantization results in negligible performance drops but the effect of distillation is hard to be found. Though Figures 4-7 have illustrated that \"an auxiliary loss noticeably impacts the internal representation\", more quantitative analysis on the improvements over strong baseline methods are encouraged, e.g., the real-runtime speed, memory footprint, model size, training time cost, and average return. \n  - What if we further decrease the value of $\\lambda$? Besides, it would be better to report the selection of $\\tau$ in this paper.\n  - I failed to find the descriptions of the experiment settings, such as the dataset, metrics, etc. Did I miss anything? \n  - It seems that the authors conduct all experiments under the same circumstance. I expect more results reported on popular benchmarks with mainstream DRL methods. \n  - \"Our experiments show that these combined methods can effectively compress a policy network down to 0.5% of its original size, without any loss in performance\" with a $200\\times$ compression ratio seems inconsistent with Table 3.\n  - It would be better to include a brief introduction to the \"Atari Breakout environment\" in Figure 1. \n\n# Reference:\n- [1] QKD: quantization-aware knowledge distillation. arXiv2019\n- [2] Policy distillation. ICLR2016\n- [3] Distillation strategies for proximal policy optimization. arXiv2019\n- [4] Quantized reinforcement learning (QUARL). arXiv2019",
            "clarity,_quality,_novelty_and_reproducibility": "Since the proposed methods are rather simple, the clarity and reproducibility seem ok. However, the quality and novelty are my major concerns. From the view of model compression, the real contribution of this paper can be quite limited. The technique used in the paper has been common knowledge in network quantization and distillation (i.e., Section 4.1.1 and 4.2.2). Though the AUXILIARY CRITIC LOSS seems interesting, the experiments are too weak to support the contribution of this paper. I am not sure whether the proposed scheme could be generally effective in DRL.",
            "summary_of_the_review": "The main track may not be the best venue for the current manuscript. The authors are encouraged to solve the weaknesses and re-submit this paper to the following conference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4535/Reviewer_zGhX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4535/Reviewer_zGhX"
        ]
    },
    {
        "id": "DQq_MTMD3S",
        "original": null,
        "number": 2,
        "cdate": 1666667551214,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667551214,
        "tmdate": 1666667551214,
        "tddate": null,
        "forum": "UpyXmNMdQEn",
        "replyto": "UpyXmNMdQEn",
        "invitation": "ICLR.cc/2023/Conference/Paper4535/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This submission proposed to generated a compact and quantized version for actor-critic type reinforcement learning model. Basically, it first distilled a full-precision student model by introducing an auxiliary loss for critic model. Then the learnt model is served as initializer for Quantization-Aware Training (QAT).",
            "strength_and_weaknesses": "Weaknesses:\n1. The distillation is decoupled with QAT process: they are conducted in a pipe-line fashion. Though author claimed that the submission is a smoother transition, a more valuable work would be solving the instability of `Quantization-aware Distillation'. Directly applying distillation-quantization makes the work less innovative.\n2. Quantization is decoupled with reinforcement learning: the quantization phrase is more like an application of quantization (that works in other domains such as computer vision and neural language processing) to reinforcement learning. DoReFa and STE does not consider any properties that inherits in actor-critic models.\n\nStrength:\n1. The distillation method and analysis somehow enlight reinforcement learning understanding. Author may pay more attention to distillation part, instead of adding quantization if it is decoupled with reinforcement learning.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Overall, it is a litte hard to follow as too many analysis is accompanied with the method description.\n2. Novlety is limited in the application of quantization.",
            "summary_of_the_review": "Basically, author should 1) pay more attention to distillation, including analysis. 2) Get rid of quantization if it is just a plugin for the method. 3) make progress in integration of distillation and quantization in reinforcement learning scenario.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4535/Reviewer_A7VX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4535/Reviewer_A7VX"
        ]
    },
    {
        "id": "66W3K9GSgc",
        "original": null,
        "number": 3,
        "cdate": 1666759345785,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666759345785,
        "tmdate": 1666759345785,
        "tddate": null,
        "forum": "UpyXmNMdQEn",
        "replyto": "UpyXmNMdQEn",
        "invitation": "ICLR.cc/2023/Conference/Paper4535/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a new loss function that combines KL loss and Huber loss and can be used in actor-critic-based teacher networks.\nThey designed a 3-phases algorithm to distill knowledge from a full-precision teacher network to a low-bit student network.\nThey discussed the influence of the choice of teacher algorithm.",
            "strength_and_weaknesses": "Strengths:\n1. A loss function is proposed to distill a dual-head network, which is suitable for actor-critic-based teacher networks.\n\nWeaknesses:\nMajor\n1. This paper claims they proposed a new Distillation loss and detail it in Section 4.1. \nHowever, instead of using this proposed loss, the proposed QPD algorithm only uses the traditional distillation loss.\nDo you have any reason for this?\n\n2. The proposed loss simply combines two existing loss functions, and the novelty is limited.\n\n3. The experiments can not evaluate the effectiveness of the proposed methodology.\n\n4. The writing should be improved since some sentences and tables are confusing.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is poor in clarity, quality, and novelty. \nI can not foresee the reproducibility of this paper.",
            "summary_of_the_review": "This paper proposed a new loss function that can be used in the dual-head network.\nBut the writing is poor, and the novelty is limited.\nSo I recommend rejecting this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4535/Reviewer_s1rF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4535/Reviewer_s1rF"
        ]
    }
]