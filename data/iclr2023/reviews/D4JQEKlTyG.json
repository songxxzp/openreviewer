[
    {
        "id": "whn1uh9hN8g",
        "original": null,
        "number": 1,
        "cdate": 1666432559078,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666432559078,
        "tmdate": 1670838293099,
        "tddate": null,
        "forum": "D4JQEKlTyG",
        "replyto": "D4JQEKlTyG",
        "invitation": "ICLR.cc/2023/Conference/Paper164/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThe paper studies \"individual differential privacy\", i.e., given a specific example x, to what extent can one determine the membership (in a dataset D) of that example x given the output of a given algorithm?\n\n",
            "strength_and_weaknesses": "\nThe idea is interesting, it allows to evaluate to what extent an individual suffered privacy loss due to the publication of a learnt model.\n\nThe paper could be made clearer (avoiding ambiguity among others by using symbols in addition to text when confusion is possible) and more self-contained (reviewing briefly already existing concepts).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n\nFigure 2 is not very surprising given that the groups are created based on gradient norm at some point.  It may be more interesting to study the difference in gradient norm across groups constructed in advance not considering any gradient norm.\n\nThe steps 3 and 4 in Algorithm 1 are unclear to me, e.g., how exactly does rounding happen, what does \"compute RDP\" in line 4 mean exactly, where are the results of the \"compute RDP\" operation in line 4 stored (and where are they used later on), ... ?  The same holds for the explanation in Section 3.3 of the rounding.  It may help to define variables rather than vaguely describe concepts in words such as \"different privacy costs\" (the privacy costs of what exactly is meant here?)",
            "summary_of_the_review": "\nThe paper is interesting and with some more effort can be made sufficiently clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper164/Reviewer_9tdY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper164/Reviewer_9tdY"
        ]
    },
    {
        "id": "uy-dBoukIQ",
        "original": null,
        "number": 2,
        "cdate": 1666619995300,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619995300,
        "tmdate": 1666670819748,
        "tddate": null,
        "forum": "D4JQEKlTyG",
        "replyto": "D4JQEKlTyG",
        "invitation": "ICLR.cc/2023/Conference/Paper164/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents an algorithm to accurately account for individual differential privacy guarantees in DPSGD. ",
            "strength_and_weaknesses": "Strength: Compared with worst-case DP guarantees, the individual DP guarantee of a sample allows us to better understand the privacy guarantee of a specific data sample, and its impact over model parameters.\n\nWeaknesses: \n\nW1. The proposed algorithm does not correctly estimate individual-level privacy as in Definitions 1 and 2. Hence, the claimed contribution is not justified and the impact of this paper is questionable. \n\n(1) The proposed algorithm modifies the underlying DPSGD algorithm for training. In particular, the algorithm modifies clipping norms for individuals, whereas the original DPSGD clips all gradient samples using the same constant. This change would affect the privacy analysis in DPSGD, and hence, it is unclear whether the algorithm could still correctly estimate the individual privacy guarantee. \n\n(2) The authors claim that their techniques are used for the output-specific individual privacy of DPSGD. Hence, we expect the algorithm to take the output of DPSGD as the input, which is the noisy gradient sum.  However, in the proposed algorithm, only the DP estimates for individual clipping norms (which, again, is not a component of the original DPSGD) are taken into account. It is unclear how the noisy gradient sums in multiple iterations, as the ``trajectory`` of output, impact the output of the algorithm. \n\n(3) No ground truth about individual privacy is provided. Hence, it is confusing how the paper evaluates the accuracy/correctness of the proposed algorithm. \n\nW2. The proposed techniques are straightforward.\n\n(1) The claim that subsampling in DPSGD complicates the privacy analysis is not well justified. The worst-case privacy analysis of DPSGD already incorporates subsampling, as shown in the following papers: \"R\u00e9nyi Differential Privacy of the Sampled Gaussian Mechanism\" by Zhang et al., and \"Subsampled R\u00e9nyi Differential Privacy and Analytical Moments Accountant\" by Wang et al. Thus, it is why subsampling brings a challenge in the analysis of individual DP.\n\n(2) Overall, the algorithm simply computes the R\u00e9nyi divergence based on different clipping norms, which is merely an application of existing analysis.\n\n(3) The computation cost of N (which is the size of the private dataset) does not seem to be a problem for individual privacy accounting by definition, since otherwise you would miss some data samples and the accounting is not individual-level anymore (instead it is group level).\n\n(4) The technique for reducing the computation cost is to simply group data samples by their estimates of clipping norms at each iteration. This technique is straightforward. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the paper are low. \n\n\n",
            "summary_of_the_review": "This paper provides an algorithm for individual privacy analysis in DPSGD. However, the proposed algorithm does not correctly analyze the individual privacy as defined in their paper. In addition, the techniques proposed are straightforward. Furthermore, no ground truth is given.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper164/Reviewer_JYd4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper164/Reviewer_JYd4"
        ]
    },
    {
        "id": "NKd8qRVDru",
        "original": null,
        "number": 3,
        "cdate": 1666669068485,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669068485,
        "tmdate": 1670838877794,
        "tddate": null,
        "forum": "D4JQEKlTyG",
        "replyto": "D4JQEKlTyG",
        "invitation": "ICLR.cc/2023/Conference/Paper164/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors provide an efficient algorithm for individual privacy accounting when using DP-SGD. By checking the individual privacy parameters, the authors find that these parameters are highly correlated to individual training loss. The authors also verify the validity of their individual privacy parameters by the results of membership inference attacks.",
            "strength_and_weaknesses": "Major strengths:\n1. This is the first result showing the correlation between individual training loss and individual privacy parameters. Although this idea is natural if we consider the influence of the data points in the tail of the data distribution on both loss and privacy guarantee, it is great to see such a detailed experimental analysis.\n\nMinor strengths:\n1. This paper is easy to follow.\n2. The visualization of the experimental results is good.\n\nMajor weaknesses:\n1. I see that the authors are aware of the work by Feldman and Zenic (2021) on the Renyi Filter. It is not explained in this paper why they choose to use the concept by Redberg and Wang (2021) instead. (The authors are not giving enough acknowledgment to Redberg and Wang (2021) in their Definitions 1 and 2.)\n2. There is no theoretical result on the underlying reason behind this correlation between training loss and privacy parameters.\n3. In Section 6, the correlation between the attack success rate and the average $\\varepsilon$ is from the non-private model. This does not support the relationship between the individual privacy parameters and the actual privacy risk. This part of the experiment needs to be removed or replaced.\n\nMinor weaknesses:\n1. In Figure 4, I do not understand why the Pearson correlation coefficient is not calculated with the original points but with the fitted curve.\n2. Algorithm 1 needs to be revised. I think $q$ should be $p$ which is the sampling ratio for each batch. The authors have not defined $I_j$, and I guess $m$ is the batch size.\n3. In Section 3.4, the authors propose to release $\\varepsilon_i$ to the owner of the $i$th example. I think there needs a theoretical proof of why this does not incur additional privacy loss. I think the calculation of $\\varepsilon_i$ depends on the differentially private model which depends on the other examples. Therefore, we should be careful about what has been released and how much of the privacy budget has been used up.\n4. The usage of individual clipping is discussed in Section C but I do not think this is a common practice. If we do not adopt this individual clipping but only global clipping, will the result still show the correlation between loss and privacy as strong as it is now?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written. The statements in the paper are clear and concise. The novelty of this paper is good (see the strengths). I believe that the experimental results are reproducible (also see arXiv 2209.15596).",
            "summary_of_the_review": "This paper is novel in terms of finding a correlation between training loss and individual privacy. Although without any theoretical analysis of this phenomenon, this paper provides various experimental results to support their finding. \n\n---\n After discussion with Area Chair and other reviewers \n\n---\nIt would be great if the authors could explain directly how Definition 2 is used in considering the privacy guarantee for a group. Currently, there is a lack of evidence why Definition 2 and Theorem 3.1 is necessary for Section 5, which is the highlight of this paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper164/Reviewer_rPUG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper164/Reviewer_rPUG"
        ]
    }
]