[
    {
        "id": "kAwvL028BnP",
        "original": null,
        "number": 1,
        "cdate": 1666164009774,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666164009774,
        "tmdate": 1666164419579,
        "tddate": null,
        "forum": "pHMpgT5xWaE",
        "replyto": "pHMpgT5xWaE",
        "invitation": "ICLR.cc/2023/Conference/Paper4228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors propose a primal heuristic for solving MIP, following much of Neural Diving strategy from Nair et al (2021). Instead of fixing predicted variables from ML model, authors propose to solve a modified MIP problem which constrains the solution space to be the L1 ball around the predicted solution. Authors demonstrate across two datasets from ML4CO competition and two synthetic datasets from Ecole package that the proposed method works better than the baseline of fixing variables from predicted solution.",
            "strength_and_weaknesses": "The main strengths are the simplicity of the approach and the practical appeal. It is conceivable predicted variables from ML model is often not feasible, hence making only a \"soft\" commitment and searching in an L1 ball seems like a practically useful solution. The method is also conceptually simple and intuitive.\n\nThe main weakness of this paper is that the paper builds upon Neural Diving of Nair et al (2021) but authors don't compare against Neural Diving. While Neural Diving uses SelectiveNet to identify variables to fix, authors simply rely on confidence scores. Hence it is unclear whether the proposed predict-and-search (PS) method is superior to fixing strategy, or this is just the consequence of using a naive method for uncertainty quantification. I understand an open-source implementation of Neural Diving is not available (as far as I know), but given high similarity of the proposed method and Neural Diving, I still think authors should've implemented SelectiveNet (the only component that seems to be missing in the current implementation) and compare against full Neural Diving algorithm from Nair et al.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The basic idea of the paper is easily understandable. Authors do a reasonable job describing the intuition behind their work. However, authors make a few mistakes in mathematical notations and description of experiments, which poses a moderate difficulty clearly understanding the paper.\n\nQuality/Novelty: The proposed method is a relatively simple refinement of Neural Diving. However, the strategy of using L1-constrained optimization instead of fixing variables seems like a practically useful & broadly applicable idea. Hence this strategy might indeed make a good impact if its empirical advantage against Neural Diving is well-established. As mentioned in Strengths & Weaknesses, however, authors don't compare against full-fledged Neural Diving with SelectiveNet, and thus it is a bit difficult to evaluate whether the proposed method actually has an empirical advantage.\n\nReproducibility: I don't see source code, generated datasets, or scripts being shared in supplementary material. Hence reproducing this paper will take a considerable effort re-implementing the training code as well as integration with SCIP/Gurobi. \n\nA few detailed notes on clarity:\n\nSection 3.1.2. Equation (7). It's worth mentioning this factorization is also proposed from Nair et al.\n\nIn equation (8), shouldn't this be normalized by $|L_i|$ to be a value between 0 and 1?\n\nUnder equation 11, $||\\hat{x}(I) - \\hat{x}(I)||$ seems to be a typo of $||\\hat{x}(I) - x(I)||$.\n\nAs authors themselves mention, proposition 1 is quite obvious. Usually such obvious fact should just be mentioned in the narrative rather than dedicating a proposition.\n\ngap_p in Table 1 doesn't seem to be explained. It seems like the percentage of the improvement of the gap.\n\nIn Section 5.1, I wonder how $(k_0, k_1)$ and $\\Delta$ were chosen in these experiments? \n\nIn Section 5.2, $(k_0, k_1)$ values are repeated as (700, 0) four times. Seems like a mistake?",
            "summary_of_the_review": "While the paper heavily builds upon Neural Diving of Nair et al (2021), authors deviate from Neural Diving in their decision to not use SelectiveNet for uncertainty quantification. Hence, it is unclear whether the claimed improvement over the fixing strategy is the consequence of using a naive uncertainty quantification method, or the benefit of the proposed Predict-and-Search strategy.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Using ML models for primal heuristics might introduce some bias in MIP solutions, but the bias/fairness concerns of MIP problems are to my knowledge not very well known, so it is difficult for me to assess the consequence of the introduced bias.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4228/Reviewer_E5hv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4228/Reviewer_E5hv"
        ]
    },
    {
        "id": "mnghukuNSjo",
        "original": null,
        "number": 2,
        "cdate": 1666648944056,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648944056,
        "tmdate": 1666648944056,
        "tddate": null,
        "forum": "pHMpgT5xWaE",
        "replyto": "pHMpgT5xWaE",
        "invitation": "ICLR.cc/2023/Conference/Paper4228/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a heuristic for constructing good feasible solutions for families of related mixed-integer linear programming (MILP) problems. The heuristic proceeds by taking an instance from a given MILP family and 1) using a machine learning model to output weights (roughly: approximate solution values) for each decision variable, then 2) solve a sub-MILP in the (small) neighborhood around the point proposed by the predictive model. Computational results suggest that this heuristic can outperform the suite of heuristics in SCIP and Gurobi.",
            "strength_and_weaknesses": "The proposed method is a natural and reasonable upon ML-based heuristics that are based on variable fixing. While this extension is not particularly deep and uses \"off-the-shelf\" concepts, the computational results seem to indicate its utility. The paper is extremely well-written and does a great job introducing each of the concepts used to develop the algorithm.\n\nWhy did the authors not compare against the ML-based fixing algorithms that they reference in the introduction? That seems to be the fairest baseline for comparison, as these algorithms are the most similar in setting and approach to the one presented by the authors.\n\nI would appreciate more detail (in the appendix, but also some in the text) about the implementation details of the algorithm. For example, I am curious how the algorithm is implemented with SCIP and Gurobi -- presumably it is used via a heuristic callback with both solvers? If so, can the authors break out how much time is spent in the callback (and how many calls), and what fraction of the total allotted wall time this eats up?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "What is the \"pre-processing\" time required by the new algorithm: e.g., model training, but also the time required to select hyperparameters like Delta, k_0, and k_1?",
            "summary_of_the_review": "The paper is very well-written, and provides a natural but worthwhile extension of a recent stream of algorithms for producing heuristic solutions to families of related MILP instances. My concerns are mostly based around the baseline for comparison used in the computational section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4228/Reviewer_BsbB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4228/Reviewer_BsbB"
        ]
    },
    {
        "id": "TsgJ7f1njb",
        "original": null,
        "number": 3,
        "cdate": 1666668453711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668453711,
        "tmdate": 1666669083258,
        "tddate": null,
        "forum": "pHMpgT5xWaE",
        "replyto": "pHMpgT5xWaE",
        "invitation": "ICLR.cc/2023/Conference/Paper4228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a predict-and-search framework for solving mixed-integer linear programming (MILP) problems. Specifically, they first predict the solution distributions, and then search for near-optimal solutions within a trust region constructed from the prediction. Experiments demonstrate that the proposed framework improves the performances of two state-of-the-art optimization solvers SCIP and Gurobi on several datasets.",
            "strength_and_weaknesses": "Strengths:\n1. The proposed predict-and-search framework is novel.\n2. The trust region in the search algorithm is well designed.\n3. Experiments demonstrate that the proposed framework improves the performance of SCIP and Gurobi on several datasets.\n\nWeaknesses:\n1. The paper is hard to follow as the presentation of the proposed approach is unclear. For example, the definition of the so-called \u201cweighted conditional marginal probability distribution\u201d is confusing.\n2. The authors may want to add more baselines to demonstrate the effectiveness of the proposed approach. For example, as the predicting part of the method mainly follows [1], the authors should add [1] as a baseline.\n3. The authors may want to further discuss on the connections and differences between the proposed predict-and-search approach and existing predicting based (e.g., [1]) and searching based methods (e.g., [2]).\n4. The authors only consider discrete variables to be binary, which limits the applications of the proposed framework.\n5. In the 4th line in Section 5.2, why are the values of the size parameters the same?\n\nMinor Issue:\n1. The authors may want to use \u201c\\citep{}\u201d instead of \u201c\\cite{}\u201d for some of the citations.\n\n[1] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid von Glehn, Pawel Lichocki, Ivan Lobov, Brendan O\u2019Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja, Pengming Wang, Ravichandra Addanki, Tharindi Hapuarachchi, Thomas Keck, James Keeling, Pushmeet Kohli, Ira Ktena, Yujia Li, Oriol Vinyals, and Yori Zwols. Solving mixed integer programs using neural networks, 2020.\n\n[2] Li Z, Chen Q, Koltun V. Combinatorial optimization with graph convolutional networks and guided tree search[J]. Advances in neural information processing systems, 2018, 31.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The presentation is unclear.\n2. The predict-and-search framework is novel.\n3. The authors did not provide any materials to ensure the reproducibility.",
            "summary_of_the_review": "Overall, I believe that this paper is marginally below the acceptance threshold. Though the authors propose a novel predict-and-search framework for solving mixed-integer linear programming problem, they miss some important baselines to demonstrate the effectiveness of their proposed framework. Moreover, they may want to improve the writing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4228/Reviewer_DX2F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4228/Reviewer_DX2F"
        ]
    },
    {
        "id": "7Cn2yF-vzjV",
        "original": null,
        "number": 4,
        "cdate": 1666766190317,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666766190317,
        "tmdate": 1666766190317,
        "tddate": null,
        "forum": "pHMpgT5xWaE",
        "replyto": "pHMpgT5xWaE",
        "invitation": "ICLR.cc/2023/Conference/Paper4228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers a neural network based algorithm for solving MILPs. The main novelty lies in the proposed trust region algorithm to construct solutions from the output of the neural network.\n",
            "strength_and_weaknesses": "I think the paper considers an interesting problem, which is potentially of value to those working in the field of machine learning for combinatorial optimization. My specific comments and questions are as follows:\n\n1) I think a more detailed comparison with the neural diving method of Nair et al 2020 would be really interesting. The authors mention the work of Nair et al 2020, however, it is not clear how the performance of their approach would compare.\n\n2) In addition, I think it is interesting to consider how the performance of other neural network based MILP solver methods would compare against the proposed method.\n\n3) The algorithm to solve Eq 11 seems critical for the proposed framework. In my opinion, it would increase the readability to include this algorithm in the main body of the paper.\n\n4) Is the algorithm/solver for SOLVE(M\u2019) in Alg. 1 discussed somewhere in the paper?\n\n5) I think a simple yet interesting baseline to compare with the proposed method is by simply solving the relaxed LP and running the trust region algorithm on the solution of the relaxed LP.\n\n6) As far as I understand, the reported numerical results are for the MILP instances in the test set, not training or validation set. Is this correct? Also, is this explicitly stated somewhere?\n\n7) I think it would be nice to include a discussion on the neural network generalization. How does the performance on the training set compare to performance on the test set?\n\n8) In my opinion, the claim that \u201cMost of the end-to-end approaches directly predict solutions to MILP problems, ignoring feasibility requirements enforced by model constraints. As a result, the solutions provided by ML methods could potentially violate constraints.\u201d needs references to some works where this is the case. \n\nMinor issues:\n1) The font size for Fig.3 is too small.\n2) Typo in second sentence of the conclusion: supervise(d)-learning\n3) Typo in page 8: \u201care took into consideration\u201d\n4) Typo in page 7, equation (6) and (6)\n5) Extra parenthesis in the second line of the equation block for L(\\theta): logp_(\n6) Typo in the line under eq 11, \\hat{x}_I - \\hat{x}_I\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity could be improved in my opinion by focusing more of the presentation on what is new to this work such as the trust region algorithm.\n",
            "summary_of_the_review": "I think the paper is interesting, however comparison to other work seems could be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4228/Reviewer_jKg3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4228/Reviewer_jKg3"
        ]
    }
]