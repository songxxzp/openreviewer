[
    {
        "id": "-utWAAas2m",
        "original": null,
        "number": 1,
        "cdate": 1666536858522,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666536858522,
        "tmdate": 1666537035127,
        "tddate": null,
        "forum": "aOKs_OFS9g",
        "replyto": "aOKs_OFS9g",
        "invitation": "ICLR.cc/2023/Conference/Paper1093/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the risk measure instead of the classically expected return in the offline RL setting. This paper shows that under some assumptions it is equivalent to minimize a risk measure in the latent space and in the natural space. A practical approach is proposed to use CVaR as the objective function.",
            "strength_and_weaknesses": "Strengths: easy to follow, good performance\nWeaknesses: Incremental idea, insufficient experiments",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: good\nNovelty: poor. the idea is incremental\nReproducibility: good",
            "summary_of_the_review": "The paper is easy to follow and combines distributional RL with offline RL. My concerns are:\n1. the idea is incremental and the performance is not surprising. The method simply combines CVaR and an RL method. There is no specific design for the offline setting. Since distributional RL methods like C51 can achieve better performance than classical RL methods, the performance shown in this paper is not surprising.\n2. For experiments, only one environment is used, which is not very convincing. More results should be provided.\n\ntypos: the equation above Eq. 3: s_i+1a_i -> s_i+1, a_i; a MDP -> an MDP; equation before Lemma 4.1.1: \\phi(s_0) -> \\phi(s_H)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1093/Reviewer_1ct4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1093/Reviewer_1ct4"
        ]
    },
    {
        "id": "9WUHedH4lxn",
        "original": null,
        "number": 2,
        "cdate": 1666582398873,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582398873,
        "tmdate": 1666885640004,
        "tddate": null,
        "forum": "aOKs_OFS9g",
        "replyto": "aOKs_OFS9g",
        "invitation": "ICLR.cc/2023/Conference/Paper1093/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to train a distributional actor-critic algorithm in a latent space for high-dimensional data in offline reinforcement learning. Risk measure instead of the classic expected return is optimized under the risk-sensitive RL framework. ",
            "strength_and_weaknesses": "Strengths: \n1. This paper combines the methods from prior methods. \n\nWeaknesses:\n1. This paper lacks novelty. The latent model for encoding high-dimensional states is borrowed from Rafailov et al. (2021), and the distributional Q-learning is originated from Ma et al. (2021).\n2. The writing is coarse, and many typos appear abruptly.  \n3. Theoretical analysis doesn\u2019t support the proposed algorithm due to unsatisfactory assumptions. \n4. Experiments are only conducted on customized walker datasets. \n\nRafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning from images with latent space models. In Learning for Dynamics and Control, pp. 1154\u20131168. PMLR, 2021.\n\nYecheng Ma, Dinesh Jayaraman, and Osbert Bastani. Conservative offline distributional reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is coarse and the originality is limited. Code is provided for reproducibility. ",
            "summary_of_the_review": "Overall, I am concerned that the originality and depth of this contribution may not quite yet reach the high bar set for ICLR. The writing errors make reading stumble. \n\nWriting errors: \n- Section 1, paragraph 2, \u201cit may be be more \u2026\u201d\n- Section 3, paragraph - Latent variable model, \u201cUsing this factorization, the evidence lower bound (ELBO) (Odaibo, 2019) and a really interesting theoretical approach (Levine, 2018), the following objective function for the latent variable model is derived \u2026 \u201d by any reference? \n- Section 3, paragraph - Latent variable model, \u201c \u2026 and D a decoder\u201d\n- Section 3, paragraph \u2013 Distributional RL, \u201cFor example, O-RAAC (Urp\u00b4\u0131 et al., 2021) and following a previous work (Fujimoto et al., 2019), decomposes the actor into two different component an imitation actor and a perturbation model.\u201d\n- Section 3, paragraph \u2013 Distributional RL, \u201c\u2026 , is introduced to prevent for too optimistic estimation\u201d\n- Section 4, starting paragraph, \u201cSince training with high dimensional space directly failed and following previous work (Rafailov et al., 2021; Lee et al., 2019), we encode our \u2026\u201d\n- \u2026\u2026\n\nTheoretical concern: the first two assumptions require the encoder $\\phi$ to be a one-to-one mapping function, while this is hard to be satisfied when using non-linear neural networks state encoders, for example, $\\phi$ occasionally maps both two states $s$ and $s\u2019$ to zero vectors.  This conflicts with the use of trained variational encoder in the algorithm part, so the theoretical analysis doesn\u2019t support the proposed algorithm. \n\nExperiments are not standard and only walker_walk task is considered. The dataset is collected by running SAC based on classic states provided by DeepMind Control suite. If the dataset is collected by policy which is trained not based on pixel input, why you trained your model on this dataset via pixel input. Limited number of baselines are considered here, and the paper didn\u2019t include implementation details of these baseline methods. \n\nIs LODAC-O a O-RAAC trained with your latent encoder? \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1093/Reviewer_eHQL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1093/Reviewer_eHQL"
        ]
    },
    {
        "id": "wQZP5G0gWX",
        "original": null,
        "number": 3,
        "cdate": 1666640876877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640876877,
        "tmdate": 1669825939076,
        "tddate": null,
        "forum": "aOKs_OFS9g",
        "replyto": "aOKs_OFS9g",
        "invitation": "ICLR.cc/2023/Conference/Paper1093/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approach for Offline RL by explicitly modeling risk through CVaR. The paper targets stochastic environments and makes the case that modeling risk is essential for  performing well in these settings by demonstrating improved performance in standard control tasks with noise injection in the rewards. ",
            "strength_and_weaknesses": "Strengths\n\n1. The proposed approach for risk-sensitive RL in a latent space is simple to implement, by combining with several prior offline RL algorithms.\n2. The motivation for necessity of modeling risk in stochastic environments is on point, and should be helpful in developing algorithms for these environments broadly. \n\nWeaknesses\n\n1. The practical implementation section is very vague and it is unclear how exactly the components of LODAC differ from prior approaches like Rafailov et al. (2021) More details regarding the algorithm is essential for better understanding the contributions\n\n2. The experimental settings do not conform to the motivations for risk-sensitive RL. The only stochasticity induced is a form of Bernoulli noise added to the rewards, and the dynamics are still deterministic. I think it is important to perform analyses on environments with stochastic dynamics to really understand the benefits of risk-sensitive algorithms.  \n\n3. The experimental comparisons are insufficient and are significantly minimal compared to evaluations that offline RL algorithms usually do. For example, only a single walker agent is considered for evaluation. I would suggest at least evaluating on the entire D4RL suite and comapring with other baselines like CQL, IQL, COMBO etc. ",
            "clarity,_quality,_novelty_and_reproducibility": "The motivations of the paper re interesting and important. However insufficient details are provided about the practical algorithm and experiments. ",
            "summary_of_the_review": "The paper has interesting insights and motivations, but the experimental comparisons are incomplete and missing external baselines, evaluation environments and experiments settings. Please refer to the Weaknesses section above. \n\n--- AFTER REBUTTAL ---\n\nAfter reading the authors' responses, I am keeping my score and not recommending accept for the paper because only my first concern is reasonably addressed, and I am not convinced by the justification for significantly minimal experimental evaluations. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1093/Reviewer_c8jd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1093/Reviewer_c8jd"
        ]
    },
    {
        "id": "mYP8r2uTxwD",
        "original": null,
        "number": 4,
        "cdate": 1666847050711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666847050711,
        "tmdate": 1666847050711,
        "tddate": null,
        "forum": "aOKs_OFS9g",
        "replyto": "aOKs_OFS9g",
        "invitation": "ICLR.cc/2023/Conference/Paper1093/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces LODAC, a variant offline actor critic method, which first embeds the state space into a latent space and also include risk measure into consideration. On the theory side, the authors show that minimizing risk measure in the latent space and in the natural space. On the empirical side, the authors consider an artificial environemnt (walker) which forces the deterministic reward function to be stochastic and justifies the efficacy of the proposed method. ",
            "strength_and_weaknesses": "Strength: \n - The equivalence between minimizing risk measure in the latent space and in the natural space seems interesting. \n\nWeaknesses:\n - Third paragraph in the intro: I don\u2019t think I buy the logic here. \u201cClassical\u201d RL methods can also be evaluated under \u201cstochastic\u201d environment since their goal is to maximize **expected** total reward. The importance of developing risk-sensitive methods is to consider risk while maximizing total reward, which should not be counted toward training policies under stochastic environment. \n - The paper seems to be a naive combination of risk-sensitive offline RL and latent offline RL. I did not find any novelty beyond these two points. I suggest the authors further classify this. \n - Experiment seems too simple. I believe walker environment itself is already a low-dim env (18, see (Tassa et al., 2018)). It seems there is no need to further embed into low dimension. I suggest the authors consider a more complex environment. \n - The authors should also compare the results with other SOTA offline RL methods: COMBO, CQL, etc. There should be a trade-off between reward and risk. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Yes. \n\nQuality: Yes. \n\nNovelty: little. \n\nReproducibility: I believe it should be fine. ",
            "summary_of_the_review": "The paper has an overall good presentation and some motivation. However, given the weaknesses stated above, I recommend reject. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1093/Reviewer_5Q9C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1093/Reviewer_5Q9C"
        ]
    }
]