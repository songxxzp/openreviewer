[
    {
        "id": "SI5pjnwb6-",
        "original": null,
        "number": 1,
        "cdate": 1666311827845,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666311827845,
        "tmdate": 1669226703149,
        "tddate": null,
        "forum": "UY5zS0OsK2e",
        "replyto": "UY5zS0OsK2e",
        "invitation": "ICLR.cc/2023/Conference/Paper2669/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a multiscale method for operator learning based on a transformer architecture. Inspired by hierarchical methods for solving PDEs, the key idea is to add self-attention layers to a architecture with multiple downscaling and upscaling layers (i.e., reduce and decompose operations). The authors also use a Sobolev norm to train the network, which they find improves the recovery of high-frequency signal content. The method is evaluated by learning solution operators for multiple different differential equations, and appears to outperform other recent methods.",
            "strength_and_weaknesses": "The method appears to outperform the baseline approaches for learning neural operators by a fair margin, and so may be of interest to the large community working on physics-informed neural networks and neural operators.\n\nI also find the appendix to be very helpful in supporting some of the arguments made in the main paper. For example, the elaboration on the connection to hierarchical matrix methods and additional implementation details.\n\nThe architecture and method are also fairly straightforward to understand.\n\nOn the flip side, since the main technical contribution of the paper seems to be the architecture, I was expecting to see more from the evaluation. For example, the following points.\n- There are no qualitative comparisons to the baselines; the only qualitative results (showing the predicted solutions) in the main paper are two output solutions of the elliptic equation from the proposed method.\n- While there is a discussion of computational complexity, it's not clear what the actual practical runtime and memory usage are. One of the main advantages of neural operators is that they usually offer significant improvements in runtime compared to conventional solvers (albeit at the cost of accuracy). In that respect, transformers are usually more computationally expensive than, e.g., CNN-based architectures, and so I think it's critical to include some analysis of the computational performance. \n- The authors mention the \"Galerkin Transformer\"  (Cao 2021) as another transformer-based architecture introduced in the context of operator learning. I wonder why this was not evaluated? How does that method compare to the proposed architecture?\n- There is no discussion/evaluation of some of the architectural choices. For example, how does the choice of window size affect the performance?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is mostly clear and probably reproducible, especially with the additional details provided in the appendices. As I mentioned previously, I have some reservations about the quality of the evaluation. With respect to the novelty of the approach, the main new idea appears to be the application of multiscale transformers to the domain of neural operators. Still, transformers have been considered for this application before (as the authors note), and the idea of multiscale transformers is also not new (see, e.g., Multiscale Vision Transformers by Fan et al. [2021]). \n\nThe readability of the paper could also be improved in some areas, and I provide a few minor comments below.\n- The references are not typeset correctly; author names are given in plain text rather than being enclosed in parenthesis.\n- Fig. 4.2: Use epochs on the x-axis for all plots for clarity (currently switches from y-axis to x-axis from the left to right panels).\n- Fig. 4.2: The plots should compare the HT-Net trained with H1 loss vs. HT-Net trained with L2 loss on the same axes. This allows direct comparison of the performance of the network trained with the different loss functions. In that case, the results from evaluating the models with the L2 metric could be used for one plot and the other plot would show evaluating the models with the H1 metric. \n",
            "summary_of_the_review": "Overall, I lean slightly negative on this paper. The results appear to outperform the baselines, but I'm curious why the recent transformer architecture for neural operators was not included in the baselines, and there seem to be some missing evaluation items: analysis of runtime/memory, ablation study of architectural decisions (since the architecture is the main contribution), and qualitative comparison to baseline approaches. Perhaps the authors can address some of these concerns in their response, though.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2669/Reviewer_oKvB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2669/Reviewer_oKvB"
        ]
    },
    {
        "id": "F0ufUNbhcf8",
        "original": null,
        "number": 2,
        "cdate": 1666634832340,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634832340,
        "tmdate": 1666634832340,
        "tddate": null,
        "forum": "UY5zS0OsK2e",
        "replyto": "UY5zS0OsK2e",
        "invitation": "ICLR.cc/2023/Conference/Paper2669/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The manuscripts an attention based multi-grid solution for PDEs where different scales are present in the solution. This is done by a hierarchical attention structure that is used to map low feature solutions to high feature solutions and back. The hierarchy is created with a network architecture that resembles U-Net of Ladder architectures, spiced with transformer layers. The manuscript describes a hierarchical discretisation that have features at multiple scales. Reduce operators go to coarser level and decompose operation does the opposite.\n\nA good choice as these have been shown to be able to handle multiple resolutions in an efficient manner using the transformer architecture.   The result are very encouraging and provide significant improvement over the state-of the -art. ",
            "strength_and_weaknesses": "The efficient hierarchical operator learning of the input-output mapping of parametric PDEs using attention. The emphasis on getting the high frequency components correctly is also required for performance.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is  clear enough, high quality content and has novelty.",
            "summary_of_the_review": "The attention seems to be last world in many high performance application.  I think that in this case really for being able to bring the prior art features to the fine grid solution  in the decompose operator.  It would be nice to know (or have discussion) what are the salient features that are in the coarse solution that enable a successful generation of the high resolution solution. it is definitely coming as a prior from the training data.  All in all an excellent paper definitely of interest to people finding methods to solve PDEs in efficient way",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2669/Reviewer_2aoA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2669/Reviewer_2aoA"
        ]
    },
    {
        "id": "nW0udWdAZq",
        "original": null,
        "number": 3,
        "cdate": 1666678369996,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678369996,
        "tmdate": 1666678369996,
        "tddate": null,
        "forum": "UY5zS0OsK2e",
        "replyto": "UY5zS0OsK2e",
        "invitation": "ICLR.cc/2023/Conference/Paper2669/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new hierarchical method to solve multiscale PDE problems of structured domains using attention-based temporal updates and hierarchical matrix reduce and decomposition. The method is evaluated on standard benchmark 2D multiscale PDE problems, showing promising results in comparison to existing work. \n",
            "strength_and_weaknesses": "The main strength of the paper is the novelty of the approach. To the best of my knowledge, this work is first to suggest a hierarchical neural method based on the hierarchical matrix approach. The machinery involved in devising such an approach is based on a large body of theory and practice. The application domain of multiscale PDE is challenging, and the evaluation is convincing.\n\nThe main weakness of the paper is its exposition. This is unfortunately a hard read, which probably limit the impact of the paper outside the sub-community of PDE and numerical methods savvies. On the one hand, compressing the necessary details to nine pages is definitely a challenging task. On the other hand, illustrations of key operations would definitely help the reader. For instance, the discussion related to hierarchical discretization could benefit from an illustration showing a specific case. A similar comment can be made regarding the reduce and decomposition actions. I would also consider to de-clutter the current notations. Perhaps instead of using index notations, you can switch the operators performing reduce/decomposition.\n\nAnother potential weakness is the evaluation setup. How do you choose hyper-parameters for the network? Is there a validation set? How many times every experiment is run? Are the results reported in e.g., Tab. 1 for the basline approaches reported elsewhere (say in Li et al. 2021) or these are new numerics you achieved? Essentially, all these questions aim toward understanding whether there is a mature benchmark for this problem domain. \n\nFinally, briefly discussing the limitations of the approach is needed. For instance, is it possible to use the method (probably not as is) for unstructured domains such as point clouds or general domains where natural neighborhood information is not avilable?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The exposition of the paper can be improved as suggested above. The work is of high quality in terms of methodology and rigor, and I believe it is novel. Reproducibility is probably highly unlikely without the code. \n",
            "summary_of_the_review": "The task considered in this paper of solving multiscale PDE problems is highly challenging and complex. The proposed method tackles the problem in a novel approach, suggesting to compute hierarchical features in a v-shape updates procedure, going over all different hierarchical levels. As mentiond above, this is unfortunately a tough read which may potentially limit the impact of the approach and its wider accpetance. This in itself does not mean the paper should not be published, but rather I encourage the authors to make the effort to improve their exposition.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2669/Reviewer_uEBd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2669/Reviewer_uEBd"
        ]
    },
    {
        "id": "MlWdmfR3I6q",
        "original": null,
        "number": 4,
        "cdate": 1667493907333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667493907333,
        "tmdate": 1667493907333,
        "tddate": null,
        "forum": "UY5zS0OsK2e",
        "replyto": "UY5zS0OsK2e",
        "invitation": "ICLR.cc/2023/Conference/Paper2669/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper uses a hierarchical attention based neural networ to learn the solution operator associated to multiscale PDEs. In addition, the second contribution is the sobolev type norm used as the loss function, giving more weight to the higher frequencies of the target. ",
            "strength_and_weaknesses": "I have mixed feelings about this paper. I have appreciated the connection made with hierarchical matrices. The experimental results are also interesting, assuming the experiments have been conducted correctly (see comments below). \n\n\nHowever, relation with previous work seems to have treated very poorly, which makes the novelty of the paper hard to evaluate. \n\nFor instance, the sentence \"The idea is reflected in Liu et al. (2021); Zhang et al. (2022) to a certain degree.\" is vague and differences should be motivated and made much more precise, given most of the code is adapted from Liu et al. (2021). \n\nFor the second contribution (H1 loss), there is no mention to previous work, even though Sobolev type loss have already been proposed to train neural networks, e.g. in [Yu].\n\nFor the experiments section: Testing on a common number of epochs is not a good practice: the number of epochs should not be the same for each model, but chosen using the validation set. \n\nAs a more adapted baseline, have you tried FNO, preserving more modes in the Fourier transform in order to learn high-frequency outputs?\n\nLastly, it is not clear for me why this architecture would correspond to a Neural Operator. Can the network take as input arbitrary data (do you take as input positional encodings?). Does it satisfy the condition of \"discretization invariance\" [Kovachki]? I would like a more careful discussion on this point. \n\n\n[Yu]: https://arxiv.org/pdf/2205.14300.pdf\n\n[Kovachki]: https://arxiv.org/pdf/2108.08481.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "* The clarity of the paper is okay for the most part, except for the \"Hierarchical Discretization\" paragraph which is particularly hard to follow with complicated notations associated to the indexes.\n\n* There are quite a bit of typos scattered throughout the paper.\n\n* I have appreciated that the authors have provided the code. ",
            "summary_of_the_review": "In the paper's current state, I would lean towards recommending rejection. However, I am willing to reconsider.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2669/Reviewer_Pg7t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2669/Reviewer_Pg7t"
        ]
    }
]