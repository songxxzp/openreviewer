[
    {
        "id": "QuqS3paJML",
        "original": null,
        "number": 1,
        "cdate": 1666642334251,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642334251,
        "tmdate": 1666642334251,
        "tddate": null,
        "forum": "XE0cIoi-sZ1",
        "replyto": "XE0cIoi-sZ1",
        "invitation": "ICLR.cc/2023/Conference/Paper2612/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of conducting contrastive learning on graphs and introduce a single-pass graph contrastive learning framework for both homophilic and heterophilic graphs. The authors also try to provide some theoretical analysis to demonstrate that the design is effective. The experimental results show that the proposed objective outperforms existing graph contrastive learning works.",
            "strength_and_weaknesses": "This paper is generally well organized, and the motivation of the proposed method is clearly described. The reviewer has the following concerns:\n\n(i) Originality: My first concern is the novelty, and the reviewer feels the technical contributions are quite limited. For theoretical contribution, the theoretical analysis provided by the paper is quite similar to previous works. For instance, Lemma 1 (Concentration Property of Aggregated Features) has been proposed by work [1]; the inner product with the spectral norm in Lemma 1 is just a trivial extension of [1]. Lemma 2  and Theorem 2 (contrastive loss is equivalent to the matrix factorization) have also been proposed by [2]. The concept of the augmentation graph has been proposed by  [2]. Moreover, the proposed objective function (Single-Pass Graph Contrastive Loss in Eq. (9)) is the same as the spectral contrastive loss proposed by previous work [2]. The contribution of this paper is over-claimed.\n\n(ii) Clarity. The design of the model is also unclear, and there is no clear connection between the theoretical results and the algorithm. For instance, the design of hard top-k positive and negative pairs and neighbor sampling are important but do not have theoretical justification. There are several assumptions on the node label/feature/neighborhood distributions. The non-linear operations in the GCN have also been dropped. Although theorem 1 is related to homophily and heterophily, why can the single-pass graph contrastive loss address the learning problem homophily in definition 1? What is the relation between Theorem 1 and single-pass graph contrastive loss? Current theorems can not prove the effectiveness of the proposed objective on homophily and heterophily graphs.\n\n(iii) Quality: The experimental results are also not sufficient. Lack of some important ablation studies and qualitative analysis. For instance, what are the effects of top-k positive sampling and surrounding T-hop neighbors on the performance? To gain more insight into the behavior of the unsupervised method, it would also be better to visualize the embedding space learned.  \n\n[1] Is Homophily a Necessity for Graph Neural Networks. ICLR 2022.\n[2] Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. NeurIPS 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above.",
            "summary_of_the_review": "Overall, this paper's high-level idea and the objective function have been proposed by previous work [2]. The clarity and evaluation of this paper should also be improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2612/Reviewer_5Ey9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2612/Reviewer_5Ey9"
        ]
    },
    {
        "id": "sI-guOYhF7k",
        "original": null,
        "number": 2,
        "cdate": 1666691072233,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691072233,
        "tmdate": 1666691072233,
        "tddate": null,
        "forum": "XE0cIoi-sZ1",
        "replyto": "XE0cIoi-sZ1",
        "invitation": "ICLR.cc/2023/Conference/Paper2612/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Summary and main concern. Is a dual-pass necessary for computing contrastive loss even when heterophily is taken into account? The concentration analysis and the factorization interpretation are very interesting in answering this question. A single passs contrastive learning is implemented. Many benchmarks are explored in the experimental section. \n\nThe key to this insight is to embed nodes from the same class close and nodes from different classes far apart. In this regard, aggregated features as normal variables less to bound the variance (concentration lemma). Interestingly the bounds are governed by the singular values of the weights and also their correlation. As a result, nodes with similar labels have a similar embedding, and a contrastive loss can emerge from distinguishing close nodes from far away (in the sense of adjacency powers or k-hops).  Close nodes lead to positive edges (pairs) with in turn lead to a transformed graph that bridges graph contrastive learning with matrix factorization. \n\nExperimental results are very impressive. The appendix is very informative and the code is released. \n\nSuggestion. Using the concentration property explore the minimization o the entropy (can be a nice regularizer since the concavity of entropy)",
            "strength_and_weaknesses": "* Strengths. The simplicity of single-pass contrastive learning with a nice theoretical background (concentration bounds). \n* Weaknesses. K-hops are not really scalable (suggest using other measures such as learnable commute times to avoid OOM). ",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity. Very easy to follow and direct (without any syntactic sugar). \n* Novelty. Theoretical underpinning and nice experimental results. \n* Reproducibility. Code released with novel datasets. ",
            "summary_of_the_review": "A nice paper, very direct and out-standing. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Ok",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2612/Reviewer_Lvdk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2612/Reviewer_Lvdk"
        ]
    },
    {
        "id": "f95G-WF8WP",
        "original": null,
        "number": 3,
        "cdate": 1667439862139,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667439862139,
        "tmdate": 1667439862139,
        "tddate": null,
        "forum": "XE0cIoi-sZ1",
        "replyto": "XE0cIoi-sZ1",
        "invitation": "ICLR.cc/2023/Conference/Paper2612/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a single-pass contrastive learning method for graph data, without the need of graph augmentation or encoder perturbation to generate contrastive samples. In particular, they select the positive samples based on the inner-product of the hidden representation for each central node. The method is quite simple. For evaluations on both homophilic and heterophilic graphs, the performance of the proposed method is sufficiently justified.",
            "strength_and_weaknesses": "Strengths:\n\n1. Directly exploring the inner-product of hidden representations as the similarity to select positive samples is able to handle the heterophilic case, which is more reasonable than typical methods such as DGI and GMI where the positive samples are constructed from the neighbors. \n\n2. While the proposed method is simple, the authors have tried to theoretically reveal the benefits of the proposed method. Although the conclusions from Lemma 1 to Theorem 1 are not that surprising, they somehow strictly explain the motivation of this paper. \n\n3. Experiments are fully carried out, with sufficient comparisons with many baselines and the ablations. Even in certain cases, the performance is not as good as SOTA, the validity of the proposed method and stability of the training process are mostly supported.\n\nWeaknesses:\n\n1. The assumption that each node is generated IID is not very reasonable. This makes the conclusion of Lemma 1 irrelated to the graph structure of the input data. In graph learning, people are more interested in how the distribution of different node is related to each other, and prefer that the nodes are sampled from a non-IID distribution. Otherwise, Lemma 1 is also applicable to the case when using MLP (instead of GNN) for node hidden representation, which is not desirable. When considering the goal of this paper, Lemma 1 is also less informative in telling how the node homophily affects the hidden representation.\n\n2. Lemma 2 and Theorem 1 are interesting, and make valuable results on how the contrastive loss in Eq. (5) is related to transformed graph, why performing contrastive learning helps node classification (Eq. 7), and how the edge homophily of the transformed graph affects the contrastive learning bound (Eq. 7). However, all these results are unable to justify the validity of selecting the positive nodes according to the inner product of hidden representation vectors, which is the central contribution of this paper. Lemma 2 and Theorem 1 hold regardless of how the transformed graph (the positive samples) is constructed. It is still unclear why the proposed theorems can support the benefit of the proposed single-pass contrastive learning against previous methods such as DGI and GMI. In other words, if we build the transformed graph by DGI or other sampling method, do Lemma 2 and Theorem 1 still hold? If so, what is the point they make?\n\nI am happy if the authors can comment if I have missed something important. \n\n\nQuestions:\n\n1. What is the theorem in (Ma et al. 2021)\n\n2. It is suggested to indicate that F_i is Z_i in Lemma 2. \n\n3. What is the relationship between Lemma 1 and other theorems?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is over-all well written.\n\nPlease see the above comments about the concerns on the contributions.",
            "summary_of_the_review": "The biggest concern about this paper is that the theoretical results seem unable to support the benefit of the proposed contrastive learning method against other previous approaches.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2612/Reviewer_yTDn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2612/Reviewer_yTDn"
        ]
    },
    {
        "id": "1deQtRIJnsC",
        "original": null,
        "number": 4,
        "cdate": 1667487652841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667487652841,
        "tmdate": 1667487652841,
        "tddate": null,
        "forum": "XE0cIoi-sZ1",
        "replyto": "XE0cIoi-sZ1",
        "invitation": "ICLR.cc/2023/Conference/Paper2612/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In summary, this paper provides one single-pass contrastive learning method which can work for both homophiles and heterophilous graphs. One single-pass contrastive loss is designed based on the concentration property, and its minimizer is equivalent to MF over the transformed graph. \n\n",
            "strength_and_weaknesses": "Pros:\n1. The proposed single-pass contrastive loss is interesting and effective for both two kinds of graphs.\n2. The model performance can be guaranteed under a set of assumptions.\n\n\nCons:\n\nCompared with the existing contrastive learning methods, the advantages of single-pass loss are unclear.\nBased on the assumptions mentioned in Section 4.1, the Concentration Property can be guaranteed for both two graphs. Then, what are the challenges to applying contrastive learning methods on the heterophilous graphs? And how the proposed single-pass loss solved this problem? What are the advantages? Besides, \nIn the experiments, it would be better to show the influence of $K_{pos}$ in Section 6.3.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The new idea in contrastive learning is provided, but it lacks detailed comparisons with the literature under the heterophily settings as mentioned before.",
            "summary_of_the_review": "It is interesting to designing the single-pass loss for contrastive learning, while it is not well motivated when applying them under the heterophily settings. Then, the contribution compared with the existing methods is limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2612/Reviewer_RpbG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2612/Reviewer_RpbG"
        ]
    }
]