[
    {
        "id": "ltglWBVI-x",
        "original": null,
        "number": 1,
        "cdate": 1665945597156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665945597156,
        "tmdate": 1665945597156,
        "tddate": null,
        "forum": "bLmSMXbqXr",
        "replyto": "bLmSMXbqXr",
        "invitation": "ICLR.cc/2023/Conference/Paper3939/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses task-specific diversity problem, which exhibits in policies with similar expected return (what the authors called quality). The task specificity is framed by a set of scalar functions of a trajectory defined by the user, which are called Behavior Descriptors (BDs). A BD captures some property of a given trajectory, which is preferred by the user, e.g., number of times that a self-driving agent shifts gear (say the less the better). The authors propose an estimator for each BD, based on which they compute the gradient of BD w.r.t the policy's parameters. This gradient is then used to augment standard policy grad in a tunable manner, the result of which is a class of algorithms, called population-based RL algs.",
            "strength_and_weaknesses": "Strengths:\n\n1. The main objective (to produce a set of diverse policies at multiple quality levels) seems useful and interesting to investigate. \n2. Formulation of the Quality-Similar Diversity (QSD) is novel and seems to achieve superior results. \n\nWeaknesses:\n\n1. The paper lacks some analysis of sensitivity as well as computational overhead. In particular, it is not clear how the rate with which $\\lambda_i$'s are changed will impact convergence and stability of the algorithm as a whole. Additionally, the choice of $f$ can have significant impact on wall-clock time and computational requirements. \n2. The exposition of concepts can be improved as it lacks accuracy at points, which causes confusion (see the comments below). ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is *relatively* clear and easy to follow. However, there are points, which pauses/baffles the reader (see the comments below). \n\nQuality: The submission provides both theoretical exposition of the problem and experimental evaluation of the proposed method. Broadly speaking, it meets the level of an accepted ICLR paper. \n\nReproducibility: The authors provide several hyper parameters and detailed general implementations. However, I do not see a reproducibility statement in the paper about code release. ",
            "summary_of_the_review": "- Definition of BD needs more explanation. Do you need some restrictions such as being bounded (or finite)? \n\n- First paragraph of page 3: what do you mean by a measure function? Do you mean a function satisfying to be non-negative, zero for the empty set, and countable additive? If yes, I would probably make it explicit and simply discuss what these properties mean in the case of $f$. If no, then the definition of $f$ does not require it to be non-negative. I am wondering if $f$ can also be negative, then the QSD score can be zero for some given M-partition even if all $\\mathcal{B}_i$ are strictly non-zero. This may make the definition counterintuitive. \n\n- Page 3, first paragraph: depending on the definition of $f$, its evaluation might become a computational bottleneck. Some explanation on computational complexity would be apropos. \n\n- Section 3, Lines 5-6: \u201cNote that both the user-specified BD and the quality of a policy are defined on a trajectory\u201d -> not correct according to your definition of quality. The return (not the quality) is defined for a trajectory. \n\n- Page 4, first paragraph: suggestion: for the BD advantage function, you can observe that its expectation over $\\pi_{\\theta}$ is zero, which simply follows from your definition of value functions. Hence, in practice, it might be quite beneficial to subtract the mean (taken over mini-batch) from BD advantage to make the implementation more robust. \n\n- Moreover, for computation of advantage functions, I strongly recommend the following rather than GAE:\n\u201cDirect Advantage Estimation\u201d, https://arxiv.org/abs/2109.06093\n\n- Section 3.2: For maximizing diversity, section 4 of the following reference also seems relevant:\nhttps://arxiv.org/abs/2110.01548 \n(They have an ensemble of Q networks, while you have a set of policy networks.)\n\n\nMinors:\n\n- Last paragraph of Introduction: the acronym PBT used before it is defined. \n\n- First paragraph of section 2: your reward function is indeed an expected reward as it only defines over state-action and not the full transition ($r(s,a) = \\mathbb{E}_{s\u2019} ~r(s,a,s\u2019)$).\n\n- First paragraph of section 2: $T$ is not defined. More importantly, it looks like that you are considering episodic MDPs (also evident from allowing $\\gamma=1$). If so, make it explicit. Moreover, it is not clear whether you have termination at $t=T$ or not. If yes, you need to also define terminal states (whose value is zero by definition). \n\n- Section 3, 3rd line -> I would say \u201cconventional\u201d rater than \u201ctraditional\u201d, since the reference is too recent to be a tradition. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3939/Reviewer_bTHx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3939/Reviewer_bTHx"
        ]
    },
    {
        "id": "x8gA4_mlTBE",
        "original": null,
        "number": 2,
        "cdate": 1666581498723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581498723,
        "tmdate": 1666581498723,
        "tddate": null,
        "forum": "bLmSMXbqXr",
        "replyto": "bLmSMXbqXr",
        "invitation": "ICLR.cc/2023/Conference/Paper3939/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the topic of diversity in reinforcement learning. The authors propose the quality similar diversity problem, whose goal is finding diverse policies within multiple quality levels. In particular, the paper considers task-specific diversity where the user defines certain behavior descriptors (BDs) that are a function of trajectories and capture the type of diversity the user prefers. Given this problem statement, the authors present methods for estimating BDs via the policy gradient theorem. The gradient is then combined with a population-based training via an adaptive diversity loss. This leads to an RL algorithm that optimizes the quality-similar population diversity of a set of policies. Empirical evaluations show improved performance over prior methods w.r.t. diversity of policies across multiple quality levels. ",
            "strength_and_weaknesses": "**Strengths**\n- The problem statement is interesting and distinct from diversity problems studied in prior work. In particular, this paper aims at finding a set of diverse policies in a hierarchy of quality levels. This is useful in certain AI applications such as games and appears related to curriculum learning tasks. The problem formulation in this paper can be of independent interest to other topics in RL as well.\n- Besides the quality-similar diversity problem statement, another technical contribution of this paper is noting that an exact gradient of the user-specified BD based on trajectories can be computed directly using the policy gradient theorem. Thus, one can obtain an unbiased sample-based approximation to the gradient and utilize techniques developed for policy gradient methods.\n- Guidelines for adaptive diversity loss and handling multiple quality levels are useful.\n- The paper presents a thorough review and categorization of prior diversity methods.\n- The paper is well-written and well-organized. The authors provide a solid motivation behind their work and discuss applications and intuition behind their design choices throughout the paper.\n- The empirical evaluations show strong performance compared to prior methods in both continuous and discrete tasks. \n\n**Weaknesses** \n\n- The setting considered in the paper requires multiple design choices, which brings the practicality of the method into question. Apart from designing rewards, one needs to design a set of diversity design functions that depend on trajectories (which include a large number of states and actions) and may be difficult to elicit desired behavior. Another design element is the diversity measure function (an example is given which is the determinant of a kernel matrix.)\n- In Section 3.2, the authors mention a relationship between large $\\lambda$, which determines the strength of diversity, and optimistic initial values used for exploration. This connection is unclear and the two seem rather distinct from my perspective.\n- Despite the authors discussing this in Section 4 and Appendix, It is still unclear to me whether particular choices of BD would favor certain algorithms. Since BDs can be technically anything, it would be good to also empirically compare the performance of algorithms given random BDs.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and the quality and novelty of technical contributions are good. Code is not included in the supplementary material. ",
            "summary_of_the_review": "The presented method appears to be useful, practical, simple, and well-motivated. The authors do a good job explaining their method, justifying their design choices, and providing intuition. There are no major weaknesses that would justify rejection. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3939/Reviewer_7pVo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3939/Reviewer_7pVo"
        ]
    },
    {
        "id": "lHVN9vZZXEo",
        "original": null,
        "number": 3,
        "cdate": 1666647147234,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647147234,
        "tmdate": 1666647147234,
        "tddate": null,
        "forum": "bLmSMXbqXr",
        "replyto": "bLmSMXbqXr",
        "invitation": "ICLR.cc/2023/Conference/Paper3939/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the question of policy diversity in RL, with the specific goal of optimizing the diversity of obtained policies _within_ different quality levels (as one would, for example, want different AI difficulty levels within video games). \n\nTo do so, the authors first propose a framework that describes this specific task, which they refer to as QSD (quality-similar diversity), with an associated QSD score. This score is meant to apply to diversity metrics built upon user-specified behavior descriptors (BDs), as opposed to diversity metrics that are learned as part of the optimization process. \n\nSecond, the authors show that one can obtain unbiased estimates of the gradient of such a user-specified BD with respect to the policy (and thus, an estimate of the diversity metric's gradient). \n\nFinally, the authors use population-based algorithms to optimize the QSD score, showing across a variety of continuous and discrete tasks that their proposed algorithm (QSD-PBT) outperforms a variety of RL algorithms with different approaches to diversity optimization.",
            "strength_and_weaknesses": "# Strengths\n- Clarity: this paper is very clearly written; the overall phrasing of the problem, literature review, and description of the methodology are all very clear.\n- The experiments verify the usefulness of the proposed approach, and compare to a wide range of baselines.\n- The main theoretical results on estimating the diversity gradient may be of independent interest.\n- The details in the final QSD-PBT algorithm are investigated with a corresponding ablation study which confirms the discussion in the main paper.\n\n# Weaknesses\n- The one section I found could benefit from some clarification is $\\S$ 3.3; I found the description of the mechanism by which the different $\\lambda_i$ are obtained somewhat unclear. Am I correct in understanding that for each stratification level, all $\\lambda_0$ are equal (set to a high value which presumably is not particularly influential)? How does this tie into the reference to $\\lambda_{\\infty, 1}$ and $\\lambda_{\\infty, 2}$?\n- One potential weakness is that all evaluations of QSD-PBT use the introduced QSD-score metric. I'd be interested in whether the authors considered other ways of combining the stratified diversity terms of (1) (for example, one could consider the worst diversity score across all intervals, although the resulting optimization difficulties may be sufficient to discard this out of hand). \n\n# Questions\n- Maybe I missed this result: do you report the highest quality discovered by each algorithm (regardless of diversity)? On a related note, how are the quality intervals defined for Figure 1: are they predetermined by the range of possible rewards, or are they chosen based on the highest reward over all approaches $R_{max}$?\n- Figure 1 shows that there is a trade-off between quality and diversity; however, this trade-off disappears in most of Figure 3. Do you know what this could be due to?",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed above, I found the paper to be overall quite clear and well written. The quality of the theoretical and experimental sections seems on par with ICLR expectations. Finally, the paper addresses an interesting and - to my knowledge - new sub-problem in RL. Although the QSD problem could benefit from a stronger motivation, the overall contribution is sufficiently novel. \n\nBased on my familiarity with the relevant literature, however, I am not certain of the novelty of the proposed algorithm.",
            "summary_of_the_review": "This paper addresses the problem of learning diverse policies of differing quality. This paper is well-written, with theoretical results that may be of independent interest, and a novel population-based algorithm that is experimentally verified across a variety of tasks. Although I have a few remaining questions about the experimental evaluation, this is overall an interesting work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3939/Reviewer_rdoE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3939/Reviewer_rdoE"
        ]
    }
]