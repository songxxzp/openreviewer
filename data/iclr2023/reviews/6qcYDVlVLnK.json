[
    {
        "id": "_cwRLzz40kk",
        "original": null,
        "number": 1,
        "cdate": 1665920814529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665920814529,
        "tmdate": 1670378019587,
        "tddate": null,
        "forum": "6qcYDVlVLnK",
        "replyto": "6qcYDVlVLnK",
        "invitation": "ICLR.cc/2023/Conference/Paper2804/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper discusses the memorization effect of deep neural networks and proposes a framework to mitigate the negative effects of memorizing wrong information when learning with noisy labels. The analyses focus on the tradeoff between estimation error and approximation error due to the model capacity. By two case studies, learning with a fixed encoder and learning with an unfixed encoder, the authors conclude that restricting the search space by fixing the encoder reduces the estimation error but possibly increases approximation errors. Based on the takeaways from the tradeoff analyses, the authors further propose a learning framework that tries to combine the beneficial parts of fixing the encoder (low estimation error) and not fixing the encoder (low approximation error). Specifically, they exploit the power of self-supervision that regularizes supervised learning with self-supervised features. The advantage of the proposed framework is also theoretically analyzed in a simplified case. They also run experiments on both the synthetic label noise and real-world label noise to demonstrate the effectiveness of their framework.",
            "strength_and_weaknesses": "**Strengths**\n1. The paper theoretically reveals how the noise rate affects traditional learning with noisy labels. Besides, the theoretical bounds are connected to the model capacity, which motivates the design of their framework.  \n\n2. The proposed framework is demonstrated to be effective from both the theoretical and experimental aspects. Specifically, the theoretical analyses show why and how the self-supervised features can help regularize the learning with noisy labels (weak supervision), which provides insights for understanding the benefit of self-supervised learning in making the model robust to noisy labels.\n\n**Weaknesses and Questions**\n1. The paper claims that \"early stopping will handle overfitting wrong labels at the cost of underfitting clean samples if not tuned properly\". However, the issue may be also not addressed by this paper. Perhaps, holistic regularization cannot avoid the underfitting issue well. \n\n2. It is not very new to exploit contrastive learning to handle noisy labels, e.g., [1] and [2]. More discussions about related work can be added. \n\n3. It seems that Figure 2 is not highly related to the proposed method of this paper. Besides, the power of reducing model capacities of three different paths is different. Could they always converge the same classifier?\n\n4. The analysis of the approximation error is not very solid. Besides, the trade-off analysis may contradict the double descent phenomenon in deep learning. How to address this concern?\n\n5. It is a bit hard to follow \"a larger $\\mathcal{C}$ will lead to smaller approximation error but at the cost of larger estimation error given large $N$\". It seems that the paper provides an error bound, but not the exact values. \n\n6. Assumption 2 is a bit to understand since different classes always have different memorization rates in training [3]. \n\n7. Theorem 2 is not highly related to the proposed method/framework. It seems that the theorem tells us the power of the Gaussian assumption in tackling noisy labels, which is similar to [4]. \n\n8. From Figure 5, subfigures (a)--(c) show the crossing points between fixed and unfixed encoders are different, i.e., the noise rates are 0.0, 0.4, and 0.7 for different datasets. The authors should provide some insights into this observation.\n\n9. The authors claim the performance of a fixed encoder depends on the pre-trained model. However, current experiments only focus on SimCLR. It is interesting to see whether the proposed framework works well with other self-supervised learning methods.\n\n10. It is interesting to see whether the disentangled representations would be beneficial.\n\n11. Minor comments: \n- The name of Section 3 could be revised. The memorization effect mainly claims that the deep network would first fit training data with clean labels and incorrect labels. The analysis and discussion are more related to the memorization and impacts of mislabeled data.\n- The citation format needs to be revised. Some ''\\cite{}'' need to be changed to ''\\citep{}''.  \n- It is interesting to show the effect of batch size on the effectiveness of the regularizer since the regularizer is based on self-supervised learning.\n----\n[1] Shikun Li, et al. Selective-Supervised Contrastive Learning with Noisy Labels. CVPR 2022.   \n[2] Diego Ortego, et al. Multi-objective Interpolation Training for Robustness to Label Noise. CVPR 2021.  \n[3] Yiwen Wang, et al. Symmetric Cross Entropy for Robust Learning with Noisy Labels. ICCV 2019.  \n[4] Kimin Lee, et al. Robust Inference via Generative Classifiers for Handling Noisy Labels. ICML 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity.** The clarity could be improved to further enhance this paper. See the above questions.\n\n**Quality.** The quality of this paper is great for me. \n\n**Novelty.** The conceptual novelty is a bit limited. The technical novelty is great for me. \n\n**Reproducibility.** The reproducibility is good. ",
            "summary_of_the_review": "This paper presents work in learning with noisy labels. Both theoretical analysis and experimental results can provide insights into the community. The reviewer affirms its worth. The responses to the mentioned concerns are expected. \n\n####Post rebuttal#### \n\nThe response is detailed and addresses my concerns. I hence raise my score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_wiWY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_wiWY"
        ]
    },
    {
        "id": "7aFJo5FDh4G",
        "original": null,
        "number": 2,
        "cdate": 1666512285625,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666512285625,
        "tmdate": 1670496375556,
        "tddate": null,
        "forum": "6qcYDVlVLnK",
        "replyto": "6qcYDVlVLnK",
        "invitation": "ICLR.cc/2023/Conference/Paper2804/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new approach to tackle the problem of learning with noisy labels from the perspective of representation regularization. In detail, the self-supervised learning (SSL) head and supervised noisy labels head (SL) after the feature extractor (encoder) are utilized to extract SSL features and SL features respectively, and then a regularization loss is proposed to minimize the distance between the SSL features and SL features.",
            "strength_and_weaknesses": "Strength:\nThe proposed framework is extendable and can be plugged into other robust loss functions to further improve performance. \n\nExtensive experiments validate the effectiveness of the proposed method, and theoretical analyses are provided to support the claim.\n\nWeakness: \nThe motivation of the method is hard to follow and not convincing. Although the authors hope to use representation regularizers to cut off some redundant function space without hurting the optima, the actual method proposed in the paper seems to be not related to what the author claims.  Also, what\u2019s the high-level motivation of the proposed method? How does the self-supervised branch work in the method?\n\nIn table 2 and table 3, the improvement of the proposed regularization loss is limited compared with previous methods.\n\nAre there any large real-world datasets with label noise other than toy datasets, such as CIAFR variants? If yes, what\u2019s the empirical performance of those datasets?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is not easy to follow, especially the motivation for the proposed method. I'm fine with each component of the proposed method is not new, such as self-supervised head, l1 and l2 regularization, but the motivation of combining them together is important to me.",
            "summary_of_the_review": "With the findings above, I currently give the paper a reject score.\n\nUpdate: I raised my score to 6 after discussing with AC and other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_DbNo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_DbNo"
        ]
    },
    {
        "id": "YY8K5GqjoBv",
        "original": null,
        "number": 3,
        "cdate": 1666559407048,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559407048,
        "tmdate": 1666559407048,
        "tddate": null,
        "forum": "6qcYDVlVLnK",
        "replyto": "6qcYDVlVLnK",
        "invitation": "ICLR.cc/2023/Conference/Paper2804/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries to use regularization between learned representations to study the problem of learning with noise features. The paper offers some theoretical discussions, and proposes a new methods, however, I do not see how the theory is connected with the proposed method other than a generic lesson learned on \"restricting the search space with regularization is a reasonable approach\", which is true, but probably we do not need an entire theoretical discussion for this lesson. ",
            "strength_and_weaknesses": "- strength\n    - the paper studies an interesting problem on learning with noise labels\n    - the paper is condensed with theoretical discussions, might offer interesting results in a broader scope\n- weakness\n    - the discussion of the theoretical result does not seem rigorous enough (see below)\n    - the paper contains a lot of contents, some contents might be better to be moved to appendix. It's quite hard to parse all the theoretical information within the main manuscript. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity\n    - the paper contains a great amount of information, a little bit hard to fully appreciate all the contributions. \n    - how do Theorem 1 and Corollary 1 contribute to the remaining discussion of this paper other than offering some generic discussion in the insight section? \n    - observations 1-4 are more like hypotheses\n\n- Quality\n    - Theorem 1 seems to have some issues\n        - no definition for Ne, or maybe e. \n        - in the proof body of Theorem 1, the term Ne also suddenly appears with no noticeable context. \n    - Theorem 2 also has issues in its body\n        - why the body needs to specific an extra term N if N is infinity, and batchsize B, since neither of them appear in the theorm body\n        - no clear definition of \\Sigma, the only one I can find is in assumption 3, but both usages appear generic, thus no clear evidence saying they are referring to the same thing.\n        - Theorem 2 needs assumptions 1-3 should to be specified in its own body, not as free texts above.   \n\n- Novelty:\n   - learning with regularization over representations has been discussed elsewhere in the broader scope of noise and robust learning e.g., [1], but is probably novel in this setting, thus it should be fine. \n        - [1] Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation",
            "summary_of_the_review": "It's a nice and interesting work, but there are too many issues in the theoretical discussions, I think the work needs a deep and rigorous pass on the theoretical discussions before being published. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_L66U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_L66U"
        ]
    },
    {
        "id": "5plqp0246h",
        "original": null,
        "number": 4,
        "cdate": 1666731436545,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731436545,
        "tmdate": 1666731436545,
        "tddate": null,
        "forum": "6qcYDVlVLnK",
        "replyto": "6qcYDVlVLnK",
        "invitation": "ICLR.cc/2023/Conference/Paper2804/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes how the representation learned by a DNN and its downstream generalization is affected by noise in the training labels. The key insight in this paper is that the authors suggest to decouple the classifier into an encoder and a linear model. The encoder is fixed for a downstream task, while the 'linear' model component is fine-tuned. This insight comes from the fact that 'smaller' (in terms of VC dimension) hypothesis classes have more difficulty learning label noise. Consequently this can serve as a regularizer on the task. Given noisy label scenario, the paper gives an upper bound on the estimation error for a particular hypothesis class that depends on the amount of label noise (through a $(1 - \\epsilon^2)$ term where $\\epsilon$ is the noise parameter), and VC dimension of the model class. A version of this theorem for a fixed decoupled encoder+linear model setting  suggests that the fixed encoder setting leads to a reduction in the ability of the class to fit noisy labels at the expense of higher approximation errors. Overall, this  insight is instantiated on an SSL setting where a precise relationship is revealed between the quality of the representations and the error on noisy examples. The paper concludes with experiments to back up this claim.",
            "strength_and_weaknesses": "### Strengths\n- The key insight of this work is that constraining the representation learning portion of a classifier, to be fixed, helps regularize the ability of the classifier to memorize noisy labels. This insight is demonstrated effectively through the theorems and empirical analyses. \n\n\n### Weaknesses\n\n- While the theorems are helpful, generally, it is a bit hard for me to read too much into whether they should be meaningful. In the sense that we now know that the VC dimension term, for neural networks, might not encode as much information as we thought about the generalization behavior of the neural networks hypothesis classes.\n\n- Can the authors comment on how other performance properties of these trained models are affected by noise in labels? For example, should these insights also translate to something like the model's calibration?\n\n- The insights from section 5.1 are interesting and it might make sense to discuss these in the introduction. However, the results seem to suggest that one needs to one which noise regime that one is in. For example, for a task, in practice, I would have to know when the data has high symmetric label noise or how much bias to expect. These seem like challenging things to know ahead of time.\n\n- Some papers that might be useful to consider: Feldman et. al. (Does learning require memorization? a short tale about a long tail), and Li et. al. (How does a Neural Network's Architecture Impact its Robustness to Noisy Labels?)",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and states its contributions clearly. As far as I am aware, the idea to fix a model's encoder as a way to regularize its representations is novel.\n\nThe key improvement that this paper needs in its writing is in regards to the use of citep vs cite. There are several repeated instances where citep should've been used instead of cite e.g. line 9 of second paragraph of introduction",
            "summary_of_the_review": "This paper is well-written and clear. Overall, the paper provides an important insight about how to stem a model's reliance on  noisy labels using a fixed encoder and regularizing a model's representations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_kNp2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_kNp2"
        ]
    },
    {
        "id": "6FwvzRP-qU",
        "original": null,
        "number": 5,
        "cdate": 1667150654321,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667150654321,
        "tmdate": 1670352820671,
        "tddate": null,
        "forum": "6qcYDVlVLnK",
        "replyto": "6qcYDVlVLnK",
        "invitation": "ICLR.cc/2023/Conference/Paper2804/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work discusses how regularizing the representations of deep neural networks can help improve model generalization when the dataset is noisy. In particular, the authors first discuss how limiting the capacity of a neural network can help improve generalization in noisy data settings. This is achieved via the regularization of the representation space of the model. Regularization is performed by penalizing the distance between the representations from the projection head of SSL and SL features. Experimental results are demonstrated on various noise types and noise severities to suggest that the additional regularization objective can help improve the generalization performance of various noisy data training algorithms.",
            "strength_and_weaknesses": "## Strengths\n1. The key strength of this work is that it utilizes the advances in contrastive learning to regularize the representations in a supervised learning framework.\n2. The experimental observations complement some of the theoretical implications. (I was not able to appreciate the theoretical results and found them less intuitive, but I would like to see what other reviewers feel about it)\n3. The regularization method can be added on top of any existing method for training under label noise and results in consistent gains on the CIFAR10 dataset.\n4. I would have liked to see a larger portion of the paper devoted to the empirical analysis and ablations which is the strength of the work in my view.\n\n\n## Weaknesses and Questions:\n1. Theoretical Analysis: The theoretical analysis and assumptions seem to be very distant from the empirical approach. \n   (1) The notations are very confusing and non-succinct. \n   (2) The assumptions such as 0 variance, and infinite training data size are unrealistic. \n   (3) Connection between the theoretical results and the empirical approach is weak -- Gaussian features and discussion on the relevance of the theoretical analysis on the empirical observations.\n\n2. Figure 2 is very confusing till we read the first paragraph of page 5. Similarly, in Figure 1, what is the training setup? these things should ideally be self-contained in the captions.\n\n3. What is the problem setup in Figures 4 and 5? What is the noise level for asymmetric and instance-level noise? How many samples get down-sampled?\n\n4. Experimental Configurations: How was the value of $\\lambda$ selected? What was the pertaining dataset for the encoder? Which augmentations were used?\n\n4. Comparisons with state of art defenses such as ELR and SOP are missing. Refer to this paper for a list of more competitive baselines: https://proceedings.mlr.press/v162/liu22w.html\n\n5. It is suggested to fix the encoder for high symmetric label noise based on Figure 5. Are you doing this for the table numbers?\n\n6. How is observation 3 explained? What is its relevance? Can you provide some more information about the same?\n\n7. Results in Table 1 are only provided for the CIFAR10 dataset. This is not comprehensive enough.\n\n## Other Comments\n1. \\citet{} and \\citep{} have been used incorrectly throughout the paper. Please fix this.\n2. Figure 4 appears after Figure 5\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The use of contrastive learning for representation regularization in noisy dataset training is novel. \n2. The paper is not written very well. In particular, the empirical setting in various figures is hard to understand, and the notation in the theoretical section is too complicated and in-concise.",
            "summary_of_the_review": "The paper shows strong empirical gains in mislabeled data settings, but misses comparisons with state-of-the-art methods. I am unsure about the significance of the theoretical results.\n\nRaising score to 8 after author rebuttal",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_Y66C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2804/Reviewer_Y66C"
        ]
    }
]