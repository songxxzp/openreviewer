[
    {
        "id": "b5LldCIbjuA",
        "original": null,
        "number": 1,
        "cdate": 1666592216353,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592216353,
        "tmdate": 1666592216353,
        "tddate": null,
        "forum": "-qg8MQNrxZw",
        "replyto": "-qg8MQNrxZw",
        "invitation": "ICLR.cc/2023/Conference/Paper1110/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "this paper presents SeaFormer: a new transformer architecture designed to reduce the well-known complexity problem with the vanilla structure. The resulting architecture, O(HW), is applied in a semantic segmentation task for mobile devices. Experiments shows SOTA results in a latency/mIoU. An ablation study shows the impact of each proposed contribution An additional short experiment is proposed to show that this new transformer architecture is generic and can be used in a classification task.   ",
            "strength_and_weaknesses": "Strength:\n- The proposed transformer is O(WH), which breaks the original drawback of transformers. Even if many existing models already proposed modification in order to do similar optimisation, this paper combines an Axial attention branch (context branch) with a high resolution convolution branch (spatial branch). Both experiments and ablation study show that this is a relevant strategy. \n- Both experiments and ablation study are convincing for semantic segmentation. \n\nWeaknesses: \n- The main contribution seems to be the new transformer architecture combining context and spatial branches, resulting to a generic bloc that breaks the complexity. This contribution is mainly presented in the context of mobile segmentation application. I wonder if considering in a more generic way this contribution would implies more impact: 1) As you point with the small experiment on classification, this is very generic and can be used for classification, detection, segmentation (semantic, instances, ...); and 2) blocks are designed for mobile applications (MobileNet blocks) and could be extended to GPU systems with more classical operations without changing the O(WH) complexity. Extending experiments in this way would be interesting with may be more impact for the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear, of good quality, with contribution and associated code based on openmmlab is provided. ",
            "summary_of_the_review": "The main contribution of the paper is a new attention block with O(WH) complexity, which combines an axial attention branch with a high resolution convolutional block. This block is used into an semantic segmentation task designed for mobile devices. Experiments are convincing. As I pointed before, it should be interesting to extend experiments in order to show the genericity of the approach regarding other tasks and GPU systems. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1110/Reviewer_6wEd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1110/Reviewer_6wEd"
        ]
    },
    {
        "id": "SfvLKl7FTh",
        "original": null,
        "number": 2,
        "cdate": 1666614420561,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614420561,
        "tmdate": 1666614420561,
        "tddate": null,
        "forum": "-qg8MQNrxZw",
        "replyto": "-qg8MQNrxZw",
        "invitation": "ICLR.cc/2023/Conference/Paper1110/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a model called squeeze-enhanced Axial Transformer (SeaFormer) for mobile semantic segmentation. The model is based on the previous TopFormer with two-branch architecture and integrates axial attention and enhancement into an attention block. The proposed attention block reduced memory and time complexity a lot. The proposed model achieved encouraging performance on several semantic segmentation datasets, i.e., ADE20K, Pascal Context and COCO-stuff.",
            "strength_and_weaknesses": "Strength:\n+ The squeeze axial attention saves a lot of cost, enabling real-time performance.\n+ To overcome the loss of details in the global context extraction, there is a design for detail enhancement using depth-wise convolution to aggregate local spatial details.\n+ The experiments show gains from TopFormer.\n\nWeakness\n- The pipeline of the proposed method is most built on TopFormer, which also have dual branches of spatial feature pyramid and global semantics, and fusion blocks. The only difference is the axial attention and detail enhancement kernel. However, the axial attention is not novel, and was utilized in previous segmentation works, e.g., Axial-DeepLab, Max-DeepLab. What\u2019s the difference between the proposed axial attention and previous works? For the detail enhancement kernel, is it in the SeaFormer layer in context branch or spatial branch? The Fusion block in Figure 3 is the same as the one in Figure 2?\n- Although the proposed model have about 1% mIoU gain over TopFormer, it is not the state-of-the-art. It is not suitable to claim this method achieves the state-of-the-art performance.\n- A lot of citations are from arXiv, but some are already published.\n- For the results on Cityscape dataset in supplemental material, the latency seems higher than TopFormer with same backbone. Why not present results of the half resolution or more backbones for more comparison?\n- In table 3, it may be better to present the latency of components (although found in Supplemental material).\n- Better to separate Table 2, Table 4. No (a, b, c) annotation for Table 4. Table 4 is referenced before Table 3.\n- The number of parameters of the SeaFormer is about 1.2 ~1.6 times of TopFormer\u2019s. What are the additional cost?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality:\nThe paper is kind of easy to read, but the writing may be a little careless, such as citations, notation in Tables and figures.\n\nNovelty:\nThe novelty of this paper is not satisfactory, since both pipeline and proposed modules are not proposed at the first time. (Discussed in Weakness)\n\nReproducibility:\nThe paper provides code.\n",
            "summary_of_the_review": "This paper presents a model based on previous work with incremental achievement. The proposed axial attention and enhancement is not novel. The evaluation of the important datasets, cityscape, is not clear. Thus, I consider this paper may not be ready to be published in ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concern.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1110/Reviewer_wqtJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1110/Reviewer_wqtJ"
        ]
    },
    {
        "id": "dXa7X8xGvik",
        "original": null,
        "number": 3,
        "cdate": 1666662205526,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662205526,
        "tmdate": 1669237607827,
        "tddate": null,
        "forum": "-qg8MQNrxZw",
        "replyto": "-qg8MQNrxZw",
        "invitation": "ICLR.cc/2023/Conference/Paper1110/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presented a new mobile ViT architecture (SeaFormer) for semantic segmentation. The authors improved the computational cost of ViT architecture by proposing an efficient attention block, and light segmentation head. They showed solid results on ADE20K, Pascal Context, and COCO-stuff segmentation datasets.\n\nPost rebuttal:\n\nThanks for clarifying the cityscapes results, and ImageNet. My rating will stay the same. ",
            "strength_and_weaknesses": "Strength:\n\n* The paper presents a new architecture for mobile semantic segmentation using ViT. Such dense prediction task is very challenging, and usually requires global attention blocks in ViTs to model long range dependencies. The paper presented an efficient attention block utilizing axial attention to tackle segmentation task.\n\n* The paper presented solid numbers on 3 different segmentation datasets: ADE20K, Pascal Context, and COCO-Stuff. They also showed promising results on ImageNet for the image classification task.\n\nWeaknesses:\n\n* The paper proposed an efficient model for segmentation, but they didn't show any results on CityScapes to show the efficiency on dataset with large input size.\n* The paper forgot to compare to some of recent work (e.g. EfficientFormer [1]).\n* The paper listed a whole subsection (Shared STEM) in the method section which can imply it is a new work, instead it is based on previous work.\n\n[1] Li, Y., Yuan, G., Wen, Y., Hu, E., Evangelidis, G., Tulyakov, S., Wang, Y. and Ren, J., 2022. EfficientFormer: Vision Transformers at MobileNet Speed. arXiv preprint arXiv:2206.01191.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very clearly with enough empirical evidence to support their claims. The work is based on axial attention, but there is enough originality to the method from an architecture design perspective.",
            "summary_of_the_review": "The paper presented SeaFormer: a new mobile-ViT model for semantic segmentation. The authors proposed an efficient architecture that model spatial and global information utilizing convnets, and an efficient axial attention block. Empirically, they showed solid results on ADE20K, Pascal Context, and COCO-stuff. This work benefit the effort to push ViT based models to be more mobile friendly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1110/Reviewer_sRfJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1110/Reviewer_sRfJ"
        ]
    },
    {
        "id": "eWHVeqJzl6d",
        "original": null,
        "number": 4,
        "cdate": 1666691691545,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691691545,
        "tmdate": 1666691691545,
        "tddate": null,
        "forum": "-qg8MQNrxZw",
        "replyto": "-qg8MQNrxZw",
        "invitation": "ICLR.cc/2023/Conference/Paper1110/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper present an incremental improvement of axial attention. Instead of performing attention across the two axes for all pixels, feature maps are collapsed along the horizontal and vertical axes into two vectors. Self-attention is performed on these and the updated vectors can then be distributed across the original feature map at the input resolution. An additional detail enhancement block provides the original context of the feature map yielding the final output of a squeeze-enhance axial attention block. It is combined with a backbone (called the STEM) relying on MobileNet V2 blocks and some fusion modules which provide higher resolution information. The resulting SeaFormer architecture is evaluated for semantic segmentation and image classification, where there is a strong emphasis on the inference speed of the overall network, including when measured on mobile devices.",
            "strength_and_weaknesses": "Strengths:\n- The results show that the overall architecture is indeed somewhat better and faster in the evaluated settings, even when actually running on a mobile device.\n- The proposed architecture, specifically the new attention block does seem to have some novelty, however, it is fairly small incremental when comparing it to the axial attention.\n\nWeaknesses:\n- I'm mainly a bit surprised by some of the results. When looking at table 4, the clear take-away message is that the newly proposed attention is better than all other attentions. I'm assuming that the other attention mechanisms have been adjusted in such a way to keep the FLOP count roughly consistent, which could be a reasonable explanation why they don't outperform this fairly strong approximation of the vanilla global attention. But I find the investigation of this somewhat lacking. Does this now mean that we should start to use the new attention mechanism everywhere, by simply scaling it up to match the FLOP counts of the original global attention? Or is there some limit to this? Given that the main contribution of this paper is the squeeze-enhance axial attention, I would like to gain some more insights into why it seemingly works better than the global attention. However, I can't really gain these insights from the paper.\n- Most of the actually interesting results are in the supplementary and I don't understand why the authors chose to write the paper like this. The supplementary actually has results on cityscapes, the only dataset used for experiments with a proper private test set. Furthermore it contains a more detailed analysis of the runtime, which is one of the main motivations for the whole idea of the paper. And finally, it also contains results of experiments with the proposed attention block in a SWIN former. I think this experiment is crucial to show that the proposed novelty actually has some form of generality. However, none of these results are even mentioned in the main paper, I don't really understand why it would be written in this way.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly easy to follow, although the writing could be slightly improved in some parts. I think the experiment clearly highlight some interesting aspects, but I find it hard to get actual insights into the proposed attention variant. Since the proposed improvement is somewhat incremental and we don't gain interesting insights, the whole approach feels a little ad hoc and more like and engineering paper, especially since some of the more interesting experiments are pushed out to the supplementary for some reason. I do estimate that the results should be reproducible, given that the authors already now uploaded code to some anonymous repository, thus seemingly willing to release the code.",
            "summary_of_the_review": "There are some compelling experiments show that some interesting aspects of the proposed attention mechanism exist, however, I would like to actually get a better understanding why it apparently works so much better and what the limitations are. Overall I'm thus a bit torn when it comes to accepting the paper or rejecting it. However, depending on the other reviews and rebuttal, I'm willing to improve my rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1110/Reviewer_xU22"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1110/Reviewer_xU22"
        ]
    }
]