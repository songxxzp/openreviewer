[
    {
        "id": "4UNwJU7wGru",
        "original": null,
        "number": 1,
        "cdate": 1666399733105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666399733105,
        "tmdate": 1666805706841,
        "tddate": null,
        "forum": "7wk9PqiiW2D",
        "replyto": "7wk9PqiiW2D",
        "invitation": "ICLR.cc/2023/Conference/Paper3757/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The following work proposes a self-supervised representation for learning. Unlike prior methods, the proposed framework relies only on raw audio. Feature vectors are extracted based on a combination of NCCF, log F0, delta log F0, and average energy per frame. The resulting N-D feature vector is k-means clustered to achieve discrete codes. Self-supervised training objectives from HuBERT and SPANBert are then used to learn the final representation. The resulting features are then applied to downstream unsupervised TTS and emotion recognition.",
            "strength_and_weaknesses": "Strengths:\n- Fairly simple setup to achieve explicit incorporation of prosodic features\n- Demonstrated applicability in several downstream tasks\n\nWeaknesses:\n- While I have no doubt their setup works in practice, I do have some concerns regarding the K-means clustering.\n  - Not all frames contain valid F0 values. It's not clear how this is being handled, but constants and interpolated values will introduce devations from the true data distribution\n  - It's also not clear how to best combine these individual attributes. Scaling individual features can have drastic effects on which one dominates the clustering task.\n- I don't think there's an ablation on the SpanBert objective\n- Error bars missing in Table 1. Furthermore, MOS is not a good metric for comparing fine grained differences between various models compared to performing pairwise preference testing.\n- While there is no known good way to truly measure the quality of synthesized F0 and energy, I would be very careful about making claims backed up by a single qualitative comparison in Figure 4.\n- I'm not sure I follow the logic behind the claim that wave2vec, hubert, and waveLM filter out long-range prosodic information.\n- It's not fully clear how necessary the large transformer architecture with the self supervised objectives matter at all for some of the downstream tasks. I suppose an even simpler baseline would've been to individually quantize each prosodic feature into K bins using K-means (separately per feature). Then for each downstream task, pair each 1-hot encoding with its own embedding matrix to map to a lower dimensional representation. Alternatively, perhaps for the emotion recognition task, one could even just directly condition on the raw values like in Mellotron (Valle 2020), appending the user-normalized raw values with HuBERT features. \n\nQuestions for clarity:\n  - Can you clarify how EER is computed?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the technical portion of this work was fairly easy to follow. I generally trust that improvements have been made. I can certainly hear some improvements in the provided audio samples.\n\nI don't think the novelty of this work is very high. Algorithmically, vector quantization (k-means) has been explored and used for resynthesis in [1]. This work takes it a bit further by incorporating additional features such as log F0, energy, and NCCF.\n\n[1] Polyak et al.  Resynthesis from Discrete Disentangled Self-Supervised Representations. Interspeech 2021\n  ",
            "summary_of_the_review": "Overall, easy to read paper with some promising results. However, I think the novelty is somewhat low and the complexity of the solution (large transformers with self supervised learning objectives) might not be necessary for some of the downstream tasks demonstrated in this work. However, I'm certainly happy to change my review if the authors can demonstrate otherwise.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3757/Reviewer_CWFe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3757/Reviewer_CWFe"
        ]
    },
    {
        "id": "JCP1vvhGnV",
        "original": null,
        "number": 2,
        "cdate": 1666641478414,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641478414,
        "tmdate": 1669747173102,
        "tddate": null,
        "forum": "7wk9PqiiW2D",
        "replyto": "7wk9PqiiW2D",
        "invitation": "ICLR.cc/2023/Conference/Paper3757/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a BERT-based architecture for extracting prosody embeddings from speech. The pre-training of the model is performed following a self-supervised strategy similar to that of HuBERT but based on prosody features to get the cluster labels. The performance of the proposed approach is evaluated in an unsupervised Text-To-Speech Task proposed in previously published work.",
            "strength_and_weaknesses": "The paper exploits recent ideas in speech processing and text-to-speech technologies, to propose a new and promising idea to incorporate prosodic information into speech synthesis/generation tasks. The proposed approach is theoretical well supported, and the authors provided experiments and results with enough empirical evidence of the improvements achieved by their approach. Nice work.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear even though the complexity of the proposed approach is high. The novelty is clear, both in terms of the methods and performance improvement in unsupervised TTS. The proposed approach is not easily reproducible due to the inherent complexity of the proposed approach, the multiple stages of the pipeline, and the complexity of the task under study. The authors included a statement with a pledge to share checkpoint models and code with hyperparameters values and source code for training. Either way, the stage related to phone and word alignment could be better explained. ",
            "summary_of_the_review": "The paper proposed a BERT-based architecture and a self-supervised training strategy for prosodic-related embedding representation extraction from speech signals. The prosodic learned features are used in an unsupervised TTS task yielding a real improvement in terms of Mean Opinion Score and an ASR Intelligibility metric (Word Error Rate). The proposed approach is built on top of recent advances in speech processing and unsupervised TTS technologies and proposed new ideas to incorporate prosodic features to enhance the natural and expressive of synthetic speech.\n\nThis is a great work, congratulations to the authors.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3757/Reviewer_PaY1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3757/Reviewer_PaY1"
        ]
    },
    {
        "id": "NogiQC2wcq",
        "original": null,
        "number": 3,
        "cdate": 1666772786745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666772786745,
        "tmdate": 1666773379206,
        "tddate": null,
        "forum": "7wk9PqiiW2D",
        "replyto": "7wk9PqiiW2D",
        "invitation": "ICLR.cc/2023/Conference/Paper3757/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work explores utilizing self-supervised pretraining on speech data for improving prosody modeling in TTS and emotion recognization tasks. The proposed model, ProsodyBERT, is similar to HuBERT / SpanBERT, but with more input features added (f0, energy, NCCF). Experiments are conducted on two tasks: TTS (on DailyTalk + VCTK datasets) and emotion recognition (on IEMOCAP dataset).",
            "strength_and_weaknesses": "Strength:\n - The idea of this work is sound\n - The experimental results looks interesting\n\nWeaknesses\n - The novelty is limited.\n - Technical correctness issues, as explained in the next section.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n - Writing style needs improvement. For example, the correct use of `\\citep{}` and `\\citet{}`\n - It's not clear how \"6.1.2 EXPRESSIVENESS CONTROL\" is conducted.\n\nQuality: \n - I have concerns on the technical correctness. For examples:\n   - This work claims to learn prosody representation. However, key prosody components, e.g. pace / rhythm, are not concerned in the proposed model.\n   - The TTS experiment used FastSpeech 2 as one of the baseline, and claimed that used \"the official FastSpeech 2 checkpoint\". However, there is no such \"official FastSpeech 2 checkpoint\" ever released.\n   - Even if the authors used a non-official third-party FastSpeech 2 checkpoint, it's unlikely that the checkpoint was trained on the same dataset as used in this work (DailyTalk), which is not a popular TTS dataset. As such, the comparison in Table 1 is not fair at all.\n   - \"The most commonly used prosody representations include acoustic attributes like fundamental frequency (F0), energy, and duration.\" Please provide references.\n   - Fig 2 doesn't make well sense -- it looks like the TTS prediction only depends on z_speaker, z_align, z_prosody but nothing else. How about content?\n - I also have other concerns on the experiment setup. \n   - For the TTS experiment -- are the improvement from using extra features extracted by another model pre-trained with extra data, or from using extra features extracted by ProsodyBERT pre-trained with extra data? A comparison between UTTS + HuBERT and UTTS + ProsodyBERT would be required to show the improvement are from ProsodyBERT.\n \nNovelty:\n - Limited. The proposed model is similar to HuBERT / SpanBERT, except for adding more input features (f0, energy, NCCF).",
            "summary_of_the_review": "This paper tackle on an important problem (self-supervised prosody modeling). However, the contribution from the works is limited, and there are issues on the technical correctness.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3757/Reviewer_J32w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3757/Reviewer_J32w"
        ]
    },
    {
        "id": "R2HzU2rKDx-",
        "original": null,
        "number": 4,
        "cdate": 1667270911020,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667270911020,
        "tmdate": 1667270911020,
        "tddate": null,
        "forum": "7wk9PqiiW2D",
        "replyto": "7wk9PqiiW2D",
        "invitation": "ICLR.cc/2023/Conference/Paper3757/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces ProsodyBERT for learning prosody representations to strengthen the prosodic aspects in unsupervised text-to-speech synthesis (TTS). Similar to HuBERT (Hsu et al, 2021), ProsodyBERT is a masked encoder model trained to predict K-means cluster labels, except in this case the K-means is performed on domain-informed features (such as pitch, energy, and dynamics, etc) to capture the long-term temporal structure of prosody. These learned hidden features then become targets for training a prosody predictor, and then downstream a duration predictor. To evaluate the effectiveness of ProsodyBERT, the paper demonstrates expressive TTS, interpolation between speaker\u2019s prosodic styles, and how ProsodyBERT features improve emotion recognition in speech. \n",
            "strength_and_weaknesses": "Strengths\n\nThis paper provides a targeted and effective solution for modeling prosody, which is crucial to synthesizing expressive and natural speech. The overall design of the modeling, training and inference pipeline is well-thought-out, leveraging strong prior work as building blocks, such as HuBERT (Hsu et al, 2021) and SpanBERT (Joshi et al, 2020) for prosody representation learning and UTTS (Lian et al, 2022a,b) as the unsupervised text-to-speech synthesis, and demonstrates how ProsodyBERT representations can be added to strength prosody generation. The provided synthesis examples are compelling, and the improvement on the emotion recognition task shows that the choice of features and learned representations are useful.\n\nWeaknesses \n\nSince the main contribution of the paper is on learning prosody representations and predicting prosodic features for synthesis, it could be helpful to provide more grounding on what prosody is, and how it motivates subsequent design choices in modeling. Informally, prosody is both a general phenomenon, could also be a person\u2019s signature way of speaking. What is the dependency structure between style, speaker, prosody, duration? How does the paper choose its current ordering of conditioning and predicting them? In the appendix, zero-shot speaker synthesis was mentioned as a motivating factor for prosody features to be not dependent on speaker. Perhaps this is a design choice inherited from prior work that focuses on the zero-shot setup that is also well-suited for the current setting? \n\nFor comparison to prior work, it could be helpful to have a paragraph discussing how the baselines are chosen, what other models are considered but may not be as relevant, or practically not feasible to compare to. In what ways it may or may not be an apples-to-apples comparison. For example, what are the model sizes involved, which may be a result of the different design objectives of the prior models? \n\nSome examples seem to be missing from the demo page, for example \u201cemotion control\u201d examples, and there\u2019s only one example for \u201cexpressiveness control\u201d. For the \u201cexpressive TTS\u201d examples, would it be informative to include a few examples for each text to show the range of expressiveness that is possible. It seems all the text sentences are from the original dataset. How does the approach perform on unseen text?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThe paper is overall well-written. As it builds on a lot of prior work, at times it becomes understandably hard to be self-contained, and some components of the modeling are only mentioned in passing. For example, what is \u201cstyle\u201d, in addition to speaker? What is z_align? At times, it took a while to figure out what was proposed in prior work and what was new. For Figure 2, would it be possible to visually distinguish which modules are new and which modules and pipeline are building on prior work. The current dotted-lined box in Figure 2(a) is helpful. Is Figure 2(c) a zoomed in illustration of the \u201cshared predictor\u201d module in Figure 2(b)? \n\nClarification questions on the K-means procedure: What is the dimensionality of the extracted features, and how are they weighted when performing K-means? Was there a need to explore different weightings or dimensionality reduction? How are the interdependencies between the different features accounted for? What is the actual dimensionality that the K-means was performed in? \n\nQuality\n\nThe proposed approach is well-thought-out. The experiments include both quantitative, qualitative and subjective evaluations. Figures such as Figure 4 helps the reader understand the differences in what the approaches are able to capture, which is very helpful. More discussion on comparison to prior work and samples could strengthen the quality of the work.\n\nNovelty \n\nIn contrast to prior work that used mostly pitch and energy when considering prosody, this paper proposes to use more expressive domain-informed features to better capture prosody, and to model it using a HuBERT-like approach, making it possible to learn general prosody from large amounts of (unpaired) speech audio.\n\nReproducibility \n\nThe description of ProsodyBERT is detailed, and the TTS pipeline builds on prior published work. The code for ProsodyBERT is included in the supplementary material, and it was mentioned in the paper that the checkpoint will be released at a later point. \n",
            "summary_of_the_review": "The paper brings to attention the importance of prosody, and provides an effective approach to modeling it and adding it to unsupervised TTS systems. The approach is sound, and the results sound compelling. More discussion on the evaluation and comparison to prior work could help position this work and its contributions. A more complete demo page would also strengthen the paper\u2019s contributions. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3757/Reviewer_925W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3757/Reviewer_925W"
        ]
    }
]