[
    {
        "id": "rJH5gZcn0N",
        "original": null,
        "number": 1,
        "cdate": 1666003789162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666003789162,
        "tmdate": 1668689422080,
        "tddate": null,
        "forum": "5Egggz1q575",
        "replyto": "5Egggz1q575",
        "invitation": "ICLR.cc/2023/Conference/Paper4083/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, authors try to explain decision making of an RL agent from another perspective, and that is to highlight which trajectories in the training data are salient for the taken action. That is similar to prior saliency methods except that instead of highlighting input features, in this paper the trajectories from the training data are getting salient.",
            "strength_and_weaknesses": "Following are suggestions, questions, and concerns that I had while reading the paper. \n\n1- Discussing how the current work aligns with findings from [4] and [5] could improve the paper.\n\n2- Combining the proposed method with salient feature extraction [1, 2, 3] and memory understanding [4, 5] could lead to significant insights.\n\n3- According to Section 3, any RL algorithm can be used. If so, it would be worthwhile if authors could provide results for other well-known RL algorithms to verify the proposed method's generalizability and reliability.\n\n4- How the role of memory is studied? Because of the complex update mechanisms in the RNN, we do not really know how removing a set of trajectories from the training data would affect it during the training. So how can the results be trusted?\n\n5- This work assumes that the training process of RL agents is deterministic. But we often see that due to randomness involved in the training, it\u2019s possible to fall into a local minima so the distance between a given policy and an explanation policy could not necessarily be due to lack of cluster_i, but could be due to the stochasticity involved during training. How can we be assured that it is not the case?\n\n6- Providing results for more environments could be beneficial to check if the method is generalizable and robust.\n\nMinor issues:\n\nIn Section 2, \u201cexplain an action\u2019s agent by identifying\u201d -> \u201cexplain an agent\u2019s action by identifying\u201d\n\n[1] Nikaash Puri, Sukriti Verma, Piyush Gupta, Dhruv Kayastha, Shripad Deshmukh, Balaji Krishnamurthy, and Sameer Singh. Explain your move: Understanding agent actions using specific and relevant feature attribution\n\n[2] Rahul Iyer, Yuezhang Li, Huao Li, Michael Lewis, Ramitha Sundar, and Katia Sycara. Transparency and explanation in deep reinforcement learning neural networks\n\n[3] Samuel Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari agents\n\n[4] Anurag Koul, Sam Greydanus, Alan Fern. Learning Finite State Representations of Recurrent Policy Networks\n\n[5] Mohamad H Danesh, Anurag Koul, Alan Fern, Saeed Khorram. Re-understanding Finite-State Representations of Recurrent Policy Networks",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well-written, easy to follow, and clear. The goal is described as clearly as possible. Also, all the tools used for implementations and experimentation are referenced which is nice. Finally, although no source code is provided, given the detailed pseudo-code provided in the paper, reproducibility should not be a major concern.",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4083/Reviewer_CPZK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4083/Reviewer_CPZK"
        ]
    },
    {
        "id": "K28-WGdltR",
        "original": null,
        "number": 2,
        "cdate": 1666447654242,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666447654242,
        "tmdate": 1666597905081,
        "tddate": null,
        "forum": "5Egggz1q575",
        "replyto": "5Egggz1q575",
        "invitation": "ICLR.cc/2023/Conference/Paper4083/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is about explanations in reinforcement learning (RL). The idea is to focus on trajectories in offline settings (in a dataset) and find a cluster of trajectories that best explains a particular decision/behavior. The approach finds embeddings of trajectories, then clusters them, and verifies which of the clusters most contributes to a decision, by looking at the original dataset \"minus\" each of the clusters to see the influence (aka in terms of how close the behavior is to the original when trained with the restricted set). The approach is evaluated on three domains, and the paper contains a small user study to verify whether generated explanations match human thoughts about which trajectories contribute to decisions.\n",
            "strength_and_weaknesses": "The main strengths of the paper are that it is a very clear paper (both in terms of technical content, and in terms of structure/narrative) and that it provides a very simple, yet practical approach to work on explanations, trajectories and RL.\n\nThe first (clarity) is not yet visible in the abstract (which is not too clear I think) but the rest of the paper is highly clear about motivations, formalizations, choices, experimental setup and outcomes. All formalizations are at the right level; for example, Markov decision processes (etc) are not even defined, since all relevant matters are at the level of trajectories, and the text is doing a very good job to explain exactly what is needed. The five steps in Section 3 are very clear. Maybe one could explain a little more about the choice for embeddings (and technical details and choices), and I do not think Fig 1 is even necessary, but overall this is done well. The experimental setup is well motivated and the outcomes clear. I like the small \"shopping list\" in Section 4.1. which gives all details. Maybe a bit more detail on (for example) the attribution (and the matching) could be given here though.\n\nAnother big strength seems to lie in the simplicity of the (technical) approach. I think that it is very easy to (fore)see lots of variations on instantiations of parts of the algorithm to experiment with, and the general approach of trajectory \n\nA small weakness is the related work section, which could give more detail on explainable RL, which is quite an active subarea already, and this section does not provide enough state-of-the-art yet, I think. I think the offline RL paragraph is less important since it is merely a setting/context for this paper. I also think that the paper lacks detail and positioning in the context of trajectory clustering in RL, for example when it comes to personalization. There's quite some work on that, see for example as a starting point:\n\nZhu, F., Guo, J., Xu, Z., Liao, P., Yang, L., & Huang, J. (2018, September). Group-driven reinforcement learning for personalized mhealth intervention. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 590-598). Springer, Cham.\n\nA bigger weakness is that I am not sure the explanations actually \"work\". Technically I see what the method is doing, but conceptually I am not (yet) convinced this type of explanation is very useful. The basic idea is to look at clusters of trajectories and see whether we can match a decision/behavior to whether we have seen traces in the dataset that somehow provide a hunch on whether we have seen something like this before. It seems natural (also in the illustrations) that if a decision appears in one or more trajectories, it may be more likely that these trajectories can be used in an explanation, but if the dataset containst all kinds of trajectories (good and bad) then this does not sound logical. Technically clustering trajectories can always be done, but the actual \"meaning\" of it for explanations is less clear to me from reading the paper. The computational experiments only look at \"matching\" the behaviors (which are a bit in line with what I wrote in the above) but the human evaluation is even less clear about the usefulness and meaning. That is, in 70 per cent of the cases the best explanation coincided with the generated one but it also involves random trajectories (for which the probability that the decision was on them feels much lower). Overall, the evaluation seems to be very short, and not enough analyzed. Also, in the computational experiments (maybe also for the humans, but I could not find out) the clustering is probabilistic (as in: the attribution is a distribution, in the last row of Tables 1 and 2); how does this factor in the explanations? This is unclear to me, since in the end (alg 5, line 6) a particular cluster is assigned, but what if the distribution is very uniform? What happens with the results, what drives these results, and how will the (conceptual power of the) explanations be affected?\n\nSmall things:\n- I did not understand the motivation for the \"average of these output tokens\" (page 3) nor \"how\" this is done.\n- \"These choices.... of clustering\" at the end of page 5: can you elaborate on these choices?\n- The numbers of trajectories seems small (60, 717, etc). How does this influence the results? Is there any motivation here? Also, can you motivate more the choice for the 30 and 35 sublength trajectories?\n",
            "clarity,_quality,_novelty_and_reproducibility": "As said in the above, clarity and quality are good and ok, reproducibility should not be a problem (even from the description alone) and novelty is ok but also somewhat unclear.\n",
            "summary_of_the_review": "Very clear paper with a practical approach that could be extended in many ways. The conceptual usefulness of this type of explanations is a bit unclear to me, and the evaluation does not help here. Positioning of the work is done ok, but it misses some relevant directions, for example clustering-based approaches in RL.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4083/Reviewer_NGMa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4083/Reviewer_NGMa"
        ]
    },
    {
        "id": "pKxTP_MpQ8-",
        "original": null,
        "number": 3,
        "cdate": 1666572396210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666572396210,
        "tmdate": 1666572396210,
        "tddate": null,
        "forum": "5Egggz1q575",
        "replyto": "5Egggz1q575",
        "invitation": "ICLR.cc/2023/Conference/Paper4083/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper discusses the problem of explaining decisions made by an RL policy. The authors propose learning trajectory embeddings, followed by clustering, and then learning different policies for datasets constructed by removing different clusters. Candidate clusters are then chosen based on which policies lead to most deviation in chosen actions compared to the original dataset policy. Finally, the cluster with the minimum embedding distance from the original dataset is chosen to be most attributable to explaining the behavior at the given state. ",
            "strength_and_weaknesses": "Strength \n\n- Well motivated problem\n- Simple method\n- Fairly well done experiments\n\nWeaknesses\n\n- Limited scope in terms of experiments",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear to read, high in quality, has limited novely in my opinion, and is fairly reproducible.",
            "summary_of_the_review": "The paper is written well, explains most concepts in detail and motivates the problem well. The experiments are done well, although I am not sure what kind of tasks one should really be testing explainability on. As I am not well aware of the relevant literature (and it\u2019s quite possible there is minimal literature around RL explainability), it is hard for me to comment how effective the current evaluation setup really is. My current hunch is that the experiments considered are too simplistic in terms of gathering understanding on explainability. One minor question I have on this: The PCA plots for Seaquest do not seem to have a clear structure. Any comments on this? The mean action-value difference might not be a suitable metric if the action chosen by the other policy is out of distribution for the Q-value trained on the original policy. Thought on this? Why do you use a MSE as the action contrast measure for the continuous case (given you\u2019re comparing probability distributions).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4083/Reviewer_XJVh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4083/Reviewer_XJVh"
        ]
    },
    {
        "id": "Q3nBWYGQ0F",
        "original": null,
        "number": 4,
        "cdate": 1666601263503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601263503,
        "tmdate": 1666601263503,
        "tddate": null,
        "forum": "5Egggz1q575",
        "replyto": "5Egggz1q575",
        "invitation": "ICLR.cc/2023/Conference/Paper4083/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the Explainable RL. To identify which trajectory among offline dataset contributes most to the behavior of the learnt policy, the authors first discretize the offline trajectory dataset into different groups, and train a separate policy for each group, representing the counterfactual outcome if that group is excluded from the training set. To explain a new action at a given state, the policies which are very unlikely to take that action would be chosen as the candidates for explaination, among which the one with training data mostly similar to the original data would be identified as the final explainer.",
            "strength_and_weaknesses": "Strength:\n1. This paper tries to learn the explainability of RL methods from a more complete perspective(trajectory information) instead of local information(current state information). And it's interesting to conduct a human study to verify the rationality of the learned explanation.\n2.  The proposed method is reasonable.\n\nWeaknesses:\n1. The descriptions of the experimental results should be improved, especially, more detailed annotations on the visual results are necessray to help the readers understand better.\n2. Without comparison methods, it's hard to evaluate the effectiveness of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. the writing is not clear enough - the paper simply lists its major algorithms steps without explaination the design motivations.\n2.  reproducibility would be bad\n\n Most details in the method is too arbitrary or vague.  For example, the encoding of trajectories is an important part, but the paper doesn't show the concrete form of the encoder. Besides, as in Alg.1, taking average of embeddings of all tokens in a trajectory seems too simple, without consideration of difference in the embeddings of state, action and reward. In Alg.5, the metric of actions distance is also missing, which remains problems like how to calculate distance between action 'left' and action 'right'? The estimation of c_final in Alg.5 is also lack of reasons.\n\n3.  The evaluation experiments is not sufficient enough. In human evaluation, the paper doesn't compare the method with other explaination methods in RL. In quantitative analysis, some parts of results contradict claims in the paper and are lack of comparisons with other methods too.\n\n4.  The faithfulness of the proposed method is not mentioned.",
            "summary_of_the_review": "This paper needs more comparison reuslts and more detailed analysis to enhance its persuasion.\nSome questions and suggestions:\n\n1. Fig.2 shows the clusters of trajectory embeddings, this paper said these clusters represented semantically meaningful high-level behaviors, but there are no specific behavior demonstration for each corresponding cluster to support this claim, and Fig.2b didn't show a perfect clusterd results.\n2. Fig.3 shows the action suggestion is influenced by trajectories distant from the cell under consideration, but this is concluded by the qualitative results, in my opinion, I can't see any attribution from traj.ii&iii to the corresponding decision at position (1,1).\n3. About the quantitative analysis, this paper said thet the relatively large initial state value estimation for the original policy indicates the original policy can have access to all behaviors, I don't think this explanation is reasonable, maybe it's just an overestimated value.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4083/Reviewer_UWVh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4083/Reviewer_UWVh"
        ]
    }
]