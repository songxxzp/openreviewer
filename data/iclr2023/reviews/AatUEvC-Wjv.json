[
    {
        "id": "OIKff_RF4SW",
        "original": null,
        "number": 1,
        "cdate": 1665657316475,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665657316475,
        "tmdate": 1665667595101,
        "tddate": null,
        "forum": "AatUEvC-Wjv",
        "replyto": "AatUEvC-Wjv",
        "invitation": "ICLR.cc/2023/Conference/Paper5030/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "As I understand: For learning an agent across multiple RL tasks that can quickly adapt to new tasks, the authors combine several established paradigmas: \n- decision transformers as the base archictecture for the learning agent, trained from offline RL data across tasks; \n- adding additional \"adaption\" parameters (layers);\n- these parameters are pre-trained using the Hyper net training (Alg 1) and then used as initalization;\n- when a new task arrives, with demonstrator data, then first the adaptation layer is fine-tuned (from the above initializatin) on that new task's demonstrator data (while the base params are kept). This is a trade-off to be able to be adapt, while avoiding too much variance of the overall architecture upon new tasks (and forgetting old ones).\n\nSome of these ideas come from NLP and are here tansferred to DT.\n\nIn the experiments, HDT outperforms some baselines and some questions are tried to be answered experimentally, e.g., if the hyper net learns to use the task-specific data (5.3).",
            "strength_and_weaknesses": "**Strengths:**\n\nThe overall problem - multi-task learning - and the choice of the interesting recent DT architecture as base, are well motivated.\n\nThe method combines several paradigms and seems to transfer insights from NLP Transformer models, which is nice.\n\nThe experiments seem to indicate, that the method works comparably well.\n\n\n**Weaknesses and specific points for improvement:**\n\nMany paradigmas are put together (meta, offline RL, IL, \u2026), sometimes with more, sometimes with less motivation. For instance, why combine offline RL base-training with expert-data as task-specific info? Where does this scenario happen in practice (i.e., where do we have the base offline RL data plus a demonstrator for each task)? It is not entirely counterintuitive, but I didn't see the clear motivation. Maybe the main reason is that only this setting (i.e., data from a demonstrator in the new task) fits well into the quite restrictive DT format where the data has to be in the form of s/a/r-triplets. But how about just using offline RL data for the new task as well (or is there too little information content in non-expert data?). A stronger motivation would help here.\n\nThe wiriting clarity and understanding of the method needs to be improved, e.g.,\n* The action variable $a$ in Alg 1/2 comes from nowhere. It needs to be related to the trajectories etc. Essentially, variables always have to be properly introduced (within a context/algo).\n* I couldn't follow the argument in sec 5.3. Why again does Fig 3 (a) show that the hyper net in fact learns to use task-specific knowledge?\n* Imo, DT is introduced a bit too briefly although it is at the very core and a quite new paradigm. I didn't understand how the reward-to-go is obatained but I guess this simply the retrospective sum since we are in an offline setting - OK but maybe clarify. How about this length K \"lag\" fed into the transofmer. Why not, as in classic RL, just feed the *single* current state (plus desired reward in the sense of the DT idea) and output an action? Is this to account for non-Markov, or for an agent memory, or for task info? There is an informal comment on this on p4, but it feels vauge - then again, maybe one just has to accept that these sort of approaches are less motivated by understanding and more by experiments.\n\nFurther comment on experiment: I'm a bit surprised about the very poor performance of some of the baslines in Table 2 (0 success). This can be a hint that they are not tuned properly enough.\n\nIn terms of novelty/significance, there seems to be some novelty, however, the paper is heavily based on \"just\" combining several well-established ideas.",
            "clarity,_quality,_novelty_and_reproducibility": "See also above.\n\nRe. correctness: The evidence is mostly empirical; I did not check all details of the experiments. Overall, the experimental section is sensibly structured (several baselines, questions answered by comparison with other models, etc.); however, in the specifics, I did not find all details understandable, e.g., the point above (re. 5.3).",
            "summary_of_the_review": "The multi-task problem is well-motivated, the interesting recent DT approach is combined with further ideas to address it; the novelty exists but is limited and the writing and explanations need to be improved. Hence, I do not see a fundamental issue; however, the contribution seems rather limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5030/Reviewer_k7sT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5030/Reviewer_k7sT"
        ]
    },
    {
        "id": "w0Y557rrgC",
        "original": null,
        "number": 2,
        "cdate": 1666614923739,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614923739,
        "tmdate": 1669300515929,
        "tddate": null,
        "forum": "AatUEvC-Wjv",
        "replyto": "AatUEvC-Wjv",
        "invitation": "ICLR.cc/2023/Conference/Paper5030/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new approach\n\nDecision Transformers (DT) have demonstrated strong performances in offline reinforcement learning settings, but quickly adapting to unseen novel tasks remains\nchallenging. To address this challenge, we propose a new framework, called\nHyper-Decision Transformer (HDT), that can generalize to novel tasks from a\nhandful of demonstrations in a data- and parameter-efficient manner. To achieve\nsuch a goal, we propose to augment the base DT with an adaptation module,\nwhose parameters are initialized by a hyper-network. When encountering unseen tasks, the hyper-network takes a handful of demonstrations as inputs and\ninitializes the adaptation module accordingly. This initialization enables HDT to\nefficiently adapt to novel tasks by only fine-tuning the adaptation module. We validate HDT\u2019s generalization capability on object manipulation tasks. We find that\nwith a single expert demonstration and fine-tuning only 0.5% of DT parameters,\nHDT adapts faster to unseen tasks than fine-tuning the whole DT model. Finally,\nwe explore a more challenging setting where expert actions are not available, and\nwe show that HDT outperforms state-of-the-art baselines in terms of task success\nrates by a large margin. Demos are available on our project page.",
            "strength_and_weaknesses": "# Strengths\n- important problem of interests for the community\n- promising initial results, the proposed approach seems to be quite effective \n- simple, intuitive approach\n- some interesting insights on how the method scales with the number of training tasks and model size\n\n# Weaknesses\n### 1. evaluation on a single domain\n- The method is evaluated only on the tasks from Meta World, a robotic manipulation domain. Hence, it is difficult to judge whether the results will generalize to other domains. I strongly recommend running experiments on a different benchmark such as Atari which is commonly used in the literature. This would also verify whether the method works with discrete action spaces and high-dimensional observations. \n\n### 2. evaluation on a setting created by the authors, no well-established external benchmark\n- The authors seem to create their own train and test splits in Meta World. This seems strange since Meta World recommends a particular train and test split (e.g. MT10 or MT50) in order to ensure fair comparison across different papers. I strongly suggest running experiments on a pre-established setting so that your results can easily be compared with prior work (without having to re-implement or re-run them). You don't need to get SOTA results, just show how it compares with reasonable baselines like the ones you already include. Otherwise, there is a big question mark around why you created your own \"benchmark\" when a very similar one exists already and whether this was somehow carefully designed to make your approach look better.\n\n### 3. limited number of baselines\n- While you do have some transformer-based baselines I believe the method could greatly benefit from additional ones like BC, transformer-BC, and other offline RL methods like CQL or IQL. Such comparisons could help shed more light into whether the transformer architecture is crucial, the hypernetwork initialization, the adaptation layers, or the training objective. \n\n### 4. more analysis is needed\n- It isn't clear how the methods compare with the given expert demonstrations on the new tasks. Do they learn to imitate the policy or do they learn a better policy than the given demonstration? I suggest comparing with the performance of the demonstration or policy from which the demonstration was collected. \n\n- If the environment is deterministic and the agent gets to see expert demonstrations, isn't the problem of learning to imitate it quite easy? What happens if there is more stochasticity in the environments or the given demonstration isn't optimal?\n\n- When finetuning transformers, it is often the case that they forget the tasks they were trained on. It would be valuable to show the performance of your different methods on the tasks they were trained on after being finetuned on the downstream tasks. Are some of them better than the others at preserving previously learned skills?\n\n### 5. missing some important details\n- The paper seems to be missing some important details regarding the experimental setup. For example, it wasn't clear to me how the learning from observations setting works. At some point you mention that you condition on the expert observations while collecting online data. Does this assume the ability to reset the environment in any state / observation? If so, this is a big assumption that should be more clearly emphasized and discussed. how exactly are you using the expert observations in combination with online learning? \n\n- There are also some missing details regarding the expertise of the demonstrations at test time. Are these demonstrations coming from an an expert or how good are they? \n\n# Minor\n- sometimes you refer to generalization to new tasks. however, you finetune your models, so i believe a better term would be transfer or adaptation to new tasks. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\nThe paper is clear overall. However, it is missing some important details as explained above. \n\n# Quality \nThe paper is well-motivated, proposes a sensible approach, and compares it with a number of different baselines and ablations on one domain, demonstrating promising results. However, I believe the empirical evaluation could be further improved by including evaluations on well-established benchmarks, other domains, and a few more relevant baselines, as detailed above. The paper could also benefit from more extensive analysis. \n\n# Novelty \nWhile the different parts of the approach aren't novel, this particular combination and application is, so I believe the work contains enough novelty for this to not be a factor against acceptance, assuming all the other more important issues are adequately addressed.\n\n# Reproducibility\nI couldn't find any mention of the code. While the method contains some details regarding the implementation and experimental setup, these are far from enough to easily reproduce the work. Do you have any plans of open sourcing the code? This is an important factor in my decision. ",
            "summary_of_the_review": "This paper proposes a new approach for finetuning on new tasks after offline training on a set of different tasks. While the authors present some promising initial results, the empirical evaluation requires more work to warrant acceptance at the ICLR conference. In particular, the generality of the approach cannot be assessed without evaluating it on multiple well-established benchmarks designed in prior work (rather than by the authors themselves). More baselines and analysis could also further strengthen the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5030/Reviewer_aTrS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5030/Reviewer_aTrS"
        ]
    },
    {
        "id": "3dnNI5CU16",
        "original": null,
        "number": 3,
        "cdate": 1666654246996,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654246996,
        "tmdate": 1666654246996,
        "tddate": null,
        "forum": "AatUEvC-Wjv",
        "replyto": "AatUEvC-Wjv",
        "invitation": "ICLR.cc/2023/Conference/Paper5030/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, authors study the important question of efficiently adapting decision transformers when having access to a handful of demonstrations for unseen tasks. They propose to augment the base DT with an adaptation module that can be controlled by a hyper-net, which can take the demonstration as inputs. The author conduct experiments on meta world and shows that their results are better than other baselines.",
            "strength_and_weaknesses": "Strength:\nThe problem is well motivated, and efficiently adapting large pertained decision transformers with a handful of observation is an important line of research. Also the LfO setting is interesting since in many real life robotic applications it\u2019s hard to obtain state action pairs as demonstrations. The paper is also easy to follow. \n\nWhat I like most is the experiment design and the details. While at first sight, the proposed method seems very complicated, the design is justified in the ablation study. E.g., Fig 3 suggests that the hypernet is better than other simpler ways of adapting transformers like prompting (PDT) and IA3. It\u2019s also nice to see the comparison with meta RL methods like SimPL, but it\u2019s expected that SimPL is not as good as the proposed method, since SimPL does not take into the demonstration in the unseen tasks. It\u2019s also interesting to see that the hyper net can encode the task-specific information as in sec5.3. While the paper only has one environment Metaworld, but meta45 is a pretty challenging task alone. I think this could be interesting for researchers studying decision transformers.\n\nWeakness:\nWhile I think metaworld is enough, more environments could be nicer, like Kitchen, which seems to be more widely adopted in the previous literature.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to read. I like the demo webpage with a clear summary of main ideas and performance.",
            "summary_of_the_review": "In general, I think it's a good paper and recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5030/Reviewer_FfBQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5030/Reviewer_FfBQ"
        ]
    },
    {
        "id": "n4BWI4D5po",
        "original": null,
        "number": 4,
        "cdate": 1666665148957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665148957,
        "tmdate": 1666665148957,
        "tddate": null,
        "forum": "AatUEvC-Wjv",
        "replyto": "AatUEvC-Wjv",
        "invitation": "ICLR.cc/2023/Conference/Paper5030/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new way to apply Decision-Transformer (DT) [1] to unseen tasks through the adapter layer initialized by a Hyper-network. They showed their method could adapt faster and better to unseen tasks with few parameter updates when comparing with previous works Prompt-DT [2], SiMPL [3], IA3 [4], and variations of their model. Especially their model can adapt even only with the demonstration of unseen tasks where expert action is unavailable.\n",
            "strength_and_weaknesses": "Strength\n- This paper proposes a new way for DT to adapt the unseen tasks through additional neural network layers called an adapter layer. It shows better performance than baselines and model variations.\n- Through designing, their adaptation can cover not just full trajectories but also demonstration data. Their model can adapt the unseen tasks with and without expert action.\n- Their model is compared with many variations, such as without hyper-network initialization or training the adapter with DT simultaneously.\n\nWeaknesses\n- Some parts of this paper are not clear to me.\n    - In section 3.4 equation (3), the convolutional network's input is unclear. Why did you concatenate $L_s(h^o)+L_t(h^o)+L_l(h^o)$ and $L_{\\hat{r}}(h^o)+L_t(h^o)+L_l(h^o)$? Is it possible to use $L_s(h^o)+L_t(h^o)+L_l(h^o)+L_{\\hat{r}}(h^o)$?\n    - In section 4.2 HDB-IA3, what is the meaning that the models do not utilize position-wise rescaling? \n    - Table 2 is hard to understand. Does that mean the success rate? The average success rate when achieving to get a success episode? You mentioned that the data efficiency is based on the top-2 number of online rollout episodes until a success episode. Does it mean the number in the table is the average of the two number of online rollout episodes?\n    - In section 5.2, relating to the unclarity of table 2, could you explain more about why PDT hardly improves success rates?\n    - > First, HDT achieves similar training task performance compared with the pre-trained DT model as in Figure 3 (a), which shows that the hyper-network could distinguish the task-specific information of training tasks.\n        - Is it because the multi-task DT can distinguish the task-specific information through a given state, action, and reward sequence?\n- Typo in conclusion. grinned -> grind.",
            "clarity,_quality,_novelty_and_reproducibility": "Except for some parts, this paper is well-written and clearly explains and evaluates their model. Their model is novel, different from Prompt-DT or IA3 methods and outperforms them. Through the details about the hyperparameters, it looks reproducible.",
            "summary_of_the_review": "This paper proposes a new method for DT to adapt the unseen tasks. The way to do this is novel and shows better performance compared with previous works [2,3,4]. For me, some parts are unclear, which I hope to be updated, but overall, this paper is well-written and good to share in our community.\n\n\n[1]Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" Advances in neural information processing systems 34 (2021): 15084-15097.\n\n[2] Xu, Mengdi, et al. \"Prompting decision transformer for few-shot policy generalization.\" International Conference on Machine Learning. PMLR, 2022.\n\n[3] Nam, Taewook, et al. \"Skill-based Meta-Reinforcement Learning.\" arXiv preprint arXiv:2204.11828 (2022).\n\n[4] Liu, Haokun, et al. \"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.\" arXiv preprint arXiv:2205.05638 (2022).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5030/Reviewer_9MRj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5030/Reviewer_9MRj"
        ]
    }
]