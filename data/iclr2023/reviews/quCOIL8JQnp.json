[
    {
        "id": "vsLOyo_y0s",
        "original": null,
        "number": 1,
        "cdate": 1665695067804,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665695067804,
        "tmdate": 1665695067804,
        "tddate": null,
        "forum": "quCOIL8JQnp",
        "replyto": "quCOIL8JQnp",
        "invitation": "ICLR.cc/2023/Conference/Paper4569/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a diffusion model-based data augmentation method. The proposed method combines the contrastive loss with the diffusion model generation process to generate data with a large diversity between different classes. It shows that using the proposed method generates data and trains a classifier, the trained classifier is more robust to adversarial attacks than DDIM, the SOTA diffusion model.    ",
            "strength_and_weaknesses": "Strengths\n+ The paper provides a clear description of the background. \n+ The theoretical analysis part is clearly described. \n\nWeaknesses:\n- Regarding the theoretical analysis, as mentioned above, I appreciate that the analysis part is clear. However, I have the following two questions: (1) For the simulation test, the authors mainly focus on the case $\\hat{\\mu}=c\\mu$. The case that improves the clean accuracy. I am wondering why not verify the other case $\\hat{\\mu}=c(\\mu - \\epsilon 1_d)$ that improves the robust accuracy. (2) It would be great if the authors could provide more justification for the conclusion drawn from the theoretical analysis: \"The synthetic data can help improve the classification task especially when the representation of different classes is more distinguishable in the synthetic distribution.\" From the analysis, the most obvious conclusion one could draw is under what situation the clean accuracy is the best ( $\\hat{\\mu}=c\\mu$), and under what situation, robust accuracy is the best. This situation is based on the difference between the synthetic distribution and the original distribution. It is not that obvious between the connection between the above situations to the difference between the representations of different classes in the synthetic data.  \n\n- The proposed technique is not entirely clear to me. The proposed data augmentation is clear. But the overall training process is not clear. I am not sure what the procedure would be like. It either generates data using the proposed method and uses the augmented dataset to train the model or generates data using the proposed method and applies adversarial training on the augmented data, or others. My understanding is the second one. It would be great if the authors could clearly describe the overall procedure. If my understanding is correct, it brings a new question, whether the robustness comes from data augmentation or adversarial training. The paper does not justify this. \n\n- The evaluation setup is not clear enough, mainly how to generate attacks. My understanding is training a model using the proposed method and then generating attacks based on the trained model. Another way is to generate attacks on a vanilla model and use those adversarial samples to test models trained by the proposed method. It would be great if the authors could make this setup clear. The improvement over DDPM and DDIM is marginal (BTW, what is the reason for using DDIM for CIFAR-10 and DDPM for Traffic sign data). I would also suggest reporting the mean and standard errors of multiple runs to show the improvement is not because of randomness.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed technique is not clearly stated, and the novelty is limited. The results should be able to reproduce with an additional description of the missing details in the proposed technique. The overall quality is below the bar. ",
            "summary_of_the_review": "This paper proposes a new defense to train adversarially robust classifiers. As mentioned above, the descriptions of the proposed method are not entirely clear, and the technical contribution is not that significant. Besides, the empirical improvement is marginal and is not well justified. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "This paper studies the security of DNN and ML models. It would safe to include an ethical statement. The authors could discuss how the proposed technique could be used by attackers and defenders. ",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4569/Reviewer_H9p7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4569/Reviewer_H9p7"
        ]
    },
    {
        "id": "nJ4VFehjhF8",
        "original": null,
        "number": 2,
        "cdate": 1666177817915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666177817915,
        "tmdate": 1666177817915,
        "tddate": null,
        "forum": "quCOIL8JQnp",
        "replyto": "quCOIL8JQnp",
        "invitation": "ICLR.cc/2023/Conference/Paper4569/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the setting where supplemental synthetic data is used to augment the training sample and improve generalization. Specifically, authors focus on the adversarial setting where prediction errors consider worst case perturbations within a neighbourhood of test points. In particular, in a simplified setting, authors verified necessary properties of the synthetic data source in order fo it to yield performance gains: synthetic class conditionals must be far from their real counterparts, while still being separable by the same the decision boundary as the original data. They then use this insight and define image generation procedures that generate samples that, while still being perceptually close to the data distribution, it is as far as possible from it. Empirical assessment shows improvements when using the proposed \"guided\" generation approach combined with adversarial training.",
            "strength_and_weaknesses": "Pros:\n\n+The paper tackles relevant problems from a practical perspective: how to best leverage synthetic data in order to obtain robust classifiers.\n\n+The practical approach is motivated from findings obtained formally in a simplified setting, rather than solely based on intuition.\n\n+The approach used to sample synthetic data might be more generally applicable in settings other than the ones covered in the paper.\n\nCons:\n\n-It's unclear whether observed improvements are due to the properties of the synthetic data or the simple fact the training sample became bigger. I would suggest an experiment where you train models on a subsample of the training data (say half of it) supplemented by synthetic data up to the same size as the original sample. Comparing this model with the one training on the full training sample would help control for the contribution of the sample size.\n\n-Results aren't contextualized with past work in that numbers reported in previous work are not reported. In particular, baselines results without additional data in table 2 seem weak (entries 7 and 11 in this leaderboard seem so use the same architecture and threat model: https://robustbench.github.io/#div_cifar10_Linf_heading).\n\n-It's also unclear to what extent the results observed in the simplified setting transfer to real data. It seems to me the results are a direct consequence of the choice of model class, so it seems the optimal synthetic data source is the one that introduces points lying far from the decision boundary. I wonder how conclusions would change had a different model class been chosen, e.g., SVMs.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written and easy to follow.\n\nQuality: The paper clearly states the hypothesis under analysis, and provides a simplified framework from which design decisions used in practical cases are inspired.\n\nNovelty: The approach employed to guide data generation is novel to my knowledge.\n\nReproducibility: While authors did include some implementation details in the text, it seems to me that it would be difficult to reproduce results without access to code.\n\n",
            "summary_of_the_review": "The paper tackles a relevant problem and proposes practical approaches to guide generative processes towards samples that do not trivially match the training data. However, it's unclear to me whether the observed improvements are due to simply increasing the sample size or to the specific properties of the generated data. Moreover, I'm not confident that results on CIFAR-10 are at least on par with recent work under similar settings.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4569/Reviewer_96gf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4569/Reviewer_96gf"
        ]
    },
    {
        "id": "TunQIIhe7J",
        "original": null,
        "number": 3,
        "cdate": 1666856135983,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666856135983,
        "tmdate": 1666856135983,
        "tddate": null,
        "forum": "quCOIL8JQnp",
        "replyto": "quCOIL8JQnp",
        "invitation": "ICLR.cc/2023/Conference/Paper4569/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In the paper, Contrastive-Guided Diffusion Process (Contrastive-DP) for robust training is presented. Using Contrastive-DP, a diffusion model to generate new data could be created. For a classification task, using a contrastive setup, better results than DDIM are obtained. The article is heavily inspired by Gowal et al. (2021) and (Song et al., 2021a). The most prominent point is using both real data and synthetic data as in (Carmon et al., 2019) and the effort to select discriminative features. Thus, it can be viewed as an extension to the previous work.",
            "strength_and_weaknesses": "(+)\n- The idea of generating more discriminative images to obtain more robustness is powerful and valid.\n- The reasoning for selecting new synthetic data in a contrastive framework is valid.\n\n(-)\n- The inspiration comes from previous work, the idea is incremental.\n- In Table 1, no better results for robust accuracy on \u201cReal data/d=2\u201d is obtained. Also same values are repeated for \u201cReal\u201d and \u201cc=0.5\u201d. \n- There is a problem in Algorithm 1. For Lines 5-6, a more clear explanation is needed. At Line 5, z is created and it is not clearly shown how it is used.\n- The results are presented for only WRN-28-10. The results should be presented for recent neural network architectures that achieve state-of-the-art performance values on the used datasets.\n- Song et al.\u2019s 12th formula is used as the 4th formula in the paper. However no random noise is applied to make it deterministic here. Ablation study for nondeterministic setups should also be included. \n-DDIM also used CelebA, LSUNBedroom and LSUNChurch datasets, i.e. not only CIFAR-10 and Traffic Signs. In this work, the authors  should also demonstrate the performance on those relatively more complex datasets.\n- Results given in CIFAR-10 are incremental. Also in Table 3, there is nearly no difference between DDPM and Contrastive-DP. Also no std. values are given in the tables. Multiple initializations and training runs should be run and the standard deviations should be reported in the tables.\n-Time complexity should be analyzed. Using Contrastive-DP instead of DDIM, there may be a tradeoff between time and accuracy. Is the given incremental results worthy of it?\n- Figure 1 is hard to understand. Should be explained clearly in its caption.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is not very clear. There are also some small grammar mistakes like \u201cthe bigger \u2026 the small(er)\u201d.\n\nQuality: The paper needs more quality figures (legend for Fig. 1 is missing) to explain the method.\n\nNovelty: The idea is sligthly novel.\n\nReproducibility: The code is uploaded but it is not reproducible. For instance, some hyperparamaters are missing in the article (c, tau etc.).\n",
            "summary_of_the_review": "As the method is not explained very clearly, the paper lacks necessary experiments, and explanations, which are explained above, and the results seem to produce incremental gains or none, the contribution of the paper is not adequate for ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4569/Reviewer_4Hr9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4569/Reviewer_4Hr9"
        ]
    }
]