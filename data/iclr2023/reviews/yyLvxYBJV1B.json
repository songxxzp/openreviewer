[
    {
        "id": "jQFUMTrxfvD",
        "original": null,
        "number": 1,
        "cdate": 1666257842817,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666257842817,
        "tmdate": 1666257842817,
        "tddate": null,
        "forum": "yyLvxYBJV1B",
        "replyto": "yyLvxYBJV1B",
        "invitation": "ICLR.cc/2023/Conference/Paper1859/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The  paper propose a new concept, which is anytime domain adaptation.  \n\nUnlike the normal domain adaptation setting, the authors aim to make a generalised version of DA, which is tested under both at the level of input (resolution) and network (width, depth).\n\nTo achieve it,  the main idea of the paper is training two networks as teacher and student with switchable depth, width and input resolutions to enable testing under a wide range of computation budgets.\n\nExtensive experiments are conducted on 4  benchmark datasets, and the most related work \"slim DA\" is also included for testing.",
            "strength_and_weaknesses": "Firstly, I like the new concept proposed by this paper,  which considers domain alignment in addition to varying both network (width and depth) and input (resolution) scales to enable testing under a wide range of computation budgets. Such concept is quite pratical, since we need to apply different scales network on different terminals. \n\nBut also, although I like this concept, we feel it maybe a little bit like the mixture of several concepts. Just as mentioned in the paper, anytime network plus domain adaptation. What about first do the normal domain adaptation and then do the anytime inference? \n\nAnd as for the method part, it is said that \"the student minnet is trained to match its output with the average prediction of the teacher subnets, while the student subnets are trained to match their outputs with the prediction of the teacher supernet.\" why make this setting, I guess that it mainly for minimize the performance gap, but I don't think it would make much difference. And if it works, why don't make the same setting for the other subnet like the second small sub network?\n\nAnd in this setting, I see that the authors set the teacher and the student network the same architecture. It is not flexible enough I think.",
            "clarity,_quality,_novelty_and_reproducibility": " quality of the paper is good.\n\n clarity of the paper is good.\n\nand originality of the paper is good.\n",
            "summary_of_the_review": "So my concerns are:\n\n- What is the main contribution of this paper? What is the main difference between anytime network + da\n\n- Why choose to match average performance? why not apply it to other subset networks?\n\n- What about different architecture pairs.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1859/Reviewer_Z9Q2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1859/Reviewer_Z9Q2"
        ]
    },
    {
        "id": "RQozMS2tkB0",
        "original": null,
        "number": 2,
        "cdate": 1666260225565,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666260225565,
        "tmdate": 1666260367985,
        "tddate": null,
        "forum": "yyLvxYBJV1B",
        "replyto": "yyLvxYBJV1B",
        "invitation": "ICLR.cc/2023/Conference/Paper1859/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed an domain adaptation method, adapted to different computational budgets when applying in the target domain. Specially, it considers the network depth, width and input resolutions in different budgets, and aims to learn a network that can be tailored to different network depth/width and input data sizes. The basic idea is based on the student-teacher framework, that distills the knowledge from larger capacity sub-networks to smaller capacity sub-networks. It also takes advantage of the switchable BNs and pseudo-label in the proposed method. The experiments show that the proposed method achieves higher performance than the DDA method, and marginally better than SlimDA. ",
            "strength_and_weaknesses": "1. Strength\n\n(1) The task to tackle is a realistic setting that needs to adapt to different computational budgets in domain adaptation. \n\n(2) The paper proposed a progressive student-teacher model to learn the sub-nets in different budgets. The proposed training loss is reasonable and demonstrates its effectiveness in DA benchmark datasets.\n\n2. Weakness\n\n(1) The basic idea of this work is simple and reasonable, however, combines the existing ideas of switchable sub-nets, BN, and the pseudo-label approach in domain adaptation.  \n\n(2) In Fig.3, the proposed approach is compared with the other methods, e.g., DDA, MSDNet. The curves show better performance when the budget is lower. Comparisons for larger computational budget should be also shown on these datasets. \n\n(3) Since the training is based on distilling knowledge from larger sub-net to smaller sub-net. How does this training approach affect the higher capacity subnet. Will it may hurt the performance of the larger capacity subnet?\n\n(4) The budget is pre-set, limiting its application if taking budget, might be out of range of pre-set budgets. How to handle this case? How does the competitors handle these out-of-range cases?\n\n(5) How about the performance of extension to using larger backbones as teacher/student networks? \n\n(6) How to understand the 'anytime' in the proposed setup.  'Anytime' may not be accurate to express the idea of DA adapted to different budgets.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is mostly clear and the authors are suggested to public the codes for reproducibility.",
            "summary_of_the_review": "The paper tackles a realistic problem in domain adaptation, and proposed an effective method for the anytime domain adaptation task. My major questions are on the more comparisons/evaluations, and clarifications on the extensible to out-of-range budget and larger backbone network. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1859/Reviewer_Qxxc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1859/Reviewer_Qxxc"
        ]
    },
    {
        "id": "Ip88J8_xCb",
        "original": null,
        "number": 3,
        "cdate": 1666638474696,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638474696,
        "tmdate": 1666638474696,
        "tddate": null,
        "forum": "yyLvxYBJV1B",
        "replyto": "yyLvxYBJV1B",
        "invitation": "ICLR.cc/2023/Conference/Paper1859/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to study a novel domain adaptation setting, i.e., anytime domain adaptation, by combining anytime prediction and domain adaptation. Specifically, two networks are trained as teacher and student with switchable depth, width, and input resolutions to enable testing under a wide range of computation budgets. A bootstrapped recursive distillation approach is designed to train the student subnets with the knowledge from the teacher network. In this way, the target features are brought close to the source and the learned knowledge can be better transferred to a smaller network for efficient inference. Experiments are conducted on four benchmark datasets to show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n1. The studied topic, i.e. anytime domain adaptation, is a novel domain adaptation setting with practical applications. The idea is interesting and the motivation is clearly explained.\n2. The proposed method, i.e. teacher-student framework, with bootstrapped recursive distillation makes good sense and seems to be effective.\n3. The experiments are convincing. The results are much better than the compared baselines. The ablation studies are promising.\n4. The presentation is generally clear and fluent.\n\nWeaknesses:\n1. It needs more discussion on the limitations of directly combining existing domain adaptation and anytime prediction techniques.\n2. The visualization of domain adaptation results is insufficient, such as t-SNE. I am interested to see how the visualization looks like under different computation budgets.\n3. Some closely related reviews and surveys on domain adaptation are missing, such as \"A review of domain adaptation without target labels\", \"A Review of Single-Source Deep Unsupervised Visual Domain Adaptation\". The format of references is consisitent.\n4. The analysis on the reasons that the proposed method can perform better than the baselines is not well summarized.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper studies a novel domain adaption by combining with anytime prediction. The proposed method is novel and effective. The new problem and method indicate high quality and novelty of this paper. The proposed method is clearly explained and implementation details can ensure reproducibility.",
            "summary_of_the_review": "Novel and interesting adaptation setting, effective method, convincing experiments and superior results but insufficient analysis, generally good presentation but inconsistent reference format.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1859/Reviewer_fqZv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1859/Reviewer_fqZv"
        ]
    },
    {
        "id": "1tVF21HY2F",
        "original": null,
        "number": 4,
        "cdate": 1666679473919,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679473919,
        "tmdate": 1666679473919,
        "tddate": null,
        "forum": "yyLvxYBJV1B",
        "replyto": "yyLvxYBJV1B",
        "invitation": "ICLR.cc/2023/Conference/Paper1859/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Combining the interests to of two separate fields of inquiry the authors of this work tackle the problem of Anytime inference for Domain Adaptation through the use of a recursive knowledge distillation. Being able to perform inference over multiple computational budgets is a beneficial strategy for deployed machine learning approach and prior work has shown an undesirable sensitivity to domain shifts, this work proposes an useful extension which would allow models to perform anytime inference over shifted target domains.",
            "strength_and_weaknesses": "The paper tackles and underexplored problem area of anytime domain adaptation and presents an effective solution which substantively improves the performance of anytime networks over unsupervised target domains. \nThe paper is well written and easy to follow, with the exception of the sandwich rule, all components of the approach are intuitively motivated and empirically verified. \n\nThe main weaknesses of the paper is that the improvements over SlimDA appear to be marginal at best and its relationship with that work contemporary work is not well explained. Additional, the utility of anytime domain adaptation is not immediately evident as any system which employs it would necessarily need to expend substantial MACs in order to learn the adapted network for the target domain. \nIf the authors could directly address why the process of learning the adaptation network should not be considered in the need for efficient inference over a target dataset it would improve the quality of the work.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and provides a novel solution to infrequently studied problem area.\nWhile there are questions of how sensitive the presented results are towards hyperparameters of the method, reproducing the algorithm itself should be straightforward from the presented work.",
            "summary_of_the_review": "The paper provides an interest approach to tackling the underexplored problem of anytime domain adaptation. The improvements over anytime baselines are consistent though not substantial and general utility of this problem area are not immediately apparent. \nHowever, this work thorough and explores and interesting new direction, as we collectively work towards more reliable machine learning systems this work provides an interesting perspective.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1859/Reviewer_yBmg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1859/Reviewer_yBmg"
        ]
    }
]