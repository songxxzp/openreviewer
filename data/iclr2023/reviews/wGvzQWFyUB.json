[
    {
        "id": "x6sHJxvkZz",
        "original": null,
        "number": 1,
        "cdate": 1666657634352,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657634352,
        "tmdate": 1666681723684,
        "tddate": null,
        "forum": "wGvzQWFyUB",
        "replyto": "wGvzQWFyUB",
        "invitation": "ICLR.cc/2023/Conference/Paper5018/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a new algorithm for Personalized RecSys under Interaction Grounded Learning (IGL) paradigm where a personalized policy is learned to maximum unobservable user satisfaction with only implicit feedbacks. The authors alter the independence assumption used in vanilla IGL (Xie et al 2021/2022) to enable personalized feedback that depends on the context. They also increase the reward state from 2 to 3 to capture non-extreme (neutral) user state. Based on the modified assumptions, the authors propose a new IGL algorithm with Inverse Kinematics (different from contrastive learning algorithm proposed in original IGL in 2022) and a novel reward decoder that handles three states.  Empirical results are shown with both simulation and real-world News recommendation dataset, where better fairness is achieved when feedbacks are synthetically designed to be user dependent. They also AB test in production setting and is comparable with contextual bandit with reward engineering.",
            "strength_and_weaknesses": "Strength:\n\n- The authors find a novel application of IGL paradigm on personalized RecSys and propose key modifications that adapt to user modeling use case. The resulting model solves an important problem in RecSys where we usually lack explicit feedback on the objective that need to be optimized, and implicit feedbacks are sensitive to user subgroups such that ad-hoc reward engineering is undesirable.\n- The application of Inverse Kinematics on IGL problem is novel, and it is technically non-trivial.\n- The introduction of 3-state reward decoder is non-trivial and interesting.\n- The result on synthetic feedback scenario shows improved fairness, and the algorithm is comparable to SOTA production candidate with hand-crafted reward engineering.\n\nWeakness:\n\nDespite the novelty and contributions, there are some ambiguities that may need further clarification\n\n- When and how is the DN(\u2026) function learned? It seems PU-learning is used where only negative labels are partially provided, and the labeling probability is somehow known. But in the empirical experiments it is unclear how these set of information is obtained. Moreover, it is non-trivial to estimate labeling probability in real-world, and there is not much explanation on how this hyperparameter is chosen and how it impact the performance.\n- The synthetic experiments with Facebook and Covertype datasets are designed such that feedbacks are heavily dependent on user type, making it in favor of the proposed algorithm. And only simple CB are used as baselines for comparison. The real-production result is comparable to reward-engineering, while the significance of the lift is not clear as the confidence lower bound <1 for likes and there is a positive lift in dislike. Comparison to action-inclusive IGL in Xie et al 2022 is also lacking, making it less convincing whether the user dependence assumption is the main reason for the lift.\n- The authors propose different assumption and learning algorithm than vanilla IGL. Yet no theoretical analysis is provided to prove whether the new independence assumption and the learning algorithm converges to the optimal solution. The enhancement on fairness is also supported with only empirical evidence, thus might limit the impact of the work.\n\nMinor issues:\n\n- There might be a typo in the last line of equation (2) where the prior on numerator is `p(a,x)` whereas in equation(3) it is `p(a|x)`.\n- Missing figure for learning curves for IK, reward and policy learning. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and originality of the work is high, while the writing can be further improved for clarity. Especially, Algorithm 1 is not very informative. Instead of using \u2018.predict\u2019 and \u2018.learn\u2019, it would be clearer if the author provide the exact loss function and steps used to train KI and Policy, as for RecSys audience who are less familiar with KI it is not obvious how KI is learned.",
            "summary_of_the_review": "The paper proposes novel solution for personalized reward learning with implicit feedback in IGL paradigm, and come up with important modification of original IGL (Xie et al 2022). Despite limited theoretical result and baselines, the contribution is potentially significant to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5018/Reviewer_4RJN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5018/Reviewer_4RJN"
        ]
    },
    {
        "id": "1LxQaRxP2D",
        "original": null,
        "number": 2,
        "cdate": 1666859761196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666859761196,
        "tmdate": 1670201114505,
        "tddate": null,
        "forum": "wGvzQWFyUB",
        "replyto": "wGvzQWFyUB",
        "invitation": "ICLR.cc/2023/Conference/Paper5018/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a personalized reward learning method for recommender systems. The authors apply the recent Interaction Grounded Learning (IGL) paradigm to address the challenge of learning representations of diverse user communication modalities. The proposed personalized IGL is designed for context-dependent feedback, with inverse kinematics as an IGL objective and the capability of modeling more than two latent states. Both simulations and experiments on real production data have demonstrated the proposed IGL is able to address two typical challenges for modern online recommender systems, i.e., feedback-reward dependence assumption and the number of latent reward states.",
            "strength_and_weaknesses": "Strengths:\n1. This paper tackles an important research problem, optimizing latent user satisfaction in recommender systems, through an interesting angle, i.e., learning personalized reward functions.\n2. The proposed IGL method is reasonable and capable of solving domain-specific challenges.\n3. The proposed method is rigorously derived in Section 3.\n\nWeaknesses:\n1. Background on contextual bandit and interaction grounded learning should be strengthened.\n2. The authors claimed that this method could handle both implicit and explicit feedback, while only implicit feedback is considered in experiments.\n3. The empirical performance improvements seem to be barely marginal, and only three types of contexts are considered in the facebook news recommendation scenario. What is the number of contexts on production datasets?\n4. Many details of experiments and implementations are missing, like the definition of metrics, scale of production dataset, model parameters, etc.\n5. The paper is generally well-written, but its clarity can be further improved. For example, the meaning of $a$ / $r$ in recommender systems should be clearly defined.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to my comments above for clarity issues.\n\nI think the idea of IGL for recommendation is novel and reasonable. The proposed method also solves two domain-specific challenges, though left with many others to be further tackled.\n\nThe authors also provide the source code for reproducibility check.\n",
            "summary_of_the_review": "Given the listed strengths and weaknesses, I think this is a borderline paper.\n\n## After rebuttal\nThe authors' response has somewhat alleviated my concerns. Thus I decide to raise my score from 5 to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5018/Reviewer_iWcq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5018/Reviewer_iWcq"
        ]
    },
    {
        "id": "5LM4zsgKag",
        "original": null,
        "number": 3,
        "cdate": 1667234416269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667234416269,
        "tmdate": 1667234416269,
        "tddate": null,
        "forum": "wGvzQWFyUB",
        "replyto": "wGvzQWFyUB",
        "invitation": "ICLR.cc/2023/Conference/Paper5018/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper posits that reward signals from users are not necessarily clear and cannot be easily attributed to genuine preference but might be the result of various biases. In particular implicit signals (watches/clicks) do not map directly to user satisfaction. Moreover, different users can have different ways of signaling preferences. The authors propose Interaction Grounded Learning or personalized\nreward learning (IGL). IGL is a learning paradigm where a learner optimizes for unobservable\nrewards by interacting with the environment and associating observable feedback with the true latent reward.\nThe adaptations to the case of recommendation of IGL are presented in the paper and the final formulation of the algorithm involves a mapping between conditional probabilities and estimated potential rewards. On that basis, a Bandit-type algorithm is trained and experiments are conducted on artificial offline data and real news recommendation data with positive results. \n\n",
            "strength_and_weaknesses": "The paper is well written and well motivated, in particular, the issue of extracting reward out of implicit signals in recommendations is an open question. The paper also includes results from a production system that is not typically the case in this type of work. Moreover, the idea of applying IGL principles in the area of recommender systems is to the best of my knowledge novel and justified well. \nThe experimental section is somewhat restricted to results from the live experiment and there is no clear comparison to e.g. bandit algorithms with a simpler implicit signal to reward mapping hence it is unclear how much of an advantage the method hold compared to potentially simpler alternatives. It is also somewhat unclear against what baseline the method is tested in the live experiments. \nOne somewhat negative point is that this work is relatively narrow even in the space of recommender systems and might interest only a small subset of the ICLR audience, a more IR/recommender systems conference might be a more appropriate venue. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is well written, and there is some degree of novelty in that IGL is being applied to the recommendation problem and in particular to mapping implicit signals to rewards. The treatment of the problem and the derivation of the mapping to rewards could be clearer as it is somewhat difficult to follow. It would be helpful if more of an intuition is provided at every step.  \nThe author seem to be willing to publish the code and most datasets from the experimental section which would provide adequate reproducibility of a big part of the experimental section. \n",
            "summary_of_the_review": "Overall an interesting work with a novel treatment of a problem in the area of recommender systems. \nI'm a bit unsure about the impact of the work in a general ML conference. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5018/Reviewer_7wvr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5018/Reviewer_7wvr"
        ]
    }
]