[
    {
        "id": "F6CfEwJrJt",
        "original": null,
        "number": 1,
        "cdate": 1666537035633,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666537035633,
        "tmdate": 1666537035633,
        "tddate": null,
        "forum": "8WTAh0tj2jC",
        "replyto": "8WTAh0tj2jC",
        "invitation": "ICLR.cc/2023/Conference/Paper2012/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a model for graph representation learning called AgentNet. \n\nThe main idea of the paper is to combine graph walks with neural networks: the model consists of agents that, in parallel, explore the graph by performing the following steps: \n\n1. Updating the state of the current node\n2. Aggregate information from the node's neighborhood\n3. Update the state of the agent\n4. Move to a new node in the neighborhood\n\nThe key difference from typical GNNs is that agents maintain their own global state as they explore the graphs, while also being able to \"read\" the other agents' contributions to the node states. \n\nThrough a nice theoretical analysis, the authors show that their model is more expressive than message-passing and even higher-order GNNs, while also having a cost sublinear in the number of nodes. \n\nExperiments confirm that that model is indeed capable of solving synthetic benchmarks to measure the expressivity of GNNs at a fraction of the cost. \nResults on real-world datasets are less exciting, although AgentNet shows performance comparable to much more expensive higher-order GNNs.",
            "strength_and_weaknesses": "**Strengths**: \n- The paper presents an interesting idea and is well-written and thorough. \n- The theoretical analysis of the expressivity of AgentNets is interesting and easy to follow. \n- The proposed method is significantly less expensive than other higher-order baselines and, although the performance is not always exceptional, I believe that it's more than acceptable. \n\n**Weaknesses**: \n- What is the difference between \"pooling\" and \"readout\" in the following sentence?\n  > pooling is applied on the agent embeddings, followed by a readout function\n- I suggest removing the claim that the agents \"consciously select the next neighbor\".\n- The following claim:\n  > In Table 3 we can see that AgentNet performs well in this scenario and outperforms even the best ESAN model.\n\n  should be removed. There is no evidence to claim that the results reported in the table are significantly different between GIN (with virtual node), ESAN, and AgentNet.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: the paper is clear and well written, the exposition is easy to follow and the organization of the content is good. \n- Quality: the work is interesting and the analysis is complete. The results are not impressive but they are sufficient in light of the massive savings in computational cost.\n- Novelty: the paper draws from well-known ideas in machine learning literature but the combination of these ideas is novel and well-motivated. \n- Reproducibility: it should be possible to reproduce the results from the description of the method given in the paper and appendices. Code is provided as part of the supplementary material. ",
            "summary_of_the_review": "I really enjoyed the paper and I can safely recommend acceptance. There are a few minor issues that I ask the authors to address, but this is otherwise a good paper that would fit well at ICLR and would be of interest to the GNN community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_PGLs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_PGLs"
        ]
    },
    {
        "id": "_6SmGZQ4lTm",
        "original": null,
        "number": 2,
        "cdate": 1666561717763,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561717763,
        "tmdate": 1666561717763,
        "tddate": null,
        "forum": "8WTAh0tj2jC",
        "replyto": "8WTAh0tj2jC",
        "invitation": "ICLR.cc/2023/Conference/Paper2012/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes to learn graph-level representations by exploring the graph structure with an agent ---rather than with message-passing schemes. The authors present an extensive theoretical characterization of the expressive power of such a model, connecting it with, for instance, deterministic functions of the r-hop neighborhood around a node v. In practice, the authors show how the model is competitive with other baselines and confirm its enhanced expressive power.",
            "strength_and_weaknesses": "---Strength---\na) Novelty: moving away from message-passing schemes is a necessity of the graph learning community.\nb) Clarity: presenting intuition together with a theoretical result is always welcome.\nc) The authors precisely characterize the class of functions (graph properties) AgentNet can learn. This type of characterization is very help to guide practitioners' model choices. Eg., \"deterministic functions of the r-hop neighborhood around a node v\".\nd) Results as far as I was able to follow and check all hold. Proofs are clear and well written.\n\n---Weaknesses---\na) The main weakness in my opinion is the empirical evaluation. Since the authors tested their model in the OGB hiv task, I don't understand why other OGB datasets were not used. I don't find the TUDataset suit insightful of a practical evaluation of a model anymore. Further, the authors could have designed synthetic datasets that explore the properties known to be \"learnable\" by their model (see theory results). This could strengthen a lot the submission.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is well presented, e.g. intuition about theoretical results is always given, which facilitates its understanding. Moving \"away\" from message-passing schemes is an extremely welcome novelty. Regarding reproducibility, the authors could present in the appendix a full description of hyperparameters and details about their architecture.",
            "summary_of_the_review": "Overall, the strength of this paper outweighs its weakness (see S&W section) and therefore my recommendation is supported. If the authors are able to at least partially address my concerns in the weakness section I'm willing to raise my score. To clarify, I'm not looking for SOTA results necessarily, but to be convinced that this architecture is competitive with existing ones. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_z2nH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_z2nH"
        ]
    },
    {
        "id": "B-K5dNyi3B",
        "original": null,
        "number": 3,
        "cdate": 1666806939852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666806939852,
        "tmdate": 1669318982629,
        "tddate": null,
        "forum": "8WTAh0tj2jC",
        "replyto": "8WTAh0tj2jC",
        "invitation": "ICLR.cc/2023/Conference/Paper2012/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a new type of graph neural network with agents running learnable(intelligent) random walk on the given graph. The author studies the theoretical analysis of proposed agent based method in counting substructures, for both 1 agent case and multi agent case. To demonstrate the effectiveness, the author run experiments over counting substructure dataset and many small real-world TU dataset. The experimental results show improvement or comparable result to other GNNs. ",
            "strength_and_weaknesses": "**Strength**: \n1. The angle of conducting agent-based random walk is very interesting. And with small number of agent, the method can be more efficient than message passing gnns.\n\n\n**Weakness**:\n1. Similar to Random Node Initialization based method, it achieves higher expressivity with the cost of introducing randomness which hurts training stability and generalization. The author doesn't give enough analysis and empirical result to show the badness of randomness introduced. \n2. The theorems in section 4.1 is kind of questionable, giving it doesn't consider the badness of random. Specifically, for two isomorphism graphs, the traverse sequence for a agent can be different which leads to different embeddings that is not desirable. In the proof of theorem 2, the author claims that this kind of problem can be avoid by assuming there is a function that can assigning every possible IDDFS traversal to its isomorphism class of r-hop neighbor subgraph. The claim is misguiding as this is equivalent to say \"Deep network can learn random noise\" which is of course possible but needs tons of data and time to train and are not guranteed to converge easily. This is actually also observed in RNI based method. \n3. All other theorems in section 4.1 have similar issues. The author starts every theorem of \"there exists any configuration\" which is perhaps true but also not considering the randomness of instability, hardness of training and requirements of more data. \n4. In fact if isolating all randomness of random walks, the expressivity is known to be limited by 2-WL. Please see [Geerts 20]. Hence I highly doubt the expressivity of this designed agent-based graph neural network when taking randomness into account. Another related work that needs to compare is [Toenshoff et al. 21], which designing a very interesting subgraph encoding method with random walk. \n5. The real-world performance is actually not good, which perhaps means that the expressivity of introduced method is limited, or suffering from the randomness. This means the proposed method is not practical to use for real-world problem. \n6. The author also needs to include other datasets like ZINC and some larger dataset from Benchmarking GNN, as these datasets are observed to be correlated with expressivity. \n7. The author claims that the agent can learn intelligent random walk, it would be very interesting to see some visualization of the walk learned, and to see whether it correlates to tasks like finding specific substructure. \n\n[Geerts 20] Walk message passing neural networks and second-order graph neural networks  \n[Toenshoff et al. 21] Graph learning with 1d convolutions on random walks   ",
            "clarity,_quality,_novelty_and_reproducibility": "The presented method is original, although it has some connection to other random-walk based methods.\nThe effectiveness of proposed method is limited of the randomness that hasn't been properly addressed.  ",
            "summary_of_the_review": "To summarize, the paper studies applying learnable random walk on graphs for graph classification. Although being a different angle of performing representation learning, many issues of introduced randomness are not properly addressed, and the connection to other random-walk based graph neural networks mentioned above are not clear yet. \n\n\n------------------------------------ after rebuttal -----------------   \nI would like to raise the score as 5, considering the great effort on providing more experiments and visualization. However, the real-world usage of the proposed method is still questionable. The expressivity of the proposed method is not well understood yet. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_pbyu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_pbyu"
        ]
    },
    {
        "id": "J7n5XkIIh5g",
        "original": null,
        "number": 4,
        "cdate": 1667503521583,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667503521583,
        "tmdate": 1667503521583,
        "tddate": null,
        "forum": "8WTAh0tj2jC",
        "replyto": "8WTAh0tj2jC",
        "invitation": "ICLR.cc/2023/Conference/Paper2012/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces an agent-based GNN which differs fundamentally from the message-passing based architecture. AgentNet consists of a collection of neural agents that are trained to walk on the graph and their states are pooled to make graph classification predictions. Theoretical analysis and empirically evaluation verify good properties of the proposed model.\n",
            "strength_and_weaknesses": "**Strength** \n- This paper provides a novel perspective for graph based learning beyond message-passing mechanism\n- The proposed model enjoys several good properties, e.g. expressiveness and efficiency, which are demonstrated via theoretical and empirical evidence\n- The new architecture is practical and flexible to extend\n\n**Weaknesses**\n- Lack ablation study to verify the importance of different steps in the model\n\n**Questions**\n- Do the agents share the same set of model parameters, but only vary in the start node? I am curious if these agents can be designed to collect different patterns.\n- Will the proposed method also encounter the oversmoothing or oversquashing issue if the number of steps or the number of agents are set too large, and undereaching issue if they are set too small?\n- Can the proposed method be extended to serve for other popular downstream tasks, such as node classification and link prediction tasks?\n- It would be better if ablation study can be provided to show the importance of each model design, e.g. node update, neighborhood aggregation, etc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow with clear motivation. \n\nThe proposed method is not technically nontrivial, but is well justified with good properties and is flexible to extend. The idea is novel among the studies based on the message-passing mechanisms.\n\nThe codes are provided for reproducibility.\n",
            "summary_of_the_review": "This paper is overall well motivated and well written. The proposed agent based graph learning architecture is novel, fresh, flexible, efficient and expressive. It should be interesting to the community.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_6DSe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_6DSe"
        ]
    },
    {
        "id": "jPCmBYefnN",
        "original": null,
        "number": 5,
        "cdate": 1667591632942,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667591632942,
        "tmdate": 1667591632942,
        "tddate": null,
        "forum": "8WTAh0tj2jC",
        "replyto": "8WTAh0tj2jC",
        "invitation": "ICLR.cc/2023/Conference/Paper2012/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work focuses on the design of expressive neural network architecture for  learning on graphs, specifically aimed at improving performance on graph-level tasks. The proposed approach modifies the existing message passing mechanism by introducing the concept of an agent(s) capable of aggregating different types of graph (node, edge) information at subgraph level and then mapping it to graph level output. The key ability of such agent(s) is to perform intelligent walks over subgraphs of size l. The authors argue that such a construction enables sublinearity in learning algorithms that is independent of original graph size. The authors perform theoretical analysis of various properties of this new design in settings with both single and multiple agents. The main theoretical results revolve around the ability of the new approach to distinguish non-isomorphic subgraphs and  to count various substructures such as cycles and cliques. The authors also present and discuss tradeoff scenarios on using single vs multiple agents architectures. Three synthetic datasets are used to test these theoretical properties and compared with various representative baselines focussing on expressivity of graph neural networks. To fully elucidate the gains made in computational complexity and asses the learning performance of the proposed design, the authors present graph classification results on  a suite of real-world graphs and compare it against representative baselines. ",
            "strength_and_weaknesses": "Strengths:\n------------\n+ The topic of designing expressive graph neural networks for learning on graphs has received significant attention recently and if of great interest to community.\n+ The use of meta-level information in the form of agents to gain subgraph-level expressiveness is an interesting idea and has potential for more exploration.\n+ The property of the proposed architecture to provide sublinear computational complexity is very appealing.\n+ The paper shows strong empirical results in the settings considered by the authors  with gains either in performance or in complexity over previous baselines\n+ The ablations with respect to the number of agents and number of steps are very insightful and clearly help to discern if and when the proposed method achieves \ngain. \n\nWeaknesses\n---------------\n\n- While the idea of intelligent walk is interesting, the language of agent is not useful. The single agent case is just a single initialization of an intelligent walk with memory and multi agent case is its extention to multiple walks. It is also confusing to present polling as some sort of global communication, it is just better to present it as aggregation of information from multiple walks. \n- As the transition function is based on attention, this method is clearly very related to attention based graph neural networks. Of course, attention [1] is one way to select a subgraph to focus on for computing a node embedding and thereby graph emebdding. The authors fail to discuss how the proposed  method relates to attention based models. Both theoretical and empirical justification is required. \n- There are several strong works that are relevant to this approach and provide both theoretical and empirical insights [2,3,4]. The authors have neither discussed them nor considered them for comparisons which is a big miss.\n- While the theoretical results attempt to detail different scenarios, many of the theorems and lemmas are more of propositions or insights (e.g. Thm 2, Corollary 3, Lemma 4, thm 7, lemma 8) and it would be good to describe them so.\n- While the approach can be considered independent of graph size in some cases (not really clear if you can always avoid visiting most nodes), it does seem to depend on the degree of nodes and hence large graphs graphs where several (even ~100) nodes may have high degree, this approach will potentially struggle. Can the authors comment on such scenarios\n- More concrete theoretical results relating to the counting of substructures such as cycle and cliques are provided for single agent case. how does this results hold in multiple agent case?  \n- In experiments, using mean number of nodes as number of agents is a very strange and adhoc choice for the proposed approach. It needs better motivation than just matching the complexity of other  works.\n- Interpretations of the results are not always clear or adequate - For figure 2(d) for example, higher number of agents of course lead to fewer node visits per agent and it does not perform better than GIN. So how to interpret these results? The claim of better robustness using Table 3 is very hand wavy and needs lot more investigation. Can the authors comment more on why they think those results indicate robustness to oversmoothing and oversquashing?\n- Empirically, the gains with AgentNet are below marginal and couple this with missing comparisons, the proposed approach has lot of room for improvement. \n\n[1] HOW TO FIND YOUR FRIENDLY NEIGHBORHOOD: GRAPH ATTENTION DESIGN WITH SELF-SUPERVISION, KIM et. al. ICLR 2021\n\n[2] Random Walk Graph Neural Networks, Nikolentzos et. al. Neurips 2020\n\n[3] Walk Message Passing Neural Networks and Second-Order Graph Neural Networks, Gertz et. al. 2020\n\n[4] Ordered Subgraph Aggregation Networks, Qian et. al. June 2022",
            "clarity,_quality,_novelty_and_reproducibility": "- Originality is limited in that most part of architectures still follow conventional message passing except the introduction of meta-level information carrier in the form of agent(s). Also the marginal gains in the experiments adds less value to the novelty of the proposed approach.\n- While the overall paper is readable, there are few points that are not clear from the writing:\n    - Why isn\u2019t the proposed approach also translate into a BFS and only DFS? \n    - Before theorem 2, what do authors mean when they mention \u201c AgentNet can identify all edges between any two visited nodes? Doe you mean a path or multiple edges between two nodes?\n    - In multiple agent case, It is not clear why just having random vector initializations as agent id\u2019s ensure that agents become expressive enough to disentangle their own information at the end of learning. Why is there not a possibility that all or many agents end up learning similar information. Eventually, one would expect that agent information will be dominated by information in the nodes it traverses. \n    - It is not clear how to choose between single agent and multiple agent architectures for a given graph and problem. It appears that for most cases single agent design is not useful and that is only important for showing theoretical analysis.  \n    - In multiple-agent case, it seems that pooling may have big impact and ablation related to different pooling methods is useful for clarity.\n- The quality of the paper is fair. Many of the content in the main paper is not important (for example the propositions mentioned above), while the appendix contains a lot of proofs and architectural details. It would help to reorganize the content so as to highlight important contributions in the main paper. The technical quality of the paper can be further improved with the inclusion of the cited comparison and showing more concrete results on the theoretical and empirical gain of multiple agent scenario.\n",
            "summary_of_the_review": "The topic of designing expressive graph neural network is very interesting and the authors present an interesting idea towards that direction. The claims of sub-linearity is appealing and the authors show some improvement on classification tasks over representative baselines. However, both the theoretical results and empirical evidence fall short of strongly supporting overall claims of the paper and further the complete miss of discussion with attention based approaches and other related works, both of which informs my current assessment of the paper.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_wMRf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2012/Reviewer_wMRf"
        ]
    }
]