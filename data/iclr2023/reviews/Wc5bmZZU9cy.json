[
    {
        "id": "YxyJGV_2q8u",
        "original": null,
        "number": 1,
        "cdate": 1666329458360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666329458360,
        "tmdate": 1666329458360,
        "tddate": null,
        "forum": "Wc5bmZZU9cy",
        "replyto": "Wc5bmZZU9cy",
        "invitation": "ICLR.cc/2023/Conference/Paper3121/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper created a dataset to evaluate text-to-SQL systems on their robustness. The dataset, DR.SPIDER, is an extension of the openly available SPIDER dataset. It added perturbation to the natural language questions, SQL statements and example database (schema and values), so more robust text-to-SQL systems would have less degradation in accuracy on the perturbed samples. The paper categorized the perturbations into 17 different categories, covering different aspects of possible real-world variations.\n\nTo obtain the perturbations, the paper employed multiple methods. It used synonym sets from previous related works, employed in-house domain experts, used online crowd-source workers (Amazon Mechanical Turks), and leveraged PLMs. The process focused more on getting a balanced and correct dataset than the effectiveness of any single method. \n\nThe paper performed extensive evaluations of state-of-the-art models on DR.SPIDER, showed the ability of robustness measurement of the dataset, and also provided insights on the factors that may affect the robustness of existing algorithms. \n\nThe authors promised to release the dataset and evaluation tools. ",
            "strength_and_weaknesses": "Strength:\n\n1) A clear and useful goal. The paper addresses a specific and useful problem of evaluating the robustness of text-to-SQL systems, where existing datasets either cannot do, or have gaps in coverage. \n2) Detailed and useful evaluation. Most of STOA algorithms are evaluated, with detailed break-downs and also examples to help reader understand.\n3) Clear writing. The paper is well written, The Reviewer had no trouble following the main idea and technical details.\n4) Helpful to the community. The dataset will be released, which would help the improvement of the robustness of related methods.\n\nWeakness:\n\n1) The necessity and complexity of using PLMs are not clear, as 1) the total amount of data generated is small, in the range of hundreds. 2) significant human review is still needed up and down the pipeline.\n2) The size of DR SPIDER (The Reviewer's impression is ~1k) is much smaller than SPIDER (~10K questions).\n\n\nDetailed suggestions:\n\n1) Section 2 paragraph 1, last sentence: it's better to explain why we need semantic-changing perturbations, e.g. to be used as negative examples? or to test the model's ability to distinguish between closely related but different semantics? \n2) Section 3.2 paragraph 2: better explain what is \"naturally occuring  tables and columns\".\n3) Last paragraph in Section 3.2: Why split the filtered data into chunks, while ensuring each chuck have one annotators, why not just annotate all without chuck-splitting?\n4) Section 4. Upper-case of common model names, \"Bert-large\" -> \"BERT-large\" etc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good. The Reviewer had no trouble following the main idea and details.\n\nQuality: good. The paper addressed a clearly defined problem and the evaluations supported the claim.\n\nNovelty: ok. The methodology employed are mostly known in the field. As pointed out in the strength&weakness, the cost/benefit of using  PLM is not very clear.\n\nReproducibility: good. The authors promised to release the data and tools. ",
            "summary_of_the_review": "The paper provides a good extension to the SPIDER dataset to measure the robustness of text-to-SQL models. It addresses a clearly defined and important aspect of evaluation. The construction of the dataset is well thought, and the effectiveness of it has been demonstrated with experiments involves SOTA models. The paper is also clearly written and easy to follow. The resulted dataset could be a useful resource for the community in the future.\n\nThe only concern is the size of the set (a magnitude smaller than SPIDER, if the Reviewer understood correctly). The final size of the set is not clearly mentioned in the abstract, intro or conclusion (only mentioned \"17 perturbations\"). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3121/Reviewer_MJgi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3121/Reviewer_MJgi"
        ]
    },
    {
        "id": "eaTnl2QCHi",
        "original": null,
        "number": 2,
        "cdate": 1666474200161,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666474200161,
        "tmdate": 1666474200161,
        "tddate": null,
        "forum": "Wc5bmZZU9cy",
        "replyto": "Wc5bmZZU9cy",
        "invitation": "ICLR.cc/2023/Conference/Paper3121/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work contributes a robustness benchmark built on top of the Spider text-to-SQL benchmark dataset. One novel aspect is the use of crowdsourcing in tandem with language models to generate paraphrases of the natural language queries. The paper presents an evaluation of recent models against this new benchmark along with a corresponding error analysis.",
            "strength_and_weaknesses": "The paper is clearly written, systematically laying out the kind of data variation they are targeting and how they aim to achieve it. The experimental results show significant problems or gaps with existing models against these variations, but the error analysis also points to potential directions for improvement. The paper seems to reflect deep domain knowledge on this problem area and good contextualization with respect to related works.\n\nI found the paraphrase categories (Table 1) very helpful.\n\nThe Appendix is extensive, containing fairly fine-grained details about the implementation and execution of the experiments, as well as even more details on the results themselves. To be honest I think the Prompt Prefix tables (Appendix C) could be put in the code repo instead but it is a minor suggestion.\n\nThe error analysis and diagnostic insights seem very promising for future work. It's interesting that PICARD got confused by \"8 youngest winners\" but I guess queries in datasets probably often ask for Top 3 vs Top 8.\n\nFleiss Kappa of 0.61 is reasonable but not perfect agreement - were there interesting patterns of disagreement among annotators?\n\nThere may be weaknesses that a deeper expert on this area may have better context on, but I saw no red or yellow flags.",
            "clarity,_quality,_novelty_and_reproducibility": "I found this work to be straightforward in a good way: they present some novel ideas for generating more meaningful variation in benchmark data which are informed by a good understanding of the problem and existing approaches. They generate this new dataset carefully and systematically, show how existing methods struggle with the new variation, and do some analysis to suggest future directions. There are prior robustness-oriented extensions in this domain, but this work seems to go further in a meaningful way. The work seems very well-executed and clearly described.\n\nTable 3: DB Perturbation \"Average\" is misspelled as \"Avergae\"",
            "summary_of_the_review": "This seems like a valuable contribution to this problem area that should spur further research and improvements.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3121/Reviewer_WKcN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3121/Reviewer_WKcN"
        ]
    },
    {
        "id": "eB6CaOVuMk",
        "original": null,
        "number": 3,
        "cdate": 1666629181034,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629181034,
        "tmdate": 1666629181034,
        "tddate": null,
        "forum": "Wc5bmZZU9cy",
        "replyto": "Wc5bmZZU9cy",
        "invitation": "ICLR.cc/2023/Conference/Paper3121/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a variation of the SPIDER evaluation benchmark called Dr. Spider. Dr. Spider with the dr standing for Diagnostic Robustness, improves the original SPIDER benchmark by introducing augmentations to the Natural Language Quries and the SQL. While the augmentations do not change the benchmark, the challenge systems' understanding of natural language, and SQL. The augmentations aim to reveal whether models are overfitting to SPIDER's perks and to challenge systems to generalize better. The paper's experiments show that all state-of-the-art systems score lower on Dr. Spider, thus proving the need for an augmented SPIDER.",
            "strength_and_weaknesses": "## Strengths\n\n+ The paper is well-written and easy to follow.\n+ It is important that SQL benchmarks test for linguistic variations and challenge systems to do more than lexical matching between the NLQs and the SQL queries\n+ The experiments are convincing and thorough.\n+ Figure 1 and Table 1 are great at helping readers follow along with the particularities of Dr. Spider.\n\n## Weaknesses\n\n- Did not see any.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is well-written and easy to follow.\n* Figure 1 and Table 1 are great at helping readers follow along with the particularities of Dr. Spider.\n* Augmenting datasets and benchmarks is not a new idea, but, as the experiments show, it is necessary. I think that applying a known approach where necessary is a positive.\n* The paper is clear on how the data was collected and the appendices add all necessary details to understand the data collection.\n",
            "summary_of_the_review": "This is a well-written paper with no major weaknesses.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3121/Reviewer_QnJr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3121/Reviewer_QnJr"
        ]
    },
    {
        "id": "QGckIvXZkDm",
        "original": null,
        "number": 4,
        "cdate": 1667117923707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667117923707,
        "tmdate": 1667117923707,
        "tddate": null,
        "forum": "Wc5bmZZU9cy",
        "replyto": "Wc5bmZZU9cy",
        "invitation": "ICLR.cc/2023/Conference/Paper3121/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a benchmark to test the robustness of text-to-SQL models. Specifically, the benchmark is created to test whether such models work well (i) when names of particular columns in a table are replaced with synonymous strings (DB perturbations), (ii) when natural language questions are replaced by its paraphrases (NLQ perturbation) and (iii) when minor changes are in the natural language query and the SQL query (SQL perturbation). The paper extends the Spider dataset (Yu et al 2018) to cover all the perturbations. They propose a total of 17 perturbations belonging to these 3 categories.\n\nDB perturbation: For DB perturbation the paper proposes replacing column names by synonyms and abbreviations. Moreover, they also propose a novel perturbation where they replace a column by one-or-more multiple equivalent semantic columns (e.g. the \u201cname\u201d column can be replaced by two columns \u201cfirst_name\u201d, \u201clast_name\u201d etc)\n\nNLQ perturbation: For NLQ perturbation, the paper proposes a scalable approach in which first crowd workers propose 5 paraphrases of 100 questions. This is followed by experts filtering and categorizing the paraphrases into 9 categories. Next, to scale the process, an LLM is prompted with paraphrases from each category to generate paraphrases of new questions. This is followed by another automated filtering stage where an NLI model is used followed by the final round of expert filtering.\n\nSQL perturbation: For SQL perturbation, they replace operations like comparisons, sort-order, etc from both the natural language and SQL query.\n\nThe paper tests a variety of text-to-SQL models on this dataset and finds that the most robust models suffer from a 14% drop in performance with around a 50.7% drop on the most challenging perturbation.",
            "strength_and_weaknesses": "**Strengths**\n\n- Developing robust text-to-SQL models is important and this benchmark can serve an important role to test that\n- The paper covers more comprehensive perturbation for all components of text-to-SQL tasks. Moreover the use of LLMs to generate perturbation makes their process scalable without making the task artificial (e.g. same fluency scores of questions)\n- The paper is clearly written and easy to understand\n\n**Weaknesses**\n\n- I would have liked to see the robustness performance of large LMs such as GPT-3. It would be interesting to see how much incontext learning with large LMs can be robust to the proposed perturbations",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well written and easy to follow\n\nQuality: I believe the proposed dataset will be of high quality and would serve as an interesting benchmark to test the robustness of text-to-sql models\n\nNovelty: Even though robustness of text-to-sql models were studied before, the paper does a more extensive and thorough job of creating perturbations. Hence, I believe the paper still makes a novel contribution\n\nReproducibility: The dataset will be released to the public, so yes.",
            "summary_of_the_review": "I enjoyed reading the paper and I believe this will be an interesting and strong benchmark. I am leaning toward accepting the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3121/Reviewer_gzxj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3121/Reviewer_gzxj"
        ]
    }
]