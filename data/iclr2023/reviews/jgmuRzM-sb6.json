[
    {
        "id": "sO5vQxVzE8",
        "original": null,
        "number": 1,
        "cdate": 1666005437734,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666005437734,
        "tmdate": 1666005437734,
        "tddate": null,
        "forum": "jgmuRzM-sb6",
        "replyto": "jgmuRzM-sb6",
        "invitation": "ICLR.cc/2023/Conference/Paper33/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors apply GFlowNets to the problem of scheduling operations in a computational graph on homogeneous parallel hardware. The evaluate conditional GFlowNets for the first time(?) with the computational graph being the conditioning input and the schedule being the output. They also innovate by conditioning on the reward temperature in order to find a good trade-off between diversity of the generated samples and their average reward. Diversity is important in this application because the generative policy is trained using an imperfect but cheap to compute proxy and only a few of the generated schedules are then evaluated on the ground truth hardware. GFlowNets are interesting in this context because their training objective makes them sample with probability proportional to the reward (rather than maximizing the reward as in other RL approaches). Experiments show improvements against previous methods, including standard RL methods.",
            "strength_and_weaknesses": "GFlowNets are a new RL variant (less than a year old) with only a few applications yet, so this paper introduces a new type of application, in the realm of scheduling, and an approach which may be applicable more broadly. The successfully show empirical success on two GFlowNet novelties: conditioning on a rich input (the computational graph) and on temperature (to control the diversity-reward trade-off).\n\nIn page 5, they claim that previous works on GFlowNets did not consider the conditional case, but this is not true and they themselves mention in section 4 that Bengio et al 2021b introduced the theoretical framework for conditional GFlowNets. What is true is that may be the first (or among the first, with concurrent work) to validate experimentally this framework and show its usefulness.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, of good quality. Novelty is mostly in the form of experiments and a new application domain for a fairly new method (introduced at NeurIPS 2021). ",
            "summary_of_the_review": "This paper introduced a new form of application for GFlowNets, mapping a computational graph to a distribution over efficient schedules for it. To achieve this, the demonstrate the feasibility and success of the conditional GFlowNet formalism and also show how to use it to control the trade-off between diversity and average reward by conditioning on the reward temperature (exponent of the reward).\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper33/Reviewer_NBTZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper33/Reviewer_NBTZ"
        ]
    },
    {
        "id": "l-xLwdqhy",
        "original": null,
        "number": 2,
        "cdate": 1666558863965,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558863965,
        "tmdate": 1669442083908,
        "tddate": null,
        "forum": "jgmuRzM-sb6",
        "replyto": "jgmuRzM-sb6",
        "invitation": "ICLR.cc/2023/Conference/Paper33/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a method to explain the GNN using a newly proposed tool graph flow network(GFlowNets). This framework enables the model to connect the trajectories of the same graph even with different node sequences. The paper also proposed an efficient cut vertex criteria to speed up the whole process. Compared with traditional Tarjan methods, the proposed algorithm is much more efficient. Also, the experiment part shows that the result outperforms some other baselines.",
            "strength_and_weaknesses": "The authors proposed a new framework based on GFlowNet. This network is more explainable since it's based on the conditional state flow network. The whole network is trained to minimize the discrepancy between the sum of the outgoing flow and the final reward and incoming flow. This framework is intuitive and novel. The authors also proposed an efficient algorithm to speed up the parent state exploration process. \n\nFor the weaknesses, I have some concerns regarding the experiment part and the design of the loss function.\n\nJust like the authors claimed, the reward doesn't contain the sparsity or consistency part, which can be important in the GNN explainer model.\n\nSecondly, I'm a little confused since the performance shown in the paper is somehow different from the ones in the PGExplainer. Like the BA-community/Tree-cycles/BA-2motifs, there is a huge gap. Could the authors explain why? I found the performance from the arxiv version here. https://arxiv.org/pdf/2011.04573v1.pdf . Meanwhile, they are some other state-of-the-art baselines that are not compared in the paper, like IB-SUBGRAPH, GraphMask, DIR(Discovering invariant rationales for graph neural networks).  \n\nI also have a question regarding the GFlowNet, since it is somehow related to reinforcement learning, I wonder if the model is robust. Or do we need to carefully tune the model to make it work?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, although the readers may need some background knowledge of GFlowNet to have a better understanding. The proposed algorithm provides an efficient method to find the parent state, which is easy to reimplement and can be easily extended to other scenarios. For the novelty, I'm a little concerned since the GFlowNet is borrowed from others' work. For the proposed algorithm, I'm also not sure whether it's novel enough.",
            "summary_of_the_review": "This paper provides a new perspective for graph explainers. The paper also proposed an easy but efficient algorithm to find the parent state. My main concern is the gap in performance. Another concern is the novelty.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper33/Reviewer_CzDM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper33/Reviewer_CzDM"
        ]
    },
    {
        "id": "KvNgvGSsClj",
        "original": null,
        "number": 3,
        "cdate": 1666625843363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625843363,
        "tmdate": 1669361599434,
        "tddate": null,
        "forum": "jgmuRzM-sb6",
        "replyto": "jgmuRzM-sb6",
        "invitation": "ICLR.cc/2023/Conference/Paper33/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel strategy to obtain a connected subgraph as the interpretation of node classification and graph classification tasks. The proposed strategy starts from a state with only one node and learns a policy to achieve state transition via sampling new nodes and adding them to the current state. In general, the techniques used in this paper are novel under the topic of GNN interpretation, which reveals further insights on how to understand in what ways GNN predictions are made. However, there are also clear disadvantages in this paper. See the Summary of The Review for more details.",
            "strength_and_weaknesses": "Strength:\n\n(1) This paper is solid in techniques: definitions are clearly presented, and proofs are also provided in the appendix.\n\n(2) A novel approach is proposed borrowing the idea of generative flow networks to obtain subgraphs as interpretations for GNNs, which could reveal novel insights under this research topic.\n\n(3) The empirical performance superiority on most datasets seems to be promising.\n\nWeaknesses:\n\n(1) The motivation of this paper is unclear: why do we need this model considering the existing GNN interpretation approaches? The presented motivations are not convincing \u2013 see Summary of The Review for more details.\n\n(2) There are certain phenomena not clarified in the experiments \u2013 see Summary of The Review for more details.\n\n(3) Only one real-world dataset is involved in the experiments performed in this paper.\n\n(4) The writing of this paper needs to be further polished. I did notice some typos, such as \u201cGFlowExplainer does not pre-training process\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper does not provide a clear motivation for the proposed strategy; besides, how the proposed strategy avoids the disadvantages of previous works mentioned in the introduction should be further elaborated.\n\nQuality: The adopted techniques are sound in this paper, but the organization of this paper can be further improved \u2013 see Summary of The Review for more details.\n\nNovelty: This paper is novel in techniques under the topic of GNN interpretation.\n\nReproducibility: Most of the codes are provided. All datasets are open source.\n",
            "summary_of_the_review": "This paper proposes a strategy inheriting the idea of Generative Flow Networks to generate subgraphs as interpretations for GNN predictions, which is novel in techniques. Meanwhile, the theorems and lemmas are supported with detailed proofs. However, there are multiple unaddressed questions that prevent this paper from being well-prepared for publication. For example, the author mentioned that the primary motivation of this paper is: (1) searching subgraphs is a combinatorial problem, and it is hard to solve; and (2) previous works model the selection of subgraphs as a sequence generation problem, which is not beneficial. However, first, most GNN interpretation works based on learning do not require solving any combinatorial problem (e.g., [1]); second, we may also regard the proposed approach in this paper based on state transitioning as a strategy based on sequence generation. I thus believe the motivations above deserve a more rigorous justification. Moreover, there is no detailed discussion on how to choose an appropriate stopping criterion. Should all generated subgraphs reach the constraint of size K_M? Will there be cases where the size of the underlying interpretation subgraph is different from each other? If they exist, how could we design an appropriate stopping criterion that fits all scenarios? \n\nFinally, I would still have extra concerns as follows.\n\n(1) There is a policy network in Fig. 1, while this notion is never mentioned elsewhere in this paper.\n\n(2) The reported AUC seems to be much worse compared with the reported accuracy in the vanilla papers of baselines (e.g., [1]). Will it be more comprehensive to compare both AUC and accuracy?\n\n(3) There is no evidence showing that the exhibited superiority is (partially) because the proposed strategy does not generate subgraphs in a sequential manner.\n\n(4) Will it be better to introduce the definitions in Section 3.3 before these notions are referred to?\n\n(5) Is there any specific reason for the unique performance inferiority on BA-Community datasets compared with the baselines?\n\n(6) Is it possible to include more than one real-world dataset for more comprehensive experiments?\n\n[1] Rex Ying et al. Gnnexplainer: Generating explanations for graph neural networks. NeurIPS 2019.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper33/Reviewer_SPuf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper33/Reviewer_SPuf"
        ]
    },
    {
        "id": "v9yXgZ08NQ_",
        "original": null,
        "number": 4,
        "cdate": 1666650105387,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650105387,
        "tmdate": 1669445900657,
        "tddate": null,
        "forum": "jgmuRzM-sb6",
        "replyto": "jgmuRzM-sb6",
        "invitation": "ICLR.cc/2023/Conference/Paper33/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to use GFlowNet in a new application, explainability for GNNs. To me, this is a resemble\u00a0application for GFlowNet, though, I'm not that much familiar\u00a0with the explainability problem of GNNs. Intuitively, GFLowNet is able to generate subgraphs based on learning multiple trajectories\u00a0which maximize a specific reward. In this paper, the authors use MI as the reward which seems to be consistent with the previous models in this domain.\u00a0\n\nThe paper is fairly well-structured and easy to\u00a0follow. The most of methodological part of the paper is repeating the first GFLowNet paper. My main concern with this paper is that the authors use \"flow matching condition\" that has known issues. So, it raised some questions regarding some of the qualitative results.\u00a0\n\n",
            "strength_and_weaknesses": "**Pros:**\n\n- Applying GFlowNet in a new domain, i.e. GNNs.\n\n**Cons:**\n\n- Based on the loss in eq. 18, the authors only use the naive \"flow matching condition\" which is underspecified and tends to have biased to smaller trajectories. Besides that, it is computationally very inefficient. These are already known and new methods have been proposed. \n\n- This also raises a question about the quantitative results. To me, it is not clear if the shown results in Figure 3 which show a smaller sub-graph for the proposed method are due to these biases or if it is really beneficial from GFlowNet. I would rather like to see the results for the \"trajectory balance\" based loss. \n\n- I'm also not sure how much gain they could get from the efficient parent state explorations. An ablation study around it would be gratefully appreciated. I think a naive solution still should not be that complicated.  \n\n- Evaluation metrics are not enough. I would like to see comparisons in terms of Fidelity and Sparsity as well. \n\n- I also believe some of the baselines are missed. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The paper is not novel from a theoretical perspective but is a use case of GFlowNet in a new domain, i.e. explainability for GNNs. \n\nAs I'm not that\u00a0familiar\u00a0with the explainability of GNNs, it is hard for me to say that much regarding related works and experiments.\u00a0But I felt  the paper missed a few recent works in this domain and evaluation metrics are not complete. ",
            "summary_of_the_review": "The paper seems to be a good application paper, however, it needs some modification in terms of the loss as well as experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper33/Reviewer_uNyQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper33/Reviewer_uNyQ"
        ]
    }
]