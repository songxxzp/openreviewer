[
    {
        "id": "GM_jMZd8rd",
        "original": null,
        "number": 1,
        "cdate": 1666551290350,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666551290350,
        "tmdate": 1666565398562,
        "tddate": null,
        "forum": "j6zUzrapY3L",
        "replyto": "j6zUzrapY3L",
        "invitation": "ICLR.cc/2023/Conference/Paper2827/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new class of self-supervised encoder that learns the latent representation of data instances via a diffusion process that encodes the interaction interaction strength between data pairs. Two versions of the model are introduced, DIFFormer-s and DIFFormer-a, both demonstrating superior performance in a range of tasks.",
            "strength_and_weaknesses": "Strength:\n- This paper does a good job in motivating and describing the approach. From my perspective, the theory is quite interesting and elegant. The authors also provide a unified view for MLP, GCN, and GAT from their framework.\n- The model achieves improved performance over a broad range of tasks.\n- Extensive experiment results are provided in the Appendix on the influence of batch sizes, compute cost, etc. \n\nWeakness:\n- The model may have challenges in scaling to larger dataset due to the O(NKd^2) scaling for DIFFormer-s and O(N^2Kd^2) scaling for DIFFormer-a. I don\u2019t fully understand the GPU memory cost result in Table 8. Why does the memory cost only increase by 4 times when the batch size increases from 5000 to 200000? How about the compute and memory cost for DIFFormer-a?\n- For the CIFAR-10 and STL-10 experiments, have the authors considered stronger semi-supervised learning baselines like Meta Pseudo Labels? \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written and easy to follow. The approach is novel to the extent of the reviewer\u2019s knowledge. \n- The authors provided code and detailed instruction to reproduce the results.\n",
            "summary_of_the_review": "In summary, this paper provides a novel, principled approach to learn the latent geometry of a dataset via an energy function that encodes pairwise diffusion strength. The theory is sound and well-motivated. Empirical results show improvements on a range of tasks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2827/Reviewer_ckRq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2827/Reviewer_ckRq"
        ]
    },
    {
        "id": "fcn-Q89nAxv",
        "original": null,
        "number": 2,
        "cdate": 1666638720669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638720669,
        "tmdate": 1666638720669,
        "tddate": null,
        "forum": "j6zUzrapY3L",
        "replyto": "j6zUzrapY3L",
        "invitation": "ICLR.cc/2023/Conference/Paper2827/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, a discrete diffusion process along the lines of Zhou et al (2004) but with features inspired by GNN works such as Velikovic et al (2017) is studied for use in semi-supervised node feature prediction.  It is argued that the process can be derived from an energy function.\nMany experiments and comparisons with other models are done, and the results are competitive and arguably better.",
            "strength_and_weaknesses": "Strengths: Clearly written and many experimental results.\n\nWeaknesses: This is a much studied field and I did not find the discussion of advances over previous work entirely convincing.  \n\nThe performance improvements are marginal (1 sigma).\n\nIt was not clear to me how the \"simple diffusivity model\" could satisfy the conditions of theorem 1.  If the function g is linear in z^2, then \\delta is quadratic.  But there is no nondecreasing concave quadratic function on R.  The \"advanced diffusivity model\" uses a nonpolynomial function.  Perhaps there is some implicit restriction on the domain, as suggested by the comment below Eq. (8) that f(z^2)=2-z^2/2 is non-negative, and the comment about layer normalization.  It is important to clarify this point.\n\nTheorem 1 might follow from quasiconvexity of the energy function.",
            "clarity,_quality,_novelty_and_reproducibility": "This is both a good discussion of a class of diffusion models which include and generalize many GNN models, and an empirical study of their use in graph learning.  Similar models are studied in physics and as a mathematical physicist I did not find the novelties claimed at the top of page 3 (the model definition and theorem 1) surprising.  I am not conversant enough with the ML literature to know how novel they are in that community.  Similarly the experimental results are nice but not so much better than previous quoted work to consider this a signficant advance.  Publishable but not obviously at the standard of this conference.",
            "summary_of_the_review": "Interesting contribution to a much studied field which makes incremental progress. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2827/Reviewer_cBoH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2827/Reviewer_cBoH"
        ]
    },
    {
        "id": "f3Oop0P1Bly",
        "original": null,
        "number": 3,
        "cdate": 1666840946687,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666840946687,
        "tmdate": 1666840946687,
        "tddate": null,
        "forum": "j6zUzrapY3L",
        "replyto": "j6zUzrapY3L",
        "invitation": "ICLR.cc/2023/Conference/Paper2827/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce an energy-constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances\u2019 information by their interactions. In their method, the diffusion process is constrained by descent criteria w.r.t. a principled energy function that characterizes the global consistency. Experiments show that the model could achieve superior performance in various tasks.",
            "strength_and_weaknesses": "Strength:\n\nIn this paper, the authors proposed a geometric approach to model sample points on the data manifold through diffusion on graphs. The proposed approach and methodology are interesting.  Moreover, the method achieved improved performance on multiple datasets. \n\n\nWeakness:\n\nThe author could give more detail about the difference between the proposed method and label propagation used for semi-supervised learning.  How does the data sample graph affect the performance of the method? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was well written. Additional discussions could make the paper stronger.  E.g., could the authors also compare the running time for different methods on the tasks in sections 5.3 and 5.4?",
            "summary_of_the_review": "The method proposed in the paper is novel, and the writing and structure of the paper are good. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2827/Reviewer_tDZ1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2827/Reviewer_tDZ1"
        ]
    },
    {
        "id": "2AuR84kA2gH",
        "original": null,
        "number": 4,
        "cdate": 1667238985358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667238985358,
        "tmdate": 1667239782674,
        "tddate": null,
        "forum": "j6zUzrapY3L",
        "replyto": "j6zUzrapY3L",
        "invitation": "ICLR.cc/2023/Conference/Paper2827/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a theoretical framework which leverages a diffusivity function $\\mathbf S_{ij}$ to capture interdependencies in input data. In the extremes, one can consider an IID assumption - in which case the diffusivity function is just the constant identity matrix, and the model simplified to an MLP - or a setting where the inputs are vertices in a known graph - in which case using a diffusivity function $\\mathbf S_{ij}$ with nonzero weights between edges $(i,j) \\in \\mathcal E$ can result in either a GCN or GAT, depending on the choice of weights.\n\nBeyond incorporating existing architectures in a unified framework, this perspective also suggests that the diffusivity function can be computed based on the input features. The authors introduce the crucial idea of minimizing an energy function which measures the quality of instance states at a given step, and then prove the existence of a diffusivity function which guarantees that the energy of the instances will be minimized in each step. This result has one remaining design decision - the specification of a non-negative decreasing function - and the authors explore two obvious choices for this function, leading to a \"simple diffusivity model\" DIFFormer-s which is linear in terms of the number of inputs, and an \"advanced diffusivity model\" DIFFormer-a, the latter of which is capable of capturing more complex latent geometry but has quadratic complexity in the number of inputs.\n\nA large array of experiments, both synthetic and on real data, are included, with an array of strong baselines. The authors also perform a comprehensive ablation study to assess various aspects of the design decisions.",
            "strength_and_weaknesses": "This paper is very strong in many respects. The theoretical framework fits well in backdrop of a larger ongoing effort to unify various machine learning models. The motivation and presentation is very clear throughout, both from a technical/mathematical sense as well as general writing style. The theorem which is proved is not just a nice-to-have, but rather fundamental to the solution, and the authors emphasize this in their presentation beautifully. I have no major weaknesses to report.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above, I found the writing very clear. I think the work is of the highest quality, is sufficiently novel, and felt the model was explained in enough technical detail to be reproduced from the paper alone. (The PyTorch-style pseudo-code is a welcome addition in the appendix, but I assume the authors will also release their code for complete reproducibility.)",
            "summary_of_the_review": "This was an outstanding work, very polished and complete. Definitely ready for publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2827/Reviewer_Hw3e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2827/Reviewer_Hw3e"
        ]
    }
]