[
    {
        "id": "TbQOXYF2dPi",
        "original": null,
        "number": 1,
        "cdate": 1666280778489,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666280778489,
        "tmdate": 1666280778489,
        "tddate": null,
        "forum": "tx-KRrFC2b",
        "replyto": "tx-KRrFC2b",
        "invitation": "ICLR.cc/2023/Conference/Paper3509/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper looks at the problem of applying offline RL to learning solutions to games. The complication that arises is that the demonstration data may not exhibit a solution to the game, and as we're offline we cannot collect new equilibrium demonstrations. This question that is investigated in this paper is exactly this: how can one learn an equilibrium in an offline fashion. They propose an algorithmic solution that is a simple two-step approach: learn a game-model, and apply a game-solving algorithm on the game-model as an approximation solution to the underlying game. They empirically demonstration their approach on several small games. ",
            "strength_and_weaknesses": "**Strengths**\n - Their approach is intuitively simple, in a good way, and sensible for solving a real problem. \n - The authors include a nice set of diverse baselines to understand the limitations of most current approaches including BC and offline model-based RL, alongside combinations of them. \n - The authors perform a nice sensitivy analysis to the quality of the offline data across all methods. \n - The problem is introduced and framed in English well such that a reader from any of the diverse positions of background could understand well the main point they're investigating. Despite this, I would recommend condensing this as it resulted in less technical details from being included in the main body of the paper. \n\n**Weaknesses**\n - I do not believe they've correctly positioned this paper within the literature. This work is an example of an EGTA algorithm sat at the extrema of the algorithmic design space: a high-fidelity game-model is constructed alongside allowing no further queries to the simulator (that is exact). The authors claim that EGTA is precluded from being used in this case, but at the core of their work is a game-model that's being solved through game-theory, which is precisely EGTA. I am further skeptical of the claim \"offline RL is not enough since it can only learn the best strategy for one agent independently\", could the authors explain or cite evidence as to why this must be true? Moreover, I'm surprised at the lack of discussion on behavioural cloning. If the demonstration data is exactly displaying an equilibrium then there is no issue; whereas, the more off-equilibrium the demonstration data, the more this approach matters alongside the difficulty of the correction.\n - The abstract claims a formal introduction to the proposed problem of offline equilibrium finding; however, the text contains only an informal one-sentence definition. The paper could be improved by formally defining the problem and characterizing interesting properties or questions within the space (e.g., approximation errors, selecting a single equilibrium when many exist, etc.).\n - The success of their method (game-model, then solving) strongly depends on the state-action space covered during demonstration versus the space covered by the desired solution. This is because the game-model cannot reasonably exhibit entirely novel transitions. I raise this point because in the games demonstrated in this paper it is possible that the state-action space is exceptionally well covered. I would be curious to know how this approach works outside of this domain and wonder if the authors have investigated this at all.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper offers a novel approach to game-solving without any access to a simulator. I have not seen previous work in this space using modern machine learning techniques but would be surprised if this is unstudied in the field of Game Theory.\n\nThe paper contains a sizeable appendix that provides details to parts of their implementation. However, there are many moving parts across all the different ablations and baselines, and I suspect without releasing their code the results could not be reproduced. ",
            "summary_of_the_review": "This work demonstrated that learned game-models can be solved as a proxy for the true game in an offline setting (wrt. the true game). The paper is framed as formally introducing a new problem (ie., offline equilibrium finding), but I found this component of the exposition absent. Moreover, the empirical results are all in an exceptionally idealized setting for the method: the equilibrium state-action is likely covered by any arbitrarily bad demonstration profile. Despite these concerns, the work contains a straightforward/trivial (in a good way) approach to a reasonable problem and provides a nice set of preliminary experiments in the space. \n\nIn its current state due to unsupported/incorrect claims and misaligned problem framing I would not advocate for its publication, but with moderate revisions to the text believe it could be ready.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3509/Reviewer_APEF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3509/Reviewer_APEF"
        ]
    },
    {
        "id": "8qtskp9HYGL",
        "original": null,
        "number": 2,
        "cdate": 1666412992125,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666412992125,
        "tmdate": 1666412992125,
        "tddate": null,
        "forum": "tx-KRrFC2b",
        "replyto": "tx-KRrFC2b",
        "invitation": "ICLR.cc/2023/Conference/Paper3509/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose offline equilibrium finding, which can essentially be summarized as a method of equilibrium finding in unknown games using only a training set of gameplay--by first building an approximate model of the game from the training set and then computing an equilibrium of the model. Experiments are run to demonstrate the efficacy of the method.",
            "strength_and_weaknesses": "I find the premise of the paper interesting--offline RL is of obvious interest, and applying it to games causes additional issues as the authors rightfully point out. For example, the fact that independent offline RL (e.g., the tested algorithms MOPO and BAIL) fails in this setting is fairly clear: given strategy profile $\\sigma$ in an extensive-form game, even if each player $i$ builds a perfect environment (i.e., game + opponent) model and independently computes a perfect strategy in that model (i.e., a best response $\\sigma_i'$ to $\\sigma_{-i}$), it would be quite surprising if somehow the profile $\\sigma'$ turned out to be close to equilibrium. Thus, other methods are required.\n\nMy main issue is that the paper is very light on theory and new ideas. I concede that to some extent this is my own theoretical background seeping through, but in any case I found the technical section of the paper to be straightforward. It seems to boil down to: \"to solve a game you don't know, first approximate the game and then solve the approximated game\", applying known techniques in turn to each of the two subproblems.\n\nThe experimental section of the paper is fairly robust, but basically verifies what one would expect from the methods tested, and to me contained no surprises. I would also like to see larger experiments--perhaps in a game large enough that directly computing an equilibrium is hard, and in which the BC+MB method outperforms pure BC. \n\nQuestions:\n\n1. Why should we expect a Fourier transform of node frequencies to be illuminating? Are the nodes ordered in some meaningful way such that, say, the \"amplitude at frequency 2\" is a meaningful thing?\n\n1. I did not understand Figures 4(h) and 5(h). In particular, what's a \"player\" for those figures? Aren't Kuhn and Leduc both two-player games? \n\n1. In the experiments in Section 5.4, how was a Nash equilibrium computed for the purpose of collecting the OEF dataset? The games seem too large to use an actual algorithm that is guaranteed to compute general-sum Nash.\n\n1. The experiments in the paper are all with approximate equilibrium finders (namely, based on deep learning), despite the fact that some of the games are small enough that better equilibrium finders (e.g., tabular CFR+) could have been used instead. Given this, to what extent are the experimental observations dependent on the quality of the equilibrium found vs. the quality of the approximated game model? For example, the NashConv values in (2p) Leduc poker seem rather large, and my first guess would be that this is due to equilibrium finder failure--Leduc poker is a fairly simple game, so I would guess that 20000 data points is plenty to build a decent game model.\n\n1. How is $\\alpha$ selected for BC+MB? In particular, you state that $\\alpha$ is selected by \"testing the final policies in a real game to get the best final policy\"--against what opponent(s) were the policies optimized? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written, and seems to solve a new and interesting problem. The main ideas of the paper, though, are quite straightforward, and as such I consider the novelty of the paper to be its weakest point.\n\nCode to reproduce experiments is provided. I have not run the code.",
            "summary_of_the_review": "An interesting concept, but I'm not sure of novelty, as the main ideas of the paper are fairly natural and not analyzed very deeply. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3509/Reviewer_2VY7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3509/Reviewer_2VY7"
        ]
    },
    {
        "id": "Oe29tyf5gVw",
        "original": null,
        "number": 3,
        "cdate": 1666561999130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561999130,
        "tmdate": 1669651396326,
        "tddate": null,
        "forum": "tx-KRrFC2b",
        "replyto": "tx-KRrFC2b",
        "invitation": "ICLR.cc/2023/Conference/Paper3509/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduced a new problem called offline equilibrium finding and designed a model-based method to apply any online equilibrium finding algorithm to the OEF setting.",
            "strength_and_weaknesses": "Strength\n1. The problem is relatively new and the method is novel.\n2. The authors gave a thorough literature review of the equilibrium-finding algorithms and give a sensible discussion on why introducing the offline equilibrium finding. \n3. The experimental results are also extensive.\n\nWeakness\n1. The proposed method is model-based but the environment model is hard to learn. Specifically, the dynamics of the game environment may be time-variant and stochastic and cannot easily be characterized by a DNN proposed by the authors. It is worth exploring how the OEM performs when the model is perfectly known in simulation and how its performance will be degraded when the model is not well learned.\n2. In Fig 4(h), why does the NashConv not exist for MB when the number of players is 2 and 3 and the same issue for BC when the number of plays is 4 and 5? Similar question for MB when the number of players is 3 in Fig 5(h).\n3. The parameters used in the OEM are only briefly discussed in the Appendix (Tables 1 for BC and 2 for the model). It will be helpful to conduct some ablation study on these parameters and demonstrate how the performance is affected by these parameters.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear and of high quality.\nThe problem and model are novel.\nThe experimental setup is too complex to reproduce the results. The results are only reproducible with Opensourced codes.",
            "summary_of_the_review": "Considering the high quality and novelty of the paper, I would recommend the acceptance of this work.\n\n----------\nI have read the authors' responses and would like to keep the same score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3509/Reviewer_XR74"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3509/Reviewer_XR74"
        ]
    },
    {
        "id": "zQyIuakgLX",
        "original": null,
        "number": 4,
        "cdate": 1666869626281,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666869626281,
        "tmdate": 1666870132725,
        "tddate": null,
        "forum": "tx-KRrFC2b",
        "replyto": "tx-KRrFC2b",
        "invitation": "ICLR.cc/2023/Conference/Paper3509/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies offline multi-agent reinforcement learning. The authors propose a framework for offline equilibrium finding, where the learner first trains a model $E$ by the offline dataset, and then compute an equilibrium using the model and some proper online algorithms. The authors also consider to set the behavior cloning policy as the regularization for the online equilibrium.",
            "strength_and_weaknesses": "Strength: The paper focuses on offline multi-agent reinforcement learning, which is a very challenging problem in related fields. The authors consider model-based framework for finding the equilibrium, which is even more challenging, since estimating the model in multi-agent learning problems is essentially hard.\n\n\n\nWeaknesses:  1) The motivation is not clear. In the example in Section 2, by watching the replays of the other player $B$, the learner $A$ could only find the best response to $B$'s policy, instead of an equilibrium. I am confused that how to connect this scenario with equilibrium finding. 2) The contribution in algorithm design is incremental, where no useful insights about multi-agent learning is provided.  3) It is unfair to compare the proposed algorithm with the offline RL algorithms for single agent, since the target of those algorithms is not to find an equilibrium. 4) This paper is lack of theorectical insights. It would be much better by providing the sample complexity analysis.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation should be improved. For example, the notation of state $s$ appears in Section 4.1 for the first time, but it is not formally defined in the preliminaries. Also it should emphasized that whether $a$ is the joint-action.",
            "summary_of_the_review": "Overall I think this paper is incremental, and the motivation is not clear. I consider to reject this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3509/Reviewer_ZPQh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3509/Reviewer_ZPQh"
        ]
    }
]