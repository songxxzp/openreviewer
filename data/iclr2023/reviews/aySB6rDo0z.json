[
    {
        "id": "QBVcjZl8LmM",
        "original": null,
        "number": 1,
        "cdate": 1666602132145,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602132145,
        "tmdate": 1666656709714,
        "tddate": null,
        "forum": "aySB6rDo0z",
        "replyto": "aySB6rDo0z",
        "invitation": "ICLR.cc/2023/Conference/Paper4089/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an algorithm called CSVE, which imposes penalization on states rather than state actions used in CQL. By doing such, we may benefit from learning a better conservative value since learning a conservative Q-value over joint state-action space is more challenging. The paper provides theoretical results that are improved from that of CQL. Although it requires learning a model to learn, the experiment results suggest that CCSVE overall performs better than CQL.",
            "strength_and_weaknesses": "Strength\n- Theoretically well supported\n- Quite an original idea and algorithm design\n- Descent performance\n\nWeakness\n- Requires model:\n\nWhen compared to other model-free methods, CSVE requires estimating dynamics as well, so it may not be very fair to compare to other model-free algorithms. Since the authors argue that the proposed method is not very sensitive to model bias, it would be also interesting to see how CSVE is affected by varying model bias.\n\n- Not enough interpretation on why the suggested algorithm is good:\n\nIt is kind of vague in the paper why suggested algorithm works well when compared to CQL. Is it really easy to learn pessimistic V value instead of Q? Can it be shown as an example? Or is V-learning algorithm is just better than Q-learning in these domains, even in the online setting? It is hard to see the clear reason from the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Both the technical and presentation quality of the paper is good, and the clarity is also OK. However, there are some typos in the paper. The algorithm suggested in the paper is also quite original.",
            "summary_of_the_review": "Overall, I enjoyed reading this paper and I think there will be a number of readers interested in this paper. There are quite a few typos in the papers, and I would like to see more empirical results (e.g. when the model is incorrect), but overall I recommend acceptance of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4089/Reviewer_zsLW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4089/Reviewer_zsLW"
        ]
    },
    {
        "id": "X1WZvHCVuj1",
        "original": null,
        "number": 2,
        "cdate": 1666665433159,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665433159,
        "tmdate": 1666665433159,
        "tddate": null,
        "forum": "aySB6rDo0z",
        "replyto": "aySB6rDo0z",
        "invitation": "ICLR.cc/2023/Conference/Paper4089/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "A major challenge in offline RL is the distribution mismatch between the behavior policy generating the offline data and the target policy we want to learn. Such a mismatch can result in an overestimation error in the Bellman update, which can further result in divergence. The paper proposes to learn a conservative state value estimate (in contrast to the existing CQL, which learns conservative action-value estimates). The conservative state-value estimate is learned by subtracting a term from the regular Bellman backup. The term is proportional to the density ratio of the target and behavior distribution. The main theory shows the bound of the expected state value (i.e., expectation over states) under both the offline data distribution and the target policy distribution. In algorithmic design, the conservative state value estimate is used to learn a critic in an actor-critic algorithm. Experiments on Mujoco domains are conducted to show its effectiveness. \n",
            "strength_and_weaknesses": "Strength: \n\n1. The paper studies an interesting topic which could be practically useful to a broad range of RL applications. \n\n2. The paper has a reasonably clear presentation. \n\nWeaknesses: \n\n---------\n1. The basic idea is incremental to CQL. The actions can be marginalized from the CQL\u2019s bound to get the state value bound. But the paper does not discuss any connections. \n\nOn close examination, both the theoretical result and algorithmic design are highly similar to CQL.\n\n---------\n2. The theorems are not informative. First, the theoretical results are bounding E[V(s)], but CQL provides a bound for any state-action pair. Bounding E[V(s)] does not give a guarantee for any state value estimate\u2019s conservativeness.  \n\nSecond, before showing the bound, shouldn\u2019t the convergence be shown first? To me, it is even unclear if the proposed conservative state value Bellman update would converge or not. \n\n---------\n4. Experiments. I expect to see clear evidence to show at least three things: \n1). The necessity of learning a conservative state value, rather than action-value; \n2). The the effectiveness of using the proposed method to learn conservative state value, rather than other methods (for example, using CQL and use the learned policy to get a state value estimate); \n3). The comparison to IQL (Offline RL with implicit q learning by Ilya et al.), which has a similar high-level idea. IQL attempts to avoid overestimation by avoiding learning action values. ",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above section. ",
            "summary_of_the_review": "I have limited time to review the paper. But to the best knowledge, the current weaknesses of the paper outweigh the strength. I will refer to the author's response and other reviews and adjust my score accordingly. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4089/Reviewer_pQB8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4089/Reviewer_pQB8"
        ]
    },
    {
        "id": "qtKc0URwv4r",
        "original": null,
        "number": 3,
        "cdate": 1666907717757,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666907717757,
        "tmdate": 1666907762002,
        "tddate": null,
        "forum": "aySB6rDo0z",
        "replyto": "aySB6rDo0z",
        "invitation": "ICLR.cc/2023/Conference/Paper4089/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new algorithm--CSVE--that learns conservative estimates of state value functions by penalizing values of OOD states. The authors prove that the estimated state value functions are lower-bounds of the true value in expectation over any state distribution. Finally, the authors evaluate their algorithm against state-of-the-art offline RL baselines. ",
            "strength_and_weaknesses": "Strengths:\n\n(1) The paper has a clear organization and is well-written.\n\n(2) CSVE achieves impressive empirical performance against strong offline RL baselines.\n\nWeaknesses:\n\n(1) The algorithm itself is very similar to existing conservative offline RL algorithms, particularly CQL and COMBO. To my understanding, the authors do demonstrate theoretical and empirical advantages over CQL but not COMBO. In my opinion, COMBO is more similar as both CSVE and COMBO are model-based in the sense that they require learning a dynamics model.\n\n(2) Though the backbone of the CSVE algorithm is clear, the authors add several extensions such as learning an ensemble of transition models (which COMBO does not do), and using a separate policy extraction step rather than alternating value- and policy-improvement as CQL, COMBO do. It is not clear from the empirical analysis whether those changes are important to the empirical performance, or the fact that the penalty is over states rather than state-action pairs (which is the central novelty of the proposed algorithm). The authors should consider such ablations, as well as a comparison to COMBO, in the experiments to make their results more compelling.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and organized well. I am primarily concerned with the novelty of the contribution, as it is very similar to existing algorithms as CQL or COMBO. ",
            "summary_of_the_review": "To my understanding, the key idea of the proposed CSVE algorithm is to consider a penalty over states rather than state-action pairs. However, CSVE also consider some differences in algorithm design (see (2) in weaknesses) to similar baselines, so it is difficult to judge how important this key idea is to the improved performance. I believe the paper could be drastically improved by removing theses extensions as ablations, as well as considering COMBO as an additional baseline. I feel that CSVE and COMBO are much more closely related than CSVE and CQL, as the former are both model-based algorithms. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4089/Reviewer_KLUN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4089/Reviewer_KLUN"
        ]
    },
    {
        "id": "EQJ13A_0rBL",
        "original": null,
        "number": 4,
        "cdate": 1667118927368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667118927368,
        "tmdate": 1667118927368,
        "tddate": null,
        "forum": "aySB6rDo0z",
        "replyto": "aySB6rDo0z",
        "invitation": "ICLR.cc/2023/Conference/Paper4089/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a CQL-like method for penalizing out-of-distribution states in offline RL. The method first describes an approach for off-policy evaluation with OOD state penalization, and then incorporates into the training pipeline for Q-functions, and then uses it for offline RL.",
            "strength_and_weaknesses": "Strength: The method is clearly intuitive and a good idea to do, so that's a plus.\n\nWeaknesses:\n\n- Many design decisions, no clear explanation: There are many design decisions in the paper that have not been ablated or explained in more detail -- for example, why AWR, and not just standard SAC/CQL-like policy extraction? why learn separate V(s) and Q(s, a), and not just directly penalize Q(s, a) on OOD states? \n\n- Missing comparisons: the method uses a dynamics model to obtain new states, yet the method doesn't compare to COMBO? Seeing the resutls, seems like COMBO would perform better than this, which means that the results are not significant...\n\n- Novelty: It seems like if the Q(s, a) were directly penalized on OOD states, and policy actions, the method in this paper would just be COMBO identically, and indeed COMBO is just a more generalization of this approach. So, unless the choice of V(s) can be justified rigorously, I don't think this paper provides a novel method.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clear, but there are quite a few typos, etc, which can be fixed. Novelty: I don't think the paper is novel. I think that the method in the paper is of high quality, but it already exists in prior work if I am understanding properly. ",
            "summary_of_the_review": "In all, I am not convinced if this paper is doing anything more than COMBO. Moreover, the paper doesn't compare to COMBO, and doesn't justify design decisions. In all, this makes me think that the paper is not yet significant for the community, and must address these issues. Thus, I opt to give a reject score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4089/Reviewer_uGHe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4089/Reviewer_uGHe"
        ]
    }
]