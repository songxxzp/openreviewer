[
    {
        "id": "VusKDt5mn4E",
        "original": null,
        "number": 1,
        "cdate": 1666681919699,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681919699,
        "tmdate": 1670629130357,
        "tddate": null,
        "forum": "IJwhRE510b",
        "replyto": "IJwhRE510b",
        "invitation": "ICLR.cc/2023/Conference/Paper8/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors repurpose the well-known knowledge distillation paradigm, which is typically used for improving model accuracy, for the task of transferring the capability of reducing negative flip rate (NFR) from an ensemble to a single model. To this end, the authors devise a method called Ensemble Logit Difference Inhibition (ELODI). ELODI leverages the advantages of deep ensembles for NFR reduction at the running cost of a single model by (1) training deep networks using a Logit Difference Inhibition (LDI) loss with respect to a reference ensemble and then (2) using the resulting single model to perform inference.",
            "strength_and_weaknesses": "Strengths:\n\n* ELODI trains a deep homogeneous ensemble that can achieve Positive-Congruent Training (PC-Training) performance and be distilled to a single model (which does not necessarily need to be a specific legacy model). This allows the proposed ELODI method to reduce NFR while retaining the accuracy gain introduced by the new model.\n\n* The authors introduce Logit Difference Inhibition (LDI), a knowledge distillation objective that penalizes significant differences between the logits of the reference homogeneous ensemble and the single student model. Such an objective allows for effective distillation of the reference ensemble to a single student model.\n\n* ELODI exhibits efficient inference as it does not require the homogeneous reference ensemble to be evaluated at inference time but rather only the single student model.\n\n* The choice of homogeneous ensembles as reference ensembles is empirically well supported by the findings summarized in Figure 4. Moreover, the hypothesis of reducing logit displacement magnitude (on which the introduced LDI objective relies) is validated empirically in Section 3.2.\n\n* The authors have conducted extensive experiments on multiple image classification benchmarks to assess the effectiveness and efficiency of ELODI in comparison with several PC-training and ensemble methods. The experimental results demonstrate that ELODI performs positive congruent training by reducing NFR with large logit displacement and reducing the variance of the ensemble\u2019s logits. It is also empirically demonstrated that this behavior can be transferred to a single model which considerably reduces the inference time. For instance, this allows ELODI to achieve comparable ER-NFR performance to that of the ensemble paragon [Yan et al., 2021] at the inference cost of a single model.\n\n* The authors have conducted a plethora of additional experimental studies to assess the effect of various aspects including:\n\n    (1) model update settings (including fine-tuning on other datasets, transitivity of NFR reduction induced in chain of model updates, and ELODI\u2019s applicability to updates across dissimilar architectures);\n\n    (2) design choices (including the effects of using homogeneous instead of heterogeneous ensembles for guidance, changes in the guiding ensemble\u2019s architecture, and different choices of distillation loss functions);\n\n    (3) exploratory parameter analysis assessing the effects of the loss weight $\\alpha$, the size of the reference ensemble as well as the manner in which the logits are inferred (offline compared to online).\n\n-------------------------------------------------------------------------------------------------\n\nWeaknesses:\n\n* At the end of the introductory paragraph of Section 3, the authors claim that \u201cthe averaging operation in deep ensembles is also often performed in the logit space (Lakshminarayanan et al., 2017), which makes it interesting for our study.\u201d Nevertheless, I am not confident how often this is the case, namely because in classification settings voting schemes are typically used upon applying the activation function of choice (e.g., softmax) to the logits estimated by each member model rather than averaging the logits. I would encourage the authors to further clarify their motivation behind the choice of the averaging aggregation scheme in the logit space, or consider revising the aforementioned statement in the paper.\n\n* The assumption that the logits outputted by the base members of a homogeneous ensemble follow a normal distribution is a fair assumption to make. This is also empirically supported in Section 3.2. However, [1] suggests that, for this to hold, the base networks need to be trained in a sufficient number of epochs. Therefore, including a brief discussion in Section 3.2 on the relationship between the number of training epochs and the members\u2019 logit distributions would be quite insightful. In addition, I believe that [1] is relevant to this paperr as it discusses logit distributions of deep neural networks and thus I would suggest that the authors consider including it as a part of their related work.\n\n    [1] Seguin, L., Ndirango, A., Mishra, N., Chung, S., & Lee, T. (2021). Understanding the Logit Distributions of Adversarially-Trained Deep Neural Networks. arXiv preprint arXiv:2108.12001.\n\n* In Section 3.2, while discussing Figure 3a, the authors state the following: \u201cIf we move to the ensemble case, the logit displacement follows another normal distribution with scaled co-variance matrix\u201d. However, according to Figure 3a, apart from the scaled co-variance matrix, the mean is shifted. This should also be included in the same discussion.\n\n-------------------------------------------------------------------------------------------------\n\nMinor weaknesses:\nThere are also certain grammatical and typographical errors and remarks that require attention. Some of them are summarized as follows:\n- In the \u201cContributions\u201d paragraph on page 2, \u201cthe\u201d should be removed from the phrase \u201cobtain the their performance for PC-training\u201d. Later in the same paragraph, the hyphen should be removed from \u201cwith re- spect to\u201d.\n- Section 3.1, at the end of page 3: In the phrase \u201cEach $\\phi(x)$ or $\\phi(x)$\u201d, I believe that one of the $\\phi(x)$\u2019s should be corrected.\n- In the second paragraph of Section 4.1, replace \u201can model pair\u201d with \u201ca model pair\u201d.\n- Section 4.2: In the sentence right after Eq. (6), it is mentioned that \u201cdifference below $\\xi$ is tolerated\u201d; however, according to Eq. (6) it seems that a difference below $\\xi$ is truncated, i.e. not tolerated. Later in that same paragraph, \u201cwhen number of classes are extremely largencated\u201d should be corrected to \u201cwhen the number of classes is extremely large\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is well written, organized, and technically detailed. The used notation is clear and consistent throughout the paper.\n\nQuality:\nThe design and justifications for the proposed Ensemble Logit Difference Inhibition method are technically sound. The observations made regarding the reduction in NFR and inference cost introduced by the proposed method as well as its capability to maintain image classification accuracy gains are empirically well supported. Overall, the work seems to be well developed and mature in terms of quality.\n\nNovelty:\nFrom a methodological perspective, the contribution of this work can be considered incremental since it is built upon the well-established knowledge distillation framework that utilizes the proposed simple yet well-crafted Logit Difference Inhibition (LDI) objective. Nevertheless, the authors provide an extensive analysis on the role of deep ensembles in a knowledge distillation scheme which sheds light on their capability of reducing NFR and obtaining performance for PC-training with a single model, at the running cost of a single model. To the best of my knowledge, this is the first work to conduct such an extensive analysis regarding the aforementioned points.\n\nReproducibility:\nThe experiments were conducted on widely-used image classification benchmark datasets which are publicly available. On the other hand, the code for the proposed ELODI method is not made available by the authors in the present anonymized version, however, the architecture of the method is clearly explained in the paper, thus it is safe to say that one should be able to implement ELODI by following its detailed description in Section 4.",
            "summary_of_the_review": "This paper leveraged deep ensembles for NFR reduction and showed that they can be distilled into single models by means of a Logit Difference Inhibition (LDI) objective. Despite the methodologically incremental contribution, the proposed ensemble method is capable of reducing NFR while maintaining accuracy and can be distilled to a readily deployable single model which exhibits much lower inference time compared to the deep ensemble. The latter in particular makes this work applicable to large-scale real-world systems where online inference and low latency are paramount. Overall, although there are certain weak points that need to be addressed (listed in the \u201cWeaknesses\u201d part of this review), the strengths of this work certainly outweigh its weaknesses. Therefore, I consider this paper to be a fairly good fit for ICLR and I recommend that it is accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper8/Reviewer_uaPa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper8/Reviewer_uaPa"
        ]
    },
    {
        "id": "K7HS1jjN4K",
        "original": null,
        "number": 2,
        "cdate": 1666717568027,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717568027,
        "tmdate": 1666717568027,
        "tddate": null,
        "forum": "IJwhRE510b",
        "replyto": "IJwhRE510b",
        "invitation": "ICLR.cc/2023/Conference/Paper8/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors tackle the problem of updating existing models in production-scale\nsystems while performing Positive-Congruent Training (minimize negative flip\nrate and error rate simultaneously). To do so, they make two key observations:\n\n- Previous studies have demonstrated that ensembles can reduce NFR without\n  negative accuracy impact.\n- Using ensembles may not be a viable strategy in real applications due to\n  computational costs.\n\nBased on these observations, the authors propose ELODI, a novel model\ndistillation method that seeks to achieve PC-Training performance of ensembles\nusing a single model. First, they analyze how the distribution of logit spaces\nof both homogeneous and heterogenous ensembles behave and conclude that, as\nensembles become larger, NFR consistently decreases. This is further validated\nwith experiments. Next, they propose a novel, margin-based distillation loss\nfunction, namely LDI, that attempts to mimic the variance reduction phenomenon\npresent in ensembles. The authors perform extensive experiments and conclude\nthat ELODI has similar performance (NFR and ER) to an ensemble method with\nreduced inference costs.",
            "strength_and_weaknesses": "*Strengths:\n\n- The paper is well-motivated, since the problem the authors present frequently\n  occurs in production systems.\n- The problem of negative flips in the context of ensembles is clearly\n  explained in Section 3 and further details the experiments in (Yan et al.\n  2021).\n- The experimental section compares the results of ELODI with several other\n  state-of-the-art techniques.\n\n*Weaknesses:\n\n- ELODI works mainly for homogeneous ensembles. This goes against one of the\n  biggest advantages of using ensembles, which is to combine models with\n  different architectures.\n- Training a model using ELODI requires one to first train an ensemble and, as\n  the authors themselves point out, the number of models in the ensemble\n  drastically impacts the quality of the distilled model.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written and easy to follow.\n- There are minor typos throughout the text (e.g., \"res- pect\").\n- Figure 1 is in page 2, but is referenced for the first time in page 5.\n- Figures 3 and 4 are somewhat hard to understand, since there is too much\n  content packed into very little space.\n- Algorithm 1 seems unnecessary, given that the procedure is explained\n  throughout the text.\n- The novelty of the paper is the distillation loss function targeted at\n  approximating the logits between a new model and an ensemble. The authors\n  compare the proposed loss with several baselines and conclude that it gives\n  better results.\n- The paper seems to be easy to reproduce (hopefully the authors will publish\n  their source code).",
            "summary_of_the_review": "The authors propose ELODI, a distillation technique that reaps both the\nbenefits of ensembles to reduce NFR and ER and distillation to mimic the\nbehavior of the said ensemble using a single model. Although mainly limited to\nhomogeneous ensembles, ELODI has the potential to positively affect deep\nlearning model deployment in production.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper8/Reviewer_V2YQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper8/Reviewer_V2YQ"
        ]
    },
    {
        "id": "3ov0UrDZoJJ",
        "original": null,
        "number": 3,
        "cdate": 1666753390377,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666753390377,
        "tmdate": 1666753390377,
        "tddate": null,
        "forum": "IJwhRE510b",
        "replyto": "IJwhRE510b",
        "invitation": "ICLR.cc/2023/Conference/Paper8/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method in the area of Positive-Congruent Training, by distilling a homogeneous ensemble to a single student model. The authors carefully analyze the properties of logic displacement magnitude in ensembles, and present a method called Ensemble Logit Difference Inhibition (ELODI). The proposed approach is validated on two standard image classification datasets.",
            "strength_and_weaknesses": "Strength\n\n- The analysis on the properties of the distribution of ensemble predictions is interesting, and the simulation studies are clear and supportive of the findings. \n- The algorithm is well presented with detailed follow up discussions. \n\nWeakness\n- I am not super clear on the motivation of the proposed method. The loss function of the distillation algorithm is a key part but it's basically just cross entropy plus a term to panelize l_2 difference in the logit. Despite being used for model updating, are there any other benefits can be discussed, such as smoothing the landscape of the student model? It seems to me using and additional LDI term could have some alternative motivations, and the use case seems a bit unnatural to me. \n- Although the discussion on the properties of ensemble prediction distributions are empirically correct, can the authors provide more discussions in this area? For example, where is the randomness coming from? Initialization, randomness in training algorithms, etc? For trees, there have been some statistical results (see for example https://arxiv.org/pdf/1510.04342.pdf, https://www.jmlr.org/papers/volume17/14-168/14-168.pdf?ref=https://githubhelp.com). I wonder whether some primitive results can be discussed here.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is written clearly. I feel the novelty of the approach is a bit limited since the key part is on modifying the loss function used in knowledge distillation; the part on the distribution of ensemble distributions is only empirically without any theoretical supports.\n\nSome minor things: \n\n- Figure 2 the blue color is not accurate when laid over by the orange. \n- Better to write down exact formulation of L_CE in page 6.",
            "summary_of_the_review": "This paper introduces a new knowledge distillation method with the goal to reducing NFR. Although the analysis and the methods are well presented, I feel the novelty of the approach is a bit limited since the key part is on modifying the loss function used in knowledge distillation; and the part on the distribution of ensemble distributions is only empirically without any theoretical supports.\n\nThat being said, I think if the authors can provide a better explanation on the benefits of the proposed loss function, and provide more theoretical discussion on the prediction distributions of ensembles, the paper can be much improved. \n\nI mark the paper as marginally below the threshold, but also note I am not familiar with this specific application area. So I am willing for further discussions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper8/Reviewer_38Tg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper8/Reviewer_38Tg"
        ]
    },
    {
        "id": "phCn3MpAiO",
        "original": null,
        "number": 4,
        "cdate": 1667556939042,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667556939042,
        "tmdate": 1667556939042,
        "tddate": null,
        "forum": "IJwhRE510b",
        "replyto": "IJwhRE510b",
        "invitation": "ICLR.cc/2023/Conference/Paper8/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the task of updating a model by learning a stronger model while ensuring that formerly correctly classified instances will get wrong predictions as seldom as possible, a quantity called a negative flip rate (NFR). The paper starts with a theoretical analysis of using ensembles in this context. It then proposes a new training objective to distill a single model based on an ensemble, with the goal of minimizing NFR. Experiments on image datasets ImageNet and iNaturalist show the effectiveness of ELODI compared to other NFR-reducing methods.",
            "strength_and_weaknesses": "Strengths:\n* Theoretical analysis of the differences of average logits in ensembles;\n* The paper is mostly clearly written, with plenty of insightful figures and tables;\n* The proposed method ELODI is effective in reducing NFR while mostly retaining accuracy.\n\nWeaknesses:\n* At the end of Section 4, it is stated that the logit matching loss (Hinton et al., 2015) is a special case of the proposed LDI term. Therefore, I find it absolutely necessary to do an ablation study to see whether the proposed method improves over using the plain logit matching loss. That is, the paper should have experimental comparisons of $\\xi=0.1$ against $\\xi=0$. Currently there is no such comparison in the paper (nor in the appendices).\n* The theoretical part first starts by making claims about any particular fixed $x$, while considering the training process (or training seed) as a source of randomness, hence the random variable $\\phi^{(i)}(x)$. This nicely gives results stated by Eqs. (1), (2) and (3). However, in Section 3.2 there seems to be a shift to consider $x$ also as a random quantity, because there is a single Gaussian distribution about the whole architecture, e.g. see the last paragraph of page 4 with $\\phi\\sim\\mathcal{N}(\\mu_1,\\Sigma_1)$. This is confusing and should be clarified. If there is now indeed a single Gaussian per architecture, what is the theoretical justification behind that? If there is still only a Gaussian per instance, then how are the covariance matrices estimated for Figure 3?\n* The first variance term $Var(\\hat{\\phi}(x))$ in Eq.(5) does not make sense to me. As minimization is over $\\hat{\\phi}(x)$ and thus this quantity is bound within the expression to be minimized, then what is the random variable that the variance operator is applied to?\n\nMinor weaknesses:\n* From the introduction it is not quite clear whether the term positive-congruent training is novel or not;\n* The text about Figure 2 suggests that particularly the negative flips with large cross-model logit displacement are reduced by ensembles, highlighting the percentage of 90.7% in the red box. However, it would then also be important to provide the respective percentage outside the red box. Is it significantly smaller?\n* The last rows of page 3 have '$\\phi^{(i)}$ and $\\phi^{(i)}$' instead of '$\\phi^{(i)}$ and $\\psi^{(i)}$';\n* Sometimes $\\phi$ is in bold and sometimes not (e.g. in Eqs (1) and (3)), not being consistent.\n* The number of current Figure 6 should be Figure 5 instead. The text already refers to it as Figure 5.\n* In Figure 6b (that is 5b), it has not been explained what ensemble size 0 means.\n* It has not explained clearly enough what the difference between online and offline distillation is.\n* The current theory and the discussion around it say nothing about decreasing the difference between $\\mu_1$ and $\\mu_2$ which could also be a separate goal. In principle, this might possibly be achieved by changing the training objective for the members of the ensemble. Maybe it is a bad idea, but I feel this should be at least mentioned.\n\nSuggestions:\n* The acronyms LDI and ELODI are both about logit difference inhibition: wouldn't it be better to use the same suffix, such as LDI and ELDI, or LODI and ELODI?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is sufficiently novel and reproducible, and also mostly clear, except for the shortcomings highlighted above.",
            "summary_of_the_review": "The paper addresses an important topic and proposes new effective methods. It also studies the theoretical justification behind the proposed methods. In my opinion, an important aspect has not been covered as an ablation study, justifying the difference of the proposed method from the existing logit matching loss by Hinton et al 2015. There are some shortcomings in the theoretical part, as highlighted above in the list of weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper8/Reviewer_asve"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper8/Reviewer_asve"
        ]
    }
]