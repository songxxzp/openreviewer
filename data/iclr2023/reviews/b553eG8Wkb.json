[
    {
        "id": "6-w7_ur4WxE",
        "original": null,
        "number": 1,
        "cdate": 1666494090230,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666494090230,
        "tmdate": 1666494090230,
        "tddate": null,
        "forum": "b553eG8Wkb",
        "replyto": "b553eG8Wkb",
        "invitation": "ICLR.cc/2023/Conference/Paper212/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present LABOR, a method that experimentally shows to be better than LABOR neighbor sampling and layer sampling approaches.",
            "strength_and_weaknesses": "Strengths:\n1. This work addresses the Neighborhood Explosion Phenomenon (NEP), the curse of dimensionality version of Graph Network Training.\nWeaknesses:\n1. The abstract states the method's superiority but not by how much or against what the method is compared.\n2. The same happens in the main body of the work: it is not clearly stated how much better is the proposed pipeline.\n3. The last point is important because the proposed method depends on a long series of assumptions and results that are true in general or valid in limit cases. Thus, the actual effectiveness of the method, which could be shown with experiments, is not indeed shown. Thus, it is challenging to state whether the method is suitable.",
            "clarity,_quality,_novelty_and_reproducibility": "The pipeline is well explained with a solid explanation of the theoretical foundation. However, the experimental results are not conclusive enough to state the usefulness of the method.",
            "summary_of_the_review": "It is an interesting paper with a great deal of theoretical justification. However, the fundaments rest on general assumptions whose usefulness needs to be assessed with experiments. But, as presented, these experiments are not conclusive enough to judge the pipeline benefits.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper212/Reviewer_M7QF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper212/Reviewer_M7QF"
        ]
    },
    {
        "id": "Ar6SJeeI_3Z",
        "original": null,
        "number": 2,
        "cdate": 1666617577886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617577886,
        "tmdate": 1666617577886,
        "tddate": null,
        "forum": "b553eG8Wkb",
        "replyto": "b553eG8Wkb",
        "invitation": "ICLR.cc/2023/Conference/Paper212/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed a new sampling  strategy: LABOR, which employs the Poisson Sampling to achieve a better sampling variance. Furthermore, it introduces a fanout hyperparameter $k$ and  delicate importance sampling probability to sample much fewer vertices without sacrificing quality. ",
            "strength_and_weaknesses": "## Strengths\n - Well organized and easy to follow. \n - Detailed analyze about the sampling probability with   fanout hyperparameter $k$. \n\n## Weakness\n - Technical contribution is limited. Principally, This paper follow the layer-wise sampling paradigm and replace the uniform sampling with the Poisson sampling. The paper also show that this sampling strategy can obtain a smaller sampling variance. This point seems a marginal improvement over the existing sampling framework. Concretely, except for the smaller sampling variance, I can not figure out the benefits of adopting this sampling strategy in the large-scale GNN training from the analysis in Section 3. From the theoretical perspective, would it improve the convergence speed  or reduce the memory cost, or other benefits? \n\n Since the efficient training \n - Lack of comparison with other layer-wise baselines. As mentioned in Introduction, there are several layer-wise sampling algorithms, such as AS-GCN[Huang et al., 2018] and GNS[Dong et al., 2021] (Both of them have released the source code.). However, this paper only make the comparison with LADIES. This is not sufficient  considering both studies have released source codes. \n\n - The experiments can not demonstrate the superiority of the proposed method. First, the paper only shows the training loss curve of all the methods. This is not sufficient since we can not get any cue about how the sampling strategy would affect the generalization ability unless the validation curves or the metrics on the test set are reported. Despite that, the improvement over the training loss is still very marginal and we can only observe its difference according to the zoom-in figure. \n \n - Moreover, to demonstrate the effectiveness of the proposed method. It's better to make such comparisons:  \n   - Vanilla GNN model with full-batch on the small data: to check the performance gain/drop of the proposed sampling method.\n   - Other kinds of sampling strategies, e.g. GraphSAINT [1] and ClusterGCN [2].\n\n\n\n [1] Zeng, Hanqing, et al. \"Graphsaint: Graph sampling based inductive learning method.\" arXiv preprint arXiv:1907.04931 (2019).\n\n [2] Chiang, Wei-Lin, et al. \"Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks.\" Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clear and written with a high quality. My main concerns are the novelty and the experimental justification.  I have no idea of the reproducibility since there is no source code released, but I think the experiments are easy to reproduce. ",
            "summary_of_the_review": "This paper is about a new sampling strategy of the layer-wise sampling in the GNN training. The details are clear and the analysis of sampling parameters is OK. However, the contribution of this  modification  looks marginal and the experiments failed to demonstrate the superiority of the proposed method. I vote the rejection for this paper.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None. ",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper212/Reviewer_5q4F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper212/Reviewer_5q4F"
        ]
    },
    {
        "id": "kimvA2CDCb",
        "original": null,
        "number": 3,
        "cdate": 1667238198940,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667238198940,
        "tmdate": 1667238198940,
        "tddate": null,
        "forum": "b553eG8Wkb",
        "replyto": "b553eG8Wkb",
        "invitation": "ICLR.cc/2023/Conference/Paper212/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is about speeding up the GNN training with a new way for sampling which combines layer and neighbor sampling methods. ",
            "strength_and_weaknesses": "Strength:\nThe paper is about how to perform sampling GNN training. The sampling method proposed seems a reasonable way to combine the layer and neighbor sampling methods.\n\nWeakness:\n\n1: The proposed method aims to replace NS using a variance reduction sampling and achieves better convergence. The question is whether NS achieves SOTA performance on the standard GNNs datasets?   \n\n2: My main concern is on the experiment part:\n\n1) The paper is about sampling method to speed up GNN training. However, there are no result on wall time for training GNNs.\n\n2) There is no accuracy reported for all the four datasets tested in the experiment part.\n\n3) No comparison with latest GNN fast training methods.  \n\n4) missing ablation studies. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is with good writing. The sampling strategy seems novel to me. \n",
            "summary_of_the_review": "I think the paper is interesting.  Due to limited experiment results, I am not fully convinced by this method for speeding up GNNs training.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper212/Reviewer_3iA9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper212/Reviewer_3iA9"
        ]
    }
]