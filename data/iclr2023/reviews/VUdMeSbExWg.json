[
    {
        "id": "6wVq4MuuyJ",
        "original": null,
        "number": 1,
        "cdate": 1666488365873,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666488365873,
        "tmdate": 1669236420680,
        "tddate": null,
        "forum": "VUdMeSbExWg",
        "replyto": "VUdMeSbExWg",
        "invitation": "ICLR.cc/2023/Conference/Paper2841/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides an imitation learning framework that solves correlated equilibria of mean-field games. The authors proposed an adaptive mean field correlated equilibrium as a generalization of the classic mean field Nash equilibrium that considers the correlated devices. They also offered a framework for imitation learning with the help of entropy regularization.",
            "strength_and_weaknesses": "**Strength:**\n- The model considers a generalized version of Nash equilibrium where the recommendations are sampled from a stochastic policy. This is a more general framework compared to the existing work.\n- The authors provided an imitation learning algorithm that finds the unique equilibrium of the problem.\n- The computational efficiency of the mean-field states is enhanced with the introduction of the signatures and the neural network training architectures.\n- There are numerical results provided that compare with some of the state-of-the-art imitation learning algorithms and showcase the effectiveness of the proposed algorithm.\n\n**Weakness:**\n- There are very few words on the priliminaries of imitation learning, and the introduction of the algorithm CMFIL is not well arranged from my perspective. They are less friendly to readers without enough background knowledge on such topics.\n- Section 4.2 seems to be too brief to me, and it is not fully explained how the signature is used for training.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\n- Equations, such as (4) and (5), are formatted without spaces after the equal signs\n\n**Other Questions:**\n- It seems like your algorithm finds the maximum entropy mean field correlated equilibrium instead of the AMFCE, and I am wondering how is the max entropy version related to the adapted version of the equilibrium. Especially, which equilibrium does it converge to when the regularization goes to zero?\n",
            "summary_of_the_review": "I think the authors considered a fairly interesting problem setting and proposed a generalization of Nash equilibrium for mean-field games. The proposed algorithm for solving the max entropy equilibrium also enjoys good empirical performance on some of the real-world datasets. Despite some of the clarity issues regarding how the paper is written, I feel like it is a solid paper on learning mean-field games.\n\n==========================\n\nI thank the reviewers for addressing my concerns. I will keep my score for the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2841/Reviewer_a1Rt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2841/Reviewer_a1Rt"
        ]
    },
    {
        "id": "CdMW3Kqw3VI",
        "original": null,
        "number": 2,
        "cdate": 1666633643881,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633643881,
        "tmdate": 1666633643881,
        "tddate": null,
        "forum": "VUdMeSbExWg",
        "replyto": "VUdMeSbExWg",
        "invitation": "ICLR.cc/2023/Conference/Paper2841/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies mean field games (MFG) using a notion of correlated equilibrium and proposes an inverse reinforcement learning (IRL) method. The authors mean field games where a representative agent solves a finite horizon MDP, and globally, we look for a Nash equilibrium. They introduce a notion of correlated equilibrium in this context and prove some properties. Even simple models might have several such solutions. So the authors propose a modification based on a maximum-entropy approach. Then, they introduce an IRL method and implement it on several examples. ",
            "strength_and_weaknesses": "As mentioned above, the main contribution is the IRL method for correlated equilibria in MFG. There are relatively few papers addressing the question of IRL for MFG so it is interesting to foster our understanding of this problem. \n\nHowever, I am not sure if the notion of time-dependent correlated equilibrium in MFG is the first one, as claimed by the authors (it seems that some relevant references should be added on this point), and there are several statements and proofs that I could not fully understand. \n\nSome detailed comments: \n(1) \u201cthis paper is the first focusing on MFCE with the correlation device providing time-dependent recommendations and allowing adaptive belief updates for individual agents.\u201d\nHere I would appreciate a more detailed comparison with the following references:\n- (Campi & Fischer 2022): their model is also set in finite horizon and it seems to me that their Remark 3.1 explains how to see the recommendation as being time-dependent\n- Muller, P., Elie, R., Rowland, M., Lauriere, M., Perolat, J., Perrin, S., Geist, M., Piliouras, G., Pietquin, O. and Tuyls, K., 2022. Learning Correlated Equilibria in Mean-Field Games. arXiv preprint arXiv:2208.10138: their model seems to cover both static and finite horizon problems\n\n(2) Example 1: It seems that the problem is actually static. It would be nice to have a time dependent example, since the authors claim that the time-dependent structure is one of the main features of the proposed model.\n\n(3) Corollary 2: I do not understand the (one-line) proof. It would be useful to spell out the objective and the constraints. My understanding is that there is a constraint on $\\mu$ which involves the function $\\Phi$ defined in (3). I do not see why this constraint is linear, given that $\\Phi$ can depend in a non-linear way on $\\mu_t$. Please clarify. \n\n(4) Proposition 2: I am sorry but I do not understand the proof of necessity. Could you please explain where the inequality comes from? First of all, the assumption on $\\mathcal{R}$ is a non-strict inequality whereas here there is a strict inequality. Second, the inequality in the assumption is summed over time steps. But here this is time-step per time-step. The sum can be negative without each term being negative. Last, since $a\u2019$ is chosen outside the expectations on $\\pi$ and $\\rho$, it seems that this reasoning only allows for \u201cglobal\u201d deviations, i.e., choosing the sequence of actions a priori, before seeing the recommended actions. But in the definition of AMFCE, it seems that we should allow any deviation function $u$, which allows picking the new action after seeing the recommended action. I am not sure how to resolve this and I think that further clarification is required to make sure that the proof is correct. \n\n(5) Equation (8): It is hard to understand the meaning of $L$ because there is $\\rho$ in the right-hand side but not in the left-hand side. I am having trouble understanding how $L$ can characterize an AMFCE if it does not depend on $\\rho$. I would appreciate more clarifications on this point (see also the next comment).\n\n(6) Proposition 3: From the definition of MFRL, the operator in the left-hand side should return a pair $(\\boldsymbol\\pi,\\boldsymbol\\rho)$. But in the right-hand side, the $\\argmin$ only returns $\\boldsymbol\\pi$. More generally it seems that $\\boldsymbol\\rho$ is frequently omitted although it is a key component of the AMFCE notion, which is quite confusing. \n\n(7) Proposition 3: Definition 5 defines the MaxEnt-AMFCE concept without neural networks. But when it is used in Proposition 3, it involves neural networks. Then it is not clear whether the two notions coincide exactly (I imagine that fixing a neural network architecture implies some changes in the MaxEnt-AMFCE definition).\n\nTypo: \n- Between (3) and (4): Should $\\pi_i$ be $\\pi_t$?\n- Page 4: \u201chas an incentive unilaterally deviate\u201d \u2192 to\n- The notations seem to frequently switch $i$ and $t$ for time, which is a bit confusing. Could you please harmonize the notation?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Although most of the high-level explanations are quite clear, several important parts of the paper need clarification before I can claim that I am confident the definitions and the results are correct.",
            "summary_of_the_review": "Overall, the paper makes an interesting contribution to a relatively little studied question. However, at this stage it seems to me that extra clarifications are needed to assess the novelty and the correctness of the results.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2841/Reviewer_bLhC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2841/Reviewer_bLhC"
        ]
    },
    {
        "id": "9e4gXQm7Se",
        "original": null,
        "number": 3,
        "cdate": 1666699265879,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699265879,
        "tmdate": 1666699265879,
        "tddate": null,
        "forum": "VUdMeSbExWg",
        "replyto": "VUdMeSbExWg",
        "invitation": "ICLR.cc/2023/Conference/Paper2841/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a variant of mean field games, where there is a correlation device, and subsequently introduce the corresponding equilibrium concept.\n\nIt analyzes this type of game/equilibrium concept, in particular an existence result (Thm 1).\n\nAdditionally, the imitation learning objective for learning the policies (rewards) is proposed, based on translating GAIL to this setting.\n\nA limited number of experiments are conducted.",
            "strength_and_weaknesses": "**Strengths:**\n\n\nMean field games, correlated eq., and generatie adversarial imitation learning are all relevant and interesting areas, and the authors combine them.\nThe authors propose quite a collection of non-trivial mathematical results about this combination.\nFrankly I did not have time to check the details of the proofs, but the authors kind of seem to know what they are doing, and the high level structure of the proof steps for the existence results (Thm 1) using Kakutani make sense.\n\n\n**Weaknesses and improvement points:**\n\n\nThere are quite a lot of technical elements combined in this work,\nin particular about \"foward\" MF game theory; and then comes GAIL which is already in itself a non-trivial topic.\nThis is no principle problem, but makes  it a bit overwhelming to read and to understand details.\n\nOne thing I found a bit confusing: are the IL aspects also an original contribution or not? Because they are not listed under the bullet points in Sec. 1. I guess it's original contribution in the sense of the first formulation of the GAIL principle for this mean field game setting?\n\nThe writing overall is OKish, but clarity of details needs to be improved, in particular the introductions of the basic concepts and definitions:\n* Where is state defined? Is the state space finite?\n* I appreciate that the reader is given intuitions about the concepts (e.g., below Def 1). However, I think a bit more mathematical precision would be good here. E.g., how exactly is the Law(s) defined (mathematical expression). I guess there may even be some conditions such that there is such a distro over states from a set/distro of agents.\n* A bit confusing that in  Sec. 3, pi does not seem to be the agents policy anymore, but instead part of the recommendation mechanism. (Instead the swap function is sort of the agents's policy now.)\n\nIn terms of motivation, I allocate this paper more on the abstract mathematical side than the real-application-motivated side. This is OK - otfen done - but nonetheless there are some motivational weaknesses from my point of view: E.g., the maximum entropy regularier is often used, but frankly I think there is only a vague principled motivation, while the main motivation is usually that it can be efficiently calculated or such things. I'm completely missing the motivation why rational or somewhat rational agents (which is the underlying assumption with game theory) should perform entropy-based equilibrium selection.\n\nOverall, I'm not an expert on mean field games; from a high level the work seems to make sense, though I cannot fully verify the originality of the contributions there; and also not the full technical soundness.",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "Technical paper combining interesting areas, with several mathematical contributions; in terms of correctness of proofs, the authors seem to know what they are doing, but I did not check details of the proofs. \n\nLimits and weaknesses are partially in terms of writing and complexity of the overall paper, as well as motivation of some concepts/assumptions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2841/Reviewer_V9BL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2841/Reviewer_V9BL"
        ]
    }
]