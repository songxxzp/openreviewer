[
    {
        "id": "Aysp3MOZEX",
        "original": null,
        "number": 1,
        "cdate": 1666445850560,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666445850560,
        "tmdate": 1666445876861,
        "tddate": null,
        "forum": "8RExG-EKC22",
        "replyto": "8RExG-EKC22",
        "invitation": "ICLR.cc/2023/Conference/Paper5926/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work designs a more general IMLE objective by introducing an adaptive neighbourhood radii. The theoretical derivation and the proposed curriculum learning strategy are beautiful and impressive. Some experiments are also conducted to verify the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1. The idea of introducing an adaptive radii for IMLE is interesting, which increase the degree of the vanilla IMLE.\n2. Leveraging lemma 1 to construct the adaptive is very clever. It provides a very useful tool to depict the distribution of the distance between the training example and a generated sample.\n3. The visual results tend to prove that this adaptive objective could produce more diverse example.\n\nWeaknesses:\n1. The written of this paper is not very clear. Beginning from lemma 3, the notation $w_i$ is frequently appeared. But there isn't any explanation about this notation, and why does introduce $w_i$?\n2. I don't understand the definition of $L_{\\{\\tau_i\\}_i}$ in the first line of the proof for lemma 3 in appendix. If possible, please provide more details.\n3. The number of sample $m$ should play an important role in this work. Even this work design the curriculum strategy to avoid increasing $m$. It would be better to offer some results under different values of $m$.\n4. Comparing with the vanilla IMLE, why does the proposed objective achieve better results? This paper does not clearly claim this point.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity on writing should be further improved. In my opinion, the design of the adaptive IMLE is novel.\n",
            "summary_of_the_review": "This work devises a theoretically sound objective to generalize the IMLE, and achieves stable and better results. Even though the writing of this paper should be improved, it is still a beautiful work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_uhYC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_uhYC"
        ]
    },
    {
        "id": "bOkVkqyWCv",
        "original": null,
        "number": 2,
        "cdate": 1666681228953,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681228953,
        "tmdate": 1666681228953,
        "tddate": null,
        "forum": "8RExG-EKC22",
        "replyto": "8RExG-EKC22",
        "invitation": "ICLR.cc/2023/Conference/Paper5926/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper enables training GANs with a few amount of data from scratch. It is built on top of an existing method IMLE which is improved to adaptive IMLE to avoid model collapse or model overfitting. The more generalized form of IMLE is presented which covers the original formulation as a special case. Extensive experiments on different datasets are conducted to show that the proposed method could generate higher quality and more diverse results.",
            "strength_and_weaknesses": "+ Strict prove on theoretical guarantees of the generalized formulation hold under weaker condition\n+ A systematic level up of vanilla IMLE algorithm\n+ Obvious improvement of results under few-shot setting\n\nA good work about making GAN training less data-hungry and potentially using less compute resources, which is applicable to personalized generation with a small library of data. Please find my minor concerns below:\n\nTo understand how the number of training data is affecting the performance, it is better to conduct an experiment by ablating this hyper-parameter and show a curve about how FID will increase with less training examples.\n\nJust curious what is the smallest number of images that the proposed method could work? 100? 10? 1? The previous transferring learning based methods, though requiring pretraining on an external data, are more aligned with human learning process of learning new knowledge quickly after learning old knowledge and show even 1-shot setting is possible. The proposed method belongs to learning from scratch with a few amount of data. Is it limited in for example just learning 100 faces of one person (obama) instead of learning 100 faces of different persons? Essentially, GAN should be a data engine to benefit more downstream tasks. If the generated is still less diverse, it may not be quite useful as compared to transfer learning approaches.\n\nIt is suggested to include some results on non-facial data as face has been heavily explored, in order to see how robust the algorithm is cross different type of datasets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work's presentation of proposed framework is clear and easy to follow. It is a more generalized version of a previous method. Reproducibility is upon the code release though the algorithm of each step is already given.",
            "summary_of_the_review": "A new perspective to tackle the few shot generation problem. Good work but I worried a bit about its performance over other non-facial dataset and eventually how useful it will be compared to transfer learning setting. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not aware of concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_xM5B"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_xM5B"
        ]
    },
    {
        "id": "rrgpHBLPeF4",
        "original": null,
        "number": 3,
        "cdate": 1667063113283,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667063113283,
        "tmdate": 1669118904120,
        "tddate": null,
        "forum": "8RExG-EKC22",
        "replyto": "8RExG-EKC22",
        "invitation": "ICLR.cc/2023/Conference/Paper5926/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new objective for the problem of few-shot image synthesis based on extending implicit maximum likelihood estimation (IMLE). Specifically, they derive a data-adaptive IMLE objective for training the implicit generative model. Experiment results on six few-shot image synthesis datasets, namely Grumpy Cat, Obama, Cat, Panda, Dog, and the subset of FFHQ, demonstrate the proposed method outperforms the previous few-shot image synthesis methods.",
            "strength_and_weaknesses": "### Strengths\n* The problem setup of few-shot image synthesis is practical and interesting.\n* The paper performs extensive experiments across diverse datasets.\n* The paper shows solid experimental results, both quantitatively and qualitatively.\u00a0\n\n### Weaknesses\n* While the problem definition (few-shot image synthesis) is interesting in general, I am not convinced about the detailed problem setup in not using auxiliary datasets. An explanation highlighting the importance of such a setup or comparing the results with fine-tuning-based methods would be welcome.\n* The paper is hard to follow. Considering that the paper proposes the generalized version of IMLE, the clarity of the paper would be much better if it explicitly mentioned the limitations of IMLE (at least in brief) in the theoretical aspect. However, the paper only mentions that \u201crestrictive conditions need to be satisfied for the theoretical guarantees of IMLE to hold, such as requiring a uniform optimal likelihood for all data points\u201d. Why, what, and how are such conditions restrictive, while the proposed method is not?\u00a0\n* \u00a0I think the ablation study (Table 3) should be plotted (instead of Table) to compare IMLE and adaptive IMLE. Few-shot synthesis problems are generally vulnerable to the overfitting of the generator. As the proposed adaptive IMLE objective includes additional regularizations (e.g., $\\min$ with $\\delta \\tau_i >0$)than IMLE, hence more difficult to optimize, I wonder whether the performance gain is simply from alleviating the overfitting or from indeed \u201cbetter\u201d objective design. At least, I strongly recommend authors put more details on how the ablation study is performed (e.g., \u201cbest\u201d performance during the training is reported); but still, showing plots with respect to number of iterations would be much more welcome.\n* While the proposed method shows superior performance among the reported baselines, the current manuscript misses some recent baselines (even outperforming adaptive IMLE), e.g., [1]. At least, a discussion with this baseline would strengthen the manuscript more.\n\n[1]\u00a0FakeCLR: Exploring Contrastive Learning for Solving Latent Discontinuity in Data-Efficient GANs, ECCV 2022. (arXiv: Jul 2022)\n\n### Minor comment\n* The equation number is not stated in the sentence between Eqn. 3 and Eqn. 4.\n\n### Questions\n* Why does the precision of IMLE be better than the proposed Adaptive IMLE?\n* The proposed method can be applied to any few-shot synthesis problem; do the authors try to apply the method in different domains?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity.** The clarity of the paper can be improved. For instance, a more comprehensive (theoretical) explanation of the limitation of IMLE and a comparison between IMLE and the proposed method would be helpful.\n\n**Quality.** While the paper performs the experiments on diverse datasets, yet needs to include recent baselines that outperform the presented method.\u00a0\n\n**Novelty.** The proposed method seems novel, yet it could be clearer due to not very good clarity.\n\n**Reproducibility.** The supplementary material contains the code for reproducing the results, indicating good reproducibility.\u00a0",
            "summary_of_the_review": "This paper tackles the few-shot image synthesis problem by extending the IMLE objective to be \u201cimage-adaptive\u201d. However, I feel the overall clarity could be improved. For the evaluation side, I am not fully convinced due to the missing baseline and the concerns about ablation studies. In these respects, I am currently on the negative side.\u00a0",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_owTn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_owTn"
        ]
    },
    {
        "id": "W_Gsojqgvj",
        "original": null,
        "number": 4,
        "cdate": 1667159737262,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667159737262,
        "tmdate": 1667159737262,
        "tddate": null,
        "forum": "8RExG-EKC22",
        "replyto": "8RExG-EKC22",
        "invitation": "ICLR.cc/2023/Conference/Paper5926/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a generalized formulation of IMLE, Adaptive IMLE, for few-shot image synthesis. As the generator trained in GAN suffers from the mode collapse issue, i.e., GAN may overfit to a subset of training samples. Then, IMLE is proposed to avoid this issue by optimizing training images to have some similar generated images. However, as strict restrictions are applied on IMLE and may impede training efficiency in the few-shot setting, this paper proposed Adaptive IMLE by adjusting the radius w.r.t. each training data. Detailed theoretical analysis is provided, and the experimental study demonstrates its effectiveness. However, the discussion is very limited.",
            "strength_and_weaknesses": "Strength:\n1.\tFormulating a generalized form of techniques is essential for further development, and the few-shot learning setting is specifically studied.\n2.\tClear paper writing about background introduction and method derivation & development.\n3.\tConsistent performance gain and promising results. Visualization are also provided.\n\nWeakness:\n1.\tMy biggest concern is regarding ignoring w_i in the Eq.5. The explanation seems incomplete (the unweighted objective, since tau_i and there fore w_i is fixed). Please double-check and clearly explain why w_i can be ignored.\n2.\tWhy Prec drops slightly while FID and Rec. are improved largely? It is necessary to explain the reason. Acturally, Prec are very high and I am not sure whether it Is a good metric.\n3.\tTraining details should be provided. Do you need additional large-scale data for pre-training? Also, do you need to find the nearest generated samples for all training data within each batch? If yes, the memory usage is pretty large and I wonder whether your approach can address this issue.\n4.\tThe comparison between IMLE and adaptative IMLE is only on Obama and FFHQ while more experimental comparisons are performed in Table 1-2. Can you explain the reason? From my perspective, the comparison with vanilla IMLE is much more critical.\n5.\tFor the optimization of IMLE, one weakness is that the neighbor search is centered w.r.t. each training data, then, it is possible that a very limited number of generated samples are selected in one batch. Will that be risky, and do your methods can mitigate this issue also?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper provides a theoretical analysis of IMLE by providing a generalized formulation, which is important.\nThe pseudo-code is provided for reproducibility.",
            "summary_of_the_review": "The discussion is very limited. Please see my comments above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_jG1N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_jG1N"
        ]
    },
    {
        "id": "9EIQA3wc3I",
        "original": null,
        "number": 5,
        "cdate": 1667183219107,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667183219107,
        "tmdate": 1667183219107,
        "tddate": null,
        "forum": "8RExG-EKC22",
        "replyto": "8RExG-EKC22",
        "invitation": "ICLR.cc/2023/Conference/Paper5926/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of few-shot image synthesis, i.e., learning to generate samples from a distribution given only few examples. A previous approach, dubbed Implicit Maximum Likelihood Estimation (IMLE), showed that one perform few-shot synthesis by minimizing a loss that penalizes the distance between generated samples and true samples. This loss was shown to exhibit nice properties, such as convergence to the true likelihood, but operated under strong assumptions (e.g., all training images have the same likelihood). This work builds upon IMLE by proposing an adaptive version that is able to weigh examples from the training set based on their difficulty and theoretically show that their loss is a generalization of IMLE operating under weaker assumptions. Results on few-shot image synthesis benchmarks show that learning with the Adaptive IMLE can improve generation.",
            "strength_and_weaknesses": "\n**Strengths:**\n- The new adaptive loss function is principled in that it can be shown in a limiting sense to converge to a maximum likelihood-based objective.\n- The empirical results show quantitative and qualitative improvements in terms of generation quality and diversity of the images. To the reviewer's knowledge, comparisons against strong few-shot image synthesis baselines are considered for comparison.\n\n**Weaknesses:**\n- Some notions of the theory would be impractical to implement, such as choosing the correct weights $w_i$ in the optimization for certain distance metrics, which requires computing the volume of a neighborhood around a given datapoint.\n- There are some issues in the clarity/derivations that would be good to improve upon.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n**Clarity:** \nOverall, I found the paper fairly easy to read, but had some questions regarding some of the proposed theory and derivations:\n- There is some handwaving in the derivation from equation (3) to (4). In particular, $\\tau_i$ is chosen to depend on $T_{\\theta}$ prior to equation (3), but then the $\\tau_i$ term is dropped in the loss function in equation (4). Can we truly treat $\\tau_i$ as a constant here? \n- The derivation in Lemma 2 appears for a fixed sample $z_j$, rather than an expectation over the empirical distribution.\n- There are no assumptions placed on the distribution $p_{\\theta}$, which makes it hard to verify some of the results. For example, why does the limit in the third to last line in the proof of Lemma 4 exist? At a minimum, one would require continuity of $p_{\\theta}$ at the datapoints to assert this.\n\n**Novelty and significance:** \nTo the reviewer\u2019s knowledge, this extension of IMLE appears novel, but it could be that I am unaware of certain prior work. The experimental results also appear quite solid.\n\n**General comments:**\n- Ultimately, the loss that ends up being used by the authors is equation (5) versus the original IMLE equation (1). The only difference here is in taking the maximum between the distance and the threshold $\\delta \\tau_i$. The weights, $w_i$, however, can be computed in closed-form for certain metrics, such as the distance induced by an $\\ell_p$-norm. Empirically, did it have an impact on performance if the loss with weighting $w_i$ was used versus equation (5)? \n- Along these lines, it would be nice to see some low-dimensional examples showcasing how the proposed loss actually helps in \u201cadapting\u201d to difficult examples. Figure 1 provides some intuition, but it would be nice to see some 2- or 3-dimensional experiments showing this more concretely.\n- Since the framework allows flexibility in choosing a distance metric $d(\\cdot,\\cdot)$, did the authors try different metrics other than $\\ell_2$-distance, such as an LPIPS metric? Did this influence performance at all?\n- There is a missing equation reference in the middle of page 5 ",
            "summary_of_the_review": "Overall, I think that the paper presents a nice extension of previous work in order to improve few-shot image synthesis. The experimental results seem promising, but there is some lack of polish in the presentation of the material. I am leaning more on the positive-side based on the results.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_Me9N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5926/Reviewer_Me9N"
        ]
    }
]