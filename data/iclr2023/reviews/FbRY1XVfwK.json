[
    {
        "id": "fGNJtqOyYZp",
        "original": null,
        "number": 1,
        "cdate": 1666683557126,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683557126,
        "tmdate": 1666683557126,
        "tddate": null,
        "forum": "FbRY1XVfwK",
        "replyto": "FbRY1XVfwK",
        "invitation": "ICLR.cc/2023/Conference/Paper4769/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the technique of sampling from a Gibbs distribution via the Ideal Hamiltonian Monte-Carlo (HMC) method. Specifically, the ideal HMC method is derived by iterative running the HMC from the current spatial position, but with a resampled velocity for a certain period of integration time. The integration time plays a key role in proving the mixing time of the algorithm: It is known that even for the Gaussian case, with a constant integration time, the mixing time is proportional to $\\Omega(\\kappa)$. To improve this, this paper proposes the Chebyshev integration time, which are derived from the roots of the Chebyshev polynomials. It is proved that using the proposed integrating time, the resulting ideal HMC method enjoys a mixing time of order $O(\\sqrt{\\kappa})$, for the Gaussian case. The authors also provide empirical evidence that the improved rate generalizes to non-Gaussian tasks.",
            "strength_and_weaknesses": "Strength:\nThis paper proposes the Chebyshev integration time which leads to a provably faster instance of the Ideal HMC method, in the Gaussian case. Empirical evidence shows that the improved rate generalizes to non-Gaussian tasks.\n\nWeakness:\nThe current theoretical result only applies to the Gaussian case. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is well-written in terms of presenting the proposed method. However, the intuition of why this Chebyshev integration time accelerates the Ideal HMC method remains unclear. \n\nNovelty: The proposed Chebyshev integration time is novel as it improves over the lower bound for constant integration time. Empirical evidence suggests that the same technique may also improve the efficiency of sampling for non-Gaussian tasks.",
            "summary_of_the_review": "The Chebyshev integration time is novel, but the current analysis is limited to the Gaussian case. Intuition behind the success of this technique is missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4769/Reviewer_XHWf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4769/Reviewer_XHWf"
        ]
    },
    {
        "id": "YvDyheYzRpI",
        "original": null,
        "number": 2,
        "cdate": 1666705656250,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666705656250,
        "tmdate": 1666705656250,
        "tddate": null,
        "forum": "FbRY1XVfwK",
        "replyto": "FbRY1XVfwK",
        "invitation": "ICLR.cc/2023/Conference/Paper4769/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new time-varying integration duration for HMC using the roots of Chebyshev polynomials. \nIn a very restrictive setting, they paper proves an upper bound on the number of iterations required to reach Wasserstrain-2 distance less than an a specified error threshold. This setting is where exact HMC mechanism is applicable (i.e. the equations of motion can be solved in closed form). This closed-form solution only exists for Gaussian distributions (as far as I know).\nNonetheless, experiments shows that their scheme has benefits in case the target distribution is not Gaussian but is associated with a smooth strongly convex potential energy function.\n ",
            "strength_and_weaknesses": "The paper is well-written and the proofs seem correct. Though, I should highlight I did not check all the proof details closely. \nThe newly found bound is interesting though the studied theoretical setting, namely exact HMC (i.e. where the equations of motion are solved in closed-form) is only applicable to Gaussians and therefore, it is quite restrictive because clearly, for sampling from Gaussians there exist better algorithms and HMC is not used. \nNonetheless, this theoretical finding may open the way for future theoretical works that consider more general families of distributions. \nAlso, the presented empirical results show that the proposed algorithm can outperform the baseline on models other than Gaussian. \n  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The quality of the work is high. The found upper bound is novel (though, it covers a very restrictive setting) and the algorithm seems reproducible (though I have not checked the code). \n",
            "summary_of_the_review": "This paper presents a new method to adapt the HMC's integration time. It is shown that on the experimental models, the proposed method can outperform the baseline. \nThey also find new bounds for a very restrictive case where HMC's state evolution can be solved in closed-form. As such, I am not sure if it can be called significant. Nonetheless, I think this finding is interesting and valuable.  It may also open doors to future theoretical works. \n \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concern",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4769/Reviewer_dujW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4769/Reviewer_dujW"
        ]
    },
    {
        "id": "BQp2jqEzoJ",
        "original": null,
        "number": 3,
        "cdate": 1666714294901,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714294901,
        "tmdate": 1666714294901,
        "tddate": null,
        "forum": "FbRY1XVfwK",
        "replyto": "FbRY1XVfwK",
        "invitation": "ICLR.cc/2023/Conference/Paper4769/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a time-varying integration method for accelerating the mixing of Hamiltonian Monte Carlo. In particular, when the potential $f$ is quadratic function, i.e., the target distribution is Gaussian, the ideal HMC with the proposed time-varying integration enjoy a $O(\\sqrt{\\kappa}\\log(1/\\epsilon))$ iteration complexity in terms of Wasserstein-2 distance. The authors further develop a practical HMC algorithm using the proposed integration time. Experimental results show that the developed algorithm can also lead to better sampling performance for the general smooth and strongly convex potentials.",
            "strength_and_weaknesses": "Strength:\n\n* this paper proposes a novel time-varying integration method for HMC and proves a faster convergence compared to the constant-time integration method.\n* this paper incorporated the proposed integration into a practical HMC algorithm and demonstrate its better performance in experiments.\n\nweakness:\n* The theory is limited to quadratic cases, while prior works cover all smooth and strong convex cases.\n* The explanation of the acceleration effect of Chebyshev integration time is not clear.\n* The stepsize is not tuned for HMC with constant-time integration (as mentioned in Section 4, the stepsize for HMC with constant integration time is set by (5)).\n\nQuestions:\n* What\u2019s the role of $\\bar \\Phi_K(\\lambda)$ in the algorithm or theory? Why can equation (11) illustrate the acceleration of the proposed Chebyshev integration time?\n* The authors mention that the permutation $\\sigma$ is not needed in the analysis, does it mean that Theorem 1 holds for any permutation?\n* (5) is only one choice of constant integration time, can we find a better choice to gain faster convergence? \n* It would be better to compare the total integration time of the proposed method and baselines.\n* Following the previous comment, can we fix the total integration time and directly optimize the integration time in each iteration according to Lemma 2?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity needs to be improved, more explanations regarding why Chebyshev integration time can lead to acceleration need to be added.\n\nNovelty and originality are acceptable, no prior works have shown similar results.\n",
            "summary_of_the_review": "My major concern is (1) the limitation of theory (i.e., only for quadratic potential) and (2) lack of thorough discussion and exploration of baseline method (e.g., constant integration time) in terms of both theory and experiment.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4769/Reviewer_Kxzm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4769/Reviewer_Kxzm"
        ]
    },
    {
        "id": "HwJJ4YcbuD",
        "original": null,
        "number": 4,
        "cdate": 1666728116637,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666728116637,
        "tmdate": 1669850238006,
        "tddate": null,
        "forum": "FbRY1XVfwK",
        "replyto": "FbRY1XVfwK",
        "invitation": "ICLR.cc/2023/Conference/Paper4769/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the possibility of accelerating Hamiltonian Monte Carlo (HMC) methods for sampling from distributions $\\pi$. For a $L$-smooth and $m$-convex $f$, the complexity of sampling from $\\pi \\;\\alpha\\; e^{-f}$ (via an \"ideal method\" that does not discretize time) is of the order $\\kappa \\log(1/\\delta)$, where $\\delta$ is the desired precision and $\\kappa={L/m}$ is the condition number. The present paper shows that one can lower the complexity of the ideal method by a factor of $\\sqrt{\\kappa}$ when $f$ is quadratic (and so $\\pi$ is Gaussian). This requires knowledge of $L$ and $m$, which one uses to choose special step sizes defined via Chebyshev polynomials. Experiments suggest that the same choice of weights works beyond the case of quadratic $f$; in fact the method seems to \"beat the competition\" even for nonconvex likelihoods. ",
            "strength_and_weaknesses": "*Strengths*\n\n- This seems to be the first acceleration result for HMC.  \n- The use of Chebyshev polynomials is somewhat interesting.\n\n*Weaknesses*\n\n- Unfortunately, the theory does not cover any cases of practical interest.\n- The experimental success metrics are all defined in terms of effective sample size (ESS). This is a useful proxy of MCMC quality, but it does not prove convergence, nor does it quantify this convergence in harder problems. It would be nice to see other metrics as in: can one approximate the integrals of some \"interesting/hard functions\" from the MC trajectories of different methods?\n- In line with the previous remark, it'd be nice to look at some *hard* distributions coming from real-life Bayesian statistics. What do the credible regions look like? Does one reobtain known results at lower cost?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite clear. I did not ceck the experiments carefully, but the code is well-documented and \"looks reproducible\". The approach also seems novel in this context.\n\nI do not think the paper is of great interest, given that the theory is severely limited, and the experiments do not provide a clear indication of practical aspects of this method. One issue I would like to see explored is this: if the likelihood is complicated, maybe it's hard to compute $\\kappa$. What does one do in practice? Is the method sensitive to the upper bound $L$ and the lower bound $m$?",
            "summary_of_the_review": "The paper has a nice idea, but, at it is present form, its interest is too limited from both theoretical and practical perspectives. \n\nUPDATE on Nov 30th\n\nI have increased my \"novelty\" and overall scores. I thank the authors for their responses. To comment on a few points. \n\na) I agree ESS is a standard surrogate for convergence. It would be nice, however, to see a problem with eg. a multimodal likelihood and show (empirically) that accelerated HMC converges faster than the standard method. \n\nb) I understand that the above point is (as the authors say) disjoint from their goals, as it'd go beyong the logconcave setting. However, given the lack of theoretical results for general logconcave f, I think the paper would greatly benefit from a more extensive empirical evaluation of the accelerated method in more practical settings. \n\nc) I thank the authors for the clarifiction regarding sensibility to the smoothness ans strong convexity parameters. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4769/Reviewer_GRbp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4769/Reviewer_GRbp"
        ]
    }
]