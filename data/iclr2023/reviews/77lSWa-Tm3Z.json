[
    {
        "id": "nQKaz65iQd",
        "original": null,
        "number": 1,
        "cdate": 1666272686699,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666272686699,
        "tmdate": 1666272686699,
        "tddate": null,
        "forum": "77lSWa-Tm3Z",
        "replyto": "77lSWa-Tm3Z",
        "invitation": "ICLR.cc/2023/Conference/Paper4828/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This articles positions itself in the context of designing \"interpretable by design\" models: models are built from the start to provide an interpretable explanation alongside their predictions. To achieve this goal, the model performs its prediction by iteratively making queries about the example to classify within a predefined set of interpretable queries. The sequence of queries serving as the explanation for the prediction. The paper follows the Information Pursuit (IP) method, where the next query is chosen by maximizing the mutual information between the variable to predict and the result of the query given the past query history.\n\nPrevious work in the IP framework learned a joint generative model over the query answers and the final prediction, using MCMC sampling on that model to compute the mutual information required to choose the next query. The paper shows that the IP selection strategy can be formulated as the solution to a variational optimization problem involving two functions: a classifier that predicts the output variable given the history of queries, and a querier, that chooses the next query. This formulation alleviates the need for learning a joint generative model, drastically reducing the computational cost.\n\nThe paper provides extensive experimental comparison with other models from the literature and on multiple tasks, proving the proposed V-IP model to be very competitive across the board.",
            "strength_and_weaknesses": "**Strengths:**\n- The proposed approach is an elegant and well-grounded variational reformulation of the IP strategy, which is itself a strong strategy in this context.\n- The formulation of the optimization problem and its implementation using deep neural networks is clear and detailed.\n- The experimental validation is extensive and discusses in depth the advantages and limitations of the proposed V-IP.\n- The proposed model is competitive with previous state of the art with a significantly smaller computing cost, which is a strong contribution.\n\n**Weaknesses:**\n1. Regarding the straight-through optimization of the softmax of the querier model, as discussed in appendix D this requires the following function to be differentiable wrt to the one-hot output of the softmax. In V-IP, the following function is the concatenation of the next query result to the queried dataset. How is that step made differentiable? I suppose this is related to the masking-based architecture, but the paper does not seem to explain it.\n2. This is a minor point, but I think figures 3a illustrates well how query sets consisting in observing small patches of an image are very artificial given the high-level problem at hand (interpretability). The 4th observation is the one that tips the balance of the model from cat to dog, and yet it mostly consists in a black square of background. Could that maybe illustrate some overfitting in the model?",
            "clarity,_quality,_novelty_and_reproducibility": "The article is very clear and well written. It provides a new, solid and elegant reformulation of the problem of Information Pursuit.\n\nThe appendix provides (almost, see weakness 1 above) all necessary information to reproduce all the experiments in the paper.",
            "summary_of_the_review": "This paper proposes a new method, which is well theoretically founded and empirically shows significant computational gains compared to the state of the art. There are a few details that can be clarified, but this is a good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4828/Reviewer_G1eQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4828/Reviewer_G1eQ"
        ]
    },
    {
        "id": "7IhLcSQanC",
        "original": null,
        "number": 2,
        "cdate": 1666558301823,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558301823,
        "tmdate": 1666558433506,
        "tddate": null,
        "forum": "77lSWa-Tm3Z",
        "replyto": "77lSWa-Tm3Z",
        "invitation": "ICLR.cc/2023/Conference/Paper4828/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper authors proposed a variational information pursuit for interpretable classification model / estimation scheme. Authors idea is motivated by generative variant that they denote G-IP. Authors propose a complete framework with model defintions explanation of the training scheme and proof that loss does what it is supposed to do. Finally they show with experiments how the proposed method works in contrast to baselines. ",
            "strength_and_weaknesses": "Positives: \n- I find the paper very interesting and clearly written. \n- Idea is neat and does seem to work in practice. \n- I feel that in terms of quantitative performance it would be better to emphasize much more the speed of inference. Maybe even with some Figures, so that the message is really hammered home that V-IP is much faster than G-IP. \n- Theoretical development is also a plus. \n\nNegatives:\n- I do not understand why in (V-IP) authors need to formulate their objective as a constraint optimization task. Why not to directly optimize f,g? \n- Authors state in the Introduction that \"We empirically demonstrate the efficacy of the proposed method over generative modelling on various computer vision and NLP tasks\", but in the case of MNIST clearly V-IP is not the clear winner. I am thinking that authors should modify these statements to reflect the results more accurately. \n- Thing that is sometimes hard to do is to put plot standard deviations in Figs and mark them into the results Tables. In RL lit, this is standard practice, you need to repeat the experiments some number of times to obtain those std estimates. Please consider adding them where ever possible.\n- Clear limitation of the present paper is to have good Q set that allows for interpretation. If one constructs the set Q basically randomly, is it going to give useful information to the practitioner? It is on the other hand quite clear that if Q is very limited, then the proposed method would not work too well against non-interpretable models. Some discussions about this could be good to have. ",
            "clarity,_quality,_novelty_and_reproducibility": "Variational Information Pursuit for Interpretable Predictions:\nSection 2: When you write \"in almost all cases our method performs better.\" it would be good to add precision about what is ment to be better? \n\nSection 3: you say that \"algorithm terminates after $L$ queries if ... \", $L$ is here capitalized so it seems to indicate a user defined parameter. Would better be said that if \narg max I is 0 algorithm terminates. Please clarify. \n- About stopping criterion in difference of entropies: \", which is difficult to compute without explicit generative modeling of the query-answer distribution\". Could you clarify this last part, if it is hard to estimate then how you do it? \n\nSection 4.2: please explain RAM and RAM+ at least with few words, maybe in a similar way as G-IP had been explained in earlier Sections. In Fig 4,  why RAM performance goes down in Huffington dataset? \n\nIn Fig 4. I would like to see baseline performance of some reasonable non-interpretable model that does not use queries but just takes the whole input as is and produces one output. It would be important to to see how much we lose by trying to be interpretable.\n\nMinors: \n- Please select to use either V-IP or VIP.",
            "summary_of_the_review": "I find the ideas in the paper to be quite interesting and potentially very useful for practitioners. The main experimental results are not necessarily very compelling in terms of classifier accuracy, but in terms of inference speed they are. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4828/Reviewer_jPMz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4828/Reviewer_jPMz"
        ]
    },
    {
        "id": "67Eely3-Bqp",
        "original": null,
        "number": 3,
        "cdate": 1667599871972,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667599871972,
        "tmdate": 1667599871972,
        "tddate": null,
        "forum": "77lSWa-Tm3Z",
        "replyto": "77lSWa-Tm3Z",
        "invitation": "ICLR.cc/2023/Conference/Paper4828/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method called V-IP (Variational Information Pursuit) that does a multi-step prediction to improve interpretability instead of doing a one-pass prediction like other neural nets do. In each step, only a small sets of features (i.e. \"query set\" called in the paper) are revealed and the goal is to make a prediction using the minimun number of steps i.e. part of the feature sets. It can derive interpretability becasue the subsets of features causing a big increase of the prediction of the ground truth class between steps can be seen as important rationales of why model makes such prediction.\n\nPreviously, most method resort to using generative models to model the distributions between labels and subsets of features to pick which parts of features can maximally predict the target by resorting to MCMC sampling methods (the baseline called G-IP). Or others have proposed using reinforcement learning to sequentially select the feature sets that predict the correct target. The proposed method, V-IP, instead learns to greedily choose the subsets of features that maximize the downstream classifiers to predict the target y in each step as measured in the KL divergence, which can be seen as the mutual information of the current selected features in each step. Note V-IP can be seen as a RL method that has an immediate reward and the decay factor gamma set to 0. In the classification part, V-IP experiments with set-based and mask-based classifiers to predict and find the mask-based ones perform better. In a wide-variety of datasets including images, medical diagnosises, and texts data, the V-IP outperforms recent G-IP related methods and RL-based methods. ",
            "strength_and_weaknesses": "# Strength\n\n- The writing is clear and easy to follow. \n- The examples of interpretability shown in Fig. 3 are interesting to see.\n- I like the careful details of biased samplings and the set-based classifiers v.s. mask-based classifiers.\n- The experiments seem thorough, but some more experiments can be helpful. See below.\n\n# Weaknesses\n\n1. IMHO, the novelty side may be a little bit low since this method can be seen as a RL method with immediate reward of improving the classifiers predictions. Other inventions like initial random sampling and subsequent biased sampling are new in my opinions.\n2. G-IP seems to perform quite similarly or sometimes better than V-IP (Fig. 4) in larger datasets like CUB-200 with large query size 312. It seems contradictory to what authors state \"We thus expect the gains of V-IP to be most evident on large-scale datasets\". An ablation study that improves from small to large number of examples in a dataset may help verify what the authors claim.\n3. There is no consistent baselines across the datasets, which may make comparisons difficult. If it's easy to do, can authors also run the G-IP on the datasets in Table 3 and Figure 5 CIFAR-10/CIFAR-100 to see if the proposed method V-IP is indeed better than G-IP? Or authors can comment on why such comparisons are not easy due to code inaccessibility etc.\n4. I would love to see a more complete ablation study than the Supp. E (Fig. 11) that compare another version <1> No initial random sapling, and put a subset of results into a small table in the main text if possible.\n5. The metrics reported in the experimental section are mostly borrowed from other papers. Although it's great to directly compare to the SOTA numbers, there maybe subtle differences in the implementation leading to such result. For example, can authors please confirm fi those baselines BSODA, REFUEL in Table 3 and Figure 5 are using the same classifier architectures and training strategies the same as V-IP and the only difference lies in the strategy of selecting the features? If not, such number should not be directly compared, or some ablation studies are needed e.g. using BSODA's classifiers instead of the V-IP classifiers.",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\n\nI enjoy reading the paper and it's quite smooth and clear. One little thins is maybe authors can illustrate that those queries are in fact the features in a dataset in Fig. 1. Although I understand the queries are more general, it makes me confused that if an oracle is needed to get the answer of these questions or it needs to be learned. I realize that those questions in Fig. 1 are in fact just a feature value in the dataset in the experiment sections so no oracle is needed.\n\n# Quality\nThe experiments are overall good. One important thing is can authors please add standard deviation on both Table 2, 3 and Figure 4, 5 to understand the significance of each method?\n\n# Originiality\nIMHO, I think the work has slightly lower originiality since these methods are similar to RL-based methods. \n1. It reminds me of an earlier work [1] that also uses RL to do per-instance feature selection. Can authors please comment on the relations?\n2. Can authors comment on in what scenarios the proposed greedy approach work better (gamma=0), and in what scenarios the RL-based approaches (gamma > 0) can be better? [1] seems to show that the RL-based approaches perform better.\n\n# Thoughts (may not be important)\n1. Can this method be further improved by combining with a generative approach such as partial VAE? For example, a way to improve V-IP is that for each feature selection doing an imputation for the rest of unselected features and use all the features to send to the classifier. I think it will improve in the image space where there is a high-degree of correlations. Do you think if this approach will improve the accuracy, and also the interpretability?\n\n\n[1] INVASE: Instance-wise Variable Selection using Neural Networks: https://openreview.net/forum?id=BJg_roAcK7",
            "summary_of_the_review": "Overall I think the authors do a great job in writing and the examples presented are interesting to see. But given the crowded space of this problem (feature subsets selection), I am hoping to see more complete comparisons and other interesting questions as suggested above. The experimental number also lacks standard deviation which I believe is important. Although I find this work slightly less original, I don't mind such work being accepted as long as there are concrete evaluations and ablation studies to make readers learn something from it.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4828/Reviewer_VaCA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4828/Reviewer_VaCA"
        ]
    }
]