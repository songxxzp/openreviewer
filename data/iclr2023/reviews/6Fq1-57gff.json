[
    {
        "id": "HzOOI_P5d37",
        "original": null,
        "number": 1,
        "cdate": 1666411610994,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666411610994,
        "tmdate": 1666411610994,
        "tddate": null,
        "forum": "6Fq1-57gff",
        "replyto": "6Fq1-57gff",
        "invitation": "ICLR.cc/2023/Conference/Paper2103/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "While many techniques for model fairness have been proposed, the majority of them assume that the distributions of training and deployment data are identical, which is often not the case in practice. In particular, the bias between labels and sensitive groups changes, which may impair the performance of machine learning algorithms. To this end, this paper proposes a novel fair training framework that is divided into two parts: using a pre-processing approach to reflect the shifted correlation and using any existing in-processing algorithms for fair training on top of the improved data. Experimental results have validated the effectiveness of the proposed method under correlation shifts.\n",
            "strength_and_weaknesses": "Advantages:\n- This article is well-written in general and addresses a timely, and seemingly overlooked, issue in fairness.\n- Solid theoretical and simulation results. \n- Strong mitigation performance.\n\nDrawbacks:\n- One minor concern is that the authors only consider scenarios in which we know the deployment data distributions. Will the proposed solution be useful despite our lack of knowledge about deployment data distributions?\n- Furthermore, it would be great if the authors could demonstrate the results using the fairness and accuracy trade-off curve rather than a single point on the curve. This could help to better demonstrate the effectiveness of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed idea is sound, the paper is well written. The proposed method, in my opinion, is reproducible.",
            "summary_of_the_review": "This paper works on an timely problem, has a solid theoretical analysis and strong experimental results. I recommend acceptance of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2103/Reviewer_c8Rf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2103/Reviewer_c8Rf"
        ]
    },
    {
        "id": "R3kCDp_w18",
        "original": null,
        "number": 2,
        "cdate": 1666742273106,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666742273106,
        "tmdate": 1668818916924,
        "tddate": null,
        "forum": "6Fq1-57gff",
        "replyto": "6Fq1-57gff",
        "invitation": "ICLR.cc/2023/Conference/Paper2103/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the distributional shift problem in fair machine learning. The authors first propose a type of distributional shift called correlation shift, which studies the Pearson\u2019s correlation coefficient between the label and the sensitive attribute. The connection between the correlation shift and the notorious accuracy-fairness tradeoff. Based on that, the authors propose a pre-processing strategy that adjusts the ratio of data to correct the shift. Experimental results demonstrate the efficacy of the proposed technique.",
            "strength_and_weaknesses": "Strengths:\n- Distributional shifts is indeed an important problem in machine learning.\n- The proposed technique is able to handle correlation shift in the given settings.\n\nConcerns/Questions:\n- When we consider distributional shifts, should we only consider the joint distribution of label y and sensitive attribute z? I am quite surprised that we can study it without any assumption on the distribution of input feature x. The model is essentially modeling $P(y|x, z, \\theta)$ (or $P(x,y,z|\\theta)$ depending on whether the model is discriminative or not). It seems likely that $P(y,z)$ and the marginal distributions are the same but the underlying input feature's distribution $P(x)$ is different, which might still cause bad performance on either accuracy/fairness.\n- Is there any relationship between the proposed correlation shift and the concept shift in many OOD literature? \n- I am not sure how to read the simulation results in section 3. Is each blue/red dot correspond to one classifier? How are the so-called synthetic classifier generated? And what are their architectures (logistic regression, SVM, MLP)?\n- As mentioned in several sections, there are existing works on other types of distributional shifts. I think some of them should be considered as important baseline methods. \n- There should be a dedicated paragraph/section to discuss the related work and how this work differs with them.\n- Minor comments: (1) fairness is a broad area which is not only group fairness, so it is better to specify that the work is on group fairness rather than just 'fairness'; (2) before using an abbreviation, its full name should be written out first to avoid confusion (e.g., AI -> artificial intelligence (AI)); (3) please carefully check typos or grammatical errors in the revised version.\n==== After rebuttal ====\nI think the concern on x's distribution remains, e.g., how should we define the 'no drastic change in the distribution of x' more formally? Would it be too strict to hold in real-world scenarios? Why (or why not)?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper studies a novel and interesting problem, i.e., correlation shift. But the writing needs much improvement, which is sometimes vague and lack of important contents. ",
            "summary_of_the_review": "The studied problem is important as it helps with the generalization of fair machine learning methods. However, I have concerns regarding its motivation and settings for analysis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2103/Reviewer_9fri"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2103/Reviewer_9fri"
        ]
    },
    {
        "id": "o0QtgAtz7z",
        "original": null,
        "number": 3,
        "cdate": 1666977595113,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666977595113,
        "tmdate": 1666977595113,
        "tddate": null,
        "forum": "6Fq1-57gff",
        "replyto": "6Fq1-57gff",
        "invitation": "ICLR.cc/2023/Conference/Paper2103/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the challenges of in-processing fair algorithms with respect to accuracy and fairness in the setting of biased deployment data. A pre-processing step is proposed to mitigate this issue, along with an optimization approach.c",
            "strength_and_weaknesses": "Strengths: \nAn important problem is addressed here. It is valuable to understand how bias affects fairness properties. The paper is generally well-written.\n\nWeakness:\n\n1. The correlation between the sensitive attribute and the label could be more nuanced depending on the role of the sensitive attribute. For example, if the sensitive attribute is a confounder, a control variable, or even a mediator. In each case, the relation would affect the fairness properties differently. Adding some discussion around this would be helpful.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written and clear.",
            "summary_of_the_review": "I tend to marginally accept this paper given the importance of the problem and the said approach.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2103/Reviewer_gS56"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2103/Reviewer_gS56"
        ]
    },
    {
        "id": "96VJN-pERO",
        "original": null,
        "number": 4,
        "cdate": 1667510327631,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667510327631,
        "tmdate": 1671155522135,
        "tddate": null,
        "forum": "6Fq1-57gff",
        "replyto": "6Fq1-57gff",
        "invitation": "ICLR.cc/2023/Conference/Paper2103/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper discusses the idea of enforcing fairness when training and testing/deployment distributions are not identical by making two contributions-- (i) the authors propose the notion of \"correlation shifts\" which analytically captures the aforementioned issue; (ii) the authors propose a pre-processing optimization approach that corrects for correlation shifts and existing in-processing approaches can then be used on the transformed data to ensure fairness. They conduct experiments on a number of baselines that show the efficacy of their approach and also propose dataset configurations with correlation shifts in testing.",
            "strength_and_weaknesses": "- Strengths:\n1. The approach is a useful contribution to the field of fair ML since it is (a) well-motivated theoretically, (b) is a pre-processing step so advances in in-processing fair methods can still be utilized, and (c) the relaxed version of the pre-processing optimization problem is convex, ensuring \"optimality\" in approximating the true optimal.\n2. The authors conduct extensive experiments to evaluate their approach with many correlation-shifted versions of datasets (training-set as Adult dataset (1996) with test-set as ACS income (2021), for example) which I believe can also be used as benchmarks for future work in this field. I also appreciate the inclusion of empirical results when the correlation shift range $[\\alpha, \\beta]$ is not known exactly (Section B.9), or misspecified (Section 5.3). \n3. I believe the fact that the approach can work with multiple notions of fairness (such as Equalized Odds and Demographic Parity) is also a major strength.\n4. The simulation results and toy examples discussed throughout the paper increase readability and understanding quite a bit.\n\n- Weaknesses:\n1. While the authors mention many related works under \"Fairness-Specific Shifts\", I am not quite sure why some of these approaches cannot be applied to the correlation-shift problem covered in the paper. Surely, subpopulation shifts (Maity et al, 2021) seem to be a more specific and constrained version of within-group correlation shifts, is there a way to link these together theoretically? Similarly, what are the exact differences between demographic shifts (Giguere et al, 2022) and correlation shifts? It might be beneficial if the authors can provide more details explaining the above, and possible even some theoretical/empirical evidence on the same.\n2. Can the correlation shift study and pre-processing optimization approach studied in the paper have direct impact on the few existing works on poisoning attacks on fairness of classifiers [1-3]? As these attacks poison the training data, there is bound to be some correlation shift between the \"original\" test distribution (where the attacker would like the defender to fail) w.r.t the poisoned training distribution, and possible the pre-processing approach proposed can counteract the negative effects of these attacks. It would be interesting to see this discussed as well.\n\n\n- References:\n- [1] https://ojs.aaai.org/index.php/AAAI/article/view/17080\n- [2] https://link.springer.com/chapter/10.1007/978-3-030-67658-2_10\n- [3] https://link.springer.com/chapter/10.1007/978-3-031-00123-9_30",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity and Quality: Contributions of the paper are clear and it is well-written.\n- Novelty: The contributions of the paper are novel.\n- Reproducibility: The authors have provided source code for experiments but I have not run them myself.",
            "summary_of_the_review": "I recommend this paper for acceptance based on the novel ideas proposed, the extensive experimental evaluation, and theoretical analysis. To strengthen the work further, there are a few papers related to this work for which the authors could provide some more comparative details as part of the discussion section.\n\n__Edit__: After the discussion period with the AC and other reviewers, I have thought about some of the additional concerns raised. I am thus changing my score to borderline reject, but I do believe the work holds immense potential once it addresses these concerns. The main issue stems from the solution approach itself (I am summarizing from the AC/reviewer comments):\n- The authors assume that a correlation constant between the training and deployment data exists. Thus, problem is not being solved. Instead, the \"answer\" is already included in the approach to achieve fairness in the deployment phase. The challenge in addressing the data bias change is how to estimate the correlation change-- even if it is chosen to be provided as a range instead of a precise number. It is non-trivial to utilize approaches such as those of (Huang et al., 2006; Zhang et al., 2013) for estimating the correlation factor range as these are not designed to handle the shift between the label and the sensitivity attribute.\n- The experiments concerning unknown correlations, a large range of correlations, or the scenario when the knowledge of deployment data distribution is lacking, are conducted on a synthetic dataset, which might not coincide with real-world datasets.\n- After some more thought, I am inclined to agree with Reviewer 9fri's concerns regarding the input distribution, and the scenario when the input distribution itself changes drastically.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2103/Reviewer_H3mp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2103/Reviewer_H3mp"
        ]
    }
]