[
    {
        "id": "wpPgzHXGv4I",
        "original": null,
        "number": 1,
        "cdate": 1666340323867,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666340323867,
        "tmdate": 1666340323867,
        "tddate": null,
        "forum": "-bVsNeR56KS",
        "replyto": "-bVsNeR56KS",
        "invitation": "ICLR.cc/2023/Conference/Paper1622/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a contrastive learning approach to learning hierarchical document representations for multilingual and cross-lingual retrieval. The approach encodes a document in a hierarchical fashion by encoding each sentence in a document into a single fixed-length vector with an XLM-R encoder. The sequence of sentence representations is encoded by a document encoder that is trained from scratch. The overall training objective is a combination of a contrastive and masked language modeling loss. The contrastive loss masks out one of the sentence representations that is provided to the document encoder and uses an InfoNCE-like objective with intra-document as well as cross-document negatives.\n\nThe model is trained on CC-100-like data on 108 languages for 200k steps and evaluated on multilingual and cross-lingual retrieval benchmarks. The model is able to do significantly better when compared with mBERT, XLM-R and ICT baselines.",
            "strength_and_weaknesses": "Strengths\n1. The paper is well-written and explained. \n2. The motivation for the overall approach is clear and the idea is conceptually simple. With only a few different moving parts in the model, the authors are to run fairly thorough ablations.\n3. The results on both multilingual and cross-lingual retrieval benchmarks are significantly better than the baselines considered.\n\nWeaknesses\n1.  The paper's motivation seems to focus a little too much on the \"cross-lingual\" aspect of things rather than just as a multilingual hierarchical contrastive learning approach. The authors' arguments center around the fact that sentences are ordered similarly with documents across different languages.\n2. The paper lacks some details about the dataset used. The paper cites the CC-100 paper, but was it exactly CC-100? Or did you pre-process commoncrawl yourselves?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & Quality - The paper is clear and well-explained.\nNovelty - The paper isn't particularly novel, but it combines well-established InfoNCE-like contrastive learning approaches with multilingual pre-trained masked language models.\nReproducibility - The approach is simple enough that it should be possible to implement without too many details. The paper however does lack some information regarding the number of negatives used or dataset processing details.",
            "summary_of_the_review": "A simple and easy-to-implement contrastive learning approach to improve multilingual and cross-lingual retrieval. The baselines aren't particularly strong, but still a strong contribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1622/Reviewer_Hf6d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1622/Reviewer_Hf6d"
        ]
    },
    {
        "id": "veR5gsT6EAQ",
        "original": null,
        "number": 2,
        "cdate": 1666585143495,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585143495,
        "tmdate": 1666585143495,
        "tddate": null,
        "forum": "-bVsNeR56KS",
        "replyto": "-bVsNeR56KS",
        "invitation": "ICLR.cc/2023/Conference/Paper1622/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces MSM, a hierarchical multilingual encoder pre-trained with hierarchical contrastive learning. MSM applies XLMR as sentence encoder, and uses another transformer encoder to encode the document level context. During training, both masked language model loss and masked sentence model loss are applied. At inference time, the document encoder is discarded and only the sentence encoder is used. Experimental results showed that the hierarchical pre-training strategy yielded SOTA results on four cross-lingual retrieval tasks.",
            "strength_and_weaknesses": "Strength:\n- The paper is well organized and easy to follow.\n- The proposed hierarchical contrastive learning pre-training objective is simple and effective on cross-lingual retrieval tasks.\n\nWeaknesses:\n- I am a bit surprised that the document encoder is discarded after pre-training. I thought self-attention in the document encoder will be important for modeling the universal sequential sentence relation across languages.  It is unclear to me that why only sentence encoder is used for downstream finetuning. ",
            "clarity,_quality,_novelty_and_reproducibility": "Good.",
            "summary_of_the_review": "The paper introduces a simple yet effective method for improving cross-lingual retrieval. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1622/Reviewer_TsJC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1622/Reviewer_TsJC"
        ]
    },
    {
        "id": "pAVzA3upKO8",
        "original": null,
        "number": 3,
        "cdate": 1666742137158,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666742137158,
        "tmdate": 1666742137158,
        "tddate": null,
        "forum": "-bVsNeR56KS",
        "replyto": "-bVsNeR56KS",
        "invitation": "ICLR.cc/2023/Conference/Paper1622/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work aims to improve the cross-lingual retrieval capability of dual encoder models.\nGiven access to multi-lingual document corpus and the intuition that sentence ordering across languages are often similar, this work proposes to use sequential sentence relation to facilitate cross-lingual representation learning.\n\nMore specifically it proposes a hierarchical architecture (MSM), which first encodes sentences to embeddings, and then performs doc level MLM (with contrastive loss) to model sentence relations. The MLM task is constructed by pairing masked sentence representation produced by the doc transformer and the original sentence embedding. In order to combine intra and inter document negative examples, an adaptive bias term is added to the logits to make sure that one does not dominate the other.\n\nThe model is initialized from XLMR, and the pretraining loss combines token MLM and sentence MLM. The pretraining corpus is a clean version of common crawl (similar to that of XMLR).\n\nExperiments on 4 cross lingual retrieval tasks show that MSM significantly improves cross-lingual transferability over baselines (XLMR, mBERT), and also improves the fine-tune settings.  \nExperiment against stronger baselines (ICT, MSM) on 2 of the tasks shows improvement of MSM over baselines. These stronger baselines are previously only used in monolingual settings.\nOblation study shows that the best setting is 1) combining inter and intra-document negatives 2) 512 negatives 3) asymmetric projection layer  4) 2 layers of document encoder\n",
            "strength_and_weaknesses": "Overall this paper represents a great result. The idea of using sequential sentence relation to facilitate cross-lingual representation learning is both novel and effective. The bias term added to combine intra and inter-document negatives is novel.\n\nOne concern I have for the proposed approach is its efficiency -- masking document representation with leave-one-sentence-out scheme seems very expensive, since the document encoder needs to be recomputed for every masking.\n\nIt is also counter intuitive that more negatives performs worse than 512 negatives -- usually larger patch sizes improves contrastive learning.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to read and can be reproduced.",
            "summary_of_the_review": "Novel and effective approach, with a few minor questions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1622/Reviewer_VGZA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1622/Reviewer_VGZA"
        ]
    },
    {
        "id": "orn5yE_tSso",
        "original": null,
        "number": 4,
        "cdate": 1666868766708,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666868766708,
        "tmdate": 1666868766708,
        "tddate": null,
        "forum": "-bVsNeR56KS",
        "replyto": "-bVsNeR56KS",
        "invitation": "ICLR.cc/2023/Conference/Paper1622/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work proposes a new method for cross-lingual and multilingual dense retrieval, focusing on applications of query-passage and sentence retrieval in multilingual setups. The approach relies on parallel document-aligned and assumes that the sentences in such documents are roughly in the same order; the idea is then to combine the standard MLM objective with its bilingual sentence-level variant, termed Masked Sentence Modeling (MSM), hoping to guess the correct sentence (once the sentence is masked in the document), where the document encoder is shared across languages. \n\nThe experiments are conducted on four retrieval tasks from prior work, and gains over (mostly) weak baselines are reported.",
            "strength_and_weaknesses": "Strengths:\n- The idea of MSM is simple and neat, although it is very similar to the idea of next sentence prediction - the difference here is that the model performs masked sentence prediction.\n- The ablation study shows the usefulness of introducing the MSM objective.\n\nThe paper is simply not at a state to be considered for publication, with a series of major flaws as follows.\nWeaknesses:\n- Only a partial awareness of very related work on cross-lingual information retrieval (CLIR), with many strong reference works and baselines omitted from the paper completely and omitted from the comparisons. Here, only a few directly relevant papers are mentioned:\n-- https://arxiv.org/pdf/2112.09118.pdf\n-- https://arxiv.org/abs/2204.02292\n-- https://arxiv.org/abs/2004.09813\n-- https://arxiv.org/pdf/2101.08370.pdf\n-- There is also work on multilingual Longformers (e.g., https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096)\n- Related to the point above, those papers provide much stronger baselines - the baselines in this submission are simply weak and inadequate. When doing sentence retrieval, the paper should compare against strong multilingual sentence encoders and not the original off-the-shelf models. \n- The paper also does conflate query-passage and sentence retrieval, and does not evaluate on document retrieval at all. There are huge differences on how to approach each 'granularity of information' when doing retrieval, and the paper does not seem to pay attention to that: e.g., check this work for further details: https://arxiv.org/pdf/2101.08370.pdf\n- The paper also critically requires parallel data to work -> if one has parallel data, one of the must-have baselines are also MT-based query-translate or passage-translate approaches which sometimes/often work better than standard encoder-based approaches.\n- There are no discussions on how different target languages might impact the results: are all the languages equally difficult, which ones might cause major drops of performance and, most importantly, why? The paper treats multilinguality very superficially. \n\nThere are other (minor) weaknesses, including problems with language and presentation, but the major ones are mostly listed above.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper will not have any major impact as it omits many major baselines and a lot of very relevant work, offering only basic comparisons and lacking insightful side analyses. It makes a minor methodological contribution by combining document-level and sentence-level masked language modeling, which is not evaluated against cutting-edge CLIR methods.\n\nIt should be possible to reproduce the main results in the paper - it does not mention whether the results are average over several random seeds or not (and which random seed was used).",
            "summary_of_the_review": "The paper lacks strong baselines, shows only partial awareness of the current cutting-edge CLIR methodology. and it is difficult to contextualise its results (does it really bring any major improvement for CLIR?). It also does not delve deeper into intricacies of multilinguality and differences between sentence/passage/document retrieval. There are also presentation problems which make the paper seem incomplete and half-finished ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe"
        ]
    }
]