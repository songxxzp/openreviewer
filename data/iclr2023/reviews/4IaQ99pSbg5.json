[
    {
        "id": "XV9Luocrao1",
        "original": null,
        "number": 1,
        "cdate": 1666340639189,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666340639189,
        "tmdate": 1666340639189,
        "tddate": null,
        "forum": "4IaQ99pSbg5",
        "replyto": "4IaQ99pSbg5",
        "invitation": "ICLR.cc/2023/Conference/Paper1239/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed to combine variational autoencoder, prototypical network, and generative adversarial network in a way to solve unsupervised disentanglement. The proposed method was evaluated only empirically.",
            "strength_and_weaknesses": "The author attempted to solve an important and challenging problem -- unsupervised disentanglement -- but seems to have several fundamental misunderstandings of existing theories and tools:\n\n- The author misinterpreted the result of [Locatello et al., 2019] and claimed that unsupervised disentanglement is possible *as long as there is a proper inductive bias*. However, the author did not mathematically prove the proposed method can actually lead to disentanglement.\n- It seems the author does not know the difference between VAE and beta-VAE. They cited the original VAE paper (well there is even no citation in Section 2.2) but used beta-VAE instead, showing that they are unaware of the basic derivation of the learning objective (log-likelihood vs. information bottleneck).\n- The method is overcomplicated. It is unclear which part is essential and which part is optional. For example, the effect of \"latent space GAN\" is confusing.",
            "clarity,_quality,_novelty_and_reproducibility": "- Many parts of this work are based on verbal explanations without mathematical formulation. Some terms are not used in their more common sense in the context of disentanglement, such as intervention (causal inference) and action (group theory).\n- Several parts of the proposed method are not technically sound. For example, Eq. (3) uses a product of log-likelihood and KL divergence without any explanation.\n- It is hard to understand how to implement each part. The author did not provide the code, either.\n",
            "summary_of_the_review": "This work tried to solve a challenging problem but did not properly use existing methods and theories. The author misinterpreted and miscited some existing works. The mechanism of the proposed method is unclear due to the confusing writing style of this paper. Therefore I'm afraid I  cannot recommend acceptance of this paper to ICLR.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1239/Reviewer_L6Gs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1239/Reviewer_L6Gs"
        ]
    },
    {
        "id": "aYKsTmp18Sg",
        "original": null,
        "number": 2,
        "cdate": 1666549875872,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666549875872,
        "tmdate": 1666549875872,
        "tddate": null,
        "forum": "4IaQ99pSbg5",
        "replyto": "4IaQ99pSbg5",
        "invitation": "ICLR.cc/2023/Conference/Paper1239/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new approach for training disentangled variational autoencoder (VAE). The approach builds on the top of FactorVAE, and adds a prototypical network that clusters pairs of generated samples that differ in one latent dimension. The encoder and decoder are trained to be easy to be classified by the prototypical network, thus encouraging a disentangled latent space. Experiments are conducted on dSprites, 3DShapes, MPI3D, and CelebA datasets.",
            "strength_and_weaknesses": "\nStrength:\n\n* The idea is technically sound.\n\nWeaknesses:\n\n* The writing needs a lot of improvement. The main text is not self-contained: some important technical details and definitions of notations are deferred to the appendix. In addition, even after reading both the main text and the appendix, some important details are still unclear to me. Also, there are many typos. Detailed questions are below.\n\n    - Page 2: Given that locally isometric is the motivation of one key component of the proposed algorithm, it is important to explain what it means instead of only mentioning the term. \n    - Page 2: what does \"the interpretability of GANs\" mean here?\n    - Page 3: equation 13 should be equation 2\n    - Page 3: The core algorithm of generating samples for prototypical networks is deferred to the appendix. The important notations that the following paper depends on (e.g., support set, query set) are also in the appendix and are not defined anywhere in the main text. \n    - Page 3: \"The the\" should be \"The\"\n    - The notations are not consistent throughout the paper: z and \\hat{z}_k are sometimes in bold and sometimes not. The proposed algorithm is named ProtoVAE or Proto-VAE in different places.\n    - Page 4: Again, the important details of the prototypical network (needed for understanding the paper) are deferred to Appendix A.2.\n    - Equation 3: I understand the KL coefficient is used for scaling down the loss when the dimension is uninformative. However, why do you choose KL instead of other distance metrics? This is not explained in the paper.\n    - Equation 6 does not match what is described in the text \"the prototypes are actions of the different dimensions on a particular example and the query example is the action of a randomly chosen dimension on that example\". Because of that, I am not able to check the correctness/soundness of this part of the algorithm.\n    - Page 6: \"To ensure that interventions in these dimensions do not contribute to the losses, we scale equations 5 and 3 with the KL divergence values of the intervened latent dimension.\" is already explained before. Maybe you can remove it.\n    - Appendix B.1: Is \"k_{(k\\not= i)}\" a typo? k is not used in this line.\n\n* The scores of the proposed approach are much worse than state-of-the-art, and the baseline scores reported in the paper are worse than what was reported in prior work. Details are below.\n\n    - Most of the reported scores of baselines are worse than what was reported in prior work (e.g., https://arxiv.org/pdf/1906.06034.pdf). More importantly, the proposed ProtoVAE is built upon FactorVAE, but the ProtoVAE's scores are worse than FactorVAE's scores reported in prior work (e.g., https://arxiv.org/pdf/1802.05983.pdf).\n    - I understand that different implementations may lead to different scores. As the paper states, the hyperparameter settings and experimental conditions are taken from Locatello et al. But the reported scores are still not consistent with what was reported in Locatello et al.\n\n    Therefore, I am not convinced by the experimental results and have concerns about how much ProtoVAE improves upon prior work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity, quality, and reproducibility: as described above, the paper needs improvement in these aspects.\n\nNovelty: As far as I know, the idea of incorporating a prototypical network in training disentangled VAEs has not been explored before and is novel.",
            "summary_of_the_review": "\nOverall, the idea is interesting, but the current writing quality and experimental results are below the threshold. I would recommend the authors polish the writing and add more clarifications on the experimental results.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1239/Reviewer_69qY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1239/Reviewer_69qY"
        ]
    },
    {
        "id": "JKKZPpf0lE",
        "original": null,
        "number": 3,
        "cdate": 1666607366421,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607366421,
        "tmdate": 1666607366421,
        "tddate": null,
        "forum": "4IaQ99pSbg5",
        "replyto": "4IaQ99pSbg5",
        "invitation": "ICLR.cc/2023/Conference/Paper1239/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an unsupervised approach to learning disentangled latent factors within a VAE-like model. The core idea of the approach appears to be the swapping of a single value within the latent code between batch items during training; the decoded original latent and latent with the swapped value can then be compared allowing the network to isolate specific changes in the data space that the swapping induces. The training proceedusre forces the model to learn to ensure that each single latent dimension only changes one factor in the data space. Experimental results seem to indicate that the proposed approach does a significantly better job at disentanglement than existing approaches when compared using a range of measures. \n",
            "strength_and_weaknesses": "### Strengths\n\n- Experimental results appear to show good results. An appropriate range of measures is used, and the method is compared against a range of other approaches.\n- I _think_ the overall idea is sound, and is novel\n\n### Weaknesses\n\n- Clarity and presentation leaves a lot to be desired (see below)\n- Many of the experimental aspects are rather unclear\n- It's not clear to me that the \"latent space GAN\" is actually a GAN; is it not just a discriminator network that's used in the loss? \n",
            "clarity,_quality,_novelty_and_reproducibility": "There are considerable issues with the clarity of this paper, to the extent that actually trying to decipher the model being proposed is extremely hard. Section 2 attempts, but fails, to clearly explain the model and all the design decisions. It is rather difficult to decipher where the actual contributions in the model design come from as they are poorly explained. Too much space is devoted to explaining existing models (VAE or actually Beta-VAE in Section 2.2, prototypical nets in 2.4, etc). One has to refer to the algorithmic description in the appendix to try and understand section 2.3 - a plain English description of how the model works and why is missing.\n\nFurther to these points, the technical writing is of quite low quality and littered with errors, such as duplicate equations and wrong cross-referencing. In Algorithm 1 there are considerable issues of clarity and undefined variables (what is $k$ for example - in the second for loop its clear, but it appears in the comments of the first loop). \n\nIn terms of reproducibility, it might be possible to reproduce the results with trial and error to try and get all the architectural components to match, but this would likely be a significant undertaking.",
            "summary_of_the_review": "As I said in the strengths section, I do think that there might be a good, novel, idea buried within this paper. However as currently written that idea is not clear enough to justify acceptance. Hopefully the authors will work on this aspect. In terms of scoring for correctness, novely and significance I've tried to be positive, scoring on what I think the paper is trying to say rather than necessarily what it does say.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1239/Reviewer_Fvdi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1239/Reviewer_Fvdi"
        ]
    },
    {
        "id": "fQu9PdE3B_I",
        "original": null,
        "number": 4,
        "cdate": 1666973774273,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666973774273,
        "tmdate": 1666973774273,
        "tddate": null,
        "forum": "4IaQ99pSbg5",
        "replyto": "4IaQ99pSbg5",
        "invitation": "ICLR.cc/2023/Conference/Paper1239/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approach, called ProtoVAE, that learns disentangled representations from unsupervised data. The overarching goal is to incorporate two inductive biases into the learned generative model: unique and consistent changes to the latent representations, and local isometry. ProtoVAE is based on a variational autoencoder, with several modifications. First, interventions are performed in the latent space such that both a reconstruction of the original image and a generated image corresponding to the intervened latent representation are produced. Second, a prototypical network is trained with pairs of images as input to predict both the intervention dimension and the magnitude of the change. Third, a GAN discriminator attempts to predict whether a latent representation was produced by the original inference network or if it was the result of an intervention. Experiments on dSprites, 3DShapes, MPI3D, and CelebA datasets show that the method is successfully able to learn disentangled representations.",
            "strength_and_weaknesses": "Strengths\n- The overall goal of learning disentangled representations from unsupervised data is important for interpretability and controllable generation.\n- Strong experimental results on disentanglement metrics.\n\nWeaknesses\n- The proposed model is complex: it combines a VAE, a GAN, and a prototypical network. In order to justify each of these components, the experiments should isolate their relative contributions but I did not see such results in the paper.\n- The paper is difficult to read. The description of the model is not sufficient to understand how all the components fit together. Specific  matters that remain unclear include: how the interventions were performed, how the support and query sets are generated, and how the traversal dimensions for the visualizations are selected.\n- One of the stated key inductive biases is local isometry, but what exactly this means, why it is valuable, and its corresponding mathematical expressions require further elaboration.\n- The relationship between the proposed ProtoVAE and previous VAE-style approaches to disentangled representation learning is not clearly explained. What aspects of the ProtoVAE are novel and what aspects are included in previous works?",
            "clarity,_quality,_novelty_and_reproducibility": "There are several issues with clarity as mentioned above, including description of the method, explanation of local isometry, and relationship to previous work. Due to the lack of clearly explained relationships between the proposed model and baseline methods, novelty is difficult to assess. Experimental results appear to be strong but the justification for each individual component of the method is weak. Reproducibility is also a concern as the model is complex yet code is not provided (the linked Github repository has only a placeholder readme as of the writing of this review).",
            "summary_of_the_review": "Overall, the paper investigates an important goal, and there are potentially novel aspects contained in the proposed ProtoVAE model. However, the precise nature of the relationship between the ProtoVAE and previous baselines is still unclear in my mind. The disentanglement results are strong but the experimental justification for each of the components is also underdeveloped in the current version of the paper. I am hoping that these issues can be resolved during the discussion period.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1239/Reviewer_jevi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1239/Reviewer_jevi"
        ]
    }
]