[
    {
        "id": "rqln4wFVUh",
        "original": null,
        "number": 1,
        "cdate": 1666597182322,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597182322,
        "tmdate": 1670487743938,
        "tddate": null,
        "forum": "7hYCGFacpz",
        "replyto": "7hYCGFacpz",
        "invitation": "ICLR.cc/2023/Conference/Paper4449/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes Renamer, which builds upon the Transformer architecture to achieve variable renaming variance. Their approach involves two parts: view anonymization and referent binding, and they focus on representing x86-64 blocks in their implementation. Specifically, view anonymization maps all registers with the same bitwidth to the same token for input embedding, and referent binding adds an attention mask so that the attention is computed only among registers referring to the same location in memory. They evaluate their approach on BHive benchmark for throughput prediction, and add their Renamer upon BERT-Tiny, BERT-Mini, and BERT-Small. Empirical results show that their Renamer variants outperform the baseline BERT architectures on both the original test set and a variant of the test set with renamed registers. ",
            "strength_and_weaknesses": "Strengths:\n\nIn general, renaming invariance is an important desired property of neural networks for code, and the authors present a domain where achieving renaming invariance improves the performance over baseline architectures.\n\nWeaknesses:\n\nOne key issue of this work is that the empirical study is not solid. First, NA in Table 1 and Table 2 is not well justified. Training with canonicalization does not add more computational costs compared to other methods, while it achieves much better results than other baselines. Therefore, even if the authors do not have enough computational resources to evaluate larger BERT models, which should further improve the results, at least the authors should complete the evaluation of smaller BERT models.\n\nFurthermore, there is no comparison to SOTA models to justify the significance of the results. To meet their assumptions of register renaming invariance, the authors only train and evaluate on a subset of the original BHive benchmark, where they remove inputs with implicit operands. This makes it difficult to directly compare numbers in this paper to prior works that evaluate on this benchmark. Thus, the authors should evaluate the SOTA models on their subset and present the results.\n\nMeanwhile, although the paper starts with a general definition of renaming invariance, the separation of view and referent seems specific to the task in their evaluation, and does not add much to the variable renaming invariance that has been discussed in prior works. In particular, there is no analysis on why and how Renamer outperforms canonicalization.\n\nAlso, the description of Renamer architecture modification is unclear. My understanding is that view anonymization is similar in spirit to canonicalization, while referent binding adds additional constraints on the Transformer attention. In this case, I think Renamer does not preserve the expressive power of Transformer with canonicalization. More explanation about the implementation of modifications is helpful.",
            "clarity,_quality,_novelty_and_reproducibility": "The description of Renamer implementation is not clear enough.\n\nThe proposed approach is not technically very novel. The separation of view and referent seems specific to the task in their evaluation, and does not add much to the variable renaming invariance that has been discussed in prior works.\n\nThe authors did not provide the code and data for their experiments. As the technical details are not clear, it could be hard to reproduce their results.",
            "summary_of_the_review": "Proposing approaches to achieve renaming invariance while preserving the overall task performance is a good research topic. However, the evaluation in this work is not solid, the description of their approach is unclear, and the implementation of Renamer seems specific to the task in their evaluation. Therefore, I recommend a rejection.\n\n---------------\nI thank the authors for the response and paper revision. The added experiments on Backward dataset help me better understand the approach. However, I still think the technical novelty is limited, and the applicability of the approach is a bit narrow. Therefore, I slightly improve my score to 5.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4449/Reviewer_1jmm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4449/Reviewer_1jmm"
        ]
    },
    {
        "id": "g-RU0SMjXRu",
        "original": null,
        "number": 2,
        "cdate": 1666688619924,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688619924,
        "tmdate": 1666688619924,
        "tddate": null,
        "forum": "7hYCGFacpz",
        "replyto": "7hYCGFacpz",
        "invitation": "ICLR.cc/2023/Conference/Paper4449/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces the concept of renaming invariance and develops a model with this property. They carry out experiments and demonstrate that their model performs better.",
            "strength_and_weaknesses": "The paper develops good intuition with the application to formal languages and concrete examples. Experimental results are very strong on standard task.\n\nThe main weakness is that they only work with one dataset, so it's hard to know how much the technique generalizes. Since they mention applications to mathematics or synthetic grammars, it would have been nice to see such experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. It should be reproducible based on the method description and training setup. To my knowledge the work original. Transformers with invariances exist. There are invariances similar in spirit in natural language but their definition is unique enough to be novel.",
            "summary_of_the_review": "The paper is well-timed to be interesting given the interest in apply machine learning to code and mathematics.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4449/Reviewer_KJQk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4449/Reviewer_KJQk"
        ]
    },
    {
        "id": "CvjlDulXFE",
        "original": null,
        "number": 3,
        "cdate": 1666911455405,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666911455405,
        "tmdate": 1666911455405,
        "tddate": null,
        "forum": "7hYCGFacpz",
        "replyto": "7hYCGFacpz",
        "invitation": "ICLR.cc/2023/Conference/Paper4449/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presents a modified Transformer (Renamer) which is invariant under renaming of variables. The design is tested on a task to predict CPU clock cycles to execute blocks of assembly language. The proposed model is compared against an unmodified Transformer which is optionally also trained on inputs with renamed variables (augmented), or adds a preprocessing step to fix variable names for the input (canonicalized). Renamer reduces the error for the studied task compared to all three baseline models.\n",
            "strength_and_weaknesses": "The paper is well presented, both the task and architecture changes are well motivated and described clearly. The specific task studied is interesting and the proposed changes might also benefit other tasks. \n\nThe main weakness of the paper is its narrow scope. \n\nA single dataset is used for evaluation. Further, inputs which contain implicit operands are removed from the evaluation. These operations read to / write from hardcoded registers, so these are not invariant to renaming. Could they have been kept in the dataset but the hardcoded variables handled differently from those that can be renamed? \n\nThe explanation of view is closely tied to this task. Considering other tasks that are invariant to variable renaming, what is the equivalent concept and what are the implications for the proposed method? For example, if evaluating C code, variable types provide semantic information which can be separated from information about the referent. What about working with a dynamically typed language? Or similarly, with the masked attention stage, what is the effect of having a variable reassigned during a code block so that its referent changes? \n\nOverall, the paper neither seriously discusses nor tests whether the proposed method is applicable for any other tasks which are invariant to renaming of variables. To make the work more impactful it should be clarified whether and how the method would need to be modified for other tasks, preferably with demonstration of its effectiveness through additional evaluations. \n\nOnly small models are evaluated, and benefits observed in small models do not always remain for larger models. This is a minor weakness. Being able to achieve good results using small models is valuable in itself; with evaluations on larger models we might also see how much is gained by architectural changes vs more compute. \n\nQuestions:\n\nIt\u2019s interesting that the augmented model shows only a very small drop in performance when tested on renamed inputs, but overall performs less well than the model trained without data augmentation. Did you try training for longer with more variants? I\u2019d be interested to know the limits of data augmentation to improve robustness to renaming in this case. \n\nWhy did the canonicalized model take longer to train? My understanding is that the inputs were first modified to a standard version, then the model trained on these. It\u2019s not immediately obvious to me why this should take much longer to train than the basic setup. I may have misunderstood something here. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is communicated well.\n\nThe technique is novel, but potentially narrow.",
            "summary_of_the_review": "I would recommend to accept this paper for publication. The modification to the standard Transformer architecture is simple but effective.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4449/Reviewer_uTBf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4449/Reviewer_uTBf"
        ]
    }
]