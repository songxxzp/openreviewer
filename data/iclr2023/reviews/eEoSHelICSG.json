[
    {
        "id": "DJBt2APRYc",
        "original": null,
        "number": 1,
        "cdate": 1666458485166,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666458485166,
        "tmdate": 1666458485166,
        "tddate": null,
        "forum": "eEoSHelICSG",
        "replyto": "eEoSHelICSG",
        "invitation": "ICLR.cc/2023/Conference/Paper2897/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Modern methods in SSL form representations based on known or constructed relationships between samples and are particularly effective in this task. Here, this paper aim to extend this framework to incorporate algorithms based on kernel methods, where the embeddings are constructed from linear maps acting on the kernel feature space. ",
            "strength_and_weaknesses": "Strengths:\n1\u3001The theoretical derivation is rigorous;\n2\u3001The contribution description is relatively clear;\n3\u3001The writing process is clear.\n\nWeaknesses:\n1. Please describe the motivation of the proposed method in detail in the abstract;\n2. For the organization of other parts of this article, it should be directly specific to the chapter, rather than described in sequence;\n3. In the experimental part, the equipment type used and some hyper-parameter settings (the specific usage of the data set) should be described in detail;\n4. The experimental part only proves the effectiveness of the proposed method, and does not compare with other advanced algorithms;\n5. The design concept of the paper is complicated and cumbersome, please prove its reproducibility;\n6. The last sentence of the paragraph above equation (14) lacks punctuation;\n7. Tables are also a language. Don't just mention that the results of your proposed method are better than others, but try to explain why.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The description of this paper is clear, the derivation process is detailed, and it is novel, but the complex process and design make me worry about its reproducibility.",
            "summary_of_the_review": "1\u3001For a contrastive and non-contrastive loss, this paper provide closed form solutions when the algorithm is trained over a single batch of data.\n2\u3001This paper show that a version of the representer theorem in kernel methods can be used to formulate kernelized SSL tasks as optimization problems.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2897/Reviewer_FCEZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2897/Reviewer_FCEZ"
        ]
    },
    {
        "id": "y5xAywH022",
        "original": null,
        "number": 2,
        "cdate": 1666564196340,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666564196340,
        "tmdate": 1666564196340,
        "tddate": null,
        "forum": "eEoSHelICSG",
        "replyto": "eEoSHelICSG",
        "invitation": "ICLR.cc/2023/Conference/Paper2897/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies self-supervised learning (SSL) with kernels. The authors prove the representer theorem for SSL and provide explicit expressions of the optimal induced kernel for some SSL methods using contrastive or non-contrastive loss functions. The prediction accuracy on downstream tasks is analyzed based on the complexity quantity related to kernels. In numerical experiments, the authors investigate hyper-parameter tuning and the relation between data augmentation and generalization property. ",
            "strength_and_weaknesses": "Strength\n- The paper considers SSL with a linear map on an RKHS. Due to the restriction of the problem setting, the authors provide simple theoretical results that promote an intuitive understanding of SSL. The problem falls into a simple optimization problem. An explicit form of the solution is presented for non-contrastive/contrastive loss. \n\nWeaknesses\n- Nowadays, SSL is widely used for learning tasks with large data sets. Considering such a situation, the scope of this paper is relatively narrow. This paper focuses on SSL with kernel features. Though Proposition 3.1 deals with a general class of loss functions, the theoretical analysis is provided only for VICReg loss and spectral contrastive loss.\n- In analyzing the generalization performance of downstream tasks, the complexity quantity s_N(K) is considered. It is unclear how the complexity quantity relates to the concept of alignment and uniformity [1], which are regarded as important features in SSL.[1] Wang, and Isola, Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, ICML,2020.\n- In numerical experiments, feature extraction with RBF kernel is reported. However, most readers will be interested in feature extraction using deep neural networks (DNNs). The authors showed the SSL with neural tangent kernel (NTK) regime in the appendix. The authors should conduct intensive numerical studies using DNN and investigate how much their theoretical findings for the NTK regime explain numerical results. Though the authors discussed it in section 5, the numerical results in the present paper are not very informative in understanding the usefulness of SSL in practice. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not hard to follow. Since the applicability of theoretical results seems relatively narrow, I think that the paper does not have an impact that much. ",
            "summary_of_the_review": "Though the theoretical results in the paper are rigid, the scope of the paper seems narrow. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2897/Reviewer_qCTa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2897/Reviewer_qCTa"
        ]
    },
    {
        "id": "a0KMqN14JR",
        "original": null,
        "number": 3,
        "cdate": 1666670286094,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670286094,
        "tmdate": 1666670286094,
        "tddate": null,
        "forum": "eEoSHelICSG",
        "replyto": "eEoSHelICSG",
        "invitation": "ICLR.cc/2023/Conference/Paper2897/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors consider \"joint embedding\" self-supervised learning (SSL) formulations applied to a kernel learning setting.  That is, finding / learning a linear mapping in the feature space (induced by some kernel) that minimizes commonly used SSL learning objectives, such as for contrastive learning.\n\nThe authors prove properties about this mapping function and the solution to such problems, and derive closed form optimal solutions to specific formulations (objectives), as well as a semi definite programming formulation to solve a general objective over batches.\n\nThe authors experimentally explore the SSL kernel induced kernels - comparing the induced kernel from the SSL formulation (which leverages an adjacency matrix as part of fitting the kernel space mapping / the SSL objective) with the base kernel (RBF kernels used with NTK in the appendix).  They compare qualitatively by plotting the similar pairs under each kernel on the spiral toy dataset, and compare classification accuracy and for varying hyper parameters / settings. ",
            "strength_and_weaknesses": "Strengths:\n1. It is an interesting idea and direction to consider contrastive / SSL representation learning approaches from the perspective of kernel learning.\n\n2. Several useful properties are proven about the SSL kernel formulation, and solutions derived (including closed-form) for SSL formulations / objectives - which could be useful to researchers and practitioners.\n\n3. Analysis and experiments were formed to better understand ad explain the SSL induced kernels.\n\nWeaknesses:\n1.  The paper feels a bit incomplete - the properties developed are not unexpected, and the intro argues for understanding of common modern SSL representation learning methods.  However, the paper really just presents a kernel algorithm for SSL and there does not really seem to be a connection to deep learning based SSL learning methods.\n\nEssentially it's not really clear what is the real goal or objective of the paper.  If it's to propose a new SSL representation learning algorithm using kernel representations, then it needs more experiment evaluation to demonsrate its benefits over existing approaches.  If instead it's to reveal insights about current SSL representation learning approaches and losses used - this was also not accomplished.  As such, it doesn't feel complete.\n\n2. Experiments seem lacking.  Comparison with more methods and more datasets would be better.  As mentioned above, if the point is a new kernel SSL algorithm - more thorough comparison with other kernel algorithms and with existing representation learning algorithms on multiple datasets and tasks should be done - not just a simple classification task on EMNIST and MNIST comparing only to using the standard SVM classifier with RBF kernel.   If the point is to understand modern SSL algorithms, experiments using those algorithms and showing how the proposed method helps understand their results should be included.\n\nAdditionally the classification results do not seem very convincing for the benefit of using the proposed approach - in particular if I understand it correctly, Figure 3 show that the accuracy by simply using augmented training data as opposed to the induced kernel based on the augmentations consistently yields higher test accuracy.\n\n\n3. The results / resulting formulations and properties are not very surprising / seem somewhat expected, including the method itself, and seem similar and closely related to much prior work on learning on graphs or manifolds (using kernels).  It's not completely clear to me how this brings much novelty beyond past kernel approaches to learning across a graph / manifold - as these use the graph laplacian / manifold assumption in a similar way.  I think more discussion and comparison is warranted.  Baselines for comparing prediction results for a relationship-informed kernel learning representation approach should include such laplacian / manifold informed versions of the corresponding estimators, like graph / manifold regularized SVM.\n\nExamples of such work:\n- \"Kernels and Regularization on Graphs\" Smola and Kondor.\n- \"Learning on Graph with Laplacian Regularization\" Ando and Zhang\n- \"Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples\" Belkin et al.\n\n\n4. Minor comments: \n- In the introduction, one type of self supervised learning missing is the other major class of SSL not mentioned - predicting held-out parts of the input data (as in transformer models).\n- Figure 1 does not seem very helpful to me and doesn't seem to explain anything. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and good quality development of the approach and formulations, but the message of the paper is not very clear and seems like the intro and motivation do not match the body and experiments of the paper.  I think the idea of considering modern SSL loss using kernel representations is novel - so that is original - but the particular idea and formulation seems straight forward and inline with past work on kernel learning on graphs and manifolds.\n\nReproducibility as high as code is provided.",
            "summary_of_the_review": "While the development of the properties and formulations, and closed form solutions for this approach to learning and induced SSL kernel could be useful, overall I feel more like the paper is incomplete and lacks a coherent purpose and objective to achieve, and so falls short.  I think it could benefit from more experiments and comparisons, and elucidating the connection with current SSL approaches and what it reveals about them (as motivated in the introduction).   ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2897/Reviewer_Gmj4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2897/Reviewer_Gmj4"
        ]
    }
]