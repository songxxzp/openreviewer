[
    {
        "id": "aNfZ0qoYWae",
        "original": null,
        "number": 1,
        "cdate": 1666528536728,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666528536728,
        "tmdate": 1668809987673,
        "tddate": null,
        "forum": "dL35lx-mTEs",
        "replyto": "dL35lx-mTEs",
        "invitation": "ICLR.cc/2023/Conference/Paper5271/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper empirically revisits a previous hypothesis/belief from the continual learning (CL) literature, that catastrophic forgetting is not the only important quantity in CL, and that often CL methods that mitigate forgetting can harm the \"forward transfer\" between tasks.\n\nThe paper offers a new notion of transferability, by proposing an alternative evaluation method: after training a task, one *freezes* the representation learned so far, and uses it for training a $k$-shot linear probing model on the *next* consecutive task.\n\nUsing this alternative evaluation method, the authors experimentally show a positive correlation between low-forgetting and high-transferability, when comparing several different continual/multitask-learning algorithms. Finally, lower forgetting is shown to be somewhat correlated to more diverse representations.",
            "strength_and_weaknesses": "# Strengths\n1. **Importance of the research question.** The area of continual learning (CL) is rapidly growing and many *heuristic* methods are proposed to mitigate forgetting and solve continual settings. Thus, I find it important to understand the interplay between different quantities of interest, and specifically, between the forgetting and the forward transfer.  \nThe paper challenges common definitions in CL and proposes a new perspective on transferability, yielding interesting results.\n\n---\n\n# Weaknesses\n\n1. **Proposed transferability notion - Motivation, comparison to existing literature and discussion.**\n     While I agree that the proposed evaluation method (using linear probing while fixing the learned representations) makes sense and is *indeed* interesting, I would expect a clearer motivation to be given for it than the one in page 4. There is also place for more discussion.  \n     Why should one prefer this notion of transferability over the one proposed in [Wolczyk et al. 2021]? Why is an auxiliary-linear-probing even interesting when continual learning algorithms are expected to yield a *single* model?  \n     Moreover, [Wolczyk et al. 2021] showed that common CL algorithms do not help to (their definition of) forward transfer, and the \"preserved\" knowledge does not help learning future tasks, either faster (their Figure 3) or better (their Figure 4). In that case, how come these algorithms help linear probing (as seen in the new paper's experiments)? This is a little surprising for me.  \n\n1. **Experimental methodology.**\n\n     1. **Some important CL approaches were not tested.** Specifically, no regularization method (e.g., EWC) or parameter-isolation method (e.g., PackNet) were used. I believe these are more important in the paper's context than meta-learning or multitask approaches (FOMAML and MT).\n\n     1. **It is important to report the average accuracy as well.** The average forgetting and forward transfer cannot tell the whole story.\n\n     1. **There are more immediate/natural experiments to conduct.**\n     The main statement (Section 3.2) is more comparative in nature (i.e., \"less forgetful representations transfer better\") than the one in Wolczyk et al., 2021 (i.e., popular CL methods mitigate forgetting but do not contribute to transferability).  \n     That being said, I feel like comparing *different* CL methods, like done here, can be methodologically problematic (e.g., perhaps LP-FT simply yields better representations that are better for *any* cause, forgetting and transferability included).  \n     The existing experiments *are* interesting, but I believe simpler experiments could be more convincing: Fix the CL method (e.g., rehearsal/regularization), but test the influence of its main hyperparameter (e.g., buffer-size/regularization-strength, respectively) on forgetting and forward transfer.\n\n     1. **Statistical significance of results.** Even though the authors plot the error-bars according to *one* standard deviation (if I am not mistaken), the results for CIFAR-10 are often not statistically significant (see figures 1 and 3). In contrast, for CIFAR-100 the reported results are more siginificant. Consider performing additional repetitions.",
            "clarity,_quality,_novelty_and_reproducibility": "1. **Clarity.** The paper is clear and quite easy to follow.  \nMinor remarks:  \n    - In some of the figures it is slightly hard to compare both subfigures.  \nSpecifically, figure 1 and 3 could benefit from ordering both subfigures according to a monotonic ordering of the forgettings in the left subfigures. Otherwise it is difficult to verify that the same ranking/trend of the forgetting applies for the forward transfer as well.  \n    - It is worth explaining in Figure 3 why the forgetting is positive for CIFAR-100.  \n    - While the [Hadsell et al. 2020] paper offers a great *review* on the field of CL, it seems like the paper reviewed here uses it several times is a reference for empirical findings/observations that do not originate in that paper and do not appear fully in it.\n\n1. **Quality.** My reservations, especially on the methodologies are written above.\n\n1. **Reproducibility.** Sufficient details, no issues.",
            "summary_of_the_review": "The research question is important and interesting. However, it calls for a better motivation and a better comparison to existing work.  \nThe experiments *do* show interesting phenomena, but the methodology lacks some important aspects like I wrote above.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5271/Reviewer_gCd6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5271/Reviewer_gCd6"
        ]
    },
    {
        "id": "c3M1_dei-s2",
        "original": null,
        "number": 2,
        "cdate": 1666590132293,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590132293,
        "tmdate": 1668726363682,
        "tddate": null,
        "forum": "dL35lx-mTEs",
        "replyto": "dL35lx-mTEs",
        "invitation": "ICLR.cc/2023/Conference/Paper5271/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an experimental analysis of catastrophic forgetting versus forward knowledge transfer when tasks are trained continually. The hypothesis posed by the paper is that, contrary to previous findings in the literature, forward transfer does not necessarily imply more forgetting or vice-versa. The authors propose a reformulation of the forward transfer method, which relies on 'linear probing', i.e. training a simple linear classifier on top of a network trained on previous tasks, and testing this simple classifier on the target task test data. The authors also measure feature diversity as a measure of transferability of representations. In the experiments, the authors test their ideas in five benchmark datasets and compare their results to baselines such as finetuning, multitask learning, example-replay methods and metalearning methods. The authors find that less forgetful representations (features) tend to transfer better to new tasks, and these are also more diverse. ",
            "strength_and_weaknesses": "Strengths: \n- The paper tackles a very important question of whether more (or less) catastrophic forgetting is necessarily associated to more (or less) forward transfer. \n- The paper is clearly written, well aligned with literature in the area and easy to follow.\n\nWeaknesses: \n- The first main weakness that I find in this paper is in the experiments, in particular the testing of linear probing versus other methods. On page 5, architecture and training details, you mention that baseline models are trained for 50 and 100 epochs when training from scratch, depending on the dataset, and for 20 epochs only when trained from a pre-trained model. When using k-shot linear probing, you train for 100 epochs in all cases. How can you justify that this difference in training iterations is not biasing your results (in this case favouring k-shot linear probing)? \n- Although from the experimental results I agree with the statements that \"when continual learning experience begins from a randomly initialized model, retaining the knowledge of the past tasks or forgetting less on those tasks is a good inductive bias for forward transfer\" and a similar statement for pre-trained models later in the text, I think that these statements are missing a very important aspect: relatedness of tasks. Intuitively, if tasks are not related at all then not forgetting may not influence transfer at all, since only negative transfer will occur to unrelated tasks, or even more forgetting will be beneficial, depending on how much the 'direction' of forgetting is towards the new task; if tasks are related, then not forgetting will be certainly beneficial. In your experimental framework, how is task relatedness considered?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. Considering that the paper presents an experimental analysis only, the quality of the paper is satisfactory for this type of contribution. The novelty and originality of the paper lies in the research question being evaluated: to what extent forgetting related to forward transfer? The reproducibility of the paper is fair for someone working in this area.",
            "summary_of_the_review": "The good aspect that I find in this paper is the research question being examined. I think that papers like this, which are focused on examining fundamental/conceptual questions in the area, are certainly required for its progress. On the other hand, I do not find the findings of the paper 100% convincing, and I find the conclusions too broad compared to the extent of the experiments. Although I appreciate the experimental results, I would be convinced if at least one of the following was addressed (or ideally the two of them): 1) more insights into how task relatedness would affect forgetting and transfer (see one of the items I listed as 'weaknesses'); 2) a theoretical analysis supporting the experimental findings. For these reasons, and although I appreciate the effort of the paper on examining important fundamental questions in continual learning, in this first review phase I consider it marginally below the threshold of acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5271/Reviewer_2ozN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5271/Reviewer_2ozN"
        ]
    },
    {
        "id": "6mkMbPIcBi",
        "original": null,
        "number": 3,
        "cdate": 1666646082769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646082769,
        "tmdate": 1668509732712,
        "tddate": null,
        "forum": "dL35lx-mTEs",
        "replyto": "dL35lx-mTEs",
        "invitation": "ICLR.cc/2023/Conference/Paper5271/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new way of measuring forward transfer. They define forward transfer as how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. And they are looking for an answer to the question of \"whether less forgetful representations are more transferable?\" And the results show that less forgetful representations lead to a better forward transfer.",
            "strength_and_weaknesses": "Strength:\n- I think the motivation of the paper and the question they try to answer is quite significant for continual learning. Studying the relation between the forgetting and the forward transfer is one of the key aspects of continual learning.\n\nWeaknesses:\n- I think the paper lacks motivation that why the proposed definition is better than the existing transferability notions. \n- In my opinion, the paper should be written more clearly. The findings do not contradict to the previous findings but it is written in a way that they contradict. I think that is confusing for the readers. ",
            "clarity,_quality,_novelty_and_reproducibility": "I think the main problem of this paper is the clarity. I think people can reproduce the results with given details. ",
            "summary_of_the_review": "The motivation and the research question of the paper is significant and interesting for the community. If the paper is clearly written and explains the differences/similarities with existing approaches more clearly, it could be a much stronger paper. \n\nUpdate:\nAfter checking other reviews, responses, and the revised version, I change my score to 6. The motivation is more clear now. But I still share the concern of Reviewer 2ozN that the conclusions are too broad compared to the extent of the experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5271/Reviewer_pY2z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5271/Reviewer_pY2z"
        ]
    },
    {
        "id": "mhA1nAboj8",
        "original": null,
        "number": 4,
        "cdate": 1666798469328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666798469328,
        "tmdate": 1668440420104,
        "tddate": null,
        "forum": "dL35lx-mTEs",
        "replyto": "dL35lx-mTEs",
        "invitation": "ICLR.cc/2023/Conference/Paper5271/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper demonstrates that mitigating catastrophic forgetting leads to more diverse features that improve the generalization performance on future classification tasks (as measured by learning a classifier on a few data points)\n\nIn continual learning literature, learning is often constrained to prevent forgetting on older data; additionally, the model is evaluated on new tasks at the same time it is trying to prevent forgetting on old information; the constraint on future leanring due to the old tasks can degrade the forward transfer performance. \n\nIn this paper, the authors propose to isolate the impact remembering old information from the forward transfer performance. They measure the forward trasnfer performance in a separate phase by learning a classifier on top of the fixed representations using a few data point, and evaluate the classifier on a held-out set. After the forward transfer performance is measured, the model is updated on the new task using one of the many continual learning methods. \n\nFinally, the authors show that the models that forget less have more diverse features based on a metric that they propose. ",
            "strength_and_weaknesses": "# Strengths \nThe results in the paper are clear, and support the claims made by the authors---models that forget less demonstrate much better forward transfer performance as per the new definition of forward transfer. Additionally, the paper is clearly written, and all the terms are defined concisely.\n\nThe paper includes a reasonable set of baselines, and the empirical studies cover multiple benchmarks. They report results for randomly initialization networks, and pre-trained models. \n\nThe work has direct implication for the research in transfer learning, and demonstrate that foundation models that are trained to perform well on the base task trasnfer better.\n\n# Weaknesses\n\n1. If the main contribution of a paper is based on empirical results, it is crucial to fully specify the experimental protocol. The paper doesn't include details on how the hyper-parameters were selected for different methods; did the authors tune the hyper-parameters for each method individually? What were the range of hyper-parameters used for this tuning? Why is the base learning rate different for LP-FT and other methods? The experimental protocol must be fully specified in the paper or the appendix for the paper to be accepted. \n\n2. The primary contribution of the paper is the observation that foundation models that do well on the base task (can do all tasks well with little forgetting) are better at transferring than models that don't perform well on the base task (due to forgetting). The notion of continual learning is not the main emphasis of the paper. \n\n3. The compromise between remembering old information, and forward transfer is a crucial aspect of continual learning in my opinion and should not be ignored by designing an experimenl protocol that hides it. A continual learner system, in almost all cases, cannot divide experience in separate tasks, and learn a seperate predictor for each task. ",
            "clarity,_quality,_novelty_and_reproducibility": "# Suggestions\n\n\n1. I would recommend adding LP baseline that fixes the represetantions throughout learning, and just learns the final classifier for every task i.e. a baseline that removes the second phase fine-tuning of LP-FT. I suspect this baseline would do quite well, especially for the pre-trained models. \n\n2. The authors should report standard error and not standard deviation on the graph. \n\n3. \n\n# Questions: \nWhy limit LP-FT to the pre-trained setting only? ",
            "summary_of_the_review": "\n# Closing thoughts\nI am personally not convinved that the problem setting that the authors consider, separate tasks with task ids, is the meat of the continual learning problem. I also think that the compromise between remembering past information, and future learning is an important one, and we should design algorithms that can make the right compromise based on experience instead of designing a protocol that ignores the compromise. That said, I still think the paper would be useful for a small audience who do care about task incremental with task id settings. \n\nOnce the protocol for hyper-parameter selection is clear and is included, I'm happy to raise my score to an accept because I consider my primary role as to judge the scientific intregity of the work, and not to pass a judgement on the problem setting. \n\n# Update\nChanging score from 5 to 6. The authors have added details of hyper-parameter selection, and added a baseline that main claim in the paper more clear and stronger. I think the paper would be a useful addition to the conference. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5271/Reviewer_CFvX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5271/Reviewer_CFvX"
        ]
    }
]