[
    {
        "id": "8TjztwvBZ4",
        "original": null,
        "number": 1,
        "cdate": 1666546610494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666546610494,
        "tmdate": 1666546610494,
        "tddate": null,
        "forum": "0WVNuEnqVu",
        "replyto": "0WVNuEnqVu",
        "invitation": "ICLR.cc/2023/Conference/Paper1230/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a reinforcement learning-based approach for test panel selection in order to make diagnostic predictions under cost constraints. The paper deals with data imbalance in the data by a rewarding shaping approach. It proposed a semi-model-based approach to learning the diagnostic policy. Experiments are carried out on three real-world datasets to demonstrate the utility of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1. the paper deals with an important problem in machine learning for health care.\n2. The paper proposes a semi-model-based RL approach to learning the diagnostic policy, which seems suitable to the problem formulation.\n3. Experiments are carried out on three real-world datasets, which is a fair amount of different problem settings. The proposed method is compared to 12 alternatives, which is fairly exhaustive.\n\nWeakness:\n1. It is not clear to me how necessary it is to directly optimize for F1 via reward shaping, given that there are alternative linear metrics available for imbalanced classification such as the AM metric. See, for examples,\nhttps://jmlr.org/papers/v18/15-226.html\nhttps://proceedings.mlr.press/v28/menon13a.html\n\n2. It is not clear to me how interpretable the diagnostic policy might be. This is in contrast to the tree-based baseline methods compared to the proposed methods.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: I think the paper is of good quality. The authors justify the claims in the paper through their problem and method formulation as well as empirical results.\n\nClarity: I think the paper is well written, and the major arguments made by the paper can be followed easily. One particular claim that is not quite clear to me is the multi-objective formulation of the problem in (1). It seems to me this is a set of constrained optimization problems under various cost budgets rather than multiobjective?\n\nOriginality: I think the problem that the paper seeks to address is new and interesting. The paper offers a sensible approach to formulating and solving the problem.",
            "summary_of_the_review": "Overall, I am leaning toward accepting the paper because the paper deals with an interesting problem in ML for health, formulates and develops a reasonable solution to the problem, and evaluates the proposed method rigorously across multiple real-world datasets over multiple baseline methods.\n\nMy concerns are centered around the necessity of using the F1 metric, the interpretability of the proposed method given that the methods address problems in the healthcare domain, and the multi-objective formulation of the problem. It may also be desirable for the authors to discuss the applicability of alternative metrics for imbalanced classification such as the AM metrics and potentially carry out empirical comparison.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1230/Reviewer_i4nR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1230/Reviewer_i4nR"
        ]
    },
    {
        "id": "M4Xx4C41GWn",
        "original": null,
        "number": 2,
        "cdate": 1666554694872,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666554694872,
        "tmdate": 1670886730446,
        "tddate": null,
        "forum": "0WVNuEnqVu",
        "replyto": "0WVNuEnqVu",
        "invitation": "ICLR.cc/2023/Conference/Paper1230/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies sequential selection of laboratory test panels, formulated as a cost-sensitive adaptive feature selection/acquisition problem. The two main innovations are: \n1. For diagnostic performance, instead of maximizing accuracy, this work proposes to maximize F1 score to address high class imbalance (how high ). Since the F1 score cannot be written as an average of episode returns and thus cannot be directly formulated as a reward function, the paper presents new theoretical insights on transformations of the optimization problem which led to a reward shaping approach that allows RL to be applied. \n2. Viewing the problem as multi-objective optimization of diagnostic performance and cost, the proposed reward function is able to identify a superset of all Pareto optimal solutions, allowing one to recover the Pareto front of optimal F1 for each cost.\n\nExperiments were conducted on real-world datasets - ferritin, sepsis and AKI - and achieves good diagnostic performance while having lower costs than competitors. ",
            "strength_and_weaknesses": "**Strengths:**\n- The problem studied by this paper is interesting and important for many clinical classification problems subject to various budget constraints. \n- Novel theoretical insights on the how the optimization problem can be transformed to formulate a reward function that is an average of episode rewards.\n- Empirical results are strong, the proposed approach shows favorable performance on three different datasets. \n\n**Weaknesses:**\n- The title is missing keywords such as \"cost-sensitive\" and \"active feature acquisition\" (\"diagnostic policy learning\" might be a new word to many readers). \n- In Fig 5, some of the red/blue points lie outside the Pareto front. How are the Pareto front curves fitted? \n- Some statements could use a bit more clarification:\n  - in Step 3, $(s',a')$ with the apostrophe is typically used to indicate next state-action pair, where here in the occupancy definition they represent the previous state-action pair. It is suggested that the authors change the notation here to something else like $(\\tilde{s}, \\tilde{a})$ throughout. \n  - in the abstract, \"the F1 score cannot be formulated as a simple sum of cumulative rewards\" - on first read this sentence is confusing, it seems to be discussing the cumulative rewards of a single trajectory, but the F1-score is ill defined at the trajectory/instance level. Only after reading the paper did I understand it's actually talking about the sum (or average) of multiple trajectories. I suggest this sentence be rephrased. \n  - on page 4, Step 2: \"Fix any specific pair (B,B')\" should be \"Fix any specific pair (B,K)\"\n  - in Sec 4 the definition of TP(\\pi) vs TP(\\mu) seems inconsistent on their normalization. Are they the number of true positives, or the true positive rate? \n  - in Sec 3.1 and Fig 1 \"A penalty will [be] generated if the diagnosis does not match the ground truth $y$.\" This sentence is inconsistent with Sec 4 Step 3 reward definition $\\mathrm{TN} + \\lambda \\mathrm{TP} + \\rho \\mathrm{Cost}$ where no penalty is assigned, only positive rewards for true pos/neg. \n  - On page 5: regarding \"semi-model-based\", could you clarify why the reward needs to be learned? It seems that the reward r(s,a) should only depend on the diagnosis prediction and the true label. \n  - Sec 3.1 should also clarify that the transition dynamics of the MDP can be perfectly specified. \n   - in Sec 5.3: Eqn (3) is not only the panel selector, it's panel/prediction selector which is the policy network. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** This paper is largely well written. \n\n**Quality:** The proposed method is accompanied with high-level summary of the main theoretical results and detailed derivations in appendix, and validated on three real-world clinical datasets. \n\n**Novelty:** Based on the background of this reviewer, this work presents an interesting solution for active feature acquisition that incorporates classification metrics other than accuracy (here the F1-score). I am not too familiar with the literature of this area so cannot comment on the extensiveness of related works. \n\n**Reproducibility:** Code is provided as part of the supplement and looks like a good-faith effort of showing commitment of open-sourcing from by the authors. ",
            "summary_of_the_review": "The problem studied is interesting and relevant, the proposed solution makes sense. Certain claims need refinement and experiments need some clarifications. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1230/Reviewer_Ws9d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1230/Reviewer_Ws9d"
        ]
    },
    {
        "id": "nfZEDXLKliX",
        "original": null,
        "number": 3,
        "cdate": 1666665864752,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665864752,
        "tmdate": 1666665864752,
        "tddate": null,
        "forum": "0WVNuEnqVu",
        "replyto": "0WVNuEnqVu",
        "invitation": "ICLR.cc/2023/Conference/Paper1230/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates the use of reinforcement learning for lab test panel optimization, aiming to dynamically prescribe test panels based on available observations, to maximize diagnosis/prediction accuracy while keeping testing at a low cost. Given that clinical diagnostic data are often highly imbalanced, the authors maximize the F1 score instead of the error rate, and further develop a reward-shaping approach by leveraging the duality of policy optimization, so that the problem can be solved by standard RL methods. ",
            "strength_and_weaknesses": "Strength:\n1. The studied problem is very interesting and practically useful, and I think using reinforcement learning for lab test panel optimization is an appropriate way.\n2. The proposed method seems sound to me (but I did not check the proofs).\n3. The writing is good.\n\nWeakness:\n1. The reported empirical performance of the proposed method has an improvement in most cases, compared to other methods. However, the F1 score still seems pretty low, which may still not be appropriate for clinical use.\n\n2. I would suggest the authors pay attention to the format, avoiding using too many \\vspace{}; some parts can be moved to the appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear. The proposed method is novel, as far as I know. The proposed method is also sound to me.",
            "summary_of_the_review": "The presentation is clear. The proposed method is novel and sound to me. Although the empirical performance is better than other methods in most cases, the F1 score still seems pretty low, which may still not be appropriate for clinical use.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1230/Reviewer_Yh4E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1230/Reviewer_Yh4E"
        ]
    }
]