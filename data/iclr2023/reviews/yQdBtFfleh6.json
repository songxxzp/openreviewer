[
    {
        "id": "4J6TubtkV-",
        "original": null,
        "number": 1,
        "cdate": 1666429848802,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666429848802,
        "tmdate": 1666429848802,
        "tddate": null,
        "forum": "yQdBtFfleh6",
        "replyto": "yQdBtFfleh6",
        "invitation": "ICLR.cc/2023/Conference/Paper371/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper indicates that a model with residual-like bocks can also be considered as a learnable Markov chain.\nThey prove that a Markov chain can shift source input to the target domain through L nodes in an efficient way if the Markov chain is $\\delta$-convex.\nBy solving a Markov chain optimization problem they introduce the panel connection method, which is acting like a model regularization.\nFinally, they conduct experiments to demonstrate the superiority of the Markov chain.",
            "strength_and_weaknesses": "Strength:\n\n[1] Interpret the skip connection with the help of the Markov chain is reasonable and innovative.\n\n[2] When exploring the optimization of the Markov chain, the gradient to $z_l$ seems to add a simple penalization on the $z_l$, which is interesting and easy to use.\n\n[3] This paper evaluated their method on both Natural Language Processing and Computer Vision. Results show that their method improves the performance and the convergence of deep neural networks.\n\nWeaknesses:\n\n[1] Can authors analyze why the proposed method can solve the model degradation problem? Will the penal connection make the convolutional layer more sparsity, and can other regularization methods (i.e. $L_1$ Norm) solve this problem?\n\n[2] Experiments on more deeper networks (i.e. ResNet101, ResNet152, ResNet110, ResNet1202) are needed to evaluate for demonstrating that the Markov chain can better solve the model degradation problem than the residual-like model.\n\n[3] I have noticed ReLU is put before Linearly Layer in Figure 1 in the supplementary materials, so the model used in the main paper is whether ResNets or PreActResNets?",
            "clarity,_quality,_novelty_and_reproducibility": "[1] This paper is clearly written and well organized.\n\n[2] The main claim is well supported by either proof or experimental results.\n\n[3] Rethinking the skip connection model as a learnable Markov chain is novel.\n\n[4] The method is easy to reproducible.\n",
            "summary_of_the_review": "This paper rethinks the skip connection model as a learnable Markov chain and proposes a novel penal connection for better optimization.\nMost of the theorem and experiments are satisfied. However, there are still some concerns as written in \"Strength And Weaknesses\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper371/Reviewer_Pv9F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper371/Reviewer_Pv9F"
        ]
    },
    {
        "id": "M1-Lqz9bJ60",
        "original": null,
        "number": 2,
        "cdate": 1666578568708,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578568708,
        "tmdate": 1668995766245,
        "tddate": null,
        "forum": "yQdBtFfleh6",
        "replyto": "yQdBtFfleh6",
        "invitation": "ICLR.cc/2023/Conference/Paper371/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper rethinks the skip connection model as a learnable Markov chain and proposes a penal connection to convert a residual-like model to a Markov chain for more efficient training. Experiments on MLP and CV demonstrate the superiority of this plug-in method.",
            "strength_and_weaknesses": "Pros\n\n- This paper formulated models with skip connections as a learnable Markov chain. And introduced the ideal direction for improving the efficiency of the Markov chain.\n- The proposed penal connection is a plug-in operation and the experiment evaluation the efficiency of this method.\n\nCons\n\n- Previous works [1,2] have already formulated the VGG-like models with the Markov chain, can you explain the advantage of ResNet over VGG under the Markov chain\u2019s guide?\n- The compared models\u2019 performance in Table 2 is lower than their original paper, can you explain why, and can the proposed method promote their SOTA implementation?\n\n[1] Opening the black box of Deep Neural Networks via Information \n\n[2] Markov Chain Neural Networks",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well writing and easy to follow.",
            "summary_of_the_review": "- This paper missing the comparison with the previous close-related methods.\n- This paper needs to compare their performance on a better-implied baseline.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper371/Reviewer_CKAs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper371/Reviewer_CKAs"
        ]
    },
    {
        "id": "PqcRFXQVM4",
        "original": null,
        "number": 3,
        "cdate": 1666633722844,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633722844,
        "tmdate": 1666633722844,
        "tddate": null,
        "forum": "yQdBtFfleh6",
        "replyto": "yQdBtFfleh6",
        "invitation": "ICLR.cc/2023/Conference/Paper371/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper reformed skip connections into a learnable Markov chain and proposed a simple routine of penal connection to make any residual-like model become a learnable Markov chain. The ablation analysis and the compared experiments showed that the proposed method achieves more competitive results than the SOTA methods. Overall, this paper has a solid theoretical foundation and novel method.",
            "strength_and_weaknesses": "Strengths:\nThis paper has a solid theoretical foundation and novel method. \nWeaknesses:\n1\u3001\tThe main contribution of this paper has not been summarized.\n2\u3001\tIn Fig.1, f_{\\theta_{l}} is a residual-like block. Can you explain the difference between f_{\\theta_{l} and residual block?\n3\u3001\tHow to understander the definition of ideal direction? The relationship between \u2113(a, c) \u2265 \u2113(\u00b5a+ (1\u2212\u00b5)b, c) \u2265 \u2113(b, c) and Eq.(1).\n4\u3001\tIn Fig.4, can you plot the curves about \u2113(xl \u2212 \u03b7gxl , y) and  \u2113(xl , y)  to illustrate \u2113(xl \u2212 \u03b7gxl , y) < \u2113(xl , y) always holds?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and presents a new perspective on residual networks. I think it's great research.",
            "summary_of_the_review": "This paper reformed skip connections into a learnable Markov chain and proposed a simple routine of penal connection to make any residual-like model become a learnable Markov chain. The ablation analysis and the compared experiments showed that the proposed method achieves more competitive results than the SOTA methods. Overall, this paper has a solid theoretical foundation and novel method. I think it can be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper371/Reviewer_Z5HW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper371/Reviewer_Z5HW"
        ]
    },
    {
        "id": "4HSDyC6XfN",
        "original": null,
        "number": 4,
        "cdate": 1667141528193,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667141528193,
        "tmdate": 1667141528193,
        "tddate": null,
        "forum": "yQdBtFfleh6",
        "replyto": "yQdBtFfleh6",
        "invitation": "ICLR.cc/2023/Conference/Paper371/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper formulates skip connection based models a learnable Markov chain. The paper then introduces the content of an efficient Markov chain which maps data from the input to the target domain efficeintly. A simple regularisation scheme is introduce to enable the learning of efficient Markov chains. Evaluation is performed on Transformer and ResNet type models. ",
            "strength_and_weaknesses": "Strengths:\n* The Markov Chain perspective on skip connection type models is novel and interesting.\n* The paper includes useful illustrations e.g. Fig 2 and Fig 3, which make the paper clearer.\n* The paper includes encouraging results on top-1 accuracy on ImageNet-1k across several state of the art models.\n\nWeaknesses:\n* Central claims of the paper are not clear: Consider the \u201cinefficient\u201d Markov chain in Fig 2 (a). Both the inefficient and efficient Markov chains successfully convert data from the source to the target domain. In fact, for a classification type model there should be no difference in the finally test accuracies of both efficient and inefficient models. If learning an efficient Markov chain is hard, it is not clear what is the advantage of an efficient Markov chain. Perhaps the paper wants to claim that efficient Markov chains leads to better fit to the target distribution? \n* In Proof 2.3, the paper derives only an c value for \\epsilon. What is the quality of the approximation? Are there any upper bounds to the absolute error? This is important because the approximation has a direct impact on the quality of the proposed regularizer. (The steps in Proof 2.3 should be numbered).\n* It is not clear if a higher value of \\epsilon leads to a better model? The paper should show better evidence for this. In Fig 4 (b) the difference in \\epsilon between the two models is minimal. In fact, the value of \\epsilon peaks early in training, but is not accompanied by peak accuracy?\n* The paper should also consider including additional state of the art baselines for the machine translation task, as currently only the plain Transformer is considered.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The paper is interesting, however some central claims are not clear (see above).\n\nClarity: Clarity can be improved, especially with regards to the central claims of the paper.\n\nOriginality: To the best of my knowledge, the proposed idea is novel.\n",
            "summary_of_the_review": "The paper is interesting, however it is not clear if higher \\epsilon always leads to better test accuracy and whether having a higher \\epsilon is actually necessary. Moreover, the quality of the approximation of \\epsilon considered in the paper should be quantised with an upper bound of the error.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper371/Reviewer_Geje"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper371/Reviewer_Geje"
        ]
    }
]